{
  "metadata": {
    "title": "Microservices Architecture for eCommerce",
    "author": "Piotr Karwatka",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 122,
    "conversion_date": "2025-12-19T17:35:17.845448",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Microservices Architecture for eCommerce.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-9)",
      "start_page": 1,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "Table of contents\n\nForeword\n\nDivide and conquer\n\n2 3\n\nChange is too slow\n\n4\n\nIn e-Commerce: your software is your company\n\n4\n\nOmnichannel\n\n5\n\nAbout the authors\n\n6\n\nTable of contents\n\n10\n\nMicroservices\n\n11\n\nThe criticism\n\n14\n\nEvolutionary approach Best practices\n\n16 21\n\nCreate a Separate Database for Each Service\n\n22\n\nRely on contracts between services\n\n24\n\nDeploy in Containers\n\n24\n\nTreat Servers as Volatile\n\n25\n\nCase Studies: Re-architecting the monolith\n\n27\n\nB2B\n\n28\n\nMobile Commerce\n\n31\n\nRelated techniques and patterns\n\n41\n\nFundamentals of distributed systems\n\n42\n\nMicroservices Architecture for eCommerce\n\nDesign patterns\n\n44\n\nIntegration techniques\n\n52\n\nDeployment of microservices\n\n64\n\nServerless - Function as a Service\n\n74\n\nContinuous Deployment\n\n82\n\nRelated technologies\n\nMicroservices based e-commerce platforms\n\nPiotr Karwatka 85 Mariusz Gil Mike Grabowski\n\n86\n\nTechnologies that empower microservices achitecture\n\n74\n\nAleksander Graf\n\nDistributed logging and monitoring\n\nBlogs and resources\n\nGo to Table of Contents\n\nPaweł Jędrzejewski\n\n105 Michał Kurzeja\n\n112 Antoni Orﬁn Bartosz Picho 1\n\nForeword\n\nName a technology conference or meetup and I’ll tell you about the\n\nconstant speeches referencing microservices. This modern engineering\n\ntechnique has grown from good old SOA (Service Oriented Architecture)\n\nwith features like REST (vs. old SOAP) support, NoSQL databases and the\n\nEvent driven/reactive approach sprinkled in.\n\nWhy have they become so important? Roughly speaking, because of what\n\nscale systems achieve nowadays and the number of changes that are\n\ndeployed on a daily basis.\n\nOf course microservices aren’t a panacea. I’ve tried to make this book as\n\ninformational and candid as\n\nI can. Although we promote the\n\nmicroservices architecture across the following chapters, please also take\n\nMicroservices Architecture\n\na look at Appendix 1 authored by Spryker’s Co-Founder Alexander Graf\n\nwith a very candid and pragmatic view on this topic.\n\nfor eCommerce\n\nThis book is a rather “technical one” - starting with some Business\n\nrationale for microservices and then stepping into the engineers’\n\nshoes and trying to show you the tools and techniques required to\n\nbuild and scale modern eCommerce systems.\n\n2\n\nGo to Table of Contents\n\nDivide and conquer\n\nThe original Zalando site was built on Magento using PHP, and at one\n\nForeword\n\ntime was the biggest Magento site in the world. The German eCommerce\n\ngiant that employs over 10,000 people and ships more than 1,500 fashion\n\nbrands to customers in 15 European countries generated $3.43 billion in\n\nName a technology conference or meetup and I’ll tell you about the\n\nrevenue last year. With over 700 people on its engineering team, they\n\nconstant speeches referencing microservices. This modern engineering\n\nmoved to microservices in 18 months.\n\ntechnique has grown from good old SOA (Service Oriented Architecture)\n\nwith features like REST (vs. old SOAP) support, NoSQL databases and the\n\nThe key advantages of the microservice approach are:\n\nEvent driven/reactive approach sprinkled in.\n\nFaster Time to Market - because of the decentralized development\n\nprocess and opportunities to\n\ninnovate given to each separate\n\nWhy have they become so important? Roughly speaking, because of what\n\ndevelopment team.\n\nscale systems achieve nowadays and the number of changes that are\n\ndeployed on a daily basis.\n\nLess is more - the microservices approach leverages the Single\n\nResponsibility Principle which means that a single microservice\n\nperforms exactly one business function. Therefore developers can\n\nOf course microservices aren’t a panacea. I’ve tried to make this book as\n\ncreate\n\nmore\n\nefﬁcient,\n\nclear\n\nand\n\ntestable\n\ncode.\n\ninformational and candid as\n\nI can. Although we promote the\n\nmicroservices architecture across the following chapters, please also take\n\nDomain Expertise - business features are granularly split into separate\n\nMicroservices Architecture\n\na look at Appendix 1 authored by Spryker’s Co-Founder Alexander Graf\n\nmicro-applications. You’ll have separate services for promotions,\n\nwith a very candid and pragmatic view on this topic.\n\nfor eCommerce\n\ncheckout and products catalog. Each development team typically\n\nincludes business analysts and developers. It builds engagement and\n\nspeeds up development.\n\nThis book is a rather “technical one” - starting with some Business\n\nrationale for microservices and then stepping into the engineers’\n\nAccountability - Booking.com’s approach to development is to\n\nshoes and trying to show you the tools and techniques required to\n\npromote the teams whose features are published for production (before\n\nbuild and scale modern eCommerce systems.\n\nthe features are usually proven to increase conversion). By working on\n\n3\n\nGo to Table of Contents\n\nthe basis of microservices you’ll have separate teams accountable for\n\nparticular KPIs, providing SLA’s for their parts, etc. A side effect of this\n\napproach is usually the rise of employee effectiveness and engagement.\n\nEasier outsourcing - because services are separable and usually\n\ncontracts between them have to be well documented, it’s rather easy to\n\nuse ready-made products or outsource particular services to other\n\ncompanies.\n\nChange is too slow\n\nIt’s something I usually hear when starting a new consulting engagement.\n\nAfter a few years in the market, enterprises tend to keep the status quo,\n\nand try to keep everything running smoothly, but nowadays it’s not\n\nsufﬁcient to become a market leader. It’s crucial to experiment, change,\n\ntest and select the best solutions. But it’s extremely hard to work like that\n\nwith a team of a few dozen engineers and extremely sophisticated\n\nbusiness rules coded to the metal by thousands of lines of code. The\n\nmicroservics approach became so popular because it breaks this into\n\nsmaller, self-sufﬁcient and granular areas of responsibility that are easy to\n\ntest and deploy.\n\nIn eCommerce: your software is your company\n\nOrganizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\n\n— M. CONWAY\n\nGo to Table of Contents\n\n4\n\nthe basis of microservices you’ll have separate teams accountable for\n\nAmong all the technical challenges, microservices usually require\n\nparticular KPIs, providing SLA’s for their parts, etc. A side effect of this\n\norganizational changes inside the company. Breaking the technical\n\napproach is usually the rise of employee effectiveness and engagement.\n\nmonolith quite often goes hand in hand with dividing enterprise\n\ndepartments into agile, rapid teams to achieve faster results. In the end,\n\nEasier outsourcing - because services are separable and usually\n\nthe ﬁnal outcome is that processes that took a few months can now be\n\ncontracts between them have to be well documented, it’s rather easy to\n\nexecuted in weeks and everybody feels engaged. It’s something you\n\nuse ready-made products or outsource particular services to other\n\ncannot underestimate.\n\ncompanies.\n\nOmnichannel\n\nChange is too slow\n\nTo fulﬁll your customer’s expectations about omnichannel, you have to\n\nintegrate each and every piece of information about products, shipments,\n\nIt’s something I usually hear when starting a new consulting engagement.\n\nstocks and orders, and keep it up to datefresh. There is no single system\n\nAfter a few years in the market, enterprises tend to keep the status quo,\n\nto deal with POS applications, ERP, WMS and eCommerce\n\nand try to keep everything running smoothly, but nowadays it’s not\n\nresponsibilities. Of course, I’ve seen a few that pretend to be a One-stop\n\nsufﬁcient to become a market leader. It’s crucial to experiment, change,\n\nsolution but I’ve never seen anything like that in production. The key is to\n\ntest and select the best solutions. But it’s extremely hard to work like that\n\nintegrate systems that are optimal for their niches and already integrated\n\nwith a team of a few dozen engineers and extremely sophisticated\n\nwithin your existing processes. Microservices are great for such an\n\nbusiness rules coded to the metal by thousands of lines of code. The\n\nevolutionary approach. We’ll describe a case study - where by exposing\n\nmicroservics approach became so popular because it breaks this into\n\nthe APIs from PIM, CRM, ERP and creating a dedicated UI facade, we\n\nsmaller, self-sufﬁcient and granular areas of responsibility that are easy to\n\nleveraged on this approach to provide a sophisticated B2B solution.\n\ntest and deploy.\n\nThis eBook will try to help you decide if it is time for applying this\n\nIn eCommerce: your software is your company\n\napproach and how to start by referencing to few popular techniques\n\nand tools worth following.\n\nOrganizations which design systems ... are constrained to\n\nproduce designs which are copies of the communication\n\nLet’s get started!\n\nstructures of these organizations.\n\nPiotr Karwatka, CTO at Divante\n\n— M. CONWAY\n\n5\n\nGo to Table of Contents\n\nAbout the authors\n\nGo to Table of Contents\n\n6\n\nPiotr Karwatka\n\nCTO at Divante. My biggest project? Building the company from 1 ->\n\n150+ (still growing), taking care of software productions, technology\n\ndevelopment and operations/processes. 10+ years of professional\n\nSoftware Engineering and Project Management experience. I've also tried\n\nmy hand at writing, with the book \"E-Commerce technology for\n\nmanagers\". My career started as a software developer and co-creator of\n\nabout 30 commercial desktop and web applications.\n\nMichał Kurzeja\n\nCTO and Co-Founder of Accesto with over 8 years of experience in\n\nleading technical projects. Certiﬁed Symfony 3 developer. Passionate\n\nabout new technologies; mentors engineers and teams in developing\n\nhigh-quality software. Co-orgaizer of Wrocław Symfony Group meetups.\n\nAbout the authors\n\nMariusz Gil\n\nSoftware Architect and Consultant, focused on high value and high\n\ncomplexity, scalable web applications with 17+ years of experience in the\n\nIT industry. Helps teams and organizations adopt good development and\n\nprogramming practices. International conference speaker and developer\n\nevents organizer.\n\nBartosz Picho\n\neCommerce Solution Architect, responsible for Magento 2 technology at\n\nDivante. Specialized in application development end 2 end: from\n\nbusiness\n\nrequirements\n\nto\n\nsystem architectures, meeting high\n\nperformance and scalability expectations. Passionate technologist,\n\nexperienced in Magento 1 and 2, both Community and Enterprise\n\neditions.\n\n7\n\nGo to Table of Contents\n\nAntoni Orﬁn\n\nSolutions Architect specialized\n\nin designing highly-scalable web\n\napplications and\n\nintroducing best practices\n\ninto\n\nthe software\n\ndevelopment process. Speaker at several IT conferences. Currently\n\nresponsible for systems architecture and driving DevOps methodology at\n\nDroplr.com.\n\nMike Grabowski\n\nSoftware Developer and open source enthusiast. Core contributor to\n\nmany popular libraries, including React Native, React Navigation and\n\nHaul. Currently CTO at Callstack.io. Travels the world teaching\n\ndevelopers how to use React and shares his experience at various\n\nReact-related events.\n\nPaweł Jędrzejewski\n\nFounder and Lead Developer of Sylius, the ﬁrst Open Source eCommerce\n\nframework. Currently busy building the business & ecosystem around the\n\nproject while also speaking at international tech conferences about\n\neCommerce & APIs.\n\nAlexander Graf\n\nCo-Founder and CEO of Spryker Systems. Alexander Graf (*1980) is one\n\nof Germany’s leading eCommerce experts and a digital entrepreneur of\n\nmore than a decade’s standing. His widely-read blog Kassenzone (“The\n\nCheck-Out Area”) has kicked off many a debate among commerce\n\nprofessionals. Alexander wrote Appendix 1 to this book.\n\nGo to Table of Contents\n\n8\n\nAntoni Orﬁn\n\nAknowledgement\n\nSolutions Architect specialized\n\nin designing highly-scalable web\n\napplications and\n\nintroducing best practices\n\ninto\n\nthe software\n\nI believe in open source. This book was intended to be as open as\n\ndevelopment process. Speaker at several IT conferences. Currently\n\npossible. I would like to thank all the enthusiasts engaged in this project -\n\nresponsible for systems architecture and driving DevOps methodology at\n\ngiving me honest feedback, helping with editorials etc.\n\nDroplr.com.\n\nMateusz Gromulski, Will Jarvis, Ian Cassidy, Jacek Lampart, Agata\n\nMike Grabowski\n\nMłodawska, Tomasz Anioł, Tomasz Karwatka, Cezary Olejarczyk\n\nSoftware Developer and open source enthusiast. Core contributor to\n\nmany popular libraries, including React Native, React Navigation and\n\nHaul. Currently CTO at Callstack.io. Travels the world teaching\n\nThank you guys!\n\ndevelopers how to use React and shares his experience at various\n\nReact-related events.\n\nPaweł Jędrzejewski\n\nFounder and Lead Developer of Sylius, the ﬁrst Open Source eCommerce\n\nframework. Currently busy building the business & ecosystem around the\n\nproject while also speaking at international tech conferences about\n\neCommerce & APIs.\n\nAlexander Graf\n\nCo-Founder and CEO of Spryker Systems. Alexander Graf (*1980) is one\n\nof Germany’s leading eCommerce experts and a digital entrepreneur of\n\nmore than a decade’s standing. His widely-read blog Kassenzone (“The\n\nCheck-Out Area”) has kicked off many a debate among commerce\n\nprofessionals. Alexander wrote Appendix 1 to this book.\n\n9",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-17)",
      "start_page": 10,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "Table of contents\n\nForeword\n\nDivide and conquer\n\nChange is too slow\n\nIn e-Commerce: your software is your company\n\nOmnichannel\n\nAbout the authors\n\nTable of contents\n\nMicroservices\n\nThe criticism\n\nEvolutionary approach\n\nBest practices\n\nCreate a Separate Database for Each Service\n\nRely on contracts between services\n\nDeploy in Containers\n\nTreat Servers as Volatile\n\nRelated techniques and patterns\n\nDesign patterns\n\nIntegration techniques\n\nDeployment of microservices\n\nServerless - Function as a Service\n\nContinuous Deployment\n\nRelated technologies\n\nMicroservices based e-commerce platforms\n\nTechnologies that empower microservices achitecture\n\nDistributed logging and monitoring\n\nCase Studies: Re-architecting the monolith\n\nB2B\n\nMobile Commerce\n\nBlogs and resources\n\nGo to Table of Contents\n\n10\n\n2 3\n\n4\n\n4\n\n5\n\n6\n\n10\n\n11\n\n14\n\n16\n\n21\n\n22\n\n24\n\n24\n\n25\n\n27\n\n30\n\n39\n\n50\n\n60\n\n69\n\n72\n\n73\n\n77\n\n91\n\n98\n\n99\n\n110\n\n112\n\nTable of contents\n\nMicroservices\n\n11\n\nGo to Table of Contents\n\nMicroservices\n\nMicroservice architecture structures the application as a set of loosely\n\ncoupled, collaborating services. Each service implements a set of related\n\nfunctions. For example, an application might consist of services such as an\n\norder management service, an inventory management service, etc.\n\nServices communicate using protocols such as HTTP/REST or (a less\n\npopular approach) using an asynchronous approach like AMQP. Services\n\ncan be developed as separate applications and deployed independently.\n\nData consistency is maintained using an event-driven architecture\n\nbecause each service should have its own database in order to be\n\ndecoupled from other services.\n\nThe most common forces dictating the Microservice approach¹:\n\nMultiple teams of developers working on a single application.\n\nSystem must be easy to understand and maintain/modify, no matter the\n\nnumber of changes deployed.\n\nUrgency for new team members to be productive.\n\nNeed for continuous deployment (although possible to achieve with\n\nmonolith design, microservices include some features of DevOps\n\napproach by design).\n\nScalability requirements that require running your application across\n\nserver clusters.\n\nDesire to adopt emerging technologies (new programming languages,\n\netc.) without major risks.\n\n¹ According to: http://microservices.io/patterns/microservices.html\n\nGo to Table of Contents\n\n12\n\nThe assumptions of the orthogonal architecture followed by\n\nMicroservices\n\nmicroservices architects implies the following beneﬁts:\n\nMicroservice architecture structures the application as a set of loosely\n\nEach microservice could be deployed separately and without shutting\n\ncoupled, collaborating services. Each service implements a set of related\n\ndown the whole system.\n\nfunctions. For example, an application might consist of services such as an\n\norder management service, an inventory management service, etc.\n\nEach microservice can be developed using different technologies while\n\nallowing them to publish HTTP end-points (Golang based services can\n\nServices communicate using protocols such as HTTP/REST or (a less\n\ninteroperate with PHP, Java…).\n\npopular approach) using an asynchronous approach like AMQP. Services\n\ncan be developed as separate applications and deployed independently.\n\nBy deﬁning strict protocols (API), services are easy to test and extend\n\nData consistency is maintained using an event-driven architecture\n\ninto the future.\n\nbecause each service should have its own database in order to be\n\ndecoupled from other services.\n\nMicroservices can be easily hosted in the cloud, Docker environments,\n\nor any other server platform, and can be very easily scaled as each\n\nThe most common forces dictating the Microservice approach¹:\n\nservice can live on its own server(s), VPS(es) etc.\n\nMultiple teams of developers working on a single application.\n\nThe services are easy to replace.\n\nSystem must be easy to understand and maintain/modify, no matter the\n\nnumber of changes deployed.\n\nServices are organized around capabilities, e.g., UI, front-end,\n\nUrgency for new team members to be productive.\n\nrecommendation, logistics, billing, etc.\n\nNeed for continuous deployment (although possible to achieve with\n\nmonolith design, microservices include some features of DevOps\n\nThe scalability and deployment processes of microservice-based systems\n\napproach by design).\n\ncan be much easier to automate compared to monolithic architectures.\n\nScalability requirements that require running your application across\n\nThe DevOps approach to infrastructure along with Cloud services is\n\nserver clusters.\n\ncommonly in use. The examples of Spotify and Netﬂix² inspire IT\n\nDesire to adopt emerging technologies (new programming languages,\n\nengineers to implement continuous delivery and monitoring.\n\netc.) without major risks.\n\n¹ According to: http://microservices.io/patterns/microservices.html\n\n² https://www.nginx.com/blog/microservices-at-netﬂix-architectural-best-practices/\n\n13\n\nGo to Table of Contents\n\nDockerization of IT environments, monitoring tools and DevOps tools\n\n(Ansible, Chef, Puppet and others) can take your development team to\n\nthe the next level of effectiveness.\n\nA\n\nB\n\nOperations\n\nCore Team\n\nQuality assurance\n\nCross-functional team\n\nCross-functional team\n\nDevelopment\n\nCross-functional team\n\nCross-functional team\n\nCross-functional team\n\nCross-functional team\n\nFig. 1: A microservice approach encourages enterprises to become more agile, with\n\ncross-functional teams responsible for each service. Implementing such a company\n\nstructure, as in Spotify or Netﬂix, can allow you to adopt and test new ideas quickly, and\n\nbuild strong ownership feelings across the teams.\n\nThe criticism\n\nThe microservice approach is subject to criticism for a number of\n\nissues:\n\nGo to Table of Contents\n\n14\n\nDockerization of IT environments, monitoring tools and DevOps tools\n\nThe architecture introduces additional complexity and new problems\n\n(Ansible, Chef, Puppet and others) can take your development team to\n\nto deal with, such as network latency, message formats, load\n\nthe the next level of effectiveness.\n\nbalancing, fault tolerance and monitoring. Ignoring one of these\n\nbelongs to the \"fallacies of distributed computing”.\n\nA\n\nB\n\nAutomation is possible but in the simplest cases, tests and deployments\n\nOperations\n\nCore Team\n\nmay be more complicated than with the monolithic approach.\n\nMoving responsibilities between services is difﬁcult. It may involve\n\nQuality assurance\n\nCross-functional team\n\nCross-functional team\n\ncommunication between different teams, rewriting the functionality in\n\nanother language or ﬁtting it into a different infrastructure. On the other\n\nhand, it’s easy to test contracts between services after such changes.\n\nDevelopment\n\nCross-functional team\n\nCross-functional team\n\nStarting with the microservices approach from the beginning can lead to\n\ntoo many services, whereas the alternative of internal modularization\n\nmay lead to a simpler design.\n\nCross-functional team\n\nCross-functional team\n\nFig. 1: A microservice approach encourages enterprises to become more agile, with\n\ncross-functional teams responsible for each service. Implementing such a company\n\nstructure, as in Spotify or Netﬂix, can allow you to adopt and test new ideas quickly, and\n\nbuild strong ownership feelings across the teams.\n\nThe criticism\n\nThe microservice approach is subject to criticism for a number of\n\nissues:\n\n15\n\nGo to Table of Contents\n\nGo to Table of Contents\n\nEvolutionary approach\n\n16\n\nEvolutionary\n\napproach\n\nEvolutionary approach\n\nMartin Fowler, one of the pioneers³ of microservices used to say:\n\nAlmost all the successful microservice stories have started with a monolith that got too big and was broken up.\n\nAlmost all the cases where I've heard of a system that was built as a microservice system from scratch, has ended up in serious trouble.\n\n³ https://martinfowler.com/articles/microservices.html\n\n17",
      "page_number": 10
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 18-25)",
      "start_page": 18,
      "end_page": 25,
      "detection_method": "topic_boundary",
      "content": "External Systems\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nESB\n\n...\n\nMagento\n\nFig. 2: Initial, monolithic architecture began after 4 years of development of a\n\nlarge-scale, 100M EUR/yr B2B platform.\n\nWhen you begin a new application, how sure are you that it will be useful\n\nto your users? Starting with microservices from day one may signiﬁcantly\n\ncomplicate the system. It can be much harder to pivot if something didn’t\n\ngo as planned (from the business standpoint). During this ﬁrst phase you\n\nneed to prioritize the speed of development to basically ﬁgure out what\n\nworks.\n\nGo to Table of Contents\n\n18\n\nExternal Systems\n\nXYZ Client\n\nExternal Systems\n\nAPI Consumers\n\nERP\n\n...\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nESB\n\n...\n\nAPI Gateway\n\nMicro Services\n\nr e k o r B e g a s s e M\n\nPRICE\n\nCRM\n\nOMS\n\nPIM\n\nNOTIFY\n\nRECOMMENDATION\n\nWMS\n\nREPORT\n\nREVIEW\n\n...\n\nMagento\n\nFig. 2: Initial, monolithic architecture began after 4 years of development of a\n\nFrontend Application\n\nMobile App\n\nlarge-scale, 100M EUR/yr B2B platform.\n\nFig. 3: The very same system but after architecture re-engineering; now the system core\n\nis built upon 10 microservices.\n\nWhen you begin a new application, how sure are you that it will be useful\n\nto your users? Starting with microservices from day one may signiﬁcantly\n\nMany successful eCommerce businesses (if not all of them!) started from\n\ncomplicate the system. It can be much harder to pivot if something didn’t\n\nmonolithic, at some point, all-in-one platforms before transitioning into a\n\ngo as planned (from the business standpoint). During this ﬁrst phase you\n\nservice oriented architecture.\n\nneed to prioritize the speed of development to basically ﬁgure out what\n\nworks.\n\nRe-engineering the architecture requires a team effort of 6-12 months (18\n\nmonths in Zalando’s case) - and therefore it should have a solid business\n\nfoundation.\n\n19\n\nGo to Table of Contents\n\nThe most common reasons we’ve seen to initialize a transformation\n\nare the following:\n\nWith four to ﬁve years of development, the scope of the system is so\n\nbroad that implementing changes in one of the modules affects other\n\nareas and despite having unit-tests, making deep changes to the\n\nsystem logic is quite risky.\n\nTechnical debt in one system area is accrued to a level at which it’s\n\nextremely hard to resolve without major changes. Performance\n\nchallenges exist in the product catalog, pricing/promo rules or central\n\nuser database areas.\n\nThere is a need to coordinate separate teams or vendors in a way\n\nwhich leads to minimal interference between them.\n\nThe system is hard to test and deploy.\n\nThere is a need to implement continuous deployments.\n\nGo to Table of Contents\n\n20\n\nThe most common reasons we’ve seen to initialize a transformation\n\nare the following:\n\nWith four to ﬁve years of development, the scope of the system is so\n\nbroad that implementing changes in one of the modules affects other\n\nareas and despite having unit-tests, making deep changes to the\n\nsystem logic is quite risky.\n\nTechnical debt in one system area is accrued to a level at which it’s\n\nextremely hard to resolve without major changes. Performance\n\nchallenges exist in the product catalog, pricing/promo rules or central\n\nuser database areas.\n\nThere is a need to coordinate separate teams or vendors in a way\n\nwhich leads to minimal interference between them.\n\nThe system is hard to test and deploy.\n\nThere is a need to implement continuous deployments.\n\nBest practices\n\n21\n\nGo to Table of Contents\n\nBest practices\n\nThis eBook is intended to show you the most popular design patterns and\n\npractices related to microservices. I strongly recommend you to track the\n\nfather of the micro services approach - Sam Newman. You should check\n\nout websites\n\nlike: http://microservices.io, https://dzone.com/ and\n\nhttps://github.com/mfornos/awesome-microservices\n\n(under the “microservices” keyword). They provide a condensed dose of\n\nknowledge about core microservice patterns, decomposition methods,\n\ndeployment patterns, communication styles, data management and\n\nmuch more…\n\nCreate a Separate Database for Each Service\n\nSharing the same data structures between services can be difﬁcult -\n\nparticularly in environments where separate teams manage each\n\nmicroservice. Conﬂicts and surprising changes are not what you’re aiming\n\nfor with a distributed approach.\n\nBreaking apart the data can make information management more\n\ncomplicated the individual storage systems can easily de-sync or become\n\ninconsistent. You need to add a tool that performs master data\n\nmanagement. While operating in the background, it must eventually ﬁnd\n\nand ﬁx inconsistencies. One of the patterns for such synchronization is\n\nEvent Sourcing. This pattern can help you with such situations by\n\nproviding you with a reliable history log of all data changes that can be\n\nrolled back and forth. Eventual Consistency and CAP theorem are\n\nfundamentals that must be considered during the design phase.\n\nGo to Table of Contents\n\n22\n\nBest practices\n\nTRADITIONAL APPLICATION\n\n3-Tier Approach\n\nSingle app process or 3-Tier approach\n\nThis eBook is intended to show you the most popular design patterns and\n\nSeveral modules\n\npractices related to microservices. I strongly recommend you to track the\n\nLayered modules\n\nfather of the micro services approach - Sam Newman. You should check\n\nSingle App Process\n\nout websites\n\nlike: http://microservices.io, https://dzone.com/ and\n\nhttps://github.com/mfornos/awesome-microservices\n\nOR\n\n(under the “microservices” keyword). They provide a condensed dose of\n\nknowledge about core microservice patterns, decomposition methods,\n\ndeployment patterns, communication styles, data management and\n\nmuch more…\n\nSINGLE MONOLITH DATABASE\n\nCreate a Separate Database for Each Service\n\nMICROSERVICES APPROACH\n\nPresentation services\n\nUI\n\nSharing the same data structures between services can be difﬁcult -\n\nparticularly in environments where separate teams manage each\n\nmicroservice. Conﬂicts and surprising changes are not what you’re aiming\n\nfor with a distributed approach.\n\nBreaking apart the data can make information management more\n\nStateful services\n\ncomplicated the individual storage systems can easily de-sync or become\n\ninconsistent. You need to add a tool that performs master data\n\nmanagement. While operating in the background, it must eventually ﬁnd\n\nStatles services with related databases\n\nand ﬁx inconsistencies. One of the patterns for such synchronization is\n\nMODEL/DATABASE PER MICROSERVICE\n\nEvent Sourcing. This pattern can help you with such situations by\n\nFig. 4: Each microservice should have a separate database and be as self-sufﬁcient as it\n\nproviding you with a reliable history log of all data changes that can be\n\ncan. From a design point of view - it’s the simplest way to avoid conﬂicts. Remember -\n\ndifferent teams are working on different parts of the application. Having a common\n\nrolled back and forth. Eventual Consistency and CAP theorem are\n\ndatabase is like having a single point of failure with all conﬂicting changes deployed\n\nfundamentals that must be considered during the design phase.\n\nsimultaneously between services.\n\n23\n\nGo to Table of Contents\n\nRely on Contracts Between Services\n\nKeep all code at a similar level of maturity and stability. When you have to\n\nmodify the behaviour of a currently deployed (and stable) microservice,\n\nit’s usually better to put the new logic into a new, separate service. It’s\n\nsometimes called “immutable architecture”.\n\nAnother point here is that you should maintain similar, speciﬁc\n\nrequirements for all microservices like data formats, enumerating return\n\nvalues and describing error handling.\n\nMicroservices should comply with SRP (Single Responsibility Principle)\n\nand LSP (Liskov Substitution Principle).\n\nDeploy in Containers\n\nDeploying microservices in containers is important because it means you\n\nneed just one tool to deploy everything. As long as the microservice is in\n\na container, the tool knows how to deploy it. It doesn’t matter what the\n\ncontainer is. That said, Docker seems to have become the de facto\n\nstandard for containers very quickly.\n\nGo to Table of Contents\n\n24\n\nRely on Contracts Between Services\n\nCOMPOSE\n\n.yml Description\n\nDocker CLI\n\nKeep all code at a similar level of maturity and stability. When you have to\n\nmodify the behaviour of a currently deployed (and stable) microservice,\n\nContainer\n\nContainer\n\nContainer\n\nit’s usually better to put the new logic into a new, separate service. It’s\n\nsometimes called “immutable architecture”.\n\nSWARM\n\nAnother point here is that you should maintain similar, speciﬁc\n\nNode\n\nNode\n\nNode\n\nNode\n\nNode\n\nNode\n\nrequirements for all microservices like data formats, enumerating return\n\nSwarm\n\nSwarm\n\nvalues and describing error handling.\n\nMicroservices should comply with SRP (Single Responsibility Principle)\n\nand LSP (Liskov Substitution Principle).\n\nCLUSTER MANAGERS\n\nCluster Manager 1\n\nCluster Manager 2\n\nFig. 5: Source - Docker Blog. Docker Swarm manages the whole server cluster -\n\nautomatically deploying new machines with additional instances for scalability and high\n\nDeploy in Containers\n\navailability. Of course it can be deployed on popular cloud environments like Amazon.\n\nDeploying microservices in containers is important because it means you\n\nTreat Servers as Volatile\n\nneed just one tool to deploy everything. As long as the microservice is in\n\na container, the tool knows how to deploy it. It doesn’t matter what the\n\ncontainer is. That said, Docker seems to have become the de facto\n\nTreat servers, particularly those that run customer-facing code, as\n\nstandard for containers very quickly.\n\ninterchangeable members of a group. It’s the only way to successfully use\n\nthe cloud’s “auto scaling” feature.\n\nThey all perform the same function, so you don’t need to be concerned\n\nwith them individually. The role conﬁguration across servers must be\n\naligned and the deployment process should be fully automated.\n\n25\n\nGo to Table of Contents",
      "page_number": 18
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 26-33)",
      "start_page": 26,
      "end_page": 33,
      "detection_method": "topic_boundary",
      "content": "A monolithic application puts all its functionality into a single process...\n\nA microservices architecture puts each element of functionality into a separate service ...\n\n... and scales by replicating the monolith on multiple servers\n\n... and scales by distributing these services across servers, replicating as needed\n\nFig. 6: Original idea - Martin Fowler\n\n(https://martinfowler.com/articles/microservices.html). Scaling microservices can be\n\nefﬁcient because you can add resources directly where needed. You don’t have to deal\n\nwith storage replication, sticky sessions and all that kind of stuff because services are\n\nstateless and loosely-coupled by design.\n\nGo to Table of Contents\n\n26\n\nA microservices\n\narchitecture puts each\n\nA monolithic application\n\nelement of functionality\n\nputs all its functionality\n\ninto a separate service ...\n\ninto a single process...\n\n... and scales by distributing\n\n... and scales by\n\nthese services across servers,\n\nreplicating the monolith\n\nreplicating as needed\n\non multiple servers\n\nFig. 6: Original idea - Martin Fowler\n\n(https://martinfowler.com/articles/microservices.html). Scaling microservices can be\n\nefﬁcient because you can add resources directly where needed. You don’t have to deal\n\nwith storage replication, sticky sessions and all that kind of stuff because services are\n\nstateless and loosely-coupled by design.\n\nRelated techniques and patterns\n\n27\n\nGo to Table of Contents\n\nRelated Techniques and Patterns\n\nThis eBook is intended to give you a quick-start, practical overview of the\n\nmicroservices approach. I believe, once interested in the topic, you can\n\nﬁnd additional sources to dig into. In this chapter I would like to mention\n\njust a few programming techniques and design patterns which have\n\nbecome popular with microservices gaining the spotlight. We want to\n\ncover the full scope of building microservices and tools that can be\n\nparticularly useful to that goal.\n\nCAP theorem\n\nAlso called “Brewer theorem” after Eric Brewer, states that, for distributed\n\nsystems it’s not possible to provide more than two of the following three\n\nguarantees:\n\nConsistency - every read receives the most recent data or error.\n\nAvailability - every request receives a (non-error) response BUT without\n\na guarantee of most-recent data.\n\nPartition tolerance - interpreted as a system able to work despite the\n\nnumber of dropped messages between cluster nodes.\n\nIn other words - when it comes to communication issues (partition of the\n\ncluster), you must choose between consistency or availability. This is\n\nstrongly connected with techniques of high availability like caching and\n\ndata redundancy (eg. database replication).\n\nGo to Table of Contents\n\n28\n\nWhen the system is running normally - both availability and consistency\n\nRelated Techniques and Patterns\n\ncan be provided. In case of failure, you get two choices:\n\nThis eBook is intended to give you a quick-start, practical overview of the\n\nRaise an error (and break the availability promise) because it’s not\n\nmicroservices approach. I believe, once interested in the topic, you can\n\nguaranteed that all data replicas are updated.\n\nﬁnd additional sources to dig into. In this chapter I would like to mention\n\njust a few programming techniques and design patterns which have\n\nProvide the user with cached data (due to the very same reason as\n\nbecome popular with microservices gaining the spotlight. We want to\n\nabove).\n\ncover the full scope of building microservices and tools that can be\n\nparticularly useful to that goal.\n\nTraditional database systems (compliant with ACID6 ) prefer consistency\n\nover availability.\n\nCAP theorem\n\nEventual consistency\n\nAlso called “Brewer theorem” after Eric Brewer, states that, for distributed\n\nsystems it’s not possible to provide more than two of the following three\n\nWhen the system is running normally - both availability and consistency\n\nguarantees:\n\ncan be provided. In case of failure, you get two choices:\n\nConsistency - every read receives the most recent data or error.\n\nIt’s not a programming technique but rather something you have to think\n\nAvailability - every request receives a (non-error) response BUT without\n\nabout when designing distributed systems. This consistency model is\n\na guarantee of most-recent data.\n\nconnected directly to the CAP theorem and informally guarantees that if\n\nPartition tolerance - interpreted as a system able to work despite the\n\nno new updates are made to a given data item, eventually all access\n\nnumber of dropped messages between cluster nodes.\n\nto that item will return the last updated value.\n\nIn other words - when it comes to communication issues (partition of the\n\nEventually consistent services are often classiﬁed as providing BASE\n\ncluster), you must choose between consistency or availability. This is\n\n(Basically Available, Soft state, Eventual consistency) semantics, in\n\nstrongly connected with techniques of high availability like caching and\n\ncontrast to traditional ACID guarantees.\n\ndata redundancy (eg. database replication).\n\n6 https://en.wikipedia.org/wiki/ACID\n\n29\n\nGo to Table of Contents\n\nTo achieve eventual consistency, the distributed system must resolve data\n\nconﬂicts between multiple copies of replicated data. This usually consists\n\nof two parts:\n\nExchanging updates between servers in a cluster\n\nChoosing the ﬁnal state.\n\nThe widespread model for choosing the ﬁnal state is “last writer wins” -\n\nachieved by including an update timestamp along with an updated copy\n\nof data.\n\nDesign patterns\n\nHaving knowledge of the core theories that underpin the issues which we\n\nmay encounter when developing and designing a distributed\n\narchitecture, we can now go into higher-level concepts and patterns.\n\nDesign patterns are techniques that allow us to compose code of our\n\nmicroservices in a more structured way and facilitate further maintenance\n\nand development of our platform.\n\nCQRS\n\nCQRS means Command-Query Responsibility Segregation. The core idea\n\nbehind CQRS is the extension of the CQS concept by Bertrand Meyer,\n\nwhere objects have two types of methods. Command methods perform\n\nactions in systems and always return nothing, query methods return\n\nvalues and they have no effect on the system.\n\nGo to Table of Contents\n\n30\n\nTo achieve eventual consistency, the distributed system must resolve data\n\nIn CQRS, write requests (aka commands) and read requests (aka queries)\n\nconﬂicts between multiple copies of replicated data. This usually consists\n\nare separated into different models. The write model will accept\n\nof two parts:\n\ncommands and perform actions on the data, the read model will accept\n\nqueries and return data to the application UI. The read model should be\n\nupdated if, and only if, the write model was changed. Moreover, single\n\nExchanging updates between servers in a cluster\n\nchanges in the write model may cause updates in more than one read\n\nChoosing the ﬁnal state.\n\nmodel. What is very interesting is that there is a possibility to split data\n\nstorage layers, set up a dedicated data store for writes and reads, and\n\nThe widespread model for choosing the ﬁnal state is “last writer wins” -\n\nmodify and scale them independently.\n\nachieved by including an update timestamp along with an updated copy\n\nof data.\n\nFor example, all write requests in the eCommerce application, like adding\n\na new order or product reviews, can be stored in a typical SQL database\n\nDesign patterns\n\nbut some read requests, like ﬁnding similar products, can be delegated\n\nby the read model to a graph engine.\n\nHaving knowledge of the core theories that underpin the issues which we\n\nGeneral ﬂow in CQRS application:\n\nmay encounter when developing and designing a distributed\n\narchitecture, we can now go into higher-level concepts and patterns.\n\nApplication creates a command as a result of user action.\n\nDesign patterns are techniques that allow us to compose code of our\n\nCommand is processed, write model saves changes in data store.\n\nmicroservices in a more structured way and facilitate further maintenance\n\nRead model is updated based on changes in write model.\n\nand development of our platform.\n\nPros:\n\nCQRS\n\nBetter scalability and performance.\n\nSimple queries and commands.\n\nCQRS means Command-Query Responsibility Segregation. The core idea\n\nPossibility to use different data storage and theirs functionalities.\n\nbehind CQRS is the extension of the CQS concept by Bertrand Meyer,\n\nWorks well in complex domains.\n\nwhere objects have two types of methods. Command methods perform\n\nactions in systems and always return nothing, query methods return\n\nvalues and they have no effect on the system.\n\n31\n\nGo to Table of Contents\n\nCons:\n\nIncreased complexity of the entire system.\n\nEventually consistent, read model may be out of sync with write model\n\nfor a while.\n\nPossible data and code duplication.\n\nService Interface\n\nQuery Model\n\nquery model reads from database\n\ncommand model updates database\n\nCommand Model\n\napplication routes change information to command model\n\ncommand model executes validations, and consequential logic\n\nquery services update presentations from query model\n\nuser makes a change in the UI\n\nGo to Table of Contents\n\n32\n\nCons:\n\nIncreased complexity of the entire system.\n\nUI\n\nEventually consistent, read model may be out of sync with write model\n\nfor a while.\n\nFig. 9: CQRS architecture (https://martinfowler.com/bliki/images/cqrs/cqrs.png).\n\nPossible data and code duplication.\n\nService Interface\n\nEvent Sourcing\n\nQuery Model\n\nData stores are often designed to directly keep the actual state of the\n\nsystem without storing the history of all the submitted changes. In some\n\nquery\n\nsituations this can cause problems. For example, if there is a need to\n\nmodel\n\nreads from\n\nprepare a new read model for some speciﬁc point of time (like your\n\ndatabase\n\ncurrent address on an invoice from 3 months ago - which may have\n\nchanged in the meantime - and you haven’t stored the time-stamped data\n\napplication\n\nsnapshots, it will be a big deal to reprint or modify the correct document).\n\nroutes\n\nchange\n\nCommand Model\n\ninformation\n\ncommand\n\nto command\n\nEvent Sourcing stores all changes as a time-ordered sequence of events;\n\nmodel\n\nmodel\n\nupdates\n\neach event is an object that represents a domain action from the past. All\n\ndatabase\n\nevents published by the application object persist inside a dedicated,\n\ncommand model\n\nexecutes validations,\n\nappend-only data store called Event Store. This is not just an audit-log for\n\nand consequential\n\nlogic\n\nthe whole system because the main role of Event Store is to reconstruct\n\napplication objects based on the history of the related events.\n\nquery services update\n\nuser makes a change\n\npresentations from query\n\nin the UI\n\nmodel\n\n33\n\nGo to Table of Contents",
      "page_number": 26
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 34-51)",
      "start_page": 34,
      "end_page": 51,
      "detection_method": "topic_boundary",
      "content": "PRESENTATION\n\nSome options for consuming events\n\nCart\n\nItem 1 added\n\nCart ID\n\nCart Item\n\nDate\n\nCart ID\n\nItem 2 added\n\nItem 1 removed\n\nCustomer\n\nAddress\n\n...\n\nItem key\n\nItem name\n\nQuantity\n\nEXTERNAL SYSTEMS AND APPLICATIONS\n\n...\n\nShipping information added\n\nMATERLIALIZED VIEW\n\nPublished events\n\nPersisted events\n\nReplayed events\n\nQUERY FOR CURRENT STATE OF ENTITIES\n\nEvent store\n\nFig. 10: Event Sourcing overview\n\n(https://docs.microsoft.com/en-us/azure/architecture/patterns/_images/event-sourcing-o\n\nverview.png).\n\nConsider the following sequence of domain events, regarding each\n\nOrder lifecycle:\n\nOrderCreated\n\nOrderApproved\n\nOrderPaid\n\nOrderPrepared\n\nOrderShipped\n\nOrderDelivered\n\nGo to Table of Contents\n\n34\n\nDuring the recreation phase, all events are fetched from the EventStore\n\nand applied to a newly constructed entity. Each applied event changes\n\nPRESENTATION\n\nthe internal state of the entity.\n\nSome options for\n\nconsuming events\n\nCart\n\nCart Item\n\nCart ID\n\nItem 1 added\n\nCart ID\n\nThe beneﬁts of this approach are obvious. Each event represents an\n\nDate\n\nItem key\n\nItem 2 added\n\nCustomer\n\nEXTERNAL\n\naction, which is even better if DDD is used in the project. There is a trace\n\nItem name\n\nSYSTEMS AND\n\nAddress\n\nAPPLICATIONS\n\nItem 1 removed\n\nQuantity\n\nof every single change in domain entities.\n\n...\n\n...\n\nShipping information added\n\nMATERLIALIZED VIEW\n\nBut there are also some potential drawbacks here… How can we get the\n\nPublished events\n\ncurrent states of tens of objects? How fast will object recreation be if the\n\nPersisted\n\nReplayed events\n\nQUERY FOR CURRENT\n\nevents\n\nevents list contains thousands of items?\n\nSTATE OF ENTITIES\n\nEvent store\n\nFig. 10: Event Sourcing overview\n\nFortunately, the Event Sourcing technique has prepared solutions to\n\n(https://docs.microsoft.com/en-us/azure/architecture/patterns/_images/event-sourcing-o\n\nthese problems. Based on the events, the application can update one or\n\nverview.png).\n\nmore from materialized views, so there is no need to fetch all objects from\n\nthe event history to get their current states.\n\nConsider the following sequence of domain events, regarding each\n\nOrder lifecycle:\n\nIf the event history of the entity is long, the application may also create\n\nOrderCreated\n\nsome snapshots. By “snapshot”, I mean the state of the entity after every\n\nOrderApproved\n\nn-th event. The recreation phase will be much faster because there is no\n\nOrderPaid\n\nneed to fetch all the changes from the Event Store, just the latest\n\nOrderPrepared\n\nsnapshot and further events.\n\nOrderShipped\n\nOrderDelivered\n\n35\n\nGo to Table of Contents\n\nUser Interface\n\nCommand Bus\n\nCommand Handler\n\nDomain Model\n\nDomain Model\n\nDomain Model\n\nCommand Handler\n\ns u B t n e v E\n\nEvent store\n\nFig. 11: Event Sourcing with CQRS\n\n(https://pablocastilla.ﬁles.wordpress.com/2014/09/cqrs.png?w=640).\n\nGo to Table of Contents\n\nQuery Facade\n\nThin Data Layer\n\nData\n\nEventHandler\n\n36\n\nEvent Sourcing works very well with CQRS and Event Storming, a\n\nUser Interface\n\ntechnique for domain event identiﬁcation by Alberto Brandolini. Events\n\nfound with domain experts will be published by entities inside the write\n\nmodel. They will be transferred to a synchronous or asynchronous event\n\nbus and processed by event handlers. In this scenario, event handlers will\n\nCommand Bus\n\nQuery Facade\n\nbe responsible for updating one or more read models.\n\nPros:\n\nCommand Handler\n\nThin Data Layer\n\nPerfect for modeling complex domains.\n\nPossibility to replay all stored events and build new read models.\n\nReliable audit-log for free.\n\nDomain\n\nDomain\n\nModel\n\nModel\n\nCons:\n\nData\n\nDomain\n\nModel\n\nQueries implemented with CQRS.\n\nEventually consistent model.\n\ns\n\nu\n\nB\n\nCommand Handler\n\nEventHandler\n\nt\n\nn\n\nEvent driven data management\n\ne\n\nv\n\nE\n\nMicroservices should be coupled as loosely as possible, It should be\n\npossible to develop, test, deploy and scale them independently.\n\nEvent\n\nSometimes an application should even be able to work without particular\n\nstore\n\nservices (to comply with HA - high availability)… To achieve these\n\nrequirements, each microservice should have a separate data store.\n\nFig. 11: Event Sourcing with CQRS\n\nSounds easy - but what about the data itself? How to spread the\n\n(https://pablocastilla.ﬁles.wordpress.com/2014/09/cqrs.png?w=640).\n\ninformation changes between services? What about consistency within\n\nthe data?\n\n37\n\nGo to Table of Contents\n\nOne of the best solutions is simply using events. If anything important\n\nhappened inside a microservice, a speciﬁc event is published to the\n\nmessage broker. Other microservices may connect to the message broker,\n\nreceive, and consume a dedicated copy of that message. Consumers may\n\nalso decide which part of the data should be duplicated to their local\n\nstore.\n\nSafe publishing of events from the microservice is quite complicated.\n\nEvents must be published to the message broker if, and only if, data\n\nstored in a data store has changed. Other scenarios may lead to huge\n\nconsistency problems. Usually it means that data and events should\n\npersist inside the same transaction to a single data store and then\n\npropagate to the rest of the system.\n\nSwitching from theory to a practical point of view, it’s quite a common\n\ncase to use RabbitMQ as a message broker. RabbitMQ is a very fast and\n\nefﬁcient queue server written in Erlang with wide set of client libraries for\n\nthe most popular programming languages. A popular alternative to\n\nRabbitMQ is Apache Kafka, especially for bigger setups or when event\n\nstream mining and analytics is critical.\n\nSpreading data across multiple separated data stores and achieving\n\nconsistency using events can cause some problems. For example, there is\n\nno easy way to execute a distributed transaction on different databases.\n\nMoreover, there can also be consistency issues because when events are\n\ninside the message broker, somewhere between microservices, the state\n\nof the whole system is inconsistent. The data store behind the original\n\nmicroservice is updated but changes aren’t applied on data stores behind\n\nother microservices. This model, called Eventually Consistent,\n\nGo to Table of Contents\n\n38\n\nOne of the best solutions is simply using events. If anything important\n\nis a Data will be synchronized in the future but you can also stop some\n\nhappened inside a microservice, a speciﬁc event is published to the\n\nservices and you will never lose your data. They will be processed when\n\nmessage broker. Other microservices may connect to the message broker,\n\nservices are restored.\n\nreceive, and consume a dedicated copy of that message. Consumers may\n\nalso decide which part of the data should be duplicated to their local\n\nIn some situations, when a new microservice is introduced, there is a need\n\nstore.\n\nto seed the database. If there is a chance to use data directly from\n\ndifferent „sources of truth”, it’s probably the best way to setup a new\n\nSafe publishing of events from the microservice is quite complicated.\n\nservice. But other microservices may also expose feeds of theirs events,\n\nEvents must be published to the message broker if, and only if, data\n\nfor example in the form of ATOM feeds. New microservices may process\n\nstored in a data store has changed. Other scenarios may lead to huge\n\nthem in chronological order, to compile the ﬁnal state of new data stores.\n\nconsistency problems. Usually it means that data and events should\n\nOf course, in this scenario each microservice should keep a history of all\n\npersist inside the same transaction to a single data store and then\n\nevents, which can sometimes be a subsequent challenge.\n\npropagate to the rest of the system.\n\nIntegration techniques\n\nSwitching from theory to a practical point of view, it’s quite a common\n\ncase to use RabbitMQ as a message broker. RabbitMQ is a very fast and\n\nefﬁcient queue server written in Erlang with wide set of client libraries for\n\nSystem\n\nintegration\n\nis key\n\nto developing efﬁcient microservices\n\nthe most popular programming languages. A popular alternative to\n\narchitecture. Services must talk to each other in a consistent way. The\n\nRabbitMQ is Apache Kafka, especially for bigger setups or when event\n\noverall structure of a platform could be easily discoverable by hiding all of\n\nstream mining and analytics is critical.\n\nthe dependencies behind facades like a common API gateway.\n\nSpreading data across multiple separated data stores and achieving\n\nMoreover, all of\n\nthat communication should use authentication\n\nconsistency using events can cause some problems. For example, there is\n\nmechanisms as microservices are commonly exposed to the outside\n\nno easy way to execute a distributed transaction on different databases.\n\nworld. They should not be designed with the intention of residing only in\n\nMoreover, there can also be consistency issues because when events are\n\nour ﬁrewall-protected network. We show two possible ways of making our\n\ninside the message broker, somewhere between microservices, the state\n\nintegration secure by using token based techniques such as OAuth2 and\n\nof the whole system is inconsistent. The data store behind the original\n\nJWT.\n\nmicroservice is updated but changes aren’t applied on data stores behind\n\nother microservices. This model, called Eventually Consistent,\n\n39\n\nGo to Table of Contents\n\nAPI Gateways\n\nWith the microservices approach, it’s quite easy to make internal network\n\ncommunication very talkative. Nowadays, when 10G network connections\n\nare standard in data-centers, there may be nothing wrong with that. But\n\nwhen it comes to communication between your mobile app and backend\n\nservices, you might want to compress as much information as possible\n\ninto one request.\n\nThe second reason to criticise microservices might be a challenge with\n\nadditional sub-service calls like authorization, ﬁltering etc.\n\nTo overcome the mentioned obstacles, we can use the API Gateway\n\napproach. It means you can compile several microservices using one\n\nfacade. It combines multiple responses from internal sub-services into a\n\nsingle response.\n\nWith almost no business logic included, gateways are an easy and safe\n\nchoice to optimize communication between frontend and backend or\n\nbetween different backend systems.\n\nGo to Table of Contents\n\n40\n\nAPI Gateways\n\nView\n\nController\n\nSingle entry poiont\n\nWith the microservices approach, it’s quite easy to make internal network\n\nProduct Info Service\n\nREST\n\nModel\n\ncommunication very talkative. Nowadays, when 10G network connections\n\nare standard in data-centers, there may be nothing wrong with that. But\n\nTraditional server-side web application\n\nAPI Gateway\n\nRecommendation Service\n\nwhen it comes to communication between your mobile app and backend\n\nREST\n\nservices, you might want to compress as much information as possible\n\ninto one request.\n\nView\n\nController\n\nClient specific APIs\n\nReview Service\n\nAMQP\n\nProtocol translation\n\nModel\n\nThe second reason to criticise microservices might be a challenge with\n\nBrowser/Native App\n\nadditional sub-service calls like authorization, ﬁltering etc.\n\nFig. 12: Using an API gateway you can compose your sub-service calls into easy to\n\nunderstand and easy to use facades. Trafﬁc optimization, caching and authorization are\n\nadditional beneﬁts of such an approach\n\nTo overcome the mentioned obstacles, we can use the API Gateway\n\napproach. It means you can compile several microservices using one\n\nThe API Gateway - which is an implementation of classic Proxy patterns -\n\nfacade. It combines multiple responses from internal sub-services into a\n\ncan provide a caching mechanism as well (even using a vanilla-Varnish\n\nsingle response.\n\ncache layer without additional development effort). With this feature\n\nalone, using cloud approaches (like Amazon solutions), can scale API and\n\nservices very easily.\n\nWith almost no business logic included, gateways are an easy and safe\n\nchoice to optimize communication between frontend and backend or\n\nAdditionally, you can provide common authorization layers for all services\n\nbetween different backend systems.\n\nbehind the gateway. For example - that’s how Amazon API Gateway\n\nService7 + Amazon Cogito8 work.\n\n7 https://aws.amazon.com/api-gateway/\n\n8 http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-ﬂow.html\n\n41\n\nMobile Apps\n\nAWS Lambda functions\n\nReceive incoming request\n\nCheck for Item in dedicated cache\n\nCheck throttling configuration\n\nCheck current RPS rate\n\nExecute backend call\n\nEndpoints on Amazon EC2\n\nWebsites\n\nInternet\n\nIf found return cached Item\n\nIf above allowed ratio return 429\n\nAny other publicly accessible endpoint\n\nServices\n\nAmazon Cloud Watch\n\nFig. 13: Amazon API Gateway request workﬂow\n\nhttps://aws.amazon.com/api-gateway/details/). Amazon gateway supports caching and\n\nauthorization features in spite of your web-service internals.\n\nSwagger9 can help you, once a Gateway has been built, with direct\n\nintegration and support to Amazon services.\n\nBackend for Frontends\n\nA typical example of an API Gateway is the backend for frontends (BFF)\n\npattern. It is about facades and compiling several microservices into\n\noptimized / device or channel-oriented API services. Its microservice\n\ndesign pattern was proposed by Sam Newman of Thought Works (author\n\nof “Building Microservices”): to create single purpose edge APIs for\n\nfrontends and other parties.\n\nCreating such a facade-API brings at least two beneﬁts to your\n\napplication:\n\nIf you manage to have a few micro services behind your facade, you can\n\navoid network latency - which is especially important on mobile devices.\n\n9 http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-ﬂow.html\n\n42\n\nUsing a facade, you can hide all network trafﬁc between services\n\nexecuting the sub-calls in internal networks from the end-client.\n\nMobile Apps\n\nAWS Lambda\n\nfunctions\n\nReceive incoming\n\nCheck throttling\n\nrequest\n\nconfiguration\n\nExecute\n\nThen you can optimize your calls to be more compliant with a speciﬁc\n\nEndpoints on\n\nCheck for Item in\n\nCheck current RPS\n\nbackend\n\nAmazon EC2\n\ndedicated cache\n\nrate\n\ncall\n\nIf above allowed ratio\n\nIf found return cached\n\nWebsites\n\nInternet\n\ndomain model. You can model the API structures by merging and\n\nreturn 429\n\nItem\n\ndistributing subsequent service calls instead of pushing this logic to the\n\nAny other publicly\n\naccessible endpoint\n\nAPI client’s code.\n\nServices\n\nAmazon Cloud Watch\n\nThe diagram below shows a migration from General Purpose API to a\n\nFig. 13: Amazon API Gateway request workﬂow\n\nhttps://aws.amazon.com/api-gateway/details/). Amazon gateway supports caching and\n\ndedicated backends for frontends approach which integrates the\n\nauthorization features in spite of your web-service internals.\n\nsub-services into logic.\n\nSwagger9 can help you, once a Gateway has been built, with direct\n\nTeam A\n\nTeam B\n\nintegration and support to Amazon services.\n\nMobile App\n\niOS App\n\nAndroid App\n\nBackend for Frontends\n\nMobile Team\n\nWeb Team\n\nA typical example of an API Gateway is the backend for frontends (BFF)\n\npattern. It is about facades and compiling several microservices into\n\niOS BFF\n\nAndroid BFF\n\noptimized / device or channel-oriented API services. Its microservice\n\nGeneral Purpose Server-side API\n\ndesign pattern was proposed by Sam Newman of Thought Works (author\n\nAPI Team\n\nof “Building Microservices”): to create single purpose edge APIs for\n\nInventory\n\nWishlist\n\nCatalog\n\nfrontends and other parties.\n\nTeam C\n\nTeam D\n\nCreating such a facade-API brings at least two beneﬁts to your\n\napplication:\n\nTeam A\n\nTeam B\n\nFig. 14: Backend for frontends architecture is about minimizing the number of backend\n\nIf you manage to have a few micro services behind your facade, you can\n\ncalls and optimizing the interfaces to a supported device.\n\navoid network latency - which is especially important on mobile devices.\n\n9 http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-ﬂow.html\n\n43\n\nGo to Table of Contents\n\nThere are many approaches to separate backend for frontends and\n\nroughly speaking it always depends on the differences in data required by\n\na speciﬁc frontend, or usage-patterns behind speciﬁc API clients. One can\n\nimagine a separate API for frontend, mobile apps - as well as separate\n\ninterfaces for iOS and Android if there are any differences between these\n\napplications regarding how service calls are made or their respective data\n\nformats.\n\nOne of the concerns of having a single BFF per user interface is that you\n\ncan end up with lots of code duplication between the BFFs themselves.\n\nPete Hodgson (ex. Thought Works) suggests that BFFs work best when\n\norganized around teams. The team structure should drive how many BFFs\n\nyou have. This is a pragmatic approach to not over-engineer your system\n\nbut rather have one mobile API if you have one mobile team etc.\n\nIt’s then a common pattern to separate shared algorithms, models and\n\ncode to separate the shared service or library used by frontend-related\n\nfacades. Creating such duplications can be avoided.\n\nLet me quote a conclusion on BFF presented by Sam Newman himself:\n\nBackends For Frontends solve a pressing concern\n\nfor mobile\n\ndevelopment when using microservices. In addition, they provide a\n\ncompelling alternative to the general-purpose API backend, and many\n\nteams make use of them for purposes other than\n\njust mobile\n\ndevelopment. The simple act of limiting the number of consumers they\n\nsupport makes them much easier to work with and change, and helps\n\nteams developing customer-facing applications retain more autonomy10.\n\n10 http://samnewman.io/patterns/architectural/bff/\n\nGo to Table of Contents\n\n44\n\nToken based authorization (oauth2, JWT)\n\nThere are many approaches to separate backend for frontends and\n\nroughly speaking it always depends on the differences in data required by\n\na speciﬁc frontend, or usage-patterns behind speciﬁc API clients. One can\n\nAuthorization is a key feature of any enterprise grade application. If you\n\nimagine a separate API for frontend, mobile apps - as well as separate\n\nremember the beginnings of web 2.0 and Web API’s back then, a typical\n\ninterfaces for iOS and Android if there are any differences between these\n\nauthorization scenario was based on an API key or HTTP authorization.\n\napplications regarding how service calls are made or their respective data\n\nWith ease of use came some strings attached. Basically these “static”\n\nformats.\n\n(API key) and not strongly encrypted (basic auth.) methods were not\n\nsecure enough.\n\nOne of the concerns of having a single BFF per user interface is that you\n\ncan end up with lots of code duplication between the BFFs themselves.\n\nHere, delegated authorization methods come into action. By delegated,\n\nwe mean that authorization can be given by an external system / identity\n\nPete Hodgson (ex. Thought Works) suggests that BFFs work best when\n\nprovider. One of the ﬁrst methods of providing such authentication was\n\norganized around teams. The team structure should drive how many BFFs\n\nthe OpenID standard11 developed around 2005. It could provide a One\n\nyou have. This is a pragmatic approach to not over-engineer your system\n\nLogin and Single Sign On for any user. Unfortunately, it wasn’t widely\n\nbut rather have one mobile API if you have one mobile team etc.\n\naccepted by identiﬁcation providers like Google, Facebook or e-mail\n\nproviders.\n\nIt’s then a common pattern to separate shared algorithms, models and\n\ncode to separate the shared service or library used by frontend-related\n\nThe OAuth standard works pretty similarly to OpenID. The authorization\n\nfacades. Creating such duplications can be avoided.\n\nprovider allows Application Developers to register their own applications\n\nwith the required data-scope to be obtained in the name of the user. The\n\nLet me quote a conclusion on BFF presented by Sam Newman himself:\n\nuser authorizes speciﬁc applications to use with their account.\n\nBackends For Frontends solve a pressing concern\n\nfor mobile\n\nFacebook or Google Account login screens are a well known part of oauth\n\ndevelopment when using microservices. In addition, they provide a\n\nauthorization.\n\ncompelling alternative to the general-purpose API backend, and many\n\nteams make use of them for purposes other than\n\njust mobile\n\ndevelopment. The simple act of limiting the number of consumers they\n\nsupport makes them much easier to work with and change, and helps\n\nteams developing customer-facing applications retain more autonomy10.\n\n10 http://samnewman.io/patterns/architectural/bff/\n\n11 http://openid.net/\n\n45\n\nGo to Table of Contents\n\nFig. 15: Authorization screen for Google Accounts to authorize external application to\n\nuse Google APIs in the name of the user.\n\nAfter accepting the application request the authority party returns a\n\ntemporary Access Token which should be used with API calls to verify the\n\nuser identity. The Internal Authorization server checks tokens with its own\n\ndatabase of issued tokens - paired with user identities, ACLs, etc.\n\nAuthorization tokens are issued for a speciﬁc amount of time and should\n\nbe invalidated afterwards. Token authorization is 100% stateless; you\n\ndon’t have to use sessions (like with good, old session based\n\nauthorization)12. OAuth 2.0 requires SSL communication and avoids\n\nadditional request-response signatures required by the previous version\n\n(requests were signed using HMAC algorithms); also, the workﬂow was\n\nsimpliﬁed with 2.0 removing one additional HTTP request.\n\n12 http://stackoverﬂow.com/questions/7561631/oauth-2-0-beneﬁts-and-use-cases-why\n\nGo to Table of Contents\n\n46\n\nBROWSER\n\nAPPLICATION\n\nAUTHORIZATION SERVER\n\nRESOURCE SERVER\n\n1: Request application page\n\n1.1: Redirect to Authorization Server\n\n2: Request Login\n\n2.1: Deliver Login page\n\n3: Enter Login details and authorization access\n\n4: Send Login details\n\n5: Validate Login details\n\n6: Redirect to Application\n\n7: User Valid and Authorization access\n\n7.1: Get Access Token\n\nFig. 15: Authorization screen for Google Accounts to authorize external application to\n\n7.2: Return Access Token\n\nuse Google APIs in the name of the user.\n\n7.3: Get data\n\n7.3.2: Return data\n\n7.3.1: Check Access token\n\nAfter accepting the application request the authority party returns a\n\n7.4: Generate Page\n\n7.5\n\ntemporary Access Token which should be used with API calls to verify the\n\nuser identity. The Internal Authorization server checks tokens with its own\n\ndatabase of issued tokens - paired with user identities, ACLs, etc.\n\nFig. 16: Authorization ﬂow for oauth2.\n\nAuthorization tokens are issued for a speciﬁc amount of time and should\n\nOAuth tokens don’t push you to display the authentication dialog each\n\nbe invalidated afterwards. Token authorization is 100% stateless; you\n\ntime a user requires access to their data. Following this path would make\n\ndon’t have to use sessions (like with good, old session based\n\nit impossible to check e-mail in the background or do any batch\n\nauthorization)12. OAuth 2.0 requires SSL communication and avoids\n\nprocessing operations. So how to deal with such background-operations?\n\nadditional request-response signatures required by the previous version\n\nYou should use “ofﬂine” tokens13 - which are given for longer time periods\n\n(requests were signed using HMAC algorithms); also, the workﬂow was\n\nand can also be used to remember client credentials without requiring\n\nsimpliﬁed with 2.0 removing one additional HTTP request.\n\nlogin/password each time the user hits your application.\n\nThere is usually no need to rewrite your own OAuth code as many open\n\nsource libraries are available for most OAuth providers and frameworks.\n\nJust take a look on Github!\n\n13 https://auth0.com/docs/tokens/refresh-token\n\n12 http://stackoverﬂow.com/questions/7561631/oauth-2-0-beneﬁts-and-use-cases-why\n\n47\n\nGo to Table of Contents\n\nThere are SaaS solutions for identity and authorization, such as Amazon\n\nCogito14 or Auth015 that can be easily used to outsource the authorization\n\nof your API’s.\n\nJSON Web Tokens (JWT)\n\nYet another approach to token based authorization is JWT16 (JSON Web\n\nTokens). They can be used for stateless claim exchange between parties.\n\nAs OAuth tokens require validation by the authenticating party between\n\nall requests - JSON Web Tokens are designed to self-contain all\n\ninformation required and can be used without touching the database or\n\nany other data source.\n\nJWT are self-contained which means that tokens contain all the\n\ninformation. They are encoded and signed up using HMAC.\n\nThis allows you to fully rely on data APIs that are stateless and even make\n\nrequests to downstream services. It doesn't matter which domains are\n\nserving your APIs, so Cross-Origin Resource Sharing (CORS) won't be an\n\nissue as it doesn't use cookies17.\n\n14 https://aws.amazon.com/cognito/\n\n15 https://auth0.com/how-it-works\n\n16 https://jwt.io/\n\n17 https://jwt.io/introduction/\n\n48\n\nValidation of HMAC tokens18 requires the knowledge of the secret key\n\nThere are SaaS solutions for identity and authorization, such as Amazon\n\nused to generate the token. Typically the receiving service (your API) will\n\nCogito14 or Auth015 that can be easily used to outsource the authorization\n\nneed to contact the authentication server as that server is where the\n\nof your API’s.\n\nsecret is being kept19.\n\nPlease take a look at the example.\n\nJSON Web Tokens (JWT)\n\nYet another approach to token based authorization is JWT16 (JSON Web\n\nExample token:\n\nTokens). They can be used for stateless claim exchange between parties.\n\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxM- jM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOn RydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh 7HgQ\n\nAs OAuth tokens require validation by the authenticating party between\n\nall requests - JSON Web Tokens are designed to self-contain all\n\ninformation required and can be used without touching the database or\n\nany other data source.\n\nContains following informations: Please take a look at the example.\n\n{\n\nHeader\n\nJWT are self-contained which means that tokens contain all the\n\n\"alg\": \"HS256\",\n\n(algorithm and token type)\n\ninformation. They are encoded and signed up using HMAC.\n\n\"typ\": \"JWT\"\n\n}\n\n{\n\nPayload\n\nThis allows you to fully rely on data APIs that are stateless and even make\n\n\"sub\": \"1234567890\",\n\n(data)\n\nrequests to downstream services. It doesn't matter which domains are\n\n\"name\": \"John Doe\",\n\nserving your APIs, so Cross-Origin Resource Sharing (CORS) won't be an\n\n\"admin\": true\n\nissue as it doesn't use cookies17.\n\n}\n\nHMACSHA256(\n\nSignature\n\nbase64UrlEncode(header) + \".\" +\n\nbase64UrlEncode(payload),\n\n14 https://aws.amazon.com/cognito/\n\n) secret base64 encoded\n\n15 https://auth0.com/how-it-works\n\n16 https://jwt.io/\n\n18 https://en.wikipedia.org/wiki/Hash-based_message_authentication_code\n\n17 https://jwt.io/introduction/\n\n19 https://jwt.io/introduction/\n\n49\n\nGo to Table of Contents\n\nJWT tokens are usually passed by the HTTP Bearer header, then stored\n\nclient side using localStorage or any other resource. Tokens can be\n\ninvalidated at that time (exp claim included into token).\n\nOnce returned from authorization, service tokens can be passed to all API\n\ncalls and validated server side. Because of the HMAC based signing\n\nprocess, tokens are safe.\n\nBROWSER\n\nSERVER\n\n1. POST /users/login with username and password\n\n3. Returns the JWT to the Browser\n\n2. Creates a JWT with a secret\n\n4. Sends the JWT on the Authorization Header\n\n6. Sends response to the client\n\n5. Check JWT signature. Get user information from the JWT\n\nFig. 17: JWT based authorization is pretty straight forward and it’s safe. Tokens can be\n\ntrusted by authorized parties because of the HMAC signature; therefore information\n\ncontained by them can be used without checking ACL’s and any further permissions.\n\nDeployment of microservices\n\nIf done wrong, microservices may come with an overhead of operational\n\ntasks needed for the deployments and maintenance. When dividing a\n\nmonolithic platform into smaller pieces, each of them should be easy to\n\ndeploy in an automatic way.\n\n18 https://en.wikipedia.org/wiki/Hash-based_message_authentication_code\n\n19 https://jwt.io/introduction/\n\nGo to Table of Contents\n\n50\n\nMicroservices should be coupled as loosely as possible, It should be\n\npossible to develop, test, deploy and scale them independently.\n\nSometimes an application should even be able to work without particular\n\nservices (to comply with HA - high availability)… To achieve these\n\nrequirements, each microservice should have a separate data store.\n\nSounds easy - but what about the data itself? How to spread the\n\ninformation changes between services? What about consistency within\n\nthe data?\n\nJWT tokens are usually passed by the HTTP Bearer header, then stored\n\nNowadays, we see two main concepts that facilitates such a process -\n\nclient side using localStorage or any other resource. Tokens can be\n\ncontainerization and serverless architecture.\n\ninvalidated at that time (exp claim included into token).\n\nDocker and containerization\n\nOnce returned from authorization, service tokens can be passed to all API\n\ncalls and validated server side. Because of the HMAC based signing\n\nIf you are not familiar with containerization, then here are the most\n\nprocess, tokens are safe.\n\ncommon beneﬁts that make it worth digging deeper into this concept:\n\nDocker allows you to build an application once and then execute it in all\n\nBROWSER\n\nSERVER\n\nyour environments no matter what the differences between them.\n\nDocker helps you to solve dependency and incompatibility issues.\n\n1. POST /users/login with username and password\n\nDocker is like a virtual machine without the overhead.\n\n2. Creates a JWT\n\nwith a secret\n\n3. Returns the JWT to the Browser\n\nDocker environments can be fully automated.\n\nDocker is easy to deploy.\n\n4. Sends the JWT on the Authorization Header\n\n5. Check JWT signature.\n\nDocker allows for separation of duties.\n\nGet user information\n\n6. Sends response to the client\n\nfrom the JWT\n\nDocker allows you to scale easily.\n\nDocker has a huge community.\n\nFig. 17: JWT based authorization is pretty straight forward and it’s safe. Tokens can be\n\ntrusted by authorized parties because of the HMAC signature; therefore information\n\ncontained by them can be used without checking ACL’s and any further permissions.\n\nLet's start with a quote from the Docker page:\n\nDocker containers wrap up a piece of software in a complete ﬁlesystem\n\nDeployment of microservices\n\nthat contains everything it needs to run: code, runtime, system tools,\n\nsystem libraries – anything you can install on a server. This guarantees that\n\nIf done wrong, microservices may come with an overhead of operational\n\nit will always run the same, regardless of the environment it is running in.\n\ntasks needed for the deployments and maintenance. When dividing a\n\nmonolithic platform into smaller pieces, each of them should be easy to\n\nThis might sound familiar: virtualization allows you to achieve pretty much\n\ndeploy in an automatic way.\n\nthe same goals but in contrast to virtualization, Docker runs all processes\n\ndirectly on the host operating system. This helps to avoid the overhead of\n\n18 https://en.wikipedia.org/wiki/Hash-based_message_authentication_code\n\na virtual machine (both performance and maintenance).\n\n19 https://jwt.io/introduction/\n\n51\n\nGo to Table of Contents",
      "page_number": 34
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 52-59)",
      "start_page": 52,
      "end_page": 59,
      "detection_method": "topic_boundary",
      "content": "Docker achieves this using the isolation features of the Linux kernel such\n\nas Cgroups and kernel namespaces. Each container has its own process\n\nspace, ﬁlesystem and memory. You can run all kinds of Linux distributions\n\ninside a container. What makes Docker really useful is the community and\n\nall projects that complement the main functionality. There are multiple\n\ntools to automate common tasks, orchestrate and scale containerized\n\nsystems. Docker is also heavily supported by many companies, just to\n\nname a couple: Amazon, Google, Microsoft. Currently, Docker also allows\n\nus to run Windows inside containers (only on Windows hosts).\n\nDocker basics\n\nBefore we dig into using Docker for the Microservices architecture let’s\n\nbrowse the top-level details of how it works.\n\nImage - holds the ﬁle system and parameters needed to run an\n\napplication. It does not have any state and it does not change. You can\n\nunderstand an image as a template used to run containers.\n\nContainer - this is a running instance of an image. You can run multiple\n\ninstances of the same image. It has a state and can change.\n\nImage layer - each image is built out of layers. Images are usually built by\n\nrunning commands or adding/modifying ﬁles (using a Dockerﬁle). Each\n\nstep that is run in order to build an Image is an image layer. Docker saves\n\neach layer, so when you run a build next time, it is able to reuse the layers\n\nthat did not change. Layers are shared between all images so if two\n\nimages start with similar steps, the layers are shared between them. You\n\ncan see this illustrated below.\n\nGo to Table of Contents\n\n52\n\nDocker achieves this using the isolation features of the Linux kernel such\n\nas Cgroups and kernel namespaces. Each container has its own process\n\nspace, ﬁlesystem and memory. You can run all kinds of Linux distributions\n\ninside a container. What makes Docker really useful is the community and\n\nall projects that complement the main functionality. There are multiple\n\ntools to automate common tasks, orchestrate and scale containerized\n\nsystems. Docker is also heavily supported by many companies, just to\n\nname a couple: Amazon, Google, Microsoft. Currently, Docker also allows\n\nus to run Windows inside containers (only on Windows hosts).\n\nDocker basics\n\nFig. 18: You can use https://imagelayers.io/ to analyze Docker image layers and compare\n\nthem to each other. For example: ruby, python, node images share ﬁve layers - this means\n\nthat if you download all three images the ﬁrst 5 layers will be downloaded only once.\n\nBefore we dig into using Docker for the Microservices architecture let’s\n\nbrowse the top-level details of how it works.\n\nAs you can see, all compared images share common layers. So if you\n\ndownload one of them, the shared layers will not be downloaded and\n\nImage - holds the ﬁle system and parameters needed to run an\n\nstored again when downloading a different image. In fact, changes in a\n\napplication. It does not have any state and it does not change. You can\n\nrunning container are also seen as an additional, uncommitted layer.\n\nunderstand an image as a template used to run containers.\n\nRegistry - a place where images and image layers are kept. You can build\n\nContainer - this is a running instance of an image. You can run multiple\n\nan image on your CI server, push it to a registry and then use the image\n\ninstances of the same image. It has a state and can change.\n\nfrom all of your nodes without the need to build the images again.\n\nImage layer - each image is built out of layers. Images are usually built by\n\nOrchestration (docker-compose) - usually a system is built of several or\n\nrunning commands or adding/modifying ﬁles (using a Dockerﬁle). Each\n\nmore containers. This is because you should have only one concern per\n\nstep that is run in order to build an Image is an image layer. Docker saves\n\ncontainer. Orchestration allows you to run a multi-container application\n\neach layer, so when you run a build next time, it is able to reuse the layers\n\nmuch easier and docker-compose is the most commonly used tool to\n\nthat did not change. Layers are shared between all images so if two\n\nachieve that. It has the ability to run multiple containers that can be\n\nimages start with similar steps, the layers are shared between them. You\n\nconnected with networks and share volumes.\n\ncan see this illustrated below.\n\n53\n\nGo to Table of Contents\n\nVM vs. Container\n\nAs mentioned earlier, Docker might seem similar to virtual machines but\n\nworks in an entirely different way.\n\nVirtual machines work exactly as the name suggests: by creating a\n\nvirtualized machine that the guest system is using. The main part is a\n\nHypervisor running on the host system and granting access to all kinds of\n\nresources for the guest systems. On top of the Hypervisor, there are Guest\n\nOS’s running on each virtual machine. Your application is using this Guest\n\nOS.\n\nWhat Docker does differently is directly using the host system (no need\n\nfor Hypervisor and Guest OS), it runs the containers using several features\n\nof the Linux kernel that allow them to securely separate the processes\n\ninside them. Thanks to this, a process inside the container cannot\n\ninﬂuence processes outside of it. This approach makes Docker more\n\nlightweight both in terms of CPU/Memory usage, and disk space usage.\n\nApp 1\n\nApp 2\n\nApp 3\n\nBins/Libs\n\nBins/Libs\n\nBins/Libs\n\nGuest OS\n\nGuest OS\n\nGuest OS\n\nHypervisor\n\nHost Operating System\n\nInfrastructure\n\nGo to Table of Contents\n\n54\n\nVM vs. Container\n\nApp 1\n\nApp 2\n\nApp 3\n\nBins/Libs\n\nBins/Libs\n\nBins/Libs\n\nAs mentioned earlier, Docker might seem similar to virtual machines but\n\nworks in an entirely different way.\n\nDocker Engine\n\nVirtual machines work exactly as the name suggests: by creating a\n\nvirtualized machine that the guest system is using. The main part is a\n\nOperating System\n\nHypervisor running on the host system and granting access to all kinds of\n\nInfrastructure\n\nresources for the guest systems. On top of the Hypervisor, there are Guest\n\nOS’s running on each virtual machine. Your application is using this Guest\n\nOS.\n\nFig. 19: Similar features, different architecture - Virtualization vs, Dockerization. Docker,\n\nWhat Docker does differently is directly using the host system (no need\n\nleverages containerization - lightweight abstraction layer between application and the\n\nfor Hypervisor and Guest OS), it runs the containers using several features\n\noperating system / hardware. It separates the user processes but without running the\n\nwhole operating system/kernel inside the container.\n\nof the Linux kernel that allow them to securely separate the processes\n\ninside them. Thanks to this, a process inside the container cannot\n\ninﬂuence processes outside of it. This approach makes Docker more\n\nFrom dev to production\n\nlightweight both in terms of CPU/Memory usage, and disk space usage.\n\nOk, so we have the technical introduction covered. Now let’s see how\n\nApp 1\n\nApp 2\n\nApp 3\n\nDocker helps to build, run and maintain a Microservice oriented\n\nBins/Libs\n\nBins/Libs\n\nBins/Libs\n\napplication.\n\nGuest OS\n\nGuest OS\n\nGuest OS\n\nDevelopment\n\nHypervisor\n\nHost Operating System\n\nDevelopment is usually the ﬁrst phase where Docker brings some extra\n\nvalue, and it is even more helpful with Microservice oriented applications.\n\nInfrastructure\n\nAs mentioned earlier, Docker comes with tools that allow us to orchestrate\n\na multi-container setup in a very easy way. Let's take a look at the beneﬁts\n\nDocker brings during development.\n\n55\n\nGo to Table of Contents\n\nEasy setup - low cost of introducing new developers\n\nYou only need to create a Docker conﬁguration once and then each new\n\ndeveloper on the team can start the project by executing a single\n\ncommand. No need to conﬁgure the environment, just download the\n\nproject and run docker-compose up. That's all!\n\nThis might seem too good to be true but I have a good, real-life example\n\nof such a situation. I was responsible for a project where a new front-end\n\ndeveloper was hired. The project was written in a very old PHP version\n\n(5.3) and had to be run on CentOS. The developer was using Windows\n\nand he previously worked on Java projects exclusively. I had a quick call\n\nwith him and we went through a couple of simple steps: downloading and\n\ninstalling Docker, cloning the git repository and running docker-compose.\n\nAfter no more than 30 minutes he had a perfectly running environment\n\nand was ready to write his ﬁrst lines of code!\n\nNo dependencies version mismatch issue\n\nThis issue often arises if a developer is involved in multiple projects, but it\n\nescalates in Micro-service oriented applications. Each service can be\n\nwritten by a different team and using different technologies. In some\n\ncases (itusually happens quite often) there might be a version mismatch\n\nwithin the same technology used in different services. A simple example:\n\none service is using an older elastic version, and another a newer one.\n\nThis can be dealt withaccomplished by conﬁguring two separate versions\n\nbut it is much easier to run them side-by-side in dedicated containers. A\n\nvery simple example of such a conﬁguration for docker-compose would\n\nlook like this:\n\nGo to Table of Contents\n\n56\n\nEasy setup - low cost of introducing new developers\n\nservice_x_elastic: image: elasticsearch:5.2.2 service_y_elastic: image: elasticsearch:2.4.4\n\nYou only need to create a Docker conﬁguration once and then each new\n\ndeveloper on the team can start the project by executing a single\n\ncommand. No need to conﬁgure the environment, just download the\n\nproject and run docker-compose up. That's all!\n\nPossibility to test if the application scales\n\nThis might seem too good to be true but I have a good, real-life example\n\nof such a situation. I was responsible for a project where a new front-end\n\nTesting if the application scales is pretty easy with Docker. Of course, you\n\ndeveloper was hired. The project was written in a very old PHP version\n\nwon't be able to make some serious load testing on your local machine,\n\n(5.3) and had to be run on CentOS. The developer was using Windows\n\nbut you can test if the application works correctly when a service is scaled\n\nand he previously worked on Java projects exclusively. I had a quick call\n\nhorizontally. Horizontal scalability usually fails if the Microservice is not\n\nwith him and we went through a couple of simple steps: downloading and\n\nstateless and the state is not shared between instances. Scaling can be\n\ninstalling Docker, cloning the git repository and running docker-compose.\n\nvery easily achieved using docker-compose:\n\nAfter no more than 30 minutes he had a perfectly running environment\n\nand was ready to write his ﬁrst lines of code!\n\ndocker-compose scale service_x=4\n\nNo dependencies version mismatch issue\n\nAfter running this command there will be four containers running the\n\nsame service_x. You can (and you should) also add a separate container\n\nThis issue often arises if a developer is involved in multiple projects, but it\n\nwith a load balancer like HAProxy in front of them. That's it. You are ready\n\nescalates in Micro-service oriented applications. Each service can be\n\nto test!\n\nwritten by a different team and using different technologies. In some\n\ncases (itusually happens quite often) there might be a version mismatch\n\nNo more “works on my conﬁguration\" issues\n\nwithin the same technology used in different services. A simple example:\n\none service is using an older elastic version, and another a newer one.\n\nDocker is a solution that allows one conﬁguration to be run everywhere.\n\nThis can be dealt withaccomplished by conﬁguring two separate versions\n\nYou can have the same - or almost the same - version running on all\n\nbut it is much easier to run them side-by-side in dedicated containers. A\n\ndeveloper machines, CI, staging, and production. This radically reduces\n\nvery simple example of such a conﬁguration for docker-compose would\n\nthe amount of “works on my conﬁguration\" situations. At least it reduces\n\nlook like this:\n\nthe ones caused by different setups.\n\n57\n\nGo to Table of Contents\n\nContinuous Integration\n\nNow that you have a working development setup, conﬁguring a CI is\n\nreally easy. You just need to setup your CI to run the same\n\ndocker-compose up command and then run your tests, etc. No need to\n\nwrite any special conﬁguration; just bring the containers up and run your\n\ntests. I've worked with different CI servers like Gitlab CI, Circle CI, Jenkins\n\nand the setup was always quick and easy. If some tests fail, it is easy to\n\ndebug too. Just run the tests locally.\n\nPre-production\n\nWhen you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\nshared between your local disk and the disk inside a container. When\n\ndeveloping an application, you usually setup containers to share all\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\neach change. On pre-production and production servers, project ﬁles\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nThe other common change applies to ports. When using Docker for\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nGo to Table of Contents\n\n58\n\nContinuous Integration\n\nSo when running on any production or pre-production servers you usually\n\nput a load balancer in front of the containers.\n\nNow that you have a working development setup, conﬁguring a CI is\n\nreally easy. You just need to setup your CI to run the same\n\nThere are many tools that make running pre-production servers much\n\ndocker-compose up command and then run your tests, etc. No need to\n\neasier. You should deﬁnitely check out projects like Docker Swarm,\n\nwrite any special conﬁguration; just bring the containers up and run your\n\nKubernetes and Rancher. I really like Rancher as it is easy to setup and\n\ntests. I've worked with different CI servers like Gitlab CI, Circle CI, Jenkins\n\nreally easy to use. We use Rancher as our main staging management tool\n\nand the setup was always quick and easy. If some tests fail, it is easy to\n\nand all co-workers really enjoy working with it. Just to give you a small\n\ndebug too. Just run the tests locally.\n\ninsight into how powerful such tools are: all our team members are able\n\nto update or create a new staging environment without any issues - and\n\nwithin a few minutes!\n\nPre-production\n\nProduction\n\nWhen you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe production conﬁguration should be exactly\n\nthe same as\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\npre-production. The only small difference might be the tool you use to\n\nshared between your local disk and the disk inside a container. When\n\nmanage the containers. There are a multitude of popular tools used to run\n\ndeveloping an application, you usually setup containers to share all\n\nproduction containers but my two favorites are Amazon EC2 Container\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\nService and Google Cloud with Kubernetes on top. Both tools allow you\n\neach change. On pre-production and production servers, project ﬁles\n\nto scale easily on new hosts.\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nOne important thing you should keep in mind when going with Docker on\n\nproduction - monitoring and logging. Both should be centralized and\n\nThe other common change applies to ports. When using Docker for\n\neasy to use.\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nCons\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nDocker has some downsides too. The ﬁrst one you might notice is that it\n\n59\n\nGo to Table of Contents",
      "page_number": 52
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 60-67)",
      "start_page": 60,
      "end_page": 67,
      "detection_method": "topic_boundary",
      "content": "takes some time to learn how to use Docker. The basics are pretty easy to\n\nlearn, but it takes time to master some more complicated settings and\n\nconcepts. The main disadvantage for me is that it runs very slowly on\n\nMacOS and Windows. Docker is built around many different concepts\n\nfrom the Linux kernel so it is not able to run directly on MacOS or\n\nWindows. It uses a Virtual Machine that runs Linux with Docker.\n\nSummary\n\nDocker and the Microservice architecture approach work very well\n\ntogether and both concepts gain popularity each year. Over the past 4\n\nyears, we have been able to observe how Docker has gotten better and\n\nmore mature with each release. At the same time, the whole ecosystem\n\nhas grown and new tools have been published giving us more possibilities\n\nthat we could not have thought of. By using Docker, we are able to easily\n\nrun our Microservice oriented applications on our developer machines\n\nand then run the same setup on pre- and production servers. Right now\n\nwe can conﬁgure a setup within minutes and then release our application\n\nto a server also within minutes. I'm really curious about what new\n\npossibilities we will get in the coming months.\n\nServerless - Function as a Service\n\nServerless is not exclusively bound to microservice oriented applications\n\nbut it is deﬁnitely good to know this concept, as it might be helpful in\n\nmany cases.\n\nGo to Table of Contents\n\n60\n\nLet me start with a couple of quotes that might be helpful for you to\n\ntakes some time to learn how to use Docker. The basics are pretty easy to\n\nunderstand what serverless is about:\n\nlearn, but it takes time to master some more complicated settings and\n\nconcepts. The main disadvantage for me is that it runs very slowly on\n\nMacOS and Windows. Docker is built around many different concepts\n\nServerless is a new cloud computing trend that changes the way you think about writing and maintaining applications.\n\nfrom the Linux kernel so it is not able to run directly on MacOS or\n\nWindows. It uses a Virtual Machine that runs Linux with Docker.\n\n— AUTH0.COM\n\nDeploy your applications as independent functions, that respond to events, charge you only when they run, and scale automatically.\n\nSummary\n\nDocker and the Microservice architecture approach work very well\n\n— SERVERLESS.COM\n\ntogether and both concepts gain popularity each year. Over the past 4\n\nyears, we have been able to observe how Docker has gotten better and\n\nServerless architectures refer to (..) custom code that's run in ephemeral containers.\n\nmore mature with each release. At the same time, the whole ecosystem\n\nhas grown and new tools have been published giving us more possibilities\n\nthat we could not have thought of. By using Docker, we are able to easily\n\n— MARTINFOWLER.COM\n\nrun our Microservice oriented applications on our developer machines\n\nand then run the same setup on pre- and production servers. Right now\n\nAs you can see, each of the quotes looks at serverless from a totally\n\nwe can conﬁgure a setup within minutes and then release our application\n\ndifferent perspective. This does not mean that some of the quotes are\n\nto a server also within minutes. I'm really curious about what new\n\nbetter, I think that all describe serverless in a very good way.\n\npossibilities we will get in the coming months.\n\nServerless is considered to be a very bad name for what we are talking\n\nabout. This is for two reasons:\n\nServerless - Function as a Service\n\nServerless as a concept has a broader meaning than what it usually\n\nServerless is not exclusively bound to microservice oriented applications\n\nrefers to; Serverless architecture can be used to describe both Backend as\n\nbut it is deﬁnitely good to know this concept, as it might be helpful in\n\na Service and Function as a Service. Usually, and also in this article, we are\n\nmany cases.\n\ninterested in the latter: FaaS.\n\n61\n\nGo to Table of Contents\n\nServerless is a lie. The truth is that servers are still there, Ops are also\n\nthere. So why is this called „serverless” - it’s called so because you, as a\n\nbusiness or as a developer, do not need to think about servers or ops.\n\nThey are hidden behind an abstraction that makes them invisible to you.\n\nBoth servers and ops are managed by a vendor like Amazon, Google,\n\nMicrosoft, etc.\n\nIn the context of microservice architecture, FaaS is the concept that is\n\ninteresting for us.\n\nServerless providers\n\nCurrently, there are 4 major Clouds that allow us to use serverless\n\narchitecture:\n\nAWS Lambda - named as the ﬁrst adopter of FaaS, easily integrates\n\nwith the rest of Amazon Web Services such as SNS or S3.\n\nGoogle Cloud Functions - still in beta, allows us to run our functions in\n\nGoogle Cloud. The drawback is, it currently only supports Node.js and\n\nJavaScript.\n\nAzure Functions - supports the widest range of languages (JavaScript,\n\nC#, F#, Python, PHP, Bash, Batch, and PowerShell) and easily allows us\n\nto integrate with Github for storing our code.\n\n\n\nIBM Bluemix OpenWhisk -\n\nit uses the open-source Apache\n\nOpenWhisk project running on top of the IBM Bluemix infrastructure.\n\nGo to Table of Contents\n\n62\n\nServerless is a lie. The truth is that servers are still there, Ops are also\n\nThe most notable feature is that you can use your Docker images to run\n\nthere. So why is this called „serverless” - it’s called so because you, as a\n\nas functions. A meaningful use-case of IBM OpenWhisk is a DarkVision\n\nbusiness or as a developer, do not need to think about servers or ops.\n\nApplication20, which shows how that technology can be used with\n\nThey are hidden behind an abstraction that makes them invisible to you.\n\ntechniques like Visual Recognition, Speech to Text and Natural Language\n\nBoth servers and ops are managed by a vendor like Amazon, Google,\n\nUnderstanding.\n\nMicrosoft, etc.\n\nAlthough it seems that we have a choice, we must keep in mind that\n\nIn the context of microservice architecture, FaaS is the concept that is\n\ncommonly, such services are tightly coupled with other services of the\n\ninteresting for us.\n\nparticular Cloud, such as databases, message brokers or data storages.\n\nMostly, the wiser choice is just to use the serverless functionality of the\n\nCloud that we already use to run the rest of our microservices.\n\nServerless providers\n\nIn the next sections, we’ll use AWS Lambda for all of the examples, but\n\nCurrently, there are 4 major Clouds that allow us to use serverless\n\nthe core concepts remain the same across all of the serverless providers.\n\narchitecture:\n\nAWS Lambda - named as the ﬁrst adopter of FaaS, easily integrates\n\nFaaS\n\nwith the rest of Amazon Web Services such as SNS or S3.\n\nIn an FaaS approach, developers are writing code - and code only. They\n\nGoogle Cloud Functions - still in beta, allows us to run our functions in\n\ndo not need to care about the infrastructure, deployment, scalability, etc.\n\nGoogle Cloud. The drawback is, it currently only supports Node.js and\n\nThe code they write represents a simple and small function of the\n\nJavaScript.\n\napplication.\n\nAzure Functions - supports the widest range of languages (JavaScript,\n\nC#, F#, Python, PHP, Bash, Batch, and PowerShell) and easily allows us\n\nto integrate with Github for storing our code.\n\nIBM Bluemix OpenWhisk -\n\n\n\nit uses the open-source Apache\n\nOpenWhisk project running on top of the IBM Bluemix infrastructure.\n\n20 https://github.com/IBM-Bluemix/openwhisk-darkvisionapp\n\n63\n\nGo to Table of Contents\n\nIt is run in response to a trigger and can use external services:\n\nTrigger\n\nExternal service\n\nFunction\n\nFig. 20: Basic function as a service architecture consists of only two elements: the function\n\nto be run and a trigger to listen for. Usually the function is also connected to third-party\n\nservices like a database.\n\nA trigger can be almost anything. Based on AWS Lambda, the most\n\npopular FaaS service, the trigger might be:\n\nAPI call (any HTTP request).\n\nS3 bucket upload.\n\nNew event in queue.\n\nScheduled jobs.\n\nOther AWS Lambda functions.\n\nand many others, you can check it:\n\nhttp://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html.\n\nGo to Table of Contents\n\n64\n\nEach function should comply with the following rules:\n\nIt is run in response to a trigger and can use external services:\n\nIt should not access the disk - AWS allows using a temporary /tmp\n\ndirectory that allows storing 512MB of data.\n\nExternal\n\nIt should be stateless and share-nothing. You can imagine it as a server\n\nservice\n\nTrigger\n\npowered up and conﬁgured to only handle one request (and then\n\ndestroyed).\n\nConcise - your function should not take too long to run (usually\n\nFunction\n\nseconds, but up to 300 seconds for AWS Lambda).\n\nOnce you have such a function, you just upload it to your service provider\n\nFig. 20: Basic function as a service architecture consists of only two elements: the function\n\nand provide some basic conﬁguration. From that moment, on each action\n\nto be run and a trigger to listen for. Usually the function is also connected to third-party\n\nservices like a database.\n\nconﬁgured as a trigger, your function will be executed. The service\n\nprovider tracks how long it takes for your function to execute, and\n\nmultiplies the time by the amount of RAM conﬁgured (that's a limit you\n\nA trigger can be almost anything. Based on AWS Lambda, the most\n\ncan change). You pay for GB-seconds of execution. This means that if your\n\npopular FaaS service, the trigger might be:\n\nfunction is not executed, you do not pay anything and if your function is\n\nexecuted thousands of times during one day, you pay only for the\n\nAPI call (any HTTP request).\n\nGB-seconds your function took to run. There are no charges for scaling or\n\nS3 bucket upload.\n\nidle time.\n\nNew event in queue.\n\nScheduled jobs.\n\nThe cost of one GB-second on AWS Lambda is currently $0.00001667 -\n\nOther AWS Lambda functions.\n\nthis means that if your application requires 1024MB of RAM, and runs\n\nand many others, you can check it:\n\noverall for 1,000,000 seconds (one million seconds), that is 277 hours\n\nhttp://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html.\n\n(over 11 days), you will be charged $16.77; There is also an additional\n\nprice of $0.20 per 1 million requests. It gets even better if you check out\n\n65\n\nGo to Table of Contents\n\nthe free tier that Amazon offers. Each month you get 3,200,000 seconds\n\nto run a function with a 128MB memory limit for free. That’s over 890h -\n\nover 37 days!\n\nI think the calculations above clearly show that you can gain a huge\n\nbeneﬁt by moving some parts (or all parts) of your application to a FaaS\n\nprovider. You get the scalability and ops for free, as you do not need to\n\ntake care of it.\n\nInternally, functions are run in small, ephemeral and stateless containers\n\nthat are spawn if your application needs to scale up.\n\nYou can ﬁnd an interesting cost comparison to EC2 instances here:\n\nhttps://www.trek10.com/blog/lambda-cost/.\n\nArchitecture\n\nI won’t describe the architecture details of a serverless application in this\n\narticle as it should be quite straightforward when writing a microservice\n\napplication. The obvious and required step is to move as much\n\npresentation and logic to the customer as possible. Usually, your\n\napplication front-end should be a mobile app or a single-page app.\n\nOn the back-end, you can start with a very simple architecture where the\n\nfunction is triggered by an API call and then connects to a DynamoDB\n\ninstance (or any other on premise data source like MongoDB, MySQL) to\n\nfetch/modify some data. Then, you can apply direct read access to some\n\ndata in your DynamoDB and allow clients to fetch the data directly,\n\nGo to Table of Contents\n\n66\n\nbut handle all data-modifying requests using your function. You can also\n\nthe free tier that Amazon offers. Each month you get 3,200,000 seconds\n\nintroduce Event Sourcing very easily by having one function that records\n\nto run a function with a 128MB memory limit for free. That’s over 890h -\n\nan event and other functions that take the event in order to refresh your\n\nover 37 days!\n\nread model.\n\nI think the calculations above clearly show that you can gain a huge\n\nYou can also use FaaS to implement batch processing: split the stream of\n\nbeneﬁt by moving some parts (or all parts) of your application to a FaaS\n\ndata into smaller chunks and then send them to another function that will\n\nprovider. You get the scalability and ops for free, as you do not need to\n\nrun multiple instances of itself simultaneously. This allows you to process\n\ntake care of it.\n\nthe data much faster. FaaS is often used to do real-time log processing.\n\nInternally, functions are run in small, ephemeral and stateless containers\n\nthat are spawn if your application needs to scale up.\n\nIt’s easy!\n\nYou can ﬁnd an interesting cost comparison to EC2 instances here:\n\nhttps://www.trek10.com/blog/lambda-cost/.\n\nJust a quick „hello world” example to show you how easily you can start\n\nwriting serverless applications:\n\nArchitecture\n\nexports.handler = (event, context, callback) => { callback(null, 'Hello World'); };\n\nI won’t describe the architecture details of a serverless application in this\n\narticle as it should be quite straightforward when writing a microservice\n\napplication. The obvious and required step is to move as much\n\npresentation and logic to the customer as possible. Usually, your\n\nSummary\n\napplication front-end should be a mobile app or a single-page app.\n\nBeneﬁts\n\nOn the back-end, you can start with a very simple architecture where the\n\nfunction is triggered by an API call and then connects to a DynamoDB\n\nFaaS is easy to learn and implement, and it allows you to reduce the time\n\ninstance (or any other on premise data source like MongoDB, MySQL) to\n\nto market. It also allows you to reduce costs, and to scale easily. Each\n\nfetch/modify some data. Then, you can apply direct read access to some\n\nfunction you write ﬁts easily into a sprint, so it is easy to write serverless\n\ndata in your DynamoDB and allow clients to fetch the data directly,\n\napplications in agile teams.\n\n67\n\nGo to Table of Contents",
      "page_number": 60
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 68-75)",
      "start_page": 68,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "Drawbacks\n\nThere might be a small vendor lock-in if you do not take this into\n\nconsideration and do not introduce proper architecture. You should be\n\naware of the communication overhead that is added by splitting the app\n\ninto such small services. The most common issues mentioned are\n\nmultitenancy (the same issue as with running containers on Amazon) and\n\ncold start - when scaling up, it takes some time to handle the ﬁrst request\n\nby a new container. It might also be a bit harder to test such an\n\napplication.\n\nGood use-cases\n\nHere are some use-cases that are interesting in my opinion:\n\nMostly static pages, including eCommerce; You can host static content\n\non a CDN server or add cache in front of your functions.\n\nData stream analysis.\n\nProcessing uploads - image/video thumbnails, etc.\n\nActions users pay for. For example, adding watermarks to an ebook.\n\nOther cases when your application is not fully using the server capacity\n\nor you need to add scalability without investing much time and money.\n\nGo to Table of Contents\n\n68\n\nDrawbacks\n\nContinuous Deployment\n\nThere might be a small vendor lock-in if you do not take this into\n\nJust imagine that each of your microservices needs to be ﬁrst built and\n\nconsideration and do not introduce proper architecture. You should be\n\nthen deployed manually, not even mentioning running unit tests or any\n\naware of the communication overhead that is added by splitting the app\n\nkind of code-style tools. Having tens of those would be extremely\n\ninto such small services. The most common issues mentioned are\n\ntime-consuming and would often be a major bottleneck in the whole\n\nmultitenancy (the same issue as with running containers on Amazon) and\n\ndevelopment process.\n\ncold start - when scaling up, it takes some time to handle the ﬁrst request\n\nby a new container. It might also be a bit harder to test such an\n\nHere comes the idea of Continuous Deployment - the thing that puts the\n\napplication.\n\nworkﬂow of your whole\n\nIT department together.\n\nIn Continuous\n\nDeployment we can automate all things related with building Docker\n\ncontainers, running unit and\n\nfunctional tests and even testing\n\nGood use-cases\n\nperformance of newly built services. At the end, if everything passes -\n\nnothing prevents us from automatically deploying working solutions into\n\nHere are some use-cases that are interesting in my opinion:\n\nproduction.\n\nMostly static pages, including eCommerce; You can host static content\n\nThe most commonly used software that handles the whole process is\n\non a CDN server or add cache in front of your functions.\n\nJenkins, Travis CI, Bamboo or CircleCI. We’ll show you how to do it using\n\nJenkins.\n\nData stream analysis.\n\nProcessing uploads - image/video thumbnails, etc.\n\nDesigning deployment pipeline\n\nActions users pay for. For example, adding watermarks to an ebook.\n\nGoing from the big picture, a common pipeline could look like this:\n\nOther cases when your application is not fully using the server capacity\n\nGitHub\n\nJenkins\n\nor you need to add scalability without investing much time and money.\n\ngit push\n\nwebhook\n\ndeploy\n\nAWS\n\nFig. 21: Overview of our ﬁnal Continuous Deployment pipeline.\n\n69\n\nGo to Table of Contents\n\nMost of the hard work is done by that nice looking guy, called Jenkins.\n\nWhen someone pushes something to our Git repository (e.g. Github), the\n\nwebhook triggers a job inside our Jenkins instance. It can consist of the\n\nfollowing steps:\n\n1. Build Docker image.\n\n2. Run unit-tests inside the container.\n\n3. Push image to our images repository (e.g. Docker Hub, Amazon ECR).\n\n4. Deploy using Ansible or task schedulers like Amazon ECS.\n\na. Run functional tests (Selenium).\n\nb. Run performance tests (JMeter).\n\nAfter all this, we can set up a Slack notiﬁcation that will inform us of\n\nsuccess or failure of the whole process. The important thing is, that we\n\nshould keep our Jenkins instance clean, so running all of the unit tests\n\nshould be done inside a Docker container.\n\nCoding our pipeline\n\nOnce we have the idea of our build process, we can code it using the\n\nJenkinsﬁle. It’s a ﬁle that describes our whole deployment pipeline. It\n\nconsists of stages and build steps. Mostly, at the end of the pipeline we\n\ninclude post actions that should be ﬁred when the build was successful or\n\nfailed.\n\nWe should keep this ﬁle in our application’s code repository - that way\n\ndevelopers can also work with it, without asking DevOps for changes in\n\nthe deployment procedure.\n\nGo to Table of Contents\n\n70\n\nMost of the hard work is done by that nice looking guy, called Jenkins.\n\nHere is a sample Jenkinsﬁle built on the basis of the previously mentioned\n\nWhen someone pushes something to our Git repository (e.g. Github), the\n\nsteps. As we can see, the ﬁnal step is to run another Jenkins job named\n\nwebhook triggers a job inside our Jenkins instance. It can consist of the\n\ndeploy. Jobs can be tied together to be more reusable - that way we can\n\nfollowing steps:\n\ndeploy our application without having to run all of the previous steps.\n\n1. Build Docker image.\n\n#!groovy pipeline { agent any stages { stage('Build Docker') { steps { sh \"docker build ...\" } } stage('Push Docker Image') { steps { sh 'docker push ...' } } stage('Deploy') { steps { build job: 'deploy' } } }\n\n2. Run unit-tests inside the container.\n\n3. Push image to our images repository (e.g. Docker Hub, Amazon ECR).\n\n4. Deploy using Ansible or task schedulers like Amazon ECS.\n\na. Run functional tests (Selenium).\n\nb. Run performance tests (JMeter).\n\nAfter all this, we can set up a Slack notiﬁcation that will inform us of\n\nsuccess or failure of the whole process. The important thing is, that we\n\nshould keep our Jenkins instance clean, so running all of the unit tests\n\nshould be done inside a Docker container.\n\nCoding our pipeline\n\nOnce we have the idea of our build process, we can code it using the\n\nJenkinsﬁle. It’s a ﬁle that describes our whole deployment pipeline. It\n\nconsists of stages and build steps. Mostly, at the end of the pipeline we\n\npost { success { slackSend color: 'good', message: \"Build Success\" } failure { slackSend color: 'danger', message: \"Build Failed\" } } }\n\ninclude post actions that should be ﬁred when the build was successful or\n\nfailed.\n\nWe should keep this ﬁle in our application’s code repository - that way\n\ndevelopers can also work with it, without asking DevOps for changes in\n\nthe deployment procedure.\n\n71\n\nGo to Table of Contents\n\nRelated technologies\n\nGo to Table of Contents\n\n72\n\nMicroservices based eCommerce platforms\n\nThere are major open-source platforms that were built using the\n\nMicroservices approach by design. This section tries to list those that we\n\nthink could be used as a reference for designing your architecture - or\n\neven better - could be used as a part of it.\n\nSylius\n\nSylius is the ﬁrst Open Source eCommerce platform constructed from\n\nstandalone components. What does it mean in practice? Every aspect of\n\nthe shopping process is handled by individual PHP libraries. While the\n\nproject itself provides a complete shop solution with a REST API, these\n\ndecoupled components can be used separately to build Microservice\n\napplications.\n\nLet’s say we need to have two services for handling a Product Catalog and\n\nPromotions, respectively. The solution would be to take the two\n\nRelated\n\ncomponents and use them to develop two standalone applications.\n\nBefore Sylius, you would have needed to write everything from scratch or\n\ntechnologies\n\nstrip the functionality from an existing eCommerce software.\n\nOn top of that, Sylius is based on the highly scalable Symfony framework,\n\nwhich integrates with a wide range of caching solutions, from Redis,\n\nMemcache to Varnish. It also provides tools for RAPID API development\n\nwith JSON/XML support, which allows you to prototype your microservice\n\nin a much shorter timeframe and lower the costs of development.\n\n73\n\nGo to Table of Contents\n\nWhen you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\nshared between your local disk and the disk inside a container. When\n\ndeveloping an application, you usually setup containers to share all\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\neach change. On pre-production and production servers, project ﬁles\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nThe other common change applies to ports. When using Docker for\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nSpryker\n\nSpryker is a “Made in Germany” eCommerce platform created with a\n\nSOA approach with separated Backend (ZED) and Frontend (YVES)\n\napplications. The platform is designed with high throughput and\n\nscalability in mind. It’s not the classic microservices approach - you can\n\nlearn more about Spryker’s founder’s view on that in Appendix 1 to this\n\nbook.\n\nElasticsearch\n\nMobile SDK\n\nYVES Shop front end\n\nKV Storage\n\nREST API\n\nSession Storage\n\nRPC\n\nPayment\n\nMail\n\nBI Business Intelligence\n\nZED Back end\n\nPIM\n\nETL\n\nERP\n\nQueue\n\nMySQL\n\nPostgreSQL\n\nFig. 14: Backend for frontends architecture is about minimizing the number of backend\n\ncalls and optimizing the interfaces to a supported device.\n\nThe Spryker source code is available on Github:\n\nhttps://github.com/spryker. The platform comes with an interesting\n\nlicensing model - per developer seat (not related to revenues, servers\n\netc…).\n\n74\n\nWhen you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\nshared between your local disk and the disk inside a container. When\n\ndeveloping an application, you usually setup containers to share all\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\neach change. On pre-production and production servers, project ﬁles\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nThe other common change applies to ports. When using Docker for\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nDocker has some downsides too. The ﬁrst one you might notice is that it\n\nSpryker\n\nOpen Loyalty\n\nSpryker is a “Made in Germany” eCommerce platform created with a\n\nA loyalty/rewards program that can be easily integrated with eCommerce\n\nSOA approach with separated Backend (ZED) and Frontend (YVES)\n\nand/or POS. It’s interesting because of the CDB module (Central Data\n\napplications. The platform is designed with high throughput and\n\nBase) which is responsible for gathering a 360deg. view of each customer.\n\nscalability in mind. It’s not the classic microservices approach - you can\n\nlearn more about Spryker’s founder’s view on that in Appendix 1 to this\n\nOpen Loyalty leverages the CQRS and Event Sourcing design patterns.\n\nbook.\n\nYou can use it as a headless CRM leveraging a REST API (with JWT based\n\nauthorization).\n\nElasticsearch\n\nWe’ve seen many cases of Open Loyalty being used as CRM and\n\nMobile\n\nmarketing automation.\n\nSDK\n\nYVES\n\nShop\n\nKV Storage\n\nfront end\n\nREST API\n\nSession Storage\n\nCUSTOMER VIEW\n\nADMIN VIEW\n\nON-LINE\n\nINTERNAL\n\nRPC\n\nClient Cockpit\n\nPayment\n\neCommerce\n\nMail\n\nERP\n\nBI\n\nZED\n\neCommerce Cockpit\n\nPIM\n\nBusiness\n\nBack end\n\nIntelligence\n\nETL\n\nERP\n\nCustomer\n\nAdmin\n\nMobile Cockpit*\n\nAdmin Cockpit\n\nOFF-LINE\n\nSaaS\n\nPOS\n\nQueue\n\nMySQL\n\nPostgreSQL\n\nOFF-LINE DATA\n\nMarketing Automation\n\nPOS cockpit\n\nFig. 14: Backend for frontends architecture is about minimizing the number of backend\n\ncalls and optimizing the interfaces to a supported device.\n\nE-MAIL, SMS, PUSH NOTIFICATION\n\nThe Spryker source code is available on Github:\n\nOld Components\n\nhttps://github.com/spryker. The platform comes with an interesting\n\nMerchant\n\nNew Components\n\nlicensing model - per developer seat (not related to revenues, servers\n\n* Mobile app needs to be developed.\n\nFig. 23: Open Loyalty architecture - each application works as separate service.\n\netc…).\n\n75",
      "page_number": 68
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 76-85)",
      "start_page": 76,
      "end_page": 85,
      "detection_method": "topic_boundary",
      "content": "The platform is open source and you can ﬁnd the code on Github\n\n(https://github.com/DivanteLtd/open-loyalty).\n\nMore information: http://openloyalty.io.\n\nTechnologies that empower the microservices architecture\n\nPimcore is an Enterprise Content platform for:\n\nCMS - content management.\n\nPIM - master data management for products.DAM - digital assets\n\nmanagement for attachments, videos and pictures.\n\neCommerce Framework - for building checkout features and managing\n\norders.\n\nThe Pimcore REST API22 can be used to make Pimcore an eCommerce\n\nbackend for Mobile applications or to extend existing eCommerce\n\nplatform catalog capabilities, etc.\n\nIt’s a open source technology developed in Austria with a really active\n\ncommunity and version 5.0 (based on Symfony Framework) on the\n\nhorizon.\n\nMore on Pimcore: http://pimcore.org.\n\n22 https://www.pimcore.org/docs/latest/Web_Services/index.html\n\nGo to Table of Contents\n\n76\n\nThe platform is open source and you can ﬁnd the code on Github\n\nTechnologies that empower the microservices architecture\n\n(https://github.com/DivanteLtd/open-loyalty).\n\nMore information: http://openloyalty.io.\n\nThe microservices architecture introduces new concepts that sometimes\n\nalso require new or different tools compared to the monolithic approach.\n\nAlso, keeping in mind, that this approach may lead to more complexity of\n\nTechnologies that empower the microservices architecture\n\nour platform, we should automate as many things as we can from the\n\nbeginning.\n\nPimcore is an Enterprise Content platform for:\n\nWe’ll show you some of the most widely used tools and technologies that\n\nCMS - content management.\n\ncould empower your development by making things easier, more\n\nautomated and are very suitable when diving into Microservices.\n\nPIM - master data management for products.DAM - digital assets\n\nmanagement for attachments, videos and pictures.\n\nAnsible\n\neCommerce Framework - for building checkout features and managing\n\norders.\n\nDevOps\n\nis an agile way\n\nto maintain software.\n\nIt emphasizes\n\ncommunication between IT and SD23.\n\nThe Pimcore REST API22 can be used to make Pimcore an eCommerce\n\nbackend for Mobile applications or to extend existing eCommerce\n\nAnsible is a tool for automation of DevOps routines. Ansible uses an\n\nplatform catalog capabilities, etc.\n\nagentless architecture which means that no additional software is needed\n\nto be installed on target machines; communication is done by issuing\n\nIt’s a open source technology developed in Austria with a really active\n\nplain SSH commands.\n\nIt automates applications deployment,\n\ncommunity and version 5.0 (based on Symfony Framework) on the\n\nconﬁguration management, workﬂow orchestration and even cloud\n\nhorizon.\n\nprovisioning – all in one tool. Shipping with nearly 200 modules in the\n\ncore distribution, Ansible provides a vast library of building blocks for\n\nMore on Pimcore: http://pimcore.org.\n\nmanaging all kinds of IT tasks.\n\n22 https://www.pimcore.org/docs/latest/Web_Services/index.html\n\n23 https://pl.wikipedia.org/wiki/DevOps\n\n77\n\nGo to Table of Contents\n\nAnsible composes each server (or group of them, named inventory) from\n\nreusable roles. We can deﬁne ours, such as Nginx, PHP or Magento, and\n\nthen reuse them for different machines. Roles are next tied together in\n\n“Playbooks” that describe the full deployment process.\n\nThere’re plenty of well-written, already made Playbooks that you could\n\nadapt and reuse for conﬁguring your infrastructure. As an example, when\n\ninstalling Magento you could use:\n\nhttps://github.com/aslaen/AnsiblePlaybooks/tree/master/ansible-magen\n\nto-lemp.\n\nTo conﬁgure our ﬁrst servers with the Nginx web server and PHP, we\n\nshould ﬁrst create two roles that will be next used in a ﬁnal Playbook.\n\n1. Nginx: # in ./roles/nginx/tasks/main.yml - name: Ensures that nginx is installed apt: name=nginx state=present\n\nname: Creates nginx conﬁguration from Jinja template ﬁle template: src: \"/etc/nginx/nginx.conf.j2\" dest: \"/etc/nginx/nginx.conf\"\n\nGo to Table of Contents\n\n78\n\nAnsible composes each server (or group of them, named inventory) from\n\n2. PHP: # in ./roles/php/tasks/main.yml - name: Ensures that dotdeb APT repository is added apt_repository: repo=\"deb http://packages.dotdeb.org jessie all\" state=present\n\nreusable roles. We can deﬁne ours, such as Nginx, PHP or Magento, and\n\nthen reuse them for different machines. Roles are next tied together in\n\n“Playbooks” that describe the full deployment process.\n\nThere’re plenty of well-written, already made Playbooks that you could\n\nname: Ensures that dotdeb key is present apt_key: url=https://www.dotdeb.org/dotdeb.gpg state=present\n\nadapt and reuse for conﬁguring your infrastructure. As an example, when\n\ninstalling Magento you could use:\n\nhttps://github.com/aslaen/AnsiblePlaybooks/tree/master/ansible-magen\n\nname: Ensures that APT cache is updated apt: update_cache=yes\n\nto-lemp.\n\nname: Ensures that listed packages are installed apt: pkg=\"{{ item }}\" with_items: - php7.0-cli - php7.0-curl - php7.0-fpm\n\nTo conﬁgure our ﬁrst servers with the Nginx web server and PHP, we\n\nshould ﬁrst create two roles that will be next used in a ﬁnal Playbook.\n\n1. Nginx:\n\n# in ./roles/nginx/tasks/main.yml\n\nHaving these roles, we can now deﬁne a playbook that will combine them\n\nname: Ensures that nginx is installed\n\nto set-up our new server with nginx and php installed:\n\napt: name=nginx state=present\n\nname: Creates nginx conﬁguration from Jinja template ﬁle\n\n# in ./php-nodes.yml - hosts: php-nodes roles: - nginx - php\n\ntemplate:\n\nsrc: \"/etc/nginx/nginx.conf.j2\"\n\ndest: \"/etc/nginx/nginx.conf\"\n\n79\n\nGo to Table of Contents\n\nThe last thing we need to do is to tell Ansible the hostnames of our\n\nservers:\n\n# in ./inventory [php-nodes] php-node1.acme.org php-node2.acme.org\n\nDeployment is now as easy as typing a single shell command that will tell\n\nAnsible to run the php-nodes.yml playbook on hosts from the inventory\n\nﬁle as root (-b):\n\n$ ansible-playbook -i inventory php-nodes.yml -b\n\nAs we deﬁned two hosts in a “php-nodes” group, Ansible is smart\n\nenough to run the Playbook concurrently for every server. That way we’re\n\nable to make a deployment on a bigger group of machines at once\n\nwithout wasting time doing it one-by-one.\n\nReactJS\n\nReact is an open source user interface (UI) component library. It was\n\ndeveloped at Facebook to facilitate creation of interactive web interfaces.\n\nIt is often referred to as the V in the “MVC” architecture as it makes no\n\nassumptions about the rest of your technology stack.\n\nGo to Table of Contents\n\n80\n\nWith React, you compose your application out of components. It\n\nThe last thing we need to do is to tell Ansible the hostnames of our\n\nembraces what is called component-based architecture - a declarative\n\nservers:\n\napproach to developing web interfaces where you describe your UI with a\n\ntree of components. React components are highly encapsulated,\n\n# in ./inventory\n\nconcern-speciﬁc, single-purpose blocks. For example, there could be\n\n[php-nodes]\n\nphp-node1.acme.org\n\ncomponents for address or zip code that together create a form. Such\n\nphp-node2.acme.org\n\ncomponents have both a visual representation and dynamic logic.\n\nSome components can even talk to the server on their own, e.g., a form\n\nDeployment is now as easy as typing a single shell command that will tell\n\nthat submits its values to the server and shows conﬁrmation on success.\n\nAnsible to run the php-nodes.yml playbook on hosts from the inventory\n\nSuch interfaces are easier to reuse, refactor, test and maintain. They also\n\nﬁle as root (-b):\n\ntend to be faster than their imperative counterparts as React - being\n\nresponsible\n\nfor rendering your UI on screen - performs many\n\noptimisations and batches updates in one go.\n\n$ ansible-playbook -i inventory php-nodes.yml -b\n\nIt’s most commonly used with Webpack - a module bundler for modern\n\nJavascript. One of its features - code-splitting - allows you to generate\n\nmultiple Javascript bundles (entry points) allowing you to send clients\n\nAs we deﬁned two hosts in a “php-nodes” group, Ansible is smart\n\nonly the part of Javascript that is required to render that particular screen.\n\nenough to run the Playbook concurrently for every server. That way we’re\n\nable to make a deployment on a bigger group of machines at once\n\nOne of the interesting movements in frontend-development nowadays is\n\nwithout wasting time doing it one-by-one.\n\nan Isomorphic approach. Which means that both frontend and backend\n\nare sharing the same code. In this particular case, frontend app can be\n\ncreated in React and backend code run by NodeJS.\n\nReactJS\n\nReact is an open source user interface (UI) component library. It was\n\nNodeJS\n\ndeveloped at Facebook to facilitate creation of interactive web interfaces.\n\nIt is often referred to as the V in the “MVC” architecture as it makes no\n\nNodeJS is a popular (de facto industry standard) JavaScript engine that\n\nassumptions about the rest of your technology stack.\n\n81\n\nGo to Table of Contents\n\ncan be used server-side and in CLI environments. There are plenty of\n\nJavaScript\n\nWeb\n\nframeworks\n\navailable,\n\nlike\n\nExpress\n\n(https://expressjs.com/) and HapiJS (https://hapijs.com/) - to name but\n\ntwo. As NodeJS is built around Google’s V8 JavaScript engine (initially\n\ndeveloped as Chrome/Chromium JS engine) it’s blazingly fast. Node\n\nleverages the events-polling/non-blocking IO architecture to provide\n\nexceptional performance results and optimizes CPU utilization (for more,\n\nread about the c10k problem: http://www.kegel.com/c10k.html).\n\nNode.js Server\n\nRequest\n\nRequest\n\nEvent Loop\n\nPOSIX Async Threads\n\nNon- blocking IO\n\nDelegate\n\nRequests\n\nRequests\n\nSingle Thread\n\nFig. 24: Node.js request ﬂow. Node leverages Event polling and maximizing the memory\n\nand CPU usage on running parallel operations inside single threaded environment.\n\ninteroperate with frontend JS code very easily. There is an emerging trend\n\nof building Universal apps - which more or less means that the same code\n\nbase is in use on the frontend and backend. One of the most interesting\n\nand popular frameworks for building Isomorphic apps is React Js\n\n(https://facebook.github.io/react/).\n\nNodeJS is used as a foundation for many CLI tools - starting from the\n\nmost popular “npm” (Node Package Manager), followed by a number of\n\ntools like Gulp, Yeoman and others.\n\nGo to Table of Contents\n\n82\n\ncan be used server-side and in CLI environments. There are plenty of\n\nJavaScript is the rising star of programming languages. It can even be\n\nJavaScript\n\nWeb\n\nframeworks\n\navailable,\n\nlike\n\nExpress\n\nused for building desktop applications - like Visual Studio Code or Vivaldi\n\n(https://expressjs.com/) and HapiJS (https://hapijs.com/) - to name but\n\nweb browser (!); these tools are coded in 100% pure JavaScript - but for\n\ntwo. As NodeJS is built around Google’s V8 JavaScript engine (initially\n\nthe end users, nothing differs from standard desktop applications. And\n\ndeveloped as Chrome/Chromium JS engine) it’s blazingly fast. Node\n\nthey’re portable between operating systems by default!\n\nleverages the events-polling/non-blocking IO architecture to provide\n\nexceptional performance results and optimizes CPU utilization (for more,\n\nOn the server side, NodeJS is very often used as an API platform because\n\nread about the c10k problem: http://www.kegel.com/c10k.html).\n\nof the platform speed. The event polling architecture is ideal for rapid but\n\nshort-lived requests.\n\nNode.js Server\n\nRequest\n\nUsing “npm” one can install almost all available libraries and tools for the\n\nJS stack - including frontend and backend packages. As most modern\n\nPOSIX\n\nEvent\n\nNon-\n\nAsync\n\nRequest\n\nLoop\n\nlibraries (eg. GraphQL, Websockets) have Node bindings, and all modern\n\nblocking\n\nThreads\n\nIO\n\nDelegate\n\ncloud providers support this technology as well, it’s a good choice for\n\nRequests\n\nbackend technology backing microservices.\n\nSingle\n\nThread\n\nRequests\n\nFamous NodeJS users\n\nFig. 24: Node.js request ﬂow. Node leverages Event polling and maximizing the memory\n\nNode.js helps us solve this (boundary between the browser and server) by enabling both the browser and server applications to be written in JavaScript. It uniﬁes our engineering specialties into one team which allows us to understand and react to our users’ needs at any level in the technology stack.\n\nand CPU usage on running parallel operations inside single threaded environment.\n\ninteroperate with frontend JS code very easily. There is an emerging trend\n\nof building Universal apps - which more or less means that the same code\n\nbase is in use on the frontend and backend. One of the most interesting\n\nand popular frameworks for building Isomorphic apps is React Js\n\n— Jeff Harrel, Senior Director of Payments Products and\n\n(https://facebook.github.io/react/).\n\nEngineering at PayPal24\n\nNodeJS is used as a foundation for many CLI tools - starting from the\n\n24 https://www.paypal-engineering.com/2013/11/22/node-js-at-paypal/\n\nmost popular “npm” (Node Package Manager), followed by a number of\n\ntools like Gulp, Yeoman and others.\n\n83\n\nGo to Table of Contents\n\nLinkedIn\n\nOne reason was scale. The second is, if you look at Node, the thing it’s best at doing is talking to other services.\n\n— Mobile Development Lead, Kiran Prasad25\n\neBay\n\nWe had two primary requirements for the project. First, was to make the application as real time as possible–i.e., maintain live connections with the server. Second, was to orchestrate a that display huge number of eBay-speciﬁc services information on the page–i.e.\n\n— Senthil Padmanabhan, Principal Web Engineer at eBay26\n\nOther projects that leverage NodeJS:\n\nUber\n\nhttps://nodejs.org/static/documents/casestudies/Nodejs-at-Uber.pdf\n\nNetﬂix\n\nhttp://thenewstack.io/netﬂix-uses-node-js-power-user-interface/\n\nGroupon\n\nhttp://www.datacenterknowledge.com/archives/2013/12/06/need-speed-groupon-m\n\nigrated-node-js/\n\n25 http://venturebeat.com/2011/08/16/linkedin-node/\n\n26 http://www.ebaytechblog.com/2013/05/17/how-we-built-ebays-ﬁrst-node-js-application/\n\nGo to Table of Contents\n\n84\n\nSwagger\n\nThis powerful tool is too commonly used only for generating nice-looking\n\ndocumentation for APIs. Basically, swagger is for deﬁning the API\n\ninterfaces using simple, domain-driven JSON language.\n\nThe editor is only one tool from the toolkit but other ones are:\n\nCodegen - for generating the source code scaffold for your API -\n\navailable in many different languages (Node, Ruby, .NET, PHP).\n\nUI - most known swagger tool for generating useful and nice looking\n\ninteractive documentation.\n\nEverything starts with a speciﬁcation ﬁle describing all the Entities and\n\ninterfaces for the REST API. Please take a look at the example below:\n\n{\n\n\"get\": {\n\n\"description\": \"Returns pets based on ID\",\n\n\"summary\": \"Find pets by ID\",\n\n\"operationId\": \"getPetsById\",\n\n\"produces\": [\n\n\"application/json\",\n\n\"text/html\"\n\n],\n\n\"responses\": {\n\n\"200\": {\n\n\"description\": \"pet response\",\n\n\"schema\": {\n\nLinkedIn\n\nSwagger\n\nOne reason was scale. The second is, if you look at Node, the\n\nThis powerful tool is too commonly used only for generating nice-looking\n\nthing it’s best at doing is talking to other services.\n\ndocumentation for APIs. Basically, swagger is for deﬁning the API\n\ninterfaces using simple, domain-driven JSON language.\n\n— Mobile Development Lead, Kiran Prasad25\n\nThe editor is only one tool from the toolkit but other ones are:\n\neBay\n\nCodegen - for generating the source code scaffold for your API -\n\navailable in many different languages (Node, Ruby, .NET, PHP).\n\nWe had two primary requirements for the project. First, was to\n\nmake the application as real time as possible–i.e., maintain\n\nUI - most known swagger tool for generating useful and nice looking\n\nlive connections with the server. Second, was to orchestrate a\n\nhuge number of eBay-speciﬁc services\n\nthat display\n\ninteractive documentation.\n\ninformation on the page–i.e.\n\nEverything starts with a speciﬁcation ﬁle describing all the Entities and\n\n— Senthil Padmanabhan, Principal Web Engineer at eBay26\n\ninterfaces for the REST API. Please take a look at the example below:\n\nOther projects that leverage NodeJS:\n\n{ \"get\": { \"description\": \"Returns pets based on ID\", \"summary\": \"Find pets by ID\", \"operationId\": \"getPetsById\", \"produces\": [ \"application/json\", \"text/html\" ], \"responses\": { \"200\": { \"description\": \"pet response\", \"schema\": {\n\nUber\n\nhttps://nodejs.org/static/documents/casestudies/Nodejs-at-Uber.pdf\n\nNetﬂix\n\nhttp://thenewstack.io/netﬂix-uses-node-js-power-user-interface/\n\nGroupon\n\nhttp://www.datacenterknowledge.com/archives/2013/12/06/need-speed-groupon-m\n\nigrated-node-js/\n\n25 http://venturebeat.com/2011/08/16/linkedin-node/\n\n26 http://www.ebaytechblog.com/2013/05/17/how-we-built-ebays-ﬁrst-node-js-application/\n\n85\n\nGo to Table of Contents",
      "page_number": 76
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 86-93)",
      "start_page": 86,
      "end_page": 93,
      "detection_method": "topic_boundary",
      "content": "\"type\": \"array\", \"items\": { \"$ref\": \"#/deﬁnitions/Pet\" } } },\n\n\"default\": { \"description\": \"error payload\", \"schema\": { \"$ref\": \"#/deﬁnitions/ErrorModel\" } } } }, \"parameters\": [ { \"name\": \"id\", \"in\": \"path\", \"description\": \"ID of pet to use\", \"required\": true, \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"collectionFormat\": \"csv\" } ] }\n\n$ref relates to other entities described in the ﬁle (data models, structures\n\netc). You can use primitives as the examples and return values (bool,\n\nstring…) as well as hash-sets, compound objects and lists. Swagger allows\n\nyou to specify the validation rules and authorization schemes (basic auth,\n\noauth, oauth2).\n\nGo to Table of Contents\n\n86\n\n\"type\": \"array\",\n\n{ \"oauth2\": { \"type\": \"oauth2\", \"scopes\": [ { \"scope\": \"email\", \"description\": \"Access to your email address\" }, { \"scope\": \"pets\", \"description\": \"Access to your pets\" } ], \"grantTypes\": { \"implicit\": { \"loginEndpoint\": {\n\n\"items\": {\n\n\"$ref\": \"#/deﬁnitions/Pet\"\n\n}\n\n}\n\n},\n\n\"default\": {\n\n\"description\": \"error payload\",\n\n\"schema\": {\n\n\"$ref\": \"#/deﬁnitions/ErrorModel\"\n\n}\n\n}\n\n}\n\n},\n\n\"parameters\": [\n\n{\n\n\"url\":\n\n\"name\": \"id\",\n\n\"http://petstore.swagger.wordnik.com/oauth/dialog\" }, \"tokenName\": \"access_token\" }, \"authorization_code\": { \"tokenRequestEndpoint\": {\n\n\"in\": \"path\",\n\n\"description\": \"ID of pet to use\",\n\n\"required\": true,\n\n\"type\": \"array\",\n\n\"items\": {\n\n\"type\": \"string\"\n\n\"url\": \"http://petstore.swagger.wordnik.com/oauth/requestToken\", \"clientIdName\": \"client_id\", \"clientSecretName\": \"client_secret\" }, \"tokenEndpoint\": {\n\n},\n\n\"collectionFormat\": \"csv\"\n\n}\n\n]\n\n}\n\n\"url\":\n\n\"http://petstore.swagger.wordnik.com/oauth/token\", \"tokenName\": \"access_code\" } } } } }\n\n$ref relates to other entities described in the ﬁle (data models, structures\n\netc). You can use primitives as the examples and return values (bool,\n\nstring…) as well as hash-sets, compound objects and lists. Swagger allows\n\nyou to specify the validation rules and authorization schemes (basic auth,\n\noauth, oauth2).\n\n87\n\nGo to Table of Contents\n\nLast but not least swagger the OpenAPI speciﬁcation format has become\n\nmore and more a standard and should be considered when starting new\n\nAPI projects. It’s supported by many external tools and platforms -\n\nincluding Amazon API Gateway27.\n\nFig. 25: Swagger UI generates a nice-looking speciﬁcation for your API along with a\n\n“try-it-out” feature for executing API calls directly from the browser.\n\nElasticsearch\n\nThe simplest way to start with a microservices approach in eCommerce is\n\noften to delegate the search\n\nfeature to an external tool\n\nElasticearch/Solr or to SaaS solutions like Klevu.\n\nElasticsearch is a search engine available via REST API (updates, reads,\n\nsearches…). It can be a micro service for catalog operations in\n\neCommerce and it’s often used to leverage the NoSQL scalability of its\n\ninternal document database over standard SQL solutions.\n\n27 https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\nGo to Table of Contents\n\nlike\n\n88\n\nLast but not least swagger the OpenAPI speciﬁcation format has become\n\nElasticsearch supports full-text search with faceted ﬁltering and support\n\nmore and more a standard and should be considered when starting new\n\nfor most major languages with stemming and misspelling correction\n\nAPI projects. It’s supported by many external tools and platforms -\n\nfeatures.\n\nincluding Amazon API Gateway27.\n\nThere are plenty of modules to Magento and other platforms that\n\nsynchronize product feeds to ES and then provide catalog browsing via\n\nES web-services - without touching the relational database.\n\nElasticsearch is even used for log analysis with tools like Kibana and\n\nLogstash. With its ease of use, performance and scalability characteristics,\n\nit is actually best choice for most eCommerce and content related sites.\n\nElastic is well supported by cloud providers like Amazon and supports\n\nFig. 25: Swagger UI generates a nice-looking speciﬁcation for your API along with a\n\nDocker.\n\n“try-it-out” feature for executing API calls directly from the browser.\n\nElasticsearch\n\nGraphQL\n\nThe simplest way to start with a microservices approach in eCommerce is\n\nModeling a great REST API is hard - using and supporting changes in an\n\noften to delegate the search\n\nfeature to an external tool\n\nlike\n\nAPI over time is sometimes even harder. GraphQL (http://graphql.org) is\n\nElasticearch/Solr or to SaaS solutions like Klevu.\n\na query language; a proposition to a new way of thinking about APIs.\n\nElasticsearch is a search engine available via REST API (updates, reads,\n\nWidely used REST APIs are organized around HTTP endpoints. GraphQL\n\nsearches…). It can be a micro service for catalog operations in\n\nAPIs are different; they are built in terms of types and ﬁelds, and relations\n\neCommerce and it’s often used to leverage the NoSQL scalability of its\n\nbetween them. It gives clients the ability to ask for what they need directly\n\ninternal document database over standard SQL solutions.\n\ninstead of many different REST requests. All the necessary data will be\n\nqueried and returned with a single call.\n\n27 https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\n89\n\nGo to Table of Contents\n\nData deﬁnition:\n\ntype Project { name: String tagline: String contributors: [User] }\n\nSample query:\n\n{ project(name: \"GraphQL\") { tagline } }\n\nQuery result:\n\n{ \"project\": { \"tagline\": \"A query language for APIs\" } }\n\nGraphQL was developed\n\ninternally by Facebook\n\nin 2012 and\n\nopen-sourced 3 years later with Relay, a JavaScript framework for building\n\ndata-driven React applications. Nowadays, the GraphQL ecosystem is\n\ngrowing rapidly; both server and frontend libraries are available for many\n\nprogramming languages and developers have dedicated tools for Graph-\n\nQL API design. Many other organizations, including Github, Pinterest and\n\nShopify are adopting GraphQL because of its beneﬁts.\n\nGo to Table of Contents\n\n90\n\nDistributed logging and monitoring\n\nData deﬁnition:\n\ntype Project {\n\nDistributed systems require new levels of application monitoring and\n\nname: String\n\nlogging. With monolithic applications you can track one log-ﬁle for events\n\ntagline: String\n\n(usually) and use some Zabbix triggers to get a complete view of a\n\ncontributors: [User]\n\nserver's state, application errors, etc.\n\n}\n\nSample query:\n\nGraylog\n\n{\n\nWith distributed services you have to track a whole bunch of new metrics:\n\nproject(name: \"GraphQL\") {\n\ntagline\n\n}\n\nNetwork latency - which can ruin the communication between crucial\n\n}\n\nparts.\n\nApplication errors on the service level and violation of service-contracts.\n\nQuery result:\n\nPerformance metrics.\n\nSecurity violations.\n\n{\n\n\"project\": {\n\nTo make it even worse, you must track all those parameters across several\n\n\"tagline\": \"A query language for APIs\"\n\nclusters in real time. Without such a level of monitoring, no high\n\n}\n\navailability can be achieved and the distributed system is even more\n\n}\n\nvulnerable to downtime than a single monolithic application.\n\nGraphQL was developed\n\ninternally by Facebook\n\nin 2012 and\n\nThe good news is that nowadays there are plenty of tools to measure\n\nopen-sourced 3 years later with Relay, a JavaScript framework for building\n\nweb-app performance and availability. One of the most interesting is\n\ndata-driven React applications. Nowadays, the GraphQL ecosystem is\n\nGraylog (http://graylog.org).\n\ngrowing rapidly; both server and frontend libraries are available for many\n\nprogramming languages and developers have dedicated tools for Graph-\n\nUsed by many microservice predecessors like LinkedIn, eBay, and Twilio,\n\nQL API design. Many other organizations, including Github, Pinterest and\n\nGraylog centralizes logs into streams.\n\nShopify are adopting GraphQL because of its beneﬁts.\n\n91\n\nGo to Table of Contents\n\nFig. 26: In graylog you’ve got access to messages in real time with alerts conﬁgured for\n\neach separate message stream.\n\nGraylog is easy to integrate, leveraging HTTP communication, syslog\n\n(with UDP support for minimum network load) or third party log collectors\n\nlike ﬂuentd29. It can be integrated with e-mail, SMS, and Slack alerts.\n\nFig. 27: Alerts conﬁguration is a basic feature for providing HA to your microservices\n\necosystem.\n\n29 http://www.ﬂuentd.org/\n\nGo to Table of Contents\n\n92\n\nDistributed systems require new levels of application monitoring and\n\nlogging. With monolithic applications you can track one log-ﬁle for events\n\n(usually) and use some Zabbix triggers to get a complete view of a\n\nserver's state, application errors, etc.\n\nNew Relic\n\nWhereas Graylog is focused around application logging, New Relic is\n\ncentered around the performance and numeric metrics of your\n\napplications and servers: network response times, CPU load, HTTP\n\nresponse times, network graphs, as well as application stack traces with\n\nFig. 26: In graylog you’ve got access to messages in real time with alerts conﬁgured for\n\neach separate message stream.\n\ndebugging information.\n\nGraylog is easy to integrate, leveraging HTTP communication, syslog\n\n(with UDP support for minimum network load) or third party log collectors\n\nNew Relic works as a system daemon with native libraries for many\n\nlike ﬂuentd29. It can be integrated with e-mail, SMS, and Slack alerts.\n\nprogramming languages and servers (PHP, NodeJS…). Therefore, it can\n\nbe used even in production where most other debugging tools come with\n\ntoo much signiﬁcant overhead. We used to work with New Relic on\n\nproduction clusters - even with applications with millions of unique users\n\nper month and dozens of servers in a cluster.\n\nWe used to implement our own custom metrics to monitor response\n\ntimes from 3rd party services and integrations. Similarly to Graylog, New\n\nRelic can set up dashboards and alerts.\n\nFig. 27: Alerts conﬁguration is a basic feature for providing HA to your microservices\n\necosystem.\n\n29 http://www.ﬂuentd.org/\n\n93\n\nGo to Table of Contents",
      "page_number": 86
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 94-101)",
      "start_page": 94,
      "end_page": 101,
      "detection_method": "topic_boundary",
      "content": "Fig. 28: The coolest feature of New Relic is stack-trace access - on production, in real time.\n\nGo to Table of Contents\n\n94\n\nNew Relic Insights\n\nData visualization tools and customizable dashboards, allow you to\n\nobserve business analytics data and performance information at the same\n\ntime.\n\nBy combining application, environment and business data - like\n\ntransactions, pageviews and order details - into one reporting tool, you\n\ncan more precisely see how your app performance affects your business.\n\nFig. 28: The coolest feature of New Relic is stack-trace access - on production, in real time.\n\nFig. 29: New Relic Insights Data Explorer with sample plot.\n\n95\n\nGo to Table of Contents\n\nNew Relic Insights NRQL Language\n\nYou can also use the NRQL (New Relic Query Language) with syntax\n\nsimilar to SQL language to explore all collected data and create\n\napplication metric reports.\n\nFor example, you can attach customer group IDs to order requests to\n\ncheck if particular customer groups have an unusually bad experience\n\nduring the order process.\n\nFig. 30: New Relic usage of NRQL with sample output.\n\nGo to Table of Contents\n\n96\n\nNew Relic Insights NRQL Language\n\nTake care of the front-end using New Relic Browser\n\nYou can also use the NRQL (New Relic Query Language) with syntax\n\nAnother powerful feature allows you to easily detect any javascript issue\n\nsimilar to SQL language to explore all collected data and create\n\non the front-end of your application. Additionally, New Relic will show you\n\napplication metric reports.\n\na detailed stack trace and execution proﬁle.\n\nFor example, you can attach customer group IDs to order requests to\n\ncheck if particular customer groups have an unusually bad experience\n\nduring the order process.\n\nFig. 31: The New Relic Browser module displays a list of javascript issues on front-end\n\napplication.\n\nFig. 30: New Relic usage of NRQL with sample output.\n\n97\n\nGo to Table of Contents\n\nCase Studies:\n\nRe-architecting the monolith\n\nGo to Table of Contents\n\n98\n\nCase Studies: Re-architecting the Monolith\n\nHere I’ll brieﬂy present two case studies of the microservices evolution\n\nwhich I’ve been able to observe while working at Divante.\n\nB2B\n\nOne of our B2B clients came to us with the following issues to be\n\nsolved:\n\nCase Studies:\n\nWhile on Magento 1 with SKUs catalog exceeding 1M products -\n\nperformance bottlenecks relating to catalog and catalog updates\n\nRe-architecting\n\nbecame hard to work-around.\n\nthe monolith\n\nMonolithic architecture, strongly tied to external systems (such as CRM,\n\nERP, WMS) hindered changes and development of new features.\n\nCRM that became the SPoF (Single Point of Failure). Pivotal CRM was in\n\ncharge of too many key responsibilities including per-customer pricing,\n\ncart management and promotions.\n\nSerious amount of technological debt due to legacy code.\n\nScalability problems - the platform should be able to handle a new\n\nbusiness model that requires broadening the offer and entering new\n\nmarkets.\n\nThe online platform was generating 100M+ EUR revenue/year at the\n\ntime. The challenge was a serious one.\n\n99\n\nGo to Table of Contents\n\nThe architecture of this system resembled a \"death star\". However, its\n\ncomplexity was not between microservices, but between external\n\nsystems.\n\nThe ﬁrst instinct was to move the site 1:1 from legacy Magento 1 to a new\n\nplatform. OroCommerce and Magento 2 were considered.\n\nThe work on collecting business requirements from stakeholders inside\n\nthe company and putting them into the Business Requirements\n\nDocument (BRD) was quickly started. We formulated nearly 1,000\n\nbusiness requirements.Then we mapped them into features. Finally, we\n\nscored each available platform on its ability to meet the requirements:\n\nFunctionality available out of the box.\n\nFunctionality after customization.\n\nFunctionality requiring additional/external modules.\n\nNew features.\n\nWe double-checked both platforms in terms of technical solutions,\n\nscalability, performance, possibility of modiﬁcation and the possibility of\n\nfurther development.\n\nDuring the analysis, we realized that it would be somewhat risky to collect\n\nall the requirements for such a huge platform right away. We felt that\n\nbefore we had ﬁnished analyzing the requirements, they would have\n\nchanged a few times already. Brief research showed us that none of the\n\nsystems were capable of meeting all the speciﬁc requirements, both\n\nfunctional and non-functional. We realized this was not the right approach\n\nand could lead us back to where we started - a monolithic application. 100\n\nThe architecture of this system resembled a \"death star\". However, its\n\nBefore you decide to take a similar step (to go along with a ready-made\n\ncomplexity was not between microservices, but between external\n\nplatform in the center of a microservices eco-system), look at the pros &\n\nsystems.\n\ncons of this approach.\n\nThe ﬁrst instinct was to move the site 1:1 from legacy Magento 1 to a new\n\nPros & Cons of choosing an end-2-end platform:\n\nplatform. OroCommerce and Magento 2 were considered.\n\nPros:\n\nThe work on collecting business requirements from stakeholders inside\n\nthe company and putting them into the Business Requirements\n\nRapid development and time to market.\n\nDocument (BRD) was quickly started. We formulated nearly 1,000\n\nbusiness requirements.Then we mapped them into features. Finally, we\n\nIt’s usually a stable, well-tested product.\n\nscored each available platform on its ability to meet the requirements:\n\nA community that will help in solving many problems.\n\nFunctionality available out of the box.\n\nThe possibility to use a large base of ready-made, fully-featured\n\nFunctionality after customization.\n\nmodules (Magento Marketplace).\n\nFunctionality requiring additional/external modules.\n\nOfﬁcial support from the software provider.\n\nNew features.\n\nOfﬁcial updates, security patches.\n\nWe double-checked both platforms in terms of technical solutions,\n\nscalability, performance, possibility of modiﬁcation and the possibility of\n\nCons:\n\nfurther development.\n\nIt’s still a monolithic application that sooner or later will lead us to the\n\nDuring the analysis, we realized that it would be somewhat risky to collect\n\nstarting point - problems with scalability and maintenance.\n\nall the requirements for such a huge platform right away. We felt that\n\nbefore we had ﬁnished analyzing the requirements, they would have\n\nVery high licensing costs for the Enterprise version.\n\nchanged a few times already. Brief research showed us that none of the\n\nsystems were capable of meeting all the speciﬁc requirements, both\n\nfunctional and non-functional. We realized this was not the right approach\n\nand could lead us back to where we started - a monolithic application.\n\n101\n\nGo to Table of Contents",
      "page_number": 94
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 102-109)",
      "start_page": 102,
      "end_page": 109,
      "detection_method": "topic_boundary",
      "content": "Large platforms require specialists with speciﬁc skills for a particular\n\nsystem who can be difﬁcult to acquire.\n\nReady-made functionalities often requires serious modiﬁcations to fully\n\nadapt them, which can lead to incompatibility with the base system - no\n\nupdates or patches.\n\nThey often provide outdated solutions, limiting the introduction of\n\nmodern technologies.\n\nA New approach\n\nEventually, after conducting a feasibility study, we suggested that our\n\nclient use a more optimal way of solving the problem.\n\nThe fundamental assumption was to abandon migration to a new\n\nplatform and change the architecture by deconstructing the current\n\nsystem and deploying it as an eco-system of distributed microservices. In\n\norder to succeed, we needed an effective analysis and implementation\n\nprocess.\n\nGo to Table of Contents\n\n102\n\nLarge platforms require specialists with speciﬁc skills for a particular\n\nSystem architecture analysis (decomposition)\n\nMicroservice X architecture analysis\n\nMicroservice Y architecture analysis\n\nMicroservice Z architecture analysis\n\nIT Architects team\n\n...\n\nsystem who can be difﬁcult to acquire.\n\nReady-made functionalities often requires serious modiﬁcations to fully\n\nadapt them, which can lead to incompatibility with the base system - no\n\nMicroservices implementation team\n\nMicroservice X implementation\n\nMicroservice Y implementation\n\nMicroservice Z implementation\n\n...\n\nupdates or patches.\n\nThey often provide outdated solutions, limiting the introduction of\n\nMicroservice X adoption on Magento\n\nMagento developers team\n\n...\n\nmodern technologies.\n\nFig. 7: Agile analysis and implementation process to achieve goals.\n\nA New approach\n\nThe ﬁrst step of the \"architecture analysis\" process was the development\n\nof a high-level architecture of the entire system by a team of architects,\n\nEventually, after conducting a feasibility study, we suggested that our\n\nfocused on service responsibilities. The results of their work included:\n\nclient use a more optimal way of solving the problem.\n\nKey business processes supported by the system.\n\nThe fundamental assumption was to abandon migration to a new\n\nGoals and requirements for scalability, security, performance, SLA and\n\nplatform and change the architecture by deconstructing the current\n\npotential development directions.\n\nsystem and deploying it as an eco-system of distributed microservices. In\n\nIdentiﬁed risks.\n\norder to succeed, we needed an effective analysis and implementation\n\nBlock diagram of disclosed microservices:\n\nprocess.\n\nDeﬁned scope and responsibility of each service.\n\nPaReveal patterns of integration between services, taking into\n\naccount emergency situations handling, avoiding SPoF.\n\nDeﬁned events and business objects.\n\nHigh-level architecture diagram of the system.\n\nThe architects worked together along with the client. The client’s domain\n\nexperts were engaged in session-based workshops using the event\n\n103\n\nGo to Table of Contents\n\nstorming technique borrowed from the popular Domain Driven Design\n\n(DDD) domain modeling approach. You can ﬁnd more information on the\n\ntechnique on its creator’s blog:\n\nhttp://ziobrando.blogspot.com/2013/11/introducing-event-storming.html.\n\nBased on the collected data, the team provided the implementation team\n\nwith complete documentation.\n\nAfter several workshops, a distributed architecture with dedicated main\n\nareas/services was created with the following key services deﬁned:\n\nPRICING - managing individual prices and promotions for clients.\n\nPIM (product information management) - responsible for product\n\ninformation and attributes; with planned 1mln+ SKUs it must be\n\nimplemented as a scalable, probably NoSQL based data warehouse.\n\nWMS (warehouse management system) - product stock management.\n\nCRM (customer relationship management) - in charge of syncing data\n\nwith Pivotal CRM (orders, statuses, shopping carts …).\n\nREPORT - reporting and monitoring features.\n\nNOTIFY - user notiﬁcations and alerts management.\n\nREVIEW - product reviews system.\n\nRECOMMENDATIONS - recommendations engine.\n\nFRONTEND APP - in the ﬁrst version - the good, old Magento1; then\n\nit was planned to move this layer to a ReactJS + NodeJS thin client.\n\nMOBILE APP.\n\nWe started with a 20 page architecture document and then created a list\n\nof standards for coding each separate service.\n\nGo to Table of Contents\n\n104\n\nstorming technique borrowed from the popular Domain Driven Design\n\nWe\n\ntried\n\nto\n\nleverage\n\nthe HTTP protocol standards, providing\n\n(DDD) domain modeling approach. You can ﬁnd more information on the\n\ndocumentation and technical requirements, such as speciﬁc frameworks\n\ntechnique on its creator’s blog:\n\nand database servers to be used. It’s very important to make use of such\n\nsynthetic and consistent standards while dealing with distributed\n\nhttp://ziobrando.blogspot.com/2013/11/introducing-event-storming.html.\n\nsoftware.\n\nBased on the collected data, the team provided the implementation team\n\nWe decided to start by implementing the ﬁrst service that is critical for the\n\nwith complete documentation.\n\nsystem due to its SPoF and which would give us the best performance\n\nresults: PRICING and PIM.\n\nAfter several workshops, a distributed architecture with dedicated main\n\nareas/services was created with the following key services deﬁned:\n\nIt was crucial to ﬁgure out how to separate the platform from Pivotal CRM\n\nfor calculating end-client product prices and therefore to avoid a SPoF\n\nPRICING - managing individual prices and promotions for clients.\n\nand maintain High Availability (initially the platform used real-time\n\nPIM (product information management) - responsible for product\n\nWebService calls to get the prices from the CRM when users entered the\n\ninformation and attributes; with planned 1mln+ SKUs it must be\n\npage).\n\nimplemented as a scalable, probably NoSQL based data warehouse.\n\nWMS (warehouse management system) - product stock management.\n\nPIM was selected to solve problems with growing the SKUs database by\n\nCRM (customer relationship management) - in charge of syncing data\n\nmoving to an ElasticSearch NoSQL solution instead of Magento’s EAV\n\nwith Pivotal CRM (orders, statuses, shopping carts …).\n\nmodel.\n\nREPORT - reporting and monitoring features.\n\nNOTIFY - user notiﬁcations and alerts management.\n\nWe created these services as separate Symfony3 applications that were\n\nREVIEW - product reviews system.\n\nintegrated with the Magento1 frontend later on.\n\nRECOMMENDATIONS - recommendations engine.\n\nFRONTEND APP - in the ﬁrst version - the good, old Magento1; then\n\nRoughly speaking - we just removed the Magento1 modules responsible\n\nit was planned to move this layer to a ReactJS + NodeJS thin client.\n\nfor the catalog and wrote our own which called the micro-services instead\n\nMOBILE APP.\n\nof hitting the database.\n\nWe started with a 20 page architecture document and then created a list\n\nThen we followed this path further, by rewriting and exchanging\n\nof standards for coding each separate service.\n\nmonolithic modules with distributed services one by one.\n\n105\n\nGo to Table of Contents\n\nThe project was ﬁnished with a roughly cut-down Magento (serving only\n\nas an application frontend) and 9 services supporting all the business\n\nlogic. One day, if needed, we can simply move on from Magento,\n\nimplementing a new frontend using a ReactJS/NodeJS stack or any other\n\nmodern tech stack.\n\nBeginning\n\nIT Systems\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nESB\n\n...\n\n...\n\nMAGENTO\n\nGo to Table of Contents\n\n106\n\nStep 1\n\nThe project was ﬁnished with a roughly cut-down Magento (serving only\n\nas an application frontend) and 9 services supporting all the business\n\nlogic. One day, if needed, we can simply move on from Magento,\n\nimplementing a new frontend using a ReactJS/NodeJS stack or any other\n\nIT Legacy systems\n\nmodern tech stack.\n\nBeginning\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nIT Systems\n\n...\n\n...\n\nESB\n\nWMS\n\nERP\n\nCRM\n\nPIM\n\n...\n\n...\n\nESB\n\nMicro Services\n\nMAGENTO\n\nPRICE\n\nPIM\n\nMAGENTO\n\nWMS\n\n...\n\n107\n\nGo to Table of Contents\n\nStep 2\n\nIT Systems\n\nIT Legacy systems\n\nXYZ Client\n\nERP\n\nPIM\n\nWMS\n\n...\n\nAPI Gateway\n\nMicro Services\n\nPRICE\n\nWMS\n\nPIM\n\nCRM\n\nREPORT\n\nNOTIFY\n\nOMS\n\nREVIEW\n\nRECOMMENDATION\n\nr e k o r B e g a s s e M\n\n...\n\nMagento Frontend Application\n\nMobile App\n\nFig. 8: Evolutionary (notrevolutionary) steps to create a new platform from a monolithic\n\napplication.\n\nGo to Table of Contents\n\nt n e\n\ni l\n\nC Z Y X\n\n108\n\nStep 2\n\nEach service was designed with its own denormalized database\n\n(ElasticSearch or PerconaDB for relational data orders) and was designed\n\nwith high availability in mind. Data between services is exchanged via a\n\nRabbitMQ data bus using an Event Driven Data Management approach4.\n\nIT Systems\n\nIT Legacy systems\n\nXYZ Client\n\nt\n\nn\n\ne\n\ni\n\nl\n\nC\n\nZ\n\nERP\n\nPIM\n\n...\n\nWMS\n\nY\n\nX\n\nWe haven’t decided (at this point) to go with any technology other than\n\nPHP, so all services were implemented using the Symfony framework;\n\nmostly for simplicity, as well as cost optimization of the development\n\nprocess.\n\nAPI Gateway\n\nYou can ﬁnd more great technologies that focus on microservices later in\n\nMicro Services\n\nthis book and at https://github.com/mfornos/awesome-microservices.\n\nr\n\ne\n\nk\n\nPRICE\n\nCRM\n\nOMS\n\no\n\nr\n\nB\n\ne\n\nPIM\n\nNOTIFY\n\nRECOMMENDATION\n\ng\n\nTo sum-up our challenge please ﬁnd our notes on the pros and cons of\n\na\n\ns\n\ns\n\ne\n\nWMS\n\nREPORT\n\nREVIEW\n\nM\n\nthe microservice approach below:\n\n...\n\nPros:\n\nSmall teams can work in parallel to create new, and maintain current,\n\nservices. Many of you have probably experienced problems with working\n\nMagento Frontend Application\n\nMobile App\n\nin large teams, as we did.\n\nFig. 8: Evolutionary (notrevolutionary) steps to create a new platform from a monolithic\n\napplication.\n\nThe possibility of using heterogeneous technologies - ElasticSearch for\n\nproducts, PerconaDB for orders.\n\nIncreased critical fault-toleranceby using bulkheads/service contracts.\n\nIncremental replacement of legacy code and original systems with new,\n\neffective solutions.\n\n4 https://www.nginx.com/blog/event-driven-data-management-microservices/\n\n109",
      "page_number": 102
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 110-117)",
      "start_page": 110,
      "end_page": 117,
      "detection_method": "topic_boundary",
      "content": "Scalability - we can scale only the services that require it.\n\nProgrammers have a lot of fun, so it’s quite easy to keep the team\n\nmotivated.\n\nCons:\n\nExtensive client involvement is required during the BA phase.\n\nNew skills and quite a lot of architectural experience is required from\n\ndevelopers and architects to design the initial phases.\n\nNew challenges in maintaining the monitoring of the entire\n\ninfrastructure.\n\nMobile Commerce\n\nOne of the coolest features of the microservices architecture is that you’re\n\nno longer bound to your one-and-only platform. It’s crucial, particularly\n\nwhen the application at hand has to meet different expectations. In our\n\ncase - an eCommerce platform with dental equipment - we have three\n\ndifferent areas to be covered:\n\nState-of-the-art content management system with e-learning features.\n\nBasic eCommerce features - checkout and promotions for ordering\n\ndental equipment. CRM features, user proﬁles and segmentation for\n\ntracking all the users.\n\nThe platform was designed to work on mobile devices only.\n\nGo to Table of Contents\n\n110\n\nScalability - we can scale only the services that require it.\n\nAt the start we considered whether or not to use one platform for the\n\nbackend, or maybe to write dedicated solutions. It’s hard to ﬁnd software\n\nwith enterprise\n\nlevel CMS, PIM, CRM and eCommerce features\n\nProgrammers have a lot of fun, so it’s quite easy to keep the team\n\naltogether.\n\nmotivated.\n\nTherefore we decided to go with the following software products:\n\nCons:\n\nPimcore - as a CMS and PIM; we created all the content (e-learning,\n\nExtensive client involvement is required during the BA phase.\n\n5\n\nstatic pages, product content) in Pimcore and expose it via API.\n\nNew skills and quite a lot of architectural experience is required from\n\nMagento2 - as a checkout and for eCommerce features.\n\ndevelopers and architects to design the initial phases.\n\nDedicated iOS and Android apps for the frontend.\n\nNew challenges in maintaining the monitoring of the entire\n\ninfrastructure.\n\nWe used the “Backend for Frontends” approach described in this eBook\n\nto provide optimized API gateways for both mobile applications and the\n\nMobile Commerce\n\nRWD website. Key areas like product content and e-learning pages were\n\nfully manageable in Pimcore and provided the end client with HTML\n\nOne of the coolest features of the microservices architecture is that you’re\n\nrenderings.\n\nno longer bound to your one-and-only platform. It’s crucial, particularly\n\nwhen the application at hand has to meet different expectations. In our\n\nMagento checkout was integrated using API REST calls for placing orders.\n\ncase - an eCommerce platform with dental equipment - we have three\n\ndifferent areas to be covered:\n\nNowadays, all new open source products (and of course, not just\n\nopen-source) expose most of their features via API. It’s cool to focus on\n\nState-of-the-art content management system with e-learning features.\n\nthe end client’s value (frontend) and not reinvent the wheel on the\n\nBasic eCommerce features - checkout and promotions for ordering\n\nbackend.\n\ndental equipment. CRM features, user proﬁles and segmentation for\n\ntracking all the users.\n\nWe did almost no custom development work on the backends!\n\nThe platform was designed to work on mobile devices only.\n\n5 http://pimcore.org - Enterprise grade Content Management platform, PIM and DAM\n\n111\n\nGo to Table of Contents\n\nAppendix 1: Microservices and unicycling by Alexander Graf\n\nThanks to Alexander Graf, the founder of Spryker.com for this part.\n\nInitially published\n\nas\n\na blog post on Alexanders’ blog:\n\nhttps://tech.spryker.com/microservices-and-unicycling-9ed452998b77.\n\nAfter the unspeakable NoSQL hype of about two years ago had reached\n\nits peak “Why are you still working with relational databases?”, the topic\n\nof microservices was brought to the fore in discussions about back-end\n\ntechnologies. In addition, with React, Node & Co., the front-enders have\n\ndeveloped quite a unique little game that, it seems, nobody else can see\n\nthrough. After about two years of Spryker, I have had the pleasure of\n\nbeing able to follow these technical discussions ﬁrst-hand. During my\n\ntime with the mail order giant Otto Group, there was another quite clearly\n\ndeﬁned technical boogeyman — the so-called Host System, or the AS400\n\nmachines, which were in use by all the main retailers. Not maintainable,\n\nancient, full of spaghetti code, everything depended on it, everything\n\nwould be better if we could be rid of it and so on and so forth — so I was\n\ntold. On the other side were the business clowns — I’m one, too — for\n\nwhom technology was just a means to an end. Back then, I thought those\n\nwho worked in IT were the real hard workers, pragmatic thinkers, who only\n\nanswered to the system and whose main goal was to achieve a high level\n\nof maintainability. Among business people there were, and there still are,\n\nthose I thought only busied themselves with absurd strategies and who\n\nbanged on about omnichannel, multi-channel, target group shops and\n\nthe like. Over the last eight years of Kassenzone.de, these strategies were\n\nalways my self-declared ﬁnal boss. It was my ultimate aim to disprove\n\nthem and demonstrate new approaches.\n\nGo to Table of Contents\n\n112\n\nTo my great disappointment, I have come to realize that people in IT — or\n\nAppendix 1: Microservices and unicycling by\n\n‘developers’ as they are called today — work with the same thought\n\nAlexander Graf\n\nprocesses as the business clowns. There is an extremely high tendency to\n\nchase after trends and basic technical problems are not sufﬁciently\n\nThanks to Alexander Graf, the founder of Spryker.com for this part.\n\nanalyzed, nor are they taken seriously enough. Microservices is a\n\nInitially published\n\nas\n\na blog post on Alexanders’ blog:\n\nwonderful example of this. It is neither an IT strategy, nor is it an\n\nhttps://tech.spryker.com/microservices-and-unicycling-9ed452998b77.\n\narchitecture pattern. At most, it describes just a type of IT and system\n\norganization. Just\n\nlike omnichannel. Omnichannel doesn’t mean\n\nAfter the unspeakable NoSQL hype of about two years ago had reached\n\nanything. It’s a cliché that is pretty much just ﬁlled with “blurb” and the\n\nits peak “Why are you still working with relational databases?”, the topic\n\nsame way of thinking is apparent on the topic of microservices. From the\n\nof microservices was brought to the fore in discussions about back-end\n\noutside, omnichannel can be seen as the result of strong growth if a\n\ntechnologies. In addition, with React, Node & Co., the front-enders have\n\ncompany’s range of services can, therefore, cause it to become a leader in\n\ndeveloped quite a unique little game that, it seems, nobody else can see\n\nmany channels. This is exactly what happens with microservices, which\n\nthrough. After about two years of Spryker, I have had the pleasure of\n\nmay be the result of strong growth in IT, because you have to divide large\n\nbeing able to follow these technical discussions ﬁrst-hand. During my\n\napplications into services so that you don’t have too many developers\n\ntime with the mail order giant Otto Group, there was another quite clearly\n\nworking on them at the same time. But this is far cry from being an IT\n\ndeﬁned technical boogeyman — the so-called Host System, or the AS400\n\nstrategy. In many conversations at the code.talks conference, this\n\nmachines, which were in use by all the main retailers. Not maintainable,\n\nimpression was (unfortunately) conﬁrmed. Yoav Kutner (founder of\n\nancient, full of spaghetti code, everything depended on it, everything\n\nMagento1) cut his teeth on the rollout of the ﬁrst of the big Magento1\n\nwould be better if we could be rid of it and so on and so forth — so I was\n\nprojects and reports with a shake of the head that developers always\n\ntold. On the other side were the business clowns — I’m one, too — for\n\nfollow the next hype without having considered where the real problem\n\nwhom technology was just a means to an end. Back then, I thought those\n\nlies. Yes, I know that that sounds all very general, but let’s have a closer\n\nwho worked in IT were the real hard workers, pragmatic thinkers, who only\n\nlook at the topic of microservices.\n\nanswered to the system and whose main goal was to achieve a high level\n\nof maintainability. Among business people there were, and there still are,\n\nMartin Fowler, IT guru and champion of microservices has written dozens\n\nthose I thought only busied themselves with absurd strategies and who\n\nof articles on the subject and describes microservices as follows:\n\nbanged on about omnichannel, multi-channel, target group shops and\n\nthe like. Over the last eight years of Kassenzone.de, these strategies were\n\nalways my self-declared ﬁnal boss. It was my ultimate aim to disprove\n\nthem and demonstrate new approaches.\n\n113\n\nGo to Table of Contents\n\nIn short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and are inde- pendently deployable by fully automated deployment ma- chinery. There is a bare minimum of centralized manage- ment of these services, which may be written in different programming languages and use different data storage technologies.\n\nThis does sound quite promising and it can also help with the\n\ncorresponding problems. Otto’s IT team has already reached the\n\nChampions League where this is concerned and produced the obligatory\n\narticle, called “On Monoliths and Microservices” on the subject. Guido\n\nfrom Otto also referred to this topic at the code.talks event:\n\nWhen we began the development of our new Online Shop otto.de, we chose a distributed, vertical-style architecture at an early stage of the process. Our experience with our previ- ous system showed us that a monolithic architecture does not satisfy the constantly emerging requirements. Growing volumes of data, increasing loads and the need to scale the organization, all of these forced us to rethink our approach.\n\nThere are also other examples which beneﬁt excellently from this\n\napproach. Zalando is an example of a company which is open about using\n\nit in “From Jimmy to Microservice”. The approach can also crop up for\n\nquickly growing tech teams, such as that of Siroop.\n\nGo to Table of Contents\n\n114\n\nIn short, the microservice architectural style is an approach to\n\nWhat’s often forgotten when people sing its praises are the costs\n\ndeveloping a single application as a suite of small services,\n\nassociated with such an approach. Martin Fowler calls these costs the\n\neach running in its own process and communicating with\n\nMicroservice Premium and clearly warns against proceeding in this\n\nlightweight mechanisms, often an HTTP resource API. These\n\ndirection without caution:\n\nservices are built around business capabilities and are inde-\n\npendently deployable by fully automated deployment ma-\n\nchinery. There is a bare minimum of centralized manage-\n\nThe microservices approach is all about handling a complex system, but in order to do so the approach introduces its own set of complexities. When you use microservices you have to work on automated deployment, monitoring, deal- ing with failure, eventual consistency, and other factors that a distributed system introduces. There are well-known ways to cope with all this, but it’s extra effort, and nobody I know in software development seems to have acres of free time. So my primary guideline would be don’t even consider micros- ervices unless you have a system that’s too complex to manage as a monolith.\n\nment of these services, which may be written in different\n\nprogramming languages and use different data storage\n\ntechnologies.\n\nThis does sound quite promising and it can also help with the\n\ncorresponding problems. Otto’s IT team has already reached the\n\nChampions League where this is concerned and produced the obligatory\n\narticle, called “On Monoliths and Microservices” on the subject. Guido\n\nfrom Otto also referred to this topic at the code.talks event:\n\nWhen we began the development of our new Online Shop\n\notto.de, we chose a distributed, vertical-style architecture at\n\nan early stage of the process. Our experience with our previ-\n\nous system showed us that a monolithic architecture does\n\nnot satisfy the constantly emerging requirements. Growing\n\nvolumes of data, increasing loads and the need to scale the\n\norganization, all of these forced us to rethink our approach.\n\nThere are also other examples which beneﬁt excellently from this\n\napproach. Zalando is an example of a company which is open about using\n\nit in “From Jimmy to Microservice”. The approach can also crop up for\n\nquickly growing tech teams, such as that of Siroop.\n\nFig. 29: Image: http://martinfowler.com/.\n\n115\n\nGo to Table of Contents\n\nTechnically speaking, this restriction has many different origins. Whether\n\nit’s the latencies which must be called up for one procedure due to dozens\n\nof active services, clearly difﬁcult debugging, the demanding hardware\n\nsetup or the complex data retention (each service has its own database).\n\nThe fundamentally hard to manage technology zoo notwithstanding.\n\nHere, excellent parallels to eCommerce organizations can be drawn. Who\n\nis quicker and more effective in the development and scaling of new\n\nmodels:\n\n1. a company with dozens of departments and directorates, which must\n\nbe in a permanent state of agreement, but which are extremely good in\n\neach of their individual disciplines;\n\n2. or a company at which up to 100 employees sit in one room, all\n\nknowing what’s going on and all talking to each other.\n\nThe second example is quicker, that goes without saying. With larger\n\nmodels, at which scaling is a rather uniform approach, the ﬁrst example is\n\nbetter. Not much is different in the case of microservices. To understand\n\nthis context better, Werner Vogels’ (Amazon CTO) test on his Learnings\n\nwith AWS30 is highly recommended:\n\nWe needed to build systems that embrace failure as a natu- ral occurrence even if we did not know what the failure might be. Systems need to keep running even if the “house is on ﬁre.” It is important to be able to manage pieces that are impacted without the need to take the overall system down. We’ve developed the fundamental skill of managing the “blast radius” of a failure occurrence such that the overall health of the system can be maintained.\n\n30https://www.thoughtworks.com/insights/blog/monoliths-are-bad-design-and-you-know-it\n\nGo to Table of Contents\n\n116\n\nTechnically speaking, this restriction has many different origins. Whether\n\nAlthough there are, therefore, really good guidelines for sufﬁcient\n\nit’s the latencies which must be called up for one procedure due to dozens\n\nhandling of the topic, so as to ﬁnd out whether microservices make sense\n\nof active services, clearly difﬁcult debugging, the demanding hardware\n\nfor an IT organization (not for most!), you can regularly ﬁnd contributions\n\nsetup or the complex data retention (each service has its own database).\n\nsuch as that by Sam Gibson³¹ online or on conference panels:\n\nThe fundamentally hard to manage technology zoo notwithstanding.\n\nHere, excellent parallels to eCommerce organizations can be drawn. Who\n\nIn principle, it is possible to create independent modules within a single monolithic application. In practice, this is seldom implemented. Code within the monolith most often, and quickly, becomes tightly coupled. Microservices, in con- trast, encourage architects and developers the opportunity to develop less coupled systems that can be changed faster and scaled more effectively.\n\nis quicker and more effective in the development and scaling of new\n\nmodels:\n\n1. a company with dozens of departments and directorates, which must\n\nbe in a permanent state of agreement, but which are extremely good in\n\neach of their individual disciplines;\n\nIn Kassenzone reader’s language, this pretty much means: Pure Play\n\n2. or a company at which up to 100 employees sit in one room, all\n\nbusiness models are good if they are implemented in an orderly fashion\n\nknowing what’s going on and all talking to each other.\n\nbut it will only really come good if you run many channels well. The\n\nwinning strategy is omnichannel. Now, you could simply brush such\n\nThe second example is quicker, that goes without saying. With larger\n\nstatements off, but it is astounding just how quickly and strongly such\n\nmodels, at which scaling is a rather uniform approach, the ﬁrst example is\n\nsimple thought processes spread and become the truth all by themselves.\n\nbetter. Not much is different in the case of microservices. To understand\n\nThe voices which oppose them32 are quiet in comparison, but the\n\nthis context better, Werner Vogels’ (Amazon CTO) test on his Learnings\n\narguments are quite conclusive.\n\nwith AWS30 is highly recommended:\n\nIn principle, it is possible to create independent modules within a single monolithic application. In practice, this is seldom implemented. Code within the monolith most often, and quickly, becomes tightly coupled. Microservices, in con- trast, encourage architects and developers the opportunity to develop less coupled systems that can be changed faster and scaled more effectively.\n\nWe needed to build systems that embrace failure as a natu-\n\nral occurrence even if we did not know what the failure\n\nmight be. Systems need to keep running even if the “house\n\nis on ﬁre.” It is important to be able to manage pieces that\n\nare impacted without the need to take the overall system\n\ndown. We’ve developed the fundamental skill of managing\n\nthe “blast radius” of a failure occurrence such that the overall\n\nhealth of the system can be maintained.\n\n³¹https://www.thoughtworks.com/insights/blog/monoliths-are-bad-design-and-you-know-it\n\n30https://www.thoughtworks.com/insights/blog/monoliths-are-bad-design-and-you-know-it\n\n³²http://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html\n\n117",
      "page_number": 110
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 118-122)",
      "start_page": 118,
      "end_page": 122,
      "detection_method": "topic_boundary",
      "content": "Technically and methodically, a lot is said for the use of “good”\n\nmonolithic structures for a great deal of eCommerce companies, but\n\ndoing so requires a lot of effort producing good code, something which,\n\nin the short term, you don’t have to do in the microservices world. If, then,\n\na mistake in the scaling arises, the affected CTOs would probably wish\n\nthey had the AS400 system back.\n\nThe founder of Basecamp has hit the nail on the head with his own\n\nsystem, which he describes as “The Majestic Monolith”33. And, where\n\ncontent is concerned, I’m with him:\n\nWhere things go astray is when people look at, say, Amazon or Google or whoever else might be commanding a ﬂeet of services, and think, hey it works for The Most Successful, I’m sure it’ll work for me too. Bzzzzzzzz!! Wrong! The patterns that make sense for organizations’ orders of magnitude larger than yours, are often the exact opposite ones that’ll make sense for you. It’s the essence of cargo culting. If I dance like these behemoths, surely I too will grow into one. I’m sorry, but that’s just not how the tango goes.\n\nIt’s bit like if companies who own an old bicycle, which they don’t know\n\nhow to ride properly, want a little too much. They see the unicyclist at the\n\ncircus performing dazzling tricks on his unicycle and say to themselves:\n\nMy bike is too old, that’s why I can’t ride it. I’ll just start with a unicycle\n\nright away, at least that’s forward-thinking.\n\n³³ https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\nGo to Table of Contents\n\n118\n\nAppendix 3: Blogs and resources\n\nThere are plenty of websites, blogs and books you can check to read\n\nmore about microservices and related architectural patterns. The book\n\n“Building Microservices, Designing Fine-Grained Systems” by Sam\n\nNewman and O’Reilly Media\n\n(http://shop.oreilly.com/product/0636920033158.do) should be at the\n\ntop of the top of your list. The most important information has been\n\ncollected into one place. It is all you need to know to model, implement,\n\ntest and run new systems using microservices or transform the monolith\n\ninto a distributed set of smaller applications. A must-have book for every\n\nsoftware architect. O’Reilly Media has also released another interesting\n\nbook, “Microservice Architecture” by Irakli Nadareishvili, Ronnie Mitra,\n\nMatt McLarty and Mike Amundsen\n\n(http://shop.oreilly.com/product/0636920050308.do), which is also worth\n\na read.\n\nWith knowledge from Sam Newman, you should be ready to discover\n\nwebsites like:\n\nhttp://microservices.io,\n\nhttps://github.com/mfornos/awesome-microservices,\n\nand https://dzone.com/ (under „microservices” keyword), curated lists\n\nof articles.\n\nIt’s a condensed dose of knowledge about core microservice patterns,\n\ndecomposition methods, deployment patterns, communication styles,\n\ndata management and many more… There you can also ﬁnd many\n\ninteresting presentations and talks recorded at conferences. The last\n\nwebsite speciﬁcally, https://dzone.com/, should be very interesting for IT\n\npeople.\n\nTechnically and methodically, a lot is said for the use of “good”\n\nAppendix 3: Blogs and resources\n\nmonolithic structures for a great deal of eCommerce companies, but\n\ndoing so requires a lot of effort producing good code, something which,\n\nThere are plenty of websites, blogs and books you can check to read\n\nin the short term, you don’t have to do in the microservices world. If, then,\n\nmore about microservices and related architectural patterns. The book\n\na mistake in the scaling arises, the affected CTOs would probably wish\n\n“Building Microservices, Designing Fine-Grained Systems” by Sam\n\nthey had the AS400 system back.\n\nNewman and O’Reilly Media\n\n(http://shop.oreilly.com/product/0636920033158.do) should be at the\n\nThe founder of Basecamp has hit the nail on the head with his own\n\ntop of the top of your list. The most important information has been\n\nsystem, which he describes as “The Majestic Monolith”33. And, where\n\ncollected into one place. It is all you need to know to model, implement,\n\ncontent is concerned, I’m with him:\n\ntest and run new systems using microservices or transform the monolith\n\ninto a distributed set of smaller applications. A must-have book for every\n\nWhere things go astray is when people look at, say, Amazon\n\nsoftware architect. O’Reilly Media has also released another interesting\n\nor Google or whoever else might be commanding a ﬂeet of\n\nbook, “Microservice Architecture” by Irakli Nadareishvili, Ronnie Mitra,\n\nservices, and think, hey it works for The Most Successful, I’m\n\nMatt McLarty and Mike Amundsen\n\nsure it’ll work for me too. Bzzzzzzzz!! Wrong! The patterns\n\nthat make sense for organizations’ orders of magnitude\n\n(http://shop.oreilly.com/product/0636920050308.do), which is also worth\n\nlarger than yours, are often the exact opposite ones that’ll\n\na read.\n\nmake sense for you. It’s the essence of cargo culting. If I\n\ndance like these behemoths, surely I too will grow into one.\n\nWith knowledge from Sam Newman, you should be ready to discover\n\nI’m sorry, but that’s just not how the tango goes.\n\nwebsites like:\n\nhttp://microservices.io,\n\nIt’s bit like if companies who own an old bicycle, which they don’t know\n\nhttps://github.com/mfornos/awesome-microservices,\n\nhow to ride properly, want a little too much. They see the unicyclist at the\n\nand https://dzone.com/ (under „microservices” keyword), curated lists\n\ncircus performing dazzling tricks on his unicycle and say to themselves:\n\nof articles.\n\nMy bike is too old, that’s why I can’t ride it. I’ll just start with a unicycle\n\nright away, at least that’s forward-thinking.\n\nIt’s a condensed dose of knowledge about core microservice patterns,\n\ndecomposition methods, deployment patterns, communication styles,\n\ndata management and many more… There you can also ﬁnd many\n\ninteresting presentations and talks recorded at conferences. The last\n\nwebsite speciﬁcally, https://dzone.com/, should be very interesting for IT\n\npeople.\n\n³³ https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\n119\n\nGo to Table of Contents\n\nDepending on your time, you can subscribe to the newsletter\n\n“Microservices Weekly”\n\n(http://www.microservicesweekly.com) for a\n\nweekly set of articles about architecture and container-based virtualization\n\nor\n\nvisit\n\nthe Microservices\n\nsection\n\nat\n\nthe\n\nInfoQ website\n\n(https://www.infoq.com/microservices/), one of the most\n\nimportant\n\nwebsites with articles and talks related to software development.\n\nAs you can see, knowledge is all around us. Don’t forget about Martin\n\nFowler\n\nand\n\nhis\n\n“Microservice\n\nResource\n\nGuide”\n\n(https://martinfowler.com/microservices/). Martin is Chief Scientist at\n\nToughtWorks,\n\nthe\n\npublisher\n\nof\n\n“Technology\n\nRadar”\n\n(https://www.thoughtworks.com/radar; highly recommended as well) and\n\nauthor of a few bestselling books. Martin Fowler’s wiki is a Mecca for\n\nsoftware architects and “Microservice Resource Guide” is only one of\n\nthem…\n\nGo to Table of Contents\n\n120\n\nDepending on your time, you can subscribe to the newsletter\n\n“Microservices Weekly”\n\n(http://www.microservicesweekly.com) for a\n\nweekly set of articles about architecture and container-based virtualization\n\nor\n\nvisit\n\nthe Microservices\n\nsection\n\nat\n\nthe\n\nInfoQ website\n\n(https://www.infoq.com/microservices/), one of the most\n\nimportant\n\nwebsites with articles and talks related to software development.\n\nAs you can see, knowledge is all around us. Don’t forget about Martin\n\nFowler\n\nand\n\nhis\n\n“Microservice\n\nResource\n\nGuide”\n\n(https://martinfowler.com/microservices/). Martin is Chief Scientist at\n\nToughtWorks,\n\nthe\n\npublisher\n\nof\n\n“Technology\n\nRadar”\n\n(https://www.thoughtworks.com/radar; highly recommended as well) and\n\nauthor of a few bestselling books. Martin Fowler’s wiki is a Mecca for\n\nThank you!\n\nsoftware architects and “Microservice Resource Guide” is only one of\n\nthem…\n\nIf you want to know more\n\nabout microservices,\n\njust\n\ndrop me a message at\n\npkarwatka@divante.co.\n\nwww.divante.co\n\n121\n\nGo to Table of Contents\n\nTry Our Solution\n\nGo to Table of Contents\n\n122",
      "page_number": 118
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Table of contents\n\nForeword\n\nDivide and conquer\n\n2 3\n\nChange is too slow\n\n4\n\nIn e-Commerce: your software is your company\n\n4\n\nOmnichannel\n\n5\n\nAbout the authors\n\n6\n\nTable of contents\n\n10\n\nMicroservices\n\n11\n\nThe criticism\n\n14\n\nEvolutionary approach Best practices\n\n16 21\n\nCreate a Separate Database for Each Service\n\n22\n\nRely on contracts between services\n\n24\n\nDeploy in Containers\n\n24\n\nTreat Servers as Volatile\n\n25\n\nCase Studies: Re-architecting the monolith\n\n27\n\nB2B\n\n28\n\nMobile Commerce\n\n31\n\nRelated techniques and patterns\n\n41\n\nFundamentals of distributed systems\n\n42\n\nMicroservices Architecture for eCommerce\n\nDesign patterns\n\n44\n\nIntegration techniques\n\n52\n\nDeployment of microservices\n\n64\n\nServerless - Function as a Service\n\n74\n\nContinuous Deployment\n\n82\n\nRelated technologies\n\nMicroservices based e-commerce platforms\n\nPiotr Karwatka 85 Mariusz Gil Mike Grabowski\n\n86\n\nTechnologies that empower microservices achitecture\n\n74\n\nAleksander Graf\n\nDistributed logging and monitoring\n\nBlogs and resources\n\nGo to Table of Contents\n\nPaweł Jędrzejewski\n\n105 Michał Kurzeja\n\n112 Antoni Orﬁn Bartosz Picho 1",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Foreword\n\nName a technology conference or meetup and I’ll tell you about the\n\nconstant speeches referencing microservices. This modern engineering\n\ntechnique has grown from good old SOA (Service Oriented Architecture)\n\nwith features like REST (vs. old SOAP) support, NoSQL databases and the\n\nEvent driven/reactive approach sprinkled in.\n\nWhy have they become so important? Roughly speaking, because of what\n\nscale systems achieve nowadays and the number of changes that are\n\ndeployed on a daily basis.\n\nOf course microservices aren’t a panacea. I’ve tried to make this book as\n\ninformational and candid as\n\nI can. Although we promote the\n\nmicroservices architecture across the following chapters, please also take\n\nMicroservices Architecture\n\na look at Appendix 1 authored by Spryker’s Co-Founder Alexander Graf\n\nwith a very candid and pragmatic view on this topic.\n\nfor eCommerce\n\nThis book is a rather “technical one” - starting with some Business\n\nrationale for microservices and then stepping into the engineers’\n\nshoes and trying to show you the tools and techniques required to\n\nbuild and scale modern eCommerce systems.\n\n2\n\nGo to Table of Contents",
      "content_length": 1154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Divide and conquer\n\nThe original Zalando site was built on Magento using PHP, and at one\n\nForeword\n\ntime was the biggest Magento site in the world. The German eCommerce\n\ngiant that employs over 10,000 people and ships more than 1,500 fashion\n\nbrands to customers in 15 European countries generated $3.43 billion in\n\nName a technology conference or meetup and I’ll tell you about the\n\nrevenue last year. With over 700 people on its engineering team, they\n\nconstant speeches referencing microservices. This modern engineering\n\nmoved to microservices in 18 months.\n\ntechnique has grown from good old SOA (Service Oriented Architecture)\n\nwith features like REST (vs. old SOAP) support, NoSQL databases and the\n\nThe key advantages of the microservice approach are:\n\nEvent driven/reactive approach sprinkled in.\n\nFaster Time to Market - because of the decentralized development\n\nprocess and opportunities to\n\ninnovate given to each separate\n\nWhy have they become so important? Roughly speaking, because of what\n\ndevelopment team.\n\nscale systems achieve nowadays and the number of changes that are\n\ndeployed on a daily basis.\n\nLess is more - the microservices approach leverages the Single\n\nResponsibility Principle which means that a single microservice\n\nperforms exactly one business function. Therefore developers can\n\nOf course microservices aren’t a panacea. I’ve tried to make this book as\n\ncreate\n\nmore\n\nefﬁcient,\n\nclear\n\nand\n\ntestable\n\ncode.\n\ninformational and candid as\n\nI can. Although we promote the\n\nmicroservices architecture across the following chapters, please also take\n\nDomain Expertise - business features are granularly split into separate\n\nMicroservices Architecture\n\na look at Appendix 1 authored by Spryker’s Co-Founder Alexander Graf\n\nmicro-applications. You’ll have separate services for promotions,\n\nwith a very candid and pragmatic view on this topic.\n\nfor eCommerce\n\ncheckout and products catalog. Each development team typically\n\nincludes business analysts and developers. It builds engagement and\n\nspeeds up development.\n\nThis book is a rather “technical one” - starting with some Business\n\nrationale for microservices and then stepping into the engineers’\n\nAccountability - Booking.com’s approach to development is to\n\nshoes and trying to show you the tools and techniques required to\n\npromote the teams whose features are published for production (before\n\nbuild and scale modern eCommerce systems.\n\nthe features are usually proven to increase conversion). By working on\n\n3\n\nGo to Table of Contents",
      "content_length": 2522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "the basis of microservices you’ll have separate teams accountable for\n\nparticular KPIs, providing SLA’s for their parts, etc. A side effect of this\n\napproach is usually the rise of employee effectiveness and engagement.\n\nEasier outsourcing - because services are separable and usually\n\ncontracts between them have to be well documented, it’s rather easy to\n\nuse ready-made products or outsource particular services to other\n\ncompanies.\n\nChange is too slow\n\nIt’s something I usually hear when starting a new consulting engagement.\n\nAfter a few years in the market, enterprises tend to keep the status quo,\n\nand try to keep everything running smoothly, but nowadays it’s not\n\nsufﬁcient to become a market leader. It’s crucial to experiment, change,\n\ntest and select the best solutions. But it’s extremely hard to work like that\n\nwith a team of a few dozen engineers and extremely sophisticated\n\nbusiness rules coded to the metal by thousands of lines of code. The\n\nmicroservics approach became so popular because it breaks this into\n\nsmaller, self-sufﬁcient and granular areas of responsibility that are easy to\n\ntest and deploy.\n\nIn eCommerce: your software is your company\n\nOrganizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.\n\n— M. CONWAY\n\nGo to Table of Contents\n\n4",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "the basis of microservices you’ll have separate teams accountable for\n\nAmong all the technical challenges, microservices usually require\n\nparticular KPIs, providing SLA’s for their parts, etc. A side effect of this\n\norganizational changes inside the company. Breaking the technical\n\napproach is usually the rise of employee effectiveness and engagement.\n\nmonolith quite often goes hand in hand with dividing enterprise\n\ndepartments into agile, rapid teams to achieve faster results. In the end,\n\nEasier outsourcing - because services are separable and usually\n\nthe ﬁnal outcome is that processes that took a few months can now be\n\ncontracts between them have to be well documented, it’s rather easy to\n\nexecuted in weeks and everybody feels engaged. It’s something you\n\nuse ready-made products or outsource particular services to other\n\ncannot underestimate.\n\ncompanies.\n\nOmnichannel\n\nChange is too slow\n\nTo fulﬁll your customer’s expectations about omnichannel, you have to\n\nintegrate each and every piece of information about products, shipments,\n\nIt’s something I usually hear when starting a new consulting engagement.\n\nstocks and orders, and keep it up to datefresh. There is no single system\n\nAfter a few years in the market, enterprises tend to keep the status quo,\n\nto deal with POS applications, ERP, WMS and eCommerce\n\nand try to keep everything running smoothly, but nowadays it’s not\n\nresponsibilities. Of course, I’ve seen a few that pretend to be a One-stop\n\nsufﬁcient to become a market leader. It’s crucial to experiment, change,\n\nsolution but I’ve never seen anything like that in production. The key is to\n\ntest and select the best solutions. But it’s extremely hard to work like that\n\nintegrate systems that are optimal for their niches and already integrated\n\nwith a team of a few dozen engineers and extremely sophisticated\n\nwithin your existing processes. Microservices are great for such an\n\nbusiness rules coded to the metal by thousands of lines of code. The\n\nevolutionary approach. We’ll describe a case study - where by exposing\n\nmicroservics approach became so popular because it breaks this into\n\nthe APIs from PIM, CRM, ERP and creating a dedicated UI facade, we\n\nsmaller, self-sufﬁcient and granular areas of responsibility that are easy to\n\nleveraged on this approach to provide a sophisticated B2B solution.\n\ntest and deploy.\n\nThis eBook will try to help you decide if it is time for applying this\n\nIn eCommerce: your software is your company\n\napproach and how to start by referencing to few popular techniques\n\nand tools worth following.\n\nOrganizations which design systems ... are constrained to\n\nproduce designs which are copies of the communication\n\nLet’s get started!\n\nstructures of these organizations.\n\nPiotr Karwatka, CTO at Divante\n\n— M. CONWAY\n\n5\n\nGo to Table of Contents",
      "content_length": 2814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "About the authors\n\nGo to Table of Contents\n\n6",
      "content_length": 45,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Piotr Karwatka\n\nCTO at Divante. My biggest project? Building the company from 1 ->\n\n150+ (still growing), taking care of software productions, technology\n\ndevelopment and operations/processes. 10+ years of professional\n\nSoftware Engineering and Project Management experience. I've also tried\n\nmy hand at writing, with the book \"E-Commerce technology for\n\nmanagers\". My career started as a software developer and co-creator of\n\nabout 30 commercial desktop and web applications.\n\nMichał Kurzeja\n\nCTO and Co-Founder of Accesto with over 8 years of experience in\n\nleading technical projects. Certiﬁed Symfony 3 developer. Passionate\n\nabout new technologies; mentors engineers and teams in developing\n\nhigh-quality software. Co-orgaizer of Wrocław Symfony Group meetups.\n\nAbout the authors\n\nMariusz Gil\n\nSoftware Architect and Consultant, focused on high value and high\n\ncomplexity, scalable web applications with 17+ years of experience in the\n\nIT industry. Helps teams and organizations adopt good development and\n\nprogramming practices. International conference speaker and developer\n\nevents organizer.\n\nBartosz Picho\n\neCommerce Solution Architect, responsible for Magento 2 technology at\n\nDivante. Specialized in application development end 2 end: from\n\nbusiness\n\nrequirements\n\nto\n\nsystem architectures, meeting high\n\nperformance and scalability expectations. Passionate technologist,\n\nexperienced in Magento 1 and 2, both Community and Enterprise\n\neditions.\n\n7\n\nGo to Table of Contents",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Antoni Orﬁn\n\nSolutions Architect specialized\n\nin designing highly-scalable web\n\napplications and\n\nintroducing best practices\n\ninto\n\nthe software\n\ndevelopment process. Speaker at several IT conferences. Currently\n\nresponsible for systems architecture and driving DevOps methodology at\n\nDroplr.com.\n\nMike Grabowski\n\nSoftware Developer and open source enthusiast. Core contributor to\n\nmany popular libraries, including React Native, React Navigation and\n\nHaul. Currently CTO at Callstack.io. Travels the world teaching\n\ndevelopers how to use React and shares his experience at various\n\nReact-related events.\n\nPaweł Jędrzejewski\n\nFounder and Lead Developer of Sylius, the ﬁrst Open Source eCommerce\n\nframework. Currently busy building the business & ecosystem around the\n\nproject while also speaking at international tech conferences about\n\neCommerce & APIs.\n\nAlexander Graf\n\nCo-Founder and CEO of Spryker Systems. Alexander Graf (*1980) is one\n\nof Germany’s leading eCommerce experts and a digital entrepreneur of\n\nmore than a decade’s standing. His widely-read blog Kassenzone (“The\n\nCheck-Out Area”) has kicked off many a debate among commerce\n\nprofessionals. Alexander wrote Appendix 1 to this book.\n\nGo to Table of Contents\n\n8",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Antoni Orﬁn\n\nAknowledgement\n\nSolutions Architect specialized\n\nin designing highly-scalable web\n\napplications and\n\nintroducing best practices\n\ninto\n\nthe software\n\nI believe in open source. This book was intended to be as open as\n\ndevelopment process. Speaker at several IT conferences. Currently\n\npossible. I would like to thank all the enthusiasts engaged in this project -\n\nresponsible for systems architecture and driving DevOps methodology at\n\ngiving me honest feedback, helping with editorials etc.\n\nDroplr.com.\n\nMateusz Gromulski, Will Jarvis, Ian Cassidy, Jacek Lampart, Agata\n\nMike Grabowski\n\nMłodawska, Tomasz Anioł, Tomasz Karwatka, Cezary Olejarczyk\n\nSoftware Developer and open source enthusiast. Core contributor to\n\nmany popular libraries, including React Native, React Navigation and\n\nHaul. Currently CTO at Callstack.io. Travels the world teaching\n\nThank you guys!\n\ndevelopers how to use React and shares his experience at various\n\nReact-related events.\n\nPaweł Jędrzejewski\n\nFounder and Lead Developer of Sylius, the ﬁrst Open Source eCommerce\n\nframework. Currently busy building the business & ecosystem around the\n\nproject while also speaking at international tech conferences about\n\neCommerce & APIs.\n\nAlexander Graf\n\nCo-Founder and CEO of Spryker Systems. Alexander Graf (*1980) is one\n\nof Germany’s leading eCommerce experts and a digital entrepreneur of\n\nmore than a decade’s standing. His widely-read blog Kassenzone (“The\n\nCheck-Out Area”) has kicked off many a debate among commerce\n\nprofessionals. Alexander wrote Appendix 1 to this book.\n\n9",
      "content_length": 1566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Table of contents\n\nForeword\n\nDivide and conquer\n\nChange is too slow\n\nIn e-Commerce: your software is your company\n\nOmnichannel\n\nAbout the authors\n\nTable of contents\n\nMicroservices\n\nThe criticism\n\nEvolutionary approach\n\nBest practices\n\nCreate a Separate Database for Each Service\n\nRely on contracts between services\n\nDeploy in Containers\n\nTreat Servers as Volatile\n\nRelated techniques and patterns\n\nDesign patterns\n\nIntegration techniques\n\nDeployment of microservices\n\nServerless - Function as a Service\n\nContinuous Deployment\n\nRelated technologies\n\nMicroservices based e-commerce platforms\n\nTechnologies that empower microservices achitecture\n\nDistributed logging and monitoring\n\nCase Studies: Re-architecting the monolith\n\nB2B\n\nMobile Commerce\n\nBlogs and resources\n\nGo to Table of Contents\n\n10\n\n2 3\n\n4\n\n4\n\n5\n\n6\n\n10\n\n11\n\n14\n\n16\n\n21\n\n22\n\n24\n\n24\n\n25\n\n27\n\n30\n\n39\n\n50\n\n60\n\n69\n\n72\n\n73\n\n77\n\n91\n\n98\n\n99\n\n110\n\n112",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Table of contents\n\nMicroservices\n\n11\n\nGo to Table of Contents",
      "content_length": 61,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Microservices\n\nMicroservice architecture structures the application as a set of loosely\n\ncoupled, collaborating services. Each service implements a set of related\n\nfunctions. For example, an application might consist of services such as an\n\norder management service, an inventory management service, etc.\n\nServices communicate using protocols such as HTTP/REST or (a less\n\npopular approach) using an asynchronous approach like AMQP. Services\n\ncan be developed as separate applications and deployed independently.\n\nData consistency is maintained using an event-driven architecture\n\nbecause each service should have its own database in order to be\n\ndecoupled from other services.\n\nThe most common forces dictating the Microservice approach¹:\n\nMultiple teams of developers working on a single application.\n\nSystem must be easy to understand and maintain/modify, no matter the\n\nnumber of changes deployed.\n\nUrgency for new team members to be productive.\n\nNeed for continuous deployment (although possible to achieve with\n\nmonolith design, microservices include some features of DevOps\n\napproach by design).\n\nScalability requirements that require running your application across\n\nserver clusters.\n\nDesire to adopt emerging technologies (new programming languages,\n\netc.) without major risks.\n\n¹ According to: http://microservices.io/patterns/microservices.html\n\nGo to Table of Contents\n\n12",
      "content_length": 1384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "The assumptions of the orthogonal architecture followed by\n\nMicroservices\n\nmicroservices architects implies the following beneﬁts:\n\nMicroservice architecture structures the application as a set of loosely\n\nEach microservice could be deployed separately and without shutting\n\ncoupled, collaborating services. Each service implements a set of related\n\ndown the whole system.\n\nfunctions. For example, an application might consist of services such as an\n\norder management service, an inventory management service, etc.\n\nEach microservice can be developed using different technologies while\n\nallowing them to publish HTTP end-points (Golang based services can\n\nServices communicate using protocols such as HTTP/REST or (a less\n\ninteroperate with PHP, Java…).\n\npopular approach) using an asynchronous approach like AMQP. Services\n\ncan be developed as separate applications and deployed independently.\n\nBy deﬁning strict protocols (API), services are easy to test and extend\n\nData consistency is maintained using an event-driven architecture\n\ninto the future.\n\nbecause each service should have its own database in order to be\n\ndecoupled from other services.\n\nMicroservices can be easily hosted in the cloud, Docker environments,\n\nor any other server platform, and can be very easily scaled as each\n\nThe most common forces dictating the Microservice approach¹:\n\nservice can live on its own server(s), VPS(es) etc.\n\nMultiple teams of developers working on a single application.\n\nThe services are easy to replace.\n\nSystem must be easy to understand and maintain/modify, no matter the\n\nnumber of changes deployed.\n\nServices are organized around capabilities, e.g., UI, front-end,\n\nUrgency for new team members to be productive.\n\nrecommendation, logistics, billing, etc.\n\nNeed for continuous deployment (although possible to achieve with\n\nmonolith design, microservices include some features of DevOps\n\nThe scalability and deployment processes of microservice-based systems\n\napproach by design).\n\ncan be much easier to automate compared to monolithic architectures.\n\nScalability requirements that require running your application across\n\nThe DevOps approach to infrastructure along with Cloud services is\n\nserver clusters.\n\ncommonly in use. The examples of Spotify and Netﬂix² inspire IT\n\nDesire to adopt emerging technologies (new programming languages,\n\nengineers to implement continuous delivery and monitoring.\n\netc.) without major risks.\n\n¹ According to: http://microservices.io/patterns/microservices.html\n\n² https://www.nginx.com/blog/microservices-at-netﬂix-architectural-best-practices/\n\n13\n\nGo to Table of Contents",
      "content_length": 2612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Dockerization of IT environments, monitoring tools and DevOps tools\n\n(Ansible, Chef, Puppet and others) can take your development team to\n\nthe the next level of effectiveness.\n\nA\n\nB\n\nOperations\n\nCore Team\n\nQuality assurance\n\nCross-functional team\n\nCross-functional team\n\nDevelopment\n\nCross-functional team\n\nCross-functional team\n\nCross-functional team\n\nCross-functional team\n\nFig. 1: A microservice approach encourages enterprises to become more agile, with\n\ncross-functional teams responsible for each service. Implementing such a company\n\nstructure, as in Spotify or Netﬂix, can allow you to adopt and test new ideas quickly, and\n\nbuild strong ownership feelings across the teams.\n\nThe criticism\n\nThe microservice approach is subject to criticism for a number of\n\nissues:\n\nGo to Table of Contents\n\n14",
      "content_length": 802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Dockerization of IT environments, monitoring tools and DevOps tools\n\nThe architecture introduces additional complexity and new problems\n\n(Ansible, Chef, Puppet and others) can take your development team to\n\nto deal with, such as network latency, message formats, load\n\nthe the next level of effectiveness.\n\nbalancing, fault tolerance and monitoring. Ignoring one of these\n\nbelongs to the \"fallacies of distributed computing”.\n\nA\n\nB\n\nAutomation is possible but in the simplest cases, tests and deployments\n\nOperations\n\nCore Team\n\nmay be more complicated than with the monolithic approach.\n\nMoving responsibilities between services is difﬁcult. It may involve\n\nQuality assurance\n\nCross-functional team\n\nCross-functional team\n\ncommunication between different teams, rewriting the functionality in\n\nanother language or ﬁtting it into a different infrastructure. On the other\n\nhand, it’s easy to test contracts between services after such changes.\n\nDevelopment\n\nCross-functional team\n\nCross-functional team\n\nStarting with the microservices approach from the beginning can lead to\n\ntoo many services, whereas the alternative of internal modularization\n\nmay lead to a simpler design.\n\nCross-functional team\n\nCross-functional team\n\nFig. 1: A microservice approach encourages enterprises to become more agile, with\n\ncross-functional teams responsible for each service. Implementing such a company\n\nstructure, as in Spotify or Netﬂix, can allow you to adopt and test new ideas quickly, and\n\nbuild strong ownership feelings across the teams.\n\nThe criticism\n\nThe microservice approach is subject to criticism for a number of\n\nissues:\n\n15\n\nGo to Table of Contents",
      "content_length": 1650,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Go to Table of Contents\n\nEvolutionary approach\n\n16",
      "content_length": 50,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Evolutionary\n\napproach\n\nEvolutionary approach\n\nMartin Fowler, one of the pioneers³ of microservices used to say:\n\nAlmost all the successful microservice stories have started with a monolith that got too big and was broken up.\n\nAlmost all the cases where I've heard of a system that was built as a microservice system from scratch, has ended up in serious trouble.\n\n³ https://martinfowler.com/articles/microservices.html\n\n17",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "External Systems\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nESB\n\n...\n\nMagento\n\nFig. 2: Initial, monolithic architecture began after 4 years of development of a\n\nlarge-scale, 100M EUR/yr B2B platform.\n\nWhen you begin a new application, how sure are you that it will be useful\n\nto your users? Starting with microservices from day one may signiﬁcantly\n\ncomplicate the system. It can be much harder to pivot if something didn’t\n\ngo as planned (from the business standpoint). During this ﬁrst phase you\n\nneed to prioritize the speed of development to basically ﬁgure out what\n\nworks.\n\nGo to Table of Contents\n\n18",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "External Systems\n\nXYZ Client\n\nExternal Systems\n\nAPI Consumers\n\nERP\n\n...\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nESB\n\n...\n\nAPI Gateway\n\nMicro Services\n\nr e k o r B e g a s s e M\n\nPRICE\n\nCRM\n\nOMS\n\nPIM\n\nNOTIFY\n\nRECOMMENDATION\n\nWMS\n\nREPORT\n\nREVIEW\n\n...\n\nMagento\n\nFig. 2: Initial, monolithic architecture began after 4 years of development of a\n\nFrontend Application\n\nMobile App\n\nlarge-scale, 100M EUR/yr B2B platform.\n\nFig. 3: The very same system but after architecture re-engineering; now the system core\n\nis built upon 10 microservices.\n\nWhen you begin a new application, how sure are you that it will be useful\n\nto your users? Starting with microservices from day one may signiﬁcantly\n\nMany successful eCommerce businesses (if not all of them!) started from\n\ncomplicate the system. It can be much harder to pivot if something didn’t\n\nmonolithic, at some point, all-in-one platforms before transitioning into a\n\ngo as planned (from the business standpoint). During this ﬁrst phase you\n\nservice oriented architecture.\n\nneed to prioritize the speed of development to basically ﬁgure out what\n\nworks.\n\nRe-engineering the architecture requires a team effort of 6-12 months (18\n\nmonths in Zalando’s case) - and therefore it should have a solid business\n\nfoundation.\n\n19\n\nGo to Table of Contents",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "The most common reasons we’ve seen to initialize a transformation\n\nare the following:\n\nWith four to ﬁve years of development, the scope of the system is so\n\nbroad that implementing changes in one of the modules affects other\n\nareas and despite having unit-tests, making deep changes to the\n\nsystem logic is quite risky.\n\nTechnical debt in one system area is accrued to a level at which it’s\n\nextremely hard to resolve without major changes. Performance\n\nchallenges exist in the product catalog, pricing/promo rules or central\n\nuser database areas.\n\nThere is a need to coordinate separate teams or vendors in a way\n\nwhich leads to minimal interference between them.\n\nThe system is hard to test and deploy.\n\nThere is a need to implement continuous deployments.\n\nGo to Table of Contents\n\n20",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "The most common reasons we’ve seen to initialize a transformation\n\nare the following:\n\nWith four to ﬁve years of development, the scope of the system is so\n\nbroad that implementing changes in one of the modules affects other\n\nareas and despite having unit-tests, making deep changes to the\n\nsystem logic is quite risky.\n\nTechnical debt in one system area is accrued to a level at which it’s\n\nextremely hard to resolve without major changes. Performance\n\nchallenges exist in the product catalog, pricing/promo rules or central\n\nuser database areas.\n\nThere is a need to coordinate separate teams or vendors in a way\n\nwhich leads to minimal interference between them.\n\nThe system is hard to test and deploy.\n\nThere is a need to implement continuous deployments.\n\nBest practices\n\n21\n\nGo to Table of Contents",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Best practices\n\nThis eBook is intended to show you the most popular design patterns and\n\npractices related to microservices. I strongly recommend you to track the\n\nfather of the micro services approach - Sam Newman. You should check\n\nout websites\n\nlike: http://microservices.io, https://dzone.com/ and\n\nhttps://github.com/mfornos/awesome-microservices\n\n(under the “microservices” keyword). They provide a condensed dose of\n\nknowledge about core microservice patterns, decomposition methods,\n\ndeployment patterns, communication styles, data management and\n\nmuch more…\n\nCreate a Separate Database for Each Service\n\nSharing the same data structures between services can be difﬁcult -\n\nparticularly in environments where separate teams manage each\n\nmicroservice. Conﬂicts and surprising changes are not what you’re aiming\n\nfor with a distributed approach.\n\nBreaking apart the data can make information management more\n\ncomplicated the individual storage systems can easily de-sync or become\n\ninconsistent. You need to add a tool that performs master data\n\nmanagement. While operating in the background, it must eventually ﬁnd\n\nand ﬁx inconsistencies. One of the patterns for such synchronization is\n\nEvent Sourcing. This pattern can help you with such situations by\n\nproviding you with a reliable history log of all data changes that can be\n\nrolled back and forth. Eventual Consistency and CAP theorem are\n\nfundamentals that must be considered during the design phase.\n\nGo to Table of Contents\n\n22",
      "content_length": 1493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Best practices\n\nTRADITIONAL APPLICATION\n\n3-Tier Approach\n\nSingle app process or 3-Tier approach\n\nThis eBook is intended to show you the most popular design patterns and\n\nSeveral modules\n\npractices related to microservices. I strongly recommend you to track the\n\nLayered modules\n\nfather of the micro services approach - Sam Newman. You should check\n\nSingle App Process\n\nout websites\n\nlike: http://microservices.io, https://dzone.com/ and\n\nhttps://github.com/mfornos/awesome-microservices\n\nOR\n\n(under the “microservices” keyword). They provide a condensed dose of\n\nknowledge about core microservice patterns, decomposition methods,\n\ndeployment patterns, communication styles, data management and\n\nmuch more…\n\nSINGLE MONOLITH DATABASE\n\nCreate a Separate Database for Each Service\n\nMICROSERVICES APPROACH\n\nPresentation services\n\nUI\n\nSharing the same data structures between services can be difﬁcult -\n\nparticularly in environments where separate teams manage each\n\nmicroservice. Conﬂicts and surprising changes are not what you’re aiming\n\nfor with a distributed approach.\n\nBreaking apart the data can make information management more\n\nStateful services\n\ncomplicated the individual storage systems can easily de-sync or become\n\ninconsistent. You need to add a tool that performs master data\n\nmanagement. While operating in the background, it must eventually ﬁnd\n\nStatles services with related databases\n\nand ﬁx inconsistencies. One of the patterns for such synchronization is\n\nMODEL/DATABASE PER MICROSERVICE\n\nEvent Sourcing. This pattern can help you with such situations by\n\nFig. 4: Each microservice should have a separate database and be as self-sufﬁcient as it\n\nproviding you with a reliable history log of all data changes that can be\n\ncan. From a design point of view - it’s the simplest way to avoid conﬂicts. Remember -\n\ndifferent teams are working on different parts of the application. Having a common\n\nrolled back and forth. Eventual Consistency and CAP theorem are\n\ndatabase is like having a single point of failure with all conﬂicting changes deployed\n\nfundamentals that must be considered during the design phase.\n\nsimultaneously between services.\n\n23\n\nGo to Table of Contents",
      "content_length": 2186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Rely on Contracts Between Services\n\nKeep all code at a similar level of maturity and stability. When you have to\n\nmodify the behaviour of a currently deployed (and stable) microservice,\n\nit’s usually better to put the new logic into a new, separate service. It’s\n\nsometimes called “immutable architecture”.\n\nAnother point here is that you should maintain similar, speciﬁc\n\nrequirements for all microservices like data formats, enumerating return\n\nvalues and describing error handling.\n\nMicroservices should comply with SRP (Single Responsibility Principle)\n\nand LSP (Liskov Substitution Principle).\n\nDeploy in Containers\n\nDeploying microservices in containers is important because it means you\n\nneed just one tool to deploy everything. As long as the microservice is in\n\na container, the tool knows how to deploy it. It doesn’t matter what the\n\ncontainer is. That said, Docker seems to have become the de facto\n\nstandard for containers very quickly.\n\nGo to Table of Contents\n\n24",
      "content_length": 978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Rely on Contracts Between Services\n\nCOMPOSE\n\n.yml Description\n\nDocker CLI\n\nKeep all code at a similar level of maturity and stability. When you have to\n\nmodify the behaviour of a currently deployed (and stable) microservice,\n\nContainer\n\nContainer\n\nContainer\n\nit’s usually better to put the new logic into a new, separate service. It’s\n\nsometimes called “immutable architecture”.\n\nSWARM\n\nAnother point here is that you should maintain similar, speciﬁc\n\nNode\n\nNode\n\nNode\n\nNode\n\nNode\n\nNode\n\nrequirements for all microservices like data formats, enumerating return\n\nSwarm\n\nSwarm\n\nvalues and describing error handling.\n\nMicroservices should comply with SRP (Single Responsibility Principle)\n\nand LSP (Liskov Substitution Principle).\n\nCLUSTER MANAGERS\n\nCluster Manager 1\n\nCluster Manager 2\n\nFig. 5: Source - Docker Blog. Docker Swarm manages the whole server cluster -\n\nautomatically deploying new machines with additional instances for scalability and high\n\nDeploy in Containers\n\navailability. Of course it can be deployed on popular cloud environments like Amazon.\n\nDeploying microservices in containers is important because it means you\n\nTreat Servers as Volatile\n\nneed just one tool to deploy everything. As long as the microservice is in\n\na container, the tool knows how to deploy it. It doesn’t matter what the\n\ncontainer is. That said, Docker seems to have become the de facto\n\nTreat servers, particularly those that run customer-facing code, as\n\nstandard for containers very quickly.\n\ninterchangeable members of a group. It’s the only way to successfully use\n\nthe cloud’s “auto scaling” feature.\n\nThey all perform the same function, so you don’t need to be concerned\n\nwith them individually. The role conﬁguration across servers must be\n\naligned and the deployment process should be fully automated.\n\n25\n\nGo to Table of Contents",
      "content_length": 1830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "A monolithic application puts all its functionality into a single process...\n\nA microservices architecture puts each element of functionality into a separate service ...\n\n... and scales by replicating the monolith on multiple servers\n\n... and scales by distributing these services across servers, replicating as needed\n\nFig. 6: Original idea - Martin Fowler\n\n(https://martinfowler.com/articles/microservices.html). Scaling microservices can be\n\nefﬁcient because you can add resources directly where needed. You don’t have to deal\n\nwith storage replication, sticky sessions and all that kind of stuff because services are\n\nstateless and loosely-coupled by design.\n\nGo to Table of Contents\n\n26",
      "content_length": 691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "A microservices\n\narchitecture puts each\n\nA monolithic application\n\nelement of functionality\n\nputs all its functionality\n\ninto a separate service ...\n\ninto a single process...\n\n... and scales by distributing\n\n... and scales by\n\nthese services across servers,\n\nreplicating the monolith\n\nreplicating as needed\n\non multiple servers\n\nFig. 6: Original idea - Martin Fowler\n\n(https://martinfowler.com/articles/microservices.html). Scaling microservices can be\n\nefﬁcient because you can add resources directly where needed. You don’t have to deal\n\nwith storage replication, sticky sessions and all that kind of stuff because services are\n\nstateless and loosely-coupled by design.\n\nRelated techniques and patterns\n\n27\n\nGo to Table of Contents",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Related Techniques and Patterns\n\nThis eBook is intended to give you a quick-start, practical overview of the\n\nmicroservices approach. I believe, once interested in the topic, you can\n\nﬁnd additional sources to dig into. In this chapter I would like to mention\n\njust a few programming techniques and design patterns which have\n\nbecome popular with microservices gaining the spotlight. We want to\n\ncover the full scope of building microservices and tools that can be\n\nparticularly useful to that goal.\n\nCAP theorem\n\nAlso called “Brewer theorem” after Eric Brewer, states that, for distributed\n\nsystems it’s not possible to provide more than two of the following three\n\nguarantees:\n\nConsistency - every read receives the most recent data or error.\n\nAvailability - every request receives a (non-error) response BUT without\n\na guarantee of most-recent data.\n\nPartition tolerance - interpreted as a system able to work despite the\n\nnumber of dropped messages between cluster nodes.\n\nIn other words - when it comes to communication issues (partition of the\n\ncluster), you must choose between consistency or availability. This is\n\nstrongly connected with techniques of high availability like caching and\n\ndata redundancy (eg. database replication).\n\nGo to Table of Contents\n\n28",
      "content_length": 1269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "When the system is running normally - both availability and consistency\n\nRelated Techniques and Patterns\n\ncan be provided. In case of failure, you get two choices:\n\nThis eBook is intended to give you a quick-start, practical overview of the\n\nRaise an error (and break the availability promise) because it’s not\n\nmicroservices approach. I believe, once interested in the topic, you can\n\nguaranteed that all data replicas are updated.\n\nﬁnd additional sources to dig into. In this chapter I would like to mention\n\njust a few programming techniques and design patterns which have\n\nProvide the user with cached data (due to the very same reason as\n\nbecome popular with microservices gaining the spotlight. We want to\n\nabove).\n\ncover the full scope of building microservices and tools that can be\n\nparticularly useful to that goal.\n\nTraditional database systems (compliant with ACID6 ) prefer consistency\n\nover availability.\n\nCAP theorem\n\nEventual consistency\n\nAlso called “Brewer theorem” after Eric Brewer, states that, for distributed\n\nsystems it’s not possible to provide more than two of the following three\n\nWhen the system is running normally - both availability and consistency\n\nguarantees:\n\ncan be provided. In case of failure, you get two choices:\n\nConsistency - every read receives the most recent data or error.\n\nIt’s not a programming technique but rather something you have to think\n\nAvailability - every request receives a (non-error) response BUT without\n\nabout when designing distributed systems. This consistency model is\n\na guarantee of most-recent data.\n\nconnected directly to the CAP theorem and informally guarantees that if\n\nPartition tolerance - interpreted as a system able to work despite the\n\nno new updates are made to a given data item, eventually all access\n\nnumber of dropped messages between cluster nodes.\n\nto that item will return the last updated value.\n\nIn other words - when it comes to communication issues (partition of the\n\nEventually consistent services are often classiﬁed as providing BASE\n\ncluster), you must choose between consistency or availability. This is\n\n(Basically Available, Soft state, Eventual consistency) semantics, in\n\nstrongly connected with techniques of high availability like caching and\n\ncontrast to traditional ACID guarantees.\n\ndata redundancy (eg. database replication).\n\n6 https://en.wikipedia.org/wiki/ACID\n\n29\n\nGo to Table of Contents",
      "content_length": 2397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "To achieve eventual consistency, the distributed system must resolve data\n\nconﬂicts between multiple copies of replicated data. This usually consists\n\nof two parts:\n\nExchanging updates between servers in a cluster\n\nChoosing the ﬁnal state.\n\nThe widespread model for choosing the ﬁnal state is “last writer wins” -\n\nachieved by including an update timestamp along with an updated copy\n\nof data.\n\nDesign patterns\n\nHaving knowledge of the core theories that underpin the issues which we\n\nmay encounter when developing and designing a distributed\n\narchitecture, we can now go into higher-level concepts and patterns.\n\nDesign patterns are techniques that allow us to compose code of our\n\nmicroservices in a more structured way and facilitate further maintenance\n\nand development of our platform.\n\nCQRS\n\nCQRS means Command-Query Responsibility Segregation. The core idea\n\nbehind CQRS is the extension of the CQS concept by Bertrand Meyer,\n\nwhere objects have two types of methods. Command methods perform\n\nactions in systems and always return nothing, query methods return\n\nvalues and they have no effect on the system.\n\nGo to Table of Contents\n\n30",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "To achieve eventual consistency, the distributed system must resolve data\n\nIn CQRS, write requests (aka commands) and read requests (aka queries)\n\nconﬂicts between multiple copies of replicated data. This usually consists\n\nare separated into different models. The write model will accept\n\nof two parts:\n\ncommands and perform actions on the data, the read model will accept\n\nqueries and return data to the application UI. The read model should be\n\nupdated if, and only if, the write model was changed. Moreover, single\n\nExchanging updates between servers in a cluster\n\nchanges in the write model may cause updates in more than one read\n\nChoosing the ﬁnal state.\n\nmodel. What is very interesting is that there is a possibility to split data\n\nstorage layers, set up a dedicated data store for writes and reads, and\n\nThe widespread model for choosing the ﬁnal state is “last writer wins” -\n\nmodify and scale them independently.\n\nachieved by including an update timestamp along with an updated copy\n\nof data.\n\nFor example, all write requests in the eCommerce application, like adding\n\na new order or product reviews, can be stored in a typical SQL database\n\nDesign patterns\n\nbut some read requests, like ﬁnding similar products, can be delegated\n\nby the read model to a graph engine.\n\nHaving knowledge of the core theories that underpin the issues which we\n\nGeneral ﬂow in CQRS application:\n\nmay encounter when developing and designing a distributed\n\narchitecture, we can now go into higher-level concepts and patterns.\n\nApplication creates a command as a result of user action.\n\nDesign patterns are techniques that allow us to compose code of our\n\nCommand is processed, write model saves changes in data store.\n\nmicroservices in a more structured way and facilitate further maintenance\n\nRead model is updated based on changes in write model.\n\nand development of our platform.\n\nPros:\n\nCQRS\n\nBetter scalability and performance.\n\nSimple queries and commands.\n\nCQRS means Command-Query Responsibility Segregation. The core idea\n\nPossibility to use different data storage and theirs functionalities.\n\nbehind CQRS is the extension of the CQS concept by Bertrand Meyer,\n\nWorks well in complex domains.\n\nwhere objects have two types of methods. Command methods perform\n\nactions in systems and always return nothing, query methods return\n\nvalues and they have no effect on the system.\n\n31\n\nGo to Table of Contents",
      "content_length": 2400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Cons:\n\nIncreased complexity of the entire system.\n\nEventually consistent, read model may be out of sync with write model\n\nfor a while.\n\nPossible data and code duplication.\n\nService Interface\n\nQuery Model\n\nquery model reads from database\n\ncommand model updates database\n\nCommand Model\n\napplication routes change information to command model\n\ncommand model executes validations, and consequential logic\n\nquery services update presentations from query model\n\nuser makes a change in the UI\n\nGo to Table of Contents\n\n32",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Cons:\n\nIncreased complexity of the entire system.\n\nUI\n\nEventually consistent, read model may be out of sync with write model\n\nfor a while.\n\nFig. 9: CQRS architecture (https://martinfowler.com/bliki/images/cqrs/cqrs.png).\n\nPossible data and code duplication.\n\nService Interface\n\nEvent Sourcing\n\nQuery Model\n\nData stores are often designed to directly keep the actual state of the\n\nsystem without storing the history of all the submitted changes. In some\n\nquery\n\nsituations this can cause problems. For example, if there is a need to\n\nmodel\n\nreads from\n\nprepare a new read model for some speciﬁc point of time (like your\n\ndatabase\n\ncurrent address on an invoice from 3 months ago - which may have\n\nchanged in the meantime - and you haven’t stored the time-stamped data\n\napplication\n\nsnapshots, it will be a big deal to reprint or modify the correct document).\n\nroutes\n\nchange\n\nCommand Model\n\ninformation\n\ncommand\n\nto command\n\nEvent Sourcing stores all changes as a time-ordered sequence of events;\n\nmodel\n\nmodel\n\nupdates\n\neach event is an object that represents a domain action from the past. All\n\ndatabase\n\nevents published by the application object persist inside a dedicated,\n\ncommand model\n\nexecutes validations,\n\nappend-only data store called Event Store. This is not just an audit-log for\n\nand consequential\n\nlogic\n\nthe whole system because the main role of Event Store is to reconstruct\n\napplication objects based on the history of the related events.\n\nquery services update\n\nuser makes a change\n\npresentations from query\n\nin the UI\n\nmodel\n\n33\n\nGo to Table of Contents",
      "content_length": 1573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "PRESENTATION\n\nSome options for consuming events\n\nCart\n\nItem 1 added\n\nCart ID\n\nCart Item\n\nDate\n\nCart ID\n\nItem 2 added\n\nItem 1 removed\n\nCustomer\n\nAddress\n\n...\n\nItem key\n\nItem name\n\nQuantity\n\nEXTERNAL SYSTEMS AND APPLICATIONS\n\n...\n\nShipping information added\n\nMATERLIALIZED VIEW\n\nPublished events\n\nPersisted events\n\nReplayed events\n\nQUERY FOR CURRENT STATE OF ENTITIES\n\nEvent store\n\nFig. 10: Event Sourcing overview\n\n(https://docs.microsoft.com/en-us/azure/architecture/patterns/_images/event-sourcing-o\n\nverview.png).\n\nConsider the following sequence of domain events, regarding each\n\nOrder lifecycle:\n\nOrderCreated\n\nOrderApproved\n\nOrderPaid\n\nOrderPrepared\n\nOrderShipped\n\nOrderDelivered\n\nGo to Table of Contents\n\n34",
      "content_length": 713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "During the recreation phase, all events are fetched from the EventStore\n\nand applied to a newly constructed entity. Each applied event changes\n\nPRESENTATION\n\nthe internal state of the entity.\n\nSome options for\n\nconsuming events\n\nCart\n\nCart Item\n\nCart ID\n\nItem 1 added\n\nCart ID\n\nThe beneﬁts of this approach are obvious. Each event represents an\n\nDate\n\nItem key\n\nItem 2 added\n\nCustomer\n\nEXTERNAL\n\naction, which is even better if DDD is used in the project. There is a trace\n\nItem name\n\nSYSTEMS AND\n\nAddress\n\nAPPLICATIONS\n\nItem 1 removed\n\nQuantity\n\nof every single change in domain entities.\n\n...\n\n...\n\nShipping information added\n\nMATERLIALIZED VIEW\n\nBut there are also some potential drawbacks here… How can we get the\n\nPublished events\n\ncurrent states of tens of objects? How fast will object recreation be if the\n\nPersisted\n\nReplayed events\n\nQUERY FOR CURRENT\n\nevents\n\nevents list contains thousands of items?\n\nSTATE OF ENTITIES\n\nEvent store\n\nFig. 10: Event Sourcing overview\n\nFortunately, the Event Sourcing technique has prepared solutions to\n\n(https://docs.microsoft.com/en-us/azure/architecture/patterns/_images/event-sourcing-o\n\nthese problems. Based on the events, the application can update one or\n\nverview.png).\n\nmore from materialized views, so there is no need to fetch all objects from\n\nthe event history to get their current states.\n\nConsider the following sequence of domain events, regarding each\n\nOrder lifecycle:\n\nIf the event history of the entity is long, the application may also create\n\nOrderCreated\n\nsome snapshots. By “snapshot”, I mean the state of the entity after every\n\nOrderApproved\n\nn-th event. The recreation phase will be much faster because there is no\n\nOrderPaid\n\nneed to fetch all the changes from the Event Store, just the latest\n\nOrderPrepared\n\nsnapshot and further events.\n\nOrderShipped\n\nOrderDelivered\n\n35\n\nGo to Table of Contents",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "User Interface\n\nCommand Bus\n\nCommand Handler\n\nDomain Model\n\nDomain Model\n\nDomain Model\n\nCommand Handler\n\ns u B t n e v E\n\nEvent store\n\nFig. 11: Event Sourcing with CQRS\n\n(https://pablocastilla.ﬁles.wordpress.com/2014/09/cqrs.png?w=640).\n\nGo to Table of Contents\n\nQuery Facade\n\nThin Data Layer\n\nData\n\nEventHandler\n\n36",
      "content_length": 316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Event Sourcing works very well with CQRS and Event Storming, a\n\nUser Interface\n\ntechnique for domain event identiﬁcation by Alberto Brandolini. Events\n\nfound with domain experts will be published by entities inside the write\n\nmodel. They will be transferred to a synchronous or asynchronous event\n\nbus and processed by event handlers. In this scenario, event handlers will\n\nCommand Bus\n\nQuery Facade\n\nbe responsible for updating one or more read models.\n\nPros:\n\nCommand Handler\n\nThin Data Layer\n\nPerfect for modeling complex domains.\n\nPossibility to replay all stored events and build new read models.\n\nReliable audit-log for free.\n\nDomain\n\nDomain\n\nModel\n\nModel\n\nCons:\n\nData\n\nDomain\n\nModel\n\nQueries implemented with CQRS.\n\nEventually consistent model.\n\ns\n\nu\n\nB\n\nCommand Handler\n\nEventHandler\n\nt\n\nn\n\nEvent driven data management\n\ne\n\nv\n\nE\n\nMicroservices should be coupled as loosely as possible, It should be\n\npossible to develop, test, deploy and scale them independently.\n\nEvent\n\nSometimes an application should even be able to work without particular\n\nstore\n\nservices (to comply with HA - high availability)… To achieve these\n\nrequirements, each microservice should have a separate data store.\n\nFig. 11: Event Sourcing with CQRS\n\nSounds easy - but what about the data itself? How to spread the\n\n(https://pablocastilla.ﬁles.wordpress.com/2014/09/cqrs.png?w=640).\n\ninformation changes between services? What about consistency within\n\nthe data?\n\n37\n\nGo to Table of Contents",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "One of the best solutions is simply using events. If anything important\n\nhappened inside a microservice, a speciﬁc event is published to the\n\nmessage broker. Other microservices may connect to the message broker,\n\nreceive, and consume a dedicated copy of that message. Consumers may\n\nalso decide which part of the data should be duplicated to their local\n\nstore.\n\nSafe publishing of events from the microservice is quite complicated.\n\nEvents must be published to the message broker if, and only if, data\n\nstored in a data store has changed. Other scenarios may lead to huge\n\nconsistency problems. Usually it means that data and events should\n\npersist inside the same transaction to a single data store and then\n\npropagate to the rest of the system.\n\nSwitching from theory to a practical point of view, it’s quite a common\n\ncase to use RabbitMQ as a message broker. RabbitMQ is a very fast and\n\nefﬁcient queue server written in Erlang with wide set of client libraries for\n\nthe most popular programming languages. A popular alternative to\n\nRabbitMQ is Apache Kafka, especially for bigger setups or when event\n\nstream mining and analytics is critical.\n\nSpreading data across multiple separated data stores and achieving\n\nconsistency using events can cause some problems. For example, there is\n\nno easy way to execute a distributed transaction on different databases.\n\nMoreover, there can also be consistency issues because when events are\n\ninside the message broker, somewhere between microservices, the state\n\nof the whole system is inconsistent. The data store behind the original\n\nmicroservice is updated but changes aren’t applied on data stores behind\n\nother microservices. This model, called Eventually Consistent,\n\nGo to Table of Contents\n\n38",
      "content_length": 1747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "One of the best solutions is simply using events. If anything important\n\nis a Data will be synchronized in the future but you can also stop some\n\nhappened inside a microservice, a speciﬁc event is published to the\n\nservices and you will never lose your data. They will be processed when\n\nmessage broker. Other microservices may connect to the message broker,\n\nservices are restored.\n\nreceive, and consume a dedicated copy of that message. Consumers may\n\nalso decide which part of the data should be duplicated to their local\n\nIn some situations, when a new microservice is introduced, there is a need\n\nstore.\n\nto seed the database. If there is a chance to use data directly from\n\ndifferent „sources of truth”, it’s probably the best way to setup a new\n\nSafe publishing of events from the microservice is quite complicated.\n\nservice. But other microservices may also expose feeds of theirs events,\n\nEvents must be published to the message broker if, and only if, data\n\nfor example in the form of ATOM feeds. New microservices may process\n\nstored in a data store has changed. Other scenarios may lead to huge\n\nthem in chronological order, to compile the ﬁnal state of new data stores.\n\nconsistency problems. Usually it means that data and events should\n\nOf course, in this scenario each microservice should keep a history of all\n\npersist inside the same transaction to a single data store and then\n\nevents, which can sometimes be a subsequent challenge.\n\npropagate to the rest of the system.\n\nIntegration techniques\n\nSwitching from theory to a practical point of view, it’s quite a common\n\ncase to use RabbitMQ as a message broker. RabbitMQ is a very fast and\n\nefﬁcient queue server written in Erlang with wide set of client libraries for\n\nSystem\n\nintegration\n\nis key\n\nto developing efﬁcient microservices\n\nthe most popular programming languages. A popular alternative to\n\narchitecture. Services must talk to each other in a consistent way. The\n\nRabbitMQ is Apache Kafka, especially for bigger setups or when event\n\noverall structure of a platform could be easily discoverable by hiding all of\n\nstream mining and analytics is critical.\n\nthe dependencies behind facades like a common API gateway.\n\nSpreading data across multiple separated data stores and achieving\n\nMoreover, all of\n\nthat communication should use authentication\n\nconsistency using events can cause some problems. For example, there is\n\nmechanisms as microservices are commonly exposed to the outside\n\nno easy way to execute a distributed transaction on different databases.\n\nworld. They should not be designed with the intention of residing only in\n\nMoreover, there can also be consistency issues because when events are\n\nour ﬁrewall-protected network. We show two possible ways of making our\n\ninside the message broker, somewhere between microservices, the state\n\nintegration secure by using token based techniques such as OAuth2 and\n\nof the whole system is inconsistent. The data store behind the original\n\nJWT.\n\nmicroservice is updated but changes aren’t applied on data stores behind\n\nother microservices. This model, called Eventually Consistent,\n\n39\n\nGo to Table of Contents",
      "content_length": 3144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "API Gateways\n\nWith the microservices approach, it’s quite easy to make internal network\n\ncommunication very talkative. Nowadays, when 10G network connections\n\nare standard in data-centers, there may be nothing wrong with that. But\n\nwhen it comes to communication between your mobile app and backend\n\nservices, you might want to compress as much information as possible\n\ninto one request.\n\nThe second reason to criticise microservices might be a challenge with\n\nadditional sub-service calls like authorization, ﬁltering etc.\n\nTo overcome the mentioned obstacles, we can use the API Gateway\n\napproach. It means you can compile several microservices using one\n\nfacade. It combines multiple responses from internal sub-services into a\n\nsingle response.\n\nWith almost no business logic included, gateways are an easy and safe\n\nchoice to optimize communication between frontend and backend or\n\nbetween different backend systems.\n\nGo to Table of Contents\n\n40",
      "content_length": 950,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "API Gateways\n\nView\n\nController\n\nSingle entry poiont\n\nWith the microservices approach, it’s quite easy to make internal network\n\nProduct Info Service\n\nREST\n\nModel\n\ncommunication very talkative. Nowadays, when 10G network connections\n\nare standard in data-centers, there may be nothing wrong with that. But\n\nTraditional server-side web application\n\nAPI Gateway\n\nRecommendation Service\n\nwhen it comes to communication between your mobile app and backend\n\nREST\n\nservices, you might want to compress as much information as possible\n\ninto one request.\n\nView\n\nController\n\nClient specific APIs\n\nReview Service\n\nAMQP\n\nProtocol translation\n\nModel\n\nThe second reason to criticise microservices might be a challenge with\n\nBrowser/Native App\n\nadditional sub-service calls like authorization, ﬁltering etc.\n\nFig. 12: Using an API gateway you can compose your sub-service calls into easy to\n\nunderstand and easy to use facades. Trafﬁc optimization, caching and authorization are\n\nadditional beneﬁts of such an approach\n\nTo overcome the mentioned obstacles, we can use the API Gateway\n\napproach. It means you can compile several microservices using one\n\nThe API Gateway - which is an implementation of classic Proxy patterns -\n\nfacade. It combines multiple responses from internal sub-services into a\n\ncan provide a caching mechanism as well (even using a vanilla-Varnish\n\nsingle response.\n\ncache layer without additional development effort). With this feature\n\nalone, using cloud approaches (like Amazon solutions), can scale API and\n\nservices very easily.\n\nWith almost no business logic included, gateways are an easy and safe\n\nchoice to optimize communication between frontend and backend or\n\nAdditionally, you can provide common authorization layers for all services\n\nbetween different backend systems.\n\nbehind the gateway. For example - that’s how Amazon API Gateway\n\nService7 + Amazon Cogito8 work.\n\n7 https://aws.amazon.com/api-gateway/\n\n8 http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-ﬂow.html\n\n41",
      "content_length": 2015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Mobile Apps\n\nAWS Lambda functions\n\nReceive incoming request\n\nCheck for Item in dedicated cache\n\nCheck throttling configuration\n\nCheck current RPS rate\n\nExecute backend call\n\nEndpoints on Amazon EC2\n\nWebsites\n\nInternet\n\nIf found return cached Item\n\nIf above allowed ratio return 429\n\nAny other publicly accessible endpoint\n\nServices\n\nAmazon Cloud Watch\n\nFig. 13: Amazon API Gateway request workﬂow\n\nhttps://aws.amazon.com/api-gateway/details/). Amazon gateway supports caching and\n\nauthorization features in spite of your web-service internals.\n\nSwagger9 can help you, once a Gateway has been built, with direct\n\nintegration and support to Amazon services.\n\nBackend for Frontends\n\nA typical example of an API Gateway is the backend for frontends (BFF)\n\npattern. It is about facades and compiling several microservices into\n\noptimized / device or channel-oriented API services. Its microservice\n\ndesign pattern was proposed by Sam Newman of Thought Works (author\n\nof “Building Microservices”): to create single purpose edge APIs for\n\nfrontends and other parties.\n\nCreating such a facade-API brings at least two beneﬁts to your\n\napplication:\n\nIf you manage to have a few micro services behind your facade, you can\n\navoid network latency - which is especially important on mobile devices.\n\n9 http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-ﬂow.html\n\n42",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Using a facade, you can hide all network trafﬁc between services\n\nexecuting the sub-calls in internal networks from the end-client.\n\nMobile Apps\n\nAWS Lambda\n\nfunctions\n\nReceive incoming\n\nCheck throttling\n\nrequest\n\nconfiguration\n\nExecute\n\nThen you can optimize your calls to be more compliant with a speciﬁc\n\nEndpoints on\n\nCheck for Item in\n\nCheck current RPS\n\nbackend\n\nAmazon EC2\n\ndedicated cache\n\nrate\n\ncall\n\nIf above allowed ratio\n\nIf found return cached\n\nWebsites\n\nInternet\n\ndomain model. You can model the API structures by merging and\n\nreturn 429\n\nItem\n\ndistributing subsequent service calls instead of pushing this logic to the\n\nAny other publicly\n\naccessible endpoint\n\nAPI client’s code.\n\nServices\n\nAmazon Cloud Watch\n\nThe diagram below shows a migration from General Purpose API to a\n\nFig. 13: Amazon API Gateway request workﬂow\n\nhttps://aws.amazon.com/api-gateway/details/). Amazon gateway supports caching and\n\ndedicated backends for frontends approach which integrates the\n\nauthorization features in spite of your web-service internals.\n\nsub-services into logic.\n\nSwagger9 can help you, once a Gateway has been built, with direct\n\nTeam A\n\nTeam B\n\nintegration and support to Amazon services.\n\nMobile App\n\niOS App\n\nAndroid App\n\nBackend for Frontends\n\nMobile Team\n\nWeb Team\n\nA typical example of an API Gateway is the backend for frontends (BFF)\n\npattern. It is about facades and compiling several microservices into\n\niOS BFF\n\nAndroid BFF\n\noptimized / device or channel-oriented API services. Its microservice\n\nGeneral Purpose Server-side API\n\ndesign pattern was proposed by Sam Newman of Thought Works (author\n\nAPI Team\n\nof “Building Microservices”): to create single purpose edge APIs for\n\nInventory\n\nWishlist\n\nCatalog\n\nfrontends and other parties.\n\nTeam C\n\nTeam D\n\nCreating such a facade-API brings at least two beneﬁts to your\n\napplication:\n\nTeam A\n\nTeam B\n\nFig. 14: Backend for frontends architecture is about minimizing the number of backend\n\nIf you manage to have a few micro services behind your facade, you can\n\ncalls and optimizing the interfaces to a supported device.\n\navoid network latency - which is especially important on mobile devices.\n\n9 http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-ﬂow.html\n\n43\n\nGo to Table of Contents",
      "content_length": 2274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "There are many approaches to separate backend for frontends and\n\nroughly speaking it always depends on the differences in data required by\n\na speciﬁc frontend, or usage-patterns behind speciﬁc API clients. One can\n\nimagine a separate API for frontend, mobile apps - as well as separate\n\ninterfaces for iOS and Android if there are any differences between these\n\napplications regarding how service calls are made or their respective data\n\nformats.\n\nOne of the concerns of having a single BFF per user interface is that you\n\ncan end up with lots of code duplication between the BFFs themselves.\n\nPete Hodgson (ex. Thought Works) suggests that BFFs work best when\n\norganized around teams. The team structure should drive how many BFFs\n\nyou have. This is a pragmatic approach to not over-engineer your system\n\nbut rather have one mobile API if you have one mobile team etc.\n\nIt’s then a common pattern to separate shared algorithms, models and\n\ncode to separate the shared service or library used by frontend-related\n\nfacades. Creating such duplications can be avoided.\n\nLet me quote a conclusion on BFF presented by Sam Newman himself:\n\nBackends For Frontends solve a pressing concern\n\nfor mobile\n\ndevelopment when using microservices. In addition, they provide a\n\ncompelling alternative to the general-purpose API backend, and many\n\nteams make use of them for purposes other than\n\njust mobile\n\ndevelopment. The simple act of limiting the number of consumers they\n\nsupport makes them much easier to work with and change, and helps\n\nteams developing customer-facing applications retain more autonomy10.\n\n10 http://samnewman.io/patterns/architectural/bff/\n\nGo to Table of Contents\n\n44",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Token based authorization (oauth2, JWT)\n\nThere are many approaches to separate backend for frontends and\n\nroughly speaking it always depends on the differences in data required by\n\na speciﬁc frontend, or usage-patterns behind speciﬁc API clients. One can\n\nAuthorization is a key feature of any enterprise grade application. If you\n\nimagine a separate API for frontend, mobile apps - as well as separate\n\nremember the beginnings of web 2.0 and Web API’s back then, a typical\n\ninterfaces for iOS and Android if there are any differences between these\n\nauthorization scenario was based on an API key or HTTP authorization.\n\napplications regarding how service calls are made or their respective data\n\nWith ease of use came some strings attached. Basically these “static”\n\nformats.\n\n(API key) and not strongly encrypted (basic auth.) methods were not\n\nsecure enough.\n\nOne of the concerns of having a single BFF per user interface is that you\n\ncan end up with lots of code duplication between the BFFs themselves.\n\nHere, delegated authorization methods come into action. By delegated,\n\nwe mean that authorization can be given by an external system / identity\n\nPete Hodgson (ex. Thought Works) suggests that BFFs work best when\n\nprovider. One of the ﬁrst methods of providing such authentication was\n\norganized around teams. The team structure should drive how many BFFs\n\nthe OpenID standard11 developed around 2005. It could provide a One\n\nyou have. This is a pragmatic approach to not over-engineer your system\n\nLogin and Single Sign On for any user. Unfortunately, it wasn’t widely\n\nbut rather have one mobile API if you have one mobile team etc.\n\naccepted by identiﬁcation providers like Google, Facebook or e-mail\n\nproviders.\n\nIt’s then a common pattern to separate shared algorithms, models and\n\ncode to separate the shared service or library used by frontend-related\n\nThe OAuth standard works pretty similarly to OpenID. The authorization\n\nfacades. Creating such duplications can be avoided.\n\nprovider allows Application Developers to register their own applications\n\nwith the required data-scope to be obtained in the name of the user. The\n\nLet me quote a conclusion on BFF presented by Sam Newman himself:\n\nuser authorizes speciﬁc applications to use with their account.\n\nBackends For Frontends solve a pressing concern\n\nfor mobile\n\nFacebook or Google Account login screens are a well known part of oauth\n\ndevelopment when using microservices. In addition, they provide a\n\nauthorization.\n\ncompelling alternative to the general-purpose API backend, and many\n\nteams make use of them for purposes other than\n\njust mobile\n\ndevelopment. The simple act of limiting the number of consumers they\n\nsupport makes them much easier to work with and change, and helps\n\nteams developing customer-facing applications retain more autonomy10.\n\n10 http://samnewman.io/patterns/architectural/bff/\n\n11 http://openid.net/\n\n45\n\nGo to Table of Contents",
      "content_length": 2931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Fig. 15: Authorization screen for Google Accounts to authorize external application to\n\nuse Google APIs in the name of the user.\n\nAfter accepting the application request the authority party returns a\n\ntemporary Access Token which should be used with API calls to verify the\n\nuser identity. The Internal Authorization server checks tokens with its own\n\ndatabase of issued tokens - paired with user identities, ACLs, etc.\n\nAuthorization tokens are issued for a speciﬁc amount of time and should\n\nbe invalidated afterwards. Token authorization is 100% stateless; you\n\ndon’t have to use sessions (like with good, old session based\n\nauthorization)12. OAuth 2.0 requires SSL communication and avoids\n\nadditional request-response signatures required by the previous version\n\n(requests were signed using HMAC algorithms); also, the workﬂow was\n\nsimpliﬁed with 2.0 removing one additional HTTP request.\n\n12 http://stackoverﬂow.com/questions/7561631/oauth-2-0-beneﬁts-and-use-cases-why\n\nGo to Table of Contents\n\n46",
      "content_length": 1004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "BROWSER\n\nAPPLICATION\n\nAUTHORIZATION SERVER\n\nRESOURCE SERVER\n\n1: Request application page\n\n1.1: Redirect to Authorization Server\n\n2: Request Login\n\n2.1: Deliver Login page\n\n3: Enter Login details and authorization access\n\n4: Send Login details\n\n5: Validate Login details\n\n6: Redirect to Application\n\n7: User Valid and Authorization access\n\n7.1: Get Access Token\n\nFig. 15: Authorization screen for Google Accounts to authorize external application to\n\n7.2: Return Access Token\n\nuse Google APIs in the name of the user.\n\n7.3: Get data\n\n7.3.2: Return data\n\n7.3.1: Check Access token\n\nAfter accepting the application request the authority party returns a\n\n7.4: Generate Page\n\n7.5\n\ntemporary Access Token which should be used with API calls to verify the\n\nuser identity. The Internal Authorization server checks tokens with its own\n\ndatabase of issued tokens - paired with user identities, ACLs, etc.\n\nFig. 16: Authorization ﬂow for oauth2.\n\nAuthorization tokens are issued for a speciﬁc amount of time and should\n\nOAuth tokens don’t push you to display the authentication dialog each\n\nbe invalidated afterwards. Token authorization is 100% stateless; you\n\ntime a user requires access to their data. Following this path would make\n\ndon’t have to use sessions (like with good, old session based\n\nit impossible to check e-mail in the background or do any batch\n\nauthorization)12. OAuth 2.0 requires SSL communication and avoids\n\nprocessing operations. So how to deal with such background-operations?\n\nadditional request-response signatures required by the previous version\n\nYou should use “ofﬂine” tokens13 - which are given for longer time periods\n\n(requests were signed using HMAC algorithms); also, the workﬂow was\n\nand can also be used to remember client credentials without requiring\n\nsimpliﬁed with 2.0 removing one additional HTTP request.\n\nlogin/password each time the user hits your application.\n\nThere is usually no need to rewrite your own OAuth code as many open\n\nsource libraries are available for most OAuth providers and frameworks.\n\nJust take a look on Github!\n\n13 https://auth0.com/docs/tokens/refresh-token\n\n12 http://stackoverﬂow.com/questions/7561631/oauth-2-0-beneﬁts-and-use-cases-why\n\n47\n\nGo to Table of Contents",
      "content_length": 2227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "There are SaaS solutions for identity and authorization, such as Amazon\n\nCogito14 or Auth015 that can be easily used to outsource the authorization\n\nof your API’s.\n\nJSON Web Tokens (JWT)\n\nYet another approach to token based authorization is JWT16 (JSON Web\n\nTokens). They can be used for stateless claim exchange between parties.\n\nAs OAuth tokens require validation by the authenticating party between\n\nall requests - JSON Web Tokens are designed to self-contain all\n\ninformation required and can be used without touching the database or\n\nany other data source.\n\nJWT are self-contained which means that tokens contain all the\n\ninformation. They are encoded and signed up using HMAC.\n\nThis allows you to fully rely on data APIs that are stateless and even make\n\nrequests to downstream services. It doesn't matter which domains are\n\nserving your APIs, so Cross-Origin Resource Sharing (CORS) won't be an\n\nissue as it doesn't use cookies17.\n\n14 https://aws.amazon.com/cognito/\n\n15 https://auth0.com/how-it-works\n\n16 https://jwt.io/\n\n17 https://jwt.io/introduction/\n\n48",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Validation of HMAC tokens18 requires the knowledge of the secret key\n\nThere are SaaS solutions for identity and authorization, such as Amazon\n\nused to generate the token. Typically the receiving service (your API) will\n\nCogito14 or Auth015 that can be easily used to outsource the authorization\n\nneed to contact the authentication server as that server is where the\n\nof your API’s.\n\nsecret is being kept19.\n\nPlease take a look at the example.\n\nJSON Web Tokens (JWT)\n\nYet another approach to token based authorization is JWT16 (JSON Web\n\nExample token:\n\nTokens). They can be used for stateless claim exchange between parties.\n\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxM- jM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOn RydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh 7HgQ\n\nAs OAuth tokens require validation by the authenticating party between\n\nall requests - JSON Web Tokens are designed to self-contain all\n\ninformation required and can be used without touching the database or\n\nany other data source.\n\nContains following informations: Please take a look at the example.\n\n{\n\nHeader\n\nJWT are self-contained which means that tokens contain all the\n\n\"alg\": \"HS256\",\n\n(algorithm and token type)\n\ninformation. They are encoded and signed up using HMAC.\n\n\"typ\": \"JWT\"\n\n}\n\n{\n\nPayload\n\nThis allows you to fully rely on data APIs that are stateless and even make\n\n\"sub\": \"1234567890\",\n\n(data)\n\nrequests to downstream services. It doesn't matter which domains are\n\n\"name\": \"John Doe\",\n\nserving your APIs, so Cross-Origin Resource Sharing (CORS) won't be an\n\n\"admin\": true\n\nissue as it doesn't use cookies17.\n\n}\n\nHMACSHA256(\n\nSignature\n\nbase64UrlEncode(header) + \".\" +\n\nbase64UrlEncode(payload),\n\n14 https://aws.amazon.com/cognito/\n\n) secret base64 encoded\n\n15 https://auth0.com/how-it-works\n\n16 https://jwt.io/\n\n18 https://en.wikipedia.org/wiki/Hash-based_message_authentication_code\n\n17 https://jwt.io/introduction/\n\n19 https://jwt.io/introduction/\n\n49\n\nGo to Table of Contents",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "JWT tokens are usually passed by the HTTP Bearer header, then stored\n\nclient side using localStorage or any other resource. Tokens can be\n\ninvalidated at that time (exp claim included into token).\n\nOnce returned from authorization, service tokens can be passed to all API\n\ncalls and validated server side. Because of the HMAC based signing\n\nprocess, tokens are safe.\n\nBROWSER\n\nSERVER\n\n1. POST /users/login with username and password\n\n3. Returns the JWT to the Browser\n\n2. Creates a JWT with a secret\n\n4. Sends the JWT on the Authorization Header\n\n6. Sends response to the client\n\n5. Check JWT signature. Get user information from the JWT\n\nFig. 17: JWT based authorization is pretty straight forward and it’s safe. Tokens can be\n\ntrusted by authorized parties because of the HMAC signature; therefore information\n\ncontained by them can be used without checking ACL’s and any further permissions.\n\nDeployment of microservices\n\nIf done wrong, microservices may come with an overhead of operational\n\ntasks needed for the deployments and maintenance. When dividing a\n\nmonolithic platform into smaller pieces, each of them should be easy to\n\ndeploy in an automatic way.\n\n18 https://en.wikipedia.org/wiki/Hash-based_message_authentication_code\n\n19 https://jwt.io/introduction/\n\nGo to Table of Contents\n\n50",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Microservices should be coupled as loosely as possible, It should be\n\npossible to develop, test, deploy and scale them independently.\n\nSometimes an application should even be able to work without particular\n\nservices (to comply with HA - high availability)… To achieve these\n\nrequirements, each microservice should have a separate data store.\n\nSounds easy - but what about the data itself? How to spread the\n\ninformation changes between services? What about consistency within\n\nthe data?\n\nJWT tokens are usually passed by the HTTP Bearer header, then stored\n\nNowadays, we see two main concepts that facilitates such a process -\n\nclient side using localStorage or any other resource. Tokens can be\n\ncontainerization and serverless architecture.\n\ninvalidated at that time (exp claim included into token).\n\nDocker and containerization\n\nOnce returned from authorization, service tokens can be passed to all API\n\ncalls and validated server side. Because of the HMAC based signing\n\nIf you are not familiar with containerization, then here are the most\n\nprocess, tokens are safe.\n\ncommon beneﬁts that make it worth digging deeper into this concept:\n\nDocker allows you to build an application once and then execute it in all\n\nBROWSER\n\nSERVER\n\nyour environments no matter what the differences between them.\n\nDocker helps you to solve dependency and incompatibility issues.\n\n1. POST /users/login with username and password\n\nDocker is like a virtual machine without the overhead.\n\n2. Creates a JWT\n\nwith a secret\n\n3. Returns the JWT to the Browser\n\nDocker environments can be fully automated.\n\nDocker is easy to deploy.\n\n4. Sends the JWT on the Authorization Header\n\n5. Check JWT signature.\n\nDocker allows for separation of duties.\n\nGet user information\n\n6. Sends response to the client\n\nfrom the JWT\n\nDocker allows you to scale easily.\n\nDocker has a huge community.\n\nFig. 17: JWT based authorization is pretty straight forward and it’s safe. Tokens can be\n\ntrusted by authorized parties because of the HMAC signature; therefore information\n\ncontained by them can be used without checking ACL’s and any further permissions.\n\nLet's start with a quote from the Docker page:\n\nDocker containers wrap up a piece of software in a complete ﬁlesystem\n\nDeployment of microservices\n\nthat contains everything it needs to run: code, runtime, system tools,\n\nsystem libraries – anything you can install on a server. This guarantees that\n\nIf done wrong, microservices may come with an overhead of operational\n\nit will always run the same, regardless of the environment it is running in.\n\ntasks needed for the deployments and maintenance. When dividing a\n\nmonolithic platform into smaller pieces, each of them should be easy to\n\nThis might sound familiar: virtualization allows you to achieve pretty much\n\ndeploy in an automatic way.\n\nthe same goals but in contrast to virtualization, Docker runs all processes\n\ndirectly on the host operating system. This helps to avoid the overhead of\n\n18 https://en.wikipedia.org/wiki/Hash-based_message_authentication_code\n\na virtual machine (both performance and maintenance).\n\n19 https://jwt.io/introduction/\n\n51\n\nGo to Table of Contents",
      "content_length": 3149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Docker achieves this using the isolation features of the Linux kernel such\n\nas Cgroups and kernel namespaces. Each container has its own process\n\nspace, ﬁlesystem and memory. You can run all kinds of Linux distributions\n\ninside a container. What makes Docker really useful is the community and\n\nall projects that complement the main functionality. There are multiple\n\ntools to automate common tasks, orchestrate and scale containerized\n\nsystems. Docker is also heavily supported by many companies, just to\n\nname a couple: Amazon, Google, Microsoft. Currently, Docker also allows\n\nus to run Windows inside containers (only on Windows hosts).\n\nDocker basics\n\nBefore we dig into using Docker for the Microservices architecture let’s\n\nbrowse the top-level details of how it works.\n\nImage - holds the ﬁle system and parameters needed to run an\n\napplication. It does not have any state and it does not change. You can\n\nunderstand an image as a template used to run containers.\n\nContainer - this is a running instance of an image. You can run multiple\n\ninstances of the same image. It has a state and can change.\n\nImage layer - each image is built out of layers. Images are usually built by\n\nrunning commands or adding/modifying ﬁles (using a Dockerﬁle). Each\n\nstep that is run in order to build an Image is an image layer. Docker saves\n\neach layer, so when you run a build next time, it is able to reuse the layers\n\nthat did not change. Layers are shared between all images so if two\n\nimages start with similar steps, the layers are shared between them. You\n\ncan see this illustrated below.\n\nGo to Table of Contents\n\n52",
      "content_length": 1613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Docker achieves this using the isolation features of the Linux kernel such\n\nas Cgroups and kernel namespaces. Each container has its own process\n\nspace, ﬁlesystem and memory. You can run all kinds of Linux distributions\n\ninside a container. What makes Docker really useful is the community and\n\nall projects that complement the main functionality. There are multiple\n\ntools to automate common tasks, orchestrate and scale containerized\n\nsystems. Docker is also heavily supported by many companies, just to\n\nname a couple: Amazon, Google, Microsoft. Currently, Docker also allows\n\nus to run Windows inside containers (only on Windows hosts).\n\nDocker basics\n\nFig. 18: You can use https://imagelayers.io/ to analyze Docker image layers and compare\n\nthem to each other. For example: ruby, python, node images share ﬁve layers - this means\n\nthat if you download all three images the ﬁrst 5 layers will be downloaded only once.\n\nBefore we dig into using Docker for the Microservices architecture let’s\n\nbrowse the top-level details of how it works.\n\nAs you can see, all compared images share common layers. So if you\n\ndownload one of them, the shared layers will not be downloaded and\n\nImage - holds the ﬁle system and parameters needed to run an\n\nstored again when downloading a different image. In fact, changes in a\n\napplication. It does not have any state and it does not change. You can\n\nrunning container are also seen as an additional, uncommitted layer.\n\nunderstand an image as a template used to run containers.\n\nRegistry - a place where images and image layers are kept. You can build\n\nContainer - this is a running instance of an image. You can run multiple\n\nan image on your CI server, push it to a registry and then use the image\n\ninstances of the same image. It has a state and can change.\n\nfrom all of your nodes without the need to build the images again.\n\nImage layer - each image is built out of layers. Images are usually built by\n\nOrchestration (docker-compose) - usually a system is built of several or\n\nrunning commands or adding/modifying ﬁles (using a Dockerﬁle). Each\n\nmore containers. This is because you should have only one concern per\n\nstep that is run in order to build an Image is an image layer. Docker saves\n\ncontainer. Orchestration allows you to run a multi-container application\n\neach layer, so when you run a build next time, it is able to reuse the layers\n\nmuch easier and docker-compose is the most commonly used tool to\n\nthat did not change. Layers are shared between all images so if two\n\nachieve that. It has the ability to run multiple containers that can be\n\nimages start with similar steps, the layers are shared between them. You\n\nconnected with networks and share volumes.\n\ncan see this illustrated below.\n\n53\n\nGo to Table of Contents",
      "content_length": 2775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "VM vs. Container\n\nAs mentioned earlier, Docker might seem similar to virtual machines but\n\nworks in an entirely different way.\n\nVirtual machines work exactly as the name suggests: by creating a\n\nvirtualized machine that the guest system is using. The main part is a\n\nHypervisor running on the host system and granting access to all kinds of\n\nresources for the guest systems. On top of the Hypervisor, there are Guest\n\nOS’s running on each virtual machine. Your application is using this Guest\n\nOS.\n\nWhat Docker does differently is directly using the host system (no need\n\nfor Hypervisor and Guest OS), it runs the containers using several features\n\nof the Linux kernel that allow them to securely separate the processes\n\ninside them. Thanks to this, a process inside the container cannot\n\ninﬂuence processes outside of it. This approach makes Docker more\n\nlightweight both in terms of CPU/Memory usage, and disk space usage.\n\nApp 1\n\nApp 2\n\nApp 3\n\nBins/Libs\n\nBins/Libs\n\nBins/Libs\n\nGuest OS\n\nGuest OS\n\nGuest OS\n\nHypervisor\n\nHost Operating System\n\nInfrastructure\n\nGo to Table of Contents\n\n54",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "VM vs. Container\n\nApp 1\n\nApp 2\n\nApp 3\n\nBins/Libs\n\nBins/Libs\n\nBins/Libs\n\nAs mentioned earlier, Docker might seem similar to virtual machines but\n\nworks in an entirely different way.\n\nDocker Engine\n\nVirtual machines work exactly as the name suggests: by creating a\n\nvirtualized machine that the guest system is using. The main part is a\n\nOperating System\n\nHypervisor running on the host system and granting access to all kinds of\n\nInfrastructure\n\nresources for the guest systems. On top of the Hypervisor, there are Guest\n\nOS’s running on each virtual machine. Your application is using this Guest\n\nOS.\n\nFig. 19: Similar features, different architecture - Virtualization vs, Dockerization. Docker,\n\nWhat Docker does differently is directly using the host system (no need\n\nleverages containerization - lightweight abstraction layer between application and the\n\nfor Hypervisor and Guest OS), it runs the containers using several features\n\noperating system / hardware. It separates the user processes but without running the\n\nwhole operating system/kernel inside the container.\n\nof the Linux kernel that allow them to securely separate the processes\n\ninside them. Thanks to this, a process inside the container cannot\n\ninﬂuence processes outside of it. This approach makes Docker more\n\nFrom dev to production\n\nlightweight both in terms of CPU/Memory usage, and disk space usage.\n\nOk, so we have the technical introduction covered. Now let’s see how\n\nApp 1\n\nApp 2\n\nApp 3\n\nDocker helps to build, run and maintain a Microservice oriented\n\nBins/Libs\n\nBins/Libs\n\nBins/Libs\n\napplication.\n\nGuest OS\n\nGuest OS\n\nGuest OS\n\nDevelopment\n\nHypervisor\n\nHost Operating System\n\nDevelopment is usually the ﬁrst phase where Docker brings some extra\n\nvalue, and it is even more helpful with Microservice oriented applications.\n\nInfrastructure\n\nAs mentioned earlier, Docker comes with tools that allow us to orchestrate\n\na multi-container setup in a very easy way. Let's take a look at the beneﬁts\n\nDocker brings during development.\n\n55\n\nGo to Table of Contents",
      "content_length": 2035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Easy setup - low cost of introducing new developers\n\nYou only need to create a Docker conﬁguration once and then each new\n\ndeveloper on the team can start the project by executing a single\n\ncommand. No need to conﬁgure the environment, just download the\n\nproject and run docker-compose up. That's all!\n\nThis might seem too good to be true but I have a good, real-life example\n\nof such a situation. I was responsible for a project where a new front-end\n\ndeveloper was hired. The project was written in a very old PHP version\n\n(5.3) and had to be run on CentOS. The developer was using Windows\n\nand he previously worked on Java projects exclusively. I had a quick call\n\nwith him and we went through a couple of simple steps: downloading and\n\ninstalling Docker, cloning the git repository and running docker-compose.\n\nAfter no more than 30 minutes he had a perfectly running environment\n\nand was ready to write his ﬁrst lines of code!\n\nNo dependencies version mismatch issue\n\nThis issue often arises if a developer is involved in multiple projects, but it\n\nescalates in Micro-service oriented applications. Each service can be\n\nwritten by a different team and using different technologies. In some\n\ncases (itusually happens quite often) there might be a version mismatch\n\nwithin the same technology used in different services. A simple example:\n\none service is using an older elastic version, and another a newer one.\n\nThis can be dealt withaccomplished by conﬁguring two separate versions\n\nbut it is much easier to run them side-by-side in dedicated containers. A\n\nvery simple example of such a conﬁguration for docker-compose would\n\nlook like this:\n\nGo to Table of Contents\n\n56",
      "content_length": 1676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Easy setup - low cost of introducing new developers\n\nservice_x_elastic: image: elasticsearch:5.2.2 service_y_elastic: image: elasticsearch:2.4.4\n\nYou only need to create a Docker conﬁguration once and then each new\n\ndeveloper on the team can start the project by executing a single\n\ncommand. No need to conﬁgure the environment, just download the\n\nproject and run docker-compose up. That's all!\n\nPossibility to test if the application scales\n\nThis might seem too good to be true but I have a good, real-life example\n\nof such a situation. I was responsible for a project where a new front-end\n\nTesting if the application scales is pretty easy with Docker. Of course, you\n\ndeveloper was hired. The project was written in a very old PHP version\n\nwon't be able to make some serious load testing on your local machine,\n\n(5.3) and had to be run on CentOS. The developer was using Windows\n\nbut you can test if the application works correctly when a service is scaled\n\nand he previously worked on Java projects exclusively. I had a quick call\n\nhorizontally. Horizontal scalability usually fails if the Microservice is not\n\nwith him and we went through a couple of simple steps: downloading and\n\nstateless and the state is not shared between instances. Scaling can be\n\ninstalling Docker, cloning the git repository and running docker-compose.\n\nvery easily achieved using docker-compose:\n\nAfter no more than 30 minutes he had a perfectly running environment\n\nand was ready to write his ﬁrst lines of code!\n\ndocker-compose scale service_x=4\n\nNo dependencies version mismatch issue\n\nAfter running this command there will be four containers running the\n\nsame service_x. You can (and you should) also add a separate container\n\nThis issue often arises if a developer is involved in multiple projects, but it\n\nwith a load balancer like HAProxy in front of them. That's it. You are ready\n\nescalates in Micro-service oriented applications. Each service can be\n\nto test!\n\nwritten by a different team and using different technologies. In some\n\ncases (itusually happens quite often) there might be a version mismatch\n\nNo more “works on my conﬁguration\" issues\n\nwithin the same technology used in different services. A simple example:\n\none service is using an older elastic version, and another a newer one.\n\nDocker is a solution that allows one conﬁguration to be run everywhere.\n\nThis can be dealt withaccomplished by conﬁguring two separate versions\n\nYou can have the same - or almost the same - version running on all\n\nbut it is much easier to run them side-by-side in dedicated containers. A\n\ndeveloper machines, CI, staging, and production. This radically reduces\n\nvery simple example of such a conﬁguration for docker-compose would\n\nthe amount of “works on my conﬁguration\" situations. At least it reduces\n\nlook like this:\n\nthe ones caused by different setups.\n\n57\n\nGo to Table of Contents",
      "content_length": 2874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Continuous Integration\n\nNow that you have a working development setup, conﬁguring a CI is\n\nreally easy. You just need to setup your CI to run the same\n\ndocker-compose up command and then run your tests, etc. No need to\n\nwrite any special conﬁguration; just bring the containers up and run your\n\ntests. I've worked with different CI servers like Gitlab CI, Circle CI, Jenkins\n\nand the setup was always quick and easy. If some tests fail, it is easy to\n\ndebug too. Just run the tests locally.\n\nPre-production\n\nWhen you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\nshared between your local disk and the disk inside a container. When\n\ndeveloping an application, you usually setup containers to share all\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\neach change. On pre-production and production servers, project ﬁles\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nThe other common change applies to ports. When using Docker for\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nGo to Table of Contents\n\n58",
      "content_length": 1537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Continuous Integration\n\nSo when running on any production or pre-production servers you usually\n\nput a load balancer in front of the containers.\n\nNow that you have a working development setup, conﬁguring a CI is\n\nreally easy. You just need to setup your CI to run the same\n\nThere are many tools that make running pre-production servers much\n\ndocker-compose up command and then run your tests, etc. No need to\n\neasier. You should deﬁnitely check out projects like Docker Swarm,\n\nwrite any special conﬁguration; just bring the containers up and run your\n\nKubernetes and Rancher. I really like Rancher as it is easy to setup and\n\ntests. I've worked with different CI servers like Gitlab CI, Circle CI, Jenkins\n\nreally easy to use. We use Rancher as our main staging management tool\n\nand the setup was always quick and easy. If some tests fail, it is easy to\n\nand all co-workers really enjoy working with it. Just to give you a small\n\ndebug too. Just run the tests locally.\n\ninsight into how powerful such tools are: all our team members are able\n\nto update or create a new staging environment without any issues - and\n\nwithin a few minutes!\n\nPre-production\n\nProduction\n\nWhen you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe production conﬁguration should be exactly\n\nthe same as\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\npre-production. The only small difference might be the tool you use to\n\nshared between your local disk and the disk inside a container. When\n\nmanage the containers. There are a multitude of popular tools used to run\n\ndeveloping an application, you usually setup containers to share all\n\nproduction containers but my two favorites are Amazon EC2 Container\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\nService and Google Cloud with Kubernetes on top. Both tools allow you\n\neach change. On pre-production and production servers, project ﬁles\n\nto scale easily on new hosts.\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nOne important thing you should keep in mind when going with Docker on\n\nproduction - monitoring and logging. Both should be centralized and\n\nThe other common change applies to ports. When using Docker for\n\neasy to use.\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nCons\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nDocker has some downsides too. The ﬁrst one you might notice is that it\n\n59\n\nGo to Table of Contents",
      "content_length": 2807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "takes some time to learn how to use Docker. The basics are pretty easy to\n\nlearn, but it takes time to master some more complicated settings and\n\nconcepts. The main disadvantage for me is that it runs very slowly on\n\nMacOS and Windows. Docker is built around many different concepts\n\nfrom the Linux kernel so it is not able to run directly on MacOS or\n\nWindows. It uses a Virtual Machine that runs Linux with Docker.\n\nSummary\n\nDocker and the Microservice architecture approach work very well\n\ntogether and both concepts gain popularity each year. Over the past 4\n\nyears, we have been able to observe how Docker has gotten better and\n\nmore mature with each release. At the same time, the whole ecosystem\n\nhas grown and new tools have been published giving us more possibilities\n\nthat we could not have thought of. By using Docker, we are able to easily\n\nrun our Microservice oriented applications on our developer machines\n\nand then run the same setup on pre- and production servers. Right now\n\nwe can conﬁgure a setup within minutes and then release our application\n\nto a server also within minutes. I'm really curious about what new\n\npossibilities we will get in the coming months.\n\nServerless - Function as a Service\n\nServerless is not exclusively bound to microservice oriented applications\n\nbut it is deﬁnitely good to know this concept, as it might be helpful in\n\nmany cases.\n\nGo to Table of Contents\n\n60",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Let me start with a couple of quotes that might be helpful for you to\n\ntakes some time to learn how to use Docker. The basics are pretty easy to\n\nunderstand what serverless is about:\n\nlearn, but it takes time to master some more complicated settings and\n\nconcepts. The main disadvantage for me is that it runs very slowly on\n\nMacOS and Windows. Docker is built around many different concepts\n\nServerless is a new cloud computing trend that changes the way you think about writing and maintaining applications.\n\nfrom the Linux kernel so it is not able to run directly on MacOS or\n\nWindows. It uses a Virtual Machine that runs Linux with Docker.\n\n— AUTH0.COM\n\nDeploy your applications as independent functions, that respond to events, charge you only when they run, and scale automatically.\n\nSummary\n\nDocker and the Microservice architecture approach work very well\n\n— SERVERLESS.COM\n\ntogether and both concepts gain popularity each year. Over the past 4\n\nyears, we have been able to observe how Docker has gotten better and\n\nServerless architectures refer to (..) custom code that's run in ephemeral containers.\n\nmore mature with each release. At the same time, the whole ecosystem\n\nhas grown and new tools have been published giving us more possibilities\n\nthat we could not have thought of. By using Docker, we are able to easily\n\n— MARTINFOWLER.COM\n\nrun our Microservice oriented applications on our developer machines\n\nand then run the same setup on pre- and production servers. Right now\n\nAs you can see, each of the quotes looks at serverless from a totally\n\nwe can conﬁgure a setup within minutes and then release our application\n\ndifferent perspective. This does not mean that some of the quotes are\n\nto a server also within minutes. I'm really curious about what new\n\nbetter, I think that all describe serverless in a very good way.\n\npossibilities we will get in the coming months.\n\nServerless is considered to be a very bad name for what we are talking\n\nabout. This is for two reasons:\n\nServerless - Function as a Service\n\nServerless as a concept has a broader meaning than what it usually\n\nServerless is not exclusively bound to microservice oriented applications\n\nrefers to; Serverless architecture can be used to describe both Backend as\n\nbut it is deﬁnitely good to know this concept, as it might be helpful in\n\na Service and Function as a Service. Usually, and also in this article, we are\n\nmany cases.\n\ninterested in the latter: FaaS.\n\n61\n\nGo to Table of Contents",
      "content_length": 2477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Serverless is a lie. The truth is that servers are still there, Ops are also\n\nthere. So why is this called „serverless” - it’s called so because you, as a\n\nbusiness or as a developer, do not need to think about servers or ops.\n\nThey are hidden behind an abstraction that makes them invisible to you.\n\nBoth servers and ops are managed by a vendor like Amazon, Google,\n\nMicrosoft, etc.\n\nIn the context of microservice architecture, FaaS is the concept that is\n\ninteresting for us.\n\nServerless providers\n\nCurrently, there are 4 major Clouds that allow us to use serverless\n\narchitecture:\n\nAWS Lambda - named as the ﬁrst adopter of FaaS, easily integrates\n\nwith the rest of Amazon Web Services such as SNS or S3.\n\nGoogle Cloud Functions - still in beta, allows us to run our functions in\n\nGoogle Cloud. The drawback is, it currently only supports Node.js and\n\nJavaScript.\n\nAzure Functions - supports the widest range of languages (JavaScript,\n\nC#, F#, Python, PHP, Bash, Batch, and PowerShell) and easily allows us\n\nto integrate with Github for storing our code.\n\n\n\nIBM Bluemix OpenWhisk -\n\nit uses the open-source Apache\n\nOpenWhisk project running on top of the IBM Bluemix infrastructure.\n\nGo to Table of Contents\n\n62",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Serverless is a lie. The truth is that servers are still there, Ops are also\n\nThe most notable feature is that you can use your Docker images to run\n\nthere. So why is this called „serverless” - it’s called so because you, as a\n\nas functions. A meaningful use-case of IBM OpenWhisk is a DarkVision\n\nbusiness or as a developer, do not need to think about servers or ops.\n\nApplication20, which shows how that technology can be used with\n\nThey are hidden behind an abstraction that makes them invisible to you.\n\ntechniques like Visual Recognition, Speech to Text and Natural Language\n\nBoth servers and ops are managed by a vendor like Amazon, Google,\n\nUnderstanding.\n\nMicrosoft, etc.\n\nAlthough it seems that we have a choice, we must keep in mind that\n\nIn the context of microservice architecture, FaaS is the concept that is\n\ncommonly, such services are tightly coupled with other services of the\n\ninteresting for us.\n\nparticular Cloud, such as databases, message brokers or data storages.\n\nMostly, the wiser choice is just to use the serverless functionality of the\n\nCloud that we already use to run the rest of our microservices.\n\nServerless providers\n\nIn the next sections, we’ll use AWS Lambda for all of the examples, but\n\nCurrently, there are 4 major Clouds that allow us to use serverless\n\nthe core concepts remain the same across all of the serverless providers.\n\narchitecture:\n\nAWS Lambda - named as the ﬁrst adopter of FaaS, easily integrates\n\nFaaS\n\nwith the rest of Amazon Web Services such as SNS or S3.\n\nIn an FaaS approach, developers are writing code - and code only. They\n\nGoogle Cloud Functions - still in beta, allows us to run our functions in\n\ndo not need to care about the infrastructure, deployment, scalability, etc.\n\nGoogle Cloud. The drawback is, it currently only supports Node.js and\n\nThe code they write represents a simple and small function of the\n\nJavaScript.\n\napplication.\n\nAzure Functions - supports the widest range of languages (JavaScript,\n\nC#, F#, Python, PHP, Bash, Batch, and PowerShell) and easily allows us\n\nto integrate with Github for storing our code.\n\nIBM Bluemix OpenWhisk -\n\n\n\nit uses the open-source Apache\n\nOpenWhisk project running on top of the IBM Bluemix infrastructure.\n\n20 https://github.com/IBM-Bluemix/openwhisk-darkvisionapp\n\n63\n\nGo to Table of Contents",
      "content_length": 2308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "It is run in response to a trigger and can use external services:\n\nTrigger\n\nExternal service\n\nFunction\n\nFig. 20: Basic function as a service architecture consists of only two elements: the function\n\nto be run and a trigger to listen for. Usually the function is also connected to third-party\n\nservices like a database.\n\nA trigger can be almost anything. Based on AWS Lambda, the most\n\npopular FaaS service, the trigger might be:\n\nAPI call (any HTTP request).\n\nS3 bucket upload.\n\nNew event in queue.\n\nScheduled jobs.\n\nOther AWS Lambda functions.\n\nand many others, you can check it:\n\nhttp://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html.\n\nGo to Table of Contents\n\n64",
      "content_length": 685,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Each function should comply with the following rules:\n\nIt is run in response to a trigger and can use external services:\n\nIt should not access the disk - AWS allows using a temporary /tmp\n\ndirectory that allows storing 512MB of data.\n\nExternal\n\nIt should be stateless and share-nothing. You can imagine it as a server\n\nservice\n\nTrigger\n\npowered up and conﬁgured to only handle one request (and then\n\ndestroyed).\n\nConcise - your function should not take too long to run (usually\n\nFunction\n\nseconds, but up to 300 seconds for AWS Lambda).\n\nOnce you have such a function, you just upload it to your service provider\n\nFig. 20: Basic function as a service architecture consists of only two elements: the function\n\nand provide some basic conﬁguration. From that moment, on each action\n\nto be run and a trigger to listen for. Usually the function is also connected to third-party\n\nservices like a database.\n\nconﬁgured as a trigger, your function will be executed. The service\n\nprovider tracks how long it takes for your function to execute, and\n\nmultiplies the time by the amount of RAM conﬁgured (that's a limit you\n\nA trigger can be almost anything. Based on AWS Lambda, the most\n\ncan change). You pay for GB-seconds of execution. This means that if your\n\npopular FaaS service, the trigger might be:\n\nfunction is not executed, you do not pay anything and if your function is\n\nexecuted thousands of times during one day, you pay only for the\n\nAPI call (any HTTP request).\n\nGB-seconds your function took to run. There are no charges for scaling or\n\nS3 bucket upload.\n\nidle time.\n\nNew event in queue.\n\nScheduled jobs.\n\nThe cost of one GB-second on AWS Lambda is currently $0.00001667 -\n\nOther AWS Lambda functions.\n\nthis means that if your application requires 1024MB of RAM, and runs\n\nand many others, you can check it:\n\noverall for 1,000,000 seconds (one million seconds), that is 277 hours\n\nhttp://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html.\n\n(over 11 days), you will be charged $16.77; There is also an additional\n\nprice of $0.20 per 1 million requests. It gets even better if you check out\n\n65\n\nGo to Table of Contents",
      "content_length": 2139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "the free tier that Amazon offers. Each month you get 3,200,000 seconds\n\nto run a function with a 128MB memory limit for free. That’s over 890h -\n\nover 37 days!\n\nI think the calculations above clearly show that you can gain a huge\n\nbeneﬁt by moving some parts (or all parts) of your application to a FaaS\n\nprovider. You get the scalability and ops for free, as you do not need to\n\ntake care of it.\n\nInternally, functions are run in small, ephemeral and stateless containers\n\nthat are spawn if your application needs to scale up.\n\nYou can ﬁnd an interesting cost comparison to EC2 instances here:\n\nhttps://www.trek10.com/blog/lambda-cost/.\n\nArchitecture\n\nI won’t describe the architecture details of a serverless application in this\n\narticle as it should be quite straightforward when writing a microservice\n\napplication. The obvious and required step is to move as much\n\npresentation and logic to the customer as possible. Usually, your\n\napplication front-end should be a mobile app or a single-page app.\n\nOn the back-end, you can start with a very simple architecture where the\n\nfunction is triggered by an API call and then connects to a DynamoDB\n\ninstance (or any other on premise data source like MongoDB, MySQL) to\n\nfetch/modify some data. Then, you can apply direct read access to some\n\ndata in your DynamoDB and allow clients to fetch the data directly,\n\nGo to Table of Contents\n\n66",
      "content_length": 1388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "but handle all data-modifying requests using your function. You can also\n\nthe free tier that Amazon offers. Each month you get 3,200,000 seconds\n\nintroduce Event Sourcing very easily by having one function that records\n\nto run a function with a 128MB memory limit for free. That’s over 890h -\n\nan event and other functions that take the event in order to refresh your\n\nover 37 days!\n\nread model.\n\nI think the calculations above clearly show that you can gain a huge\n\nYou can also use FaaS to implement batch processing: split the stream of\n\nbeneﬁt by moving some parts (or all parts) of your application to a FaaS\n\ndata into smaller chunks and then send them to another function that will\n\nprovider. You get the scalability and ops for free, as you do not need to\n\nrun multiple instances of itself simultaneously. This allows you to process\n\ntake care of it.\n\nthe data much faster. FaaS is often used to do real-time log processing.\n\nInternally, functions are run in small, ephemeral and stateless containers\n\nthat are spawn if your application needs to scale up.\n\nIt’s easy!\n\nYou can ﬁnd an interesting cost comparison to EC2 instances here:\n\nhttps://www.trek10.com/blog/lambda-cost/.\n\nJust a quick „hello world” example to show you how easily you can start\n\nwriting serverless applications:\n\nArchitecture\n\nexports.handler = (event, context, callback) => { callback(null, 'Hello World'); };\n\nI won’t describe the architecture details of a serverless application in this\n\narticle as it should be quite straightforward when writing a microservice\n\napplication. The obvious and required step is to move as much\n\npresentation and logic to the customer as possible. Usually, your\n\nSummary\n\napplication front-end should be a mobile app or a single-page app.\n\nBeneﬁts\n\nOn the back-end, you can start with a very simple architecture where the\n\nfunction is triggered by an API call and then connects to a DynamoDB\n\nFaaS is easy to learn and implement, and it allows you to reduce the time\n\ninstance (or any other on premise data source like MongoDB, MySQL) to\n\nto market. It also allows you to reduce costs, and to scale easily. Each\n\nfetch/modify some data. Then, you can apply direct read access to some\n\nfunction you write ﬁts easily into a sprint, so it is easy to write serverless\n\ndata in your DynamoDB and allow clients to fetch the data directly,\n\napplications in agile teams.\n\n67\n\nGo to Table of Contents",
      "content_length": 2405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Drawbacks\n\nThere might be a small vendor lock-in if you do not take this into\n\nconsideration and do not introduce proper architecture. You should be\n\naware of the communication overhead that is added by splitting the app\n\ninto such small services. The most common issues mentioned are\n\nmultitenancy (the same issue as with running containers on Amazon) and\n\ncold start - when scaling up, it takes some time to handle the ﬁrst request\n\nby a new container. It might also be a bit harder to test such an\n\napplication.\n\nGood use-cases\n\nHere are some use-cases that are interesting in my opinion:\n\nMostly static pages, including eCommerce; You can host static content\n\non a CDN server or add cache in front of your functions.\n\nData stream analysis.\n\nProcessing uploads - image/video thumbnails, etc.\n\nActions users pay for. For example, adding watermarks to an ebook.\n\nOther cases when your application is not fully using the server capacity\n\nor you need to add scalability without investing much time and money.\n\nGo to Table of Contents\n\n68",
      "content_length": 1036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Drawbacks\n\nContinuous Deployment\n\nThere might be a small vendor lock-in if you do not take this into\n\nJust imagine that each of your microservices needs to be ﬁrst built and\n\nconsideration and do not introduce proper architecture. You should be\n\nthen deployed manually, not even mentioning running unit tests or any\n\naware of the communication overhead that is added by splitting the app\n\nkind of code-style tools. Having tens of those would be extremely\n\ninto such small services. The most common issues mentioned are\n\ntime-consuming and would often be a major bottleneck in the whole\n\nmultitenancy (the same issue as with running containers on Amazon) and\n\ndevelopment process.\n\ncold start - when scaling up, it takes some time to handle the ﬁrst request\n\nby a new container. It might also be a bit harder to test such an\n\nHere comes the idea of Continuous Deployment - the thing that puts the\n\napplication.\n\nworkﬂow of your whole\n\nIT department together.\n\nIn Continuous\n\nDeployment we can automate all things related with building Docker\n\ncontainers, running unit and\n\nfunctional tests and even testing\n\nGood use-cases\n\nperformance of newly built services. At the end, if everything passes -\n\nnothing prevents us from automatically deploying working solutions into\n\nHere are some use-cases that are interesting in my opinion:\n\nproduction.\n\nMostly static pages, including eCommerce; You can host static content\n\nThe most commonly used software that handles the whole process is\n\non a CDN server or add cache in front of your functions.\n\nJenkins, Travis CI, Bamboo or CircleCI. We’ll show you how to do it using\n\nJenkins.\n\nData stream analysis.\n\nProcessing uploads - image/video thumbnails, etc.\n\nDesigning deployment pipeline\n\nActions users pay for. For example, adding watermarks to an ebook.\n\nGoing from the big picture, a common pipeline could look like this:\n\nOther cases when your application is not fully using the server capacity\n\nGitHub\n\nJenkins\n\nor you need to add scalability without investing much time and money.\n\ngit push\n\nwebhook\n\ndeploy\n\nAWS\n\nFig. 21: Overview of our ﬁnal Continuous Deployment pipeline.\n\n69\n\nGo to Table of Contents",
      "content_length": 2150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Most of the hard work is done by that nice looking guy, called Jenkins.\n\nWhen someone pushes something to our Git repository (e.g. Github), the\n\nwebhook triggers a job inside our Jenkins instance. It can consist of the\n\nfollowing steps:\n\n1. Build Docker image.\n\n2. Run unit-tests inside the container.\n\n3. Push image to our images repository (e.g. Docker Hub, Amazon ECR).\n\n4. Deploy using Ansible or task schedulers like Amazon ECS.\n\na. Run functional tests (Selenium).\n\nb. Run performance tests (JMeter).\n\nAfter all this, we can set up a Slack notiﬁcation that will inform us of\n\nsuccess or failure of the whole process. The important thing is, that we\n\nshould keep our Jenkins instance clean, so running all of the unit tests\n\nshould be done inside a Docker container.\n\nCoding our pipeline\n\nOnce we have the idea of our build process, we can code it using the\n\nJenkinsﬁle. It’s a ﬁle that describes our whole deployment pipeline. It\n\nconsists of stages and build steps. Mostly, at the end of the pipeline we\n\ninclude post actions that should be ﬁred when the build was successful or\n\nfailed.\n\nWe should keep this ﬁle in our application’s code repository - that way\n\ndevelopers can also work with it, without asking DevOps for changes in\n\nthe deployment procedure.\n\nGo to Table of Contents\n\n70",
      "content_length": 1295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Most of the hard work is done by that nice looking guy, called Jenkins.\n\nHere is a sample Jenkinsﬁle built on the basis of the previously mentioned\n\nWhen someone pushes something to our Git repository (e.g. Github), the\n\nsteps. As we can see, the ﬁnal step is to run another Jenkins job named\n\nwebhook triggers a job inside our Jenkins instance. It can consist of the\n\ndeploy. Jobs can be tied together to be more reusable - that way we can\n\nfollowing steps:\n\ndeploy our application without having to run all of the previous steps.\n\n1. Build Docker image.\n\n#!groovy pipeline { agent any stages { stage('Build Docker') { steps { sh \"docker build ...\" } } stage('Push Docker Image') { steps { sh 'docker push ...' } } stage('Deploy') { steps { build job: 'deploy' } } }\n\n2. Run unit-tests inside the container.\n\n3. Push image to our images repository (e.g. Docker Hub, Amazon ECR).\n\n4. Deploy using Ansible or task schedulers like Amazon ECS.\n\na. Run functional tests (Selenium).\n\nb. Run performance tests (JMeter).\n\nAfter all this, we can set up a Slack notiﬁcation that will inform us of\n\nsuccess or failure of the whole process. The important thing is, that we\n\nshould keep our Jenkins instance clean, so running all of the unit tests\n\nshould be done inside a Docker container.\n\nCoding our pipeline\n\nOnce we have the idea of our build process, we can code it using the\n\nJenkinsﬁle. It’s a ﬁle that describes our whole deployment pipeline. It\n\nconsists of stages and build steps. Mostly, at the end of the pipeline we\n\npost { success { slackSend color: 'good', message: \"Build Success\" } failure { slackSend color: 'danger', message: \"Build Failed\" } } }\n\ninclude post actions that should be ﬁred when the build was successful or\n\nfailed.\n\nWe should keep this ﬁle in our application’s code repository - that way\n\ndevelopers can also work with it, without asking DevOps for changes in\n\nthe deployment procedure.\n\n71\n\nGo to Table of Contents",
      "content_length": 1939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Related technologies\n\nGo to Table of Contents\n\n72",
      "content_length": 49,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Microservices based eCommerce platforms\n\nThere are major open-source platforms that were built using the\n\nMicroservices approach by design. This section tries to list those that we\n\nthink could be used as a reference for designing your architecture - or\n\neven better - could be used as a part of it.\n\nSylius\n\nSylius is the ﬁrst Open Source eCommerce platform constructed from\n\nstandalone components. What does it mean in practice? Every aspect of\n\nthe shopping process is handled by individual PHP libraries. While the\n\nproject itself provides a complete shop solution with a REST API, these\n\ndecoupled components can be used separately to build Microservice\n\napplications.\n\nLet’s say we need to have two services for handling a Product Catalog and\n\nPromotions, respectively. The solution would be to take the two\n\nRelated\n\ncomponents and use them to develop two standalone applications.\n\nBefore Sylius, you would have needed to write everything from scratch or\n\ntechnologies\n\nstrip the functionality from an existing eCommerce software.\n\nOn top of that, Sylius is based on the highly scalable Symfony framework,\n\nwhich integrates with a wide range of caching solutions, from Redis,\n\nMemcache to Varnish. It also provides tools for RAPID API development\n\nwith JSON/XML support, which allows you to prototype your microservice\n\nin a much shorter timeframe and lower the costs of development.\n\n73\n\nGo to Table of Contents",
      "content_length": 1419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "When you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\nshared between your local disk and the disk inside a container. When\n\ndeveloping an application, you usually setup containers to share all\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\neach change. On pre-production and production servers, project ﬁles\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nThe other common change applies to ports. When using Docker for\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nSpryker\n\nSpryker is a “Made in Germany” eCommerce platform created with a\n\nSOA approach with separated Backend (ZED) and Frontend (YVES)\n\napplications. The platform is designed with high throughput and\n\nscalability in mind. It’s not the classic microservices approach - you can\n\nlearn more about Spryker’s founder’s view on that in Appendix 1 to this\n\nbook.\n\nElasticsearch\n\nMobile SDK\n\nYVES Shop front end\n\nKV Storage\n\nREST API\n\nSession Storage\n\nRPC\n\nPayment\n\nMail\n\nBI Business Intelligence\n\nZED Back end\n\nPIM\n\nETL\n\nERP\n\nQueue\n\nMySQL\n\nPostgreSQL\n\nFig. 14: Backend for frontends architecture is about minimizing the number of backend\n\ncalls and optimizing the interfaces to a supported device.\n\nThe Spryker source code is available on Github:\n\nhttps://github.com/spryker. The platform comes with an interesting\n\nlicensing model - per developer seat (not related to revenues, servers\n\netc…).\n\n74",
      "content_length": 1895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "When you have your development setup up and running, it is also quite\n\neasy to push your application to a staging server. In most projects I know,\n\nthis process was pretty straight-forward and required only a few changes.\n\nThe main difference is in the so called volumes - ﬁles/directories that are\n\nshared between your local disk and the disk inside a container. When\n\ndeveloping an application, you usually setup containers to share all\n\nproject ﬁles with Docker so you do not need to rebuild the image after\n\neach change. On pre-production and production servers, project ﬁles\n\nshould live inside the container/image and should not be mounted on\n\nyour local disk.\n\nThe other common change applies to ports. When using Docker for\n\ndevelopment, you usually bind your local ports to ports inside the\n\ncontainer, i.e. your local 8080 port to port 80 inside the container. This\n\nmakes it impossible to test scalability of such containers and makes the\n\nURI look bad (no one likes ports inside the URI).\n\nDocker has some downsides too. The ﬁrst one you might notice is that it\n\nSpryker\n\nOpen Loyalty\n\nSpryker is a “Made in Germany” eCommerce platform created with a\n\nA loyalty/rewards program that can be easily integrated with eCommerce\n\nSOA approach with separated Backend (ZED) and Frontend (YVES)\n\nand/or POS. It’s interesting because of the CDB module (Central Data\n\napplications. The platform is designed with high throughput and\n\nBase) which is responsible for gathering a 360deg. view of each customer.\n\nscalability in mind. It’s not the classic microservices approach - you can\n\nlearn more about Spryker’s founder’s view on that in Appendix 1 to this\n\nOpen Loyalty leverages the CQRS and Event Sourcing design patterns.\n\nbook.\n\nYou can use it as a headless CRM leveraging a REST API (with JWT based\n\nauthorization).\n\nElasticsearch\n\nWe’ve seen many cases of Open Loyalty being used as CRM and\n\nMobile\n\nmarketing automation.\n\nSDK\n\nYVES\n\nShop\n\nKV Storage\n\nfront end\n\nREST API\n\nSession Storage\n\nCUSTOMER VIEW\n\nADMIN VIEW\n\nON-LINE\n\nINTERNAL\n\nRPC\n\nClient Cockpit\n\nPayment\n\neCommerce\n\nMail\n\nERP\n\nBI\n\nZED\n\neCommerce Cockpit\n\nPIM\n\nBusiness\n\nBack end\n\nIntelligence\n\nETL\n\nERP\n\nCustomer\n\nAdmin\n\nMobile Cockpit*\n\nAdmin Cockpit\n\nOFF-LINE\n\nSaaS\n\nPOS\n\nQueue\n\nMySQL\n\nPostgreSQL\n\nOFF-LINE DATA\n\nMarketing Automation\n\nPOS cockpit\n\nFig. 14: Backend for frontends architecture is about minimizing the number of backend\n\ncalls and optimizing the interfaces to a supported device.\n\nE-MAIL, SMS, PUSH NOTIFICATION\n\nThe Spryker source code is available on Github:\n\nOld Components\n\nhttps://github.com/spryker. The platform comes with an interesting\n\nMerchant\n\nNew Components\n\nlicensing model - per developer seat (not related to revenues, servers\n\n* Mobile app needs to be developed.\n\nFig. 23: Open Loyalty architecture - each application works as separate service.\n\netc…).\n\n75",
      "content_length": 2857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "The platform is open source and you can ﬁnd the code on Github\n\n(https://github.com/DivanteLtd/open-loyalty).\n\nMore information: http://openloyalty.io.\n\nTechnologies that empower the microservices architecture\n\nPimcore is an Enterprise Content platform for:\n\nCMS - content management.\n\nPIM - master data management for products.DAM - digital assets\n\nmanagement for attachments, videos and pictures.\n\neCommerce Framework - for building checkout features and managing\n\norders.\n\nThe Pimcore REST API22 can be used to make Pimcore an eCommerce\n\nbackend for Mobile applications or to extend existing eCommerce\n\nplatform catalog capabilities, etc.\n\nIt’s a open source technology developed in Austria with a really active\n\ncommunity and version 5.0 (based on Symfony Framework) on the\n\nhorizon.\n\nMore on Pimcore: http://pimcore.org.\n\n22 https://www.pimcore.org/docs/latest/Web_Services/index.html\n\nGo to Table of Contents\n\n76",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "The platform is open source and you can ﬁnd the code on Github\n\nTechnologies that empower the microservices architecture\n\n(https://github.com/DivanteLtd/open-loyalty).\n\nMore information: http://openloyalty.io.\n\nThe microservices architecture introduces new concepts that sometimes\n\nalso require new or different tools compared to the monolithic approach.\n\nAlso, keeping in mind, that this approach may lead to more complexity of\n\nTechnologies that empower the microservices architecture\n\nour platform, we should automate as many things as we can from the\n\nbeginning.\n\nPimcore is an Enterprise Content platform for:\n\nWe’ll show you some of the most widely used tools and technologies that\n\nCMS - content management.\n\ncould empower your development by making things easier, more\n\nautomated and are very suitable when diving into Microservices.\n\nPIM - master data management for products.DAM - digital assets\n\nmanagement for attachments, videos and pictures.\n\nAnsible\n\neCommerce Framework - for building checkout features and managing\n\norders.\n\nDevOps\n\nis an agile way\n\nto maintain software.\n\nIt emphasizes\n\ncommunication between IT and SD23.\n\nThe Pimcore REST API22 can be used to make Pimcore an eCommerce\n\nbackend for Mobile applications or to extend existing eCommerce\n\nAnsible is a tool for automation of DevOps routines. Ansible uses an\n\nplatform catalog capabilities, etc.\n\nagentless architecture which means that no additional software is needed\n\nto be installed on target machines; communication is done by issuing\n\nIt’s a open source technology developed in Austria with a really active\n\nplain SSH commands.\n\nIt automates applications deployment,\n\ncommunity and version 5.0 (based on Symfony Framework) on the\n\nconﬁguration management, workﬂow orchestration and even cloud\n\nhorizon.\n\nprovisioning – all in one tool. Shipping with nearly 200 modules in the\n\ncore distribution, Ansible provides a vast library of building blocks for\n\nMore on Pimcore: http://pimcore.org.\n\nmanaging all kinds of IT tasks.\n\n22 https://www.pimcore.org/docs/latest/Web_Services/index.html\n\n23 https://pl.wikipedia.org/wiki/DevOps\n\n77\n\nGo to Table of Contents",
      "content_length": 2142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Ansible composes each server (or group of them, named inventory) from\n\nreusable roles. We can deﬁne ours, such as Nginx, PHP or Magento, and\n\nthen reuse them for different machines. Roles are next tied together in\n\n“Playbooks” that describe the full deployment process.\n\nThere’re plenty of well-written, already made Playbooks that you could\n\nadapt and reuse for conﬁguring your infrastructure. As an example, when\n\ninstalling Magento you could use:\n\nhttps://github.com/aslaen/AnsiblePlaybooks/tree/master/ansible-magen\n\nto-lemp.\n\nTo conﬁgure our ﬁrst servers with the Nginx web server and PHP, we\n\nshould ﬁrst create two roles that will be next used in a ﬁnal Playbook.\n\n1. Nginx: # in ./roles/nginx/tasks/main.yml - name: Ensures that nginx is installed apt: name=nginx state=present\n\nname: Creates nginx conﬁguration from Jinja template ﬁle template: src: \"/etc/nginx/nginx.conf.j2\" dest: \"/etc/nginx/nginx.conf\"\n\nGo to Table of Contents\n\n78",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Ansible composes each server (or group of them, named inventory) from\n\n2. PHP: # in ./roles/php/tasks/main.yml - name: Ensures that dotdeb APT repository is added apt_repository: repo=\"deb http://packages.dotdeb.org jessie all\" state=present\n\nreusable roles. We can deﬁne ours, such as Nginx, PHP or Magento, and\n\nthen reuse them for different machines. Roles are next tied together in\n\n“Playbooks” that describe the full deployment process.\n\nThere’re plenty of well-written, already made Playbooks that you could\n\nname: Ensures that dotdeb key is present apt_key: url=https://www.dotdeb.org/dotdeb.gpg state=present\n\nadapt and reuse for conﬁguring your infrastructure. As an example, when\n\ninstalling Magento you could use:\n\nhttps://github.com/aslaen/AnsiblePlaybooks/tree/master/ansible-magen\n\nname: Ensures that APT cache is updated apt: update_cache=yes\n\nto-lemp.\n\nname: Ensures that listed packages are installed apt: pkg=\"{{ item }}\" with_items: - php7.0-cli - php7.0-curl - php7.0-fpm\n\nTo conﬁgure our ﬁrst servers with the Nginx web server and PHP, we\n\nshould ﬁrst create two roles that will be next used in a ﬁnal Playbook.\n\n1. Nginx:\n\n# in ./roles/nginx/tasks/main.yml\n\nHaving these roles, we can now deﬁne a playbook that will combine them\n\nname: Ensures that nginx is installed\n\nto set-up our new server with nginx and php installed:\n\napt: name=nginx state=present\n\nname: Creates nginx conﬁguration from Jinja template ﬁle\n\n# in ./php-nodes.yml - hosts: php-nodes roles: - nginx - php\n\ntemplate:\n\nsrc: \"/etc/nginx/nginx.conf.j2\"\n\ndest: \"/etc/nginx/nginx.conf\"\n\n79\n\nGo to Table of Contents",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "The last thing we need to do is to tell Ansible the hostnames of our\n\nservers:\n\n# in ./inventory [php-nodes] php-node1.acme.org php-node2.acme.org\n\nDeployment is now as easy as typing a single shell command that will tell\n\nAnsible to run the php-nodes.yml playbook on hosts from the inventory\n\nﬁle as root (-b):\n\n$ ansible-playbook -i inventory php-nodes.yml -b\n\nAs we deﬁned two hosts in a “php-nodes” group, Ansible is smart\n\nenough to run the Playbook concurrently for every server. That way we’re\n\nable to make a deployment on a bigger group of machines at once\n\nwithout wasting time doing it one-by-one.\n\nReactJS\n\nReact is an open source user interface (UI) component library. It was\n\ndeveloped at Facebook to facilitate creation of interactive web interfaces.\n\nIt is often referred to as the V in the “MVC” architecture as it makes no\n\nassumptions about the rest of your technology stack.\n\nGo to Table of Contents\n\n80",
      "content_length": 923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "With React, you compose your application out of components. It\n\nThe last thing we need to do is to tell Ansible the hostnames of our\n\nembraces what is called component-based architecture - a declarative\n\nservers:\n\napproach to developing web interfaces where you describe your UI with a\n\ntree of components. React components are highly encapsulated,\n\n# in ./inventory\n\nconcern-speciﬁc, single-purpose blocks. For example, there could be\n\n[php-nodes]\n\nphp-node1.acme.org\n\ncomponents for address or zip code that together create a form. Such\n\nphp-node2.acme.org\n\ncomponents have both a visual representation and dynamic logic.\n\nSome components can even talk to the server on their own, e.g., a form\n\nDeployment is now as easy as typing a single shell command that will tell\n\nthat submits its values to the server and shows conﬁrmation on success.\n\nAnsible to run the php-nodes.yml playbook on hosts from the inventory\n\nSuch interfaces are easier to reuse, refactor, test and maintain. They also\n\nﬁle as root (-b):\n\ntend to be faster than their imperative counterparts as React - being\n\nresponsible\n\nfor rendering your UI on screen - performs many\n\noptimisations and batches updates in one go.\n\n$ ansible-playbook -i inventory php-nodes.yml -b\n\nIt’s most commonly used with Webpack - a module bundler for modern\n\nJavascript. One of its features - code-splitting - allows you to generate\n\nmultiple Javascript bundles (entry points) allowing you to send clients\n\nAs we deﬁned two hosts in a “php-nodes” group, Ansible is smart\n\nonly the part of Javascript that is required to render that particular screen.\n\nenough to run the Playbook concurrently for every server. That way we’re\n\nable to make a deployment on a bigger group of machines at once\n\nOne of the interesting movements in frontend-development nowadays is\n\nwithout wasting time doing it one-by-one.\n\nan Isomorphic approach. Which means that both frontend and backend\n\nare sharing the same code. In this particular case, frontend app can be\n\ncreated in React and backend code run by NodeJS.\n\nReactJS\n\nReact is an open source user interface (UI) component library. It was\n\nNodeJS\n\ndeveloped at Facebook to facilitate creation of interactive web interfaces.\n\nIt is often referred to as the V in the “MVC” architecture as it makes no\n\nNodeJS is a popular (de facto industry standard) JavaScript engine that\n\nassumptions about the rest of your technology stack.\n\n81\n\nGo to Table of Contents",
      "content_length": 2439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "can be used server-side and in CLI environments. There are plenty of\n\nJavaScript\n\nWeb\n\nframeworks\n\navailable,\n\nlike\n\nExpress\n\n(https://expressjs.com/) and HapiJS (https://hapijs.com/) - to name but\n\ntwo. As NodeJS is built around Google’s V8 JavaScript engine (initially\n\ndeveloped as Chrome/Chromium JS engine) it’s blazingly fast. Node\n\nleverages the events-polling/non-blocking IO architecture to provide\n\nexceptional performance results and optimizes CPU utilization (for more,\n\nread about the c10k problem: http://www.kegel.com/c10k.html).\n\nNode.js Server\n\nRequest\n\nRequest\n\nEvent Loop\n\nPOSIX Async Threads\n\nNon- blocking IO\n\nDelegate\n\nRequests\n\nRequests\n\nSingle Thread\n\nFig. 24: Node.js request ﬂow. Node leverages Event polling and maximizing the memory\n\nand CPU usage on running parallel operations inside single threaded environment.\n\ninteroperate with frontend JS code very easily. There is an emerging trend\n\nof building Universal apps - which more or less means that the same code\n\nbase is in use on the frontend and backend. One of the most interesting\n\nand popular frameworks for building Isomorphic apps is React Js\n\n(https://facebook.github.io/react/).\n\nNodeJS is used as a foundation for many CLI tools - starting from the\n\nmost popular “npm” (Node Package Manager), followed by a number of\n\ntools like Gulp, Yeoman and others.\n\nGo to Table of Contents\n\n82",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "can be used server-side and in CLI environments. There are plenty of\n\nJavaScript is the rising star of programming languages. It can even be\n\nJavaScript\n\nWeb\n\nframeworks\n\navailable,\n\nlike\n\nExpress\n\nused for building desktop applications - like Visual Studio Code or Vivaldi\n\n(https://expressjs.com/) and HapiJS (https://hapijs.com/) - to name but\n\nweb browser (!); these tools are coded in 100% pure JavaScript - but for\n\ntwo. As NodeJS is built around Google’s V8 JavaScript engine (initially\n\nthe end users, nothing differs from standard desktop applications. And\n\ndeveloped as Chrome/Chromium JS engine) it’s blazingly fast. Node\n\nthey’re portable between operating systems by default!\n\nleverages the events-polling/non-blocking IO architecture to provide\n\nexceptional performance results and optimizes CPU utilization (for more,\n\nOn the server side, NodeJS is very often used as an API platform because\n\nread about the c10k problem: http://www.kegel.com/c10k.html).\n\nof the platform speed. The event polling architecture is ideal for rapid but\n\nshort-lived requests.\n\nNode.js Server\n\nRequest\n\nUsing “npm” one can install almost all available libraries and tools for the\n\nJS stack - including frontend and backend packages. As most modern\n\nPOSIX\n\nEvent\n\nNon-\n\nAsync\n\nRequest\n\nLoop\n\nlibraries (eg. GraphQL, Websockets) have Node bindings, and all modern\n\nblocking\n\nThreads\n\nIO\n\nDelegate\n\ncloud providers support this technology as well, it’s a good choice for\n\nRequests\n\nbackend technology backing microservices.\n\nSingle\n\nThread\n\nRequests\n\nFamous NodeJS users\n\nFig. 24: Node.js request ﬂow. Node leverages Event polling and maximizing the memory\n\nNode.js helps us solve this (boundary between the browser and server) by enabling both the browser and server applications to be written in JavaScript. It uniﬁes our engineering specialties into one team which allows us to understand and react to our users’ needs at any level in the technology stack.\n\nand CPU usage on running parallel operations inside single threaded environment.\n\ninteroperate with frontend JS code very easily. There is an emerging trend\n\nof building Universal apps - which more or less means that the same code\n\nbase is in use on the frontend and backend. One of the most interesting\n\nand popular frameworks for building Isomorphic apps is React Js\n\n— Jeff Harrel, Senior Director of Payments Products and\n\n(https://facebook.github.io/react/).\n\nEngineering at PayPal24\n\nNodeJS is used as a foundation for many CLI tools - starting from the\n\n24 https://www.paypal-engineering.com/2013/11/22/node-js-at-paypal/\n\nmost popular “npm” (Node Package Manager), followed by a number of\n\ntools like Gulp, Yeoman and others.\n\n83\n\nGo to Table of Contents",
      "content_length": 2714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "LinkedIn\n\nOne reason was scale. The second is, if you look at Node, the thing it’s best at doing is talking to other services.\n\n— Mobile Development Lead, Kiran Prasad25\n\neBay\n\nWe had two primary requirements for the project. First, was to make the application as real time as possible–i.e., maintain live connections with the server. Second, was to orchestrate a that display huge number of eBay-speciﬁc services information on the page–i.e.\n\n— Senthil Padmanabhan, Principal Web Engineer at eBay26\n\nOther projects that leverage NodeJS:\n\nUber\n\nhttps://nodejs.org/static/documents/casestudies/Nodejs-at-Uber.pdf\n\nNetﬂix\n\nhttp://thenewstack.io/netﬂix-uses-node-js-power-user-interface/\n\nGroupon\n\nhttp://www.datacenterknowledge.com/archives/2013/12/06/need-speed-groupon-m\n\nigrated-node-js/\n\n25 http://venturebeat.com/2011/08/16/linkedin-node/\n\n26 http://www.ebaytechblog.com/2013/05/17/how-we-built-ebays-ﬁrst-node-js-application/\n\nGo to Table of Contents\n\n84\n\nSwagger\n\nThis powerful tool is too commonly used only for generating nice-looking\n\ndocumentation for APIs. Basically, swagger is for deﬁning the API\n\ninterfaces using simple, domain-driven JSON language.\n\nThe editor is only one tool from the toolkit but other ones are:\n\nCodegen - for generating the source code scaffold for your API -\n\navailable in many different languages (Node, Ruby, .NET, PHP).\n\nUI - most known swagger tool for generating useful and nice looking\n\ninteractive documentation.\n\nEverything starts with a speciﬁcation ﬁle describing all the Entities and\n\ninterfaces for the REST API. Please take a look at the example below:\n\n{\n\n\"get\": {\n\n\"description\": \"Returns pets based on ID\",\n\n\"summary\": \"Find pets by ID\",\n\n\"operationId\": \"getPetsById\",\n\n\"produces\": [\n\n\"application/json\",\n\n\"text/html\"\n\n],\n\n\"responses\": {\n\n\"200\": {\n\n\"description\": \"pet response\",\n\n\"schema\": {",
      "content_length": 1845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "LinkedIn\n\nSwagger\n\nOne reason was scale. The second is, if you look at Node, the\n\nThis powerful tool is too commonly used only for generating nice-looking\n\nthing it’s best at doing is talking to other services.\n\ndocumentation for APIs. Basically, swagger is for deﬁning the API\n\ninterfaces using simple, domain-driven JSON language.\n\n— Mobile Development Lead, Kiran Prasad25\n\nThe editor is only one tool from the toolkit but other ones are:\n\neBay\n\nCodegen - for generating the source code scaffold for your API -\n\navailable in many different languages (Node, Ruby, .NET, PHP).\n\nWe had two primary requirements for the project. First, was to\n\nmake the application as real time as possible–i.e., maintain\n\nUI - most known swagger tool for generating useful and nice looking\n\nlive connections with the server. Second, was to orchestrate a\n\nhuge number of eBay-speciﬁc services\n\nthat display\n\ninteractive documentation.\n\ninformation on the page–i.e.\n\nEverything starts with a speciﬁcation ﬁle describing all the Entities and\n\n— Senthil Padmanabhan, Principal Web Engineer at eBay26\n\ninterfaces for the REST API. Please take a look at the example below:\n\nOther projects that leverage NodeJS:\n\n{ \"get\": { \"description\": \"Returns pets based on ID\", \"summary\": \"Find pets by ID\", \"operationId\": \"getPetsById\", \"produces\": [ \"application/json\", \"text/html\" ], \"responses\": { \"200\": { \"description\": \"pet response\", \"schema\": {\n\nUber\n\nhttps://nodejs.org/static/documents/casestudies/Nodejs-at-Uber.pdf\n\nNetﬂix\n\nhttp://thenewstack.io/netﬂix-uses-node-js-power-user-interface/\n\nGroupon\n\nhttp://www.datacenterknowledge.com/archives/2013/12/06/need-speed-groupon-m\n\nigrated-node-js/\n\n25 http://venturebeat.com/2011/08/16/linkedin-node/\n\n26 http://www.ebaytechblog.com/2013/05/17/how-we-built-ebays-ﬁrst-node-js-application/\n\n85\n\nGo to Table of Contents",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "\"type\": \"array\", \"items\": { \"$ref\": \"#/deﬁnitions/Pet\" } } },\n\n\"default\": { \"description\": \"error payload\", \"schema\": { \"$ref\": \"#/deﬁnitions/ErrorModel\" } } } }, \"parameters\": [ { \"name\": \"id\", \"in\": \"path\", \"description\": \"ID of pet to use\", \"required\": true, \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"collectionFormat\": \"csv\" } ] }\n\n$ref relates to other entities described in the ﬁle (data models, structures\n\netc). You can use primitives as the examples and return values (bool,\n\nstring…) as well as hash-sets, compound objects and lists. Swagger allows\n\nyou to specify the validation rules and authorization schemes (basic auth,\n\noauth, oauth2).\n\nGo to Table of Contents\n\n86",
      "content_length": 687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "\"type\": \"array\",\n\n{ \"oauth2\": { \"type\": \"oauth2\", \"scopes\": [ { \"scope\": \"email\", \"description\": \"Access to your email address\" }, { \"scope\": \"pets\", \"description\": \"Access to your pets\" } ], \"grantTypes\": { \"implicit\": { \"loginEndpoint\": {\n\n\"items\": {\n\n\"$ref\": \"#/deﬁnitions/Pet\"\n\n}\n\n}\n\n},\n\n\"default\": {\n\n\"description\": \"error payload\",\n\n\"schema\": {\n\n\"$ref\": \"#/deﬁnitions/ErrorModel\"\n\n}\n\n}\n\n}\n\n},\n\n\"parameters\": [\n\n{\n\n\"url\":\n\n\"name\": \"id\",\n\n\"http://petstore.swagger.wordnik.com/oauth/dialog\" }, \"tokenName\": \"access_token\" }, \"authorization_code\": { \"tokenRequestEndpoint\": {\n\n\"in\": \"path\",\n\n\"description\": \"ID of pet to use\",\n\n\"required\": true,\n\n\"type\": \"array\",\n\n\"items\": {\n\n\"type\": \"string\"\n\n\"url\": \"http://petstore.swagger.wordnik.com/oauth/requestToken\", \"clientIdName\": \"client_id\", \"clientSecretName\": \"client_secret\" }, \"tokenEndpoint\": {\n\n},\n\n\"collectionFormat\": \"csv\"\n\n}\n\n]\n\n}\n\n\"url\":\n\n\"http://petstore.swagger.wordnik.com/oauth/token\", \"tokenName\": \"access_code\" } } } } }\n\n$ref relates to other entities described in the ﬁle (data models, structures\n\netc). You can use primitives as the examples and return values (bool,\n\nstring…) as well as hash-sets, compound objects and lists. Swagger allows\n\nyou to specify the validation rules and authorization schemes (basic auth,\n\noauth, oauth2).\n\n87\n\nGo to Table of Contents",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Last but not least swagger the OpenAPI speciﬁcation format has become\n\nmore and more a standard and should be considered when starting new\n\nAPI projects. It’s supported by many external tools and platforms -\n\nincluding Amazon API Gateway27.\n\nFig. 25: Swagger UI generates a nice-looking speciﬁcation for your API along with a\n\n“try-it-out” feature for executing API calls directly from the browser.\n\nElasticsearch\n\nThe simplest way to start with a microservices approach in eCommerce is\n\noften to delegate the search\n\nfeature to an external tool\n\nElasticearch/Solr or to SaaS solutions like Klevu.\n\nElasticsearch is a search engine available via REST API (updates, reads,\n\nsearches…). It can be a micro service for catalog operations in\n\neCommerce and it’s often used to leverage the NoSQL scalability of its\n\ninternal document database over standard SQL solutions.\n\n27 https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\nGo to Table of Contents\n\nlike\n\n88",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Last but not least swagger the OpenAPI speciﬁcation format has become\n\nElasticsearch supports full-text search with faceted ﬁltering and support\n\nmore and more a standard and should be considered when starting new\n\nfor most major languages with stemming and misspelling correction\n\nAPI projects. It’s supported by many external tools and platforms -\n\nfeatures.\n\nincluding Amazon API Gateway27.\n\nThere are plenty of modules to Magento and other platforms that\n\nsynchronize product feeds to ES and then provide catalog browsing via\n\nES web-services - without touching the relational database.\n\nElasticsearch is even used for log analysis with tools like Kibana and\n\nLogstash. With its ease of use, performance and scalability characteristics,\n\nit is actually best choice for most eCommerce and content related sites.\n\nElastic is well supported by cloud providers like Amazon and supports\n\nFig. 25: Swagger UI generates a nice-looking speciﬁcation for your API along with a\n\nDocker.\n\n“try-it-out” feature for executing API calls directly from the browser.\n\nElasticsearch\n\nGraphQL\n\nThe simplest way to start with a microservices approach in eCommerce is\n\nModeling a great REST API is hard - using and supporting changes in an\n\noften to delegate the search\n\nfeature to an external tool\n\nlike\n\nAPI over time is sometimes even harder. GraphQL (http://graphql.org) is\n\nElasticearch/Solr or to SaaS solutions like Klevu.\n\na query language; a proposition to a new way of thinking about APIs.\n\nElasticsearch is a search engine available via REST API (updates, reads,\n\nWidely used REST APIs are organized around HTTP endpoints. GraphQL\n\nsearches…). It can be a micro service for catalog operations in\n\nAPIs are different; they are built in terms of types and ﬁelds, and relations\n\neCommerce and it’s often used to leverage the NoSQL scalability of its\n\nbetween them. It gives clients the ability to ask for what they need directly\n\ninternal document database over standard SQL solutions.\n\ninstead of many different REST requests. All the necessary data will be\n\nqueried and returned with a single call.\n\n27 https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\n89\n\nGo to Table of Contents",
      "content_length": 2196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Data deﬁnition:\n\ntype Project { name: String tagline: String contributors: [User] }\n\nSample query:\n\n{ project(name: \"GraphQL\") { tagline } }\n\nQuery result:\n\n{ \"project\": { \"tagline\": \"A query language for APIs\" } }\n\nGraphQL was developed\n\ninternally by Facebook\n\nin 2012 and\n\nopen-sourced 3 years later with Relay, a JavaScript framework for building\n\ndata-driven React applications. Nowadays, the GraphQL ecosystem is\n\ngrowing rapidly; both server and frontend libraries are available for many\n\nprogramming languages and developers have dedicated tools for Graph-\n\nQL API design. Many other organizations, including Github, Pinterest and\n\nShopify are adopting GraphQL because of its beneﬁts.\n\nGo to Table of Contents\n\n90",
      "content_length": 721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "Distributed logging and monitoring\n\nData deﬁnition:\n\ntype Project {\n\nDistributed systems require new levels of application monitoring and\n\nname: String\n\nlogging. With monolithic applications you can track one log-ﬁle for events\n\ntagline: String\n\n(usually) and use some Zabbix triggers to get a complete view of a\n\ncontributors: [User]\n\nserver's state, application errors, etc.\n\n}\n\nSample query:\n\nGraylog\n\n{\n\nWith distributed services you have to track a whole bunch of new metrics:\n\nproject(name: \"GraphQL\") {\n\ntagline\n\n}\n\nNetwork latency - which can ruin the communication between crucial\n\n}\n\nparts.\n\nApplication errors on the service level and violation of service-contracts.\n\nQuery result:\n\nPerformance metrics.\n\nSecurity violations.\n\n{\n\n\"project\": {\n\nTo make it even worse, you must track all those parameters across several\n\n\"tagline\": \"A query language for APIs\"\n\nclusters in real time. Without such a level of monitoring, no high\n\n}\n\navailability can be achieved and the distributed system is even more\n\n}\n\nvulnerable to downtime than a single monolithic application.\n\nGraphQL was developed\n\ninternally by Facebook\n\nin 2012 and\n\nThe good news is that nowadays there are plenty of tools to measure\n\nopen-sourced 3 years later with Relay, a JavaScript framework for building\n\nweb-app performance and availability. One of the most interesting is\n\ndata-driven React applications. Nowadays, the GraphQL ecosystem is\n\nGraylog (http://graylog.org).\n\ngrowing rapidly; both server and frontend libraries are available for many\n\nprogramming languages and developers have dedicated tools for Graph-\n\nUsed by many microservice predecessors like LinkedIn, eBay, and Twilio,\n\nQL API design. Many other organizations, including Github, Pinterest and\n\nGraylog centralizes logs into streams.\n\nShopify are adopting GraphQL because of its beneﬁts.\n\n91\n\nGo to Table of Contents",
      "content_length": 1864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Fig. 26: In graylog you’ve got access to messages in real time with alerts conﬁgured for\n\neach separate message stream.\n\nGraylog is easy to integrate, leveraging HTTP communication, syslog\n\n(with UDP support for minimum network load) or third party log collectors\n\nlike ﬂuentd29. It can be integrated with e-mail, SMS, and Slack alerts.\n\nFig. 27: Alerts conﬁguration is a basic feature for providing HA to your microservices\n\necosystem.\n\n29 http://www.ﬂuentd.org/\n\nGo to Table of Contents\n\n92",
      "content_length": 492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Distributed systems require new levels of application monitoring and\n\nlogging. With monolithic applications you can track one log-ﬁle for events\n\n(usually) and use some Zabbix triggers to get a complete view of a\n\nserver's state, application errors, etc.\n\nNew Relic\n\nWhereas Graylog is focused around application logging, New Relic is\n\ncentered around the performance and numeric metrics of your\n\napplications and servers: network response times, CPU load, HTTP\n\nresponse times, network graphs, as well as application stack traces with\n\nFig. 26: In graylog you’ve got access to messages in real time with alerts conﬁgured for\n\neach separate message stream.\n\ndebugging information.\n\nGraylog is easy to integrate, leveraging HTTP communication, syslog\n\n(with UDP support for minimum network load) or third party log collectors\n\nNew Relic works as a system daemon with native libraries for many\n\nlike ﬂuentd29. It can be integrated with e-mail, SMS, and Slack alerts.\n\nprogramming languages and servers (PHP, NodeJS…). Therefore, it can\n\nbe used even in production where most other debugging tools come with\n\ntoo much signiﬁcant overhead. We used to work with New Relic on\n\nproduction clusters - even with applications with millions of unique users\n\nper month and dozens of servers in a cluster.\n\nWe used to implement our own custom metrics to monitor response\n\ntimes from 3rd party services and integrations. Similarly to Graylog, New\n\nRelic can set up dashboards and alerts.\n\nFig. 27: Alerts conﬁguration is a basic feature for providing HA to your microservices\n\necosystem.\n\n29 http://www.ﬂuentd.org/\n\n93\n\nGo to Table of Contents",
      "content_length": 1629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Fig. 28: The coolest feature of New Relic is stack-trace access - on production, in real time.\n\nGo to Table of Contents\n\n94",
      "content_length": 123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "New Relic Insights\n\nData visualization tools and customizable dashboards, allow you to\n\nobserve business analytics data and performance information at the same\n\ntime.\n\nBy combining application, environment and business data - like\n\ntransactions, pageviews and order details - into one reporting tool, you\n\ncan more precisely see how your app performance affects your business.\n\nFig. 28: The coolest feature of New Relic is stack-trace access - on production, in real time.\n\nFig. 29: New Relic Insights Data Explorer with sample plot.\n\n95\n\nGo to Table of Contents",
      "content_length": 562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "New Relic Insights NRQL Language\n\nYou can also use the NRQL (New Relic Query Language) with syntax\n\nsimilar to SQL language to explore all collected data and create\n\napplication metric reports.\n\nFor example, you can attach customer group IDs to order requests to\n\ncheck if particular customer groups have an unusually bad experience\n\nduring the order process.\n\nFig. 30: New Relic usage of NRQL with sample output.\n\nGo to Table of Contents\n\n96",
      "content_length": 442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "New Relic Insights NRQL Language\n\nTake care of the front-end using New Relic Browser\n\nYou can also use the NRQL (New Relic Query Language) with syntax\n\nAnother powerful feature allows you to easily detect any javascript issue\n\nsimilar to SQL language to explore all collected data and create\n\non the front-end of your application. Additionally, New Relic will show you\n\napplication metric reports.\n\na detailed stack trace and execution proﬁle.\n\nFor example, you can attach customer group IDs to order requests to\n\ncheck if particular customer groups have an unusually bad experience\n\nduring the order process.\n\nFig. 31: The New Relic Browser module displays a list of javascript issues on front-end\n\napplication.\n\nFig. 30: New Relic usage of NRQL with sample output.\n\n97\n\nGo to Table of Contents",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Case Studies:\n\nRe-architecting the monolith\n\nGo to Table of Contents\n\n98",
      "content_length": 72,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Case Studies: Re-architecting the Monolith\n\nHere I’ll brieﬂy present two case studies of the microservices evolution\n\nwhich I’ve been able to observe while working at Divante.\n\nB2B\n\nOne of our B2B clients came to us with the following issues to be\n\nsolved:\n\nCase Studies:\n\nWhile on Magento 1 with SKUs catalog exceeding 1M products -\n\nperformance bottlenecks relating to catalog and catalog updates\n\nRe-architecting\n\nbecame hard to work-around.\n\nthe monolith\n\nMonolithic architecture, strongly tied to external systems (such as CRM,\n\nERP, WMS) hindered changes and development of new features.\n\nCRM that became the SPoF (Single Point of Failure). Pivotal CRM was in\n\ncharge of too many key responsibilities including per-customer pricing,\n\ncart management and promotions.\n\nSerious amount of technological debt due to legacy code.\n\nScalability problems - the platform should be able to handle a new\n\nbusiness model that requires broadening the offer and entering new\n\nmarkets.\n\nThe online platform was generating 100M+ EUR revenue/year at the\n\ntime. The challenge was a serious one.\n\n99\n\nGo to Table of Contents",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "The architecture of this system resembled a \"death star\". However, its\n\ncomplexity was not between microservices, but between external\n\nsystems.\n\nThe ﬁrst instinct was to move the site 1:1 from legacy Magento 1 to a new\n\nplatform. OroCommerce and Magento 2 were considered.\n\nThe work on collecting business requirements from stakeholders inside\n\nthe company and putting them into the Business Requirements\n\nDocument (BRD) was quickly started. We formulated nearly 1,000\n\nbusiness requirements.Then we mapped them into features. Finally, we\n\nscored each available platform on its ability to meet the requirements:\n\nFunctionality available out of the box.\n\nFunctionality after customization.\n\nFunctionality requiring additional/external modules.\n\nNew features.\n\nWe double-checked both platforms in terms of technical solutions,\n\nscalability, performance, possibility of modiﬁcation and the possibility of\n\nfurther development.\n\nDuring the analysis, we realized that it would be somewhat risky to collect\n\nall the requirements for such a huge platform right away. We felt that\n\nbefore we had ﬁnished analyzing the requirements, they would have\n\nchanged a few times already. Brief research showed us that none of the\n\nsystems were capable of meeting all the speciﬁc requirements, both\n\nfunctional and non-functional. We realized this was not the right approach\n\nand could lead us back to where we started - a monolithic application. 100",
      "content_length": 1432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "The architecture of this system resembled a \"death star\". However, its\n\nBefore you decide to take a similar step (to go along with a ready-made\n\ncomplexity was not between microservices, but between external\n\nplatform in the center of a microservices eco-system), look at the pros &\n\nsystems.\n\ncons of this approach.\n\nThe ﬁrst instinct was to move the site 1:1 from legacy Magento 1 to a new\n\nPros & Cons of choosing an end-2-end platform:\n\nplatform. OroCommerce and Magento 2 were considered.\n\nPros:\n\nThe work on collecting business requirements from stakeholders inside\n\nthe company and putting them into the Business Requirements\n\nRapid development and time to market.\n\nDocument (BRD) was quickly started. We formulated nearly 1,000\n\nbusiness requirements.Then we mapped them into features. Finally, we\n\nIt’s usually a stable, well-tested product.\n\nscored each available platform on its ability to meet the requirements:\n\nA community that will help in solving many problems.\n\nFunctionality available out of the box.\n\nThe possibility to use a large base of ready-made, fully-featured\n\nFunctionality after customization.\n\nmodules (Magento Marketplace).\n\nFunctionality requiring additional/external modules.\n\nOfﬁcial support from the software provider.\n\nNew features.\n\nOfﬁcial updates, security patches.\n\nWe double-checked both platforms in terms of technical solutions,\n\nscalability, performance, possibility of modiﬁcation and the possibility of\n\nCons:\n\nfurther development.\n\nIt’s still a monolithic application that sooner or later will lead us to the\n\nDuring the analysis, we realized that it would be somewhat risky to collect\n\nstarting point - problems with scalability and maintenance.\n\nall the requirements for such a huge platform right away. We felt that\n\nbefore we had ﬁnished analyzing the requirements, they would have\n\nVery high licensing costs for the Enterprise version.\n\nchanged a few times already. Brief research showed us that none of the\n\nsystems were capable of meeting all the speciﬁc requirements, both\n\nfunctional and non-functional. We realized this was not the right approach\n\nand could lead us back to where we started - a monolithic application.\n\n101\n\nGo to Table of Contents",
      "content_length": 2204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Large platforms require specialists with speciﬁc skills for a particular\n\nsystem who can be difﬁcult to acquire.\n\nReady-made functionalities often requires serious modiﬁcations to fully\n\nadapt them, which can lead to incompatibility with the base system - no\n\nupdates or patches.\n\nThey often provide outdated solutions, limiting the introduction of\n\nmodern technologies.\n\nA New approach\n\nEventually, after conducting a feasibility study, we suggested that our\n\nclient use a more optimal way of solving the problem.\n\nThe fundamental assumption was to abandon migration to a new\n\nplatform and change the architecture by deconstructing the current\n\nsystem and deploying it as an eco-system of distributed microservices. In\n\norder to succeed, we needed an effective analysis and implementation\n\nprocess.\n\nGo to Table of Contents\n\n102",
      "content_length": 829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Large platforms require specialists with speciﬁc skills for a particular\n\nSystem architecture analysis (decomposition)\n\nMicroservice X architecture analysis\n\nMicroservice Y architecture analysis\n\nMicroservice Z architecture analysis\n\nIT Architects team\n\n...\n\nsystem who can be difﬁcult to acquire.\n\nReady-made functionalities often requires serious modiﬁcations to fully\n\nadapt them, which can lead to incompatibility with the base system - no\n\nMicroservices implementation team\n\nMicroservice X implementation\n\nMicroservice Y implementation\n\nMicroservice Z implementation\n\n...\n\nupdates or patches.\n\nThey often provide outdated solutions, limiting the introduction of\n\nMicroservice X adoption on Magento\n\nMagento developers team\n\n...\n\nmodern technologies.\n\nFig. 7: Agile analysis and implementation process to achieve goals.\n\nA New approach\n\nThe ﬁrst step of the \"architecture analysis\" process was the development\n\nof a high-level architecture of the entire system by a team of architects,\n\nEventually, after conducting a feasibility study, we suggested that our\n\nfocused on service responsibilities. The results of their work included:\n\nclient use a more optimal way of solving the problem.\n\nKey business processes supported by the system.\n\nThe fundamental assumption was to abandon migration to a new\n\nGoals and requirements for scalability, security, performance, SLA and\n\nplatform and change the architecture by deconstructing the current\n\npotential development directions.\n\nsystem and deploying it as an eco-system of distributed microservices. In\n\nIdentiﬁed risks.\n\norder to succeed, we needed an effective analysis and implementation\n\nBlock diagram of disclosed microservices:\n\nprocess.\n\nDeﬁned scope and responsibility of each service.\n\nPaReveal patterns of integration between services, taking into\n\naccount emergency situations handling, avoiding SPoF.\n\nDeﬁned events and business objects.\n\nHigh-level architecture diagram of the system.\n\nThe architects worked together along with the client. The client’s domain\n\nexperts were engaged in session-based workshops using the event\n\n103\n\nGo to Table of Contents",
      "content_length": 2117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "storming technique borrowed from the popular Domain Driven Design\n\n(DDD) domain modeling approach. You can ﬁnd more information on the\n\ntechnique on its creator’s blog:\n\nhttp://ziobrando.blogspot.com/2013/11/introducing-event-storming.html.\n\nBased on the collected data, the team provided the implementation team\n\nwith complete documentation.\n\nAfter several workshops, a distributed architecture with dedicated main\n\nareas/services was created with the following key services deﬁned:\n\nPRICING - managing individual prices and promotions for clients.\n\nPIM (product information management) - responsible for product\n\ninformation and attributes; with planned 1mln+ SKUs it must be\n\nimplemented as a scalable, probably NoSQL based data warehouse.\n\nWMS (warehouse management system) - product stock management.\n\nCRM (customer relationship management) - in charge of syncing data\n\nwith Pivotal CRM (orders, statuses, shopping carts …).\n\nREPORT - reporting and monitoring features.\n\nNOTIFY - user notiﬁcations and alerts management.\n\nREVIEW - product reviews system.\n\nRECOMMENDATIONS - recommendations engine.\n\nFRONTEND APP - in the ﬁrst version - the good, old Magento1; then\n\nit was planned to move this layer to a ReactJS + NodeJS thin client.\n\nMOBILE APP.\n\nWe started with a 20 page architecture document and then created a list\n\nof standards for coding each separate service.\n\nGo to Table of Contents\n\n104",
      "content_length": 1403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "storming technique borrowed from the popular Domain Driven Design\n\nWe\n\ntried\n\nto\n\nleverage\n\nthe HTTP protocol standards, providing\n\n(DDD) domain modeling approach. You can ﬁnd more information on the\n\ndocumentation and technical requirements, such as speciﬁc frameworks\n\ntechnique on its creator’s blog:\n\nand database servers to be used. It’s very important to make use of such\n\nsynthetic and consistent standards while dealing with distributed\n\nhttp://ziobrando.blogspot.com/2013/11/introducing-event-storming.html.\n\nsoftware.\n\nBased on the collected data, the team provided the implementation team\n\nWe decided to start by implementing the ﬁrst service that is critical for the\n\nwith complete documentation.\n\nsystem due to its SPoF and which would give us the best performance\n\nresults: PRICING and PIM.\n\nAfter several workshops, a distributed architecture with dedicated main\n\nareas/services was created with the following key services deﬁned:\n\nIt was crucial to ﬁgure out how to separate the platform from Pivotal CRM\n\nfor calculating end-client product prices and therefore to avoid a SPoF\n\nPRICING - managing individual prices and promotions for clients.\n\nand maintain High Availability (initially the platform used real-time\n\nPIM (product information management) - responsible for product\n\nWebService calls to get the prices from the CRM when users entered the\n\ninformation and attributes; with planned 1mln+ SKUs it must be\n\npage).\n\nimplemented as a scalable, probably NoSQL based data warehouse.\n\nWMS (warehouse management system) - product stock management.\n\nPIM was selected to solve problems with growing the SKUs database by\n\nCRM (customer relationship management) - in charge of syncing data\n\nmoving to an ElasticSearch NoSQL solution instead of Magento’s EAV\n\nwith Pivotal CRM (orders, statuses, shopping carts …).\n\nmodel.\n\nREPORT - reporting and monitoring features.\n\nNOTIFY - user notiﬁcations and alerts management.\n\nWe created these services as separate Symfony3 applications that were\n\nREVIEW - product reviews system.\n\nintegrated with the Magento1 frontend later on.\n\nRECOMMENDATIONS - recommendations engine.\n\nFRONTEND APP - in the ﬁrst version - the good, old Magento1; then\n\nRoughly speaking - we just removed the Magento1 modules responsible\n\nit was planned to move this layer to a ReactJS + NodeJS thin client.\n\nfor the catalog and wrote our own which called the micro-services instead\n\nMOBILE APP.\n\nof hitting the database.\n\nWe started with a 20 page architecture document and then created a list\n\nThen we followed this path further, by rewriting and exchanging\n\nof standards for coding each separate service.\n\nmonolithic modules with distributed services one by one.\n\n105\n\nGo to Table of Contents",
      "content_length": 2723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "The project was ﬁnished with a roughly cut-down Magento (serving only\n\nas an application frontend) and 9 services supporting all the business\n\nlogic. One day, if needed, we can simply move on from Magento,\n\nimplementing a new frontend using a ReactJS/NodeJS stack or any other\n\nmodern tech stack.\n\nBeginning\n\nIT Systems\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nESB\n\n...\n\n...\n\nMAGENTO\n\nGo to Table of Contents\n\n106",
      "content_length": 393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Step 1\n\nThe project was ﬁnished with a roughly cut-down Magento (serving only\n\nas an application frontend) and 9 services supporting all the business\n\nlogic. One day, if needed, we can simply move on from Magento,\n\nimplementing a new frontend using a ReactJS/NodeJS stack or any other\n\nIT Legacy systems\n\nmodern tech stack.\n\nBeginning\n\nERP\n\nCRM\n\nPIM\n\nWMS\n\nIT Systems\n\n...\n\n...\n\nESB\n\nWMS\n\nERP\n\nCRM\n\nPIM\n\n...\n\n...\n\nESB\n\nMicro Services\n\nMAGENTO\n\nPRICE\n\nPIM\n\nMAGENTO\n\nWMS\n\n...\n\n107\n\nGo to Table of Contents",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Step 2\n\nIT Systems\n\nIT Legacy systems\n\nXYZ Client\n\nERP\n\nPIM\n\nWMS\n\n...\n\nAPI Gateway\n\nMicro Services\n\nPRICE\n\nWMS\n\nPIM\n\nCRM\n\nREPORT\n\nNOTIFY\n\nOMS\n\nREVIEW\n\nRECOMMENDATION\n\nr e k o r B e g a s s e M\n\n...\n\nMagento Frontend Application\n\nMobile App\n\nFig. 8: Evolutionary (notrevolutionary) steps to create a new platform from a monolithic\n\napplication.\n\nGo to Table of Contents\n\nt n e\n\ni l\n\nC Z Y X\n\n108",
      "content_length": 394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Step 2\n\nEach service was designed with its own denormalized database\n\n(ElasticSearch or PerconaDB for relational data orders) and was designed\n\nwith high availability in mind. Data between services is exchanged via a\n\nRabbitMQ data bus using an Event Driven Data Management approach4.\n\nIT Systems\n\nIT Legacy systems\n\nXYZ Client\n\nt\n\nn\n\ne\n\ni\n\nl\n\nC\n\nZ\n\nERP\n\nPIM\n\n...\n\nWMS\n\nY\n\nX\n\nWe haven’t decided (at this point) to go with any technology other than\n\nPHP, so all services were implemented using the Symfony framework;\n\nmostly for simplicity, as well as cost optimization of the development\n\nprocess.\n\nAPI Gateway\n\nYou can ﬁnd more great technologies that focus on microservices later in\n\nMicro Services\n\nthis book and at https://github.com/mfornos/awesome-microservices.\n\nr\n\ne\n\nk\n\nPRICE\n\nCRM\n\nOMS\n\no\n\nr\n\nB\n\ne\n\nPIM\n\nNOTIFY\n\nRECOMMENDATION\n\ng\n\nTo sum-up our challenge please ﬁnd our notes on the pros and cons of\n\na\n\ns\n\ns\n\ne\n\nWMS\n\nREPORT\n\nREVIEW\n\nM\n\nthe microservice approach below:\n\n...\n\nPros:\n\nSmall teams can work in parallel to create new, and maintain current,\n\nservices. Many of you have probably experienced problems with working\n\nMagento Frontend Application\n\nMobile App\n\nin large teams, as we did.\n\nFig. 8: Evolutionary (notrevolutionary) steps to create a new platform from a monolithic\n\napplication.\n\nThe possibility of using heterogeneous technologies - ElasticSearch for\n\nproducts, PerconaDB for orders.\n\nIncreased critical fault-toleranceby using bulkheads/service contracts.\n\nIncremental replacement of legacy code and original systems with new,\n\neffective solutions.\n\n4 https://www.nginx.com/blog/event-driven-data-management-microservices/\n\n109",
      "content_length": 1657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Scalability - we can scale only the services that require it.\n\nProgrammers have a lot of fun, so it’s quite easy to keep the team\n\nmotivated.\n\nCons:\n\nExtensive client involvement is required during the BA phase.\n\nNew skills and quite a lot of architectural experience is required from\n\ndevelopers and architects to design the initial phases.\n\nNew challenges in maintaining the monitoring of the entire\n\ninfrastructure.\n\nMobile Commerce\n\nOne of the coolest features of the microservices architecture is that you’re\n\nno longer bound to your one-and-only platform. It’s crucial, particularly\n\nwhen the application at hand has to meet different expectations. In our\n\ncase - an eCommerce platform with dental equipment - we have three\n\ndifferent areas to be covered:\n\nState-of-the-art content management system with e-learning features.\n\nBasic eCommerce features - checkout and promotions for ordering\n\ndental equipment. CRM features, user proﬁles and segmentation for\n\ntracking all the users.\n\nThe platform was designed to work on mobile devices only.\n\nGo to Table of Contents\n\n110",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Scalability - we can scale only the services that require it.\n\nAt the start we considered whether or not to use one platform for the\n\nbackend, or maybe to write dedicated solutions. It’s hard to ﬁnd software\n\nwith enterprise\n\nlevel CMS, PIM, CRM and eCommerce features\n\nProgrammers have a lot of fun, so it’s quite easy to keep the team\n\naltogether.\n\nmotivated.\n\nTherefore we decided to go with the following software products:\n\nCons:\n\nPimcore - as a CMS and PIM; we created all the content (e-learning,\n\nExtensive client involvement is required during the BA phase.\n\n5\n\nstatic pages, product content) in Pimcore and expose it via API.\n\nNew skills and quite a lot of architectural experience is required from\n\nMagento2 - as a checkout and for eCommerce features.\n\ndevelopers and architects to design the initial phases.\n\nDedicated iOS and Android apps for the frontend.\n\nNew challenges in maintaining the monitoring of the entire\n\ninfrastructure.\n\nWe used the “Backend for Frontends” approach described in this eBook\n\nto provide optimized API gateways for both mobile applications and the\n\nMobile Commerce\n\nRWD website. Key areas like product content and e-learning pages were\n\nfully manageable in Pimcore and provided the end client with HTML\n\nOne of the coolest features of the microservices architecture is that you’re\n\nrenderings.\n\nno longer bound to your one-and-only platform. It’s crucial, particularly\n\nwhen the application at hand has to meet different expectations. In our\n\nMagento checkout was integrated using API REST calls for placing orders.\n\ncase - an eCommerce platform with dental equipment - we have three\n\ndifferent areas to be covered:\n\nNowadays, all new open source products (and of course, not just\n\nopen-source) expose most of their features via API. It’s cool to focus on\n\nState-of-the-art content management system with e-learning features.\n\nthe end client’s value (frontend) and not reinvent the wheel on the\n\nBasic eCommerce features - checkout and promotions for ordering\n\nbackend.\n\ndental equipment. CRM features, user proﬁles and segmentation for\n\ntracking all the users.\n\nWe did almost no custom development work on the backends!\n\nThe platform was designed to work on mobile devices only.\n\n5 http://pimcore.org - Enterprise grade Content Management platform, PIM and DAM\n\n111\n\nGo to Table of Contents",
      "content_length": 2332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Appendix 1: Microservices and unicycling by Alexander Graf\n\nThanks to Alexander Graf, the founder of Spryker.com for this part.\n\nInitially published\n\nas\n\na blog post on Alexanders’ blog:\n\nhttps://tech.spryker.com/microservices-and-unicycling-9ed452998b77.\n\nAfter the unspeakable NoSQL hype of about two years ago had reached\n\nits peak “Why are you still working with relational databases?”, the topic\n\nof microservices was brought to the fore in discussions about back-end\n\ntechnologies. In addition, with React, Node & Co., the front-enders have\n\ndeveloped quite a unique little game that, it seems, nobody else can see\n\nthrough. After about two years of Spryker, I have had the pleasure of\n\nbeing able to follow these technical discussions ﬁrst-hand. During my\n\ntime with the mail order giant Otto Group, there was another quite clearly\n\ndeﬁned technical boogeyman — the so-called Host System, or the AS400\n\nmachines, which were in use by all the main retailers. Not maintainable,\n\nancient, full of spaghetti code, everything depended on it, everything\n\nwould be better if we could be rid of it and so on and so forth — so I was\n\ntold. On the other side were the business clowns — I’m one, too — for\n\nwhom technology was just a means to an end. Back then, I thought those\n\nwho worked in IT were the real hard workers, pragmatic thinkers, who only\n\nanswered to the system and whose main goal was to achieve a high level\n\nof maintainability. Among business people there were, and there still are,\n\nthose I thought only busied themselves with absurd strategies and who\n\nbanged on about omnichannel, multi-channel, target group shops and\n\nthe like. Over the last eight years of Kassenzone.de, these strategies were\n\nalways my self-declared ﬁnal boss. It was my ultimate aim to disprove\n\nthem and demonstrate new approaches.\n\nGo to Table of Contents\n\n112",
      "content_length": 1851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "To my great disappointment, I have come to realize that people in IT — or\n\nAppendix 1: Microservices and unicycling by\n\n‘developers’ as they are called today — work with the same thought\n\nAlexander Graf\n\nprocesses as the business clowns. There is an extremely high tendency to\n\nchase after trends and basic technical problems are not sufﬁciently\n\nThanks to Alexander Graf, the founder of Spryker.com for this part.\n\nanalyzed, nor are they taken seriously enough. Microservices is a\n\nInitially published\n\nas\n\na blog post on Alexanders’ blog:\n\nwonderful example of this. It is neither an IT strategy, nor is it an\n\nhttps://tech.spryker.com/microservices-and-unicycling-9ed452998b77.\n\narchitecture pattern. At most, it describes just a type of IT and system\n\norganization. Just\n\nlike omnichannel. Omnichannel doesn’t mean\n\nAfter the unspeakable NoSQL hype of about two years ago had reached\n\nanything. It’s a cliché that is pretty much just ﬁlled with “blurb” and the\n\nits peak “Why are you still working with relational databases?”, the topic\n\nsame way of thinking is apparent on the topic of microservices. From the\n\nof microservices was brought to the fore in discussions about back-end\n\noutside, omnichannel can be seen as the result of strong growth if a\n\ntechnologies. In addition, with React, Node & Co., the front-enders have\n\ncompany’s range of services can, therefore, cause it to become a leader in\n\ndeveloped quite a unique little game that, it seems, nobody else can see\n\nmany channels. This is exactly what happens with microservices, which\n\nthrough. After about two years of Spryker, I have had the pleasure of\n\nmay be the result of strong growth in IT, because you have to divide large\n\nbeing able to follow these technical discussions ﬁrst-hand. During my\n\napplications into services so that you don’t have too many developers\n\ntime with the mail order giant Otto Group, there was another quite clearly\n\nworking on them at the same time. But this is far cry from being an IT\n\ndeﬁned technical boogeyman — the so-called Host System, or the AS400\n\nstrategy. In many conversations at the code.talks conference, this\n\nmachines, which were in use by all the main retailers. Not maintainable,\n\nimpression was (unfortunately) conﬁrmed. Yoav Kutner (founder of\n\nancient, full of spaghetti code, everything depended on it, everything\n\nMagento1) cut his teeth on the rollout of the ﬁrst of the big Magento1\n\nwould be better if we could be rid of it and so on and so forth — so I was\n\nprojects and reports with a shake of the head that developers always\n\ntold. On the other side were the business clowns — I’m one, too — for\n\nfollow the next hype without having considered where the real problem\n\nwhom technology was just a means to an end. Back then, I thought those\n\nlies. Yes, I know that that sounds all very general, but let’s have a closer\n\nwho worked in IT were the real hard workers, pragmatic thinkers, who only\n\nlook at the topic of microservices.\n\nanswered to the system and whose main goal was to achieve a high level\n\nof maintainability. Among business people there were, and there still are,\n\nMartin Fowler, IT guru and champion of microservices has written dozens\n\nthose I thought only busied themselves with absurd strategies and who\n\nof articles on the subject and describes microservices as follows:\n\nbanged on about omnichannel, multi-channel, target group shops and\n\nthe like. Over the last eight years of Kassenzone.de, these strategies were\n\nalways my self-declared ﬁnal boss. It was my ultimate aim to disprove\n\nthem and demonstrate new approaches.\n\n113\n\nGo to Table of Contents",
      "content_length": 3604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and are inde- pendently deployable by fully automated deployment ma- chinery. There is a bare minimum of centralized manage- ment of these services, which may be written in different programming languages and use different data storage technologies.\n\nThis does sound quite promising and it can also help with the\n\ncorresponding problems. Otto’s IT team has already reached the\n\nChampions League where this is concerned and produced the obligatory\n\narticle, called “On Monoliths and Microservices” on the subject. Guido\n\nfrom Otto also referred to this topic at the code.talks event:\n\nWhen we began the development of our new Online Shop otto.de, we chose a distributed, vertical-style architecture at an early stage of the process. Our experience with our previ- ous system showed us that a monolithic architecture does not satisfy the constantly emerging requirements. Growing volumes of data, increasing loads and the need to scale the organization, all of these forced us to rethink our approach.\n\nThere are also other examples which beneﬁt excellently from this\n\napproach. Zalando is an example of a company which is open about using\n\nit in “From Jimmy to Microservice”. The approach can also crop up for\n\nquickly growing tech teams, such as that of Siroop.\n\nGo to Table of Contents\n\n114",
      "content_length": 1579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "In short, the microservice architectural style is an approach to\n\nWhat’s often forgotten when people sing its praises are the costs\n\ndeveloping a single application as a suite of small services,\n\nassociated with such an approach. Martin Fowler calls these costs the\n\neach running in its own process and communicating with\n\nMicroservice Premium and clearly warns against proceeding in this\n\nlightweight mechanisms, often an HTTP resource API. These\n\ndirection without caution:\n\nservices are built around business capabilities and are inde-\n\npendently deployable by fully automated deployment ma-\n\nchinery. There is a bare minimum of centralized manage-\n\nThe microservices approach is all about handling a complex system, but in order to do so the approach introduces its own set of complexities. When you use microservices you have to work on automated deployment, monitoring, deal- ing with failure, eventual consistency, and other factors that a distributed system introduces. There are well-known ways to cope with all this, but it’s extra effort, and nobody I know in software development seems to have acres of free time. So my primary guideline would be don’t even consider micros- ervices unless you have a system that’s too complex to manage as a monolith.\n\nment of these services, which may be written in different\n\nprogramming languages and use different data storage\n\ntechnologies.\n\nThis does sound quite promising and it can also help with the\n\ncorresponding problems. Otto’s IT team has already reached the\n\nChampions League where this is concerned and produced the obligatory\n\narticle, called “On Monoliths and Microservices” on the subject. Guido\n\nfrom Otto also referred to this topic at the code.talks event:\n\nWhen we began the development of our new Online Shop\n\notto.de, we chose a distributed, vertical-style architecture at\n\nan early stage of the process. Our experience with our previ-\n\nous system showed us that a monolithic architecture does\n\nnot satisfy the constantly emerging requirements. Growing\n\nvolumes of data, increasing loads and the need to scale the\n\norganization, all of these forced us to rethink our approach.\n\nThere are also other examples which beneﬁt excellently from this\n\napproach. Zalando is an example of a company which is open about using\n\nit in “From Jimmy to Microservice”. The approach can also crop up for\n\nquickly growing tech teams, such as that of Siroop.\n\nFig. 29: Image: http://martinfowler.com/.\n\n115\n\nGo to Table of Contents",
      "content_length": 2482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Technically speaking, this restriction has many different origins. Whether\n\nit’s the latencies which must be called up for one procedure due to dozens\n\nof active services, clearly difﬁcult debugging, the demanding hardware\n\nsetup or the complex data retention (each service has its own database).\n\nThe fundamentally hard to manage technology zoo notwithstanding.\n\nHere, excellent parallels to eCommerce organizations can be drawn. Who\n\nis quicker and more effective in the development and scaling of new\n\nmodels:\n\n1. a company with dozens of departments and directorates, which must\n\nbe in a permanent state of agreement, but which are extremely good in\n\neach of their individual disciplines;\n\n2. or a company at which up to 100 employees sit in one room, all\n\nknowing what’s going on and all talking to each other.\n\nThe second example is quicker, that goes without saying. With larger\n\nmodels, at which scaling is a rather uniform approach, the ﬁrst example is\n\nbetter. Not much is different in the case of microservices. To understand\n\nthis context better, Werner Vogels’ (Amazon CTO) test on his Learnings\n\nwith AWS30 is highly recommended:\n\nWe needed to build systems that embrace failure as a natu- ral occurrence even if we did not know what the failure might be. Systems need to keep running even if the “house is on ﬁre.” It is important to be able to manage pieces that are impacted without the need to take the overall system down. We’ve developed the fundamental skill of managing the “blast radius” of a failure occurrence such that the overall health of the system can be maintained.\n\n30https://www.thoughtworks.com/insights/blog/monoliths-are-bad-design-and-you-know-it\n\nGo to Table of Contents\n\n116",
      "content_length": 1713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Technically speaking, this restriction has many different origins. Whether\n\nAlthough there are, therefore, really good guidelines for sufﬁcient\n\nit’s the latencies which must be called up for one procedure due to dozens\n\nhandling of the topic, so as to ﬁnd out whether microservices make sense\n\nof active services, clearly difﬁcult debugging, the demanding hardware\n\nfor an IT organization (not for most!), you can regularly ﬁnd contributions\n\nsetup or the complex data retention (each service has its own database).\n\nsuch as that by Sam Gibson³¹ online or on conference panels:\n\nThe fundamentally hard to manage technology zoo notwithstanding.\n\nHere, excellent parallels to eCommerce organizations can be drawn. Who\n\nIn principle, it is possible to create independent modules within a single monolithic application. In practice, this is seldom implemented. Code within the monolith most often, and quickly, becomes tightly coupled. Microservices, in con- trast, encourage architects and developers the opportunity to develop less coupled systems that can be changed faster and scaled more effectively.\n\nis quicker and more effective in the development and scaling of new\n\nmodels:\n\n1. a company with dozens of departments and directorates, which must\n\nbe in a permanent state of agreement, but which are extremely good in\n\neach of their individual disciplines;\n\nIn Kassenzone reader’s language, this pretty much means: Pure Play\n\n2. or a company at which up to 100 employees sit in one room, all\n\nbusiness models are good if they are implemented in an orderly fashion\n\nknowing what’s going on and all talking to each other.\n\nbut it will only really come good if you run many channels well. The\n\nwinning strategy is omnichannel. Now, you could simply brush such\n\nThe second example is quicker, that goes without saying. With larger\n\nstatements off, but it is astounding just how quickly and strongly such\n\nmodels, at which scaling is a rather uniform approach, the ﬁrst example is\n\nsimple thought processes spread and become the truth all by themselves.\n\nbetter. Not much is different in the case of microservices. To understand\n\nThe voices which oppose them32 are quiet in comparison, but the\n\nthis context better, Werner Vogels’ (Amazon CTO) test on his Learnings\n\narguments are quite conclusive.\n\nwith AWS30 is highly recommended:\n\nIn principle, it is possible to create independent modules within a single monolithic application. In practice, this is seldom implemented. Code within the monolith most often, and quickly, becomes tightly coupled. Microservices, in con- trast, encourage architects and developers the opportunity to develop less coupled systems that can be changed faster and scaled more effectively.\n\nWe needed to build systems that embrace failure as a natu-\n\nral occurrence even if we did not know what the failure\n\nmight be. Systems need to keep running even if the “house\n\nis on ﬁre.” It is important to be able to manage pieces that\n\nare impacted without the need to take the overall system\n\ndown. We’ve developed the fundamental skill of managing\n\nthe “blast radius” of a failure occurrence such that the overall\n\nhealth of the system can be maintained.\n\n³¹https://www.thoughtworks.com/insights/blog/monoliths-are-bad-design-and-you-know-it\n\n30https://www.thoughtworks.com/insights/blog/monoliths-are-bad-design-and-you-know-it\n\n³²http://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html\n\n117",
      "content_length": 3443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Technically and methodically, a lot is said for the use of “good”\n\nmonolithic structures for a great deal of eCommerce companies, but\n\ndoing so requires a lot of effort producing good code, something which,\n\nin the short term, you don’t have to do in the microservices world. If, then,\n\na mistake in the scaling arises, the affected CTOs would probably wish\n\nthey had the AS400 system back.\n\nThe founder of Basecamp has hit the nail on the head with his own\n\nsystem, which he describes as “The Majestic Monolith”33. And, where\n\ncontent is concerned, I’m with him:\n\nWhere things go astray is when people look at, say, Amazon or Google or whoever else might be commanding a ﬂeet of services, and think, hey it works for The Most Successful, I’m sure it’ll work for me too. Bzzzzzzzz!! Wrong! The patterns that make sense for organizations’ orders of magnitude larger than yours, are often the exact opposite ones that’ll make sense for you. It’s the essence of cargo culting. If I dance like these behemoths, surely I too will grow into one. I’m sorry, but that’s just not how the tango goes.\n\nIt’s bit like if companies who own an old bicycle, which they don’t know\n\nhow to ride properly, want a little too much. They see the unicyclist at the\n\ncircus performing dazzling tricks on his unicycle and say to themselves:\n\nMy bike is too old, that’s why I can’t ride it. I’ll just start with a unicycle\n\nright away, at least that’s forward-thinking.\n\n³³ https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\nGo to Table of Contents\n\n118\n\nAppendix 3: Blogs and resources\n\nThere are plenty of websites, blogs and books you can check to read\n\nmore about microservices and related architectural patterns. The book\n\n“Building Microservices, Designing Fine-Grained Systems” by Sam\n\nNewman and O’Reilly Media\n\n(http://shop.oreilly.com/product/0636920033158.do) should be at the\n\ntop of the top of your list. The most important information has been\n\ncollected into one place. It is all you need to know to model, implement,\n\ntest and run new systems using microservices or transform the monolith\n\ninto a distributed set of smaller applications. A must-have book for every\n\nsoftware architect. O’Reilly Media has also released another interesting\n\nbook, “Microservice Architecture” by Irakli Nadareishvili, Ronnie Mitra,\n\nMatt McLarty and Mike Amundsen\n\n(http://shop.oreilly.com/product/0636920050308.do), which is also worth\n\na read.\n\nWith knowledge from Sam Newman, you should be ready to discover\n\nwebsites like:\n\nhttp://microservices.io,\n\nhttps://github.com/mfornos/awesome-microservices,\n\nand https://dzone.com/ (under „microservices” keyword), curated lists\n\nof articles.\n\nIt’s a condensed dose of knowledge about core microservice patterns,\n\ndecomposition methods, deployment patterns, communication styles,\n\ndata management and many more… There you can also ﬁnd many\n\ninteresting presentations and talks recorded at conferences. The last\n\nwebsite speciﬁcally, https://dzone.com/, should be very interesting for IT\n\npeople.",
      "content_length": 3036,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Technically and methodically, a lot is said for the use of “good”\n\nAppendix 3: Blogs and resources\n\nmonolithic structures for a great deal of eCommerce companies, but\n\ndoing so requires a lot of effort producing good code, something which,\n\nThere are plenty of websites, blogs and books you can check to read\n\nin the short term, you don’t have to do in the microservices world. If, then,\n\nmore about microservices and related architectural patterns. The book\n\na mistake in the scaling arises, the affected CTOs would probably wish\n\n“Building Microservices, Designing Fine-Grained Systems” by Sam\n\nthey had the AS400 system back.\n\nNewman and O’Reilly Media\n\n(http://shop.oreilly.com/product/0636920033158.do) should be at the\n\nThe founder of Basecamp has hit the nail on the head with his own\n\ntop of the top of your list. The most important information has been\n\nsystem, which he describes as “The Majestic Monolith”33. And, where\n\ncollected into one place. It is all you need to know to model, implement,\n\ncontent is concerned, I’m with him:\n\ntest and run new systems using microservices or transform the monolith\n\ninto a distributed set of smaller applications. A must-have book for every\n\nWhere things go astray is when people look at, say, Amazon\n\nsoftware architect. O’Reilly Media has also released another interesting\n\nor Google or whoever else might be commanding a ﬂeet of\n\nbook, “Microservice Architecture” by Irakli Nadareishvili, Ronnie Mitra,\n\nservices, and think, hey it works for The Most Successful, I’m\n\nMatt McLarty and Mike Amundsen\n\nsure it’ll work for me too. Bzzzzzzzz!! Wrong! The patterns\n\nthat make sense for organizations’ orders of magnitude\n\n(http://shop.oreilly.com/product/0636920050308.do), which is also worth\n\nlarger than yours, are often the exact opposite ones that’ll\n\na read.\n\nmake sense for you. It’s the essence of cargo culting. If I\n\ndance like these behemoths, surely I too will grow into one.\n\nWith knowledge from Sam Newman, you should be ready to discover\n\nI’m sorry, but that’s just not how the tango goes.\n\nwebsites like:\n\nhttp://microservices.io,\n\nIt’s bit like if companies who own an old bicycle, which they don’t know\n\nhttps://github.com/mfornos/awesome-microservices,\n\nhow to ride properly, want a little too much. They see the unicyclist at the\n\nand https://dzone.com/ (under „microservices” keyword), curated lists\n\ncircus performing dazzling tricks on his unicycle and say to themselves:\n\nof articles.\n\nMy bike is too old, that’s why I can’t ride it. I’ll just start with a unicycle\n\nright away, at least that’s forward-thinking.\n\nIt’s a condensed dose of knowledge about core microservice patterns,\n\ndecomposition methods, deployment patterns, communication styles,\n\ndata management and many more… There you can also ﬁnd many\n\ninteresting presentations and talks recorded at conferences. The last\n\nwebsite speciﬁcally, https://dzone.com/, should be very interesting for IT\n\npeople.\n\n³³ https://m.signalvnoise.com/the-majestic-monolith-29166d022228#.90yg49e3j\n\n119\n\nGo to Table of Contents",
      "content_length": 3044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Depending on your time, you can subscribe to the newsletter\n\n“Microservices Weekly”\n\n(http://www.microservicesweekly.com) for a\n\nweekly set of articles about architecture and container-based virtualization\n\nor\n\nvisit\n\nthe Microservices\n\nsection\n\nat\n\nthe\n\nInfoQ website\n\n(https://www.infoq.com/microservices/), one of the most\n\nimportant\n\nwebsites with articles and talks related to software development.\n\nAs you can see, knowledge is all around us. Don’t forget about Martin\n\nFowler\n\nand\n\nhis\n\n“Microservice\n\nResource\n\nGuide”\n\n(https://martinfowler.com/microservices/). Martin is Chief Scientist at\n\nToughtWorks,\n\nthe\n\npublisher\n\nof\n\n“Technology\n\nRadar”\n\n(https://www.thoughtworks.com/radar; highly recommended as well) and\n\nauthor of a few bestselling books. Martin Fowler’s wiki is a Mecca for\n\nsoftware architects and “Microservice Resource Guide” is only one of\n\nthem…\n\nGo to Table of Contents\n\n120",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Depending on your time, you can subscribe to the newsletter\n\n“Microservices Weekly”\n\n(http://www.microservicesweekly.com) for a\n\nweekly set of articles about architecture and container-based virtualization\n\nor\n\nvisit\n\nthe Microservices\n\nsection\n\nat\n\nthe\n\nInfoQ website\n\n(https://www.infoq.com/microservices/), one of the most\n\nimportant\n\nwebsites with articles and talks related to software development.\n\nAs you can see, knowledge is all around us. Don’t forget about Martin\n\nFowler\n\nand\n\nhis\n\n“Microservice\n\nResource\n\nGuide”\n\n(https://martinfowler.com/microservices/). Martin is Chief Scientist at\n\nToughtWorks,\n\nthe\n\npublisher\n\nof\n\n“Technology\n\nRadar”\n\n(https://www.thoughtworks.com/radar; highly recommended as well) and\n\nauthor of a few bestselling books. Martin Fowler’s wiki is a Mecca for\n\nThank you!\n\nsoftware architects and “Microservice Resource Guide” is only one of\n\nthem…\n\nIf you want to know more\n\nabout microservices,\n\njust\n\ndrop me a message at\n\npkarwatka@divante.co.\n\nwww.divante.co\n\n121\n\nGo to Table of Contents",
      "content_length": 1029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Try Our Solution\n\nGo to Table of Contents\n\n122",
      "content_length": 46,
      "extraction_method": "Unstructured"
    }
  ]
}