{
  "metadata": {
    "title": "Serverless Architectures on AWS 2nd Edition - Peter Sbarski",
    "author": "Peter Sbarski, Yan Cui, Ajay Nair",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 257,
    "conversion_date": "2025-12-19T17:44:42.256614",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Serverless Architectures on AWS 2nd Edition - Peter Sbarski.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-11)",
      "start_page": 1,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nPeter Sbarski\nYan Cui\nAjay Nair\nSECOND EDITION\n\n\nPraise for the First Edition\n“A comprehensive, clear and very practical guide to making the best use of AWS\nthroughout an application’s lifecycle. Highly recommended for anyone wanting to\nuse AWS for real-life applications!”\n—Alain Couniot, Head of Enterprise Architecture, STIB-MIVB, Belgium\n“Peter’s tome not only dives deep on Lambda, it also covers all the AWS components\nyour apps will need to run serverless. A soup-to-nuts tour de force. Well done!”\n—Sean Hull, Founder, iHeavy, Inc.\n“A great introduction for those using AWS, who want to implement a serverless\narchitecture.”\n—John Huffman, Senior Technical Consultant, Summa Technologies\n“This book is a fantastic introduction to serverless architectures and AWS. I wish\nevery technical book was as well written and easy to read! The book walks you step-by-\nstep through building a video portal, including integrating AWS Lambda, API\nGateway, S3, auth0 and Firebase. By the end you feel confident not only that you\nunderstand all the pieces and how everything fits together, but also that you are ready\nto start building your own app.”\n—Kent R. Spillner, Sr. Software Engineer, DRW\n\n\nServerless Architectures on AWS\nSECOND EDITION\nPETER SBARSKI, YAN CUI, AJAY NAIR\nM A N N I N G\nSHELTER ISLAND\n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2022 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end.\nRecognizing also our responsibility to conserve the resources of our planet, Manning books are \nprinted on paper that is at least 15 percent recycled and processed without the use of elemental \nchlorine.\nThe author and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The author and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \nof the information herein.\nManning Publications Co.\nDevelopment editor: Toni Arritola\n20 Baldwin Road\nTechnical development editor: Brent Stains\nPO Box 761\nReview editor: Aleksandar Dragosavljević\nShelter Island, NY 11964\nProduction editor: Andy Marinkovich\nCopy editor: Frances Buran\nProofreader: Jason Everett\nTechnical proofreader: Niek Palm\nTypesetter: Gordan Salinovic\nCover designer: Marija Tudor\nISBN 9781617295423\nPrinted in the United States of America\n\n\n To my mum and dad, who always supported and\nencouraged my passion for computing. \n                                                                                       —Peter Sbarski\n \n \n To my wife, who always supports and encourages me, and\nputs up with all my late-night coding sessions.\n                                                                                       —Yan Cui\n \n \n To my wife, my kids, my brother, and my parents, thank you\nfor giving me the purpose and time to do this.\n                                                                                       —Ajay Nair\n \n \n \n \n \n \n \n \n \n \n \n\n\nv\nbrief contents\nPART 1\nFIRST STEPS.....................................................................1\n1\n■\nGoing serverless\n3\n2\n■\nFirst steps to serverless\n18\n3\n■\nArchitectures and patterns\n40\nPART 2\nUSE CASES.....................................................................55\n4\n■\nYubl: Architecture highlights, lessons learned\n57\n5\n■\nA Cloud Guru: Architecture highlights, lessons \nlearned\n70\n6\n■\nYle: Architecture highlights, lessons learned\n84\nPART 3\nPRACTICUM ...................................................................97\n7\n■\nBuilding a scheduling service for ad hoc tasks\n99\n8\n■\nArchitecting serverless parallel computing\n132\n9\n■\nCode Developer University\n146\nPART 4\nTHE FUTURE................................................................165\n10\n■\nBlackbelt Lambda\n167\n11\n■\nEmerging practices\n183\n\n\nvii\ncontents\npreface\nxiii\nacknowledgments\nxv\nabout this book\nxviii\nabout the authors\nxx\nabout the cover illustration\nxxii\nPART 1 FIRST STEPS ...........................................................1\n1 \nGoing serverless\n3\n1.1\nWhat’s in a name?\n4\n1.2\nUnderstanding serverless architectures\n5\nService-oriented architecture and microservices\n7\n■Implementing \narchitecture the conventional way\n7\n■Implementing architecture \nthe serverless way\n9\n1.3\nMaking the call to go serverless\n11\n1.4\nServerless pros and cons\n14\n1.5\nWhat’s new in this second edition?\n16\n2 \nFirst steps to serverless\n18\n2.1\nBuilding a video-encoding pipeline\n19\nA quick note on AWS costs\n19\n■Using Amazon Web Services (AWS)\n20\n\n\nCONTENTS\nviii\n2.2\nPreparing your system\n21\nSetting up your system\n22\n■Working with Identity and Access \nManagement (IAM)\n22\n■Let’s make a bucket\n25\n■Creating \nan IAM role\n26\n■Using AWS Elemental MediaConvert\n28\nUsing MediaConvert Role\n29\n2.3\nStarting with the Serverless Framework\n29\nSetting up the Serverless Framework\n29\n■Bringing Serverless \nFramework to The 24-Hour Video\n31\n■Creating your first Lambda \nfunction\n33\n2.4\nTesting in AWS\n36\n2.5\nLooking at logs\n37\n3 \nArchitectures and patterns\n40\n3.1\nUse cases\n40\nBackend compute\n41\n■Internet of Things (IoT)\n41\n■Data \nprocessing and manipulation\n42\n■Real-time analytics\n42\nLegacy API proxy\n43\n■Scheduled services\n44\n■Bots and \nskills\n44\n■Hybrids\n44\n3.2\nPatterns\n45\nGraphQL\n45\n■Command pattern\n46\n■Messaging \npattern\n47\n■Priority queue pattern\n49\n■Fan-out pattern\n50\nCompute as glue\n51\n■Pipes and filters pattern\n52\nPART 2 USE CASES ...........................................................55\n4 \nYubl: Architecture highlights, lessons learned\n57\n4.1\nThe original Yubl architecture\n58\nScalability problems\n59\n■Performance problems\n59\n■Long \nfeature delivery cycles\n59\n■Why serverless?\n60\n4.2\nThe new serverless Yubl architecture\n61\nRearchitecting and rewriting\n62\n■The new search API\n62\n4.3\nMigrating to new microservices gracefully\n64\n5 \nA Cloud Guru: Architecture highlights, lessons learned\n70\n5.1\nThe original architecture\n71\nThe journey to 43 microservices\n75\n■What is GraphQL\n77\nMoving to GraphQL\n79\n■Service discovery\n80\n■Security in \nthe BFF world\n82\n5.2\nRemnants of the legacy\n82\n\n\nCONTENTS\nix\n6 \nYle: Architecture highlights, lessons learned\n84\n6.1\nIngesting events at scale with Fargate\n85\nCost considerations\n85\n■Performance considerations\n85\n6.2\nProcessing events in real-time\n86\nKinesis Data Streams\n86\n■SQS dead-letter queue (DLQ)\n87\nThe Router Lambda function\n88\n■Kinesis Data Firehose\n88\nKinesis Data Analytics\n89\n■Putting it altogether\n90\n6.3\nLessons learned\n91\nKnow your service limits\n91\n■Build with failure in mind\n93\nBatching is good for cost and efficiency\n94\n■Cost estimation is \ntricky\n95\nPART 3 PRACTICUM .........................................................97\n7 \nBuilding a scheduling service for ad hoc tasks\n99\n7.1\nDefining nonfunctional requirements\n101\n7.2\nCron job with EventBridge\n102\nYour scores\n104\n■Our scores\n105\n■Tweaking the \nsolution\n107\n■Final thoughts\n109\n7.3\nDynamoDB TTL\n109\nYour scores\n110\n■Our scores\n111\n■Final \nthoughts\n113\n7.4\nStep Functions\n113\nYour scores\n115\n■Our scores\n115\n■Tweaking the \nsolution\n116\n■Final thoughts\n119\n7.5\nSQS\n119\nYour scores\n120\n■Our scores\n120\n■Final thoughts\n122\n7.6\nCombining DynamoDB TTL with SQS\n122\nYour scores\n123\n■Our scores\n124\n■Final thoughts\n125\n7.7\nChoosing the right solution for your application\n125\n7.8\nThe applications\n125\nYour weights\n126\n■Our weights\n126\n■Scoring the solutions \nfor each application\n128\n8 \nArchitecting serverless parallel computing\n132\n8.1\nIntroduction to MapReduce\n133\nHow to transcode a video\n134\n■Architecture overview\n135\n\n\nCONTENTS\nx\n8.2\nArchitecture deep dive\n137\nMaintaining state\n138\n■Step Functions\n141\n8.3\nAn alternative architecture\n144\n9 \nCode Developer University\n146\n9.1\nSolution overview\n147\nRequirements listed\n147\n■Solution architecture\n148\n9.2\nThe Code Scoring Service\n150\nSubmissions Queue\n152\n■Code Scoring Service summary\n153\n9.3\nStudent Profile Service\n153\nUpdate Student Scores function\n155\n9.4\nAnalytics Service\n157\nKinesis Firehose\n158\n■AWS Glue and Amazon Athena\n160\nQuickSight\n163\nPART 4 THE FUTURE ......................................................165\n10 \nBlackbelt Lambda\n167\n10.1\nWhere to optimize?\n167\n10.2\nBefore we get started\n169\nHow a Lambda function handles requests\n169\n■Latency: \nCold vs. warm\n173\n■Load generation on your function and \napplication\n173\n■Tracking performance and availability\n174\n10.3\nOptimizing latency\n176\nMinimize deployment artifact size\n176\n■Allocate sufficient \nresources to your execution environment\n178\n■Optimize function \nlogic\n179\n10.4\nConcurrency\n180\nCorrelation between requests, latency, and concurrency\n181\nManaging concurrency\n181\n11 \nEmerging practices\n183\n11.1\nUsing multiple AWS accounts\n184\nIsolate security breaches\n184\n■Eliminate contention for shared \nservice limits\n185\n■Better cost monitoring\n185\n■Better \nautonomy for your teams\n185\n■Infrastructure-as-code for \nAWS Organizations\n186\n\n\nCONTENTS\nxi\n11.2\nUsing temporary stacks\n186\nCommon AWS account structure\n186\n■Use temporary stacks for \nfeature branches\n187\n■Use temporary stacks for e2e tests\n188\n11.3\nAvoid sensitive data in plain text in environment variables\n188\nAttackers can still get in\n189\n■Handle sensitive data \nsecurely\n189\n11.4\nUse EventBridge in event-driven architectures\n190\nContent-based filtering\n190\n■Schema discovery\n191\n■Archive \nand replay events\n191\n■More targets\n192\n■Topology\n192\nappendix  A\nServices for your serverless architecture\n195\nappendix  B\nSetting up your cloud\n200\nappendix  C\nDeployment frameworks\n212\nindex\n225\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 12-20)",
      "start_page": 12,
      "end_page": 20,
      "detection_method": "topic_boundary",
      "content": "xiii\npreface\nServerless technologies occupy an exciting space at the moment. Products like AWS\nLambda and DynamoDB have been around for a few years, yet they still feel new and\nthrilling, sometimes mysterious or puzzling. Many folks worldwide discuss, learn, and\nimplement systems with serverless architectures, yet we haven’t yet seen a mass level of\nadoption like that of containers. Cloud providers such as AWS continue to grow. How-\never, individuals and organizations still ask questions such as, is serverless right for me,\nand how do I architect a system correctly from the myriad of available components\nand options? \n We’ve written this book to address some of the more interesting questions we’ve\nseen across the industry and our technical community. We decided to look at use cases\nfor serverless and explore problems that usually wouldn’t seem like a good fit. More\nimportantly, we’ve tried to convey what it is to have a serverless-first mindset. Our rec-\nipe is simple: When you have a problem, offload as much of the undifferentiated\nheavy lifting onto AWS or another provider and apply the principles of serverless\narchitectures. And, if that doesn’t produce a satisfactory answer, only then go and look\nat other technologies that may help. It’s important to reiterate that you should always\nuse the right tool for the right job. However, having a set of principles and practices,\nlike viewing a potential solution through a serverless prism first, gives you a map and\nhelps make better, more robust decisions.\n This book shows a few examples of us doing it in practice. We discuss how to\napproach several problems using serverless architectures, what criteria to consider, and\nhow to deal with architectural trade-offs. We also present three real-world companies\n\n\nPREFACE\nxiv\nthat have built interesting systems using serverless architectures. These companies\ndealt with the same kinds of problems you might be solving right now, so it’s worth\nchecking out those chapters to see what potential solutions or ideas exist. \n If you are entirely new to serverless architectures, do not worry! The first three\nchapters introduce you to serverless and even get you building a small application. If\nyou are an expert already, you will enjoy the last two chapters that go deeper into AWS\nLambda and discuss emerging practices. And, before we let you go, one other thing:\nthe vast majority of this second edition is new. If you read our first edition, we think\nthat you will find this a very different book. We hope you find something interesting\nand helpful in this book and come with us on this exciting serverless journey.\n\n\nxv\nacknowledgments\nThe second edition of Serverless Architectures on AWS couldn’t have been written with-\nout the encouragement and support from my peers, colleagues, family, and friends. I\nam lucky to be surrounded by passionate technologists who continuously encourage,\ngive feedback, and provide invaluable advice.\n First and foremost, I want to say thank you to my two co-authors: Yan Cui and Ajay\nNair. I am fortunate to know these two fantastic world-class experts to whom educa-\ntion and community is always foremost. I cannot describe how thankful I am to Yan\nand Ajay for helping to write this book and making it uniquely special among the\ntechnical literature available today. I am forever grateful to both of you for being\nthere through this journey, teaching me, and sharing the benefit of your experience.\n Second, I would like to thank our editor, Toni Arritola, who once again made the\nwriting of this book a great experience. Toni did a lot of work on the first edition of\nthis book, and she worked just as hard on the second edition. It bears repeating again\nthat Toni’s thoughtful feedback on the book’s structure, language, and narrative was\nextraordinarily helpful. And, after all these years of dealing with my slipping dead-\nlines, her attention to detail and enthusiasm kept the book and its authors going.\n It goes without saying that I want to thank Sam Kroonenburg too. Sam originally\nintroduced me to AWS Lambda and the serverless mindset. He co-founded A Cloud\nGuru, the first truly serverless startup, and gave me the opportunity to hone my skills.\nIf it wasn’t for Sam and my experience at A Cloud Guru, this book wouldn’t exist. I\nwould be amiss if I also didn’t thank Ryan Kroonenburg, the other co-founder of A\nCloud Guru and Sam’s brother. Both Sam and Ryan played a big part in the\n\n\nACKNOWLEDGMENTS\nxvi\npopularization of serverless technologies with A Cloud Guru, and also the founding of\nthe first technology conference focused entirely on serverless called Serverlessconf\n(ask me for stories over a drink!). Thank you, Sam and Ryan! \n I’d also like to thank a few others who for years have given me great feedback and\nencouragement. A big thank you to Tim Wagner, Drew Firment, Allan Brown, Nick\nTriantafillou, Tait Brown, Alicia Cheah, Forrest Brazeal, Peter Hanssens, Kim Bonilla,\nIlia Mogilevsky, as well as my fellow AWS serverless heroes and all my colleagues and\nfriends at A Cloud Guru/Pluralsight. I’d also like to thank Mike Stephens from Man-\nning for helping to bring this book to fruition. \n To all the reviewers: Aliaksandra Sankova, Bonnie Malec, Borko Djurkovic, Camal\nÇakar, Carl Nygard, Chris Kottmyer, Christopher Fry, Daniel Vásquez, Eugene Serdi-\nouk, Giampiero Granatella, Gregory Reshetniak, Javier Collado Cabeza, Jose San\nLeandro, Julien Pohie, Kelly E. Hair, Kirstie G. McKenzie, Lucian-Paul Torje, Matteo\nGildone, Michael Kumm, Michal Rutka, Miguel Montalvo, Mikołaj Wawrzyniak, Pat-\nrick Steger, Paul Mcilwaine, Robert Kulagowski, Sal DiStefano, Sau Fai Fong, Shaun\nHickson, Steve Hansen, Valeriy Arsentyev, Vignesh Muthuthurai, and William Dixon,\nyour suggestions and feedback made this a better book. \n Finally, I’d like to thank my family, including my dad, my brothers Igor and Dimi-\ntri, and their spouses Rita and Alexandra. They’ve had to find more strength to listen\nto me go on about the book for yet another year. And thank you to Durdana Masud,\nwho helped me greatly throughout my writing, with both the first edition and the sec-\nond edition.\n                    —Peter Sbarski\nI would like to thank Peter Sbarski for the opportunity to contribute to this book, and\nToni Arritola for her help and guidance every step of the way. It has truly been a plea-\nsure and honor to work with them over the past 12 months.\n I would also like to thank Anahit Pogosova for sharing details of the amazing work\nthat she and her team at Yle have done. The knowledge she shared with me was very\nvaluable and contained so many useful and actional tips for anyone building data\npipelines using serverless technologies. I hope I have done her work justice in chapter\n6, even though I had to leave out so much. We can easily fill a whole book with the\ninformation she shared with me.\n I would also like to thank a few friends and colleagues who have given me opportu-\nnities and guidance along the way. I wouldn’t be the man I am today without you, and\nyour friendship means everything to me; I can’t wait to catch up with you all in person\nsoon. Big thanks to Darryl Jennings, Tom Newton, Brett Johansen, Domas Lasauskas,\nScott Smethurst, Diana Ionita, Simon Coutts, Bruno Tavares, Heitor Lessa, Erez\nBerkner, Aviad Mor, John Earner, Simone Basso, and Alessandro Simi.\n Last, but not least, I would like to thank my wonderful wife, Yinan Xue, for all the\nsupport and encouragement she has given me and continues to give me over the\nyears. You are my best friend and the love of my life, and I look forward to growing old\nand wrinkly with you!\n\n\nACKNOWLEDGMENTS\nxvii\n Oh, I almost forgot, I would like to thank my cat, Ada, for bringing so much joy\ninto our lives and all the love she has given us. That scar you left on my thigh five years\nago is still visible to this day, I really . . . wait a minute. . . .\n             —Yan Cui\nI always hoped to create a lasting contribution to the developer community and am so\nexcited to see that finally happen with the second edition of Serverless Architectures on\nAWS. My biggest thanks to Peter Sbarski for making this happen and for the opportunity\nto create this work with Yan Cui and him. It has been an honor and a pleasure to be a\npart of the team with these serverless luminaries. Thank you to the crew at Manning, and\nour editor Toni Arritola, for their everlasting patience, thoroughness, and guidance.\n This book is dedicated to the serverless community. We at AWS and other provid-\ners may build the technology, but it is you, the community and the customers, that put\nit to work to the benefit of the world. I hope this book captures the passion, depth,\nand breadth that you deserve. Keep raising the bar and changing the world, one event\nat a time.\n Finally, a special shout out to Tim Wagner for getting the whole serverless universe\nstarted.\n             —Ajay Nair\n\n\nxviii\nabout this book\nServerless technologies and architectures are fascinating and unique. They present a\ndifferent way of building software in a cloud environment. This is because serverless is\nabout offloading the undifferentiated heavy lifting to others, reducing certain opera-\ntional concerns, moving toward event-driven computing, and giving yourself space to\nfocus on what’s important—the core goals of your business or project. This book\nteaches about the serverless approach to the design of systems. You will read how\nother companies have solved problems using a serverless approach on AWS and dive\ninto numerous discussions about architecture. \n Along the way, you will learn more about event-driven computing, useful design\npatterns, organizing and deploying your code, and security. This book isn’t a collec-\ntion of tutorials you can find online. Instead, it is an attempt to share our thinking\nand understanding of the future of cloud computing, which we think is serverless.\n This book is in four parts. The first part takes you through basic serverless princi-\nples as well as crucial architectures and patterns. You will also build a small serverless\napplication in AWS to get your hands dirty. It’ll be a fun one; your application will con-\nvert video files from one format to another without running a server.\n The second part focuses on three case studies from Yubl, A Cloud Guru, and Yle.\nYou will read how other companies have solved business and technical challenges with\na serverless approach. The third part is about architecture. Here you will learn how to\nadopt the serverless-first mindset, think about the pros and cons of different architec-\ntural implementations, and tackle unexpected challenges. The three examples we\n\n\nABOUT THIS BOOK\nxix\npresent are all different, showing that a serverless approach to the design of systems is\nversatile and flexible. \n The fourth and final part of the book looks at the internals of AWS Lambda and\nemerging AWS practices. If you are already an expert on AWS and serverless, you may\nfind this section to be particularly fascinating. \n The second edition of Serverless Architectures on AWS is for serverless veterans and\nbeginners alike. No matter your experience, we think you will find something valuable\nin these pages. We hope that this book will inspire you to think serverless first. Now,\nlet’s read and build!\nAbout the code\nThis book provides many examples of code. These appear throughout the text and as\nseparate code listings. To accommodate long lines of code, listings include line-\ncontinuation markers (➥). Code appears in a fixed-width font just like this, so\nyou’ll know when you see it.\n This book is about architecture and, as such, it is not heavy on source code. Chap-\nter 2 is the only practical chapter. The source for chapter is available on GitHub at\nhttp://github.com/sbarski/serverless-architectures-aws-2. If you’d like to contribute,\nopen a pull request and we’d be happy to consider your changes. If you see a prob-\nlem, please file an issue. \nliveBook discussion forum\nPurchase of Serverless Architectures on AWS, Second Edition includes free access to liveBook,\nManning’s online reading platform. Using liveBook’s exclusive discussion features, you\ncan attach comments to the book globally or to specific sections or paragraphs. It’s a\nsnap to make notes for yourself, ask and answer technical questions, and receive\nhelp from the authors and other users. To access the forum, go to    https://livebook\n.manning.com/#!/book/serverless-architectures-on-aws-second-edition/discussion. You\ncan also learn more about Manning’s forums and the rules of conduct at https://livebook\n.manning.com/#!/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the authors can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe authors, whose contribution to the forum remains voluntary (and unpaid). We\nsuggest you try asking the authors some challenging questions lest their interest stray!\nThe forum and the archives of previous discussions will be accessible from the pub-\nlisher’s website as long as the book is in print.\n\n\nxx\nabout the authors\nPETER SBARSKI is VP of Education & Research at A Cloud Guru, AWS Serverless Hero, and\nthe organizer of Serverlessconf, the world’s first conference dedicated entirely to server-\nless architectures and technologies. His work at A Cloud Guru allows him to research and\nwrite about serverless architectures, cloud computing and AWS. Peter is always happy to\ntalk about serverless technologies at conferences and meetups year round. His other pas-\nsions include technical education, and innovation in technology and cloud computing.\nPeter holds a Ph.D. in Computer Science from Monash University, Australia. He can be\nfound on Twitter (@sbarski) and LinkedIn (linkedin.com/in/petersbarski).\nYAN CUI is a developer advocate at Lumigo and an independent consultant who helps cli-\nents around the world go faster for less by successfully adopting serverless technologies.\nHe has over a decade of experience running production workloads at scale on AWS and\nhas worked as architect and principal engineer within a variety of industries including\nbanking, e-commerce, sports streaming, and mobile gaming. Yan is an AWS Serverless\nHero and a regular speaker at conferences internationally. He is the author of Production-\nReady Serverless (Manning, 2018) and co-author of F# Deep Dives (Manning, 2014), and he\nhas also self-published several popular courses such as the AppSync Masterclass. He can\nbe found on Twitter (@theburningmonk) and LinkedIn (linkedin.com/in/theburning-\nmonk) and writes regularly on his blog (theburningmonk.com).\nAJAY NAIR is a Director of Product and Engineering with Amazon Web Services. He is the\nfounding product leader for AWS Lambda and helped build the AWS serverless portfolio\nover the last several years. Ajay has spent his career focusing on cloud native platforms,\n\n\nABOUT THE AUTHORS\nxxi\ndeveloper productivity, and big data systems. He loves spending his days helping\ndevelopers do more with less and delighting customers with the power of technology.\nAjay holds a Masters in Information Systems Management from Carnegie Mellon, USA,\nwith a Bachelors in Electrical and Electronics Engineering from Kerala University, India.\nYou can find Ajay sharing thoughts on everything from serverless to product\nmanagement on Twitter (@ajaynairthinks) or on LinkedIn (linkedin.com/in/ajnair).\n",
      "page_number": 12
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 21-29)",
      "start_page": 21,
      "end_page": 29,
      "detection_method": "topic_boundary",
      "content": "xxii\nabout the cover illustration\nThe figure on the cover of Serverless Architectures on AWS, Second Edition is “Man from\nStupno/Sisak, Croatia,” from a book by Nikola Arsenović, published in 2003. The\nbook includes finely colored illustrations of figures from different regions of Croatia,\naccompanied by descriptions of the costumes and of everyday life.\n In those days, it was easy to identify where people lived and what their trade or sta-\ntion in life was just by their dress. Manning celebrates the inventiveness and initiative\nof today’s computer business with book covers based on the rich diversity of regional\nculture centuries ago, brought back to life by pictures from collections such as this\none.\n\n\nPart 1\nFirst steps\nIf you are new to serverless architectures, you’ve come to the right place. The\nfirst three chapters of this book will give you an introduction to this exciting\ntechnology and even get you to build a small serverless application of your own.\nThe first chapter provides an overview of serverless technologies and a discus-\nsion about where we are today. The second chapter is more practical; it focuses\non giving you a hands-on experience with AWS and services such as AWS\nLambda. The third chapter describes popular and useful serverless patterns.\nLet’s get started!\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n3\nGoing serverless\nIf you ask software developers what software architecture is you might get answers\nranging from “it’s a blueprint or a plan” to “a conceptual model” to “the big pic-\nture.” This book is about an emerging architectural approach that has been\nadopted by developers and companies around the world to build their modern\napplications—serverless architectures. \n Serverless architectures have been described as somewhat of a “nirvana” for an\napplication architectural approach. It promises developers the ability to iterate as\nfast as possible while maintaining business critical latency, availability, security, and\nperformance guarantees, with minimal effort on the developers’ part.\n This book teaches you how to think about serverless systems that can scale and\nhandle demanding computational requirements without having to provision or\nThis chapter covers\nTraditional system and application architectures\nKey characteristics and benefits of serverless \narchitectures\nHow serverless architectures and microservices \nfit into the picture\nConsiderations when transitioning from server to \nserverless\nWhat’s new in this second edition?\n\n\n4\nCHAPTER 1\nGoing serverless\nmanage a single server. Importantly, this book describes techniques that can help\ndevelopers quickly deliver products to market while maintaining a high level of quality\nand performance by using services and architectures offered by today’s cloud platforms.\n1.1\nWhat’s in a name?\nBefore going in any further, we think it’s important to come to terms with the word\nserverless. There are various attempts at this already, including an official one from\nAWS (https://aws.amazon.com/serverless/) and a community favorite from Martin\nFowler (https://martinfowler.com/articles/serverless.html). Here’s how we define it:\nDEFINITION\nServerless is a qualifier that can be applied to any software or ser-\nvice offering, which requires that it is consumed as a utility service and incurs\ncost only when used.\nSimple enough, right? But there’s a lot to unpack in that simple definition. Let’s dive\ninto each of the following two required criteria to call something serverless:\nConsumed as a utility service—The “software as a service” consumption model is\nwell understood. It means that anyone using the software uses a prescribed\napplication programming interface (API) or web interface to use the software\nand customize it, while staying within any published constraints for the software\nand usage policies for the API. Salesforce, Office365, and Google Maps are well-\nknown software packages delivered as a service. What’s key here is that the\nactual infrastructure (servers, networking, storage, etc.) hosting the software\nand powering the API is completely abstracted from you as the consumer; all\nthat is visible (and all that matter) is what the API permits. \nA service also typically comes with accompanying availability, reliability, and\nperformance guarantees from the service provider. A utility service, further, has\nthe billing characteristics that we’d expect from any utility computing offering;\nthat is, you pay for usage not for reservation, subscriptions, or provisioning. All\nexisting public cloud offerings have some form of utility billing associated with\nthem. For example, Amazon Elastic Compute Cloud (EC2) allows you to pay by\nthe second for the rent of virtual machines.\nIncurs cost only when used—This means there’s zero cost for having the software\ndeployed and ready to use. Think of this as the same cost model we expect from\nour public utilities like electricity and water. You, as the consumer, pay a per\ngranular usage unit cost if you use any, but you pay zero if you use nothing. This\naspect of pure usage-based pricing is a distinguishing criterion of serverless\nofferings from the other utility services that came before it.\nIn the rest of the book, we will use the “serverless” qualifier only for software that fits\nthese criteria. For example, software that requires you to provide a server to host a\nwebsite (like the Apache web server) would not qualify because it does not meet the\nfirst criterion. Software that is available as a service but requires you to pay by subscrip-\ntion (like Salesforce) would not qualify as well because it does not meet the second\n\n\n5\nUnderstanding serverless architectures\ncriterion. A serverless architecture, by extension, is one composed entirely of serverless\ncomponents. But which components of an architecture need to be serverless for it to\nbe called as such? Let’s look at this next with an example.\n1.2\nUnderstanding serverless architectures\nLet’s take the example of a typical data-driven web application, not unlike the systems\npowering most of today’s web-enabled software. These typically consist of a backend\n(server) that accepts requests from a client and then processes the requests. \n The backend server performs various forms of computation, and the frontend cli-\nent provides an interface for users to operate via their browser, mobile, or desktop\ndevice. Data might travel through numerous application layers before being saved to a\ndatabase. The backend then generates a response that could be in the form of JSON\nor in fully rendered markup, which is sent back to the client (figure 1.1). These kinds\nof applications are conventionally architected as tiers (a presentation tier that controls\nhow the information is captured and provided to the user, an application tier that\ncontrols the business logic of the application, and a data tier with the database and\ncorresponding access controls).\nJust to clear up any misperceptions . . .\nOne of the common misunderstandings is that the “-less” in “serverless” implies\n“absence of or without” (think “sugarless,” “boneless,” and so on), which leads to\nsome colorful debates on social media on how any application architecture can claim\nto run without servers. We think “-less” here means “invisible in context of usage”\n(think “wireless,” “tasteless”). There obviously are servers somewhere! The differ-\nence is that these servers are hidden from you. There’s no infrastructure for you to\nthink about and no way to tweak the underlying operating system or virtual hardware\nconfiguration. Someone else takes care of the nitty-gritty details of infrastructure\nmanagement, freeing you from that operational overhead and giving back to you the\nmost expensive commodity there is—time.\n1. User performs an action \nthat requires data from a \ndatabase to be displayed.\n2. A request is formed \nand sent from the client \nto the web server.\n3. The request is \nprocessed and the \ndatabase is queried.\n4. Data is retrieved.\n5. An appropriate response \nis generated and sent back.\n6. Information is displayed \nto the user.\nApplication user\nWeb client\n(presentation tier)\nWeb server\n(application tier)\nDatabase\n(data tier)\nFigure 1.1\nA basic request-response (client-server) message-exchange pattern that most developers \nare familiar with. There’s only one web server and one database in this figure. Most systems are much \nmore complex.\n\n\n6\nCHAPTER 1\nGoing serverless\nSoftware architectures have evolved from the days of code running on a mainframe to\na multitier architecture where the presentation, data, and application/logic tiers are\ntraditionally separated. Within each tier, there may be multiple logical layers that deal\nwith the particular aspects of functionality or domain. There are also cross-cutting\ncomponents such as logging or exception handling systems that can span numerous\nlayers. The preference for layering is understandable. Layering allows developers to\ndecouple concerns and have more maintainable applications. Figure 1.2 shows an\nexample of a tiered architecture with multiple layers including the API, the business\nlogic, the user authentication component, and the database. \nApplication user\nUser interface components\nLayering helps to \nsegregate concerns, but \nmore layers can also \nmake changes harder \nand slower to implement.\nCross-cutting concerns \nspan numerous layers. \nA good example of this is \nlogging, which can happen \nat every layer.\nApplication tier\nCross-cutting\nconcerns\nPresentation\ntier\nData tier\nPresentation logic\nClient-side model\nClient-side service layer\nApplication programming interface\nServer-side service layer\nBusiness/domain layer\nBusiness entities/model\nData access/persistence layer\nException management\nCaching\nLogging\nCommunications\nSecurity\nDatabase\nFile storage\nFigure 1.2\nA typical three-tier application is usually made up of presentation, application, and data \ntiers. A tier can have multiple layers with specific responsibilities.\nTiers vs. layers\nThere is some confusion among developers about the difference between layers and\ntiers. A tier is a module boundary that provides isolation between major components\nof a system. For example, a presentation tier that’s visible to the user is separate\nfrom the application tier, which encompasses the business logic. In turn, a data tier\nis another separate system that manages, persists, and provides access to data.\nComponents grouped in a tier can physically reside on different infrastructures.\nLayers are logical slices that carry out specific responsibilities in an application. Each\ntier can have multiple layers that are responsible for different elements of function-\nality, such as domain services.\n\n\n7\nUnderstanding serverless architectures\n1.2.1\nService-oriented architecture and microservices\nOne blunt approach would be to combine all the layers (the API, the business logic,\nthe user authentication) into one single, monolithic code base. This may sound like\nan antipattern today, but that was indeed the approach we adopted in the early days of\ncloud-based development. Most modern approaches, however, dictate that you archi-\ntect with reusability, autonomy, composability, and discoverability in mind. \n Among the veterans of our industry, service-oriented architecture (SOA) is a well-\nknown buzzword. SOA encourages an architectural approach in which developers cre-\nate autonomous services that communicate via message passing and often have a\nschema or a contract that defines how messages are created or exchanged. \n The modern incarnation of the service-oriented approach is often referred to as\nmicroservices architecture. Modern application architectures are composed of ser-\nvices communicating through events and APIs with business logic inserted as appro-\npriate. We define microservices as small, standalone, fully independent services built\naround a particular business purpose or capability. Ideally, microservices should be\neasy to replace, with each service written in an appropriate framework and language. \n The mere fact that microservices can be written in a different general-purpose lan-\nguage or a domain-specific language (DSL) is a drawing card for many developers.\nBenefits can be gained from using the right language or a specialized set of libraries\nfor the job. Each microservice can maintain state and store data. And if microservices\nare correctly decoupled, development teams can work and deploy microservices inde-\npendently from one another. This approach of building and deploying applications as\na collection of loosely coupled services is considered the default approach to develop-\nment in the cloud today (the “cloud native” approach, if you will).\n1.2.2\nImplementing architecture the conventional way\nOnce you have decided how your application is going to be architected, and all the\nsoftware required for each of the layers is ready to go, you would think the hardest\npart is done. The truth is, that’s when some of the more complex tasks begin. Devel-\noping your desired services traditionally requires servers running in data centers or in\nMicroservices all the time?\nMicroservice approaches aren’t all a bed of roses. Having a mix of languages and\nframeworks can be hard to support and, without strict discipline, can lead to confu-\nsion down the road. Eventual consistency, coordination, discovery, and complex error\nrecovery can make things difficult in a microservices universe. \nSoftware engineering is always a game of tradeoffs. Because something is in fashion\n(like microservices) doesn’t make it universally right for all problems and use cases.\nWhat matters is knowing about the different architectural options, understanding\ntheir pros and cons, and, importantly, understanding the requirements and needs of\nyour own problem. (And, yes, in some cases and situations, having a monolith is OK.)\n\n\n8\nCHAPTER 1\nGoing serverless\nthe cloud that need to be managed, maintained, patched, and backed up. Today, you\nwould pick from a few options:\nDirectly build on VMs—The physical deployment of each service requires you to\nhave a set of instances with additional tasks to address required activities such as\nload balancing, transactions, clustering, caching, messaging, and data redun-\ndancy. Provisioning, managing, and patching of these servers is a time-consuming\ntask that often requires dedicated operations people. \nA non-trivial environment is hard to set up and operate effectively. Infra-\nstructure and hardware are necessary components of any IT system, but they’re\noften also a distraction from what should be the core focus—solving the busi-\nness problem. In our simple web application example, you would have to\nbecome an expert in building distributed systems and cloud infrastructure\nmanagement. In a cloud environment, this form of computing is often referred\nto as infrastructure as a service (IaaS).\nUse a PaaS—Over the past few years, technologies such as platform as a service\n(PaaS) and containers have appeared as potential solutions to the headache of\ninconsistent infrastructure environments, conflicts, and server management\noverhead. PaaS is a form of cloud computing that provides a platform for users\nto run their software while hiding some of the underlying infrastructure. \nTo make effective use of PaaS, developers need to write software that targets\nthe features and capabilities of the platform. Moving a legacy application\ndesigned to run on a standalone server to a PaaS service often leads to addi-\ntional development effort because of the ephemeral nature of most PaaS imple-\nmentations. Still, given a choice, many developers would understandably\nchoose to use PaaS rather than more traditional, manual solutions thanks to\nreduced maintenance and platform support requirements.\nUse containers—Containerization is considered ideal for microservices architec-\ntures because it is a way of isolating an application with its own environment.\nIt’s a lightweight alternative to full-blown virtualization that traditional cloud\nservers use. \nContainers are an excellent deployment and packaging solution especially\nwhen dependencies are in play (although they can come with their own house-\nkeeping challenges and complexities). Containers are isolated and lightweight,\nbut they need to be deployed to a server, whether in a public or private cloud or\non site. \nWhile each of these models are perfectly valid and offer varying degrees of simplicity\nand speed of development for your service, your costs are still driven by the lifecycle of\nthe infrastructure or servers you own, not to your application usage. If you purchase a\nrack at the data center, you pay for it 24/7. If you purchase a cloud instance (wrapped\nin a PaaS or running containers or otherwise), you pay for it when it runs, indepen-\ndent of whether it is serving traffic for your web app or not. \n\n\n9\nUnderstanding serverless architectures\n This leads to an entire discipline of engineers investing in improving server effi-\nciency or trying to match infrastructure lifecycle to application usage and server sizes\nto traffic patterns. This also means that all the effort spent on these tasks is time taken\naway from improving the functionality and differentiating aspects of your application.\nThis is equivalent to asking for a place to plug in your appliance and having to pay for\na share of the power generators at your utility company, as well as configuring the gen-\nerator to deliver the power in the phase, frequency, and wattage you desire no matter\nhow much you use. The actual outcome (plug in your appliance) is dwarfed by the\neffort and cost for the infrastructure required (the generators). This is where the\nserverless approach comes in. It aims for the moral equivalent of the utility approach\nwe know and love today—there when you need it, complexity abstracted away, and you\nonly pay for when you use it. \n1.2.3\nImplementing architecture the serverless way\nA serverless architecture for our sample application could be composed of different\nlayers. For example, to build the API, we would use a service that does not cost us any-\nthing if there are no API calls. To build the authentication service, we would use a ser-\nvice that does not cost us anything if there are no authentication calls. To build the\nstorage service, we would use . . . you get the picture. \n Much like the public cloud approach that offered virtual infrastructure Lego to\nassemble our cloud stack in the early days, a serverless architecture uses existing ser-\nvices from cloud providers like AWS to implement its architectural components. As an\nexample, AWS offers services to build our application primitives like APIs (Amazon\nAPI Gateway), workflows (AWS Step Functions), queues (Amazon Simple Queue Ser-\nvice), databases (Amazon DynamoDB and Amazon Aurora), and more.\n The idea of using off-the-shelf services to implement parts of our architecture is\nnot new; indeed, it’s been a best practice since the days of SOA. What’s changed in the\nlast few years is the capability to also implement the custom aspects of our applications\n(like the business logic) in a serverless manner. This ability to run arbitrary code with-\nout having to provision infrastructure to run it as a service or to pay for the infrastruc-\nture is referred to as functions as a service (FaaS). \n FaaS allows you to provide custom code, associated dependencies, and some con-\nfiguration to dictate your desired performance and access control characteristics.\nFaaS then executes this unit (referred to as a function) on an invisible compute fleet\nwith each execution of your code receiving an isolated environment with its own disk,\nmemory, and CPU allocation. You pay only for the time your code runs. A function is\nnot a lightweight instance; instead, think of it as akin to processes in an OS, where you\ncan spawn as many as needed by your application and then spin them down when\nyour application isn’t running. \n Serverless architectures are really the culmination of shifts that have been going\non for a long time: from monoliths to services and from managing infrastructure to\nincreasingly delegating the undifferentiating responsibilities. Serverless architectures\n",
      "page_number": 21
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 30-37)",
      "start_page": 30,
      "end_page": 37,
      "detection_method": "topic_boundary",
      "content": "10\nCHAPTER 1\nGoing serverless\ncan help with the problem of layering and having to update too many things. There’s\nroom for developers to remove or minimize layering by breaking the system into func-\ntions and allowing the frontend to securely communicate with services and even the\ndatabase directly. A well-planned serverless architecture can make future changes eas-\nier, which is an important factor for any long-term application. \n To recap, a serverless architecture leverages a serverless implementation for each\nof its components, using FaaS (like AWS Lambda) for custom logic. This means each\ncomponent is built as a service, with utility pricing that incurs cost only when used.\nEach component is a service and exposes no configuration or cost related to the infra-\nstructure it is running on, which means these architectures don’t rely on direct access\nto a server to work. By making use of various powerful single-purpose APIs and web\nservices, developers can build loosely coupled, scalable, and efficient architectures\nquickly. Moving away from servers and infrastructure concerns, as well as allowing the\ndeveloper to primarily focus on code, is the ultimate goal behind serverless.\nMore on FaaS\nAWS’s FaaS offering is called AWS Lambda and is one of the first from the major\ncloud providers. Note that Lambda isn’t the only game in town. Microsoft Azure Func-\ntions (http://bit.ly/2DWx5Gn), IBM Cloud Functions (http://bit.ly/2l1PWbd), and\nGoogle Cloud Functions (http://bit.ly/2CbzOem) are other FaaS services you might\nwant to look at.\nMany developers conflate serverless with FaaS offerings like AWS Lambda, which\noften leads to confusing arguments around the adoption of containers or serverless\nwhen they really mean containers or functions. We like how TJ Hallowaychuk, the cre-\nator of the Apex framework, defines what serverless is about. He once tweeted,\n“serverless != functions, FaaS == functions, serverless == on-demand scaling and\npricing characteristics (not limited to functions).” We couldn’t agree more.\nAn emerging trend is that of serverless containers; that is, leveraging containers\ninstead of functions to implement the custom logic and using the container as a util-\nity service and incurring costs only when the container runs. Services like AWS Far-\ngate or Google Cloud Run offer this capability. The difference between the two\n(functions vs. containers) is just the degree to which developers want to shift the\nboundaries of shared responsibilities. Containers give you a bit more control over\nuser space libraries and network capabilities. Containers are an evolution of the\nexisting server-based/VM model, offering an easy packaging and deployment model\nfor your application stack. You are still required to define your operating system’s\nrequirements, your desired language stack, and dependencies to deploy code, which\nmeans you continue to carry some of the infrastructure complexity. For the purpose\nof this book, we are going to focus on using FaaS for our custom logic, though you\ncan explore the usage of serverless containers for the same as well.\n\n\n11\nMaking the call to go serverless\n1.3\nMaking the call to go serverless\nThe web application example we went through is one of the simplest demonstrations\nof what can be achieved with serverless architectures. A serverless approach can also\nwork exceptionally well for organizations that want to innovate and move quickly. \n Functions and serverless architectures, in general, are versatile. You can use them\nto build backends for CRUD applications, e-commerce, back-office systems, complex\nweb apps, and all kinds of mobile and desktop software. Tasks that used to take weeks\ncan be done in days or hours as long as we chose the right combination of technolo-\ngies. Lambda functions are stateless and scalable, which makes them perfect for\nimplementing any logic that benefits from parallel processing. \n The most flexible and powerful serverless designs are event-driven, which means\neach component in the architecture reacts to a state change or notification of some\nkind rather than responding to a request or polling for information. In chapter 2, for\nexample, you’ll build an event-driven, push-based pipeline to see how quickly you can\nput together a system to encode video to different bit rates and formats. \nNOTE\nYou will find the use of events as a communication mechanism\nbetween components to be a recurring theme in serverless architectures;\nindeed, AWS Lambda’s initial launch was as an event-driven computing ser-\nvice. Building event-driven, push-based systems will often reduce cost and\ncomplexity (you won’t need to run extra code to poll for changes) and,\npotentially, make the overall user experience smoother. It goes without saying\nthat although event-driven, push-based models are a good goal, they might\nnot be appropriate or achievable in all circumstances. \nServerless architecture allows developers to focus on software design and code rather\nthan infrastructure. Scalability and high availability are easier to achieve, and the pric-\ning is often more fair because you pay only for what you use. More importantly, you\nhave the potential to reduce some of the complexity of the system by minimizing the\nnumber of layers and amount of code needed. \n Adopting a serverless approach to application development comes with significant\nagility, elasticity, and cost efficiency gains. However, it is easy to fall into the trap of try-\ning to adopt a serverless approach for all applications. We recommend keeping a few\nprinciples in mind as you start your serverless journey:\nAvoid lift-and-shift—In practice, serverless architectures are more suited for new\napplications rather than porting existing applications over. This is because exist-\ning application code bases have a lot of code that is made redundant by the\nserverless services. For example, porting a Java Spring app into Lambda brings\na heavy framework into a function, most of which exists to interact with a web\nserver (which doesn’t exist inside Lambda). \nAdopt a serverless first approach, not a serverless only approach—While there are\ncompanies like A Cloud Guru that have adopted a serverless only approach, where\n100% of their application runs as a serverless implementation, the more\n\n\n12\nCHAPTER 1\nGoing serverless\nwidespread approach that companies like Expedia and T-Mobile have adopted is\nto go serverless first. What this means is that their developers attempt to first build\nany new application in the following priority order: build as much as possible using\nthird-party services, fall back to custom services built using AWS serverless primi-\ntives like AWS Lambda, and finally, fall back to custom services built using custom\nsoftware running on infrastructure like EC2. We talk about the reasons why you\nmay have to fall back beyond custom serverless services in the next section.\nIt doesn’t have to be all or nothing—One advantage of the serverless approach is\nthat existing applications can be gradually converted to serverless architecture.\nIf a developer is faced with a monolithic code base, they can gradually tease it\napart and convert individual components into a serverless implementation (the\nstrangler pattern). \nThe best approach is to initially create a prototype to test developer assump-\ntions about how the system would function if it is going to be partly or fully server-\nless. Legacy systems tend to have interesting constraints that require creative\nsolutions, and as with any architectural refactors at a large scale, compromises\nare inevitably going to be made. The system may end up being a hybrid (as in fig-\nure 1.3), but it may be better to have some of its components use Lambda and\nthird-party services rather than remain with an unchanged legacy architecture\nthat no longer scales or that requires expensive infrastructure to run.\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nIaaS\nPaaS\nContainers\nMonolithic application\nLambda\nfunction\nAnalytics\nservice\nPayment\nservice\nNotification\nservice\nSearch\nservice\nA monolithic application \ncan be deconstructed  \ninto Lambda functions, \nthird-party services, IaaS, \nPaaS, and containers.\nThe combination of \ntechnologies should depend \non your needs and constraints. \nHowever, more technologies \nrequire more overhead, time, \nand energy. \nContainers, PaaS, IaaS, Lambda functions, and services \ncan talk to one another. If you have designed a system \nusing a combination of these technologies, you must \nconsider how the orchestration of events take place.\nFigure 1.3\nServerless architecture is not an all-or-nothing proposition. If you currently have \na monolithic application, you can begin to gradually extract components and run them in \nisolated services or compute functions. You can decouple a monolithic application into an \nassortment of IaaS, PaaS, containers, functions, and third-party services if it helps.\n\n\n13\nMaking the call to go serverless\nThe transition from a legacy, server-based application to a scalable serverless architec-\nture may take time to get right. It needs to be approached carefully and slowly, and\ndevelopers need to have a good test plan and a great DevOps strategy in place before\nthey begin.\nPick applications suited for a service-oriented architecture—Serverless architectures\nare a natural extension of ideas raised in SOAs. In a serverless architecture, all\ncustom code is written and executed as isolated, independent, and often granu-\nlar functions that are run in a compute service such as AWS Lambda. Because\nevery component is a service, serverless architectures share a lot of advantages\nand complexities with event-driven microservices architectures. This also means\napplications likely need to be architected to meet the requirements of these\napproaches (like making the individual services stateless, for example). \nKeep in mind that the serverless approach is all about reducing the amount\nof code you have to own and maintain, so you can iterate and innovate faster.\nThis means you should strive to minimize the number of components that are\nrequired to build your application. For example, you may architect your web\napplication with a rich front end (in lieu of a complex backend) that can talk to\nthird-party services directly. That kind of architecture can be conducive to a bet-\nter user experience. Fewer hops between online resources and reduced latency\nwill result in a better perception of performance and usability of the applica-\ntion. In other words, you don’t have to route everything through a FaaS; your\nfrontend may be able to communicate directly with a search provider, a data-\nbase, or another useful API. \nAlso keep in mind that moving from a monolithic approach to a more\ndecentralized serverless approach doesn’t automatically reduce the complexity\nWhat about NoOps?\nEarly on, around the time of the first conference on serverless technologies and archi-\ntectures (https://serverlessconf.io) in 2016, there was talk that serverless technol-\nogies foreshadowed the era of NoOps. Some people believed that thanks to\nserverless, companies would no longer need to think about infrastructure operations.\nThe cloud vendor will take care of everything was the thought. That assumption, that\nNoOps was a real thing, proved not to be the case.\nWhen it comes to building and running serverless applications, DevOps engineers are\nessential, except now they have a different focus. Their attention is on deployment\nautomation, testing, and working with the operations/support teams of their pre-\nferred cloud provider (rather than tweaking servers and patching operating systems). \nCompanies can get away with smaller, more specialized DevOps teams; however,\nignoring operations entirely is a recipe for disaster (and don’t let anyone else tell you\notherwise). Remember, when your application fails, customers hold you accountable,\nnot your cloud provider, so be ready and have the right people and processes in place.\n\n\n14\nCHAPTER 1\nGoing serverless\nof the underlying system. The distributed nature of the solution can introduce\nits own challenges because of the need to make remote rather than in-process\ncalls and the need to handle failures and latency across a network, which your\napplication will need to be resilient to.\nMinimize custom code—The rise of serverless means many standard application\ncomponents like APIs, workflows, queues, and databases are available as server-\nless offerings from cloud providers and third parties. It’s far more useful for\ndevelopers to spend time solving a problem unique to their domain rather than\nrecreating functionality already implemented by someone else. Don’t build for\nthe sake of building if viable third-party services and APIs are available. Stand\non the shoulders of giants to reach new heights. \nAppendix A has a short list of Amazon Web Services and non-Amazon Web\nServices that we’ve found useful. We’ll look at most of those services in more\ndetail as we move through the book. However, it goes without saying that when\na third-party service is considered, factors such as price, capability, availability,\ndocumentation, and support must be carefully assessed. \nIf you have to build a piece of custom functionality, our advice is simple: try\nto solve your problem using functions first, and if that doesn’t work explore\ncontainers and more traditional server-based architectures second. Developers\ncan write functions to carry out almost any common task, such as reading and\nwriting to a data source, calling out to other functions, and performing calcula-\ntions. In more complex cases, developers can set up more elaborate pipelines\nand orchestrate invocations of multiple functions. \n1.4\nServerless pros and cons\nThe serverless approach of building applications by quickly assembling services pro-\nvides two significant advantages: less code to write and maintain per application and\nper activity pricing for our applications. This translates into a disruptive gain in agility\nand developer productivity, and a much more streamlined alignment between devel-\nopment and finance (because any application inefficiencies or optimizations show a\ndirect, tangible financial impact). Here are a few of the specific benefits you will real-\nize by adopting serverless architecture:\nHigh scale and reliability without server management—Building large scale, distrib-\nuted systems is hard. Tasks such as server configuration and management,\npatching, and maintenance are taken care of by the vendor, as is managing the\ninfrastructure architecture for high scale and reliability, which saves time and\nmoney. For example, Amazon looks after the health of its fleet of servers that\npower AWS Lambda. \nIf you don’t have specific requirements to manage or modify server\nresources, then having Amazon or another vendor look after them is a great\nsolution. You’re responsible only for your own code, leaving operational and\nadministrative tasks to a different set of capable hands. \n\n\n15\nServerless pros and cons\nCompetitive pricing—Traditional server-based architecture requires servers that\ndon’t necessarily run at full capacity all of the time. Scaling, even with auto-\nmated systems, involves a new server, which is often wasted until there’s a tem-\nporary upsurge in traffic or new data. \nServerless systems are much more granular with regard to scaling and are cost-\neffective, especially when peak loads are uneven or unexpected. Because of their\nutility pay-per-use billing, serverless services can be extremely cost-effective; how-\never, they’re not cheaper than traditional (server and container) technologies in\nall circumstances. The best thing is to do some modeling before embarking on\na big project.\nLess code—We mentioned at the start of the chapter that serverless architecture\nprovides an opportunity to reduce some of the complexity and code, in com-\nparison to more traditional systems. Adopting a serverless approach eliminates\nundifferentiated code such as that required for orchestrating server fleets or\nrouting requests and events between components, which forms a surprisingly\nlarge part of modern code bases. \nServerless is not a silver bullet in all circumstances, however. Here are some reasons\nwhere you would want to avoid serverless architectures:\nYou are not comfortable with public cloud-based architectures. Serverless development\nis a natural extension of the move to cloud-based development, where more\nand more of the undifferentiated heavy lifting is moved to the providers. There\nare applications and business scenarios where you need to maintain your own\ndata center; in such cases, you cannot build a serverless architecture (though\nyou are welcome to host your own primitives on your infrastructure and use\nthose to build applications).\nThe services don’t meet the availability, performance, compliance, or scale needs of your\ncustomers. AWS serverless services offer an availability SLA, but their threshold\nmay be below what you need for your business. They also have a variety of com-\npliance certifications, but you must validate if they need what your business\nneeds. Services like AWS Lambda also do not offer a performance SLA, which\nmeans you may need to evaluate their performance against your desired levels.\nNon-AWS, third-party services are in the same boat. Some may have strong\nSLAs, whereas others may not have one at all. \nYour application and business needs more control or you need to customize the infrastruc-\nture. When it comes to Lambda, the efficiencies gained from having Amazon\nlook after the platform and scale functions come at the expense of being able to\ncustomize the operating system or tweak the underlying instance. You can mod-\nify the amount of RAM allocated to a function and change timeouts, but that’s\nabout it. Similarly, different third-party services will have varying levels of cus-\ntomization and flexibility. \n\n\n16\nCHAPTER 1\nGoing serverless\nYour application and business needs require you to stay vendor agnostic. If a developer\ndecides to use third-party APIs and services, including AWS, there’s a chance\nthat architecture could become strongly coupled to the platform being used.\nThe implications of vendor lock-in and the risk of using third-party services—\nincluding company viability, data sovereignty and privacy, cost, support, docu-\nmentation, and available feature set—need to be thoroughly considered.\nIn this chapter, you learned what serverless architecture is, looked at its principles,\nand how it compares to traditional architectures. In the next chapter, we’ll get our\nhands dirty by creating a small serverless, event-driven application. This will help you\nget a good taste for serverless if this is your first time trying this approach. From there,\nwe’ll explore important architectures and patterns and discuss use cases where server-\nless architectures are used to solve a problem.\n1.5\nWhat’s new in this second edition?\nFor all intent and purposes, this is a completely different book from the first edition of\nServerless Architectures on AWS. Most of the chapters have been written from the ground\nup to provide a completely different experience from the first edition.\n When the first edition of this book came out in 2017, serverless was still new and\nmany of us were learning about serverless for the first time. As such, the first edition\ngave a gentle introduction to serverless and walked the reader through a build of a\nserverless application. Since then a lot of new educational content has crossed our\ndesks, including numerous books and video courses to help us get started with server-\nless technologies on AWS.\n If you’re looking for an introduction to serverless architectures on AWS, we have\nincluded some introductory content in chapter 2 and in appendices A and B. You can\nalso find the first edition of this book on the Manning website (https://www.manning\n.com/books/serverless-architectures-on-aws). Most of the content from the first edi-\ntion is still relevant today, and with that book, you will learn to build a serverless appli-\ncation from scratch.\n But, just as serverless technologies allow us to focus on the things that differentiate\nour business, we want to focus on things that can differentiate this book with this sec-\nond edition. Instead of yet another getting started guide to serverless, this book\nfocuses on serverless use cases and interesting architectures. It is aimed at developers\nwith some experience of serverless technologies already and answers the questions\nthat many of you have been asking us. Given the switch in focus, this book does not\nhave many actual code samples. Instead, we hope to challenge the way you think\nabout serverless architecture and help you get the most out of serverless technologies\non AWS.\n\n\n17\nSummary\nSummary\nThe cloud has been and continues to be a game changer for IT infrastructure\nand software development. \nSoftware developers need to think about the ways they can maximize the use of\ncloud platforms to gain a competitive advantage. \nServerless architectures are the latest step forward for developers and organiza-\ntions to think about, study, and adopt. This exciting shift in implementing\napplication architectures will grow quickly as software developers embrace com-\npute services such as AWS Lambda. \nIn many cases, serverless applications will be cheaper to run and faster to imple-\nment. There’s also a need to reduce complexity and costs associated with run-\nning infrastructure and carrying out development of traditional software\nsystems. \nThe reduction in cost and time spent on infrastructure maintenance and the\nbenefits of scalability are good reasons for organizations and developers to con-\nsider serverless architectures.\n",
      "page_number": 30
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 38-47)",
      "start_page": 38,
      "end_page": 47,
      "detection_method": "topic_boundary",
      "content": "18\nFirst steps to serverless\nTo give you an understanding of serverless architectures, you’re going to build a\nsmall, event-driven serverless application: specifically, a video-encoding pipeline.\nYour service will transcode videos, uploaded to an S3 bucket, from their existing\nformat, resolution or bit rate to a different format or bit rate (kind of like YouTube\nonly without the frontend website).\n To build this video-encoding pipeline, you will use AWS Lambda, S3, and Ele-\nmental MediaConvert. Later, if you so desire, you can build a frontend around it,\nbut we’ll leave that for you as an exercise. If you want to see how we’ve done it our-\nselves, you can refer to our first edition that covers the frontend in some detail. \nThis chapter covers\nWriting and deploying AWS Lambda functions\nAWS services such as Simple Storage Service \n(S3) and Elemental MediaConvert\nUsing the Serverless Framework to organize and \ndeploy services\n\n\n19\nBuilding a video-encoding pipeline\n2.1\nBuilding a video-encoding pipeline\nIn this section, you’ll begin to build a small, event-driven serverless application. At a\nhigh level, you’ll learn the following in this chapter:\nHow to construct a rudimentary serverless architecture using three AWS ser-\nvices including Lambda\nHow to use the Serverless Framework to organize and deploy a serverless\napplication\nHow to run, debug, and test the serverless pipeline that you built\nLet’s talk about the event-driven pipeline you are going to build (we’ll call it The 24-Hour\nVideo). Your pipeline will encode videos that were uploaded to a designated S3 bucket\ninto different formats, resolutions, and bit rates. Because the entire process is event-\ndriven, once a file is uploaded to S3, the system triggers automatically to process the file\nand create a new version with a different encoding in a separate bucket. And because\neverything is done automatically, there’s no need for any intervention on your behalf.\n2.1.1\nA quick note on AWS costs\nMost AWS services have a free tier. Following this example, you should stay within the\nfree tier of most AWS services. AWS Elemental MediaConvert, however, is one service\nthat may end up costing you a little bit of money. You’ll use MediaConvert to\ntranscode video files. This service is pay-as-you-go without any upfront cost. Pricing is\nbased solely on the duration of the new videos MediaConvert creates, and you are\ncharged in 10-second increments.\n MediaConvert offers two pricing tiers for on-demand services: Basic and Profes-\nsional. You will use the Basic tier in this book, although we invite you to investigate the\nProfessional tier if you are going to take your application to the next level (remember\nus if you end up building the next YouTube!). The Basic tier supports features such as\nFrameworks for serverless\nYou might have heard that there are several frameworks that you can use to organize\nand deploy serverless applications. These include the AWS Serverless Application\nModel (https:// github.com/awslabs/serverless-application-model), the Serverless\nFramework (https://serverless.com), Chalice (https://github.com/aws/chalice), and\na few others. In this chapter, you’ll use Serverless Framework to organize and auto-\nmate the deployment of your serverless application. \nOur advice is to always use a framework like the Serverless Framework or Serverless\nApplication Model (SAM). Once you understand the principles of serverless architec-\ntures, a framework accelerates everything you do by leaps and bounds. Appendix C\ncontains more information on the different frameworks we have found useful. There’s\neven an introduction and a bit of a primer on the Serverless Framework that you’ll\nuse in this chapter. Have a look at appendix C when you get a chance.\n\n\n20\nCHAPTER 2\nFirst steps to serverless\nsingle-pass encoding, clipping, stitching, and overlays. The Professional tier supports\nquite a few more features.\n The per-minute rate in the Basic tier depends on the resolution and the frame rate\nof the desired output. It ranges from $0.0075 per minute for basic SD quality output to\n$0.0450 per minute for UHD output. This rate also differs based on the region you are\nin. US East 1 (North Virginia), for example, is cheaper than US West 1 (Northern Cal-\nifornia), but US East 1 is the region you’ll use throughout this book. You can see the\ntiers and the pricing information at https://aws.amazon.com/mediaconvert/pricing/.\nJust remember that there’s no free tier for MediaConvert, so you’ll start paying some-\nthing almost immediately.\n The S3 free tier allows users to store 5 GB of data with standard storage, issue\n20,000 GET requests and 2,000 PUT requests, and transfer 15 GB of data each month.\nLambda provides a free tier with 1 M free requests and 400,000 GB seconds of com-\npute time. You should be well within the free tier limitations of those services. The fol-\nlowing lists the high-level requirements for The 24-Hour Video: \nThe transcoding process converts uploaded source videos to three different res-\nolutions and bit rates: \n– 6 Mbps with a 16 × 9 aspect ratio and a resolution of 1920 × 1080\n– 4.5 Mbps with a 16 × 9 aspect ratio and a resolution of 1280 × 720\n– 1.5 Mbps with a 4 × 3 aspect ratio and a resolution of 640 × 480\nThere will be two S3 buckets: \n– Original files will go into the upload bucket. \n– Files created by AWS MediaConvert will be saved to the transcoded video\nbucket.\nTo make things simpler to manage, you’ll set up a build and deployment system using\nthe Node Package Manager (npm) and the Serverless Framework. First, here’s an\noverview on the AWS services we’ll use in this example.\n2.1.2\nUsing Amazon Web Services (AWS)\nTo create your serverless backend, you’ll use several services provided by AWS. These\ninclude Simple Storage Service (S3) for file storage, MediaConvert for video conver-\nsion, and Lambda for running custom code and orchestrating key parts of the system.\nIn this chapter, you’ll create your first Lambda function to kick off MediaConvert jobs.\nHere’s a brief description for each of the AWS services that we’ll use: \nS3 provides the storage service. Amazon S3 stores the uploaded and newly\ntranscoded videos.\nLambda handles parts of the system that require coordination or that can’t be done directly\nby other services. This function automatically runs when a file is uploaded to an S3\nbucket.\nMediaConvert encodes your videos to different resolutions and bit rates. Default presets\nremove the need to create custom encoding profiles.\n\n\n21\nPreparing your system\nFigure 2.1 shows a detailed flow of the proposed approach. Note that the only point\nwhere a user needs to interact with the system is at the initial upload stage. This figure\nand the architecture may look complex, but we’ll break the system into manageable\nchunks and tackle them one by one over the course of this chapter.\n2.2\nPreparing your system\nIt’s time to set up AWS services and install the software on your computer. Here’s what\nyou’ll install on your machine:\nNode.js and its package manager (npm) to help manage Lambda functions and\nkeep track of dependencies\nThe AWS command-line interface (CLI) to help with deployments and future\nuse cases and examples\nThe Serverless Framework (npm package) to help you organize and deploy\nyour application to AWS\nIn AWS, you’ll create\nAn Identity and Access Management (IAM) user and roles\nS3 buckets to store video files\nThe first Lambda function\nThis section may seem lengthy, but it explains a number of things that will help you\nthroughout the book. If you’ve already used AWS, you’ll be able to move through this\nsection quickly. \nUpload new\nvideo file\nTranscode\nvideo\n2. Trigger\nLambda\n4. Submit \njob\n1. S3 bucket\nCreate\ntranscode\njob\n3. Lambda\n5. AWS \nMediaConvert\n7. S3 bucket\n6. Save \nfile\nSave\ntranscoded\nvideo\nFigure 2.1\nThe 24-Hour Video backend is built with AWS S3, MediaConvert, and \nLambda. This pipeline may seem to have a lot of steps initially, but in this chapter, \nwe’ll break this down, and you’ll build a scalable serverless system in no time at all.\n\n\n22\nCHAPTER 2\nFirst steps to serverless\n2.2.1\nSetting up your system\nTo begin, you need to create an AWS account and install a number of software pack-\nages and tools on your computer. Let’s take these in order:\n1.\nCreate an AWS account. It’s free but you will need to provide your credit card\ndetails in case there are any charges if you go over the free tier allotment. \nYou can create your account at https://aws.amazon.com. We highly recom-\nmend that you set up 2 Factor Authentication (2FA) on your account as soon as\npossible. The instructions for 2FA are here: https://amzn.to/2ZASm33. \n2.\nAfter your account is created, download and install the appropriate version of\nthe AWS CLI for your system from here:\nhttp://docs.aws.amazon.com/cli/latest/userguide/installing.html \nThere are different ways to install the CLI, including an MSI installer if\nyou’re using Windows, pip (a Python-based tool), or a bundled installer if\nyou’re using Mac or Linux. \n3.\nInstall Node.js and npm. You can download Node.js from https://nodejs.org/\nen/download/ (npm comes bundled with Node.js). \nYou can install the latest version of Node.js but, at the time of writing, the\nmost up-to-date version supported by Lambda was 14. Node 14.x is what you will\ntarget when you deploy code. \nJust a heads up: in a short while you’ll need to install Serverless Framework. However,\nyou don’t need to do it now. We’ll cover that when it is time.\n2.2.2\nWorking with Identity and Access Management (IAM)\nHaving an AWS account is good, but you cannot do too much with it just yet. For\nexample, the AWS CLI you have just installed isn’t going to function. You will not be\nable to create resources, deploy, or do anything, really. To make AWS work, you’ll\nneed to create an IAM user, assign permissions to the user, and then configure the\nCLI to use the IAM user’s credentials. Let’s do that now: \n1.\nIn the AWS console, click IAM (Identity and Access Management), click Users,\nand then click Add User. \n2.\nGive your IAM user a name (in figure 2.2, we used lambda-upload for the\nname) and select the Programmatic Access check box. Selecting this check box\nallows you to generate an access key ID and a secret access key. (You’ll need\nthese keys to run aws configure in a few steps.) \n3.\nClick Next: Permissions to proceed.\n \n \n \n \n \n\n\n23\nPreparing your system\n4.\nSelect Attach Existing Policies Directly and then click the checkbox next to\nAdministratorAccess (figure 2.3). Choose Next: Tags to proceed.\nEnable Programmatic \nAccess to generate the \naccess key ID and the \nsecret access key.\nFigure 2.2\nCreating a new IAM user is straightforward when using the IAM console.\nYou are going to give \nAdministratorAccess to \nthis user because the \nServerless Framework \nwill require it at a \nlater stage. \nFigure 2.3\nMake sure to select the AdministratorAccess policy. You will need it to upload functions and \ndeploy other services.\n\n\n24\nCHAPTER 2\nFirst steps to serverless\n5.\nTags are useful for keeping an inventory and metadata, but for this example,\nyou don’t need to do anything. Click Next: Review to go forward.\n6.\nOn the final page, you can review your user details and the permissions sum-\nmary. Choose Create User to proceed. \nYou should now see a table with the username, the access key ID, and the secret access\nkey. You can also download a CSV file with this information. Go ahead and download\nit now to retain a copy of the keys on your computer and click Close to exit (figure 2.4).\nRun aws configure from a terminal on your system. The AWS CLI prompts for several\nthings: \n1.\nAt the prompt for user credentials, enter the access and secret keys generated\nfor the lambda-upload username or the username that you selected previously. \n2.\nYou’ll also be prompted to enter a region. Type us-east-1 and press Enter. We\nrecommend that you use the same region for all services (you’ll find that it’s\ncheaper and makes things easier to configure). The N. Virginia (us-east-1)\nClick Show to see the \nsecret access key.\nDownload the CSV file to \nyour computers. It has \nthe access key ID and \nthe secret acess key.\nFigure 2.4\nRemember to save the access key ID and the secret access key. You won’t be able to \nget the secret access key again once you close this window.\n\n\n25\nPreparing your system\nregion supports everything we’ll use for the duration of this book so make sure\nto use us-east-1 at all times.\n3.\nThere will be one more prompt asking you to select the default output format.\nSet it as json. \nYou are now done with the AWS CLI configuration. You created an IAM user and used\nthat user’s credentials to configure the CLI on your system. Good job! \n2.2.3\nLet’s make a bucket\nThe next step is to create a bucket in S3. This bucket will contain transcoded video\nput there by Elemental MediaConvert. All users of S3 share the same bucket name-\nspace, which means that you have to come up with bucket names that are not in use.\nIn this book, we’ll assume that this bucket is named something like serverless-video-\ntranscoded.\nTo create a bucket\n1.\nIn the AWS console, choose S3 and then click Create Bucket (figure 2.5). \n2.\nType in a name for the bucket and choose US East (N. Virginia) as the region. \n3.\nScroll to the bottom of the page and click Create Bucket to confirm. Your\nbucket should immediately appear in the console.\nGranular permissions\nThe best practice when it comes to permissions in AWS is to make them granular.\nThis means that your IAM users and roles should have only the specific permissions\nneeded to carry out their purpose. They shouldn’t have all administrator-level permis-\nsions, for example, unless there is a good reason for it. \nYou just created an IAM user that has administrator-level permissions. This flies in\nthe face of the advice we’ve just given. The reason for this is that the Serverless\nFramework, which you will use shortly, is going to need administrator-level access.\nThe Framework calls to a lot of APIs, and it’s difficult to configure an IAM user with\njust the right permissions. If you aren’t going to use the Serverless Framework and\nwant to deploy functions using the AWS CLI instead, then we’d recommend creating\nan IAM user and assigning a few specific permissions needed to upload your functions.\nBucket names\nBucket names must be unique throughout the S3 global resource space. We’ve\nalready taken serverless-video-transcoded, so you’ll need to come up with a different\nname. We suggest adding your initials (or a random string of characters) to these\nbucket names to help identify them throughout the book (for example, serverless-\nvideo-upload-ps and serverless-video-transcoded-ps).\n\n\n26\nCHAPTER 2\nFirst steps to serverless\nNOTE\nYou are going to end up needing another S3 bucket to which you will\nupload videos in the first place. The Serverless Framework creates this bucket\nfor you automatically in the next section, so you don’t need to do anything\nyet. You can create the transcoded video bucket using CloudFormation inside\nthe Serverless Framework’s serverless.yml file too, but explaining that is out-\nside the scope of this chapter (good exercise, though).\n2.2.4\nCreating an IAM role\nNow you need to create an IAM role for your first Lambda function (you will create\nthis function in a little while). The role allows your function to interact with S3 and\nElemental MediaConvert. You’ll add two policies to this role: \n\nAWSLambdaExecute \n\nAWSElementalMediaConvertFullAccess\nThe AWSLambdaExecute policy allows Lambda to interact with S3 and CloudWatch.\nCloudWatch is an AWS service for collecting log files, tracking metrics, and setting\nSet the right region to reduce costs \nand minimize latency. Your Lambda \nfunctions should be in the same region. \nIf the bucket name is taken, AWS \nshows you an error message.\nFigure 2.5\nCreating a bucket from the AWS S3 console. Remember that bucket names are globally \nunique, so you’ll have to come up with your own new name.\n\n\n27\nPreparing your system\nalarms. The AWSElementalMediaConvertFullAccess policy allows Lambda to submit\nnew transcoding jobs to Elemental MediaConvert.\n1.\nIn the AWS console, find and click IAM.\n2.\nChoose Roles. \n3.\nClick the Create Role button to begin. You will see a list of different AWS tech-\nnologies under AWS Service. Select Lambda and the Next: Permissions button. \n4.\nIn this view, you can search for and attach premade policies. Find and attach\n(by clicking the checkbox on the left) the following two policies: \n– AWSLambdaExecute\n– AWSElementalMediaConvertFullAccess \n5.\nClick Next: Tags to advance.\n6.\nClick Next: Review to proceed to the Review page.\n7.\nName your role transcode-video and click Create Role.\nAfter the role is created, you’ll see the list of your existing roles again. Choose\ntranscode-video to see what’s inside. It should look like figure 2.6.\nTwo policies have been added to the role. \nPermissions are embedded within policies.\nFigure 2.6\nTwo managed policies are needed for the transcode-video role to access S3 and \ncreate Elemental MediaConvert jobs.\n",
      "page_number": 38
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 48-58)",
      "start_page": 48,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": "28\nCHAPTER 2\nFirst steps to serverless\n2.2.5\nUsing AWS Elemental MediaConvert\nYou are going to use Elemental MediaConvert to convert uploaded video files from\none format to another. At a high level, MediaConvert works by taking a file uploaded\nto an S3 bucket, transcoding the file to one or more different versions, and then plac-\ning these versions in to another S3 bucket. \n When you create a MediaConvert job, you’ll have to specify this information,\nincluding input and output buckets, and what conversion you’d like to carry out. You\nwill also have to specify a MediaConvert endpoint. Each user has a custom endpoint,\nand you need to know where it is. Let’s find it now so that you’ll know where to look.\n1.\nIn the AWS console, select MediaConvert (it will be under the Media Services\ncategory).\n2.\nClick the hamburger icon in the top left corner (it looks like three parallel\nlines).\n3.\nChoose Account from the menu.\nYou should see the API endpoint that you will need to use in this chapter (figure 2.7).\nNote that you can always refer to these instructions to find the MediaConvert API end-\npoint if you forget where it is.\nYou are nearly there! There’s one more IAM role you need to create now to make\nthings easier later.\nThis API endpoint will be needed \nfor your first Lambda function.\nFigure 2.7\nViewing the API endpoint that you will use in this chapter\n\n\n29\nStarting with the Serverless Framework\n2.2.6\nUsing MediaConvert Role\nYou need to create a role for the MediaConvert service. MediaConvert needs to have\naccess to S3 as well as the API Gateway. Without this role, MediaConvert simply will\nnot run when you try to invoke it from Lambda. To create the role, follow these steps\n(or try it on your own!):\n1.\nIn the AWS console, click IAM and then click Roles. \n2.\nClick the Create Role button. You will see a list of different AWS technologies\nunder AWS Service. Select MediaConvert and the Next: Permissions button.\nAWS has already predefined what policies you need for this role. These are S3\nFull Access and API Gateway Full Invoke. \n3.\nChoose Next: Tags, then click Next: Review to proceed to the next page.\n4.\nName your role media-convert-role and click Create Role.\n5.\nCopy the ARN of the role to a notepad or someplace where it can be easily\nretrieved. You’ll need the role ARN as well as the API endpoint later (in listing 2.3).\n2.3\nStarting with the Serverless Framework\nThe Serverless Framework is going to help you organize your functions and deploy\nthem to AWS. This framework is powerful, so you will get a lot of flexibility in terms of\nhow to package code, what variables to use, and environments to deploy to. \n If you get stuck with the Serverless Framework, have a look at appendix C or check\nout https://serverless.com/framework/docs. Appendix C features a thorough walk-\nthrough of the Serverless Framework as well as useful hints and tips. However, if you\ndon’t find your answer there, then the online documentation is the way to go.\n2.3.1\nSetting up the Serverless Framework\nInstall the Serverless Framework by running npm install -g serverless from the ter-\nminal. At the time of writing, we used Serverless Framework 2.63. If you are using a\nlater version of the framework and something doesn’t work, you may have to apply\nyour best detective skills to figure out what’s wrong and fix it (or downgrade to 2.63).\nCREDENTIALS\nThe Serverless Framework needs access to your AWS account so that it can create and\nmanage resources on your behalf. By default, Serverless Framework uses the AWS pro-\nfile you have already configured on your machine using the AWS CLI.\nSource code on GitHub\nThe source code for this chapter can be found at https://github.com/sbarski/serverless\n-architectures-aws-2.\n\n\n30\nCHAPTER 2\nFirst steps to serverless\nHELLO WORLD!\nHaving installed the Serverless Framework and configured credentials, let’s test that it\nworks. In your terminal, run the following command:\nserverless create --template aws-nodejs --path hello-world\nChange to the newly created directory by typing cd hello-world. You should see two\nfiles in this directory: serverless.yml and handler.js. The first file, serverless.yml, is a\nproject file (a service) that describes functions, events, and resources that the func-\ntion can use. The second file, handler.js, is an example Lambda function that you can\nchange! Open handler.js and modify the implementation of the function as the fol-\nlowing listing shows.\n'use strict';\nmodule.exports.hello = async (event) => {\n  return {\n    statusCode: 200,\n    body: JSON.stringify(\n      {\n        message: 'Hello Serverless World!',\n        input: event,\n      },\n      null,\n      2\n    ),\n  };\n};\nOnce you have finished modifying the function, remember to save the file. You are\nnow ready to deploy. Run serverless deploy from the terminal and press Enter\n(make sure you are in the same directory as the serverless.yml file before you deploy;\notherwise, you’ll get an error message). \n You’ll see the Serverless Framework package up files, prepare a CloudFormation\nstack, and deploy your function to AWS. As soon as the deployment finishes, you’ll see\na bit of useful information such as the stage used for the function (dev), the region\n(us-east-1), and the name of the service (hello-world). Your function will be called\nhello-world-dev-hello a combination of the service name, stage, and the function\nexport.\n You can finally check that the function was successfully deployed by opening the\nLambda console in AWS and running the function from there. Another option is to\ninvoke the deployed function from the terminal. \n To run the function in AWS and return a response, execute the following com-\nmand in the terminal: serverless invoke --function hello. The Serverless Frame-\nwork will know to invoke this function from the cloud environment.\nListing 2.1\nA new hello-world Lambda function\nBecause this is a basic function, you \ncan run it in AWS Lambda and see a \nmessage.\nThe only line of code that you need to \nmodify before running this example\n\n\n31\nStarting with the Serverless Framework\n2.3.2\nBringing Serverless Framework to The 24-Hour Video\nNow that you have gotten the Serverless Framework to work, let’s get busy with The 24-\nHour Video. You are going to create a new function and reference it in serverless.yml.\nYou will be able to deploy the function (and then add additional functions) with a single\ncommand and, later, sustainably grow and organize your entire serverless application.\n1.\nIn a terminal window, run the following command: \nsls create --template aws-nodejs --path twentyfour-hour-video\nThe reason we used twentyfour instead of 24 is because a service name must\nbegin with an alphabetic character.\n2.\nChange to the new twentyfour-hour-video directory that was just created.\n3.\nDelete handler.js but leave serverless.yml intact.\n4.\nCreate a new subfolder called transcode-video.\nIn a moment, you’ll begin changing serverless.yml. You can stick to our implementa-\ntion (listing 2.2), however, there are five parameters that you must change to correctly\nreflect your environment. These parameters are bolded in listing 2.2:\nThe name of the upload bucket\nThe name of the transcoded video bucket\nThe video role ARN for your function\nThe MediaConvert endpoint\nThe MediaConvert role\nservice: twentyfour-hour-video \nprovider:\n  name: aws  \n  runtime: nodejs14.x  \n  region: us-east-1        \n    \ncustom:\n  upload-bucket: upload-video-bucket  \n  transcode-bucket: transcoded-video-bucket \n  transcode-video-role:\n  ➥ arn:aws:iam::038221756127:role/transcode-video \nListing 2.2\nChanging serverless.yml for your function\nServerless deploy\nYou don’t need to type serverless (as in serverless deploy) each time you want\nto deploy or do an operation. You can use the sls abbreviation. The following com-\nmands are entirely valid: sls deploy or sls invoke --function hello.\nThe provider is AWS, but Serverless Framework supports \nother Cloud providers like Azure and Google Cloud too.\nSet this to nodejs14.x if it’s not already set.\nDefines the region to deploy to. You can override \nthis setting and deploy to other regions.\nSet this custom variable to the \nname of your upload bucket.\nSet the transcode-bucket to the name \nof your transcoded video bucket. You \ncreated this bucket in section 2.2.3.\nSet the transcode-video role \nARN you created in section 2.2.4. \nUpdate the ARN to your value.\n\n\n32\nCHAPTER 2\nFirst steps to serverless\n  media-endpoint:\n  ➥ https://u4ac0ytu.mediaconvert.us-east-1.amazonaws.com \n  media-role:\n  ➥ arn:aws:iam::038221756127:role/media-convert-role \nfunctions:\n  transcode-video: \n    handler: transcode-video/index.handler\n    role: ${self:custom.transcode-video-role}\n    package:\n      individually: true\n    environment:\n      MEDIA_ENDPOINT: ${self:custom.media-endpoint}\n      MEDIA_ROLE: ${self:custom.media-role}\n      TRANSCODED_VIDEO_BUCKET: ${self:custom.transcode-bucket}\n    events:\n      - s3: ${self:custom.upload-bucket}  \nHere’s a brief explanation of everything you need to update in listing 2.2 to make it\nwork for you. Let’s begin with the upload bucket.\nUPLOAD BUCKET\nIn listing 2.2, you must specify the name of the upload bucket. This is a new bucket\nthat doesn’t yet exist. Remember, you need to use a bucket name that is globally\nunique. One way to do this is to prefix or postfix your full name (unless you have a\ncommon name) or add a few random letters and numbers. \n Serverless Framework via CloudFormation creates the bucket for you automati-\ncally. You can go for something like upload-bucket-firstname-lastname. If the bucket\nname is already taken, Serverless Framework will tell you during deployment. You’ll\nbe able to change it and try again.\nTRANSCODED VIDEO BUCKET\nIn listing 2.2, there’s a custom property called transcode-bucket. This property con-\ntains the name of your transcoded video bucket. Update this property to the name of\nthe bucket you manually created in section 2.2.3. \nLAMBDA ROLE ARN\nYou must specify an IAM role for the function. Luckily, you created a role in section\n2.2.4. You need to find the ARN of that role, copy it, and then update the parameter\ncalled transcode-video-role. To get the role ARN and update serverless.yml, follow\nthese easy steps:\n1.\nIn the IAM console, select Roles.\n2.\nFind the transcode-video role and select it.\n3.\nCopy the value for the role ARN.\n4.\nPaste the value in to the serverless.yml file for the transcode-video-role.\nMEDIACONVERT ENDPOINT\nIn listing 2.2, you’ll find a line that creates a media-endpoint variable. To get this end-\npoint, refer to section 2.2.5 or follow these steps:\nSet your personal \nMediaConvert endpoint \nfor the service to work. \nYour URL will be different \nso be sure to change this.\nSet the MediaConvert role \nARN you created in section \n2.2.6. If you kept the same \nname, change the account \nnumber (e.g., 038221751234) \nto your account number and \neverything should work.\nSpecifies the event trigger for the Lambda \nfunction, which is the S3 upload bucket\n\n\n33\nStarting with the Serverless Framework\n1.\nIn the AWS console, select MediaConvert (it will be under the Media Services\ncategory).\n2.\nClick the hamburger icon in the top left corner (it’s the button that looks like\nthree parallel lines).\n3.\nChoose Account from the menu. You’ll see the API endpoint that you should\ncopy into listing 2.2.\nMEDIACONVERT ROLE\nIn section 2.2.6, you created an IAM role for the Element MediaConvert service. In list-\ning 2.2, you needed to specify the ARN for that role. Make sure to look it up and copy\nit over correctly. Be careful not to confuse the two IAM roles that you have. The IAM\nrole created in section 2.2.4 is intended for the transcode-video Lambda function. The\nrole created in 2.2.6 is intended for MediaConvert and is the one you should use.\n2.3.3\nCreating your first Lambda function\nNow that you’ve created a serverless.yml file, change to the transcode-video folder,\nand in your terminal window, run npm init. Agree to all the options by pressing\nEnter. You can change anything you want; it will not affect your function. \n You’ll get a new file called package.json. This file can be used later if you want to\nadd additional dependencies or libraries into your function. Now, let’s discuss how\nyour new function will work and what it will do:\nThe function will invoke as soon as a new file is uploaded to an S3 bucket. \nInformation about the uploaded video will pass to the Lambda function via the\nevent object. It will include the bucket name and the name (key) of the file\nbeing uploaded. \nThe Lambda function will prepare a transcoding job for AWS MediaConvert.\nThe function will submit the job to MediaConvert and writes a message to an\nAmazon CloudWatch log stream. \nCreate a new file named index.js and open it in your favorite text editor. This file con-\ntains the first function. The important thing to note is that you must define a function\nhandler, which will be invoked by the Lambda runtime.\n Listing 2.3 shows this function’s implementation. Copy this listing into index.js.\nBefore you can deploy and run this code though, you’ll need to make a few small\nchanges as detailed in the text after the code listing.\n'use strict';\nconst AWS = require('aws-sdk');\nconst mediaConvert = new AWS.MediaConvert({\n    endpoint: process.env.MEDIA_ENDPOINT \n});\nListing 2.3\nCreating the transcode video Lambda\nGets the MediaConvert endpoint \nenvironment variable that’s set \nin serverless.yml (listing 2.2)\n\n\n34\nCHAPTER 2\nFirst steps to serverless\nconst outputBucketName =\n➥ process.env.TRANSCODED_VIDEO_BUCKET;  \nexports.handler = async (event, context) => { \n    const key = event.Records[0].s3.object.key; \n    const sourceKey = decodeURIComponent(key.replace(/\\+/g, ' ')); \n    const outputKey = sourceKey.split('.')[0]; \n    const input = 's3://' + event.Records[0].s3.bucket.name + '/' +  \n    ➥ event.Records[0].s3.object.key; \n    const output = 's3://' + outputBucketName + '/' + outputKey + '/';\n    try {\n        const job = {\n            \"Role\": process.env.MEDIA_ROLE, \n            \"Settings\": {\n                \"Inputs\": [{\n                    \"FileInput\": input,     \n                    \"AudioSelectors\": {     \n                        \"Audio Selector 1\": {\n                            \"SelectorType\": \"TRACK\",\n                            \"Tracks\": [1]\n                        }\n                    }\n                }],\n                \"OutputGroups\": [{ \n                    \"Name\": \"File Group\",\n                    \"Outputs\": [{\n                        \"Preset\": \"System-\n                         ➥ Generic_Hd_Mp4_Avc_Aac_16x9_1920x1080p_24Hz_6Mbps\",\n                        \"Extension\": \"mp4\",\n                        \"NameModifier\": \"_16x9_1920x1080p_24Hz_6Mbps\"\n                    }, {\n                        \"Preset\": \"System-\n                         ➥ Generic_Hd_Mp4_Avc_Aac_16x9_1280x720p_24Hz_4.5Mbps\",\n                        \"Extension\": \"mp4\",\n                        \"NameModifier\": \"_16x9_1280x720p_24Hz_4.5Mbps\"\n                    }, {\n                        \"Preset\": \"System-\n                         ➥ Generic_Sd_Mp4_Avc_Aac_4x3_640x480p_24Hz_1.5Mbps\",\n                        \"Extension\": \"mp4\",\n                        \"NameModifier\": \"_4x3_640x480p_24Hz_1.5Mbps\"\n                    }],\n                    \"OutputGroupSettings\": {\n                        \"Type\": \"FILE_GROUP_SETTINGS\",\n                        \"FileGroupSettings\": {\n                            \"Destination\": output \n                        }\n                    }\n                }]\n            }\n        };\n        const mediaConvertResult = await \n        ➥ mediaConvert.createJob(job).promise();\nGets the transcoded video bucket name \nthat’s specified in serverless.yml\nGets the MediaConvert role ARN \nthat’s specified in serverless.yml\nSets the location of\nthe input video for\nthe MediaConvert\njob definition\nSpecifies the Audio Selector for the \nMediaConvert job definition. You’ll \ndefault to naming a single audio \ntrack in the video.\nSets the output bucket \nfor the new video files\n\n\n35\nStarting with the Serverless Framework\n        console.log(mediaConvertResult);\n    } catch (error) {\n        console.error(error);\n    }\n};\nMEDIACONVERT OUTPUTS\nThe function in listing 2.3 declares three new outputs that define the format for your\nnewly transcoded videos (this includes bit rate, resolution, and so forth). The tem-\nplates specified in listing 2.3 are generic templates built in to MediaConvert. Luckily,\nyou aren’t forced to use the ones we’ve selected; you can choose from different tem-\nplates or even create your own. To look at other available presets in MediaConvert do\nthe following:\n1.\nIn the AWS console, select MediaConvert.\n2.\nClick the hamburger icon in the top left corner. \n3.\nChoose Output Presets.\n4.\nFrom the dropdown that says Custom Presets, select System Presets. \nYou’ll see a grid of different presets you can use (figure 2.8). Note that the grid has mul-\ntiple pages and that you can choose to see different categories (MP4, HLS, Broadcast-\nXDCAM, and so on).\nFilter by category to see how \nmany different options you \ncan choose from.\nFigure 2.8\nThe MediaConvert Output Presets page lets you select system presets or configure your \nown.\n\n\n36\nCHAPTER 2\nFirst steps to serverless\nIf you want to create a different type of video using the code in listing 2.3, select the\nname of the desired preset, and copy it into the function as you did for the others\n(remember to specify the extension and a modified name). As an example, if you\nwant to add HLS output, you’d need to include something like this in the Outputs\narray in the function:\n{\n   \"Preset\": \"System-Ott_Hls_Ts_Avc_Aac_16x9_1280x720p_30Hz_3.5Mbps\",\n   \"Extension\": \"hls\",\n   \"NameModifier\": \"_Hls_Ts_Avc_Aac_16x9_1280x720p_30Hz_3.5Mbps \"\n}\nDEPLOYMENT\nDeploy your first function from the terminal by typing sls deploy (make sure you\nissue sls deploy from the directory where serverless.yml is located). The deployment\nshould succeed, and you should see your functions in AWS. The first function is going\nto be named something like twentyfour-hour-video-dev-transcode-video. Later, if\nyou want, you can remove this function from AWS by running sls remove from the\nterminal. \n One other note: the deployment process may create an additional bucket named\nsomething \nlike \ntwentyfour-hour-video-de-serverlessdeploymentbuck-sq06y6wjku9z.\nThis is normal. The Serverless Framework creates this bucket to upload the Cloud-\nFormation templates it generates. You can safely ignore this bucket, but do not manu-\nally delete! The sls remove command will remove it for you (along with the deployed\nLambda function). \n2.4\nTesting in AWS\nTo test your first function, upload a video to the upload bucket. Follow these steps: \n1.\nGo to the S3 console.\n2.\nClick into your upload bucket and then select Upload to open the Upload page\n(figure 2.9).\n3.\nClick Add Files, select a video file from your computer, and click the Upload\nbutton. All other settings can be left as is. If you don’t have any video files to\ntest, go to https://sample-videos.com and grab one of the MP4 videos. \nAfter a time, you should see three new videos in your transcoded video bucket. These\nfiles should appear in a folder rather than in the root of the bucket (figure 2.10). The\nlength of time to produce a new video depends on the duration of the file you’ve\nuploaded. It may take five minutes (or even longer) to produce a new file so grab a\ncup of tea while you wait. \n \n \n \n\n\n37\nLooking at logs\n2.5\nLooking at logs\nHaving performed a test in the previous section, you should see three new files in your\ntranscoded video bucket. But things may not always go as smoothly (although we hope\nthey do)! In case of problems, such as new files not appearing, you can check two dif-\nferent logs for errors. The first and most important one is Lambda’s log in Cloud-\nWatch. To view the log, perform the following steps: \n \n \nClick Add Files to bring \nup the dialog box.\nFigure 2.9\nTo test in ASW, it’s better to upload a small file initially because it makes the upload and \ntranscoding go a lot quicker.\nThese files are in an output \nfolder created in the root \nof the bucket. \nFigure 2.10\nMediaConvert generates three new files and places them in a folder in the transcoded \nvideo S3 bucket.\n\n\n38\nCHAPTER 2\nFirst steps to serverless\n1.\nChoose Lambda in the AWS console and then click your function name.\n2.\nChoose the Monitor tab. You should see different graphs with numbers. One of\nthose graphs will be labeled Error Count and Success Rate. If there is a spike\n(that is, the count is more than 0), it means there is a problem. \n3.\nClick View Logs in CloudWatch to open CloudWatch. You’ll see all the log\nentries ordered by date. On the right, you’ll see which stream they belong to. \n4.\nClick each log entry to see more details including error messages. \n5.\nIf you previously saw that your Invocation error rate was more than 0, find the\nlog entry with the error and fix the problem. \nIf the Lambda logs reveal nothing out of the ordinary, take a look at the AWS Media-\nConvert logs. To view these logs: \n1.\nClick MediaConvert in the AWS console. \n2.\nChoose the hamburger icon on the left to open the sidebar. \n3.\nChoose Jobs from the menu. On the right you should see a list of jobs. \n4.\nClick a job (if it failed) to see more information (figure 2.11).  \n \n \n  \nChoose the job ID to view \ndetails about the error.\nFigure 2.11\nMediaConvert failures can occur for a variety of reasons including the source file being \ndeleted before the job started, an error with the code in the Lambda function, or a misconfiguration.\n",
      "page_number": 48
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "39\nSummary\nSummary\nThe best way to organize serverless applications is to use an Infrastructure as\nCode (IaC) framework like the Serverless Framework. \nDeploying functions and manually setting up services is great for learning, but\nit is not sustainable in the long term. The Serverless Framework can help to\norganize and deploy even the most complex serverless applications.\nServerless applications and pipelines usually consist of different services\nconnected together. In The 24-Hour Video example, we use AWS Lambda, S3,\nand Elemental MediaConvert. Most serverless applications use a combination\nof services.\nAWS CloudWatch is an important service for logging what happens within your\nAWS Lambda functions. It’s vital that you learn how to use it as you are most\ndefinitely going to need it.\nSecurity in AWS is controlled primarily via Identity and Access Manage-\nment (IAM), although there are some exceptions. If you want to become an\nexpert at AWS and serverless applications, knowing how IAM works is essential.\nEstimating cost in AWS can be tricky. A lot of services have generous free tiers\nbut can end up costing a lot if used incorrectly. Make sure you review the costs\nof all services you want to use and understand what the potential cost can be.\nWhen problems happen\nIn our experience, problems often occur because IAM permissions haven’t been con-\nfigured correctly or there was a typo somewhere in your function code. AWS doesn’t\nalways have the most descriptive error messages so, sometimes, a bit of digging\naround and investigative work with CloudWatch is required.\n\n\n40\nArchitectures\n and patterns\nWhat are the use cases for serverless architectures and what kinds of architectures\nand patterns are useful? We’re often asked these questions and queried about use\ncases as people learn about a serverless approach to designing systems. We find that\nit’s helpful to look at how others have applied this technology and what kinds of\nuse cases, designs, and architectures they’ve produced. \n This chapter gives you a solid introduction to where serverless architectures are\na good fit and how to think about the design of serverless systems. The rest of the\nbook focuses on real-world use cases and goes deep into a number of serverless\narchitectures that we’ve found particularly fascinating. \n3.1\nUse cases\nServerless technologies and architectures can be used to build entire systems, cre-\nate isolated components, or implement specific granular tasks. The scope for use of\nThis chapter covers\nUse cases for serverless architectures\nExamples of patterns and architectures\n\n\n41\nUse cases\nserverless design is broad, and one of its advantages is that it’s possible to use it for\nsmall as well as large tasks alike. We’ve designed serverless systems that power web and\nmobile applications for tens of thousands of users, and we’ve built simple systems to\nsolve specific minute problems. \n It’s worth remembering that serverless is not just about running code in a compute\nservice such as Lambda. It’s also about using third-party services and APIs to cut down\non the amount of work you must do. With this in mind, let’s look at some basic use cases.\n3.1.1\nBackend compute\nTechnologies such as AWS Lambda are a few years old, but we’ve already seen large\nserverless backends that power entire businesses. A Cloud Guru (https://acloudguru\n.com), for example, supports many thousands of users collaborating in real time and\nstreams hundreds of gigabytes of video. Another example is the insurance company,\nBranch, which from the start adopted a serverless-first approach (https://amzn.to/\n3vRumYU). \n Indeed, it is possible to create and run an entire business while having a serverless-\nfirst mindset. If you articulate that kind of philosophical approach to technology your-\nself, it will help you answer questions such as what services to adopt or how to best\nsolve a particular architectural problem. \n Startups are not the only organizations looking for agility and efficiencies from\nserverless. Established companies with long histories are also using serverless technol-\nogies and architectures to deliver value to their customers. Some of these bigger com-\npanies include well-known names like Comcast, Coinbase, Fender, Nordstrom, and\nNetflix (https://aws.amazon.com/serverless/customers/). \n3.1.2\nInternet of Things (IoT)\nPutting aside web and mobile applications, serverless is a great fit for the Internet of\nThings (IoT) applications. Amazon Web Services (AWS) has a useful IoT platform\n(https://aws.amazon.com/iot-platform/how-it-works/) that combines\nAuthentication and authorization\nCommunications gateway\nRegistry (a way to assign a unique identity to each device)\nDevice shadowing (to persist device state)\nRules engine (to transform and route device messages to AWS services)\nThe rules engine, for example, can save files to Amazon’s Simple Storage Service (S3),\npush data to an Amazon Simple Queue Service (SQS) queue, and invoke AWS\nLambda functions. Amazon’s IoT platform makes it easy to build scalable IoT back-\nends for devices without having to run a server. A serverless application backend is\nappealing because it removes a lot of infrastructure management, has granular and\npredictable billing (especially when a serverless compute service such as Lambda is\nused), and can scale well to meet uneven demands.\n\n\n42\nCHAPTER 3\nArchitectures and patterns\n3.1.3\nData processing and manipulation\nA common use for serverless technologies is data processing, conversion, manipula-\ntion, and transcoding. We’ve seen Lambda functions built by other developers for pro-\ncessing CSV, JSON, and XML files; collation and aggregation of data; image resizing;\nand format conversion. Lambda and AWS services are well suited for building event-\ndriven pipelines for data-processing tasks.\n In chapter 2, you built a powerful pipeline for converting videos from one format\nto another. This pipeline runs only when a new video file is added to a designated S3\nbucket, meaning that you only pay for the execution of Lambda when there’s some-\nthing to do and never while the system is idle. More broadly, however, we find data\nprocessing to be an excellent use case for serverless technologies, especially when we\nuse Lambda in concert with other services.\n3.1.4\nReal-time analytics\nIngestion of data such as logs, system events, transactions, or user clicks can be accom-\nplished using services such as Amazon Kinesis Data Streams and Amazon Kinesis Fire-\nhose. Kinesis Data Streams and Lambda functions are a good fit for applications that\ngenerate a lot of data that needs to be analyzed, aggregated, and stored. When it\ncomes to Kinesis, the number of functions spawned to process messages from a stream\nis the same as the number of shards (therefore, there’s one Lambda function per\nshard as figure 3.1 shows). \n If a Lambda function fails to process a batch, it retries the operation. This can\nkeep going for up to 24 hours (which is how long Kinesis will keep data around before\nit expires) each time processing fails. \nKinesis Streams can ingest a lot of messages \nthat can be processed with Lambda functions. \nData-intensive applications that perform real-time \nreporting and analytics can benefit from this architecture. \nFile\nstorage (S3)\nLambda\n(retrieve\nbatch of 100)\nDatabase\nLambda\n(retrieve\nbatch of 50)\nKinesis\nStreams\nKinesis\nStreams\nEvents/messages\nEvents/messages\nFigure 3.1\nLambda is a perfect tool to process data in near real time.\n\n\n43\nUse cases\nAmazon Kinesis Firehose is another Kinesis service designed to ingest gigabytes of\nstreaming data and then push it into other services like S3, RedShift, or Elasticsearch\nfor further analytics. Firehose is a true serverless service because it is fully managed, it\nscales automatically depending on the volume of data coming in, and there’s no need\nto think about sharding as is the case with Kinesis Data Streams. \n A great feature of Kinesis Firehose is that a Lambda function can be added to the\nstream to seamlessly process data as it is added and before it is sent to its final destina-\ntion. You can use this to transform data while it’s in flight without having to provision\nany other infrastructure. We are not going to go into much more depth right now\nbecause chapter 6 and chapter 9 discuss use cases and applications for the Kinesis\nproducts in more detail. \n3.1.5\nLegacy API proxy\nOne innovative use case of the Amazon API Gateway and Lambda that we’ve seen a\nfew times is what we refer to as the legacy API proxy. Here, developers use API Gate-\nway and Lambda to create a new API layer over legacy APIs and services, which makes\nthem easier to use. \n The API Gateway creates a RESTful interface, and Lambda functions modify\nrequest/response and marshal data to formats that legacy services understand. The\nAPI Gateway and Lambda functions can transform requests made by clients and\ninvoke legacy services directly as figure 3.2 illustrates. This approach makes legacy ser-\nvices easier to consume for modern clients that may not support older protocols and\ndata formats. \nMost legacy services will \nrequire a Lambda function \nto convert data and to \ncorrectly invoke it.\nAPI\nGateway\nLambda \n(convert/\ninvoke)\nLambda \n(convert/\ninvoke)\nLambda \n(convert/\ninvoke)\nLegacy API\nLegacy API\nLegacy\nservice\n(SOAP)\nLegacy API\nLegacy\nservice\n(XML)\nFigure 3.2\nWe can use the API proxy architecture to build a modern API interface over old services \nand APIs.\n\n\n44\nCHAPTER 3\nArchitectures and patterns\nIt’s important to note that the API Gateway can transform (to an extent) and issue\nrequests against other HTTP endpoints. But it works only in a number of fairly basic\nand limited use cases where JSON transformation is needed. In more complex scenar-\nios, however, a Lambda function is needed to convert data, issue requests, and process\nresponses. \n Take a Simple Object Access Protocol (SOAP) service as an example. You’d need\nto write a Lambda function to connect to a SOAP service and then map responses to\nJSON. Thankfully, there are libraries that can take care of much of the heavy lifting in\na Lambda function; for example, there are SOAP clients that can be downloaded\nfrom the npm registry for this purpose (see https://www.npmjs.com/package/soap).\n3.1.6\nScheduled services\nLambda functions can run on a schedule, which makes them effective for repetitive\ntasks like data backups, imports and exports, reminders, and alerts. We’ve seen devel-\nopers use Lambda functions on a schedule to periodically ping their websites to see if\nthey’re online and send an email or a text message if they’re not. You’ll find Lambda\nblueprints available for this (a blueprint is a template with sample code that can be\nselected when creating a new Lambda function). \n We’ve also seen developers write Lambda functions to perform nightly downloads\nof files off their servers and send daily account statements to users. Repetitive tasks\nsuch as file backup and file validation can also be done easily with Lambda thanks to\nthe scheduling capability that you can set and forget. Check out chapter 7 for an in-\ndepth analysis on how to go about thinking and building a scheduling service. \n3.1.7\nBots and skills\nAnother popular use of Lambda functions and serverless technologies is to build bots\n(a bot is an app or a script that runs automated tasks) for services such as Slack. A bot\nmade for Slack can respond to commands, carry out small tasks, and send reports and\nnotifications. We, for example, built a Slack bot in Lambda to report on the number\nof online sales made each day. And we’ve seen developers build bots for Telegram,\nSkype, and Facebook’s messenger platform. \n Similarly, developers write Lambda functions to power Alexa skills for Amazon\nEcho. Amazon Echo is a hands-free speaker that responds to voice commands. It runs\na virtual assistant called Alexa. Developers can implement skills to extend Alexa’s capa-\nbilities even further (a skill is essentially an app that can respond to a person’s voice;\nfor more information, see http://amzn.to/2b5NMFj). You can write a skill to order a\npizza or quiz yourself on geography. Alexa is driven entirely by voice, and skills are\npowered by Lambda.\n3.1.8\nHybrids\nAs we mentioned in chapter 1, serverless technologies and architectures are not an all-\nor-nothing proposition. They can be adopted and used alongside traditional systems.\nThis hybrid approach may work especially well if a part of the existing infrastructure is\nalready in AWS. We’ve also seen adoption of serverless technologies and architectures\n\n\n45\nPatterns\nin organizations with developers initially creating standalone components (often to\ndo additional data processing, database backups, and basic alerting) and, over time,\nintegrating these components into their main systems (figure 3.3).\n3.2\nPatterns\nPatterns are architectural solutions to problems in software design. They’re designed\nto address common problems found in software development. They’re also an excel-\nlent communications tool for developers working together on a solution. It’s far easier\nto find an answer to a problem if everyone in the room understands which patterns\nare applicable, how they work, their advantages, and their disadvantages. \n The patterns presented in this section are useful for solving design problems in\nserverless architectures. But these patterns aren’t exclusive to serverless. They were\nused in distributed systems long before serverless technologies became viable. \n Apart from the patterns presented in this chapter, we recommend that you\nbecome familiar with patterns relating to authentication, data management (e.g.,\nCQRS, event sourcing, materialized views), and error handling (e.g., Retry Pattern).\nLearning and applying these patterns will make you a better software engineer,\nregardless of the platform you choose to use. Let’s look at a few of these patterns.\n3.2.1\nGraphQL\nGraphQL (http://graphql.org) is a popular data query language developed by Face-\nbook in 2012 and released publicly in 2015. It was designed as an alternative to REST\n(Representational State Transfer) because of its perceived weaknesses (multiple\nround-trips, over-fetching, and problems with versioning). GraphQL attempts to solve\nAny legacy system can use functions and services. This \ncan allow you to slowly introduce serverless technologies \nwithout disturbing too much of the world order.\nAPI\nGateway\nLoad\nbalancer\nDatabase\nFile\nstorage\nLambda\n(calculate\ncost)\nServer\nServer\nLambda\n(save\nprofile)\nLambda\nfunction\nFigure 3.3\nThe hybrid approach is useful if you have a legacy system that uses servers.\n\n\n46\nCHAPTER 3\nArchitectures and patterns\nthese problems by providing a hierarchical, declarative way of performing queries\nfrom a single endpoint (e.g., api/graphql). Figure 3.4 shows an example of a\nGraphQL and AWS Lambda implementation. \nGraphQL gives power to the client. Instead of specifying the structure of the response\non the server, it’s defined on the client (http://bit.ly/2aTjlh5). The client can specify\nwhat properties and relationships to return. GraphQL aggregates data from multiple\nsources and returns it to the client in a single round trip, which makes it an efficient\nsystem for retrieving data. According to Facebook, GraphQL serves millions of\nrequests per second from nearly 1,000 different versions of its application. \n A GraphQL library (server) can be hosted and run from a Lambda function. You’ll\nalso find managed solutions of GraphQL such as the ever-popular AWS AppSync at\nhttps://aws.amazon.com/appsync/.\nWHEN TO USE THIS\nGraphQL is a type of composite pattern that lets you aggregate data from multiple\nplaces. Reading and hydrating data from multiple data sources is common in web\napplications and especially so in those that adopt the microservices approach. There\nare other benefits too, including smaller payloads, avoiding the need to rebuild the\ndata model, and no more versioned APIs (as compared to REST). These are just some\nof the reasons why GraphQL has become so popular in the past few years. \n3.2.2\nCommand pattern\nIn the previous section, we mentioned the fact that a single endpoint can be used to\ncater to different requests with different data (a single GraphQL endpoint, for exam-\nple, can accept any combination of fields from a client and create a response that\nmatches the request). The same idea can be applied more generally. You can design a\nOnly a single GraphQL Lambda function is needed \nto query multiple data sources. It can be a viable \nalternative to building a full RESTful interface.\nAPI Gateway/\nGraphQL\nLambda\n(GraphQL)\nDatabase\nDatabase\nDatabase\nDatabase\nDatabase\nFigure 3.4\nThe GraphQL and Lambda architecture has become popular in the serverless community.\n",
      "page_number": 59
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 67-76)",
      "start_page": 67,
      "end_page": 76,
      "detection_method": "topic_boundary",
      "content": "47\nPatterns\nsystem in which a specific Lambda function controls and invokes other functions. You\ncan connect it to an API Gateway or invoke it manually and pass messages to it to\ninvoke other Lambda functions. \n In software engineering, the command pattern (figure 3.5) is used to “encapsulate a\nrequest as an object, thereby letting you parameterize clients with different requests,\nqueue or log requests, and support undoable operations” because of the “need to issue\nrequests to objects without knowing anything about the operation being requested or the\nreceiver of the request” (http://bit.ly/29ZaoWt). The command pattern lets you decou-\nple the caller of the operation from the entity that carries out the required processing.\nIn practice, this pattern can simplify an API Gateway implementation because you\nmay not want or need to create a RESTful URI for every request. It can also make ver-\nsioning simpler. The command Lambda function could work with different versions\nof your clients and invoke the right Lambda function that’s needed by the client.\nWHEN TO USE THIS\nThis pattern is useful if you want to decouple the caller and the receiver. Having a way\nto pass arguments as an object and allowing clients to be parametrized with different\nrequests can reduce coupling between components and help make the system more\nextensible. \n3.2.3\nMessaging pattern\nMessaging patterns (figure 3.6) are popular in distributed systems because they allow\ndevelopers to build scalable and robust systems by decoupling functions and services\nfrom direct dependence on one another and allowing storage of events/records/\nrequests in a queue. The reliability comes from the fact that if the consuming service\ngoes offline, the queue retains messages (for some period), which can still be pro-\ncessed at a later time.\nA command function is used to \ninvoke other functions and services. \nIt knows which functions to invoke \nin response to data/events and how \nto call those functions.\nLambda\nfunction\nLambda\nfunction\nFile\nstorage\nDatabase\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(command)\nAPI Gateway\nFigure 3.5\nThe command pattern invokes and controls functions and services from a single function.\n\n\n48\nCHAPTER 3\nArchitectures and patterns\nThis pattern features a message queue with a sender that can post to the queue and a\nreceiver that can retrieve messages from the queue. In terms of implementation in\nAWS, you can build this pattern on top of the SQS.\n Depending on how the system is designed, a message queue can have a single\nsender/receiver or multiple senders/receivers. SQS queues typically have one\nreceiver per queue. If you need to have multiple consumers, a straightforward way to\ndo it is to introduce multiple queues into the system (figure 3.7). A strategy you could\napply is to combine SQS with Amazon SNS. SQS queues can subscribe to an SNS topic\nso that pushing a message to the topic would automatically push the message to all of\nthe subscribed queues.\nSimilar to the command pattern, there \nis one function that reads messages \nfrom a queue. It invokes appropriate \nLambda functions based on the message.\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(dispatch)\nQueue (SQS) / \nstream (Kinesis)\nData source\nData source\nData source\nFigure 3.6\nThe messaging pattern and its many variations are popular in distributed environments.\nUse multiple queues/streams to decouple \nmultiple components in your system. \nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(dispatch)\nQueue (SQS) / \nstream (Kinesis)\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(dispatch)\nQueue #2 (SQS) / \nstream #2 (Kinesis)\nData source\nData source\nData source\nFigure 3.7\nYour system may have multiple queues or streams and Lambda functions to process all \nincoming data.\n\n\n49\nPatterns\nWHEN TO USE THIS\nThe messaging pattern handles workloads and data processing. The queue serves as a\nbuffer, so if the consuming service crashes, data isn’t lost. It remains in the queue until\nthe service can restart and begin processing it again. \n A message queue can make future changes easier, too, because there’s less cou-\npling between functions. In an environment that has a lot of data processing, mes-\nsages, and requests, try to minimize the number of functions that are directly\ndependent on other functions and use the messaging pattern instead. \n3.2.4\nPriority queue pattern\nA great benefit of using a platform such as AWS and serverless architectures is that\ncapacity planning and scalability are more of a concern for Amazon’s engineers than\nfor you. But, in some cases, you may want to control how and when messages get dealt\nwith by your system. This is where you might need to have different queues, topics, or\nstreams to feed messages to your functions. \n Your system might go one step further, having entirely different workflows for mes-\nsages of different priority (the priority queue pattern). Messages that need immediate\nattention might go through a flow that expedites the process by using more expensive\nservices and APIs with more capacity. Messages that don’t need to be processed\nquickly can go through a different workflow as figure 3.8 shows.\nMessages with different priorities can \nbe dealt with by different workflows \nand different Lambda functions.\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nPriority 1\nPriority 2\nPriority 3\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nNotification service (SNS) / Queue (SQS)\nNotification service (SNS) / Queue (SQS)\nNotification service (SNS) / Queue (SQS)\nNotification service (SNS) / Queue (SQS)\nFigure 3.8\nThe priority queue pattern is an evolution of the messaging pattern.\n\n\n50\nCHAPTER 3\nArchitectures and patterns\nThe priority queue pattern might involve the creation and use of entirely different\nSNS topics, SQS queues, Lambda functions, and even third-party services. Use this pat-\ntern sparingly, however, because additional components, dependencies, and work-\nflows result in more complexity. \nWHEN TO USE THIS\nThis pattern works when you need to have a different priority for processing mes-\nsages. Your system can implement workflows and use different services and APIs to\ncater to many types of needs and users (for example, paying versus nonpaying users).\n3.2.5\nFan-out pattern\nFan-out is a type of messaging pattern that’s familiar to many AWS users. Generally,\nthe fan-out pattern pushes a message to all listening/subscribed clients of a particular\nqueue or a message pipeline. In AWS, this pattern is usually implemented using SNS\ntopics that allow multiple subscribers to be invoked when a new message is added to a\ntopic. \n Take S3 as an example. When a new file is added to a bucket, S3 can invoke a sin-\ngle Lambda function with information about the file. But what if you need to invoke\ntwo, three, or more Lambda functions at the same time? The original function could\nbe modified to invoke other functions (like the command pattern), but that’s a lot of\nwork if all you need is to run functions in parallel. The solution is to use the fan-out\npattern with SNS (see figure 3.9).\nA message added to an SNS topic can force invocation \nof multiple Lambda functions in parallel.\nLambda\nfunction\nDatabase\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nNotification \nservice (SNS)\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nNotification \nservice (SNS)\nFigure 3.9\nThe fan-out pattern is useful because many AWS services (such as S3) can’t invoke more \nthan one Lambda function at a time when an event takes place.\n\n\n51\nPatterns\nSNS topics are communications or messaging channels that can have multiple pub-\nlishers and subscribers (including Lambda functions). When a new message is added\nto a topic, it forces invocation of all the subscribers in parallel, thus causing the event\nto fan out. \n Going back to the S3 example discussed earlier, instead of invoking a single\nLambda function, you can configure S3 to push a message to an SNS topic, which\ninvokes all subscribed functions simultaneously. It’s an effective way to create event-\ndriven architectures and perform operations in parallel. Chapter 8 shows how to use\nthis pattern to perform video encoding at scale.\nWHEN TO USE THIS\nThis pattern is useful if you need to invoke multiple Lambda functions at the same\ntime. An SNS topic will retry, invoking your Lambda functions, if it fails to deliver the\nmessage or if the function fails to execute (see https://go.aws/3DTdCEK). \n Furthermore, the fan-out pattern can be used for more than just invocation of\nmultiple Lambda functions. SNS topics support other subscribers such as email and\nSQS queues. Adding a new message to a topic can invoke Lambda functions, send an\nemail, or push a message on to an SQS queue, all at the same time.\n3.2.6\nCompute as glue \nThe compute-as-glue architecture (figure 3.10) describes the idea that we can use\nLambda functions to create powerful execution pipelines and workflows. This often\ninvolves using Lambda as glue between different services, coordinating and invoking\nthem. With this style of architecture, the focus of the developer is on the design of\ntheir pipeline, coordination, and data flow. The parallelism of serverless compute ser-\nvices like Lambda helps to make these architectures appealing.\n \n \n \n \n \n \nSQS vs. SNS vs. EventBridge\nSometimes it’s hard to know which AWS service to use in which situation. When it\ncomes to event messaging, we’ve discussed SQS and SNS already, but there’s also\nAmazon EventBridge to round out the family. The following Lumigo blog post features\nan excellent summary and comparison of these services: https://bit.ly/3AYdJga. We\nhighly recommend that you take a look at it if you are trying to understand their dif-\nferences and use cases. Another good explanation comes from AWS themselves:\nhttps://go.aws/3phPOWW.\n\n\n52\nCHAPTER 3\nArchitectures and patterns\n3.2.7\nPipes and filters pattern\nThe purpose of the pipes and filters pattern is to decompose a complex processing\ntask into a series of manageable, discrete services organized in a pipeline (figure\n3.11). Components designed to transform data are traditionally referred to as filters,\nwhereas connectors that pass data from one component to the next component are\nreferred to as pipes. Serverless architecture lends itself well to this kind of pattern. This\nis useful for all kinds of tasks where multiple steps are required to achieve a result.\nLambda\n(create\nthumbnail) \nNotification\nservice\n(SNS)\nFile\nstorage\nDatabase\nFile\nstorage (S3)\nLambda\n(write log)\nLog service\n(CloudWatch)\nSearch\nservice\nNotification\nservice\n(SNS)\nLambda\n(update)\nFigure 3.10\nThe compute-as-glue architecture uses Lambda functions to connect different services \nand APIs to achieve a task. In this pipeline, a simple image transformation results in a new file, an \nupdate to a database, an update to a search service, and a new entry to a log service.\nFunctions and services \nare reused in pipelines.\nData source\nLambda \nfunction\nLambda \nfunction\nNotification\nservice (SNS)\nLambda \nfunction\nDatabase\nFile\nstorage\nData source\nLambda \nfunction\nSearch \nservice\nLambda \nfunction\nLambda \nfunction\nFigure 3.11\nThe pipes and filters pattern encourages the construction of pipelines to pass and \ntransform data from its origin (pump) to its destination (sink).\n\n\n53\nSummary\nWith this pattern, we recommend that every Lambda function be written as a granular\nservice or a task with the single-responsibility principle in mind. Inputs and outputs\nshould be clearly defined (there should be a clear interface) and any side effects min-\nimized. Following this advice will allow you to create functions that can be reused in\npipelines and, more broadly, within your serverless system. \n You might notice that this pattern is similar to the compute-as-glue architecture we\ndescribed previously. You are right, compute as glue and this pattern are closely\nrelated and are simply a variation of the same concept. \nWHEN TO USE THIS\nWhen you have a complex task, try to break it down into a series of functions (a pipe-\nline) and apply the following rules:\nMake sure your function follows the single-responsibility principle.\nClearly define an interface for the function. Make sure inputs and outputs are\nclearly stated.\nCreate a black box. Consumers of the function shouldn’t have to know how it\nworks, but they must know to use it and what kind of output to expect.\nThroughout the rest of this book, we’ll discuss and give more context to the patterns\nand architecture we explored here. With that in mind, let’s jump into the next chap-\nter and read a story about a social network called Yubl.\nSummary\nServerless architecture can support different use cases including building\nbackends for web, mobile, and IoT applications, as well as data processing and\nanalytics.\nServerless technologies like AWS Lambda are flexible. They can be combined\nwith containers or virtual machines into hybrid architectures. You don’t need to\nbe a serverless purist to achieve great outcomes.\nCertain patterns and approaches like GraphQL are well suited to serverless\narchitectures because AWS services such as AppSync are on hand and can inte-\ngrate nicely with the rest of your architecture. \nClassic software engineering patterns like messaging patterns work exception-\nally well with serverless architectures and AWS products such as SQS.\nThe fan-out pattern is one of the more common patterns. Knowing how to set it\nup using Amazon SNS is important to be effective with AWS.\nAWS has a lot of different services and products that overlap. Having a thorough\nunderstanding of when to use each service will help you make better decisions.\n \n \n \n \n\n\nPart 2\nUse cases\nYou’ve read through part 1 and now, we hope, you have a good understand-\ning of what serverless is all about. It’s time to take a look at how three companies\nuse serverless architectures to solve problems and delight their customers. In\npart 2, we present three use-case studies from Yubl, A Cloud Guru, and Yle. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n57\nYubl: Architecture\n highlights, lessons learned\nIn April 2016, I joined a social network based in London called Yubl. There I inher-\nited a monolithic backend system written in Node.js and running on a handful of\nElastic Compute Cloud (EC2) instances. The original system took 2.5 years to imple-\nment and had a long list of performance and scalability issues once it went live. With\na small team of six engineers, we managed to move the platform to serverless over the\ncourse of six months. Along the way, we added many new features and addressed the\nexisting performance and scalability issues. We reduced feature delivery time from\nmonths to days, and in some cases, hours. Although cost was not the main motivation\nfor undertaking this transformation, we made a 95% savings on our AWS bill in the\nprocess. Let’s take a peek at the original Yubl architecture.\nThis chapter covers\nThe original Yubl architecture and its problems\nThe new serverless architecture and the \ndecisions behind it\nStrategies and patterns for moving monolith \napplications to serverless\nLessons learned from this migration\n\n\n58\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\n4.1\nThe original Yubl architecture\nYubl (short for “your social bubble”) was a mobile-first, social network designed for\nthe 17 to 25-year-old demographic. The user-generated posts (called yubls) contained\nvideos as well as animated and interactive elements. The app had all the social fea-\ntures you’d find in other social networks: follow users, private and group chat, liking\nand resharing content, and others.\n The original architecture (figure 4.1) consisted of the following:\nA monolithic REST API written in Node.js and running on EC2\nA WebSockets API written in Node.js and running on EC2\nA monolithic MongoDB database hosted in MongoLab\nA CloudAMQP message queue\nA cluster of background workers written in Node.js and running on EC2   \nMongoLab\nRoute53\nELB\nAPI\nRoute53\nELB\nWebSockets\nWorkers\nFigure 4.1\nA high-level overview of the original Yubl architecture\nWhat is MongoDB and MongoLab?\nMongoDB is a popular document-oriented NoSQL database that allows you to store\nJSON documents. You can learn more about it at https://www.mongodb.com.\nMongoLab is an online service that provides MongoDB hosting as a service. You can\ncreate a MongoDB cluster with a few clicks, and MongoLab takes care of the under-\nlying infrastructure for you. You can learn more about it at https://mlab.com. Back\nin 2016, MongoLab was a viable service for running MongoDB without having to man-\nage the underlying infrastructure yourself.\n",
      "page_number": 67
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 77-88)",
      "start_page": 77,
      "end_page": 88,
      "detection_method": "topic_boundary",
      "content": "59\nThe original Yubl architecture\n4.1.1\nScalability problems\nBeing an early stage social network, the baseline traffic at Yubl was low, but they man-\naged to attract several high-profile Instagram influencers to the platform. These influ-\nencers brought many of their Instagram followers along, and with tens of thousands of\nfollowers, these influencers drove unpredictable and spiky traffic through the system\nwhenever they posted new content. \n We often saw 100x spikes in traffic as thousands of users flooded in all at once to\nsee their favorite influencer’s new content. These traffic spikes were usually short-\nlived, which was problematic for the EC2-based system because EC2 autoscaling\ncouldn’t react fast enough. It typically takes EC2 instances a few minutes to spin up. By\nthe time they are ready to serve user requests, it’s too late. The traffic spikes have\ncome and gone, and many users would have left after having experienced a laggy\nresponse time.\n As a workaround, we ran a much larger EC2 cluster, scaling up much earlier than\nwe wanted. This resulted in a lot of wasted cost because we had to pay for lots of EC2\nresources that we were not using. Our cluster of API web servers had an average utili-\nzation of from 2% to 5%.\n4.1.2\nPerformance problems\nThe monolithic MongoDB database was also a constant source of performance and\nscalability problems. Every read and write operation hit the database directly; some\nAPI operations can take a heavy toll on a MongoDB server. One example of this is a\nuser search, which is a frequently used API call and executes a complex regex query\nagainst MongoDB. Another example included user recommendations, which exe-\ncuted a complex query to find second- and third-degree connections to the current\nuser (those who follow your followers or those followed by users you follow).\n4.1.3\nLong feature delivery cycles\nThe codebase was complex, and many features were intertwined through shared\nMongoDB collections and implicit coupling through shared libraries. Although there\nwere plenty of unit tests with a reasonable code coverage, these did not prove useful\nbecause code changes often passed all the tests, only to fail when deployed to the AWS\nWhat is RabbitMQ and CloudAMQP?\nAdvanced Message Queueing Protocol (AMQP) is an application protocol for mes-\nsage-oriented middlewares. It supports message queueing and routing and is often\nused in publish-and-subscribe systems.\nRabbitMQ is an open-source message broker that implements the AMQP proto-\ncol. You can learn more abo9ut RabbitMQ at https://www.rabbitmq.com/.\nCloudAMQP is an online service that provides RabbitMQ hosting. You can learn\nmore about it at https://cloudamqp.com.\n\n\n60\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\nenvironment. The interaction with external services was mocked thoroughly and,\ntherefore, not covered by the tests. In many cases, the tests simply confirmed the\nmocks were working and returned what was requested even if the MongoDB query\ncontained syntax errors. We had little faith in the tests because they gave us too many\nfalse-positives.\n To make matters worse, every deployment required taking the whole system down\nfor 30 minutes or more, during which time users received no feedback and the app\njust appeared broken. Features used to take months to go to production. Even simple\nchanges often took weeks to complete, which was frustrating to everyone involved.\n4.1.4\nWhy serverless?\nBased on the requirements for our system and the problems the current implementa-\ntion experienced, serverless was a great fit for the following reasons:\nAWS Lambda autoscales the number of concurrent executions based on load. This\nhappens instantly and handles those unpredictable spikes we experience\neffortlessly.\nAWS Lambda deploys functions to three availability zones by default, which provides sig-\nnificant redundancy without incurring extra costs. We pay only when a function\nruns, whereas with EC2, we paid for the redundancy in a multi-AZ setup, which\nalso dilutes the traffic and reduces the resource utilization even further.\nAWS manages the underlying physical infrastructure as well as the operating system that\nour code runs on. AWS applies patches and security updates regularly and does a\nmuch better job of keeping the operating system secure than we could. This\nremoves a whole class of vulnerabilities that plague so many software systems\naround the world.\nWith tools such as the Serverless framework, the deployment pipeline for our application is\ndrastically simplified. A typical deployment takes less than a minute and has no\ndowntime because AWS Lambda automatically routes requests to the new code.\nWhen using serverless technologies such as API Gateway, Lambda, and DynamoDB, we\ndon’t have to worry about the underlying infrastructure. This lets us focus on address-\ning core business needs. Almost every line of our code is business logic! And it\nallows the development team to move quickly, knowing that what we build is\nscalable and resilient by default.\nThe number of production deployments went from four to six per month to averaging more\nthan 80 per month with the same sized team. We didn’t have to hire more people to\ngo faster, we allowed each developer to be more productive instead.\nAs we migrated more and more of the system to serverless, scalability, cost and reliability all\nimproved. There were far fewer production issues, and we were spending a frac-\ntion of what we spent on EC2 previously.\n\n\n61\nThe new serverless Yubl architecture\n4.2\nThe new serverless Yubl architecture\nBy November 2016, less than 8 months after I joined the company and started us on\nthe journey to serverless, almost the entire backend system was migrated to serverless;\nthis using a combination of services such as API Gateway, Lambda, DynamoDB, Kine-\nsis, and so much more. Along the way, we enhanced existing features and imple-\nmented countless new features. We also addressed many security issues with the\nprevious system. \n Overall, the system’s reliability increased drastically. We experienced only one\nminor outage to our production environment because of a brief Simple Storage Ser-\nvice (S3) outage. The following points are some key highlights of the new serverless\narchitecture on AWS:\nThe monolith was broken up into many microservices.\nEvery microservice has its own GitHub repository and one Continuous Integration/\nContinuous Delivery (CI/CD) pipeline. All the components that make up this\nmicroservice (API Gateway, Lambda functions, DynamoDB tables, etc.) are\ndeployed together as one CloudFormation stack using the Serverless Framework.\nMost microservices have an API Gateway REST API running under its own sub-\ndomain, such as search.yubl.com.\nEvery microservice has its own database for the data it needs. Most use\nDynamoDB, but it’s not universal because different microservices have different\ndata needs.\nEvery state change in the system is captured as an event and published to a\nKinesis Data Stream (for example, a user created new content, a user posted\nnew content, and so on).\nMost of the time, we prefer to synchronize data between microservices through\nevents rather than synchronous API calls at run time. This helps prevent cas-\ncade failures when one microservice experiences an outage in production.\nInstead, microservices subscribe to the relevant Kinesis Data Stream and copy\nneeded data from the appropriate events.\nThis diagram (https://d2qt42rcwzspd6.cloudfront.net/overall.png) shows a birds-eye\nview of this new architecture. Don’t worry about making sense of everything in the fig-\nure. It merely demonstrates the fact that you can build even complex systems using\nserverless components.\n It’s worth mentioning that the move to serverless was not one of our goals. The goal\nwas to deliver a better user experience with less downtime, more responsiveness, and\nmore scalability. Serverless technologies like Lambda, API Gateway, and DynamoDB\nhappen to be a great way to achieve our goals while also making our lives a lot easier and\nallowing us to ship features faster.\n\n\n62\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\n4.2.1\nRearchitecting and rewriting \nTo fully realize our goals, we had to rearchitect and rewrite large parts of the system.\nBut we didn’t want to migrate everything to serverless for the sake of migrating them.\nWe wanted to accelerate feature development and deliver value to our users faster\nthan before. For this, we took a pragmatic approach, whereby we made a case-by-case\ndecision on whether to rearchitect a feature when we needed to work on it. \n To mitigate our risks, we rearchitected and migrated features that had the least\nbusiness impact first. Business critical features such as timelines (which is the first\nthing you see in the app) were tackled only when we had gained sufficient confidence\nand know-how. This approach of migrating a large system piece-by-piece is commonly\nreferred to as the strangler pattern.\n In the Yubl app, you could search other users by first name, last name, and user-\nname. This was a simple feature, but it caused crippling performance issues with the\nmonolith as the number of users grew. This was because a search was implemented\nwith regex queries against MongoDB. The old implementation also didn’t allow for\nmore sophisticated ranking, so users often couldn’t find who they were looking for.\nThere was a push from the marketing team to surface influencers further up the\nsearch results as many users had followed these influencers onto the platform.\n This was the first feature that we rearchitected and migrated to serverless because\nit was both low-risk and could have a high impact. Let’s dive into how we extracted the\nsearch feature out of the monolith and built a microservice around it with its own\nREST API.\n4.2.2\nThe new search API\nOne of the first and most important steps was to ensure that our legacy monolith\nwould publish its state changes to Kinesis Data Streams. This gave us a foundation to\nbuild the new microservices by building on top of these events. To extract the search\ncapability out of the monolith, we created a new search microservice. Figure 4.2 shows\nthe high-level architecture of this search microservice.\nEC2\nKinesis Data Stream\nLambda\nCloudSearch\nLambda\nAPI Gateway\nRoute53\nsearch.yubl.com\nThe new search microservice\n1\n2\n3\n4\n5\n6\nFigure 4.2\nA high-level overview of the new architecture running on \nserverless components\n\n\n63\nThe new serverless Yubl architecture\nIf you follow the numbered arrows in figure 4.2, this is how all the pieces fit together:\n1.\nThe legacy monolith publishes all user-related events to a Kinesis Data Stream\ncalled users. These include the user-created and user-profile-updated events\nthat tell us when a new user joins or a user has updated their profile.\n2.\nA Lambda function subscribes to the users stream.\n3.\nThe Lambda function uses these events to insert, update, or delete user docu-\nments in the users index in Amazon CloudSearch.\n4.\nA new API in API Gateway with a POST /?query={string} endpoint proxies to\nanother Lambda function to handle the HTTP request.\n5.\nThe Lambda function translates a user’s query string into a search request\nagainst the users index in Amazon CloudSearch.\n6.\nTo create a user-friendly subdomain for the new REST API, a custom domain\nname in API Gateway for search.yubl.com is registered in Route53.\nFor this microservice, we chose Amazon CloudSearch instead of Amazon Elastic-\nSearch because, at the time, Amazon ElasticSearch didn’t allow you to change the\nnumber of write nodes in an ElasticSearch cluster, which is a scalability concern for\nthe write throughput. But Amazon CloudSearch was not without its problems. \n Although you can autoscale the read and write nodes independently, scaling up a\nCloudSearch cluster takes as long as 30 minutes. This did not match well with our\nspiky workload, and we had to overprovision the read cluster as a result. If I imple-\nmented this service again today, I would definitely use Amazon ElasticSearch or a\nthird-party service such as Algolia (https://algolia.com) instead.\n Before we launched the new service, we also needed to ensure all existing user\ndata was available in the CloudSearch index. To do this, we ran a one-off task to copy\nall existing user data (~800,000 users) from MongoDB to CloudSearch, while tracking\nthe most recent user profile update. Only after this was complete, did we enable the\nfunction at step 2 (figure 4.2) to start processing user updates. \n Another important detail to note here is that when we enabled the function’s Kine-\nsis subscription, we processed events from when the one-off task started. With Kinesis,\nyou are able to specify the StartingPosition of the subscription. You can configure\nthis to AT_TIMESTAMP to start processing events from a specific timestamp. Processing\nevents from when a one-off task started ensured that we didn’t miss any updates that\nhappened while Yubl was running.\n Once live, performance of the new search service was significantly improved over\nthe old search. It also removed a lot of the load on the monolith MongoDB database\nin the process, which had a positive impact on the general responsiveness of the app.\nIt also gave us a template on how to build other microservices using serverless technol-\nogies such as API Gateway and Lambda.\n\n\n64\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\n4.3\nMigrating to new microservices gracefully\nBuilding the new microservices was the easy part. The difficult part was how to\nmigrate them safely and be able to roll back quickly if there were any unforeseen\nissues. Another concern was how to do it gracefully without downtime and impact on\nour users. Suppose your starting position is a monolith where all the features are\naccessing directly a shared database (figure 4.3). Where will you begin?\nWe started to break apart this monolith into microservices built with serverless compo-\nnents such as API Gateway, Lambda, and DynamoDB. As we moved a feature out of\nthe monolith into its own microservice, we wanted the microservice to be the author-\nity over some part of the system, be it user profiles or product catalogue or customer\norders. The microservice has its own database, and other microservices (or the mono-\nlith) should not be able to reach into its database and access or manipulate data\ndirectly.\n Instead, to instigate some change in state, other microservices need to communi-\ncate with this microservice through its API. This can be HTTP-based in the form of a\nREST API call or message-based in the form of publishing an event/message to a\nqueue. The important thing is to cut off direct access to and manipulation of data that\nthe microservice is supposed to be the authority of (figure 4.4). How do you do this\ngracefully without causing significant disruption to your users?\n The challenge here is that it’s risky to do a big-bang migration because it usually\nrequires downtime. That is not to say that you should never entertain the idea of a big-\nbang migration. If you’re a small startup and have few users on your current platform,\nthen a big-bang migration with downtime is quite possibly the fastest and most effi-\ncient approach for you. But for many organizations that are undergoing such migra-\ntion, it’s important to minimize the risk and disruption caused by moving to a new\nmicroservice.\n \n \n \n \nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature E\nFeature F\nFigure 4.3\nA monolithic system where \neverything has direct access to a shared \ndatabase\n\n\n65\nMigrating to new microservices gracefully\nA common strategy is to perform the migration in multiple steps to maximize safety.\nFor example, the following process describes some likely steps as figure 4.5 illustrates:\n1.\nMove the business logic for a particular feature into a separate service and cre-\nate its own API. The new service will still use the monolith database until it has\nauthority over the data.\n2.\nFind the places where the monolith accesses this feature’s data directly and\nredirect those access points to go through the new service’s API instead. Start\nwith the least critical component first to minimize the blast radius of any\nunforeseen problems or impacts.\n3.\nMove all other direct access points to the new service’s data to go through its\nAPI (probably, one at a time).\n4.\nNow that the new service is the authority over its data, you can plan a course to\nmigrate the data out of the monolith database into its own database. You might\nuse a different database, based on your requirements for this new service. If\nyour access pattern is simple and mostly key lookups, then DynamoDB is proba-\nbly a good choice.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\nFigure 4.4\nA monolithic system where everything has direct access to a shared database\n\n\n66\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\n3\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\n1\nService\nFeature E\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\n2\nMigrate the least\ncritical component first.\nService\nFeature E\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\n4\nFigure 4.5\nGradually cut off the direct access to the shared monolith database by moving \naccess to go through the new microservice’s API instead.\n\n\n67\nMigrating to new microservices gracefully\n5.\nOnce you have created the new database, you need to migrate data from the\nmonolith database. To do so without downtime, you can treat the new database\nas a read-through and write-through cache: any updates and inserts are written\nto the monolith database and then copied to the new database (figure 4.6). \nWhen attempting to read, you will read from the new database first. If the\ndata is not found, then read from the monolith database and save the data in\nthe new database.\n6.\nRun a one-off task in the background to copy over all existing data (figure 4.6).\nTake care to ensure that you don’t overwrite newer updates. (With DynamoDB,\nhttps://amzn.to/2IbE818, this can be done using conditional writes.)\nThis is a useful pattern for extracting features from a monolith and moving them into\nmicroservices that can scale and fail independently. There is more you can do to ensure\nthat you do so safely and gracefully to minimize the potential impact on your users. For\nexample, you can route only a small percentage of traffic to the new microservice when\nit first goes live. This limits the blast radius of any unforeseen problems with the new\nmicroservice. It is especially important for microservices that are user-facing and that\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\n5\nTreat new DB as read-through/\nwrite-through cache.\nread\nwrite\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\n6\nAlso run one-off migration \njob in the background.\nRead\nWrite\nMigration\nFigure 4.6\nMigrate data to the new database gradually, without downtime.\n\n\n68\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\nhandle requests from the mobile/web client directly because they can have a big impact\non user experience. \n This approach is commonly known as the canary pattern and is not limited to sys-\ntem migrations. The term canary deployment refers to a deployment strategy where the\npattern is used for every deployment when a small percentage of traffic is directed at\nthe new version of the application, which limits the blast radius of any unforeseen\nproblems. If you’re using the Application Load Balancer (ALB) in front of your appli-\ncation, then you can configure this routing behavior there (figure 4.7). \nWhere this approach is not possible (or in the case of Yubl where ALB didn’t exist at\nthe time), you can also proxy requests from the monolith for a configurable percent-\nage of requests. Figure 4.8 shows this approach.\nSummary\nYou need to re-architect most applications to reap the full benefit of a serverless\narchitecture. Although there are solutions to lift and shift existing applications\ninto serverless, these don’t deliver optimal performance and scalability.\nTo get the full benefit of a serverless architecture, you need small, autonomous\nteams who are capable of making their own architectural decisions. Developers\nshould be responsible for more than just the code and empowered to own their\nsystem. As Amazon’s motto goes, “You build it, you run it.” \n90%\n10%\nMonolith DB\nMonolith\nFeature E\nFeature F\nService\nFeature E\nDB\nALB\nFigure 4.7\nYou can use the Application Load Balancer (ALB) to distribute traffic between \nthe monolith and the new microservices. This allows you to minimize impact of unforeseen \nproblems to a subset of users.\n10%\nMonolith DB\nMonolith\nFeature E\nFeature F\nService\nFeature E\nDB\nFigure 4.8\nEven without ALBs, you can still proxy requests by modifying the monolith.\n\n\n69\nSummary\nDevOps is simpler with serverless. You get a lot of automation out of the box,\nand tools such as Serverless Framework takes care of the rest. You still need to\nknow what metrics to pay attention to and what alerts to add, however, as opera-\ntional experience of running a production system is still valuable.\nUnit tests have a low return on investment when it comes to serverless architec-\ntures. Most functions are simple and often integrate with other services such as\nAmazon’s DynamoDB and Simple Queuing Service (SQS). Unit tests that mock\nthese integration points do not test those service interactions and give you a\nfalse sense of security.\nPrefer integration tests that exercise the real AWS services for the happy paths\nand use mocks only for failure cases that are difficult to simulate otherwise. For\nexample, execute the function code locally but have it talk to the real DynamoDB\ntables. Then use mocks when you need to test your error handling for\nDynamoDB’s throughput exceeded errors.\nServices often have to call each other in a microservices architecture. For inter-\nnal APIs that are more prone to breaking in the development environments\n(compared to AWS services), use mocks to isolate the failures. The last thing\nyou want is for an error in one service to fail the tests for all other services that\ndepend on it.\nSimulating AWS services (for example, DynamoDB, SNS, SQS) locally is not\nworth the effort. It’s easier and quicker to deploy a temporary stack, than using\nlocal simulation tools.\nWhen dealing with batched event sources like Kinesis and SQS, you need to\nthink about how to handle partial failures. You either have to make sure that\nthe operations are idempotent and can be retried without problem, or you\nneed to ensure that successfully processed items in a failed batch are not pro-\ncessed again when the batch is retried.\n\n\n70\nA Cloud Guru:\n Architecture highlights,\n lessons learned\nIn the first edition of this book, we described a serverless LMS (Learning Manage-\nment System) built by A Cloud Guru (https://acloudguru.com). At that time, A\nCloud Guru built a RESTful API backend using Amazon API Gateway, AWS\nLambda, and Google’s Firebase as its primary database. Since we published our first\nedition, A Cloud Guru has gone through a major transformation. The company\nmoved from a RESTful monolithic design to a GraphQL-driven microservices\narchitecture. This chapter describes this journey. We’ll look at the original RESTful\ndesign, the transition to microservices, how GraphQL plays a major part, and the\nlessons learned along the way. \nThis chapter covers\nA Cloud Guru’s original REST architecture\nThe reasons the team decided to migrate from \nREST to microservices and GraphQL\nLessons learned through the migration\n",
      "page_number": 77
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 89-98)",
      "start_page": 89,
      "end_page": 98,
      "detection_method": "topic_boundary",
      "content": "71\nThe original architecture\n One thing is clear though, serverless technologies allowed A Cloud Guru to re-\narchitect their platform rapidly and with minimal fuss. As a developer, you can be\nmore agile with a serverless application than with a traditional three-tier behemoth.\nThis is because, in a serverless approach, your primary focus is on the architecture of\nthe system, your data, and the code. A Cloud Guru developers didn’t need to spend\ntime and energy worrying about provisioning servers, updating server software, or\nmanaging Kubernetes clusters. That alone saved them time and gave them the oppor-\ntunity to focus on the platform elements that were critical to the business.\n5.1\nThe original architecture\nA Cloud Guru is an online educational platform for anyone wanting to learn Amazon\nWeb Services (AWS), Microsoft Azure, and Google Cloud Platform, as well as Cloud-\nrelated technologies. The core features of the platform include the following: \nOn-demand video courses \nPractice exams and quizzes\nA real-time discussion forum\nDashboards and reporting\nUser profiles and gamification\nEducational features like learning paths\nInteractive sandbox environments for students wanting to test their skills\nA Cloud Guru is also an ecommerce platform that allows students to pay for a monthly\nor yearly subscription and have access to content and features. The training architects\nwho create courses for A Cloud Guru can upload videos directly to S3. These videos\nare immediately transcoded to a variety of formats and resolutions (1080p, 720p, HLS,\nand so on).\n In 2017–2018, the A Cloud Guru platform used Firebase as its primary database. A\nnice feature of this database is that it allows client devices (the browser on your com-\nputer or phone) to receive updates in near real time without refreshing or polling.\n(Firebase uses web sockets to push updates to all connected devices at the same time.)\nThe other main components were API Gateway and AWS Lambda. Figure 5.1 shows a\nbasic high-level view of how that initial REST architecture looked. \n In the first edition of this book, we described how to build a serverless system with\na RESTful interface. We wanted to illustrate the fact that you can create sophisticated,\nscalable, and highly available platforms using functions and services provided by AWS\nand Google Cloud Platform. The A Cloud Guru team was able to do that and go far\nbeyond. They built a system that would go on to serve tens of thousands of concurrent\nusers. Figure 5.2 shows a slightly more advanced version of the same architecture as\nwas presented in the first edition of this book.\n \n \n \n\n\n72\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nAPI\nGateway\nLambda\nfunction\nFirebase\nClient Web Browser connects \nto Firebase directly.\nThere are multiple Lambda \nfunctions connected to the \nAPI Gateway.\nFigure 5.1\nA basic high-level view of the initial \nA Cloud Guru architecture. Their system was \nmore complex (there were many more Lambda \nfunctions), but in a nutshell, this is how it worked.\nStudents are given \npermission to read files \nfrom S3 via CloudFront.\nLecturers are given \npermission to upload \nto S3.\nAPI\nGateway\nAuth0\n(auth)\nFirebase\n(database)\nS3 \n(file storage)\nLambda\n(transcode\nstart)\nS3 \n(file storage)\nLambda\n(transcode\nfinish)\nFirebase\nFirebase\nMedia transcoding pipeline\nFirebase\nS3 \n(file storage)\nCloudFront\nLambda\n(forum\nanswer)\nLambda\n(answer \nsubmit)\nLambda\n(read file)\nLambda\n(upload file)\nFigure 5.2\nThis is a slightly more advanced version of the A Cloud Guru architecture. The actual \nproduction architecture had Lambda functions and services for performing payments, managing \nadministration, gaming, reporting, and analytics.\n\n\n73\nThe original architecture\nThe original system worked well and scaled as the development team expected. It was\nalso inexpensive to run with the AWS bill being just a few thousand dollars (the\nLambda and the API Gateway bill was under $1,000). Note the following about the\noriginal A Cloud Guru architecture (figure 5.2):\nThe frontend was built using AngularJS and was hosted by Netlify (https://\nnetlify.com).\nAuth0 was used to provide registration and authentication. It creates delegation\ntokens that allow an AngularJS website to directly and securely communicate\nwith other services such as Firebase.\nEvery client created a connection to Firebase using web sockets and received\nupdates from it in near real time. This meant that clients received updates as\nthey happened without having to poll (which led to a nicer user experience). \nThe training architects who created content for the platform uploaded files\n(usually videos) straight to an S3 bucket via their browser. \nFor this to work, the web application invoked a Lambda function to first\nrequest the necessary upload credentials. As soon as the credentials were\nretrieved, the client web application uploaded the file to S3 via HTTP. All of this\nhappened behind the scenes and was invisible to the training architects.\nOnce a file was uploaded to S3, the system automatically kicked off a chain of\nevents that transcoded the videos, saved the new files in another bucket,\nupdated the database, and immediately made the transcoded videos available to\nother users.\nTo view the videos, students were given permission by another Lambda func-\ntion. Permissions were valid for 24 hours, after which they were to be renewed. \nFiles were accessed via CloudFront. CloudFront ensures that users have low-\nlatency access to videos wherever they may be.\nOver time, the A Cloud Guru development team began considering the future of\ntheir serverless REST architecture. The company wanted to further accelerate the\ndevelopment of the platform, reduce blockers, and allow independent teams to focus\non different high-value features. The following were some of the considerations that\ndrove the decision to change the architecture:\nThe existing architecture that was created was, in a sense, a serverless monolith.\nThere were a large number of Lambda functions, but they connected to the\nsame Firebase database. Making a change to the database would affect nearly\nevery Lambda function and the developers working on them. This made it easy\nin the existing system for developers to step on each other’s toes.\nThe business wanted to have separate development teams owning different\nparts of the product. For example, the student-experience team would need to\nbe able to update a database and deploy a Lambda function without affecting\nthe team responsible for billing and reporting. \n\n\n74\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nTransitioning to a true microservices approach (where each microservice owns\nits data and its own view of the world) would allow teams to develop the plat-\nform in parallel. Each development team would look after a number of micros-\nervices and iterate on them as needed. This would mean moving away from a\nsingle Firebase database to multiple databases and yet still provide a way to read\nand hydrate data as needed.\nMoving to a microservices approach would give the teams a greater level of iso-\nlation. This would mean that different subsystems and components within the\ncode base would have clearer boundaries in terms of ownership and a looser\ncoupling. \nThe team wanted to find a way to minimize round trips to the backend and\nfetch only the data that was needed. Devs also wanted to be able to serve multi-\nple clients like mobile and web. While this can be accomplished with REST, the\nteam determined that GraphQL was a better fit.\nFinally, the company felt that Firebase was getting a little bit too expensive.\nGiven the platform’s access usage patterns, Amazon’s DynamoDB looked like the right\ndatabase to move to. Migrating to DynamoDB would allow teams to better manage\ninfrastructure using CloudFormation and use built-in DynamoDB features like event\ntriggers. And it would allow teams to stay entirely within the AWS environment.\n Refactoring to a proper microservices approach and moving to DynamoDB as the\nprimary database necessitated a rethink of the entire architecture. One of the main\nquestions to consider was how to get data from disparate microservices and do it as\neffectively as possible (without multiple round trips or data hydration on the client)\nwhen a user made a request. This is where GraphQL entered the picture and became\nthe focus of the new architecture. But before we get to GraphQL, let’s see how the A\nCloud Guru team split up their monolith and created their microservices first.\nA serverless monolith\nThe RESTful API design that A Cloud Guru originally created was a serverless mono-\nlith. There was a single database and functions that needed to save or load data con-\nnected to it. There is nothing wrong with building a serverless monolith. For A Cloud\nGuru, it scaled well for a long time and helped build the company. The core reason\nfor the move to a microservices design was the need for multiple teams to work in\nparallel. \nIf you are starting out today, know that it is OK to go with a monolithic approach.\nWhen you need to, you can migrate to microservices. Remember, you don’t have to\nfollow the trend and do the microservices road if it isn’t right for you.\n\n\n75\nThe original architecture\n5.1.1\nThe journey to 43 microservices\nLet’s take a look at the serverless microservices approach the company came up with.\nFirst, here are some stats of the new GraphQL re-architected A Cloud Guru platform\nat the start of 2020:\n240 million Lambda invocations per month (100 per second)\n180 million API Gateway calls per month (70 per second)\n90 TB of data transferred from CloudFront per month (274 MB per second)\nThe team began to break apart the monolith and move to a microservices architec-\nture during 2018. API Gateway and Lambdas were separated into discrete microser-\nvices, each with their own responsibilities and view of the world. In the new world of\nmicroservices, each service could be as simple as a single DynamoDB table, a couple\nof Lambda functions, and an API Gateway. Figure 5.3 shows an example of how a cou-\nple of basic microservices could look.\nThe packaging of the microservices is also interesting to note. A Cloud Guru uses\nServerless Framework and CloudFormation to organize and deploy microservices.\nSome services in a microservice are stateful, whereas others are stateless. A Lambda\nTwo basic examples of how simple microservices could be \nstructured with Lambda, API Gateway, DynamoDB, and S3.\nLambda\nfunction\nLambda\nfunction\nAPI Gateway\nLambda\nfunction\nDynamoDB\nDynamoDB\nS3 bucket\nLambda\nfunction\nAPI Gateway\nLambda\nfunction\nSample Microservice #1\nSample Microservice #2\nFigure 5.3\nThe two microservices here are akin to the simplified RESTful architecture we \ndiscussed before.\n\n\n76\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nfunction is stateless, meaning that it can be overwritten on each deployment. It’s\nalways ephemeral. A DynamoDB table or an S3 bucket is stateful; you must be careful\nto preserve the data that is already there. Your deployment process cannot overwrite\nit. Also, there can be global services and resources that don’t belong to any specific\nmicroservice. How do you think they should be deployed and managed?\n The A Cloud Guru team designed their microservice so they would have different\nCloudFormation stacks for stateless and stateful resources, as well as a stack for config-\nuration and core dependencies. Figure 5.4 shows what that looks like. \nThis approach to different CloudFormation stacks for different kinds of resources\nallows the development team to deploy the stack with the stateless resources when they\nneed to be updated without having to touch stateful resources. The same goes for the\nconfiguration and the core-dependencies stack. They can be updated without modify-\ning anything else within the microservices. This kind of separation of concerns is\nadvantageous because it can help to avoid accidental modification of stateful resources.\n There are also a few other global dependencies that exist as well, but these are not\nwithin any microservice. They include infrastructure components such as Amazon\nRedShift (data warehouse), AWS WAF (firewall), and VPCs (virtual private cloud).\nMicroservices were designed to avoid having a hard dependency on these global\nresources. In fact, there is quite a loose coupling between them. \n For example, if a microservice needs to push data into RedShift, it doesn’t do it\ndirectly. Instead, a regular ETL job pulls data out of microservices and writes it to Red-\nShift. That means microservices don’t have to know about RedShift. A microservice\ncan live and breathe on its own while a separate ETL task does its own job. Figure 5.5\nshows that it’s necessary for some resources to live outside specific microservices.\nStateless \nresources\nStateful \nresources\nConfiguration \nand core \ndependencies \nMicroservice 1\nMicroservice 2\nMicroservice 3\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nFigure 5.4\nEach microservice is in its own CloudFormation stack, which makes it easy to deploy.\n\n\n77\nThe original architecture\nThe A Cloud Guru team gradually teased apart the serverless monolith that was cre-\nated in the first place and re-implemented it with microservices. Moving from one\narchitecture to a another always takes time but a nice advantage here was that the\nteam could primarily focus on code. There was no hardware, servers, or a container\norchestration engine (like Kubernetes) to worry about. The team carefully and gradu-\nally reimplemented various components making sure that no users were affected\nduring the change.\n As part of the move to microservices, GraphQL became the solution to the ques-\ntion of how to pull the right data from different microservices when a client makes a\nrequest. After all, each microservice may have its own database and its own view of the\nworld. When a user needs to get data, how does it all happen? Which microservice has\nasked for it? And, what if multiple microservices have the required information, and\nthe client needs an aggregate response? GraphQL became the tool to query microser-\nvices and, with schema-stitching, create responses needed for the clients.\n5.1.2\nWhat is GraphQL\nWe already mentioned GraphQL in chapter 3, but let’s do a quick recap about what it\nis. GraphQL is a popular data query language developed by Facebook in 2012 and\nThese resources exist outside \nof any individual microservice. \nAmazon \nRedshift\nAWS WAF\nAmazon VPC\nMicroservice 1\nMicroservice 2\nMicroservice 3\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nFigure 5.5\nGlobal dependencies reside outside of each individual microservice. Not everything has to exist \nwithin a microservice.\n\n\n78\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nreleased publicly in 2015. It was designed as an alternative to REST because of REST’s\nperceived weaknesses (multiple round-trips, over-fetching, and problems with version-\ning). GraphQL attempted to solve these problems by providing a hierarchical, declar-\native way of performing queries from a single endpoint (for example, api/graphql).\nFigure 5.6 provides an illustration of how this looks.\nGraphQL gives power to the client. Instead of specifying the structure of the response\non the server, it’s defined on the client. The client can specify which properties and\nrelationships to return. GraphQL aggregates data from multiple sources and returns it\nto the client in a single round trip, which makes it an efficient system for retrieving data. \n According to Facebook, GraphQL serves millions of requests per second from\nnearly 1,000 different versions of its application. To further illustrate what GraphQL\nlooks like, here’s a simple query taken from https://graphql.org/learn/queries/:\n{\n  hero {\n    name\n  }\n}\nAnd one possible response to that query:\n{\n  \"data\": {\n    \"hero\": {\n      \"name\": \"R2-D2\"\n    }\n  }\n}\nGraphQL allows you to query multiple \ndatabases from a single endpoint.  \nAPI Gateway/\nGraphQL\nLambda\n(GraphQL)\nDatabase\nDatabase\nDatabase\nDatabase\nDatabase\nGraphQL\nFigure 5.6\nA GraphQL library running in a Lambda function can query multiple databases and, \nusing schema stitching, produce a result relevant for each individual client.\n\n\n79\nThe original architecture\nIn a serverless architecture, GraphQL can be run from a single Lambda function con-\nnected to the API Gateway (this is what A Cloud Guru did) or used through a service\nlike AWS AppSync. GraphQL can query and write to multiple data sources such as\nDynamoDB tables and, using schema-stitching, assemble a response that matches the\nrequest.\n5.1.3\nMoving to GraphQL\nWhen A Cloud Guru began moving to GraphQL, AWS AppSync wasn’t yet released or\neven announced for that matter. The team had one option, which was to run GraphQL\nfrom a Lambda function using the Apollo GraphQL library (https://www.apollo\ngraphql.com). \n Initially, getting the Apollo GraphQL implementation to work in a Lambda func-\ntion presented a few interesting challenges. Apollo was originally designed for long-\nrunning processes on servers and containers. The team had to make a certain number\nof tweaks to optimize it for Lambda.\n The A Cloud Guru team began using GraphQL and a design pattern called Back-\nends for Frontends (BFF). The idea behind BFF is that each client has its own API or\nendpoint that services its specific needs (for example, there’s a dedicated endpoint\nfor mobile and another for the web). Each of the endpoints can query the appropri-\nate microservices to save or load data as needed. The client doesn’t need to know\nabout the different microservices. It only needs to know which endpoint to query.\nThis pattern solves the decoupling issue present in many systems. Figure 5.7 shows an\nexample of the BFF architecture and what the A Cloud Guru team is driving toward.\nSeparate endpoints for different\nclients like mobile or web\nAPI Gateway/\nGraphQL\nWeb app\n(GraphQL)\nMicroservice 1\n(Payments)\nMobile\n(GraphQL)\nInternal\ndashboards\n(GraphQL)\nDifferent endpoints can query different\nmicroservices and databases to get the\ndata needed for the given client.\nMicroservice 3\n(Reporting)\nMicroservice 2\n(Video)\nBFF\nFigure 5.7\nAn example of the \nBFF pattern that can be applied to \nmicroservices and multiple clients\n\n\n80\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nThroughout the chapter, we’ve called the Lambda function that contains the\nGraphQL JavaScript library a GraphQL endpoint. But it’s probably better to call it a\nBFF endpoint as we now understand this pattern. Here’s how the A Cloud Guru’s imple-\nmentation works at a high level (excluding a few details like service discovery):\nA request from the user reaches the API Gateway.\nThe API Gateway invokes a Lambda function with the Apollo GraphQL library.\nThis is the BFF endpoint we’ve discussed.\nThe Apollo GraphQL library queries the microservices it knows about (more on\nhow it knows about which microservices to target in the service discovery section).\nEach microservice has an endpoint that is an API Gateway with a Lambda func-\ntion (there is a /graphql endpoint in each microservice). \nThe Lambda function runs the GraphQL library with a number of thin schema\nresolvers. It queries the databases contained within the microservice and pro-\nduces a result, which is sent back to the BFF endpoint.\nThe BFF endpoint receives responses from the different microservices it que-\nried. Using schema stitching, it assembles the final response.\nThis final response is sent back to the client via the API Gateway.\nThe GraphQL Lambda function is aware of the multitude of microservices (more on\nthis in a moment) and is able to query those when it receives a client request. Figure\n5.8 shows a high-level overview of this architecture.\n5.1.4\nService discovery\nWith 43 microservices in the system, how does GraphQL know which services to query\nwhen a client request comes in? The A Cloud Guru team built an internal service-\ndiscovery service called Sputnik (note, this is an in-house, proprietary service that’s\n4. Responses are combined together\n    using schema stitching. \nAPI Gateway\nLambda\n(GraphQL)\nMicroservice 3\nMicroservice 2\nMicroservice 1\nGraphQL\n5. Final response is sent \n    back to the client.\n1. User makes a request.\n3. GraphQL queries the \nmicroservices it knows about.\n2. GraphQL function/\n    library is invoked.\nFigure 5.8\nThe GraphQL endpoint serves as the central point for clients that need data.\n",
      "page_number": 89
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 99-106)",
      "start_page": 99,
      "end_page": 106,
      "detection_method": "topic_boundary",
      "content": "81\nThe original architecture\nnot available publicly). Sputnik consists of a database with API/URI definitions and\ndatabase schemas. The microservices know when to update Sputnik. Additionally, the\nGraphQL Lambda function knows when to query Sputnik to get schemas for each\nmicroservice and figure out where to route requests. \nDEFINITION\nService discovery is a standard technique in microservices archi-\ntecture, which solves the problem of knowing what services are available, how\nto access them, and what their interface looks like.\nSputnik is made up of Lambda functions and DynamoDB tables that contain schemas\nand URIs of different microservices. It is really a microservice that facilitates the com-\nmunication of BFF with other microservices in the system. Figure 5.9 shows how Sput-\nnik helps the BFF endpoint know where to make a query.\nTIP\nAWS has a service called Cloud Map, which is AWS’ own service discovery\nproduct. It even has a tagline that simply says, “Service discovery for cloud\nresources.” If you are looking for something like Sputnik, check out Cloud\nMap. It may work for you. You can find Cloud Map at https://aws.amazon\n.com/cloud-map/. \nThe metadata about each microservice (URI and schema) is cached at the BFF endpoint\ntoo, negating the need for the function to query Sputnik on every request. However,\nSputnik can invalidate the cache and force the Lambda function to requery it again. \nMicroservices update \nSputnik whenever their \nschema or URI changes. \nAPI Gateway\nLambda\n(GraphQL)\nMicroservice 3\nMicroservice 2\nMicroservice 1\nService discovery\nFunction queries Sputnik \nto find out URIs of all \nmicroservices and their \nschemas. \nService \ndiscovery\n(Sputnik)\nFigure 5.9\nThe service discovery (Sputnik) mechanism for A Cloud Guru. There’s an AWS service \ncalled Cloud Map that you may want to check out if you are looking for something similar.\n\n\n82\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\n5.1.5\nSecurity in the BFF world\nA Cloud Guru practices security in depth with multiple layers of security built into the\nsystem. Let’s talk about the two main components: user authentication/authorization\nand BFF to microservice security. \n In the A Cloud Guru platform, students are authenticated using the Auth0 service\nthat generates a unique JWT token for each user. All requests to AWS are made with\nthat JWT token, which is validated using a custom authorizer at the API Gateway. If\nthe JWT token is valid, the request is allowed to continue to the BFF endpoint. If it’s\nnot, then a response is generated and sent back to the client telling it that it is unau-\nthorized. This is a simple mechanism, which was also used in the original REST design\nof the platform.\n The second interesting element is how to authenticate a request made by the BFF\nto the microservice. In this scenario, A Cloud Guru uses API keys to authenticate\nrequests. Each microservice has a unique API key that the BFF includes in its request\nin the header (using the X-API-Key parameter). Microservices check the included key\nand authorize requests if everything is OK.\n5.2\nRemnants of the legacy\nThe migration from REST to GraphQL took some time because the teams were care-\nful not to cause issues for users. An interesting side effect of this was the way the sys-\ntem looked midway through the re-architecture. The team implemented new\nmicroservices and a BFF, but the old Firebase database was still in use because it was\npowering some of the elements of the user interface on the A Cloud Guru website.\nFigure 5.10 shows how that mid-way architecture looked.\nFigure 5.10\nA high-level overview of the A Cloud Guru architecture as it was going through a transition\nMicroservices #A\nMicroservices #B\nAPI Gateway\nGraphQL endpoint\nDynamoDB\nDynamoDB\nCloudFront\nS3\nClient\nFirebase\nAlgolia\nThese are two non-AWS services \nat this stage in the architecture.\n\n\n83\nSummary\nAn interesting note about figure 5.10: you can see that Firebase is still used to drive\nsome of the client-facing user interface elements. To keep the data in Firebase up to\ndate, the team used DynamoDB event streams and Lambdas to ensure that happened.\nWhen a table is updated in any microservice, that change is pushed by DynamoDB to\nthe DynamoDB event stream, which in turn invokes a Lambda function. That Lambda\nfunction analyses the change and then updates Firebase (and any other services like\nAlgolia as needed). \n Now, Firebase becomes basically a materialized view that is used to drive some parts\nof the interface. It is never directly queried, but it is there for older components that\ndepend on it. This is one of the creative decisions made by the team as they transi-\ntioned from the old serverless architecture to the new one. They were able to use Fire-\nbase while they introduced DynamoDB and microservices and move everything\nacross. \n There’s also an important lesson here in migration. You can gradually implement a\nnew architecture while keeping the old one going by splitting things into smaller\npieces and moving them one by one.\nSummary\nTeams can work on a platform without affecting each other. Different teams are\nresponsible for different microservices, and they can work on those without\naffecting anyone else.\nThere has been a substantial improvement in performance for A Cloud Guru.\nFor example, a BFF pattern fetches only the data that’s needed (great for\nmobile) and needs only one roundtrip to make that happen. This is an optimi-\nzation on what was there previously.\nThe BFF pattern helps to support multiple client types and devices. These can\nbe different depending on the requirements of the client.\nThe team also had to do additional re-engineering to make Apollo work well\nwith Lambda. These days, it shouldn’t be much of a problem, but that’s the\npain when you are an early adopter.\nAs always, security is a number one concern. More microservices create a larger\nsurface area for attacks. It’s therefore critical that microservices and endpoints\nare secured. The use of machine keys to secure communications between back-\nend components is an example of one good practice you should know about.\n\n\n84\nYle: Architecture\n highlights, lessons learned\nYle is the national broadcaster for Finland and operates their own popular stream-\ning service called Yle Areena, which is used by millions of households. For a num-\nber of years now, Yle has used serverless technologies at scale in their architecture.\nThey use a combination of AWS Fargate (https://aws.amazon.com/fargate),\nLambda, and Kinesis to process more than 500 million user-interaction events per\nday. These events feed Yle’s machine learning (ML) algorithm and help them pro-\nvide better content recommendations, image personalization, smart notifications,\nand more.1\nThis chapter covers\nYle’s big data architecture \nScalability and resilience, lessons learned\n1 I want to take this opportunity to thank Anahit Pogosova for sharing details of this architecture and the les-\nsons she and her team learned along the way.\n\n\n85\nIngesting events at scale with Fargate\n6.1\nIngesting events at scale with Fargate\nTo provide better content recommendations, Yle needs to know which content the vis-\nitors interact with the most. Yle ingests user-interaction data from streaming services\nas well as mobile and TV apps via an HTTP API. The challenge with this API is that the\ntraffic can be spiky, such as during live sporting events. And sometimes events overlap\n(for example, when the election results coverage was on at the same time as a hockey\ngame, which is the most popular sport in Finland)!\n As mentioned, Yle’s API ingests more than 500 million user-interaction events per\nday with more than 600,000 requests per minute during peak time. Live sporting events\nor special events (such as the election results) can cause peak traffic to go even higher.\nThe maximum traffic throughput they have observed is 2,500,000 requests per minute.\n Because the traffic is so spiky, the Yle team decided to use Fargate instead of AWS’s\nAPI Gateway and Lambda. Fargate, also an AWS service, lets you run containers with-\nout having to worry about underlying virtual machines. It’s part of an emerging trend\nfor serverless containers, where you use containers as a utility service.\n6.1.1\nCost considerations\nIn general, AWS services that charge you based on up time tend to be orders of magni-\ntude cheaper when running at scale, compared with those that charge based on\nrequest count. With API Gateway and Lambda, you pay for individual API requests.\nFargate, on the other hand, charges a per-hour amount based on the vCPU, memory,\nand storage resources that your containers use. You incur costs for as long as the con-\ntainers run, even if they don’t serve any user traffic.\n Paying for up time can be inefficient for APIs that don’t receive a lot of requests.\nFor example, an API that receives a few thousand requests a day would cost signifi-\ncantly less using API Gateway and Lambda. This is especially true when you consider\nthat you need some redundancy to ensure that your API stays up and running even if\na container fails or if one of the AWS availability zones (AZs) hosting your containers\nexperiences an outage. However, for high throughput APIs like the Yle API, which\nhandles hundreds of millions of requests per day, running the API in Fargate can be\nmore economical than using API Gateway and Lambda.\n6.1.2\nPerformance considerations\nA more important consideration for the Yle team was that, given how spikey their traf-\nfic can be, they would likely run into throttling limits with API Gateway and Lambda.\nA Lambda function’s concurrency is the number of instances of that function that\nserve requests at a given time. This is known as the number of concurrent executions. \n Most AWS regions have a default limit of 1,000 concurrent executions across all\nyour Lambda functions in that region. This is a soft limit, however, and can be raised by\na support request. Even though Lambda does not impose a hard limit on the maximum\nnumber of concurrent executions, how quickly you reach the required number of con-\ncurrent executions is limited by two factors:\n\n\n86\nCHAPTER 6\nYle: Architecture highlights, lessons learned\nThe initial burst limit, which ranges from 500 to 3,000 depending on the\nregion.\nAfter the initial burst limit, your functions’ concurrencies can increase by 500\ninstances per minute. This continues until there are enough instances to serve\nall requests or until a concurrency limit is reached.\nAPI traffic is often measured in requests per second (or RPS). It’s worth noting that\nRPS is not equivalent to Lambda’s concurrent executions. For example, if an API\nrequest takes an average of 100 ms to process, then a single instance of a Lambda\nfunction can process up to 10 requests per second. If this API needs to handle 100\nRPS at peak, then you will likely need around 10 Lambda concurrent executions at\npeak to handle this throughput.\n If, however, an API’s throughput jumps from 100 RPS to 20,000 RPS in the span of\n30 seconds, then you will likely exhaust the initial burst limit and the subsequent scal-\ning limit of 500 instances per minute. Eventually Lambda would be able to scale\nenough instances of your API functions to handle this peak load, but in the mean-\ntime, many API requests would have been throttled.\n Another caveat to consider is that because live events are scheduled ahead of time,\nthe Yle team can use a broadcast schedule to prescale their infrastructure in advance.\nThere is no easy way to do this with Lambda except for using provisioned concurrency\n(https://amzn.to/3faBkCU). But you’d need to allocate provisioned concurrency to\nevery Lambda function that needs to be prescaled; that would consume the available\nconcurrencies in the region. \n When used broadly like this, it can significantly impact your ability to absorb fur-\nther spikes in traffic because there might not be enough concurrency left in the\nregion if most of it is taken up by provision concurrency. Because of these scaling lim-\nits, AWS API Gateway and Lambda are not a\ngood fit for APIs with extremely spiky traffic.\nIt’s the main reason why the Yle team opted to\nbuild their API with Fargate, and that was a sen-\nsible decision.\n6.2\nProcessing events in real-time\nOnce Yle’s API ingested the user-interaction\nevents, it published them to Amazon Kinesis\nData Stream in batches of 500 records at a time\nwith an Amazon Simple Queue Service (SQS)\nqueue as the dead-letter queue (DLQ). Figure\n6.1 illustrates this process.\n6.2.1\nKinesis Data Streams\nAmazon’s Kinesis Data Streams is a fully man-\naged and massively scalable service that lets you\nFargate\nKinesis\nSQS\nFigure 6.1\nHigh-level architecture of \nYle’s ingestion API, which assimilates \nmore than 500 million events per day at \na peak throughput of more than 600,000 \nevents per minute. The events are \nforwarded to Kinesis Data Stream in \nbatches of 500 records. If the Kinesis \ndata stream is unavailable, the events \nare sent to an SQS dead-letter queue \n(DLQ) to be reprocessed later.\n\n\n87\nProcessing events in real-time\nstream data and process it in real time. Data is available to the consumers of the\nstream in milliseconds and is stored in the stream for 24 hours, by default, but that\ncan be extended to a whole year, based on your configuration. (Keep in mind that\nextra charges apply when you extend the retention period for your stream.)\n Within a Kinesis stream, the basic unit of parallelism is a shard. When you send\ndata to a Kinesis stream, the data is sent to one of its shards based on the partition key\nyou send in the request. Each shard can ingest 1 MB of data per second or up to 1,000\nrecords per second and supports an egress throughput of up to 2 MB per second. The\nmore shards a stream has, the more throughput it can handle. \n There is no upper limit to the number of shards you can have in a stream so, theo-\nretically, you can scale a Kinesis stream indefinitely by adding more shards to it. But\nthere are cost implications that you have to consider when deciding how many shards\nyou will need for your workload.\n Kinesis charges based on two core dimensions: shard hours and PUT payload\nunits. One PUT payload unit equates one request to send a record with up to 25 KB to\na Kinesis stream. If you send a piece of data that is 45 KB in size, for example, then\nthat counts as two PUT payload units. It works the same way as Amazon’s DynamoDB’s\nread and write request units.\n A Kinesis shard costs $0.015 per hour and $0.014 per million PUT payload units.\nThere are also additional charges if you enable optional features such as extending\nthe data retention period. Some of these additional costs are also charged at a per\nhour rate, such as the cost for extended data retention and enhanced fan-out.\n Because of the hourly cost, it’s not economically efficient to over-provision the\nnumber of shards you’ll need. Given the amount of throughput each shard supports,\nyou don’t need many shards to support even a high throughput system like Yle’s data\ningestion pipeline.\n Based on Yle’s prime-time traffic of 600,000 requests per minute, if we assume the\ntraffic is uniformly distributed across 1 minute, then we arrive at 10,000 requests per\nsecond. And assuming that each event is less than 25 KB in size, then Yle needs about\n10 shards to accommodate this traffic pattern. However, as we discussed, their traffic is\nspiky and, because Kinesis doesn’t support autoscaling, the Yle team over-provisions\ntheir stream, running 40 shards all the time. This gives the team plenty of headroom\nto handle unexpected spikes and to minimize the risk of data loss.\n6.2.2\nSQS dead-letter queue (DLQ)\nBecause data is the blood supply for Yle’s ML algorithms, the team wants to ensure\nthat it’s not lost when the Kinesis service experiences an outage in Yle’s region. In the\nevent the Kinesis service is out of commission, the API sends the events to the SQS\nDLQ so they can be captured and reprocessed later.\n\n\n88\nCHAPTER 6\nYle: Architecture highlights, lessons learned\n6.2.3\nThe Router Lambda function\nTo process the constant stream of events, a Lambda function called Router subscribes\nto the Kinesis data stream. This function routes events to different Kinesis Firehose\nstreams that the other microservices use.\n To make storing and querying the data more efficient, the Yle team stores the\nevents in Apache Parquet format. To do this, they use Amazon Kinesis Data Firehose\n(to batch data into large files and deliver them to S3) with AWS Glue Data Catalog (to\nprovide the schema). Figure 6.2 shows this arrangement.\n6.2.4\nKinesis Data Firehose\nKinesis Data Firehose is another member of the Amazon Kinesis family of services. It is\na fully managed service to load streaming data to a destination. Kinesis Firehose can\nsend data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service (Amazon\nES), and any HTTP endpoint owned by you or by external service providers such as\nDatadog, New Relic, and Splunk.\n A Firehose stream allows you to load streaming data with zero lines of code. Unlike\nKinesis Data Streams, a Kinesis Firehose stream scales automatically, and you pay for\nonly the volume of data you ingest into the stream. The cost for ingesting data into\nKinesis Data Firehose starts at $0.029 per GB for the first 500 TB of data per month. It\ngets cheaper the more data you ingest.\n In addition to the automated scaling, a Firehose stream can batch the incoming\ndata, compress it and, optionally, transform it using Lambda functions. It can also\nconvert the input data from JSON to Apache Parquet or to Apache ORC formats\nbefore loading it into the destination.\n Like Kinesis Data Streams, it stores data in the stream for only up to 24 hours. You\ncan configure the batch size by the maximum number of records or for a certain period\nof time. For example, you can ask the Firehose stream to batch the data into 128 MB\nfiles or 5 minutes’ worth of data, whichever limit is reached first. It’s a convenient\nFargate\nKinesis\nSQS\nLambda\nRouter\nKinesis Firehose\nKinesis Firehose\nFigure 6.2\nThe Lambda Router function routes events to different Kinesis Firehose \nstreams so they can be aggregated and converted to Apache Parquet files.\n",
      "page_number": 99
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 107-117)",
      "start_page": 107,
      "end_page": 117,
      "detection_method": "topic_boundary",
      "content": "89\nProcessing events in real-time\nservice with no management overhead for scaling, and you don’t have to write any\ncustom code to transport data to the intended target.\n To convert the data from JSON format to Apache Parquet or Apache ORC, you\nneed to use the AWS Glue Data Catalog service. A Kinesis Firehose stream uses the\nschema captured in the Glue Data Catalog before sending it to a destination.\n The Yle team uses S3 as the data lake and the destination for the Kinesis Firehose\nstreams (figure 6.3). Once the data is delivered to S3, it is further processed and con-\nsumed by a number of microservices to perform several ML tasks such as demo-\ngraphic predictions.\n6.2.5\nKinesis Data Analytics\nTo personalize the icon image for videos, the Yle team uses the contextual bandits model,\nwhich is a form of an unsupervised ML model. They use the user-interaction events to\nreward the model so it can learn what the user likes. To do that, the team uses Kinesis\nData Analytics to filter and aggregate the data from the Firehose stream and deliver it\nto a Lambda function called Reward Router. This function then calls several Reward\nAPIs to reward the personalization models the Yle team maintains (figure 6.4).\n Kinesis Data Analytics lets you run queries against streaming data using SQL or\nJava and the Apache Flink framework. Using an SQL approach, you can join, filter,\nand aggregate data across several streams without writing any custom code or running\nKinesis\nLambda\nRouter\nKinesis Firehose\nKinesis Firehose\nS3\nFigure 6.3\nThe Router function \nroutes incoming events to a number \nof Kinesis Firehose streams, one for \neach type of event. The streams then \nbatch, transform, and convert the \ndata into Apache Parque format and \nwrite it to S3.\nKinesis Firehose\nS3\nKinesis Analytics\nLambda\nReward\nRouter\n...\nReward APIs\nFigure 6.4\nThe Yle team uses Kinesis Data \nAnalytics and Lambda to reward different \npersonalization models in real time.\n\n\n90\nCHAPTER 6\nYle: Architecture highlights, lessons learned\nany infrastructure. The Java approach, however, gives you the most control over how\nyour application runs and how it processes the data.\n You can output the result of your queries to Kinesis Data Stream, Kinesis Firehose,\nor a Lambda function. Having a Lambda function as a destination gives you a lot of\nflexibility, however. You can process the results further, forward the results to any-\nwhere you want, or both. In Yle’s case, they use the Reward Router function as the des-\ntination for the Kinesis Data Analytics application and reward the relevant\npersonalization models.\n6.2.6\nPutting it altogether\nTaking a step back, you can see in figure 6.5 what Yle’s data pipeline looks like from a\nhigh level. We have omitted some minor details, such as the fact that the Kinesis Fire-\nhose streams also use Lambda functions to transform and format the data and the fact\nthat this is just the start of the journey for many of these user events. Once the data is\nsaved into S3 in Apache Parquet format, many microservices ingest the data, process\nit, and use it to enrich their ML models.\nWhat I would like to highlight in this architecture is the prevalent use of Kinesis and\nits data analytics capabilities. This includes\nKinesis Data Streams for ingesting large amounts of user events.\nFargate\nKinesis\nSQS\nLambda\nRouter\nKinesis Firehose\nKinesis Firehose\nS3\nKinesis Analytics\nLambda\nReward\nRouter\n...\nReward APIs\nFigure 6.5\nYle’s data pipeline architecture. They use Fargate to run the ingestion API because of cost and \nperformance considerations and then process the ingested events in real time using Kinesis Data Streams, \nKinesis Firehose, and Lambda. The data is transformed, compressed, and converted to Apache Parquet format\nand stored in S3 as the data lake. At the same time, they also use Kinesis Data Analytics to perform real-time\naggregations and use Lambda to reward the relevant personalization ML models.\n\n\n91\nLessons learned\nKinesis Firehose Streams for batching, formatting, and outputting data into\nlarge compressed files that are more easily consumable by the downstream ML\nmodels.\nKinesis Data Analytics for running aggregations over live streams of data in real\ntime and using a Lambda function as a destination to reward personalization\nmodels.\n6.3\nLessons learned\nThe use of these Kinesis capabilities and how they are combined is a common sight in\ndata analytics applications. However, Yle is processing events at a much higher scale\nthan most! Operating at such high scale comes with unique challenges, and the Yle\nteam has learned some valuable lessons along the way, including those that follow.\n6.3.1\nKnow your service limits\nEvery service in AWS comes with service limits. These generally fall into three categories:\nResource limits—How many of X can you have in a region. For example, Kinesis\nData Streams has a default quota of 500 shards per region in us-east-1, us-west-1,\nand eu-west-1, and 200 shards per region in all other regions. Similarly, AWS\nIdentity and Access Management (IAM) has a default quota of 1,000 roles per\nregion.\nControl-plane API limits—How many requests per second you can send to a con-\ntrol plane API to manage your resources. For example, Kinesis Data Streams\nlimits you to five requests per second to the CreateStream API.\nData-plane API limits—How many requests per second you can send to a data\nplane API to act on your data. For example, Kinesis Data Streams limits you to\nfive GetRecords requests per second per shard.\nThese limits are published in the AWS Service Quotas console. In the console, you can\nview your current limits and whether you can raise the limit.\nSOFT VS. HARD LIMITS\nLimits that can be raised are considered soft limits, and those that can’t be raised are\nconsidered hard limits. You can ask for a soft limits raise via a support ticket, or you can\ndo it in the AWS Service Quotas console. But it’s worth keeping in mind that some-\ntimes there is a limit to how far you can raise those soft limits. For example, the num-\nber of IAM roles in a region is a soft limit, but you can raise that limit to only 5,000\nroles per region. If your approach relies on raising these soft limits indefinitely, then\nthere’s a good chance that you’re using the service in a way that it’s not designed for,\nand you might have to reconsider your approach.\n Keeping an eye on your usage levels and your current limits is something that\nevery AWS user should do but is especially important when you need to operate at\nscale and you run the risk of running into those limits. For the Yle team, one of the\nimportant lessons they learned is that you need to raise the limit on the number of\n\n\n92\nCHAPTER 6\nYle: Architecture highlights, lessons learned\nFargate tasks you can run and give yourself plenty of headroom because it can take a\nfew days for AWS to raise the limit in your account. \n At present, the default limit is 1,000 concurrent Fargate tasks per region. When\nthe Yle team started out, however, the default limit was only 100, and it took the team\nthree days to raise that limit to 200.\nPROJECT THROUGHPUT AT EVERY POINT ALONG THE PIPELINE\nTo understand which service limits affect your application, look at every service along\nthe way and build a projection of how throughput changes with user traffic. Take Yle’s\ncase: as the number of concurrent users goes up, there’s more traffic going through\nthe ingestion API running in Fargate. \nHow does this increase affect the throughput that needs to be processed by\nKinesis and, therefore, the number of shards that need to be provisioned? \nBased on the current BatchSize and ParallelizationFactor configurations,\nhow many concurrent Lambda executions would be required to process the\nevents at peak load? \nGiven that many concurrent Lambda executions, how many events would be\nsent to each Kinesis Firehose stream? \nDoes your current throughput limit for Kinesis Data Firehose support that\nmany events per second?\nALWAYS LOAD TEST, DON’T ASSUME\nEvery service in the pipeline can become a bottleneck, and the best way to know that\nyour application can handle the desired throughput is to load test it. The services you\nbuild your application on might be scalable, but it doesn’t mean that your application\nis, not without the proper adjustment to its service limits. \n If your target is to handle 100,000 concurrent users, then load test it to at least\n200,000 concurrent users. Who knows, maybe your application will be successful!\nThat’s what you’re hoping for, right? Even if your application already comfortably\nhandles 50,000 concurrent users, load test it to 200,000 concurrent users anyway. You\ncan’t assume the system is infinitely scalable and that its performance characteristics\nare perfectly consistent as throughput goes up. Don’t assume anything; find out.\nSOME LIMITS HAVE A BIGGER BLAST RADIUS THAN OTHERS\nIt’s also worth mentioning that some service limits have a bigger blast radius than oth-\ners. Lambda’s regional concurrency limit is a great example of this.\n Whereas exhausting the write throughput limit on a Kinesis shard affects only\nputRecord operations against that shard, the impact is localized to a single shard in a\nsingle Kinesis stream and will not affect your application in a big way. On the other\nhand, exhausting the Lambda concurrent executions limit can have a wide-reaching\nimpact on your application because you’re likely using Lambda functions to handle a\nvariety of different workloads: APIs, real-time event processing, transforming data for\nKinesis Firehose, and so on. \n\n\n93\nLessons learned\n This is why you need to pay even more attention to those service limits that have a\nbig blast radius. In the case of Lambda, you can also use the ReservedConcurrency\nconfiguration to restrain the maximum number of concurrent executions a function\ncan have in cases where it’s appropriate and necessary.\nMIND CLOUDWATCH’S METRIC GRANULARITY\nYou should monitor your usage level and be proactive about raising service limits. One\nway to do that is by setting up CloudWatch alarms against the relevant metrics. One\ncaveat to keep in mind here is that CloudWatch often reports usage metrics at a per-\nminute granularity but the limits are per second, which applies to both Kinesis Data\nStreams and DynamoDB’s throughput metrics. In these cases, when you set up those\nCloudWatch alarms, make sure that you set up the thresholds correctly. For example,\nif the per-second throughput limit is 1, then the corresponding per-minute threshold\nshould be 60.\n6.3.2\nBuild with failure in mind\nNotice that in figure 6.1, there is a SQS DLQ? It’s there as a backup for when there is\na problem with the Kinesis Data Streams service. Kinesis Data Streams is a robust and\nhighly scalable service, but it’s not infallible.\nEVERYTHING FAILS, ALL THE TIME\nAWS CTO, Werner Vogel, famously said, “Everything fails, all the time.” It’s a fact of life\nthat even the most reliable and robust service can have a hiccup from time to time.\nRemember when S3 was down (https://aws.amazon.com/message/41926) for a few\nhours in 2017? Or that time when Kinesis Data Streams had an outage and affected\nCloudWatch and EventBridge as well (https://aws.amazon.com/message/11201)? Or\nwhen Gmail, Google Drive, and YouTube went down (http://mng.bz/ExlO)?\n At the machine level, individual disk drives or CPU cores or memory chips con-\nstantly fail and are replaced. Cloud providers such as AWS and Google have invested\nheavily into their physical infrastructure as well as their software infrastructure to\nensure that such failures do not affect their customers. In fact, by using serverless\ntechnologies such as API Gateway, Lambda, and DynamoDB, your application is\nalready protected from data center-wide failures because your code and data are\nstored in multiple availability zones (data centers) within a given AWS region. How-\never, there are occasional region-wide disruptions that affect one or more services in\nan entire AWS region, such as the S3 and Kinesis outages mentioned previously.\n What this means is that you need to build your application with failure in mind\nand have a plan B (and maybe even a plan C, D, and E) in case your primary service\nhas a bad day at the office. DLQs are a good way to capture traffic that can’t be deliv-\nered to the primary target when first asked. Many AWS services now offer DLQ sup-\nport out of the box. For example, SNS, EventBridge, and SQS all support DLQs\nnatively in case the events they capture cannot be delivered to the intended target\nafter retries. If you process events from a Kinesis Data Stream with a Lambda function,\nthen you can also use the OnFailure configuration to specify a DQL. \n\n\n94\nCHAPTER 6\nYle: Architecture highlights, lessons learned\n The more throughput your system has to process, the more failures you will\nencounter, and the more important these DLQs become. Remember, even a one-in-a-\nmillion event would occur five times a minute if you have to process 5,000,000\nrequests a minute!\nPAY ATTENTION TO RETRY CONFIGURATIONS\nAs you introduce more moving parts into your architecture and process more\nthroughput, you should also pay more attention to your timeout and retry configura-\ntions. There are two problems that often plague applications that operate at scale:\nThundering herd—A large number of processes waiting for an event are awaken\nat the same time, but there aren’t enough resources to handle the requests\nfrom all these newly awaken processes. This creates a lot of resource conten-\ntion, potentially causing the system to grind to a halt or fail over.\nRetry storm—An anti-pattern in client-server communications. When a server\nbecomes unhealthy, the client retries aggressively, which multiplies the volume\nof requests to the remaining healthy servers and, in turn, causes them to time-\nout or fail. This triggers even more retries and exacerbates the problem even\nfurther.\nRetries are a simple and effective way to handle most intermittent problems, but set-\nting these needs to be done with care. A good practice is to use exponential backoff\nbetween retry attempts and the circuit breaker pattern to mitigate the risk of retry\nstorms (https://martinfowler.com/bliki/CircuitBreaker.html).\n6.3.3\nBatching is good for cost and efficiency\nCost is one of those things that developers often don’t think about when they’re mak-\ning architectural decisions, but this can come back and bite them in a big way later.\nThis is especially true when you need to operate at scale and process large volumes of\nevents. As we discussed in section 6.1.1, one of the reasons why the Yle team decided\nto use Fargate for ingesting user-interactions events instead of API Gateway and\nLambda was cost and efficiency. \n In general, AWS services that charge you based on up time tend to be orders of\nmagnitude cheaper when running at scale, compared with those that charge based on\nrequest count. And the bigger the scale, the more you need to batch events for cost\nand efficiency. After all, processing 1,000 events with a single Lambda invocation is far\ncheaper and more efficient than processing those with 1,000 Lambda invocations. \n Processing events in a batch also reduces the number of concurrent Lambda exe-\ncutions you need to run and minimizes the risk of exhausting the regional concurrent\nexecutions limit. However, with batch processing comes the potential for partial failures.\n If you process one event at a time and that event fails enough times, then you put it\ninto the DLQ and move on to the next event. But when you process 1,000 events in a\nsingle invocation and one event fails, what do you do about the other 999 events? Do\nyou throw an error and let the invocation be retried, potentially reprocessing the 999\n\n\n95\nSummary\nsuccessful events? Do you put the failed event into a DLQ and process it later? These\nare the sort of questions that you have to answer.\n6.3.4\nCost estimation is tricky\nIf you don’t pay attention to cost, then it can pile up quickly and catch you by surprise.\nBut trying to accurately predict your cost ahead of time is also difficult; there are a lot\nof factors that can affect your cost in production. For example, looking at the architec-\nture diagram in figure 6.5, you might be focusing on the cost of Fargate, Lambda, and\nthe Kinesis family of services. There are also other peripheral services to consider,\nsuch as the cost for CloudWatch, X-Ray, and data transfer costs.\n The cost of Lambda is usually a small part of the overall cost of a serverless applica-\ntion. In fact, in most production systems, the cost of Lambda often pales in compari-\nson with the cost of CloudWatch metrics, logs, and alarms.\nSummary\nYle’s ingestion API processes more than 500 million events per day and more\nthan 600,000 events per minute at peak times. The traffic is spiky and heavily\ninfluenced by real-world events such as a live hockey match or the election\nresults.\nThe Yle team uses Fargate for the ingestion API because of cost and perfor-\nmance considerations.\nIn general, AWS services that charge you based on up time are significantly\ncheaper to use at scale compared to those services that charge you based on\nusage (number of requests, volume of data processed, etc.).\nThe Yle team uses Kinesis Data Stream, Kinesis Data Firehose, and Lambda to\nprocess, transform, and convert the ingested events to Apache Parquet format.\nThe ingested data is stored in S3 as the data lake.\nThe Yle team uses Kinesis Data Analytics to perform real-time aggregation on\nthe ingested events.\nThe aggregated events reward the relevant personalization ML models.\n \n \n \n \n \n \n \n \n \n \n \n \n\n\nPart 3\nPracticum\nIt’s time to take a look at three interesting problems and discuss how we\nwould tackle them using serverless architectures. We will not be providing\nsource code, but we will show sample architectures and discuss how to go about\ndesigning three different and unique systems. Let’s sink our teeth into these\ndelicious serverless architectures.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n99\nBuilding a scheduling\n service for ad hoc tasks\nWith serverless technologies, you can build scalable and resilient applications\nquickly by offloading infrastructure responsibilities to AWS. Doing so allows you to\nfocus on the needs of your customers and your business. Ideally, all the code you\nwrite is directly attributed to features that differentiate your business and add value\nfor your customers.\n What this means in practice is that you use many managed services instead of\nbuilding and running your own. For example, instead of running a cluster of\nRabbitMQ servers on EC2, you use Amazon Simple Queue Service (SQS). Through-\nout the course of this book, you have also read about other AWS services such as\nDynamoDB and Step Functions.\nThis chapter covers\nApproaching architectural decisions when faced \nwith a novel problem\nDefining nonfunctional requirements\nChoosing the right AWS service to satisfy \nnonfunctional requirements\nCombining different AWS services \n\n\n100\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n Therefore, an important skill is to be able to analyze the nonfunctional require-\nments of a system and choose the correct AWS service to work with. But the AWS eco-\nsystem is enormous and consists of a huge number of different services. Many of these\nservices overlap in their use cases but have different operational constraints and scal-\ning characteristics. For example, to add a queue between two Lambda functions to\ndecouple them, you can use any of the following services:\nAmazon SQS\nAmazon Simple Notification Service (SNS)\nAmazon Kinesis Data Streams\nAmazon DynamoDB Streams\nAmazon EventBridge\nAWS IOT Core\nThese services have different characteristics when it comes to their scaling behavior,\ncost, service limits, and how they integrate with Lambda. Depending on your require-\nments, some might be a better fit for you than others.\n Although AWS gives you a lot of different services to architect your system, it doesn’t\noffer any guidance or opinion on when to use which. As a developer or architect working\nwith AWS, one of the most challenging tasks is figuring this out for yourself.\n This chapter shines a light on the problem by taking you through the design pro-\ncess for a scheduling service for ad hoc tasks. It’s a common need for applications,\nand AWS does not yet offer a managed service to solve this problem. The closest thing\nin AWS is the scheduled events in EventBridge, but scheduling repeated tasks (e.g., do\nX every Y seconds) is different than scheduling ad hoc tasks (e.g., do X at 2021-08-\n30T23:59:59Z, do Y at 2021-08-20T08:05:00Z).\n The functional requirement for such a scheduling service is simple: you schedule\nan ad hoc task to be run at a specified date and time (for example, “Remind me to call\nmum on Monday, at 9:00”). What’s interesting about this is that it has to deal with dif-\nferent nonfunctional requirements depending on the application (for example, “It\nneeds to handle a million open tasks that are scheduled but not yet run”).\n For the rest of this chapter, you will see five different solutions for this scheduling\nservice using different AWS services and learn how to evaluate them. But first, let’s\ndefine the nonfunctional requirements that we will evaluate the solutions against.\nHere is the plan for this chapter:\nDefine nonfunctional requirements. The four nonfunctional requirements we will\nconsider are precision, scalability (number of open tasks), scalability (hotspots),\nand cost. All the following solutions will be evaluated against these requirements:\n– Cron with EventBridge—A simple solution using cron jobs to find open tasks\nand run them.\n– DynamoDB TTL—A creative use of DynamoDB’s time-to-live (TTL) mecha-\nnism to trigger and run the scheduled ad hoc tasks.\n– Step Functions—A solution that uses Step Function’s Wait state to schedule\nand run tasks.\n\n\n101\nDefining nonfunctional requirements\n– SQS—A solution that uses SQS’s DelaySeconds and VisibilityTimeout set-\ntings to hide tasks until their scheduled execution time.\n– Combining DynamoDB TTL and SQS—A solution that combines DynamoDB\nTTL with SQS to compensate for each other’s shortcomings.\nChoose the right solution for your application. Different applications have different\nneeds, and some nonfunctional requirements may be more important than oth-\ners. In this section, you will see three different applications, understand their\nneeds, and pick the most appropriate solution for them.\n7.1\nDefining nonfunctional requirements\nThe ad hoc scheduling service is an interesting problem that often shows up in differ-\nent contexts and has different nonfunctional requirements. For example, a dating\napp may require ad hoc tasks to remind users a date is coming up. A multiplayer game\nmay need to schedule ad hoc tasks to start or stop a tournament. A news site might use\nad hoc tasks to cancel expired subscriptions.\n User behaviors and traffic patterns differ between these contexts, which in turn\ncreate different nonfunctional requirements the service needs to meet. It’s important\nfor you to define these requirements up front to prevent unconscious biases (such as a\nconfirmation bias) from creeping in. \n Too often, we subconsciously put more weight behind characteristics that align\nwith our solution, even if they aren’t as important to our application. Defining\nrequirements up front helps us maintain our objectivity. For a service that allows you\nto schedule ad hoc tasks to run at a specific time, the following lists some nonfunc-\ntional requirements you need to consider:\nPrecision—How close to the scheduled time is the task run?\nScalability (number of open tasks)—Can the service support millions of tasks that\nare scheduled but not yet processed?\nScalability (hotspots)—Can the service run millions of tasks at the same time?\nCost\nThroughout this chapter, you will evaluate five different solutions against this set of\nnonfunctional requirements. And remember, there are no wrong answers! The goal\nof this chapter is to help you hone the skill of thinking through solutions and evaluat-\ning them. We’ll spend the rest of the chapter looking at these different solutions. Each\nprovides a different approach and utilizes different AWS services. However, every solu-\ntion uses only serverless components, and there is no infrastructure to manage. The\nfive solutions include\nA cron job with EventBridge\nDynamoDB Time to Live (TTL)\nStep Functions\nSQS\nCombining DynamoDB TTL with SQS\n",
      "page_number": 107
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 118-128)",
      "start_page": 118,
      "end_page": 128,
      "detection_method": "topic_boundary",
      "content": "102\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nAfter each solution, we’ll ask you to score the solution against the aforementioned\nnonfunctional requirements. You can compare your scores against ours and see the\nrationale for our scores. Let’s start with the solution for a cron job with EventBridge.\n7.2\nCron job with EventBridge\nThis solution uses a cron job in EventBridge to invoke a Lambda function every cou-\nple of minutes (figure 7.1). With this solution, you will need the following:\nA database (such as DynamoDB) to store all the scheduled tasks, including\nwhen they should run\nAn EventBridge schedule that runs every X minutes\nA Lambda function that reads overdue tasks from the database and runs them\nThere are a few things to note about this solution:\nThe lowest granularity for an EventBridge schedule is 1 minute. Assuming the\nservice is able to keep up with the rate of scheduled tasks that need to run, the\nprecision of this solution is within 1 minute.\nThe Lambda function can run for up to 15 minutes. If the Lambda function\nfetches more scheduled tasks than it can process in 1 minute, then it can keep\nrunning until it completes the batch. In the meantime, the cron job can start\nanother concurrent execution of this function. Therefore, you need to take\ncare to avoid the same scheduled tasks being fetched and run twice.\nThe precision of individual tasks within the batch can vary, depending on their\nrelative position in the batch and when they are actually processed. In the case\nof a large batch that cannot be processed within 1 minute, the precision for\nsome tasks may be longer than 1 minute (figure 7.2).\nIt’s possible to increase the throughput of this solution by adding a Lambda\nfunction as the target multiple times (figure 7.3).\nEventBridge\nLambda\nDynamoDB\nInvokes Lambda\nfunction every X minutes.\nDefines cron job as a schedule\n(e.g., every X minutes).\nQueries DynamoDB for\noverdue tasks to execute.\nAll scheduled tasks are\nstored in DynamoDB.\nFigure 7.1\nHigh-level architecture showing an EventBridge cron job with \nLambda to run ad hoc scheduled tasks.\n\n\n103\nCron job with EventBridge\nTime\n23:00 UTC\n23:01 UTC\n23:02 UTC\nTask 1’s scheduled time\nTask 10001’s scheduled time\nCron job runs\nTask 1 executed\nTask 10001 executed\nTask 1’s precision (~1 min)\nTask 10001’s precision (> 1 min)\nFigure 7.2\nThe precision of individual tasks inside a batch can vary greatly \ndepending on their position inside the batch.\nThe same function can be configured as a target more than once. \nEach time the cron job runs, the function would be invoked \nmultiple times, one for every time it's configured as a target.\nFigure 7.3\nYou can add the same Lambda function as a target for an EventBridge rule multiple times.\n\n\n104\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nBecause EventBridge has a limit of five targets per rule, you can use this tech-\nnique to increase the throughput fivefold. This means every time the cron job\nruns, it creates five concurrent executions of this Lambda function. To avoid\nthem all picking up and running the same tasks, you can configure different\ninputs for each target as figure 7.4 shows.\n7.2.1\nYour scores\nWhat do you think of this solution? How would you rate it on a scale of 1 (worst) to 10\n(best) against each of the nonfunctional requirements? Write down your scores in the\nempty spaces in the tables provided for this (see table 7.1 as an example). And\nremember, there are no right or wrong answers. Just use your best judgement based\non the information available.\nTable 7.1\nYour solution scores for a cron job\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nYou can configure a different input for \neach target, then each concurrent \nexecution of the Lambda function \npicks up a different segment of the \nopen tasks that need to be run.\nFigure 7.4\nYou can configure a \ndifferent input for each target to \nhave them fetch different subsets \nof scheduled tasks. Then the tasks \nare not processed multiple times.\n\n\n105\nCron job with EventBridge\n7.2.2\nOur scores\nThe biggest advantage of this solution is that it’s really simple to implement. The com-\nplexity of a solution is an important consideration in real-world projects because we’re\nalways bound by resource and time constraints. However, for the purpose of this book,\nwe will ignore these real-world constraints and only consider the nonfunctional require-\nments outlined in section 7.1. With that said, here are our scores for this solution (table\n7.2). We’ll then explain our reasons for these scores in the following subsections.\nPRECISION\nWe gave this solution a 6 for precision because EventBridge cron jobs can run at most\nonce per minute. That’s the best precision we can hope for with this solution. Further-\nmore, this solution is also constrained by the number of tasks that can be processed in\neach iteration. When there are too many tasks that need to be run simultaneously,\nthey can stack up and cause delays. These delays are a symptom of the biggest chal-\nlenge with this solution—dealing with hotspots. More on that next.\nSCALABILITY (NUMBER OF OPEN TASKS)\nProvided that the open tasks do not cluster together (hotspots), this solution would\nhave no problem scaling to millions and millions of open tasks. Each time the\nLambda function runs, it only cares about the tasks that are now overdue. Because of\nthis, we gave this solution a perfect 10 for scalability (number of open tasks).\nSCALABILITY (HOTSPOTS)\nWe gave this solution a lowly 2 for this criteria because a cron job doesn’t handle\nhotspots well at all. When there are more tasks than the Lambda function can handle\nin one invocation, this solution runs into all kinds of trouble and forces us into diffi-\ncult trade-offs.\n For example, do we allow the function to run for more than 1 minute? If we don’t,\nthen the function would time out, and there’s a strong possibility that some tasks\nmight be processed but not marked as so because the invocation was interrupted mid-\nway through. We need to either make sure the scheduled tasks are idempotent or we\nhave to choose between:\nTable 7.2\nOur solution scores for a cron job with EventBridge\nScore\nPrecision\n6\nScalability (number of open tasks)\n10\nScalability (hotspots)\n2\nCost\n7\n\n\n106\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nExecuting some tasks twice if we mark them as processed in the database after\nsuccessfully processing.\nNot executing some tasks at all if we mark them as processed in the database\nbefore we finish processing them.\nEmploying a mechanism such as the Saga pattern (http://mng.bz/AOEp) for\nmanaging the transaction and reliably updating the database record after the\ntask is successfully processed. (This can add a lot of complexity and cost to the\nsolution.)\nOn the other hand, if we allow the function to run for more than 1 minute, then we\nare less likely to experience this problem until we see a large enough hotspot that the\nLambda function can’t process in 15 minutes! Also, now there can be more than one\nconcurrent execution of this function running at the same time. To avoid the same\ntask being run more than once, we can set the function’s Reserved Concurrency set-\nting to 1. This ensures that at any moment, only one concurrent execution of the\nLambda function is running (see figure 7.5). However, this severely limits the poten-\ntial throughput of the system.\n Imagine 1,000,000 tasks that need to be run at 00:00 UTC, but the Lambda func-\ntion can process only 10,000 tasks per minute. If we do nothing, then the function\nwould timeout, be retried, and would take at least 100 invocations to finish all the\n00:00\n00:01\n00:02\n00:03\n00:04\n00:05\ntime\nmissed\nmissed\nmissed\nOn the next iteration of the cron \njob, it will try to invoke the function \nagain. These invocations would be \nthrottled by Lambda, and the cron \njob would miss those iterations.\nWhen Reserved Concurrency is set \nto 1, only one concurrent execution \nof the function is allowed to run at \nthe same time.\nTasks that are scheduled to execute \nat 00:01, 00:02, and 00:03 would \nnot have been picked up by the \ninvocation that started at 00:00 \nand instead picked up here.\nFigure 7.5\nIf we limit Reserve Concurrency to 1, then there will be only one concurrent execution of the \nLambda function running at any moment. This means some cron job cycles will be skipped.\n\n\n107\nCron job with EventBridge\ntasks. In the meantime, other tasks are also delayed, further exasperating the impact\non user experience. This is the Achille’s heel of this solution. But we can tweak the\nsolution to increase its throughput and help it cope with hotspots better. More on this\nlater.\nCOST\nWith EventBridge, cron jobs are free, but we have to pay for the Lambda invocations\neven when there are no tasks to run. You can minimize the Lambda cost if you use a mod-\nerate memory size for the cron job. After all, it’s not doing anything CPU-intensive and\nshouldn’t need a lot of memory (and therefore CPU).\n In our scenario, the main cost for this solution is the DynamoDB read and write\nrequests. For every task, you need one write request (when scheduling the task) and\none read request (when the cron job retrieves it). This access pattern makes it a good\nfit for DynamoDB’s on-demand pricing and allows the cost of the solution to grow lin-\nearly with its scale. At $1.25 per million write units and $0.25 per million read units,\nthe cost per million scheduled tasks can be as low as $1.50. That’s just the DynamoDB\ncost, and even that depends on the size of the items you need to store for each task as\nDynamoDB read/write units are calculated based on payload size. You also have to fac-\ntor in the Lambda costs too, which also depend on a number of factors such as mem-\nory size and execution duration.\n Nonetheless, this is still a cost-effective solution, even when you scale to millions of\nscheduled tasks per day. And, hence, why we gave it a score of 7. Overall, this is a good\nsolution for applications that don’t have to deal with hotspots, and it is also easy to\nimplement. As we mentioned earlier, we can also tweak the architecture slightly to\naddress its problem with hotspots.\n7.2.3\nTweaking the solution  \nEarlier, we mentioned that we can increase the throughput of this solution by allowing\nmultiple concurrent executions of the Lambda to run in parallel. We can do this by\nduplicating the Lambda function target in the EventBridge rule. Because there’s a limit\nof five targets per EventBridge rule, we can only hope for a fivefold increase at best.\nBeyond that, we can also duplicate the EventBridge rule itself as many times as we need.\n But even with these tricks, Lambda’s 15 minutes execution time limit is still loom-\ning over our head. We also have to shard the reads so that the concurrent executions\ndon’t process the same tasks. Doing that, we also incur higher operational cost and\ncomplexity as well. There are more resources to configure and manage, and there are\nmore Lambda invocations and database reads, even though most of the time they’re\nnot necessary. Essentially, we have “provisioned” (for lack of a better term) our appli-\ncation for peak throughput all the time.\n Increasing throughput this way is ineffective. A much better alternative is to fan-out\nthe processing logic based on the number of tasks that need to run. Lambda’s burst\ncapacity limit allows up to 3,000 concurrent executions to be created instantly (see\nhttps://amzn.to/2BxRuVG). This allows for a huge potential for parallel processing\n\n\n108\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\neven if we use just a fraction of it. For this to work, we need to move the business logic\nto fetch and run tasks into another Lambda function. From here, we can invoke as\nmany instances of this new function as we deem necessary when faced with a large batch\nof tasks (figure 7.6 shows this approach).\nOnce we know the number of tasks that needs to run, we can calculate the number of\nconcurrent executions we need. To alleviate the time pressure and minimize the dan-\nger of timeouts, we can add some headroom into our calculation.\n For example, if the throughput for the processing function is 10,000 tasks per min-\nute, then we can start a new concurrent execution for every 5,000 tasks. If there are\n1,000,000 tasks, then we need 200 concurrent executions. This is well below the burst\ncapacity limit of 3,000 concurrent executions in the region. \nCount\nFetch tasks\nInvoke\n(async)\nSQS\nDLQ\nSet InvocationType to Event to\nmake the invocation asynchronous.\ncron-runner\ntask-runner\nTo prevent data loss, use a dead-\nletter queue (DLQ) to capture\nevents that can’t be processed\ndue to persistent failures.\nInstead of a DLQ, you can also configure\nan On-Failure destination. The advantage\nof using Lambda Destinations instead of\nDLQ is that it captures the invocation error\ntoo, not just the invocation event.\nInstead of fetching and running the tasks,\nthe Lambda function triggered by the cron\njob now only gets a count of the number\nof tasks and invokes another Lambda\nfunction (the task-runner function).\nThe number of tasks in a given minute slot\nshould be recorded as an atomic counter.\nThis avoids expensive SCAN or QUERY \nrequests, where you would pay for every\nitem and have to page through the results.\nEventBridge\nLambda\nDynamoDB\nLambda\nFigure 7.6\nAn alternative architecture as a solution to our cron job. The solution fans out the \nprocessing logic to another function.\nExercise: Score the modified solution\nConsider how the proposed changes would affect the nonfunctional requirements of\nprecision, scalability (number of open tasks), scalability (hotspots), and costs. How\nwould you score this modified solution?\n\n\n109\nDynamoDB TTL\n7.2.4\nFinal thoughts\nCron jobs can be a simple and yet effective solution. As you saw, with some small tweaks\nit can also be scaled to support even large hotspots. However, it tends to push a lot of\nthe load onto the database. In the aforementioned scenario of 1,000,000 tasks that need\nto be run in a single minute, it would require 1,000,000 reads from DynamoDB. Luckily\nfor us, DynamoDB can handle this level of traffic, although we need to be careful with\nthe throughput limits that are in place. For example, DynamoDB has a default limit of\n40,000 read units per table for on-demand tables (https://amzn.to/3eH0THZ).\n What if there’s a way to implement the scheduling service without having to read\nfrom the DynamoDB table at all? It turns out we can do that by taking advantage of\nDynamoDB’s time-to-live (TTL) feature (https://amzn.to/2NRgARU).\n7.3\nDynamoDB TTL\nDynamoDB lets you specify a TTL value on items, and it deletes the items after the\nTTL has passed. This is a fully managed process, so you don’t have to do anything\nyourself besides specifying a TTL value for each item.\n You can use the TTL value to schedule a task that needs to run at a specific time.\nWhen the item is deleted from the table, a REMOVE event is published to the corre-\nsponding DynamoDB stream. You can subscribe a Lambda function to this stream and\nrun the task when it’s removed from the table (figure 7.7).\nExercise: Other alternatives\nWhile keeping to the same general approach of using cron jobs, are there any modi-\nfications to the basic design that can compensate for its shortcomings in precision\nand scaling for hotspots?\nLambda\nDynamoDB\nDynamoDB Streams\nLambda\nScheduler\nscheduled_tasks\nREMOVE events\nExecute\nThis table holds all the tasks that have\nbeen scheduled. The scheduled time for\nthe tasks are used as their TTL, then \nwhen their schedule time is up, they will \nbe deleted by DynamoDB TTL.\nThis function writes the scheduled task\ninto the scheduled_tasks table with the\nTTL set to the scheduled execution time.\nWhen an item is deleted by DynamoDB\nTTL, a corresponding REMOVE event\nwill be recorded in the table’s stream.\nThe Execute function listens\nfor the REMOVE event and\nruns the corresponding task.\nFigure 7.7\nHigh-level architecture using DynamoDB TTL to run ad hoc scheduled tasks.\n\n\n110\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nThere are a couple of things to keep in mind about this solution. The first, and most\nimportant, is that DynamoDB TTL doesn’t offer any guarantee on how quickly it deletes\nexpired items from the table. In fact, the official documentation (https://amzn.to/\n2NRgARU) only goes as far as to say, “TTL typically deletes expired items within 48 hours\nof expiration” (see figure 7.8). In practice, the actual timing is usually not as bleak.\nBased on empirical data that we collected, items are usually deleted within 30 minutes\nof expiration. But as figure 7.8 shows, it can vary greatly depending on the size and activ-\nity level of the table.\nThe second thing to consider is that the throughput of the DynamoDB stream is con-\nstrained by the number of shards in the stream. The number of shards is, in turn, deter-\nmined by the number of partitions in the DynamoDB table. However, there’s no way for\nyou to directly control the number of partitions. It’s entirely managed by DynamoDB,\nbased on the number of items in the table and its read and write throughputs.\n We know we’re throwing a lot of information at you about DynamoDB, including\nsome of its internal mechanics such as how it partitions data. Don’t worry if these are\nall new to you, you can learn a lot about how DynamoDB works under the hood by\nwatching this session from AWS re:invent 2018: https://www.youtube.com/watch?v=\nyvBR71D0nAQ.\n7.3.1\nYour scores\nWhat do you think of this solution? How would you rate it on a scale of 1 to 10 for\neach of the nonfunctional requirements? As before, write your scores in the empty\nspaces in table 7.3.\nTable 7.3\nYour scores for DynamoDB TTL\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nFigure 7.8\nDynamoDB TLL’s notification regarding its ability to delete expired items in tables\n\n\n111\nDynamoDB TTL\n7.3.2\nOur scores\nThe biggest problem with this solution is that DynamoDB TTL does not delete the\nscheduled items reliably. This limitation means it’s not suitable for any application\nthat is remotely time sensitive. With that said, here are our scores in table 7.4. Again,\nwe present how we arrived at these scores in the following subsections.\nPRECISION\nScheduled tasks would be run within 48\nhours of their scheduled time. A score of 1\nmight be considered a flattering score here.\nSCALABILITY (NUMBER OF OPEN TASKS)\nWe gave this solution a perfect 10 because\nthe number of open tasks equals the num-\nber of items in the scheduled_tasks table.\nBecause DynamoDB has no limit on the\ntotal number of items you can have in a\ntable, this solution can scale to millions of\nopen tasks. Unlike relational databases,\nwhose performance can degrade quickly\nas the database gets bigger, DynamoDB\noffers consistent and fast performance no\nmatter how big it gets. Figure 7.9 provides\na testimony to its performance.\nSCALABILITY (HOTSPOTS)\nWe gave this solution a 6 because it can still\nface throughput-related problems because\nit’s constrained by the throughput of the\nDynamoDB stream. But the tasks would be\nsimply queued up in the stream and would\nrun slightly later than scheduled.\n Let’s drill into this throughput constraint some more as that is useful for you to\nunderstand. As mentioned previously, the number of shards in the DynamoDB stream\nis managed by DynamoDB. For every shard the Lambda service would have a dedicated\nTable 7.4\nOur scores for DynamoDB TTL\nScore\nPrecision\n1\nScalability (number of open tasks)\n10\nScalability (hotspots)\n6\nCost\n10\nFigure 7.9\nA satisfied customer’s statement \nregarding DynamoDB’s scalability and number \nof opened tasks\n\n\n112\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nconcurrent execution of the execute function. You can read more about how Lambda\nworks with DynamoDB and Kinesis streams in the official documentation at\nhttps://amzn.to/2ZIu3Cx.\n When a large number of tasks are deleted from DynamoDB at the same time, the\nREMOVE events are queued in the DynamoDB stream for the execute function to\nprocess. These events stay in the stream for up to 24 hours. As long as the execute\nfunction is able to eventually catch up, then we won’t lose any data. \n Although there is no scalability concern with hotspots per se, we do need to con-\nsider the factors that affect the throughput of this solution. Ultimately, these through-\nput limitations will affect the precision of this solution:\nHow quickly the hotspots are processed depends on how quickly DynamoDB TTL deletes\nthose items. DynamoDB TTL deletes items in batches, and we have no control\nover how often it runs and how many items are deleted in each batch.\nHow quickly the execute function processes all the tasks in a hotspot is constrained by\nhow many instances of it runs in parallel. Unfortunately, we can’t control the num-\nber of partitions in the scheduled_tasks table, which ultimately determines the\nnumber of concurrent executions of the execute function. However, we can\noverride the Concurrent Batches Per Shard configuration setting (https://\namzn.to/2YUGE59), which allows us to increase the parallelism factor tenfold\n(see figure 7.10). \nFigure 7.10\nYou can find the Concurrent Batches Per Shard setting under Additional Settings for Kinesis \nand DynamoDB Stream functions.\n",
      "page_number": 118
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 129-137)",
      "start_page": 129,
      "end_page": 137,
      "detection_method": "topic_boundary",
      "content": "113\nStep Functions\nCOST\nThis solution requires no DynamoDB reads. The deleted item is included in the\nREMOVE events in the DynamoDB stream. Because events in the DynamoDB stream\nare received in batches, they are efficient to process and require fewer Lambda invo-\ncations. Furthermore, DynamoDB Streams are usually charged by the number of read\nrequests, but it’s free when you process events with Lambda. Because of these charac-\nteristics, this solution is extremely cost effective even when it’s scaled to many millions\nof scheduled tasks. Hence, this is why we gave it a perfect 10 for Cost.\n7.3.3\nFinal thoughts\nThis solution makes creative use of the TTL feature in DynamoDB and gives you an\nextremely cost-effective solution for running scheduled tasks. However, because\nDynamoDB TTL doesn’t offer any reasonable guarantee on how quickly tasks are\ndeleted, it’s ill-fitted for many applications. In fact, neither cron jobs nor DynamoDB\nTTL are well-suited for applications where tasks need to be run within a few seconds of\ntheir scheduled time. For these applications, our next solution might be the best fit as\nit offers unparalleled precision at the expense of other nonfunctional requirements.\n7.4\nStep Functions\nStep Functions is an orchestration service that lets you model complex workflows as state\nmachines. It can invoke Lambda functions or integrate directly with other AWS services\nsuch as DynamoDB, SNS, and SQS when the state machine transitions to a new state.\n One of the understated superpowers of Step Functions is the Wait state (https://\namzn.to/38po884). It lets you pause a workflow for up to an entire year! Normally,\nidle waiting is difficult to do with Lambda. But with Step Functions, it’s as easy as a few\nlines of JSON:\n\"wait_ten_seconds\": {\n  \"Type\": \"Wait\",\n  \"Seconds\": 10,\n  \"Next\": \"NextState\"\n}\nYou can also wait until a specific UTC timestamp:\n\"wait_until\": {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\nAnd using TimestampPath, you can parameterize the Timestamp value using data that\nis passed into the execution:\n\"wait_until\": {\n  \"Type\": \"Wait\",\n  \"TimestampPath\": \"$.scheduledTime\",\n  \"Next\": \"NextState\"\n}\n\n\n114\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nTo schedule an ad hoc task, you can start a state machine execution and use a Wait\nstate to pause the workflow until the specified date and time. This solution is precise.\nBased on the data we have collected, tasks are run within 0.01 second of the scheduled\ntime in the 90th percentile. However, there are several service limits to keep in mind\n(see https://amzn.to/2C4fGPD):\nThere are limits to the StartExecution API. This API limits the rate at which you can\nschedule new tasks because every task has its own state machine execution (see\nfigure 7.11).\nThere are limits to the number of state transitions per second. When the Wait state\nexpires, the scheduled task runs. However, when there are large hotspots where\nmany tasks all run simultaneously, these can be throttled because of this limita-\ntion (see figure 7.12).\nFigure 7.11\nStartExecution API \nlimit for AWS Step Functions\nFigure 7.12\nState transition limit for AWS Step Functions\n\n\n115\nStep Functions\nThere is a default limit of 1,000,000 open executions. Because there is one open exe-\ncution per scheduled task, this is the maximum number of open tasks the sys-\ntem can support.\nThankfully, all of these limits are soft limits, which means you can increase them with a\nservice limit raise. However, given that the default limits for some of these are pretty\nlow, it might not be possible to raise to a level that can support running a million\nscheduled tasks in a single hotspot.\n There is also the hard limit on how long an execution can run, which is one year.\nThis limits the system to schedule tasks that are no further than a year away. For most\nuse cases, this would likely be sufficient. If not, we can tweak the solution to support\ntasks that are scheduled for more than a year away (more on this later).\n7.4.1\nYour scores\nWhat do you think of this solution? How would you rate it on a scale of 1 to 10 against\neach of the nonfunctional requirements? As before, write down your scores in the\nempty spaces provided by table 7.5.\n7.4.2\nOur scores\nStep Functions gives us a simple and elegant solution for the problem at hand. How-\never, it’s hampered by several service limits that makes it difficult to scale. We will dive\ninto these limitations, but first, table 7.6 shows our scores for this solution.\nPRECISION\nAs we mentioned before, Step Functions is able to run tasks within 0.01 s precision at\nthe 90th percentile. It just doesn’t get more precise than that!\nTable 7.5\nYour solution scores for Step Functions\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nTable 7.6\nOur scores for Step Functions\nScore\nPrecision\n10\nScalability (number of open tasks)\n7\nScalability (hotspots)\n4\nCost\n2\n\n\n116\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nSCALABILITY (NUMBER OF OPEN TASKS)\nWe gave this solution a 7 because the StartExecution API limit restricts how many\nscheduled tasks we can create per second. Whereas solutions that store scheduled\ntasks in DynamoDB can easily scale to scheduling tens of thousands of tasks per sec-\nond, here we have to contend with a default refill rate of just 300 per second in the\nlarger AWS regions. Luckily, it is a soft limit, so technically we can raise it to whatever\nwe need. But the onus is on us to constantly monitor its usage against the current limit\nto prevent us from being throttled.\n The same applies to the limit on the number of open executions. While the\ndefault limit of 1,000,000 is generous, we still need to keep an eye on the usage level.\nOnce we reach the limit, no new tasks can be scheduled until existing tasks are run.\nUser behavior would have a big impact here. The more uniformly the tasks are distrib-\nuted over time, the less likely this limit would be an issue.\nSCALABILITY (HOTSPOTS)\nWe gave this solution a 4 because the limit on StateTransition per second is problem-\natic if a large cluster of tasks needs to run during the same time. Because the limit\napplies to all state transitions, even the initial Wait states could be throttled and affect\nour ability to schedule new tasks.\n We can increase both the bucket size (think of it as the burst limit) as well as the\nrefill rate per second. But raising these limits alone might not be enough to scale this\nsolution to support large hotspots with, say, 1,000,000 tasks. Thankfully, there are\ntweaks we can make to this solution to help it handle large hotspots better, but we\nneed to trade off some precision (more on this later).\nCOST\nWe gave this solution a 2 because Step Functions is one of the\nmost expensive services on AWS. We are charged based on\nthe number of state transitions. For a state machine that waits\nuntil the scheduled time and runs the task, there are four\nstates (see figure 7.13), and every execution charges for these\nstate transitions (http://mng.bz/ZxGm).\n At $0.025 per 1,000 state transitions, the cost for schedul-\ning 1,000,000 tasks would be $100 plus the Lambda cost asso-\nciated with executing the tasks. This is nearly two orders of\nmagnitude higher than the other solutions considered so far. \n7.4.3\nTweaking the solution\nSo far, we have discussed several problems with this solution:\nnot being able to schedule tasks for more than a year and hav-\ning trouble with hotspots. Fortunately, there are simple modi-\nfications we can make to address these problems.\nWait\nStart\nExecute\nEnd\nFigure 7.13\nA simple \nstate machine that waits \nuntil the scheduled time \nto run its task\n\n\n117\nStep Functions\nWait\nStart\nExecute\nEnd\nIs it time to run?\nRecurse\nFigure 7.14\nA revised state \nmachine design that can \nsupport scheduled tasks that \nare more than one year away\nEXTEND THE SCHEDULED TIME BEYOND ONE YEAR\nThe maximum time a state machine execution can run for\nis one year. As such, the maximum amount of time a Wait\nstate can wait for is also one year. However, we can extend\nthis limitation by borrowing the idea of tail recursion\n(https://www.geeksforgeeks.org/tail-recursion/) \nfrom\nfunctional programming. Essentially, at the end of a Wait\nstate, we can check if we need to wait for even more time.\nIf so, the state machine starts another execution of itself\nand waits for another year, and so on. Until eventually, we\narrive at the task’s scheduled time and run the task.\n This is similar to a tail recursion because the first exe-\ncution does not need to wait for the recursion to finish.\nIt simply starts the second execution and then proceeds\nto complete itself. See figure 7.14 for how this revised\nstate machine might look.\nSCALING FOR HOTSPOTS\nSometimes, just raising the soft limits on the number of\nStateTransitions per second alone is not going to be\nenough. Because the default limits have a bucket size of\n5,000 (the initial burst limit) and a refill rate of 1,500\nper second, if we are to support running 1,000,000 tasks\naround the same time, we will need to raise these limits by multiple orders of magni-\ntude. AWS will be unlikely to oblige such a request, and we will be politely reminded\nthat Step Functions is not designed for such use cases.\n Fortunately, we can make small tweaks to the solution to make it far more scalable\nwhen it comes to dealing with hotspots. Unfortunately, we will need to trade off some\nof the precision of this solution for the new found scalability.\n For example, instead of running every task scheduled for 00:00 UTC at exactly\n00:00 UTC, we can spread them across a 1-minute window. We can do this by adding\nsome random delay to the scheduled time. Following this simple change, some of the\naforementioned tasks would be run at 00:00:12 UTC, and some would be run at\n00:00:47 UTC, for instance. This allows us to make the most of the available through-\nput. With the default limit of 5,000 bucket size and refill rate of 1,500 per second, the\nmaximum number of state transitions per minute is 93,500:\nUses all 5,000 state transitions in the first second\nUses the 1,500 refill per second for the remaining 59 seconds\nDoing this would reduce the precision to “run within a minute,” but we wouldn’t need\nto raise the default limits by nearly as much. It’ll be a trivial change to inject a variable\namount of delay (0–59 s) to the scheduled time so that tasks are uniformly distributed\nacross the minute window. With this simple tweak, Step Functions is no longer the\n\n\n118\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nscalability bottleneck. Instead, we will need to worry about the rate limits on the\nLambda function that will run the task. \n Another alternative would be to have each state machine execution run all the\ntasks that are scheduled for the same minute in batches and in parallel. For example,\nwhen scheduling a task, add the task with the scheduled time in a DynamoDB table as\nthe HASH key and a unique task ID as the RANGE key. At the same time, atomically\nincrement a counter for the number of tasks scheduled for this timestamp. Both of\nthese updates can be performed in a single DynamoDB transaction. Figure 7.15 shows\nhow the table might look.\nWe would start a state machine execution with the timestamp as the execution name.\nBecause execution names have to be unique, the StartExe-\ncution request will fail if there’s an existing execution\nalready. This ensures that only one execution is responsi-\nble for running all the tasks scheduled for that minute\n(2020-07-04T21:53:22).\n Instead of executing the scheduled tasks immediately\nafter the Wait state, we could get a count of the number of\ntasks that need to run. From here, we would use a Map\nstate to dynamically generate parallel branches to run\nthese tasks in parallel. See figure 7.16 for how this alterna-\ntive design might look.\n Making these changes would not affect the precision\nby too much, but it would reduce the number of state\nmachine executions and Lambda invocations required.\nEssentially, we would need one state machine execution\nfor every minute when we need to run some scheduled\ntasks. There is a total of 525,600 minutes in a 365 days cal-\nendar year, so this also removes the need to increase the\nlimit on the number of open executions (again, the\ndefault limit is 1,000,000). That’s the beauty of these com-\nposable architecture components! Because there are so\nmany ways to compose them, it gives you lots of different\noptions and trade-offs.\nFigure 7.15\nSet the scheduled task as well as the count in the same DynamoDB table.\nWait\nStart\nExecute\nEnd\nGetCount\nFigure 7.16\nAn alternative \ndesign for the state machine \nthat can run tasks in batches \nin parallel\n\n\n119\nSQS\n7.4.4\nFinal thoughts\nStep Functions offers a simple and elegant solution that can run tasks at great preci-\nsion. The big drawback are its costs and the various service limits that you need to look\nout for, which hampers its scalability. But as you can see, if we are willing to make\ntradeoffs against precision, we can modify the solution to make it much more scalable. \n We looked at a couple of possible modifications, including taking some elements\nof the cron job solution and turning this solution into a more flexible cron job that\nonly runs when there are tasks that need to run. We also looked at a modification that\nallows us to work around the 1-year limit by applying tail recursion to the state\nmachine design. In the next solution, we’ll apply the same technique to SQS as it is\nbound by an even tighter constraint on how long a task can stay open.\n7.5\nSQS\nThe Amazon Simple Queue Service (SQS) is a fully managed queuing service. You can\nsend messages to and receive messages from the queue. Once a message has been\nreceived by a consumer, the message is then hidden from all other consumers for a\nperiod of time, which is known as the visibility timeout. You can configure the visibility time-\nout value on the queue, but the setting can also be overridden for individual messages.\n When you send a message to SQS, you can also use the DelaySeconds attribute to\nmake the message become visible at the right time. You can implement the scheduling\nservice by using these two settings to hide a message until its scheduled time. However,\nthe maximum DelaySeconds is a measly 15 minutes, and the maximum visibility time-\nout is only 12 hours. But all is not lost. \n When the execute function receives the message after the initial DelaySeconds, it\ncan inspect the message and see if it’s time to run the task (see figure 7.17). If not, it\ncan call ChangeMessageVisibility on the message to hide the message for up to\nanother 12 hours (https://amzn.to/3e1GVY6). It can do this repeatedly until it’s\nfinally time to run the scheduled task.\n Before you score this solution, consider that there is a limit of 120,000 inflight mes-\nsages. Unfortunately, this is a hard limit and cannot be raised. This limit has a pro-\nfound implication that can mean it’s not suitable for some use cases at all!\n Once a message is inflight, this solution would keep it inflight by continuously\nextending its visibility timeout until its scheduled time. In this case, the number of\ninflight messages equates to the number of open tasks. However, once you reach the\nExercise: Score the modified solutions\nRepeat the same scoring exercise against the modified solutions we proposed. \nHow much would it impact the system’s ability to handle a large number of open\ntasks or hotspots? \nIs there any additional cost impact (e.g., one of the proposed tweaks uses a\nDynamoDB table) that needs to be considered?\n\n\n120\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n120,000 inflight messages limit, then newer messages would stay in the queue’s back-\nlog, even if some of the newer messages might need to run sooner than the messages\nthat are already inflight. Priority is given to tasks based on when they were scheduled,\nnot by their execution time.\n This is not a desirable characteristic for a scheduling service. In fact, it’s the oppo-\nsite of what we want. Tasks that are scheduled to execute soon should be given the pri-\nority to ensure they’re executed on time. That being said, this is a problem that would\nonly arise when you have reached the 120,000 inflight messages limit. The further\naway tasks can be scheduled, the more open tasks you would have, and the more likely\nyou would run into this problem.\n7.5.1\nYour scores\nWith this severe limitation in mind, how would you score this solution? Write down\nyour scores in the empty spaces in table 7.7.\n7.5.2\nOur scores\nThis solution is best suited for scenarios where tasks are not scheduled too far away in\ntime. Otherwise, we face the prospect of accumulating a large number of open tasks\nand running into the limit on inflight messages. Also, we would need to call Change-\nMessageVisibility on the message every 12 hours for a long time. If a task is sched-\nuled to execute in a year, then that’s a total of 730 times. Multiplied that by 1,000,000\nTable 7.7\nYour solution scores for SQS\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nsendMessage with\nDelaySeconds\nChangeMessageVisibility\nMessages are hidden in the \nqueue for up to 15 minutes \nusing the DelaySeconds attribute.\nWhen the Execute function\nreceives a message, it can check\nif it’s time to run the scheduled\ntask. If so, it runs the task and\ndeletes the message from SQS.\nIf it’s not yet time to run the \nscheduled task, then the message\ncan be put back into the queue and\nhidden for up to another 12 hours.\nLambda\nScheduler\nLambda\nExecute\ntask_queue\nSQS\nFigure 7.17\nHigh-level architecture of using SQS to schedule ad hoc tasks\n\n\n121\nSQS\ntasks and that’s a total of 730 million API requests or $292 for keeping 1,000,000 tasks\nopen for a whole year. With these in mind, table 7.8 shows our scores.\nPRECISION\nUnder normal conditions, SQS messages that are delayed or hidden are run no more\nthan a few seconds after their scheduled times. Not as precise as Step Functions, but\nstill very good. This is why we gave this solution a score of 9.\nSCALABILITY (NUMBER OF OPEN TASKS)\nWe gave this solution a low score because the hard limit of 120,000 inflight messages\nseverely limits this solution’s ability to support a large number of open tasks. Even\nthough the tasks can still be scheduled, they cannot run until the number of inflight\nmessages drops below 120,000. This is a serious hinderance and, in the worst cases,\ncan render the system completely unusable. For example, if 120,000 tasks are sched-\nuled to run in one year, then nothing else that’s scheduled after that can run until\nthose first 120,000 tasks have been run.\nSCALABILITY (HOTSPOTS)\nThe Lambda service uses long polling to poll SQS queues and only invokes our func-\ntion when there are messages (http://mng.bz/Rqyj). These pollers are an invisible\nlayer between SQS and our function, and we do not pay for them. But we do pay for\nthe SQS ReceiveMessage requests they make. According to this blog post by Randall\nHunt (https://amzn.to/31MfVtl)\nThe Lambda service monitors the number of inflight messages, and when it detects that this\nnumber is trending up, it will increase the polling frequency by 20 ReceiveMessage requests\nper minute and the function concurrency by 60 calls per minute. As long as the queue\nremains busy it will continue to scale until it hits the function concurrency limits. As the\nnumber of inflight messages trends down Lambda will reduce the polling frequency by 10\nReceiveMessage requests per minute and decrease the concurrency used to invoke our\nfunction by 30 calls per-minute.\nBy keeping the queue artificially busy with a high number of inflight messages, we are\nartificially raising Lambda’s polling frequency and function concurrency. This is use-\nful for dealing with hotspots. \n Because of the way this solution works, all open tasks are kept as inflight messages.\nThis means the Lambda service would likely be running a high number of concurrent\nTable 7.8\nOur solution scores for SQS\nScore\nPrecision\n9\nScalability (number of open tasks)\n2\nScalability (hotspots)\n8\nCost\n5\n",
      "page_number": 129
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 138-149)",
      "start_page": 138,
      "end_page": 149,
      "detection_method": "topic_boundary",
      "content": "122\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\npollers all the time. When a cluster of messages become available at the same time, they\nwill likely be processed by the execute function with a high degree of parallelism. And\nLambda scales up the number of concurrent executions as more messages become\navailable. We can, therefore, use the autoscaling capability that Lambda offers.\n Because of this, we gave this solution a really good score. But, on the other hand,\nthis behavior generates a lot of redundant SQS ReceiveMessage requests, which can\nhave a noticeable impact on cost when running at scale.\nCOST\nBetween the many ReceiveMessage requests Lambda makes on our behalf and the\ncost of calling ChangeMessageVisibility on every message every 12 hours, most of\nthe cost for this solution will likely be attributed to SQS. While SQS is not an expen-\nsive service, at $0.40 per million API requests, the cost can accumulate quickly because\nthis solution is capable of generating many millions of requests at scale. As such, we\ngave this solution a 5, which is to say that it’s not great but also unlikely to cause you\ntoo much trouble.\n7.5.3\nFinal thoughts\nIf you put the scores for this solution side-by-side with DynamoDB TTL, you can see\nthat they perfectly complement each other. Where one is strong, the other is weak.\nTable 7.9 shows the ratings for both services.\nWhat if we can combine these two solutions to create a solution that offers the best of\nboth worlds? Let’s look at that next.\n7.6\nCombining DynamoDB TTL with SQS\nSo far, we have seen that the DynamoDB TTL solution is great at dealing with a large\nnumber of open tasks, but lacks the precision required for most use cases. Conversely,\nthe SQS solution is great at providing good precision and dealing with hotspots but\ncan’t handle a large number of open tasks. The two rather complement each other\nand can be combined to great effect.\n For example, what if long-term tasks are stored in DynamoDB until two days before\ntheir scheduled time? Why two days? Because it’s the only soft guarantee that DynamoDB\nTTL gives: \nTable 7.9\nOur ratings for SQS vs. DynamoDB TTL\nScore (SQS)\nScore (DynamoDB TTL)\nPrecision\n9\n1\nScalability (number of open tasks)\n2\n10\nScalability (hotspots)\n8\n6\nCost\n5\n10\n\n\n123\nCombining DynamoDB TTL with SQS\nTTL typically deletes expired items within 48 hours of expiration (https://amzn.to/\n2NRgARU).\nOnce the tasks are deleted from the DynamoDB table, they are moved to SQS where\nthey are kept inflight until the scheduled time (using the ChangeMessageVisibility\nAPI as discussed earlier). For tasks that are scheduled to execute in less than two days,\nthey are added to the SQS queue straight away. See figure 7.18 for how this solution\nmight look.\n7.6.1\nYour scores\nHow would you score this solution? Again, write your scores in the empty spaces in\ntable 7.10.\nTable 7.10\nYour solution scores for DynamoDB TTL with SQS\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nShort-term task\nLong-term task\nOnly long-term tasks are stored in\nthis table. Their TTL is set to be two\ndays before their scheduled time.\nShort-term tasks (to be run in less\nthan two days) are pushed to SQS\nright away, bypassing DynamoDB.\nUnlike in the DynamoDB TTL\nsolution, this function doesn’t run \nthe task but forwards it to SQS.\nLike the SQS solution, this\nfunction checks each message\nto see if it should run the task \nor hide it again using the\nChangeMessageVisibility API.\nDynamoDB\nDynamoDB Streams\nLambda\nscheduled_tasks\nREMOVE events\nRescheduler\nLambda\nScheduler\nChangeMessageVisibility\nLambda\nExecute\ntask_queue\nSQS\nFigure 7.18\nHigh-level architecture of combining DynamoDB TTL with SQS\n\n\n124\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n7.6.2\nOur scores\nAccording to “The Fundamental theorem of software engineering” (https://dzone\n.com/articles/why-fundamental-theorem):\nWe can solve any problem by introducing an extra level of indirection.\nLike the other alternative solutions we saw earlier in this chapter, this solution solves\nthe problems with an existing solution by introducing an extra level of indirection. It\ndoes so by composing different services together in order to make up for the short-\ncomings of each. Take a look at table 7.11 for our scores for this solution, then we’ll\ndiscuss how we arrived at these scores.\nPRECISION\nAs all the executions go through SQS, this solution has the same level of precision as\nthe SQS-only solution, 9.\nSCALABILITY (NUMBER OF OPEN TASKS)\nStoring long-term tasks in DynamoDB largely solves SQS’s problem with scaling the\nnumber of open tasks. However, it is still possible to run into the 120,000 inflight mes-\nsages limit with just the short-term tasks. It’s far less likely, but it is still a possibility that\nneeds to be considered. Hence, we marked this solution as an 8.\nSCALABILITY (HOTSPOTS)\nAs all the executions go through SQS, this solution has the same score as the SQS-only\nsolution, 8.\nCOST\nThis solution eliminates most of the ChangeMessageVisibility requests because all the\nlong-term tasks are stored in DynamoDB. This cuts out a large chunk of the cost\nassociated with the SQS solution. However, in return, it adds additional costs for\nDynamoDB usage and Lambda invocations for the reschedule function. Overall, the\ncosts this solution takes away are greater than the new costs it adds. Hence, we gave it\na 7, improving on the original score of 5 for the SQS solution.\nTable 7.11\nOur solution scores for DynamoDB TTL with SQS\nScore\nPrecision\n9\nScalability (number of open tasks)\n8\nScalability (hotspots)\n8\nCost\n7\n\n\n125\nThe applications\n7.6.3\nFinal thoughts\nThis is just one example of how different solutions or aspects of them can be com-\nbined to make a more effective answer. This combinatory effect is one of the things\nthat makes cloud architectures so interesting and fascinating, but also, so complex\nand confusing at times. There are so many different ways to achieve the same goal,\nand depending on what your application needs, there’s usually no one-size-fits-all solu-\ntion that offers the best results for all applications.\n So far, we have only looked at the supply side of the equation and what each solu-\ntion can offer. We haven’t looked at the demand side yet or what application needs\nwhat. After all, depending on the application, you might put a different weight\nbehind each of the nonfunctional requirements. Let’s try to match our solutions to\nthe right application next.\n7.7\nChoosing the right solution for your application\nTable 7.12 shows our scores for the five solutions that we considered in this chapter.\nThe solutions in this table do not include the proposed tweaks. Depending on the\napplication, some of these requirements might be more important than others.\n7.8\nThe applications\nLet’s consider three applications that might use the ad hoc scheduling service:\nApplication 1 is a reminder app, which we’ll call RemindMe. \nApplication 2 is a multi-player app for a mobile game, which we’ll call Tourna-\nmentsRUs.\nApplication 3 is a healthcare app that digitizes and manages your consent for\nsharing your medical data with care providers, which we’ll call iConsent.\nIn the reminder app, RemindMe, users can create reminders for future events, and\nthe system will send SMS/push notifications to the users 10 minutes before the event.\nWhile reminders are usually distributed evenly over time, there are hotspots around\npublic holidays and major sporting events such as the Super Bowl. During these\nhotspots, the application might need to notify millions of users. Fortunately, because\nTable 7.12\nOur scores for all five solutions\nCron job\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\n6\n1\n10\n9\n9\nScalability (number of \nopen tasks)\n10\n10\n7\n2\n8\nScalability (hotspots)\n2\n6\n4\n8\n8\nCost\n7\n10\n2\n5\n7\n\n\n126\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nthe reminders are sent 10 minutes before the event, the system gives us some slack in\nterms of timing.\n In application 2, the multi-player mobile app called TournamentsRUs, players\ncompete in user-generated tournaments that are 15–30 minutes long. As soon as the\nfinal whistle blows, the tournament ends and all participants wait for a winner to be\nannounced via an in-game popup. TournamentsRUs currently has 1.5 million daily\nactive users (DAU) and hopes to double that number in 12 months’ time. At peak, the\nnumber of concurrent users is around 5% of its DAU, and tournaments typically con-\nsist of 10–15 players each.\n In application 3, the healthcare app iConsent, users fill in digital forms that allow\ncare providers to access their medical data. Each consent has an expiration date, and the\napp needs to change its status to expired when the expiration date passes. iConsent cur-\nrently has millions of registered users, and users have an average of three consents.\n Each of these applications need to use a scheduling service to run ad hoc tasks at spe-\ncific times, but their use cases are drastically different. Some deal with tasks that are\nshort-lived, while others allow tasks to be scheduled for any future point in time. Some\nare prone to hotspots around real-world events; others can accumulate large numbers\nof open tasks because there is no limit to how far away tasks can be scheduled.\n To help us better understand which solution is the best for each application, we\ncan apply a weight against each of the nonfunctional requirements. For example,\nTournamentsRUs cares a lot about precision because users will be waiting for their\nresults at the end of a tournament. If the tasks to finalize tournaments are delayed,\nthen it can negatively impact the users’ experience with the app.\n7.8.1\nYour weights\nFor each of the applications, write a weight between 1 (“I don’t care”) and 10 (“This is\ncritical”) for each of the nonfunctional requirements in table 7.13. Remember, there\nare no right or wrong answers here! Use your best judgement based on the limited\namount of information you know about each app.\n7.8.2\nOur weights\nTable 7.14 shows our weightings. Are these scores similar to yours? We’ll go through\neach application and talk about how we arrived at these weights in the sections follow-\ning the table.\nTable 7.13\nYour ratings for RemindMe, TournamentsRUs, and iConsent\nRemindMe\nTournamentsRUs\niConsent\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\n\n\n127\nThe applications\nREMINDME\nWe gave Precision a weight of 5 for this app because reminders are sent 10 minutes\nbefore the event. This gives us a lot of slack. Even if the reminder is sent 5 minutes\nlate, it’s still OK.\n Scalability (number of open tasks) gets a weight of 10 because there are no upper\nbounds on how far out the events can be scheduled. At any moment, there can be mil-\nlions of open reminders. This makes scaling the number of open tasks an absolutely\ncritical requirement for this application. For Scalability (hotspots), we gave a weight of\n8 because large hotspots would likely form around public holidays (for example,\nmother’s day) and sporting events (for example, the Super Bowl or the Olympics).\n Finally, for Cost, we gave it a weight of 3. This perhaps reflects our general attitude\ntowards the cost of serverless technologies. Their pay-per-use pricing allows our cost to\ngrow linearly when scaling. Generally speaking, we don’t want to optimize for cost\nunless the solution is going to burn down the house!\nTOURNAMENTSRUS\nFor TournamentsRUs, Precision gets a weight of 10 because when a tournament fin-\nishes, players will all be waiting for the announcement of the winner. If the scheduled\ntask (to finalize the tournament and calculate the winner) is delayed for even a few\nseconds, it would provide a bad user experience.\n We gave Scalability (number of open tasks) a weight of 6 because only a small per-\ncentage of its DAUs are online at once and because of the short duration of its tourna-\nments. At 1.5 M DAU, 5% concurrent users at peak and an average of 10–15 players in\neach tournament, these numbers translate to approximately 5,000–7,500 open tour-\nnaments during peak times.\n For Scalability (hotspots), it received a lowly 3 because the tournaments are user-\ngenerated and have different lengths of time (between 15–30 minutes). It’s unlikely\nfor large hotspots to form under these conditions. And, as with RemindMe, we gave\nCost a weight of 3 (just don’t burn my house down!).\nICONSENT\nLastly, for iConsent, Precision received a weight of 4. When a consent expires, it\nshould be shown in the UI with the correct status. However, because the user is proba-\nbly not going to check the app every few minutes for updates, it’s OK if the status is\nupdated a few minutes (or maybe even an hour) later.\nTable 7.14\nOur scores for RemindMe, TournamentsRUs, and iConsent\nRemindMe\nTournamentsRUs\niConsent\nPrecision\n5\n10\n4\nScalability (number of open tasks)\n10\n6\n10\nScalability (hotspots)\n8\n3\n1\nCost\n3\n3\n3\n\n\n128\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n We gave a weight of 10 for Scalability (number of open tasks). This is because med-\nical consents can be valid for a year or sometimes even longer: all of these active con-\nsents are open tasks, so the system would have millions of open tasks at any moment.\nFor Scalability (hotspots) on the other hand, we gave it a weight of 1 because there is\njust no natural clustering that can lead to hotspots. And finally, cost gets a weight of 3\nbecause that’s just how we generally feel about cost for serverless applications.\n7.8.3\nScoring the solutions for each application\nSo far, we have scored each solution based on its own merits. But this says nothing about\nhow well suited a solution is to an application because, as we have seen, applications have\ndifferent requirements. By combining a solution’s scores with an application’s weights,\nwe can arrive at something of an indicative score of how well they are suited for each\nother. Let’s show you how this can be done and then you can do this exercise yourself.\nIf you recall, the following table shows our scores for the cron job solution:\nFor RemindMe, we gave the following weights:\nNow, if we multiple the score with the weight for each nonfunctional requirement, we\nwill arrive at the scores in the following table:\nScore (cron job)\nPrecision\n6\nScalability (number of open tasks)\n10\nScalability (hotspots)\n2\nCost\n7\nWeight (RemindMe)\nPrecision\n5\nScalability (number of open tasks)\n10\nScalability (hotspots)\n8\nCost\n3\nWeighted Score (Cron job × RemindMe)\nPrecision\n6 × 5 = 30\nScalability (number of open tasks)\n10 × 10 = 100\nScalability (hotspots)\n2 × 8 = 16\nCost\n7 × 3 = 21\n\n\n129\nThe applications\nThis adds up to a grand total of 30 + 100 + 16 + 21 = 167. On its own, this score means\nvery little. But if we repeat the exercise and score each of the solutions for RemindMe,\nthen we can see how well they compare with each other. This would help us pick the\nmost appropriate solution for RemindMe, which might be different than the solution\nyou would use for TournamentsRUs or iConsent. \n With that in mind, use the whitespace in table 7.15 to calculate your weighted\nscores for each of the five solutions that we have discussed so far for RemindMe. Then\ndo the same for TournamentsRUs and IConsent in tables 7.16 and 7.17, respectively.   \nTable 7.15\nWeighted scores for the app RemindMe\nCron\njob\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\nScalability (number of \nopen tasks)\nScalability (hotspots)\nCost\nTotal score\nTable 7.16\nWeighted scores for the app TournamentsRUs\nCron\njob\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\nScalability (number of \nopen tasks)\nScalability (hotspots)\nCost\nTotal score\nTable 7.17\nWeighted scores for the app iConsent\nCron\njob\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\nScalability (number of \nopen tasks)\nScalability (hotspots)\nCost\nTotal score\n\n\n130\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nDid the scores align with your gut instinct for which solution is best for each applica-\ntion? Did you find something unexpected in the process? Did some solutions not fare\nas well as you thought they might?\n If you find any uncomfortable outcomes as a result of these exercises, then they\nhave done their job. The purpose of defining requirements up front and putting a\nweight against each requirement is to help us remain objective and combat cognitive\nbiases. Table 7.18 shows our total weighted scores for each solution and application.\nThese scores give you a sense as to which solutions are best suited for each applica-\ntion. But they don’t give you definitive answers and you shouldn’t follow them blindly.\nFor instance, there are often factors that aren’t included in the scoring scheme but\nneed to be taken into account nonetheless. Factors such as complexity, resource con-\nstraints, and familiarity with the technologies are usually important for real-world\nprojects.\nSummary\nIn this chapter, we analyzed five different ways to implement a service for executing ad\nhoc tasks, and we judged these solutions on the nonfunctional requirements we set\nout at the start of the chapter. Throughout the chapter we asked you to think critically\nabout how well each solution would perform for these nonfunctional requirements\nand asked you to rate them. And we shared our scores with you and our rationale for\nthese scores. We hope through these exercises you have gained some insights into\nhow we approach problem solving and the considerations that goes into evaluating a\npotential solution:\nWhat are the relevant service limits and how do they affect the scalability\nrequirements of the application?\nWhat are the performance characteristics of the services in question and do\nthey match up with the application’s needs?\nHow are the services charged? Project the cost of the application by thinking\nthrough how the application would need to use these AWS services and apply-\ning the services’ billing model to that usage pattern.\nTable 7.18\nOur total weighted scores for RemindMe, TournamentsRUs, and iConsent\nRemindMe\nTournamentsRUs\niConsent\nCron job\n167\n147\n147\nDynamoDB TTL\n180\n115\n137\nStep Functions\n158\n160\n120\nSQS\n144\n141\n79\nDynamoDB TTL with SQS\n210\n183\n145\n\n\n131\nSummary\nThese points are a lot easier said than done and it takes practice to become proficient\nat them. The AWS services are always evolving and new services and features become\navailable all the time so you also have to continuously educate yourself as new options\nand techniques emerge. Despite having worked with AWS for over a decade, we are\nstill learning ourselves and having to constantly update our own understanding of\nhow different AWS services operate.\n Furthermore, AWS do not publish the performance characteristics for many of its\nservices. For example, how soon does Step Functions execute a Wait state after the\nspecific timestamp. If your solution depends on assumptions about these unknown\nperformance characteristics, then you should design small experiments to test your\nassumptions. In the course of writing this chapter, we conducted several experiments\nto find out how soon Step Functions and SQS executes delayed tasks. Failing to vali-\ndate these assumptions early can have devastating consequences. Months of engineer-\ning work might go to waste if an entire solution was built on false assumptions.\n At the end of the chapter we also asked you to do an exercise of finding the best\nsolution for a given problem and gave you three example applications, each with a dif-\nferent set of needs. The scoring method we asked you to apply is not fool-proof but\nhelps you make objective decisions and combat confirmation biases.\n As you brainstormed and evaluated the solutions that have been put in front of you\nin this chapter, I hope you picked up on the even more important lessons here: that\nall architecture decisions have inherit tradeoffs and not all application requirements\nare created equally. The fact that different applications care about the characteristics\nof its architecture to different degrees gives us a lot of room to make smart tradeoffs.\n There are so many different AWS services to choose from, each offering a different\nset of characteristics and tradeoffs. When you use different services together, they can\noften create interesting synergies. All of these give us a wealth of options to mix and\nmatch different architectural approaches and to engage in a creative problem-solving\nprocess, and that’s beautiful!\n\n\n132\nArchitecting serverless\n parallel computing\nThere’s a secret about AWS Lambda that we like to tell people: it’s a supercomputer\nthat can perform faster than the largest EC2 instance. The trick is to think about\nLambda in terms of parallel computation. If you can divide your problem into\nhundreds or thousands of smaller problems and solve them in parallel, you will get\nto a result faster than if you try to solve the same problem by moving through it\nsequentially. \n Parallel computing is an important topic in computer science and is often talked\nabout in the undergraduate computer science curriculum. Interestingly, Lambda,\nby its very nature, predisposes us to think and apply concepts from parallel comput-\ning. Services like Step Functions and DynamoDB make it easier to build parallel\napplications. \n In this chapter, we’ll illustrate how to build a serverless video transcoder in\nLambda that outperforms bigger and more expensive EC2 servers.\nThis chapter covers\nPrinciples of MapReduce\nServerless solution with Step Functions and EFS\n\n\n133\nIntroduction to MapReduce\n8.1\nIntroduction to MapReduce\nMapReduce is a popular and well-known programming model that’s often used to\nprocess large data sets. It was originally created at Google by developers who were\nthemselves inspired by two well-known functional programming primitives (higher-\norder functions): map and reduce. MapReduce works by splitting up a large data set\ninto many smaller subsets, performing an operation on each subset and then combin-\ning or summing up to get the result. \n Imagine that you want to find out how many times a character’s name is men-\ntioned in Tolstoy’s War and Peace. You can sequentially look through every page, one\nby one, and count the occurrences (but that’s slow). If you apply a MapReduce\napproach, however, you can do it much quicker:\n1.\nYou split the data into many independent subsets. In the case of War and Peace,\nit could be individual pages or paragraphs. \n2.\nYou apply the map function to each subset. The map function in this case scans\nthe page (or paragraph) and emits how many times a character’s name is\nmentioned. \n3.\nThere could be an optional step here to combine some of the data. That can\nhelp make the computation a little easier to perform in the next step. \n4.\nThe reduce function performs a summary operation. It counts the number of\ntimes the map function has emitted the character’s name and produces the\noverall result.\nNOTE\nIt’s important to understand that the power of MapReduce in the War\nand Peace example comes from the fact that the map step can run in parallel\non thousands of pages or paragraphs. If this wasn’t the case, then this pro-\ngram would be no different from a sequential count. \nFigure 8.1 shows what a theoretical MapReduce architecture looks like. We’ll build\nsomething like this soon.\n As you may have already guessed, real-world MapReduce applications are often\nmore complex. In a lot of cases, there are intermediary steps between map and reduce\nthat combine or simplify data, and considerations such as locality of data become\nimportant in order to minimize overhead. Nevertheless, we can take the idea of split-\nting a problem into smaller chunks, processing them in parallel, and then combining\nand reducing them to achieve the outcome you need, and we can do that with\nLambda. \n \n \n \n \n \n \n",
      "page_number": 138
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 150-157)",
      "start_page": 150,
      "end_page": 157,
      "detection_method": "topic_boundary",
      "content": "134\nCHAPTER 8\nArchitecting serverless parallel computing\n8.1.1\nHow to transcode a video\nIn the second chapter of this book, you built a serverless pipeline that converted video\nfrom one format to another. To do this, you used an AWS service called AWS Elemen-\ntal MediaConvert. This service takes a video uploaded to an S3 bucket and transcodes\nit to a range of different formats specified by you. Our goal in this chapter is to do\nsomething crazy and implement our own video transcoder service using Lambda.\nNote that this is just an experiment and an opportunity to explore highly parallelized\nserverless architectures. Our major requirements for our serverless encoding service\nare as follows:\nBuild a transcoder that takes a video file and produces a transcoded version in a\ndifferent format or bit rate. We want complete control over the transcoding\nprocess.\nUse only serverless services in AWS, such as Lambda and S3. Obviously, we are\nnot allowed to use an EC2 or a managed service to do transcoding for us.\nBuild a product that is robust and fast. It should be able to beat a large EC2\ninstance most of the time. \nLearn about parallel computing and how to think about solving these problems. \nThe solution we are about to present works, but it’s not something we recommend\nrunning in a production environment. Unless your core business is video transcoding,\nA split procedure splits the data \nset into multiple chunks.\nA map procedure performs\nfiltering and sorting.\nA combine procedure (optional)\nreduces data to a simplified form.\nA reduce procedure performs \nsummary options.\nSplit\nReduce\nMap\nMap\nMap\nMap\nMap\nProblem\nCombine\nCombine\nCombine\nFigure 8.1\nThese are the steps a fictional MapReduce algorithm could take.\n\n\n135\nIntroduction to MapReduce\nyou should outsource as much of the undifferentiated heavy lifting as possible in\norder to focus on your own unique problem. In most cases, a managed service like\nAWS Elemental MediaConvert is better; you don’t have to worry about keeping it run-\nning. Take this as just an exercise and an opportunity to learn about MapReduce and\nparallel computation (you never know when you might face a big problem that\nrequires the skill you may pick up here).\n8.1.2\nArchitecture overview\nTo transcode a file using Lambda, we need to apply principles of MapReduce. We are\nnot implementing classical MapReduce here; instead, we are taking inspiration from\nthis algorithm to build our transcoder. \n An interesting thing about Lambda (that we’ve mentioned before) is that it forces\nus to think parallel. It’s impossible to process large video files in a Lambda function if\nyou treat a single function like a traditional computer. You’d run out of memory and\ntimeout quickly. If the video file is large enough, the function would either stop after\n15 minutes of processing or exhaust all available RAM and crash. \n To get around this, we decompose the problem into smaller problems that can be\nprocessed within Lambda’s constraints. The implication is that we can try the follow-\ning to process a large video file:\n1.\nDivide the video file into a lot of tiny segments. \n2.\nTranscode these segments in parallel.\n3.\nCombine these small segments together to produce a new video file.\nWe need to parallelize as much as possible to get the most out of our serverless super-\ncomputer. For example, if some segments are ready to be combined while others are\nstill processing, we should combine the ones that are ready. Performance is the name\nof the game here. So, with that in mind, here’s an outline for our serverless transcod-\ning algorithm that, let’s say, is designed to transcode a video from one bit rate to\nanother:\n1.\nA user uploads a video file to S3 that invokes a Lambda function.\n2.\nThe Lambda function analyzes the file and figures out how to cut up the source\nfile to produce smaller video files (segments) for transcoding.\n3.\nTo make things go a little bit faster, we strip away the audio from video and save\nit to another file. Not having to worry about the audio makes executing steps\n4–6 faster. \nHow would you do video transcoding in Lambda?\nTake a moment and think about how you would build a video transcoder using AWS\nLambda. All guesses are good, and we’d love to know how you would approach the\nproblem (twitter.com/sbarski). Can you build it yourself without reading the rest of the\nchapter?\n\n\n136\nCHAPTER 8\nArchitecting serverless parallel computing\n4.\nThis step performs the split process that creates small video segments for\ntranscoding.\n5.\nNow comes the map process that transcodes segments to the desired format or\nbit rate. The system can transcode a bunch of these segments in parallel.\n6.\nThe map process is followed by a combine step that begins to merge these small\nvideo files together.\n7.\nThe final step merges audio and video together and presents the file to the\nuser. We have reduced our work to its final output. \n8.\nAs the kids say, the real final step is profit.\nHere are the main AWS services and software that we will use to build the transcoder:\nFFmpeg—In the first edition of our book, we briefly used FFmpeg, a cross-platform\nlibrary/application created for recording, converting, and streaming audio and\nvideo. This is a powerhouse of an application that is used by everyone and anyone\nranging from hobbyists to commercial TV channels. \nWe’ll use ffmpeg in this chapter to do the transcoding, splitting, and merg-\ning of video files. We’ll also use a utility called ffprobe to analyze the video file\nand figure out how to cut it up on keyframes. The ffmpeg and ffprobe binaries\nare shipped as a Lambda layer (http://mng.bz/N4MN), which allow other\nLambda functions to access and run them. You don’t necessarily have to use\nLambda layers (you can upload the ffmpeg binary with each function that will\nuse it), but that is redundant, inconvenient, and takes a long time to deploy.\nTherefore, making ffmpeg available via a Lambda layer is the recommended\nand preferred approach.\nDEFINITION\nA keyframe stores the complete image in the video stream, while a\nregular frame stores an incremental change from the previous frame. Cutting\non keyframes prevents us from losing information in the video. \nAWS Lambda—It goes without saying that we’ll use Lambda for nearly every-\nthing. Lambda runs ffmpeg and executes most of the logic. We’ll write six func-\ntions to analyze the video, extract audio, split the original file, convert\nsegments, merge video segments, and then merge video and audio in the final\nstep. \nStep Functions—To help us orchestrate this workflow, we’ll rely on Step Func-\ntions. This service helps us to define how and in what order our execution steps\nhappen, makes the entire workflow robust, and provides additional visibility\ninto the execution. \nS3—We’ll use Simple Storage Service (S3) for storing the initial and the final\nvideo. We could also use it to store the temporary video chunks, but we’ll use\nEFS for that. The reason why we chose EFS is because it is easy to mount and\naccess as a filesystem from Lambda. We’ll provide an alternative implementa-\ntion that uses S3, but it is slightly harder to get right.\n\n\n137\nArchitecture deep dive\nEFS—We’ll use the Elastic File System (EFS) for our serverless transcoder to\nstore the temporary files that we generate. There will be a lot of small video\nfiles. Luckily, EFS can grow and shrink as needed.\nDynamoDB—Although Step Functions help to manage the overall workflow and\nexecution of Lambda functions, we still need to maintain some state. We need\nto know whether certain video chunks were created or can be merged. We’ll use\nDynamoDB to store this information. Everything that’s stored is ephemeral and\nwill be deleted using DynamoDB’s Time to Live (TTL) feature.\nFigure 8.2 shows a high-level overview of the system we are about to build. We will\njump into individual components in the next section. \n8.2\nArchitecture deep dive\nLet’s explore the architecture in more detail. There’s nuance to the implementation\nand how things work. To avoid dealing with some pain later, let’s plan how we will\ndesign, build, and deploy the serverless transcoder. Before going any further, recall\nthat the entire idea is to split a giant video file into many small segments, transcode\nthese segments in parallel, and then merge them together into a new file.\nSimple Storage Service (S3)\n(source file)\nAWS Lambda\n(Transcode Video)\nAWS Lambda \n(Merge Video and Audio)\nSimple Storage Service\n(new File)\nDynamoDB\nEFS\nAWS Lambda\n(Merge Video)\nMultiple Lambda’s run here in \nparallel to split the file using the \nDynamic Parallelism feature of \nStep Functions.\nAWS Cloud\nAWS Step Functions workflow\nAWS Lambda\n(Split Audio)\nEFS\nDynamoDB\nEFS\nDynamoDB\nAWS Lambda \n(Split and Convert Video)\nEFS\nDynamoDB\nStart\nFigure 8.2\nThis is a simplified view of the serverless transcoder. There are a few more components to \nit, but we’ve avoided including them in this figure to focus on the essential elements of the architecture.\n\n\n138\nCHAPTER 8\nArchitecting serverless parallel computing\n8.2.1\nMaintaining state\nWe’ll use DynamoDB to maintain state across the entire operation of the serverless\npipeline. It’ll keep track of which videos have been created and which haven’t. To sim-\nplify the pipeline and, in particular, to simplify the code that monitors which segments\nhave been created or transcoded, we are going to use a trick. (Before going any fur-\nther, think about how you would keep track of all small video segments that have been\ncreated, transcoded, and merged given that segments will be created and processed in\nparallel.) \n The trick is to create n^2 smaller video segments. Out of one large video file, we\nshould generate 2, or 4, or 8, or 256, or 512, or more segments. Just remember to\nkeep the number of segments at n^2. Why is this? The idea is that once we’ve created\nand transcoded our video segments, we can begin merging them in any order. Having\nn^2 segments easily allows us to identify which two neighbor segments can be merged.\nAnd, we can keep track of this information in the database. \n We’ll create a basic binary tree that helps to make the logic around this algorithm\na little easier to manage. Let’s imagine that we have 8 segments. Here’s how the pro-\ncess could take place: \n1.\nSegments 3 and 4 are transcoded quicker than the rest and can be merged\ntogether (they are neighbors) into a new segment called 3-4.\n2.\nThen segment 7 is created, but segment 8 is not yet available. The system waits\nfor segment 8 to become ready before merging 7 and 8 together. \n3.\nSegment 8 is created and segments 7 and 8 can be merged together into a new\nsegment 7-8.\n4.\nThen segments 1 and 2 finish transcoding and are merged into a segment 1-2. \n5.\nThe good news is that a neighboring segment 3-4 is already available. Therefore,\nsegments 3-4 and 1-2 can be merged together into a new segment called 1-4.\n6.\nSegments 5 and 6 are transcoded and are merged into a segment 5-6. \n7.\nSegment 5-6 has a neighboring segment 7-8. These two segments are merged\ntogether to create segment 5-8. \n8.\nFinally, segments 1-4 and 5-8 can be merged together to create the final video\nthat consists of segments 1 to 8. \nBecause we have n^2 segments, we can keep track of neighboring segments and\nmerge them as soon as both neighbors (the left and the right) become available.\nAnother interesting aspect is that segments themselves can figure out who their neigh-\nbors are for merging. A segment with an index that is cleanly divisible by 2 is always on\nthe right, whereas a segment that is not cleanly divisible by 2 is on the left. For exam-\nple, a segment with an index of 6 is divisible by 2, therefore, we can figure that it’s on\nthe right, and the neighbor it needs to merge with (when it becomes available) has an\nindex of 5. Figure 8.3 illustrates how blocks can be merged together.\n DynamoDB is an excellent tool for keeping track of which segments have been cre-\nated and merged. In fact, we will precompute all possible segments and add them to\n\n\n139\nArchitecture deep dive\nDynamoDB. Then we will atomically increment counters in DynamoDB to have a\nrecord of when segments have been created and merged. This allows the processing\nengine to figure out which blocks haven’t been merged yet and which need to be\ndone next. \n This is an important part of the algorithm, so it’s worth restating it again. Each\nrecord in DynamoDB represents two neighboring segments (for example, segment 1\nand segment 2). The split-and-convert operation increments a confirmation counter\neach time a segment is created. When the confirmation counter equals 2, our system\nknows that the two neighboring segments exist and that they can be merged together. \n This information and logic are used in the Split and Convert function and in the\nMerge Video function. Our algorithm continues to merge segments and increment\nthe confirmation counter in DynamoDB until there’s nothing left to merge.\nNeighboring segments\nare merged as soon as\nthey are ready.\nFinal video\nSegment 1-8\nMerge\nSegment 1-4\nMerge\nSegment 5-8\nSegment 1\nSegment 2\nSegment 3\nSegment 4\nSegment 5\nSegment 6\nSegment 7\nSegment 8\nMerge\nSegment 1-2\nMerge\nSegment 3-4\nMerge\nSegment 5-6\nMerge\nSegment 7-8\nFigure 8.3\nSegments are merged together into a new video. The beauty of our engine is that neighboring \nsegments can be merged as soon as they are ready. There’s no need to wait for other, nonrelated, segments \nto finish processing.\nThere are more ways than one to do it\nOur use of a binary tree is just one approach to solving this problem, keeping track\nof segments and ultimately merging them together. There are myriad other ways this\ncan be done. How would you do it if you had to come up with a different approach?\n\n\n140\nCHAPTER 8\nArchitecting serverless parallel computing\nTRANSCODE VIDEO\nOur serverless transcoder kicks off once we upload a file into an S3 bucket. An S3\nevent invokes the Transcode Video Lambda and the process begins. Figure 8.4 shows\nwhat this looks like.\n The Transcode Video function performs the following steps:\n1.\nDownloads the file from S3 to a local directory on EFS.\n2.\nAnalyzes the downloaded video file and extracts metadata from it. Video key-\nframes are provided in this metadata.\n3.\nCreates the necessary directories in EFS for all future segments that are going\nto be created.\n4.\nWorks out how many segments need to be created based on the number of\nkeyframes. \nRemember that we are always creating n^2 segments. This means that we\nmay have to create some fake segments in DynamoDB. These will not really do\nanything. They are considered as segments that have already been created, so\nthey don’t need to be processed.\n5.\nCreates the necessary records in DynamoDB including any fake records that are\nneeded.\n6.\nRuns a Step Functions workflow with two different inputs. The first parameter\ntells Step Functions to run a Lambda to extract and save the audio to EFS. The\nAWS Cloud\nThe event that invokes Lambda \ncarries information about the \nvideo file uploaded to S3.\nSimple Storage Service (S3)\n(source file)\nAWS Lambda\n(Transcode Video)\nFigure 8.4\nThis is a basic and common serverless architecture. Invoking code from S3 is the bread and \nbutter of Lambda functions.\n\n\n141\nArchitecture deep dive\nsecond parameter is an array of objects that specify the start and end times of all\nsegments that need to be created. Step Functions takes this array and applies\nthe Map procedure. It fans out and creates a Lambda function for each object\nin the array, thus causing the original file to be split up by many Lambda func-\ntions in parallel.\nThe Transcode Video function is an example of a monolithic or “fat” Lambda func-\ntion because it does a lot of different things. It may not be a bad idea to split it up, but\nthat also comes with its own set of tradeoffs. In the end, whether you think this func-\ntion should be kept together or not may depend on your personal taste and philoso-\nphy. We think that this function is a pragmatic solution for what we need to do, but we\nwouldn’t be aghast if you decided to split it.\n8.2.2\nStep Functions\nStep Functions plays a central role in our system. This service orchestrates and runs\nthe main workflow that splits the video file into segments, transcodes them, and then\nmerges them. Step Functions also run a function that extracts the audio from the\nvideo file and saves it to EFS for safekeeping. The Lambda functions that Step Func-\ntions invoke include:\nSplit Audio—Extracts the audio from the video and saves it as a separate file in\nEFS.\nSplit and Convert Video—Splits the video file from a particular start point (for\nexample, 5 minutes and 25 seconds) to an end point (such as 6 minutes and 25\nseconds) and then encodes the new segment to a different format or bit rate.\nMerge Video—Merges segments together after they have been transcoded. Multi-\nple Merge Video functions will run to merge segments until one final video file\nis produced.\nMerge Video and Audio—Merges the newly created video file and the audio file to\ncreate the final output. This function uploads the new file back to S3.\nTIP\nYou don’t have to extract the audio from the video and then transcode\njust the video file separately. We decided to do that because in our tests, our sys-\ntem ran a bit faster when the video was processed on its own and then recom-\nbined with the audio. However, your mileage may vary, so we recommend that\nyou test video transcoding with and without extracting the audio first. \nStep Functions is a workflow engine that is fairly customizable. It supports different\nstates like Task (this invokes a Lambda function or passes a parameter to the API of\nanother service) or Choice (this adds branching logic). \n The one important state that we’ll use is Map. This state takes in an array and exe-\ncutes the same steps for each entry in that array. Therefore, if we pass in an array with\ninformation on how to cut up a video into segments, Map runs enough Lambda func-\ntions to process all of those arguments in parallel. This is exactly what we are going to\nbuild. We will pass an array to a Split and Convert Lambda function using the Map\n",
      "page_number": 150
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 158-170)",
      "start_page": 158,
      "end_page": 170,
      "detection_method": "topic_boundary",
      "content": "142\nCHAPTER 8\nArchitecting serverless parallel computing\ntype. Step Functions will create as many functions as necessary to cut the original\nvideo into segments. \n Here comes the more interesting part. As soon as the segments are created, Step\nFunctions begins calling the Merge Video function until all segments are merged into\na new video. We’ll add some logic to the Step Functions execution workflow to figure\nout if Merge Video needs to be called. Once all Merge Video tasks are called and pro-\ncessed, Step Functions will take the result from Split Audio and from Merge Video\nand invoke the final Merge Video and Audio function. Figure 8.5 shows what this pro-\ncess looks like.\nNow that you know what the Step Functions workflow does, let’s discuss each of the\nLambda functions in more detail.\nSPLIT AUDIO\nStep Functions runs the Split Audio Lambda function to extract audio from the video\nfile. As we’ve mentioned, this step is done to accelerate the overall workflow because,\nfrom there on, the audio portion of the file isn’t considered, and only the video por-\ntion is transcoded to another bit rate. We don’t have to do this. We can leave audio\nAWS Step Functions workflow\nThis is the only tricky bit about \nStep Functions. The Merge Video \nfunction must repeat until all \nvideo segments have been merged.\nMerge\nvideo\nchoice\nTask: Choice\nTask: Pass\nAWS Lambda (Merge Video)\nThis logic is running \nusing the Map state.\nAWS Cloud\nAWS Lambda (Split and Convert Video)\nAWS Lambda\n(Split Audio)\nMerge\naudio\ntask\nAWS Lambda \n(Merge Video and Audio)\nStart\nFigure 8.5\nThe Step Functions execution workflow does all the work in our transcoder. The video is split, \nconverted, and merged again using two main functions and a bit of logic.\n\n\n143\nArchitecture deep dive\nand video together, but in our case, our testing showed that doing this improved the\noverall performance. The Split Audio function executes the following steps:\n1.\nExtracts audio using ffmpeg and saves it to a folder in EFS.\n2.\nUpdates the relevant DynamoDB record to record that this was done.\n3.\nReturns a Success message and additional parameters (like the location of the\naudio file) to the Step Functions orchestrator.\nAt a later stage, Step Functions invokes the Merge Video Audio function with the\nparameters that were returned by the Split Audio and Merge Video functions.\nSPLIT AND CONVERT VIDEO\nThe Split and Convert Video function splits the original video file into a segment and\nconverts that segment to a new bit rate or encoding. The original video file doesn’t get\nchanged in this process; instead, the function merely extracts a segment between a\nstart time and an end time, specified in the parameters that are passed to it. These\nparameters are worked out by the Transcode Video function. \n Many hundreds of Split and Convert Video functions can run in parallel. Here are\nthe main actions that it performs:\n1.\nUsing ffmpeg, the function creates a new video file from the original one.\n2.\nIt increments a confirmation counter in the appropriate DynamoDB record to\nspecify that the segment exists.\n3.\nIf the confirmation counter is equal to 2, it then returns to the Step Functions\nworkflow with a Merge message. Otherwise, it returns with a Success message,\nwhich stops the execution of that particular Step Functions parallel execution. \nYou may recall from the previous section that, with DynamoDB, each record rep-\nresents two neighboring segments. When a record counter is incremented to 2, the\nfunction knows that the two neighboring segments exist. The function returns a\nMerge message to Step Functions, and Step Functions knows that it can begin calling\nthe Merge Video for these segments. \nMERGE VIDEO\nStep Functions calls the Merge Video function when two neighboring segments are\nready to be merged into a new single segment. The merge operation happens using\nffmpeg, and the new segment is saved to EFS. Here’s what happens in a little more detail:\n1.\nThe Merge Video function is invoked with a number of parameters passed to it\nby Step Functions. These parameters include the left and the right segments.\n2.\nUsing ffmpeg, the left and right segments are merged to create a new segment.\nThis new segment is saved to EFS.\n3.\nDynamoDB confirmation is incremented. If there are two confirmations, then\nthe function returns to the workflow with a Merge message. \n4.\nHowever, if there are two confirmations and the last two remaining segments\nhave been merged, the function returns with a MergeAudio message to the\nworkflow.\n\n\n144\nCHAPTER 8\nArchitecting serverless parallel computing\nAs you can see, the Merge Video function creates a bit of a loop. It continues to merge\nsegments, returns the Merge message, and causes Step Functions to invoke itself\nagain. This happens until the last two segments are merged, then the return type is\nchanged to MergeAudio. This is when Step Functions knows that it’s time to combine\naudio and video and invokes the Merge Video and Audio function.\nMERGE VIDEO AND AUDIO\nThe final function is Merge Video and Audio. It takes input from the Split Audio and\nMerge Video functions and merges the audio and the new video files together using\nffmpeg. The new file is saved somewhere else (in another directory) on EFS. The\nfunction can also upload the new file to an S3 bucket for easier access.\n8.3\nAn alternative architecture\nYou can build this serverless transcoder without using EFS (or Step Functions for that\nmatter). In fact, our first iteration used only S3 and SNS to perform fan-out. We\nwanted to present you with an alternative architecture that shows that you don’t neces-\nsarily have to use Step Functions or EFS if you don’t want to. This section demon-\nstrates that you can use SNS and S3 instead to achieve the same outcome. It’s nice that\nAWS provides so many building blocks that we can build our desired architecture in\ndifferent ways. \nTIP\nOne reason for adopting a different architecture could be because you\ndon’t want to pay for Step Functions and EFS. That is a reasonable concern.\nUsing S3 is likely going to be much cheaper than using EFS and will probably\nperform just as well. Once you get the code working, using S3 is straightfor-\nward, and we don’t have a reason not to recommend it. Whether you should\nuse SNS instead of Step Functions is a tougher proposition. SNS is cheaper,\nbut you will lose a lot of the robustness and observability that you get with\nStep Functions. Perhaps the best solution is to use Step Functions with S3?\nWe’ll leave it to you as an exercise to achieve. \nThis alternative implementation closely resembles what we created in the previous sec-\ntion except, as we mentioned, we’ll replace Step Functions with SNS and EFS with S3.\nFigure 8.6 shows what this architecture looks like. \n This architecture works well, but there are some improvements that can be made\nto it. For one, the implementation should be improved in case of errors such as the\nsplit or merge operation failing. Luckily, there is the Dead Letter Queue (DLQ) fea-\nture of Lambda that allows us to save, review, and even replay failed invocations. If you\nwant a challenge, we invite you to implement DLQ for this architecture to make it\nmore resilient to errors. \n The second issue is observability and knowing what’s happening with the system.\nStep Functions provides some level of visibility, but things get a little bit harder with\nSNS. One tool you can use to help yourself is AWS X-Ray. This AWS service can help\nyou understand the interactions of different services within your system. It goes with-\nout saying that CloudWatch is essential too.\n\n\n145\nSummary\nSummary\nMapReduce can work really well with a serverless approach. Lambda invites you\nto think about parallelization from the start so take advantage of that.\nYou can solve a lot of problems in Lambda and process vast amounts of data by\nsplitting it up into smaller chunks and parallelizing the operations.\nStep Functions is an excellent service for defining workflows. It allows you to\nfan-out and fan-in operations. \nEFS for Lambda is an endless local disk—it grows as much as you need. You can\nrun applications with EFS and Lambda that you couldn’t have run before. Hav-\ning said that, S3 is still likely to be cheaper so make sure to do your calculations\nand analysis before choosing EFS.\nYou can solve problems in different ways: \n– You don’t have to use Step Functions because you can use SNS (although\nStep Functions adds an additional level of robustness and visibility). \n– You don’t need to use EFS because you can use S3. \nWhen coming up with an architecture for your system, explore the available\noptions because there will be different alternatives with different tradeoffs. \nAmazon Simple \nNotification\nService (fan-out)\nAmazon Simple \nNotification\nService\nAWS Lambda\n(Split Audio)\nAWS Lambda\n(Split and Covert\nSegments)\nAWS Lambda (final \nVideo and Audio Merge)\nSimple Storage \nService (S3)\n(final file)\nAWS Lambda\n(Merge Segments)\nS3 temp\nS3 temp\nStart here\nThis is DynamoDB hiding in the \nback. It’s used to maintain state.\nAWS Cloud\nSimple Storage Service (S3)\n(source file)\nAWS Lambda\n(Analyze Video)\nS3 temp\nAmazon Simple \nNotification\nService (fan-out)\nAmazon Simple \nNotification\nService\nS3 temp\nFigure 8.6\nThe SNS and S3 architecture for the serverless transcoder\n\n\n146\nCode Developer University\nOne idea that we’ve been mulling for a while has been a web app designed to help\ndevelopers learn programming skills in a fun way with gamification and useful ana-\nlytics. Our idea, let’s call it Code Developer University (CDU), evolved into a proof-\nof-concept website with a collection of interesting programming challenges for\nbudding developers to solve and to build skills. \n Each challenge would pose a problem. The student would have space to type in\ntheir solution and then submit it to our system for processing. The system would\nrun the solution through a battery of tests and decide whether the solution passed\nor failed. If the solution failed, the user would have a chance to update their code\nand resubmit again. If the solution passed, the user would advance to the next chal-\nlenge, receiving between 50 and 500 experience points (XP) based on the difficulty\nof the problem. \nThis chapter covers\nAWS Glue and Amazon Athena\nUsing EventBridge to connect system \ncomponents\nUsing Kinesis Firehose and Lambda for at-scale \ndata processing\n\n\n147\nSolution overview\n To make the entire experience more interesting and exciting, there would be ele-\nments of gamification baked-in throughout the system. For instance, experience\npoints would be used to create various leaderboards. That way, users interested in a\nfriendly competition would be able to compete for a top 10 position. The more chal-\nlenges solved, the higher the score. These leaderboards would show the overall top 10\nperformers and then the best performers for each language like Python or JavaScript. \n If a student wanted to dig into more data and perhaps see, search, and filter more\nadvanced reports, that would be supported too. A student could, for example, look at\nthe most common mistakes that other users make (anonymized, of course) and learn\nfrom that as well. \n The original idea was lofty but doable. The key to building this project would be to\nlean on as many different AWS services as possible. That way we could focus on the\nunique aspects of the system and leave the rest of the undifferentiated heavy lifting,\nlike authentication, to AWS. At the end, and as you will see, we used the following ser-\nvices to put everything together: \nEventBridge (messaging) \nGlue (data preparation and transformation) \nAthena (data querying) \nDynamoDB (database) \nQuickSight (reporting) \nS3 (storage) \nLambda and API Gateway \nIn this chapter, we focus specifically on data, leaderboards, and reporting for CDU.\nIt’s a fascinating part of the system because it uses so many parts of the AWS ecosystem\nand because you can build something similar just as rapidly yourself. Other features of\nCDU are quite standard for a web app. There are user accounts, an HTML5 user inter-\nface, and all the basic bolts and bits you would expect. If you want to learn how to\nbuild such a system yourself, take a look at the first edition of this book, which\ndescribes a similar, albeit video-focused web application.\n9.1\nSolution overview\nThe leaderboard and reporting aspect of CDU is interesting because it is serverless,\nscalable, and, frankly, fun to implement. There are many serverless AWS services that\nmake data collection, aggregation, and analysis possible without resorting to tradi-\ntional reporting and data-warehousing products of yesteryear. Let’s take a look first at\nthe requirements and then the overall solution.\n9.1.1\nRequirements listed\nCDU is a website with user registration and account features, and the ability for users\nto access and try code exercises and receive points if they are successful in implement-\ning and solving a coding challenge. To that end, the following sections provide a list of\nhigh-level requirements for CDU.\n\n\n148\nCHAPTER 9\nCode Developer University\nGENERAL\nThe user must be able to run their code solution and determine if it passes or\nfails the tests.\nIf the tests pass, then the solution is considered to be correct.\nA correct solution awards the user some number of points, which are saved to\nthe user’s profile. \nThere should be leaderboards and advanced reports for users to view.\nThe entire system must be serverless, event-driven, and as automated as much\nas possible (no intervention from the administrator should be needed to\nupdate leaderboards and reports).\nUSERS AND EXPERIENCE POINTS\nPoints are awarded for the programming language that is used to solve the chal-\nlenge. For example, if the user codes in Python, then they get points allocated\ntoward Python. If they use JavaScript, then points are allocated to their Java-\nScript score.\nThe user’s profile should show the overall score (sum of all previous points for\nall programming languages) and scores for each programming language indi-\nvidually. The user’s profile and scores should be updated in near real time.\nThe user shouldn’t receive points for the same challenge more than once.\nLEADERBOARDS\nCDU should feature a leaderboard that shows the top scorers across different\nprogramming languages (e.g., Python and JavaScript).\nAn overall leaderboard should show the top performers (regardless of the pro-\ngramming language) for last month, last year, and all time.\nLeaderboards don’t need to be updated in real time, however, but they should\nrefresh at least every 60 minutes. There should also be a way to refresh them on\ndemand by the administrator.\nREPORTS\nApart from the leaderboard, users should also have access to more in-depth\nreports that they can search and filter. \nThe exact implementation of the reports can be left to the data team; however,\na basic report could show the best performers, similar to the leaderboard.\nReports should be refreshed at least every 60 minutes but could also be\nrefreshed sooner (if needed) by the administrator.\nAny user, not just the administrator, should have access to the leaderboard.\n9.1.2\nSolution architecture\nLet’s now take a look at a possible architecture that ought to address our major\nrequirements. Figure 9.1 shows most of the major architectural pieces. These include\nthe following three main microservices:\n\n\n149\nSolution overview\nCode Scoring Service \nStudent Profile Service \nAnalytics Service\nWe will break down the solution in the coming sections, but let’s take a look at the\nhigh-level architecture shown in figure 9.1. The Code Scoring Service runs a Lambda\nfunction that processes submitted code. If it passes the test, information is sent across\nto the EventBridge, which invokes two other microservices: \nThe Student Profile Service updates the student’s profile in the database and\nadds to the student’s overall score. \nThe Analytics Service processes and stores the user’s test data in S3, which later\nenables the creation of the QuickSight dashboards.\nThere’s actually quite a bit that happens in the Analytics Service. It is covered in detail\nin section 9.4, but here’s a high-level overview of what actually takes place in this\nmicroservice: \nThe message (with the user’s solution) is pushed into Kinesis Firehose, which\nuses a Lambda function to modify the format of the message so that it can be\nprocessed later by other AWS services.\nRun Unit \nTest Lambda\nProcess Submission\nLambda\nSubmissions\nqueue\nResults EventBridge\nUpdate Student \nScore Lambda\nStudent database\nStudent scores \nKinesis Firehose\nProcess Firehose \nSubmission Lambda\nStudent scores\nbucket\nProcess results and\nupdate summary\ntables with Glue\nQuery summaries \nin Athena\n \nQuery \nLeaderBoard \nSummary Lambda\nProcess \nLeaderboard \nSummary Lambda\nQuickSight\ndashboard\nLeaderboard\ndatabase\nCode scoring service\nAnalytics service\nStudent profile service\nTests bucket\nAWS Cloud\nStudent database\nLesson database\nSchedule\n(run every \nhour)\nFigure 9.1\nThe architecture of Code Developer University (CDU) that’s responsible for scoring and \nleaderboards\n\n\n150\nCHAPTER 9\nCode Developer University\nKinesis then stores the newly processed message (as a JSON file) in an S3 bucket.\nAWS Glue runs on schedule, which is set to trigger every 60 minutes. When that\nhappens, Glue processes the aforementioned S3 bucket and updates a Glue\nData Catalog (think: a table with metadata) that points to the data stored in S3.\nGlue then triggers a Lambda function, which uses Amazon Athena to query the\ndata stored in S3 via the Glue Data Catalog.\nOnce Athena finishes, it triggers another Lambda function that gets the result\nof the query and updates the appropriate leaderboards saved in DynamoDB.\nFinally, there’s an Amazon QuickSight report that uses Athena to query the data\nin the S3 bucket when a user wants to see more information.\nThere’s a little more detail to all the services, and you may have other questions,\nwhich should be cleared up in coming sections. Read on!\n9.2\nThe Code Scoring Service\nThe purpose of the Code Scoring Service is to receive submitted code from the user and\nrun it against a set of tests. If tests pass, the Run Unit Test Lambda creates a submission,\nwhich it puts into the submissions queue. The submission is picked up from the queue\nby the Process Submission Lambda and is enriched with data from a couple of Dyna-\nmoDB tables. Finally, the Process Submission Lambda pushes the newly enriched mes-\nsage on to Amazon EventBridge for consumption by other services in our system. \n The actual design of the Code Scoring Service is fairly straightforward, but let’s\ntake a look at its design in more detail. Figure 9.2 shows a closeup of the architecture\nbeginning with the Run Unit Test Lambda. This Run Unit Test Lambda function is\ninvoked via HTTPS (via the API Gateway) and receives a zip payload as part of the\nQuickSight vs. DynamoDB\nOne question you may be asking yourself is why are we using Amazon QuickSight for\nreporting and also storing leaderboards in DynamoDB? Isn’t that redundant? The\nreason is that QuickSight is heavy, powerful, but also slow. You can integrate it into\nyour website (in an iFrame), but it takes a long time to load. If you are committed to\nusing QuickSight to explore data in detail, then you’ll wait for 10 or 20 seconds. But\nif you want to see results quickly, waiting for it can be unbearable (AWS, please look\nat performance!). \nThis is why we store important leaderboard results in DynamoDB, which can be\nloaded and displayed to the user quickly. Then it’s up to the user to choose to see\nthe QuickSight version of this data, especially if they need more detail. You can think\nof our DynamoDB leaderboards as an informal cache for QuickSight. \nThe negative aspect of this implementation is that the DynamoDB table and the data\nin QuickSight must be synchronized. If the Student Profile Service updates the stu-\ndent’s score, but the Analytics Service fails, DynamoDB may end up showing some-\nthing different to QuickSight. There are ways to fix this though. What would you do?\nWe will discuss this in section 9.3.\n\n\n151\nThe Code Scoring Service\nrequest body. The zip payload contains the user’s code submission and metadata, such\nas what challenge the user is attempting and what programming language is being\nused. The Lambda function looks up the appropriate test in the Tests bucket (it knows\nwhich test to grab based on the lesson name) and downloads that test file from S3.\nNow the Lambda function can execute the appropriate interpreter or compiler, run\nthe unit test, and test the user’s submission.\nPrevent tight coupling of Lambda\nfunctions by putting in a queue\nbetween two Lambda functions.\nEventBridge will push out a \nmessage to two other \nmicroservices.\nAWS Cloud\nRun Unit Test \nLambda\nProcess Submission\nLambda\nSubmissions\nqueue\nResults EventBridge\nCode Scoring Service\nTests bucket\nStudent database\nLesson database\nFigure 9.2\nThe Code Scoring Service runs the user’s code and, if it’s successful, kicks of \nthe rest of the chain of events in our system.\nLambda layers\nIf you want to support multiple languages like Python, JavaScript, C++, C#, Java, and\nso on, use Lambda layers. A layer is a zip file that can contain additional libraries or\ncustom runtimes. \nYou can have a Lambda layer with a Python interpreter or a layer with a C compiler.\nMoreover, you deploy layers separately from Lambda functions, thus keeping your\nactual Lambda functions small. At run time, as long as it’s configured correctly, your\nLambda functions can access the contents of your layers (which are extracted to the\n/opt directory in the function execution environment). You can deploy as many layers\nas you like, but know that Lambda can only use up to five layers at a time. You can read\nmore about them at http://mng.bz/doJO.\n\n\n152\nCHAPTER 9\nCode Developer University\nThe Run Unit Test Lambda by itself is not particularly complex. It needs to know how\nto run a unit test and then parse the result to figure out if it passed successfully or not.\nIf the test failed, then the function sends back an HTTP response with the output\nfrom the interpreter or the compiler. Thus, the user can see the error message, fix the\ncode issue, and resubmit. Otherwise, if it passes, the function sends back a celebratory\nmessage to the user and places a message containing the user’s submission on the\nSubmissions SQS queue for further processing.\n9.2.1\nSubmissions Queue\nThe Submissions Queue is an SQS queue that sits between the only two Lambda func-\ntions in this service. When a message is placed in the queue, it leads to an invocation\nof the Process Submission Lambda that retrieves the message and enriches it with\nmore data before pushing it to the EventBridge. There are a few reasons we do this,\nincluding the following:\nOne of the requirements is to prevent the user from receiving points for the\nsame challenge multiple times. The Process Submission Lambda needs to look\nup the Student DynamoDB table to figure out whether the user has already\ncompleted this challenge. If the user has already completed that challenge,\nthen that is noted, and no points are earned.\nAssuming that the student has solved the challenge for the first time and is sup-\nposed to receive points, the Process Submission Lambda also looks up how\nmany points should be awarded from the Lesson database.\nAll of this information, including the message that came from the queue, is\ncombined and pushed to Amazon EventBridge.\nBy now you might be thinking, “Why not do everything in the initial Run Unit Test\nLambda?” The reason is to separate responsibility. The Run Unit Test function is\nintended to run code and figure out if it passes a test. The second Process Submission\nLambda function has to perform database lookups and evaluate whether the student\nshould be awarded points. As a rule of thumb, you should use multiple Lambda functions\nwhen you are dealing with different concerns rather than having everything lumped into\none. Hence, this is the reason we created two functions and introduced a message queue\nbetween them.\n Another question you may have is why we used SQS rather than have functions call\none another directly. Our recommendation is never to have functions call each other\ndirectly unless you are using a feature called Lambda Destinations (which adds a hid-\nden queue between two functions anyway). Lambda Destinations, however, only works\nfor asynchronous invocations, so it wouldn’t have been possible in our case. The Run\nUnit Test Lambda was invoked synchronously via HTTP. The reason for having a queue\nbetween two functions is to reduce coupling (e.g., the two functions have no direct\nknowledge of one another) and to have an easier time handling errors and retries.\n\n\n153\nStudent Profile Service\n We also could have chosen to use Amazon EventBridge instead, but SQS was\nacceptable in this scenario. And, if we ever wanted to enable First-In First-Out (FIFO)\nqueues at a later stage, we’d need to use SQS because EventBridge doesn’t support\nthis feature, so that further weighed our decision. \n The last action performed by the Process Submission Lambda is to push the mes-\nsage to Amazon EventBridge. As you may recall, this message contains the original\nsubmission made by the user together with additional details that consists of informa-\ntion on whether the experience points should be awarded to the user and the amount\nof those points (this information was obtained by looking up a couple of tables in the\nProcess Submission Lambda function). \n9.2.2\nCode Scoring Service summary\nThe Code Scoring Service is a relatively trivial service apart, perhaps, from running\nthe code provided by the user. Even then it’s not too difficult to unzip a file and run\nan interpreter (or a compiler) within Lambda. One important thing to mention is\nsecurity. If you are running someone else’s code in a function, you must be prepared\nthat someone will try to subvert it, find a vulnerability to exploit, and do something\nbad. Therefore, you must follow the principle of least privilege and disallow anything\nthat isn’t critical to the running of your function. This should be a rule for all Lambda\nfunctions, but in this instance, you should be doubly careful and vigilant.\n9.3\nStudent Profile Service\nThe Student Profile Service is small. Its purpose is to increment the number of experi-\nence points in the student record in the DynamoDB table. That way, the student can\nimmediately see the cumulative score added to their tally and feel good about their\nachievement. This service consists of a single Lambda function that communicates\nwith DynamoDB. This function receives an event from EventBridge, reads it, and\nupdates the user profile if the user has received any points. Figure 9.3 shows what this\nbasic service looks like.\n You may remember that earlier (in section 9.1), we posed a question about keep-\ning different tables in sync. Given that the Student Profile Service and the Analytics\nAmazon EventBridge\nAmazon EventBridge is a serverless event bus that can connect different AWS (and\nnon-AWS) services. It has a few great features that services like SQS, SNS, and Kine-\nsis do not possess. Chief among them is the ability to use more than 90 AWS ser-\nvices as event sources and 17 services as targets, automated scaling, content-\nbased filtering, schema discovery, and input transformation. But like any other tech-\nnology, it has certain deficiencies like no guarantee on ordering of events or buffering.\nAs always, what you end up choosing should depend on your requirements and the\ncapabilities of the product you are using.\n\n\n154\nCHAPTER 9\nCode Developer University\nService store similar data (namely the user’s score), what happens if one of the ser-\nvices goes down and falls out of sync with the other service? In other words, if there’s a\nfault in a service that causes a data mismatch, what can we do about it? There are a\nnumber of solutions you can think about implementing to address this problem:\nSerial invocation—One approach is to make the Analytics Service and the Stu-\ndent Profile Service run in serial rather than parallel. That way your system\nwould update the Student Profile Service first and then run through the update\nprocedure in the Analytics Service (invoking it via another EventBridge). If the\nAnalytics Service fails, the system would roll back the change in the Student\nProfile Service, and both services would continue operating in sync.\nOne source of truth—Alternatively, you could make the Analytics Service your\nsource of truth and then simply copy the data over to the Student Profile Ser-\nvice. That way you could even delete all data in the Student Profile Service and\nregenerate it as many times as necessary from the Analytics Service.\nShare the database—Both services could read and write to the same database. That\nwould avoid some problems, but then, we no longer have a microservices archi-\ntecture in which each service is responsible for its own view of the world. We\nwould end up with a distributed monolith. It must be mentioned that in many cir-\ncumstances having a distributed monolith is a fine and acceptable solution.\nThis is a basic service with one Lambda\nupdating a DynamoDB table. The Lambda\nfunction is invoked by the EventBridge.\nStudent Profile Service\nResults EventBridge\nStudent database\nUpdate Student Score Lambda\nAWS Cloud\nFigure 9.3\nThe Student Profile Service is the simplest one in this entire architecture. It’s a Lambda \nfunction that writes to a Dynamo table.\n",
      "page_number": 158
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 171-179)",
      "start_page": 171,
      "end_page": 179,
      "detection_method": "topic_boundary",
      "content": "155\nStudent Profile Service\nOrchestrator—Another approach is to have an orchestrator sit above the two services\nand monitor what is happening. If there is an error, the orchestrator could run addi-\ntional actions to compensate for the issue (for example, retry or roll back). \nQuite frankly, this is a common situation with a microservices-based approach. How do\nyou keep services in sync without having all microservices coupled to a central data-\nbase? There are different solutions to this problem but, as with anything in software\nengineering, they all have different trade-offs. In the case of CDU, we decided to update\nboth services in parallel. If an issue were to occur, we would use the Analytics Service as\nour source of truth and regenerate the data needed by the Student Profile Service.\n9.3.1\nUpdate Student Scores function\nThe Update Student Score function is shown in listing 9.1. It performs three primary\nactions:\nIt parses the event received from the EventBridge that has the scores/data.\nUpdates the amount of XP gained for the topic like JavaScript or Python.\nUpdates the total amount of XP earned by the user.\n'use strict';\nconst AWS = require('aws-sdk');\nconst sns = new AWS.SNS();\nconst dynamoDB = require('aws-sdk/clients/dynamodb');\nconst doc = new dynamoDB.DocumentClient();\nconst updateTotalXP = (record, lessons) => {\n    const date = new Date(Date.now()).toISOString();\n    const xp = lessons.filter(m => m.xp)\n    ➥ .map(m => m.xp)\n    ➥ .reduce((a, b) => a+b); \n    const params = {    \n        TableName: process.env.USER_DATABASE,\n        Key: {\n           userId: record.username\n        },\n        UpdateExpression: `set \\\n                           modified = :date, \\\n                           xp.#total = :xp`, \n        ExpressionAttributeNames: {\n           '#total': 'total'\n        },\n        ExpressionAttributeValues: {\n           ':date': date,\n           ':xp': xp\n        },\nListing 9.1\nUpdating the Student Score Lambda\nCalculates the total XP for a user by summing \nup the XP for all of the lessons. This is woefully \ninefficient to do each time but OK for an example. \nCan you think of a better way?\nThis params object has all the necessary \nattributes needed to update the relevant \nDynamoDB table. Note the ‘total’ in the \nExpressionAttributeNames. It’s a reserved \nkeyword so it has to be specified using \nExpressionAttributeNames.\n\n\n156\nCHAPTER 9\nCode Developer University\n        ReturnValues: 'ALL_NEW'\n    };\n    return doc.update(params).promise(); \n}\nconst updateTopicXP = (record) => {\n    const date = new Date(Date.now()).toISOString();\n    const lesson = {\n        lesson: record.lesson,\n        topic: record.topic,\n        modified: date,\n        xp: record.xp,\n        isCompleted: record.isCompleted\n    };\n    const params = {\n        TableName: process.env.USER_DATABASE,\n        Key: {\n            userId: record.username\n        },\n        UpdateExpression: `set \\\n                          modified = :date, \\\n                          lessons = list_append(if_not_exists(lessons,\n    ➥ :empty_list), :lesson), \\     \n                          xp.${record.topic} = \n    ➥ if_not_exists(xp.${record.topic}, :zero) + :xp`,\n        ExpressionAttributeValues: {\n            ':lesson': [ lesson ],   \n            ':empty_list': [],       \n            ':zero': 0,\n            ':date': date,\n            ':xp': parseInt(record.xp, 10)\n        },\n        ReturnValues: 'ALL_NEW'\n    };\n    return doc.update(params).promise();\n}\nexports.handler = async (event, context) => {\n    try {\n        const record = event.detail; \n        if (record.isCompleted) {\n           const user = await updateTopicXP(record);\n           if (user.Attributes.lessons.length > 0) {\n               await updateTotalXP(record, user.Attributes.lessons);\n           }\n        }\n    } catch (error) {\n         console.log(error);\n    }\n}\nThis update expression appends \na lesson to a list of lessons in \nDynamoDB. Otherwise, if a list \ndoesn’t exist, a new and empty \none is created.\nThis function is invoked via the \nEventBridge. The parameter event.detail \ncontains the information that was sent \nover from the Process Submission Lambda \nfunction in the previous section.\n\n\n157\nAnalytics Service\nThe Student Profile Service is a small microservice with a single Lambda function. Its\npurpose is to update a DynamoDB table and that’s pretty much as basic as you can get.\nThe next service, however, is not as straightforward. Let’s take a look at it now.\n9.4\nAnalytics Service\nThis is going to be a big one, so grab yourself a tea or a coffee before jumping in. If\nyou recall, the purpose of the Analytics Service is twofold:\nEnable the creation and display of QuickSight dashboards.\nMaintain leaderboards in DynamoDB that could be quickly accessed and read.\nThe data collected and processed by the Analytics Service must enable us to achieve\nthose two aims. Let’s take a look at the architecture in figure 9.4. The steps that the\nAnalytics Service takes are as follows:\n1.\nThe EventBridge service pushes a message from the Code Scoring Service on to\nthe Student Scores Kinesis Firehose.\n2.\nThe Firehose runs a Lambda that processes and transforms each incoming mes-\nsage into a format that is palatable for Amazon Glue to work on later.\n3.\nAfter the message is transformed by Lambda, Firehose stores it in an S3 bucket.\n4.\nEvery hour (or on demand) AWS Glue runs and crawls the messages stored in\nthe S3 bucket. It updates a table within the AWS Glue Data Catalog with the\nmetadata based on the crawl.\n5.\nOnce Glue is finished processing, the Query Leaderboard Summary function is\nrun. The Lambda function invokes Athena that runs a query to work out the\nleaderboard.\n6.\nAthena accesses Glue and S3 and extracts the relevant data for the query. Once\nthe query is complete, the Process Leaderboard Summary Lambda is invoked.\n7.\nThis Process Leaderboard Summary Lambda function receives the result of the\nquery from Athena, reads it, and updates the Leaderboard DynamoDB table.\n8.\nFinally, the QuickSight Dashboard component uses Athena to execute queries\nbased on what the user is trying to see in a QuickSight report.\nYou may agree that this is quite a lot to take in one go, so let’s break down the most\ninteresting components. We’ll do that in the following sections.\n \n \n \n \n \n \n \n \n \n\n\n158\nCHAPTER 9\nCode Developer University\n9.4.1\nKinesis Firehose\nKinesis Firehose provides a way to capture and stream data into Elasticsearch, Red-\nshift, and S3. AWS says that it’s the “. . . easiest way to reliably load streaming data into\ndata lakes, data stores, and analytics services” (https://aws.amazon.com/kinesis/data\n-firehose/), which sounds perfect for our use case. Kinesis Firehose, unlike other\nKinesis services, is serverless, meaning that you don’t need to worry about scaling par-\ntitions or sharding as is the case with, say, Kinesis Data Stream. It is all done for you\nautomatically. Another nice feature of Firehose is that it can run Lambda for messages\nas they are ingested. Lambda can be used to convert raw streaming data to other,\nmore useful, formats and this is exactly what we would do. In our use case, we can use\na Lambda function to convert the messages to a JSON format that would later be read\nby the AWS Glue service before storing them in S3.\n Listing 9.2 shows a Kinesis Firehose processing function that takes a message, pro-\ncesses it, creates a new record with a different set of fields, and pushes it back to Fire-\nhose for storage in S3. In this listing, we are extracting only a few properties from the\noriginal message because we don’t want to keep everything. For example, we may dis-\ncard the user’s submitted source code because we care only if they’ve passed the test\nor not. There are a few things to keep in mind in this listing:\nKinesis stores data in S3.\nGlue reads from the bucket\nand creates a data catalog\nthat can be queried by\nAthena.\nRun Glue \nOn-Demand Lambda\nResults EventBridge\nStudent scores \nKinesis Firehose\nProcess Firehose \nSubmission Lambda\nStudent scores\nbucket\nProcess results and\nupdate summary\ntables with Glue\nQuery summaries \nin Athena\n \nQuery \nLeaderboard \nSummary Lambda\nProcess \nLeaderboard \nSummary Lambda\nQuickSight\ndashboard\nLeaderboard\ndatabase\nAWS Cloud\nSchedule\n(run every \nhour)\nFigure 9.4\nThe Analytics Service architecture includes Glue, Athena, DynamoDB, Kinesis Firehose, \nQuickSight, and Lambda.\n\n\n159\nAnalytics Service\nAll transformed records must contain a recordId, result, and data. Other-\nwise, Kinesis Firehose rejects the entire record and treats it as a “transformation\nfailure.”\nThe property called recordId is passed from Firehose to Lambda. The trans-\nformed record has to contain the same recordId as the original. A mismatch\nresults in transformation failure (so, don’t make your own or append anything\nto it).\nThe property result must either be Ok (record transformed) or Dropped (record\nwas dropped intentionally). The only other allowed value is ProcessingFailed\nif you want to flag that the transformation couldn’t take place.\nThe property data is your base-64 encoded transformed record. \n'use strict';\nexports.handler = (event, context) => {\n    let records = [];\n    \n    for (let i = 0; i < event.records.length; i++) {\n        const payload = Buffer.from(\n        ➥ event.records[i].data, 'base64')\n        ➥ .toString('utf-8');                 \n        const data = JSON.parse(payload);      \n        const record = {   \n          username: data.detail.username,\n          name: data.detail.user.name,\n          lesson: data.detail.lesson,\n          topic: data.detail.topic,\n          xp: data.detail.xp,\n          hasPassedTests: (data.detail.hasPassedTests || false),\n          runTests: (data.detail.runTests || false),\n          isCompleted: (data.detail.isCompleted || false),\n          time: data.time,\n        };\n        records.push({  \n            recordId: event.records[i].recordId,\n            result: 'Ok',\n            data: Buffer.from(JSON.stringify(record)).toString('base64')\n        });\n    }\n    console.log(`Return: ${ JSON.stringify({records}) }`);\n    return Promise.resolve({\n        records\n    });\n};\nListing 9.2\nKinesis Firehose processing function\nThe original message that was \npushed to Firehose. You can \nnow extract the relevant bits \nyou might want to save in S3.\nThe record you create here \nand store in S3 will be JSON.\nAll transformed records must contain a property \ncalled recordId, result, and data. Transformation \nis the ultimate goal for this Lambda.\n\n\n160\nCHAPTER 9\nCode Developer University\nFinally, you must ensure that your response doesn’t exceed 6 MB. Otherwise, Firehose\nwill refuse to play along.\n9.4.2\nAWS Glue and Amazon Athena\nAWS Glue is a serverless ETL (extract, transform, and load) service that can scour an\nS3 bucket with a crawler and update a central metadata repository called the Glue\nData Catalog. You and other services can then use this metadata repository to quickly\nsearch for relevant information among the records scattered in S3. Glue never actu-\nally moves or copies any data. The tables with metadata it creates in the Glue Data Cat-\nalog point to the data in S3 (or other sources like Amazon Redshift or RDS). This\nmeans that the Data Catalog can be recreated from the original data if necessary.\n Amazon Athena is a serverless query service that can analyze data in S3 using stan-\ndard SQL. If you haven’t tried Athena, you have to give it a go. You simply point it to\nS3, define the schema, and begin querying using SQL. What’s even nicer is that it inte-\ngrates closely with Glue and its Data Catalog (which takes care of the schema). Once\nyou have AWS Glue configured and the Data Catalog created, you can begin querying\nAthena immediately.\n Listing 9.3 shows how to perform a query. An important thing to note is that the\nquery is asynchronous. You will not get a response once you’ve run it. You have to start\nthe query execution and then, using CloudWatch events, react to when you get the\nresult. Luckily everything can be accomplished with two Lambda functions. Listing 9.3\nshows how to execute a query and listing 9.4 shows how to process it if you have\nhooked up CloudWatch events to respond.\n'use strict';\nconst AWS = require('aws-sdk');\nconst athena = new AWS.Athena();\nconst runQuery = (view) => {\n    const params = {\n        QueryString: `SELECT * FROM \"${view}\"`, \n        QueryExecutionContext: {\n            Catalog: process.env.ATHENA_DATA_SOURCE,  \n            Database: process.env.ATHENA_DATABASE    \n        },\n        WorkGroup: process.env.ATHENA_WORKGROUP       \n    };\n    return athena.startQueryExecution(params).promise();\n}\nexports.handler = async (event) => {\n    let promises = [];\nListing 9.3\nQuery Leaderboard Summary Lambda\nViews are supported by Athena \nand are as useful as regular SQL.\nParameters such as the Catalog, \nDatabase and WorkGroup are \nset up in Athena when you \nconfigure it.\n\n\n161\nAnalytics Service\n    promises.push(runQuery(process.env\n    ➥ .ATHENA_LEADERBOARD_VIEW_TOPICS));  \n    promises.push(runQuery(process.env\n    ➥ .ATHENA_LEADERBOARD_VIEW_OVERALL)); \n    const query = await Promise.all(promises);\n    console.log('Athena Query Id', query);\n}\nListing 9.4 shows a Process Leaderboard Lambda function that responds to a Cloud-\nWatch event that contains information about the query performed in listing 9.3. Note\nthat the actual result (meaning the data itself) must be retrieved from Athena using\nthe GetQueryResults API call. When the Process Leaderboard Summary function is\ninvoked, only queryExecutionId is passed into it, but that’s enough to perform the\nGetQueryResults API call to get the data. The code in the following listing is quite\nlengthy because, apart from showing how to get a result out of Athena, it demon-\nstrates how to update a DynamoDB table.\n'use strict';\nconst AWS = require('aws-sdk');\nconst athena = new AWS.Athena();\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\nconst getQueryResults = (queryExecutionId) => {\n    const params = {\n        QueryExecutionId: queryExecutionId\n    };\n    return athena.getQueryResults(params).promise();\n}\nconst updateDynamoLeaderboard = (rows, index) => {\n    let transactItems = [];\n    const date = new Date(Date.now()).toISOString();\n    //\n    // Skip the first row because it's the label\n    // Data: [\n    //   { VarCharValue: 'topic' },\n    //   { VarCharValue: 'username' },\n    //   { VarCharValue: 'name' },\n    //   { VarCharValue: 'score' },\n    //   { VarCharValue: 'rn' }\n    // ]\n    //\n    for (let i = 0; i < rows.length; i++) {\n        const row = rows[i].Data;\nListing 9.4\nProcess Leaderboard Summary Lambda\nViews are supported by Athena \nand are as useful as regular SQL.\nYou need the \nQueryExecutionId to run \nGetQueryResults, then the \nresult of the query is yours.\n\n\n162\nCHAPTER 9\nCode Developer University\n        const params = {\n            TableName: process.env.LEADERBOARD_DATABASE,\n            Key: {\n                uniqueId: row[1].VarCharValue, //username\n                type: row[0].VarCharValue //topic\n            },\n            UpdateExpression: `set \\\n                                #name = :name, \\\n                                modified = :date, \\\n                                #rank = :rank,\n                                score = :score`,\n            ExpressionAttributeNames: {\n                '#name': 'name',\n                '#rank': 'rank'\n            },\n            ExpressionAttributeValues: {\n                ':date': date,\n                ':name': row[2].VarCharValue,\n                ':score': parseInt(row[3].VarCharValue, 10),\n                ':rank': parseInt(row[4].VarCharValue, 10)\n            },\n            ReturnValues: 'ALL_NEW'\n        }\n        transactItems.push({Update: params});\n    }\n    return dynamodb.transactWrite({TransactItems:transactItems}).promise();\n}\nexports.handler = async (event) => {\n    try {\n        if (event.detail\n            ➥ .currentState === 'SUCCEEDED') { \n            const queryExecutionId = \n            ➥ event.detail.queryExecutionId;\n            const result = \n            ➥ await getQueryResults(queryExecutionId);\n            result.ResultSet.Rows.shift();  \n            if (result.ResultSet.Rows.length > 0) {\n                const maxItemsPerTransaction = 20;  \n                for (let i = 0; i < \n    ➥ result.ResultSet.Rows.length/maxItemsPerTransaction; i++) {\n                    const factor = \n    ➥ result.ResultSet.Rows.length/maxItemsPerTransaction;\n                    const remainder = \n    ➥ result.ResultSet.Rows.length%maxItemsPerTransaction;\nWe only ever want to retrieve \nthe results and save them if \nthe query executes successfully. \nLuckily, this parameter checks \nif it’s all good.\nThe first row in the array \ncontains labels for the \ncolumns (e.g., topic, score, \netc.). Shifting that row \nremoves it because we are \nonly interested in the values.\nUpdates in chunks of 20 items. \nDynamoDB can handle 25 items \nin a transaction, but we only do \n20 here instead.\n\n\n163\nAnalytics Service\n                    let data = \n    ➥ result.ResultSet.Rows.slice(i*maxItemsPerTransaction, \n    ➥ i*maxItemsPerTransaction + Math.max(maxItemsPerTransaction, \n    ➥ remainder));  \n                    const update = await updateDynamoLeaderboard(data, \n    ➥ i*maxItemsPerTransaction);\n                    \n                }\n            }\n        } else {\n            console.log('Query Unsuccessful');\n        }\n    } catch (error) {\n        console.log(error);\n    }\n}\nServerless architectures are typically push-based and event-driven. You should try to\navoid polling whenever you can. We could have polled for the status of the query and\nthen called the Lambda function to process the result, but it would have been more\ncomplex and error prone. Instead, we rely on CloudWatch events to get notified\nabout the query state transition. Interestingly, this feature wasn’t always available, and\npeople had to poll. There really was no other option, so it’s good to see AWS adding\nthe necessary support and enabling our serverless dream to continue.\n9.4.3\nQuickSight\nAmazon QuickSight is AWS’s Business Intelligence (BI) service in the vein of Tableau.\nYou can use it to build dashboards of all kinds and embed them into your website.\nQuickSight has some really interesting features, like its ability to formulate answers\nusing natural language (this is underpinned by machine learning). \n Truth be told, however, at the time of writing, QuickSight is an underwhelming\nAWS service. It’s slow, reasonably pricey, and weirdly different enough from other\nAWS services to necessitate a steeper learning curve. Nevertheless, it is also serverless,\nand it allows us to stay within the AWS environment, which is an advantage. We hope\nthat AWS substantially improves QuickSight over the coming months and years. If you\nare looking for a BI solution, you should have a look at QuickSight but evaluate other\noptions too. \n We used QuickSight to create dashboards that read data straight from S3 via Athena\nfor the CDU. Describing how to use QuickSight is out of scope for this chapter, but it\ndoes have a fairly intuitive interface that you can click through. QuickSight isn’t\nsupported by CloudFormation (at least at the time of writing this in the second half of\n2021), so creating consistent, repeatable dashboards is challenging and that’s a\nbummer. However, if your data is in S3 and can be queried with Athena, you can always\nWe use a little bit of math to retrieve the\nnecessary records (slice) from the array.\nThis formula gets 20 or fewer rows to\nstore in DynamoDB at a time.\n",
      "page_number": 171
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 180-187)",
      "start_page": 180,
      "end_page": 187,
      "detection_method": "topic_boundary",
      "content": "164\nCHAPTER 9\nCode Developer University\nrecreate your dashboards. The main thing is having the data in the right format and\nplace, which you will have with the tools described in this chapter.\n In summary, to build an Analytics Service, AWS services such as Kinesis Firehose,\nAthena, and Glue can be what you need. These are serverless services, meaning that\nyou don’t have to think about scaling or managing them the same way that you’d need\nto think about Amazon Redshift. Nevertheless, if you decide to embark on a serverless\njourney with these services make sure to do your evaluation first. \nAre they capable of meeting all of your requirements? \nIs there a situation where, in your case, Amazon Redshift may be better? \nAthena’s charges are based on the amount of data scanned in each query; Redshift is\npriced based on the size of the instance. There could be circumstances where Athena\nis cheaper, but Redshift is faster, so you should spend a little bit of time with Excel pro-\njecting cost. Nevertheless, in many cases, especially for smaller data sets, the combina-\ntion of Athena and Glue is more than enough for most needs.\nSummary\nAWS has a variety of services and ways to capture, transform, analyze, and report\non data relevant to your application. \nCapturing, processing, and reporting on data using services such as Event-\nBridge, DynamoDB, Amazon Glue, Amazon Athena, and Amazon QuickSight\nto build a web application with three microservices leaves us with a few take-\naways, including the following: \n– Amazon QuickSight is slow (and it can be expensive). If you need to show\nleaderboards, cache them in something like DynamoDB for quick retrieval.\n– Glue and Athena are fantastic tools. Glue can index the data stored in S3,\nand Athena can search across it using standard SQL. The result is less “lift-\ning” and coding for you.\n– Kinesis Firehose has a fantastic feature that allows you to modify records\nbefore they get to whatever destination they are going to. This is a fantastic\nfeature that’s worth the price of admission.\n– Do not have Lambda functions call each directly unless you are using\nLambda Destinations. Always use a queue like SQS or EventBridge if Lambda\nDestinations is not available.\n– EventBridge is an excellent message bus for use within AWS. Apart from not\nhaving FIFO functionality (this could change by the time you read this), it\nhas a ton of excellent features, and we highly recommended it.\n\n\nPart 4\nThe future\nThe last two chapters of this book are really fun. The next chapter is on the\nAWS Lambda internals and is fascinating for anyone wanting to know how\nLambda works. The last chapter of this book is about emerging practices. It cov-\ners the usage of multiple AWS accounts, temporary CloudFormation stacks,\nmanagement of sensitive data, and the use of EventBridge in event-driven archi-\ntectures. These two chapters are some of our favorites. We hope you like reading\nthem as much as we loved working on them.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n167\nBlackbelt Lambda\nPerformance (how fast your application responds) and availability (whether or not\nyour application provides a valid response) are critical aspects of your end user expe-\nrience. When using serverless architectures, your performance also has a direct\nimpact on your costs; for example, AWS Lambda bills you for the duration your func-\ntion runs, weighted by the memory you assign to it. Serverless architectures elimi-\nnate many of the common surface areas for performance optimizations, like scaling\navailable servers or tweaking server configurations, which can make it challenging\nfor new users to understand how to go about making these optimizations. \n This chapter introduces you to key tools and approaches available to you to\nimprove performance across the various services that make up your serverless appli-\ncation. We’ll use relevant examples to demonstrate how these techniques work.\n10.1\nWhere to optimize?\nBefore we delve into how we optimize serverless architectures, let’s quickly recap\nhow to think about them. Serverless architectures have multiple conceptual layers\nThis chapter covers\nMonitoring latency, request per second, and \nconcurrency for serverless applications\nTechniques for optimizing latency\n\n\n168\nCHAPTER 10\nBlackbelt Lambda\nas figure 10.1 illustrates. Endpoints are responsible for secure interactions with your\nend users and devices, and for the ingress of requests or events to your application\nfrom the end user. Examples of endpoints you can use in your AWS Serverless archi-\ntecture include API Gateway, AWS IoT, Amazon Alexa (if you were building an Alexa\nskill), or even just the AWS SDK. \n The compute layer of your workload manages requests from external systems\n(received through the endpoints), while controlling access and ensuring requests are\nappropriately authorized. It contains the run-time environment that deploys and runs\nyour business logic embodied as Lambda functions (we’ll delve into this shortly). \n The data layer of your workload manages persistent storage from within a system. It\nprovides a secure mechanism to store states that your business logic will need. It also\nprovides a mechanism to trigger events in response to data changes, which in turn can\nfeed into other parts of your business logic. As you can imagine, this is a broad surface\narea to discuss optimizations across, so we’ll focus on the following points, highlighted\nin figure 10.1:\nFunctions\nInvocations of these functions (either via requests from endpoints or events\nfrom backend systems)\nInteractions the functions have with downstream resources\nNow that you have a conceptual understanding of the various points of optimization,\nlet’s look at the tools available to do so. We’ll discuss those in the following sections.\nExample:\nAmazon API Gateway\nAWS IoT Amazon Alexa\nExample:\nAny public API\nService running on EC2\nExample:\nAmazon SQS\nAWS Step Functions\nAmazon SNS\nExample:\nAmazon S3\nAmazon DynamoDB\nMongo on EC2\nEndpoint services\nBusiness logic\nFunctions\nFunctions\nOther services and APIs\nMessaging and workflow\ncomponents\nData services\nFigure 10.1\nConceptual architecture of a serverless application\n\n\n169\nBefore we get started\n10.2\nBefore we get started\nTo effectively optimize applications, there are certain tools and concepts we must be\nfamiliar with. In this section, we will recap what happens when a Lambda function\nexecutes and how it impacts latency, how to observe the latency and contributors to it,\nand how to generate load to a function to get enough sample data.\n10.2.1 How a Lambda function handles requests\nTo understand how to optimize functions, we need to have a shared understanding of\nhow Lambda goes about executing our functions. Let’s use an example to illustrate what\nhappens when a function is deployed. We’ll use the image-resizer-service application\nfrom the Serverless Application Repository (http://mng.bz/WBy4) for reference. This\nserverless application deploys a Lambda function (written in Node.js) and an API Gate-\nway to your AWS account in the US East (N. Virginia)east-1 region that reads images\nfrom a S3 bucket (whose name is defined at deployment) and serves them through the\nAPI Gateway. The function uses the ImageMagick library to process the image. \nNOTE\nYou need to specify a new bucket name for the application to use. Use\nthe name “image-resizer-service-demo” for this example. \nOnce deployed, click the Test App button on the page, and it will take you to the\nApplications list view on the Lambda console, where you’ll see the newly deployed\napplication. In figure 10.2 these are marked as (1) and (2), respectively.\nTo test the application, you need to navigate to the main function. Click the applica-\ntion and on the detailed view (figure 10.3); click the image’s ResizeFunction (1) to\naccess the function. \nFigure 10.2\nThe Applications view shows all deployed services and applications.\n\n\n170\nCHAPTER 10\nBlackbelt Lambda\nOnce you select the function, you are taken to the Function Overview page (figure\n10.4). Here you can test the function by selecting Test (1), but you need to configure\na sample event to supply to the function first (2).\nFigure 10.3\nApplication detail view of the image resizer service\nFigure 10.4\nThe Function Overview page lets you customize and execute the function.\n\n\n171\nBefore we get started\nYou can use the test event in listing 10.1 to test the function. However, first upload a\nfile from https://commons.wikimedia.org/wiki/File:Happy_smiley_face.png to the\nimage-resizer-service-demo bucket. If you chose to upload a different image, be sure\nto change the object name in the key field in this listing. You need to do this so that\nthe function doesn’t error out looking for an object that doesn’t exist!\n {\n\"Records\": [\n  {\n   \"eventVersion\": \"2.0\",\n   \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n   \"requestParameters\": {\n    \"sourceIPAddress\": \"127.0.0.1\"\n   },\n   \"s3\": {\n    \"configurationId\": \"testConfigRule\",\n    \"object\": {\n     \"eTag\": \"0123456789abcdef0123456789abcdef\",\n     \"sequencer\": \"0A1B2C3D4E5F678901\",\n     \"key\": \"Happy_smiley_face.png\",\n     \"size\": 1024\n    },\n    \"bucket\": {\n     \"arn\": \"arn:aws:s3::: image-resizer-service-demo \",\n     \"name\": \" image-resizer-service-demo \",\n     \"ownerIdentity\": {\n      \"principalId\": \"EXAMPLE\"\n     }\n    },\n    \"s3SchemaVersion\": \"1.0\"\n   },\n   \"responseElements\": {\n    \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuv\n    ➥ wxyzABCDEFGH\",\n    \"x-amz-request-id\": \"EXAMPLE123456789\"\n   },\n   \"awsRegion\": \"us-east-1\",\n   \"eventName\": \"ObjectCreated:Put\",\n   \"userIdentity\": {\n    \"principalId\": \"EXAMPLE\"\n   },\n   \"eventSource\": \"aws:s3\"\n  }\n ]\n}\nInvoke the function a few times to evaluate the behavior; we will use this function to\ndiscuss the various optimizations that follow. When you invoke this function, there are\ndifferent layers in play—the Lambda compute substrate, the execution environment,\nand the function code (figure 10.5). The substrate is invisible to you; the execution\nListing 10.1\nAdding a sample event\n\n\n172\nCHAPTER 10\nBlackbelt Lambda\nenvironment is instantiated on demand for scale events (like a burst of requests); the\nfunction code is instantiated for every request.\n When the first request or event arrives for your function, the AWS Lambda service\nperforms a series of steps. Once the environment exists, Lambda runs the code inside\nyour function handler. Figure 10.6 shows the steps as follows:\n1.\nDownloads your Lambda function Node.js code onto the part of the compute\nsubstrate where your code will run.\n2.\nInstantiates a new execution environment (size is based on your function allo-\ncation) with a Node.js runtime.\n3.\nInstantiates your nonfunction dependencies (in this case, ImageMagick).\n4.\nRuns the parts of your function written outside the handler (we don’t have any\nin this example).\nFunction code\nLanguage \nruntime\nFunction \nexecution \nenvironment\nCompute\nsubstrate \nFigure 10.5\nLayers involved \nin executing a function\nInstantiate\nruntime and\ndependencies\n3\nDownload\nfunction code\n1\nInstantiate new \nexecution\nenvironment\n2\nInstantiate \nnonhandler \ncode\n4\nCache execution\nenvironment\nExecute handler\ncode to\ncompletion\nUnassign\nexecution\nenvironment\nNew\nInvoke\nAssign execution\nenvironment to\nrequest\nUnassigned,\ncached \nexecution\nenvironment?\nNo\nYes\nFigure 10.6\nThe Lambda \nrequest lifecycle\n",
      "page_number": 180
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 188-196)",
      "start_page": 188,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": "173\nBefore we get started\nIn our ResizeFunction example, when your function handler runs, it processes the\nimage and returns the image metadata. Lambda considers the function as done pro-\ncessing the request when the handler logic (and any threads spawned from within the\nfunction handler) finishes executing. When the request is complete, however, AWS\nLambda does not discard the execution environment (with the run time and code ini-\ntialized). Instead, it caches the execution environment, where all processes inside the\nexecution environment are paused. AWS does not publish any official guidance on how\nlong the environment is retained in this state, but various published experiments\n(https://www.usenix.org/conference/atc18/presentation/wang-liang) show this rang-\ning from 5 to 20 minutes. \n When a subsequent request arrives during this time and a cached execution envi-\nronment is available, AWS Lambda will reuse that execution environment to service\nthe request. On the other hand, if a cached execution environment is not available,\nAWS Lambda will repeat all the steps to serve the request. This has significant implica-\ntions to both the performance of your function and how you write your function; we’ll\ndiscuss this further later in this chapter. One important behavior to remember is that\nAWS Lambda always runs only one request per execution environment. This means\nthat if all execution environments are processing requests and a new one comes in,\nAWS Lambda will instantiate a new execution environment. \n10.2.2 Latency: Cold vs. warm\nThe latency incurred due to steps 1 through 4 in this example (figure 10.6) is referred\nto commonly as the cold start penalty. We refer to the request latency for a request\nincurring a cold start as cold latency, and we refer to the actual function execution\nlatency as the warm latency. As a reminder, you incur the cold start penalty only in two\nsituations. First, you’ll see cold starts if your function has never been invoked before\nor is being invoked after an extended period (such that all cached execution environ-\nments are removed). Second, you’ll see cold starts if there is an increase in the incom-\ning request rate such that AWS Lambda needs to spawn new execution environments\nbecause all available ones are servicing requests.\n For most production scenarios, cold starts impact less than 0.5% of requests, but\ncold starts disproportionally impact functions that are invoked infrequently and func-\ntions having a burst of traffic (specifically for the requests that first lead to the\nincreased traffic). Requests that experience cold starts may also experience timeouts\nbecause the AWS Lambda timeout setting is applied to the total request latency.\n10.2.3 Load generation on your function and application\nAs you go about optimizing your application, you want to do so at a load representative\nof real-life usage. As you can see, your latency characteristics may vary based on load as\nwell. Serverless-artillery is a Nordstrom open source project. It builds on artillery.io and\nserverless.com by using the horizontal scalability and pay-as-you-go nature of AWS\nLambda to instantly and inexpensively throw arbitrary load at your services and report\nresults to an InfluxDB time-series database (other reporting plugins are available). This\n\n\n174\nCHAPTER 10\nBlackbelt Lambda\ncapability gives you performance and load testing on every commit early in your CI/CD\npipeline, so performance bugs can be caught and fixed immediately. https://\ngithub.com/Nordstrom/serverless-artillery-workshop presents a detailed walk through\non using and setting up the tool.\n10.2.4 Tracking performance and availability\nYou can’t optimize what you can’t measure. Before you go about figuring out how to\nreduce the latency and improve the availability of your serverless application, you\nmust have a consistent approach to monitor this information. AWS offers a variety of\nboth native and third-party tools for this task. To see what’s available, pick any of your\nfunctions on the AWS Lambda console, click the function in the function list in the\nAWS Lambda console, and navigate to the Monitor tab (figure 10.7). You’ll see three\ntools available to you out of the box: CloudWatch metrics (1) on the selected page,\nCloudWatch logs (2), and AWS X-Ray (3).\nIn this chapter, we’ll use CloudWatch metrics and X-Ray as the two primary tools to\nobserve the latency characteristics of the application.\nCLOUDWATCH METRICS \nEach serverless service (like AWS Lambda and API Gateway) emits standard metrics\nthat help you understand the performance and availability characteristics. For\nLambda, AWS offers the following metrics among others:\nInvocations—Total number of requests received by the given function. This is\ninclusive of all requests, independent of whether they were processed success-\nfully, throttled, or resulted in an error. This also includes any requests that were\nretried due to Lambda’s built-in retry policy (more on this later).\nThe Monitor tab has access to all the \nmetrics and insights you will need. You’ll  \nalso be able to access relevant CloudWatch  \nlogs and X-Ray traces from here.\n1\n2\n3\nFigure 10.7\nMonitoring tab for AWS Lambda functions showing three tools for monitoring\n\n\n175\nBefore we get started\nDuration—Measures the elapsed wall clock time from when the function code\nstarts executing (because of an invocation) to when it stops executing. This is a\nreasonable proxy for what your function will be billed, although not exact,\nbecause AWS Lambda rounds your billed duration to the nearest 1 milliseconds.\nErrors—Measures the number of invocations that failed due to errors in the\nfunction. Note that this does not measure errors due to problems in the AWS\nLambda service or due to throttling.\nThrottles—Measures the number of invocations that did not result in your func-\ntion code executing because your function hit either its concurrency limit or\ncaused the account to hit its concurrency limit (1,000 concurrent executions is\nthe default limit but it can be raised by contacting AWS).\nAWS X-RAY\nAWS X-Ray is a service that allows you to detect, analyze, and optimize performance\nissues with your AWS Lambda functions and trace requests across multiple services\nwithin your serverless architecture. X-Ray generates traces for a sample of requests\nthat each function receives, where a trace consists of segments for each service that\nthe request traverses. A segment may further contain subsegments that detail what\nparticular aspect of the service added to the latency of the request. To turn on X-Ray,\nyou must enable Active tracing under the Monitoring and Operations tools on the\nfunction’s Configuration tab. \n As an example, figure 10.8 shows the trace for a simple sample application. You\ncan see the total time spent in Lambda (1), the time your function took to execute\n(2), as well as the time spent in a cold start (3). X-Ray can be a useful tool to deter-\nmine where the bottlenecks in your function execution are, including whether the\ntop contributor is a cold start.\n1\n2\n3\nAWS X-Ray is great for discovering performance \nissues and improving application performance. \nYou get to see a lot of useful information to help \nyou optimize your serverless applications.\nFigure 10.8\nX-Ray trace for a sample application\n\n\n176\nCHAPTER 10\nBlackbelt Lambda\nTHIRD-PARTY TOOLS\nThere’s a growing ecosystem of non-AWS tools that can also be used for performance\nand availability monitoring from well established companies like NewRelic (https://\nnewrelic.com/) and serverless-first companies like Epsagon (https://epsagon.com/).\nWe won’t dive deep into these tools in this chapter, but we encourage you to explore\nall options and choose what works best for you from https://aws.amazon.com/\nlambda/partners/ (the AWS Lambda partner page). \n10.3\nOptimizing latency\nYou now have an understanding of what contributes to your application latency, how\nto generate load to your application to observe the latency, and what tools to observe\nthe latency. In this section, we’ll discuss how to improve it. \n Your best return on effort at optimizing latency is within individual functions. As\nthe core glue and logic component of your application, any changes made to the\nfunction can have direct and immediate impacts to the latency that your customers\nexperience and to your overall application costs. For example, reducing function exe-\ncution time by 10% reduces the cost of the function by 10%, which can be significant\nat high scale. The decision on what percentile and number to optimize for is your\nchoice, depending on your customers. For example, if you are building a website, you\nwant your response time to be less than 2 seconds at the 99th percentile; if you are\nrunning a backend API, you may be able to tolerate 10’s of seconds of response time\nat the 99th percentile.\n10.3.1 Minimize deployment artifact size\nThe size of your deployment package directly impacts your cold start penalty in two\nways. As a reminder, one of the steps that AWS Lambda undertakes on a “cold” invoke\nis downloading your code (step 1 in figure 10.9). \n The larger your function, the longer this step takes—it’s that simple! AWS Lambda\nenforces a limit of 250 MB for your functions’ deployment package, so there’s a natural\nA note on CloudWatch logs\nAs discussed in earlier chapters, CloudWatch logs capture any log activity specified\nwithin a Lambda function. CloudWatch logs can also be used in two additional ways:\nAs a data source for custom metrics—For example, you can emit data points for\nthe time spent within a specific method of your Lambda function and visualize\nand alarm on that information as a custom metric in CloudWatch metrics.\nAs a bridge to surface data to third-party tools—CloudWatch logs makes it\neasy to send data to third-party tools like NewRelic, which in turn can provide\nadditional visualization and tracing. While Lambda does support the inclusion\nof third-party agents directly via AWS Lambda Extensions, CloudWatch logs\nremains an easy way to surface operational information to other services.\n\n\n177\nOptimizing latency\n“worst case” impact for your deployment package. Second, for functions written in com-\npiled languages like Java and C#, larger deployment packages with many dependencies\ntake longer to instantiate when there are many classes to load into the CLASSPATH. As\nan example, a simple “hello world” on Java loads only 429 classes in the JVM in about\n0.1 seconds, while doing the same “hello world” using Clojure loads 1,988 classes: three\ntimes as much and taking about 1 second. \n A best practice to follow is to audit any function dependencies. Are there any\nheavyweight library dependencies that could be removed or lightweight versions that\ncan be used? Especially look for libraries that act as HTTP servers or agents; they have\nno use inside Lambda functions because Lambda acts as the server for you. For exam-\nple, instead of using the default Java Spring library, you can use the streamlined\nhttps://github.com/awslabs/aws-serverless-java-container library, which is approxi-\nmately 30% faster in experiments. In our example, instead of packaging the entire\nAWS SDK, you could include only the SDK required for accessing S3. You can audit\nyour dependencies for Node.js using tools like https://npm.anvaka.com/, for Python\nusing https://pypi.org/project/modulegraph/, or for Java using the Maven depen-\ndency tree.\n Languages also offer specific tools to reduce deployment package sizes. For exam-\nple, you can use minify for Node.js (https://www.npmjs.com/package/node-minify)\nto reduce the overall size of your Node.js function package. You can also use\nInstantiate\nruntime and\ndependencies\n3\nDownload\nfunction code\n1\nInstantiate new \nexecution\nenvironment\n2\nInstantiate \nnonhandler\ncode\n4\nCache execution\nenvironment\nExecute handler\ncode to\ncompletion\nUnassign\nexecution\nenvironment\nNew\nInvoke\nAssign execution\nenvironment to\nrequest\nUnassigned,\ncached \nexecution\nenvironment?\nNo\nYes\nFigure 10.9\nLambda execution request lifecycle\n\n\n178\nCHAPTER 10\nBlackbelt Lambda\nProGuard (https://www.guardsquare.com/en/products/proguard) to reduce the\nsize of your Java deployment package (JAR files).\n10.3.2 Allocate sufficient resources to your execution environment\nYour code requires compute resources (CPU, memory) to run. AWS Lambda provides\na single dial to set the resources required by your function: the memory setting. You\ncan change this setting by opening a Lambda function in the AWS console, selecting\nthe Configuration tab, and then selecting Edit next to General Configuration. You\ncan then experiment with different memory allocations (1 in figure 10.10). You can\nalso set the same value via the API and CLI. \nAWS Lambda allocates CPU power proportional to the memory by using the same\nratio as a general-purpose Amazon EC2 instance type such as an M3 type. For exam-\nple, if you allocate 256 MB memory, your Lambda function will receive twice the CPU\nshare than if you allocate only 128 MB. You can update the configuration and request\nadditional memory in 64 MB increments from 128 MB to 10240 MB. This change is\nnot free: AWS Lambda pricing weights the billed duration for your function by its\nmemory setting: 1 second of function execution time at 1024 MB costs the same as 8\nseconds of execution at 128 MB.\nAround December of 2020, AWS Lambda began supporting 10,240 MB of \nmemory (and 6 vCPUs) for new existing Lambda functions.\nAccording to AWS, Lambda functions with 10 GB of memory and 6 vCPUs \ncan be particularly useful for machine learning, modeling, genomics, as well \nas more traditional ETL and media-processing applications. \n1\nFigure 10.10\nIn Edit Basic Settings, you can adjust the amount of memory allocated to the function.\n\n\n179\nOptimizing latency\n Let’s experiment with the memory setting on the image-resizer-service function\nyou created so you can see the impact (if you haven’t, see section 10.2.1 earlier in this\nchapter). Set the memory to 128 MB, 256 MB, 512 MB, and 1024 MB and run a few\ntest invokes using the console (we recommend at least 10). Now note the average exe-\ncution time for those invocations from CloudWatch metrics. You should see results\nsimilar to that in table 10.1. The estimated costs are based on AWS Lambda public\npricing for 1,000 requests to the function.\nWe see that increasing the memory in this case keeps the cost relatively flat, while\nincreasing the performance ~10x. You’ll typically see these kind of gains for CPU-\nbound functions like image processing; more resources can help the function run\nfaster without changing the costs. For I/O-bound operations (such as those waiting\nfor a downstream service to respond), you’ll see no benefit in increasing the resource\nallocation. For lightweight run times like Node.js and Go, you may be able to reduce\nthe setting to the lowest (128 MB); for run times like Java and C#, going lower than\n256 MB can have detrimental effects to how the run time loads your function code.\n Finding the right resource allocation for your function requires some experimen-\ntation. The easiest path is to start with a high setting and reduce it until you see a change\nin performance characteristics. You can use the popular tuning tool at https://\ngithub.com/alexcasalboni/aws-lambda-power-tuning to help estimate your function’s\nresource usage.\n10.3.3 Optimize function logic\nAWS Lambda bills your usage based on the time your function starts executing to the\ntime it stops executing, not by CPU cycles spent or any other time-based metrics. This\nimplies that what your function does during that time is important. Consider the\nTable 10.1\nEstimated costs for 1,000 requests\nMemory\nDuration\nEstimated cost for 1,000 requests\n128 MB\n11.722965s\n$0.024628\n256 MB \n6.678945s\n$0.028035\n512 MB \n3.194954s\n$0.026830 \n1024 MB\n1.465984s\n$0.024638\nResource allocation during cold starts\nAWS Lambda respects the resource allocation while executing your function but will\nattempt to “boost” the CPU available while loading and initializing your function\ndependencies. This means that increasing the resource allocation will not really\nmake a difference to your cold starts.\n\n\n180\nCHAPTER 10\nBlackbelt Lambda\nimage-resizer-service function. When you are downloading the S3 object, your code is\nsimply waiting for S3 service to respond, and you are paying for that wait time. In this\nfunction’s case, the time spent is negligible, but this wait time can get excessive for ser-\nvices that have long response times (for example, waiting on an EC2 instance being\nprovisioned) or wait times (such as downloading a very large file). There are two\noptions to minimize this idle time:\nMinimize orchestration in code—Instead of waiting on an operation inside your\nfunction, use AWS Step functions to separate the “before” and “after” logic as\ntwo separate functions. For example, if you have logic that needs to run before\nand after an API call is made, sequence them as two separate functions and use\nan AWS Step function to orchestrate between them. \nUse threads for I/O intensive operations—You can use multiple threads within a\nLambda function (if the programming language supports it), just like code run-\nning in any compute environment. However, unlike conventional programs,\nthe best use for multi-threading isn’t for parallelizing computations. This is\nbecause Lambda does not allocate multiple cores to Lambda functions running\nwith memory less than 1.8 GB, so you need to allocate more resources to get the\nparallelization benefit. Instead, you can use threads as a way to parallelize I/O\noperations. For example, a Python version of the image_resizer function could\nact on multiple functions by executing the S3 download on a separate thread to\nthumbnailing.\nBy following these best practices, you can significantly reduce the latency (and cost!)\nof your serverless application. Finally, let’s look at concurrency, and we’ll do that in\nthe following section.\n10.4\nConcurrency \nAnother important concept to understand for AWS Lambda functions is concurrency.\nConcurrency is the unit of scale for a Lambda function. Underneath the covers, it maps\nto the number of execution environments assigned to requests. You can estimate the\nconcurrency of your function at any time with the following formula:\nConcurrency = Requests per second (TPS) * Function duration\nUsing peak values will give you peak concurrency; using average values will give you\naverage concurrency. You can monitor the concurrency for any given function (and\nfor the overall account) using the ConcurrentExecutions CloudWatch metric. AWS\nLambda enforces two limits to the concurrency of a function: \nThere is an account-wide soft limit on the total concurrent executions of all functions within\nthe account. This is set by default to 1,000 at the time of writing, and it can be raised\nto desired values through a support case. You can view the account-level setting\nby using the GetAccountSettings API and viewing the AccountLimit object.\n\n\n181\nConcurrency\nThere is also an account-wide limit on the rate at which you can scale up your concurrent\nexecutions. In larger AWS regions, you are allowed to scale instantly to 3,000 con-\ncurrent and then add 500 concurrent executions every subsequent minute; this\nlimit is lower in smaller regions. These limits may change, so be sure to refer to\nthe latest values listed in http://mng.bz/80PZ.\nThis makes it important to always estimate what your peak and average concurrency\nneeds will be, how quickly you’ll need to ramp up, and to file a request to raise limits\nas needed. \n10.4.1 Correlation between requests, latency, and concurrency\nFor most functions, concurrency increases as a function of requests and function\nduration, subject to the concurrency limits on the function and account. However, for\nfunctions used to process stream data (Kinesis and DynamoDB streams), the concur-\nrency is determined by the number of shards on the stream being processed. Given\nthat latency is determined by the function itself, this means for stream-processing\nfunctions, you may see variable request rate or throughput. To put it another way, \nEffective processing rate = Effective concurrency / average duration (events \n➥ per second)\nConsider a function that takes 1 second to process a stream with 5 shards and with a\nbatch size of 100. This means the maximum number of requests (each with 100\nrecords) that the function can process would be 5, and the maximum number of\nrecords processed at any given time would be 5 * 100 = 500. On the other hand, if the\nsame stream had 10 shards, the throughput would double as well.\n10.4.2 Managing concurrency\nAWS offers two settings for managing concurrency. The first one is the account level\nconcurrency limit that is enforced on the total concurrency across all functions within\nyour account. This limit is set to 1,000 by default and can be raised through a service\nlimit increase ticket: you cannot “self-service” this increase at the time of writing. The\nsecond is a per function concurrency control, which you can use to control the con-\ncurrency of an individual function. You only use the per function concurrency control\nif you have a function that you want to “reserve” concurrency for or a function that\nneeds to be limited in its concurrency (because of a downstream resource). \n For example, you may want to restrict how high a Lambda function scales because\nit calls an API that can only handle a certain load. If this was left unchecked, then your\nfunction could cause the downstream API to be overloaded, causing an availability for\nyour overall application. This makes monitoring concurrency and managing it an\nimportant step to follow. You can learn more about the limits and the controls here:\nhttp://mng.bz/v4mq.\n",
      "page_number": 188
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 197-204)",
      "start_page": 197,
      "end_page": 204,
      "detection_method": "topic_boundary",
      "content": "182\nCHAPTER 10\nBlackbelt Lambda\nSummary\nServerless applications do not require conventional application performance\nmonitoring steps. Instead, optimizing the performance of your function code\ngives you the most gain.\nUse the toolsets (like X-Ray) and configurations (like the memory setting) to\neasily locate and optimize performance.\nConcurrency for Lambda functions can affect your function latency (and vice\nversa), so ensure you monitor and manage it for your critical functions.\n\n\n183\nEmerging practices\nThe term serverless came about after AWS released the Lambda service back in 2014.\nIn that sense, the serverless paradigm (building applications using managed ser-\nvices, including for all your compute needs) is something of a new kid on the\nblock.\n New paradigms give us new ways to look at problems and solve them differently,\nperhaps more efficiently. This should be obvious by now as we have discussed sev-\neral serverless architectures in this book, and you must admit they look very differ-\nent than the equivalent serverful architectures; they are more event-driven, and\nthey often involve many different services working together.\n New paradigms also require us to think and work differently. For example,\ninstead of thinking about cost as a function of the size of a fleet of virtual machines\nand how long you need them for, we need to think about cost in terms of request\nThis chapter covers\nUsing multiple AWS accounts\nUsing temporary stacks\nAvoiding keeping sensitive data in plain text in \nenvironment variables\nUsing EventBridge in event-driven architectures\n\n\n184\nCHAPTER 11\nEmerging practices\ncount and execution duration. The code we write and the way we deploy and monitor\nour applications also need to change to take full advantage of this new paradigm and\nmitigate some of its limitations.\n The following emerging practices are used by teams that have successfully adopted\nserverless technologies in their organization. Many are useful outside of the context of\nserverless, such as using multiple AWS accounts and using EventBridge in an event-\ndriven architecture. Although none of them are silver bullets (is anything?), they are\nuseful in the right contexts and are ideas worth considering.\n11.1\nUsing multiple AWS accounts\nEvery AWS employee you speak to nowadays will tell you that you should have multiple\nAWS accounts and manage them with AWS Organizations (https://aws.amazon\n.com/organizations). At the minimum, you should have at least one AWS account per\nenvironment. For larger organizations, you should go further and have at least one\nAWS account per team per environment. There are many reasons why this is considered\na best practice—regardless of whether you’re working with serverless technologies—\nincluding those discussed in the following sections.\n11.1.1 Isolate security breaches\nImagine the nightmare scenario where an attacker has gained access into your AWS\nenvironment and is then able to access and steal your users’ data. This nightmare sce-\nnario can happen in many ways, and here are three that jump to mind right away:\nAn EC2 instance is exposed publicly and the attacker is able to SSH into the instance\nusing brute force. Once inside, they can use the instance’s IAM role to access\nother AWS resources.\nA misconfigured web application firewall (WAF) allows the attacker to execute a server-\nside request forgery (SSRF) attack and trick the WAF to relay requests to the EC2 metadata\nservice. This allows the attacker to find out the temporary AWS credentials used\nby the WAF server. From here, the attacker is able to access other AWS\nresources in the account. This is what happened in the Capital One data breach\nin 2019.\nAn employee accidentally includes their AWS credentials in a Git commit in a public\nGitHub repo. The attacker scans public GitHub repos for AWS credentials and\nfinds this commit. The attacker is then able to access all the AWS resources that\nthe employee had access to. AWS also scans public GitHub repos for active AWS\ncredentials and warns its customers when it finds them. But the damage is often\ndone already by the time the customer realizes it.\nUsing multiple accounts doesn’t stop these attack vectors, but it limits the blast radius\nof a security breach to a single account (and hopefully not your production account!).\n\n\n185\nUsing multiple AWS accounts\n11.1.2 Eliminate contention for shared service limits\nThroughout this book, we have talked about AWS service limits several times already.\nAs your organization and your system grow, more engineers need to work on the sys-\ntem, and you will likely create more and more services that take care of specific\ndomains within the larger system (think microservices). As this happens, you will\nlikely run into those pesky service limits more frequently because there is more con-\ntention for the shared-service limits. \n It gets worse from here. Because service limits apply at the region level and affect\nall the resources in a region, it means that one team or one service can exhaust all the\navailable throughput (for example, Lambda concurrent executions) in the region\nand throttle everything else. \n What’s more, if all the environments are run from the same AWS account, then\nsomething happening in a non-production environment can also impact users in pro-\nduction. For example, a load test in staging can consume too many Lambda concur-\nrent executions so that users are not able to access your APIs in production because\nthose API functions are throttled.\n Having separate accounts for each team and each environment eliminates the con-\ntention altogether. If a team makes mistakes or experiences a sudden traffic spike in\ntheir services, the extra throughput they consume will not impact other services. Any\nservice limit-related throttling would be contained to that account and limit the blast\nradius of these incidents. Equally, you can safely run load tests in non-production envi-\nronments knowing that they won’t affect your users in production.\n What if, within a team, the same contention exists between different services?\nMaybe one of the team’s services handles much more traffic than the rest and occa-\nsionally causes other services to be throttled. Well, then you want to move that service\ninto its own set of accounts of dev, test, staging, and production. This technique of\nusing AWS accounts as bulkheads to isolate and contain the blast radius can go as far\nas you need. You don’t have to stop at one account per team per environment. Make\nthe techniques work for you, not the other way around.\n11.1.3 Better cost monitoring\nIf everything runs from the same AWS account, then you will have a hard time attrib-\nuting your AWS costs to different environments or teams or services. Having multiple\naccounts lets you see the cost for those accounts easily.\n11.1.4 Better autonomy for your teams\nFrom a security and access control point of view, if each team has its own set of AWS\naccounts, then you can afford to give them more autonomy and control of their own\nAWS accounts. If everyone shares the same AWS account and that account is used for\nboth non-production as well as production environments, then the stakes are high.\nMistakes have a large blast radius and teams can accidentally delete or update other\nteams’ resources, or even delete users’ data in production. That is why you need to be\n\n\n186\nCHAPTER 11\nEmerging practices\ncareful in terms of managing access. It creates a lot of complexity and stress for whom-\never must manage access (typically the security team or a cloud platform team).\n In my experience, the high stakes and complexity invite gatekeeping and create fric-\ntion between the various disciplines. Feature teams often have to suffer delays as they\nwait for an over-worked platform team to grant them the access they need. Resentment\nbuilds and harmony erodes, and soon it becomes an “us versus them” situation.\n Giving every team their own AWS accounts limits the blast radius of any issues and\nlowers the stakes. You can then afford to give your teams more autonomy within their\nown accounts. The platform team/security team can instead focus on setting up\nguardrails and governance infrastructure so they can identify problems quickly. And\nthey should work with the feature teams to ensure they follow organizational best\npractices and meet your security requirements.\n11.1.5 Infrastructure-as-code for AWS Organizations\nHaving multiple AWS accounts means you need to have some way to manage them,\nespecially as you scale your organization. The number of AWS accounts can grow, and\nas more engineers join the organization, it becomes more important to have strong\ngovernance and oversight of your AWS environment.\n One of the shortcomings of AWS Organizations is that you can’t update the config-\nurations of your organization using infrastructure as code (IaC). For example, Cloud-\nFormation is a regional service and is limited to provisioning resources within a single\naccount and region. At the time of writing, the only tool that allows you to apply IaC\nto AWS Organizations is org-formation (https://github.com/org-formation/org\n-formation-cli). It’s an open source tool that lets you capture the configuration of your\nAWS accounts and the entire AWS organization using IaC. I have used it with several\nprojects and I can’t recommend it highly enough! \n A topic related to using multiple AWS accounts is the use of temporary Cloud-\nFormation stacks for temporary environments, such as those for feature branches or\nto carry out end-to-end (e2e) tests. We discuss temporary stacks next.\n11.2\nUsing temporary stacks\nOne of the benefits of serverless technologies is that you pay for them only when peo-\nple use your application. When your code is not running, you aren’t charged. Com-\nbine this with the fact that it’s easy to deploy a serverless application using tools such\nas the Serverless Framework. Because it’s so easy to create new environments and\nthere is no uptime cost for having these environments, many teams create temporary\nenvironments for when they work on feature branches or to run their e2e tests.\n11.2.1 Common AWS account structure\nIt’s common for teams to have multiple AWS accounts, one for each environment.\nThough there doesn’t seem to be a consensus on how to use these environments, we\ntend to follow these conventions:\n\n\n187\nUsing temporary stacks\nThe dev environment is shared by the team. This is where the latest development\nchanges are deployed to and tested end to end. This environment is unstable by\nnature and shouldn’t be used by other teams.\nThe test environment is where other teams can integrate with your team’s work. This envi-\nronment should be stable so it doesn’t slow down other teams.\nThe staging environment should closely resemble the production environment and may\noften contain dumps of production data. This is where you can stress test your\nrelease candidate in a production-like environment.\nAnd then there’s the production environment.\nAs discussed earlier in this chapter, it’s best practice to have multiple AWS accounts—\nat least one account per team per environment. In the dev account, you can also have\nmore than one environment—one for each developer or each feature branch.\n11.2.2 Use temporary stacks for feature branches\nWhen we start work on a new feature, we still feel our way toward the best solution for\nthe problem. The codebase is unstable and many bugs haven’t been ironed out yet.\nDeploying our half-baked changes to the dev environment can be quite disruptive:\nIt risks destabilizing the team’s shared environment.\nIt overwrites other features the team is working on.\nTeam members may fight over who gets to deploy their feature branch to the\nshared environment.\nInstead, we can deploy the feature branch to a temporary environment. Using the\nServerless Framework is as easy as running the command sls deploy -s my-feature,\nwhere my-feature is both the name of the environment and the name of the Cloud-\nFormation stack. This deploys all the Lambda functions, API Gateway, and any other\nrelated resources such as DynamoDB tables in their own CloudFormation stack. We\nare able to test our work-in-progress feature in an AWS account without affecting\nother team members’ work. \n Having these temporary CloudFormation stacks for each feature branch has negli-\ngible cost overhead. When the developer is done with the feature, the temporary stack\ncan be easily removed by running the command sls remove -s my-feature. However,\nbecause these temporary stacks are an extension of your feature branch, they exhibit the\nsame problems when you have long-lived feature branches. Namely, they get out of sync\nwith other systems they need to integrate with. This applies to the incoming events that\ntrigger your Lambda functions (such as the payloads from SQS/SNS/Kinesis), as well\nas data your function depends on (such as the data schema in DynamoDB tables). We\nfind teams that use serverless technologies tend to move faster, which makes the prob-\nlems with long-lived feature branches more prominent and noticeable.\n As a rule of thumb, don’t leave feature branches hanging around for more than a\nweek. If the work is large and takes longer to implement, then break it up into smaller\n\n\n188\nCHAPTER 11\nEmerging practices\nfeatures. When you’re working on a feature branch, you should also integrate from\nthe main development branch regularly—no less than once per day.\n11.2.3 Use temporary stacks for e2e tests\nAnother common use of temporary CloudFormation stacks is for running e2e tests.\nOne of the common problems with these tests is that you need to insert test data into\na shared AWS environment. Over time, this adds a lot of junk data in those environ-\nments and can make it difficult for other team members. For example, testers often\nhave to do manual tests on the mobile or web app, and all the test data left by your\nautomated tests can create confusion and make their job more difficult than it needs\nto be. As a rule of thumb, we always do the following:\nInsert the data a test case needs before the test.\nDelete the data after the test finishes.\nUsing the Jest (https://jestjs.io) JavaScript framework, you can capture the before\nand after steps as part of your test suite. They help keep our tests robust and self-\ncontained because they don’t implicitly rely on data to exist. They also help reduce\nthe amount of junk data in the shared dev environment.\n But despite our best intentions, mistakes happen, and sometimes we deliberately\ncut corners to gain agility in the short term. Over time, these shared environments still\nend up with tons of test data. As a countermeasure, many teams employ cron jobs to\nwipe these environments from time to time.\n An emerging practice to combat these challenges is to create a temporary Cloud-\nFormation stack during the CI/CD pipeline. The temporary stack is used to run the e2e\ntests and destroyed afterwards. This way, there is no need to clean up test data, either\nas part of your test fixture or with cron jobs. The drawbacks include the following:\nThe CI/CD pipeline takes longer to run.\nYou still leave test data in external systems, so it’s not a complete solution.\nYou should weigh the benefits of this approach against the delay it adds to your\nCI/CD pipeline. Personally, we think it’s a great approach, and we see more teams\nstarting to adopt it. To make CI/CD pipelines go faster, some teams keep a number of\nthese temporary stacks around and reuse them in a round-robin fashion. This way, you\nstill enjoy the benefit of being able to run e2e tests against a temporary environment\nbut shorten the time it takes to deploy the temporary environment (updating an exist-\ning CloudFormation stack is significantly faster than creating a new stack).\n11.3\nAvoid sensitive data in plain text in environment variables\nOne common mistake we have seen for both serverful and serverless applications is that\nsensitive data (such as API keys and credentials) is left in plain text in environment vari-\nables. When it comes to security, serverless applications are more secure because AWS\ntakes care of the security of the operational environment of our application. This\n\n\n189\nAvoid sensitive data in plain text in environment variables\nincludes securing the virtual machines our code runs on as well as their network con-\nfigurations, and it includes the security of the operating system itself.\n Our Lambda functions run on bare-metal EC2 instances that AWS manages, and\nthe EC2 instances reside in AWS-managed VPCs. There’s no easy way for an attacker to\nfind out information about the virtual machine itself, and there’s no way for attackers\nto SSH into these virtual machines.\n The operating systems are constantly updated and patched with the latest security\npatches, sometimes before the patch is even available to the general public. Such was\nthe case during the Meltdown and Spectre debacle when all EC2 instances behind\nLambda and Fargate were quickly patched against the vulnerabilities long before the\nrest of us were able to patch our container and EC2 images. Having AWS manage the\noperational environment of our code removes a huge class of attack vectors from our\nplate, but we are still responsible for the security of our application and its data.\n11.3.1 Attackers can still get in\nEven though the operational environment of our code is secured by AWS, it’s still pos-\nsible for attackers to get inside the execution environment of our functions via other\nmeans, including the following:\nAttacker successfully executes a code injection attack. For example, if your application\nor any of its dependencies use JavaScript’s eval() function against a piece of\nuser input, then you’re vulnerable to these attacks.\nAttacker compromises one of your dependencies and publishes a malicious version of the\ndependency that steals information from your application at run time. Remember that\ntime when a security researcher gained publish access to 14% of NPM packages\n(http://mng.bz/N4PN)? Or that time an attacker compromised the NPM\naccount for one of EsLint’s maintainers and published a malicious version of\neslint-scope and eslint-config-eslint (http://mng.bz/DKPn)?\nAttacker publishes a malicious NPM package with similar names to popular NPM pack-\nages and steals information from your application on initialization. An example is the\ntime when an attacker published a malicious package called crossenv using the\npopular NPM package cross-env as bait (http://mng.bz/l9d6).\nOnce inside, attackers often steal information from common, easily accessible places\nsuch as environment variables. This is why it’s so important that we avoid putting sen-\nsitive data in plain text in environment variables.\n11.3.2 Handle sensitive data securely\nSensitive data should be encrypted both in transit and at rest. This means it should be\nstored in an encrypted form; within AWS, you can use both the SSM Parameter Store\nand the Secrets Manager to store it. Both services support encryption at rest, integrate\ndirectly with AWS Key Management Service (KMS), and allow you to use Customer\nManaged Keys (CMKs). The same encrypted at-rest principle should be applied to\n",
      "page_number": 197
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 205-213)",
      "start_page": 205,
      "end_page": 213,
      "detection_method": "topic_boundary",
      "content": "190\nCHAPTER 11\nEmerging practices\nhow sensitive data is stored in your application. There are multiple ways to achieve\nthis; for example:\nStore the sensitive data in encrypted form in environment variables and decrypt\nit using KMS during cold start.\nKeep the sensitive data in SSM Parameter Store or Secrets Manager, and during\nthe Lambda function cold start, fetch it from SSM Parameter Store/Secrets\nManager.\nOnce decrypted, the data can be kept in an application variable or closure where it\ncan be easily accessed by your code. The important thing is that sensitive data should\nnever be placed back into the environment variables in unencrypted form. Our per-\nsonal preference is to fetch sensitive data from the SSM Parameter Store/Secrets Man-\nager during cold start. We would use middy’s SSM middleware (https://github.com/\nmiddyjs/middy/tree/main/packages/ssm) to inject the decrypted data into the\ncontext variable and cache it for some time.\n This way, we can rotate these secrets at the source without having to redeploy the\napplication. Once the cache expires, the middleware fetches the new values on the\nnext Lambda invocation. It also makes it easier to manage shared secrets where multi-\nple services need to access the same secret. Finally, this approach allows more granu-\nlar control of permissions because the Lambda function requires permissions to\naccess the secrets in SSM Parameter Store/Secrets Manager.\n There are other variants of these two approaches; for example, instead of storing\nencrypted secrets in environment variables, you can store them in an encrypted file\nthat is deployed as part of the application. During Lambda cold start, this file is\ndecrypted with KMS, and the secrets it contains are then extracted and stored away\nfrom the environment variables.\n11.4\nUse EventBridge in event-driven architectures\nAmazon SNS and SQS have long been the go-to option for AWS developers when it\ncomes to service integration. However, since its rebranding, Amazon EventBridge\n(formerly Amazon CloudWatch Events) has become a popular alternative, and I\nwould argue that it’s actually a much better option as the event bus in an event-driven\narchitecture.\n11.4.1 Content-based filtering\nSNS lets you filter messages via filtering policies. But you can’t filter messages by their\ncontent, you can only filter by message attributes, and you can only have up to 10 attri-\nbutes per message. If you require content-based filtering, then it has to be done in\ncode. EventBridge, on the other hand, supports content-based filtering and lets you\npattern match against an event’s content. In addition, it supports advanced filtering\nrules such as these:\n\n\n191\nUse EventBridge in event-driven architectures\nNumeric comparison\nPrefix matching\nIP address matching\nExistence matching\nAnything-but matching\nNOTE\nCheck out the blog post at http://mng.bz/B1w0 on EventBridge’s\ncontent-based filtering for more details on these advanced rules.\nIn an event-driven architecture, it’s often desirable to have a centralized event bus. It\nmakes it easy for subsystems to subscribe to events triggered by any other subsystem\nand for you to create an archive that captures everything happening in the whole\napplication (for both audit and replay purposes).\n With content-based filtering, it’s possible to have a centralized event bus in Event-\nBridge. Subscribers can freely subscribe to the exact events they want without having\nto negotiate with the event publishers on what attributes to include. This is usually not\nfeasible with SNS, and you have to use multiple SNS topics.\n11.4.2 Schema discovery\nA common challenge with event-driven architectures is identifying and versioning\nevent schemas. EventBridge deals with this challenge with its schema registry and pro-\nvides a built-in mechanism for schema discovery.\n EventBridge captures a wide range of events from AWS services (such as when an\nEC2 instance’s state has changed) in the default event bus. It provides the schema for\nthese AWS events in the default schema registry. You also can enable schema discovery\non any event bus, and EventBridge samples the ingested events and generates and ver-\nsions schema definitions for these events.\n If you’re programmatically generating schema definitions for your application\nevents already, then you can also create a custom schema registry and publish your\nschema definitions there as part of your CI/CD pipeline. That way, your developers\nalways have an up-to-date list of the events in circulation and what information they\ncan find on these events.\n Open-source tools such as the evb-cli (https://www.npmjs.com/package/\n@mhlabs/evb-cli) even let you generate EventBridge patterns using the schema defi-\nnitions in a schema registry. This is handy, especially if you’re new to EventBridge’s\npattern language!\n11.4.3 Archive and replay events\nAnother common requirement for event-driven architectures is to be able to archive\nthe ingested events and replay them later. The archive requirement is often part of a\nlarger set of audit or compliance requirements and is therefore a must-have in many\nsystems. Luckily, EventBridge offers archive and replay capabilities out of the box.\nWhen you create an archive, you can configure the retention period, which can be set\n\n\n192\nCHAPTER 11\nEmerging practices\nto indefinite. You can optionally configure a filter so that only matching events are\nincluded in the archive.\n When you need to replay events from the archive, you can choose a start and end\ntime so that only the events captured in the specified time range will be replayed. One\nthing to keep in mind about event replays is that EventBridge does not preserve the\noriginal order of the events as they were received. Instead, EventBridge looks to replay\nthese events as quickly as possible, which means you can expect a lot of concurrency\nand that most events will be replayed out of sequence.\n If ordering is important to you when replaying events, then you should check out\nthe evb-cli project mentioned earlier. Its evb replay command supports paced\nreplays, which retains the ordering of events and lets you control how quickly events\nare replayed. For example, using a replay speed of 100 replays events in real time\nmeans replaying an hour’s worth of events would take an hour.\n11.4.4 More targets\nWhereas SNS supports a handful of targets (such as HTTP, Email, SQS, Lambda, and\nSMS), EventBridge supports more than 15 AWS services (including SNS, SQS, Kinesis,\nand Lambda), and you can forward events to another EventBridge bus in another\naccount.\n This extensive reach helps to remove a lot of unnecessary glue code. For example,\nto start a Step Functions state machine, you would have needed a Lambda function\nbetween SNS and Step Functions. With EventBridge, you can connect the rule to the\nstate machine directly.\n11.4.5 Topology\nThere are different ways to arrange event buses in EventBridge. For example, you can\nhave a centralized event bus, every service can publish events to their own event bus,\nor maybe you have a few domain-specific event buses that are shared by related ser-\nvices. There is no clear consensus on which approach is the best because everyone’s\ncontext is different, and each approach has its pros and cons. However, we personally\nfavor the centralized event bus approach because it has some great advantages includ-\ning the following:\nYou can implement an archive and a schema registry in one place.\nYou can manage access and permissions in one place.\nAll the events you need are available in one event bus.\nThere are fewer resources to manage.\nBut it also has some shortcomings that you need to consider:\nThere is a single point of failure. Having said that, EventBridge is already highly\navailable, and the infrastructure that ingests, filters, and forwards events to con-\nfigured targets is distributed across multiple availability zones.\nService teams have less autonomy as they all depend on the centralized event\nbus.\n\n\n193\nSummary\nThere is also the question of AWS account topology. That is, which account do you\ndeploy the event bus to if a given environment consists of multiple AWS accounts\n(such as when you have one account per team)? Should you deploy the centralized\nevent bus in its own account or in the account that perhaps make the most sense?\nThat is a wider topic that is outside the scope of this chapter, but I recommend you\ncheck this re:Invent 2020 session by Stephen Liedig: https://www.youtube.com/\nwatch?v=Wk0FoXTUEjo. It goes into detail about the different configurations and the\npros and cons of each.\nSummary\nAnd that’s it for a list of emerging practices that you should seriously consider adopt-\ning in your projects. We call these emerging practices because they are not adopted ubiq-\nuitously but are gaining traction in the AWS community. As the AWS ecosystem and\nserverless technologies develop and mature, more practices emerge and take root. It’s\nworth remembering that no practice should be considered best in its own right, and\nyou must always consider the context and environment a practice is applied in.\n As technology and your organization change, your context changes too. Many of the\nthings that you might once consider as best practice can easily become anti-patterns.\nFor example, monorepos work great when you are a small team, but by the time you\ngrow to hundreds or perhaps thousands of engineers, monorepos present many chal-\nlenges that require complex solutions to address. \n The same goes for how we build, test, deploy, and operate software. What worked\ngreat in private data centers and server farms might not translate well to the cloud.\nAnd practices that serve us well when we have to manage both the infrastructure our\ncode runs on as well as the code itself might work against us as we build applications\nwith serverless technologies.\n Best practices and design patterns should be the start of the conversation, not the\nend. After all, these so-called best practices and design patterns are collective docu-\nmentations of things that others have done that worked for them to some degree at\nsome time. There’s no guarantee that they’ll work for you today. And it’s easy to see\nparallels from other industries. For example, did you know that lobotomies were part\nof mainstream mental healthcare from 1930s to 1950s before they were outlawed in\nthe 1970s and considered outright barbaric by today’s standards?\n\n\n195\nappendix A\nServices for your\n serverless architecture\nAWS is a giant playground of different services and products you can use to build\nserverless applications. Lambda is a key service that we discussed in this book, but\nother services and products can be just as useful, if not crucial, for solving certain\nproblems. There are many excellent non-AWS products too, so don’t feel obligated\nto use only what Amazon has to offer. Have a look at the offerings from Microsoft\nand Google too. The following sections provide a sample of services that we’ve\nfound useful. You can use this appendix as a guide to various services and products\nwe’ll discuss throughout the book.\nA.1\nAPI Gateway\nThe Amazon API Gateway is a service that you can use to create an API layer\nbetween the frontend and backend services. The lifecycle management of the API\nGateway allows multiple versions of the API to be run at the same time, and it sup-\nports multiple release stages such as development, staging, and production. API\nGateway also comes with useful features like caching and throttling requests.\n The API is defined around resources and methods. A resource is a logical entity\nsuch as a user or product. A method is a combination of an HTTP verb (such as GET,\nPOST, PUT, or DELETE) and the resource path. API Gateway integrates with\nLambda and other AWS services. It can be used as a proxy service and forward\nrequests to regular HTTP endpoints.\nA.2\nSimple Notification Service (SNS)\nAmazon Simple Notification Service (SNS) is a scalable pub/sub service designed\nto deliver messages. Producers or publishers create and send messages to a topic.\nSubscribers or consumers subscribe to a topic and receive messages over one of the\n\n\n196\nAPPENDIX A\nServices for your serverless architecture\nsupported protocols. SNS stores messages across multiple servers and data centers for\nredundancy and guarantees at-least-once delivery. At-least-once delivery stipulates that\na message will be delivered at least once to a subscriber, but on rare occasions, due to\nthe distributed nature of SNS, it may be delivered multiple times.\n In cases where a message can’t be delivered by SNS to HTTP endpoints, it can be\nconfigured to retry deliveries at a later time. SNS can also retry failed deliveries to\nLambda when throttling is applied. SNS supports message payloads of up to 256 KB.\nA.3\nSimple Storage Service (S3) \nSimple Storage Service (S3) is Amazon’s scalable storage solution. Data in S3 is stored\nredundantly across multiple facilities and servers. The event notifications system\nallows S3 to send events to SNS, SQS, or Lambda when objects are created or deleted.\nS3 is secure, by default, with only owners having access to the resources they create,\nbut it’s possible to set more granular and flexible access permissions using access con-\ntrol lists and bucket policies.\n S3 uses the concept of buckets and objects. Buckets are high-level directories or\ncontainers for objects. Objects are a combination of data, metadata, and a key. A key is a\nunique identifier for an object in a bucket. \n S3 also supports the concept of a folder as a means of grouping objects in the S3\nconsole. Folders work by using key name prefixes. A forward slash character (/) in the\nkey name delineates a folder. For example, an object with the key name documents/\npersonal/myfile.txt is represented as a folder called documents, containing a folder\ncalled personal, containing the file myfile.txt in the S3 console.\nA.4\nSimple Queue Service (SQS)\nSimple Queue Service (SQS) is Amazon’s distributed and fault-tolerant queuing ser-\nvice. It ensures at-least-once delivery of messages similar to SNS and supports message\npayloads of up to 256 KB. SQS allows multiple publishers and consumers to interact\nwith the same queue, and it has a built-in message lifecycle that automatically expires\nand deletes messages after a preset retention period. As with most AWS products,\nthere are access controls to help control access to the queue. SQS integrates with SNS\nto automatically receive and queue messages.\nA.5\nSimple Email Service (SES)\nSimple Email Service (SES) is a service designed to send and receive email. SES han-\ndles email-receiving operations such as scanning for spam and viruses and rejection of\nemail from untrusted sources. Incoming email can be delivered to an S3 bucket or\nused to invoke a Lambda notification, or create an SNS notification. These actions\ncan be configured as part of the receipt rule, which tells SES what to do with the email\nonce it arrives.\n Sending emails with SES is straightforward, but there are limits that are in place to\nregulate the rate and the number of messages sent. SES automatically increases the\nquota as long as high-quality email, and not spam, is sent.\n\n\n197\nAPPENDIX A\nServices for your serverless architecture\nA.6\nRelational Database Service (RDS)\nAmazon Relational Database Service (RDS) is a web service that helps with the setup\nand operation of a relational database in the AWS infrastructure. RDS supports the\nAmazon Aurora, MySQL, MariaDB, Oracle, MS-SQL, and PostgreSQL database\nengines. It takes care of routine tasks such as provisioning, backup, patching, recovery,\nrepair, and failure detection. Monitoring and metrics, database snapshots, and multiple\navailability zone (AZ) support are provided out of the box. RDS uses SNS to deliver noti-\nfications when an event occurs. This makes it easy to respond to database events such\nas creation, deletion, failover, recovery, and restoration when they happen.\nA.7\nDynamoDB\nDynamoDB is Amazon’s NoSQL database. Tables, items, and attributes are Dynamo’s\nmain concepts. A table stores a collection of items. An item is made up of a collection of\nattributes. Each attribute is a simple piece of data such as a person’s name or phone\nnumber. Every item is uniquely identifiable. Lambda integrates with DynamoDB\ntables and can be triggered by a table update. Global tables is a notable feature of\nDynamo that seamlessly replicates tables across different AWS regions and resolves any\ndata conflicts (using “last writer wins” reconciliation to handle concurrent updates). It\nmakes DynamoDB a good database for scalable, global applications. Finally, an in-\nmemory cache (DAX) is available for DynamoDB. It shortens the response time but\ncomes at a price.\nA.8\nAlgolia\nAlgolia is a (non-AWS) managed search engine API. It can search through semi-\nstructured data and has APIs to allow developers to integrate search directly into their\nwebsites and mobile applications. One of Algolia’s outstanding capabilities is its speed.\nAlgolia can distribute and synchronize data across 15 regions around the world and\ndirect queries to the closest data center. \n Algolia has a concept of indices (“. . . an entity where you import the data you want\nto search . . . analogous to a table within a database . . .”), records (“. . . a JSON schema-\nless object that you want to be searchable . . .”) and operations (which are essentially\natomic actions such as update or delete). These concepts are straightforward and\nmake Algolia one of the easier search platforms to use. Paid plans begin from about\n$35 per month but can quickly grow in cost, depending on the number of records and\noperations performed by your application and users.\nA.9\nMedia Services\nAWS Media Services is a new product designed for developers to build video work-\nflows. Media Services consist of the following products:\nMediaConvert is designed to transcode between different video formats at scale.\nMediaLive is a live video-processing service. It takes a live video source and com-\npresses it into smaller versions for distribution.\n\n\n198\nAPPENDIX A\nServices for your serverless architecture\nMediaPackage enables developers to implement video features such as pause\nand rewind. It can also be used to add Digital Right Management (DRM) to\ncontent.\nMediaStore is a storage service optimized for media. Its aim is to provide a low-\nlatency storage system for live and on-demand video content.\nMediaTailor enables developers to insert individually targeted ads in to the\nvideo stream.\nMedia Services provide an advanced suite of services that are superior to Elastic\nTranscoder. Nevertheless, Elastic Transcoder has a few features (such as the ability to\ncreate WebM files and animated GIFs) that Media Services is missing.\nA.10\nKinesis Streams\nKinesis Streams is a service for real-time processing of streaming big data. It’s typically\nused for quick log and data intake, metrics, analytics, and reporting. It’s different\nfrom SQS in that Amazon recommends that Kinesis Streams be used primarily for\nstreaming big data, whereas SQS is used as a reliable hosted queue, especially if more\nfine-grained control over messages such as visibility timeouts or individual delays is\nrequired. \n In Kinesis Streams, shards specify the throughput capacity of a stream. The number\nof shards needs to be stipulated when the stream is created, but resharding is possible\nif throughput needs to be increased or reduced. In comparison, SQS makes scaling\nmuch more transparent. Lambda can integrate with Kinesis to read batches of records\nfrom a stream as soon as they’re detected.\nA.11\nAthena\nAWS bills Athena as a serverless interactive query service. Essentially, this service allows\nyou to query data placed into S3 using standard SQL. In a lot of cases, there’s no need\nto run ETL (extract, transform, and load) jobs to transform your data before querying\ncan take place (although you can combine Athena with AWS Glue if you needed to\ntransform your data a certain way). As a user, you upload data to S3, prepare a\nschema, and begin querying almost immediately. \nA.12\nAppSync\nAppSync is billed as allowing developers to create “ . . . data driven apps with real-time\nand offline capabilities.” In reality, AppSync is a managed GraphQL endpoint pro-\nvided by AWS. It integrates with DynamoDB, Lambda, and Amazon Elasticsearch. If\nyou are familiar with GraphQL and GraphQL schemas, you can get started with\nAppSync straight away. If you are not familiar with GraphQL, we recommend doing a\nbit of reading beforehand (http://graphql.org/learn/). GraphQL has certainly been\nfinding its share of acclaim over the past few years, particularly among adopters of\nserverless technologies.\n\n\n199\nAPPENDIX A\nServices for your serverless architecture\nA.13\nCognito\nAmazon Cognito is an identity management service. It integrates with public identity\nproviders such as Google, Facebook, Twitter, and Amazon or with your own system.\nCognito supports user pools, which allow you to create your own user directory. This\nlets you register and authenticate users without having to run a separate user database\nand authentication service. Cognito supports synchronization of user application data\nacross different devices and has offline support that allows mobile devices to function\neven when there’s no internet access.\nA.14\nAuth0\nAuth0 (recently acquired by Okta) is a non-AWS identity management product that\nhas a few features that Cognito doesn’t. Auth0 integrates with more than 30 identity\nproviders including Google, Facebook, Twitter, Amazon, LinkedIn, and Windows\nLive. It provides a way to register new users through the use of its own user database,\nwithout having to integrate with an identity provider. In addition, it has a facility to\nimport users from other databases. As expected, Auth0 supports standard industry\nprotocols including SAML, OpenID Connect, OAuth 2.0, OAuth 1.0, and JSON Web\nToken (JWT). It’s simple to integrate with AWS Identity, Access Management, and\nCognito.\nA.15\nOther services\nThe list of services provided in this section is a short sample of the different products\nyou can use to build your application. There are many more services, including those\nprovided by large cloud-focused companies such as Google and Microsoft and smaller,\nindependent companies like Auth0. There are also auxiliary services that you need to\nbe aware of. These can help you be more efficient and build software faster, improve\nperformance, or achieve other goals. When building software, consider the following\nproducts and services: \nContent Delivery Networks (CloudFront, CloudFlare)\nDNS management (Route 53) \nCaching (ElastiCache)\nSource control (GitHub, GitLab) \nContinuous integration and deployment (GitHub Actions)\nFor every service suggestion, you can find alternatives that may be just as good or even\nbetter, depending on your circumstances. We urge you to do more research and\nexplore the various services that are currently available. \n",
      "page_number": 205
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 214-222)",
      "start_page": 214,
      "end_page": 222,
      "detection_method": "topic_boundary",
      "content": "200\nappendix B\nSetting up your cloud\nMost of the architecture described in this book is built on top of AWS. This means\nyou need a clear understanding of AWS from the perspectives of security, alerting,\nand costs. It doesn’t matter whether you use Lambda alone or have a large mix of\nservices. Being able to configure security, knowing how to set up alerts, and con-\ntrolling cost are important. This appendix is designed so that you can understand\nthese concerns and learn where to look for important information in AWS. \n AWS security is a complex subject, but this appendix gives you an overview of\nthe difference between users and roles and shows you how to create policies. This\ninformation is needed to configure a system in which services can communicate\neffectively and securely. Some of the time, you will not need to create or configure\npolicies directly; tools like Serverless framework will do it for you. But it’s still\nimportant to understand how the pieces fit together and where to look for help if\nthings go wrong. \n Cost is an important consideration when using a platform such as AWS and\nimplementing serverless architecture. It’s essential to understand the cost calcula-\ntion of the services you’re going to use. This is useful not only for avoiding bill\nshock but also for predicting next month’s bill and beyond. We look at estimating\nthe cost of services and discuss strategies for tracking costs and keeping them\nunder control. This appendix is not an exhaustive guide to AWS. If you have fur-\nther questions after reading this appendix, take a look at AWS documentation\n(https://aws.amazon.com/documentation).\nB.1\nSecurity model and identity management\nIn chapter 2, you created an Identity and Access Management (IAM) user and a\nnumber of roles in order to use Lambda, S3, and MediaConvert. In this section,\nyou’ll take your new-found knowledge and develop it further by learning about\nusers, groups, roles, and policies in more detail. \n\n\n201\nAPPENDIX B\nSetting up your cloud\nB.1.1\nCreating and managing IAM users\nAs you’ll recall, an IAM user is an entity in AWS that identifies a human user, an appli-\ncation, or a service. A user normally has a set of credentials and permissions that can\nbe used to access resources and services across AWS.\n An IAM user typically has a friendly name to help you identify the user and an\nAmazon Resource Name (ARN) that uniquely identifies it across AWS. Figure B.1\nshows a summary page and an ARN for a fictional user named Alfred. You can get to\nthis summary in the AWS console by clicking IAM, clicking Users in the navigation\npane, and then clicking the name of the user you want to view.\nYou can create IAM users to represent human users, applications, or services. IAM\nusers created to work on behalf of an application or a service sometimes are referred\nto as service accounts. These types of IAM users can access AWS service APIs using an\naccess key. An access key for an IAM user can be generated when the user is initially\ncreated, or you can create it later by clicking Users in the IAM console, clicking the\nrequired user name, selecting Security Credentials, and then clicking the Create\nAccess Key button.\n The two components of an access key are the Access Key ID and the Secret Access\nKey. The Access Key ID can be shared publicly, but the Secret Access Key must be kept\nhidden. If the Secret Access Key is revealed, the whole key must be immediately invali-\ndated and recreated. An IAM user can have, at most, two active access keys.\n If an IAM user is created for a real person, then that user should be assigned a\npassword. This password allows a human user to log into the AWS console and use ser-\nvices and APIs directly. To create a password for an IAM user, follow these steps:\n1.\nIn the IAM console, click Users in the navigation pane.\n2.\nClick the required username to open the user’s settings. \n3.\nClick the Security Credentials tab and then click Manage next to Console pass-\nword (figure B.2).\nThe ARN of the user Alfred\nFigure B.1\nThe IAM console shows metadata such as the ARN, groups, and creation time \nfor every IAM user in your account.\n\n\n202\nAPPENDIX B\nSetting up your cloud\n4.\nIn the popup, choose whether to enable or disable console access, type in a new\ncustom password, or let the system autogenerate one. You can also force the\nuser to create a new password at the next sign-in (figure B.3).\nThe Manage option is \navailable for any IAM user. \nUsers with passwords can \nlog into the AWS Console. \nFigure B.2\nIAM users have a number of options including being able to set a password, change \naccess keys, and enable multifactor authentication.\nAsking the user to set a new password is good practice, \nas long as a good password policy is established.\nFigure B.3\nMake sure to create a good password policy with a high degree of complexity if you allow \nusers to log into the AWS console. Password policy can be set up in Account Settings of the IAM console.\n\n\n203\nAPPENDIX B\nSetting up your cloud\nAfter a user is assigned a password, they can log into the AWS console by navigating to\nhttps://<Account-ID>.signin.aws.amazon.com/console. To get the account ID, click\nSupport in the upper-right navigation bar, and then click Support Center. The\naccount ID (or account number) is shown at the top of the console. You may want to\nset up an alias for the account ID also so that your users don’t have to remember it\n(for more information about aliases, see http://amzn.to/1MgvWvf). \nB.1.2\nGroups\nGroups represent a collection of IAM users. They provide an easy way to specify per-\nmissions for multiple users at once. For example, you may want to create a group for\ndevelopers or testers in your organization or have a group called Lambda to allow all\nmembers of that group to execute Lambda functions. Amazon recommends using\ngroups to assign permissions to IAM users rather than defining permissions individu-\nally. Any user who joins a group inherits permissions assigned to the group. Similarly,\nif a user leaves a group, the group’s permissions are removed from the user. Further-\nmore, groups can contain only users, not other groups or entities such as roles.\nB.1.3\nRoles\nA role is a set of permissions that a user, application, or a service can assume for a\nperiod of time. A role is not uniquely coupled to a specific user, nor does it have asso-\nciated credentials such as passwords or access keys. It’s designed to grant permissions\nto a user or a service that typically doesn’t have access to the required resource.\nMulti-factor authentication\nMulti-factor authentication (MFA) adds another layer of security by prompting users\nto enter an authentication code from their MFA device when they try to sign into the\nconsole (this is in addition to the usual username and password). It makes it more\ndifficult for an attacker to compromise an account. Any modern smartphone can act\nas a virtual MFA appliance using an application such as Google Authenticator or AWS\nVirtual MFA. It’s recommended that you enable MFA for any user who might use the\nAWS console. You’ll find the option Assign MFA Device in the Security Credentials tab\nwhen you click an IAM user in the console.\nTemporary security credentials\nAt this time, there’s a limit of 5,000 users per AWS account, but you can raise the\nlimit if needed. An alternative to increasing the number of users is to use temporary\nsecurity credentials. Temporary security credentials can be set up to expire after a\nshort while and can be generated dynamically. See Amazon’s online documentation\nat http://mng.bz/drnN for more information on temporary security credentials. You\ncan find more information about IAM users at http://mng.bz/r6zB.\n\n\n204\nAPPENDIX B\nSetting up your cloud\n Delegation is an important concept associated with roles. Put simply, delegation is\nconcerned with the granting of permissions to a third party to allow access to a partic-\nular resource. It involves establishing a trust relationship between a trusting account\nthat owns the resource and a trusted account that contains the users or applications\nthat need to access the resource. Figure B.4 shows a role with a trust relationship\nestablished for a service called CloudCheckr.\nFederation is another concept that’s discussed often in the context of roles. Federation\nis the process of creating a trust relationship between an external identity provider\nsuch as Facebook, Google, or an enterprise identity system that supports Security\nAssertion Markup Language (SAML) 2.0 and AWS. It enables users to log in via one of\nthose external identity providers and assume an IAM role with temporary credentials.\nB.1.4\nResources\nPermissions in AWS are either identity-based or resource-based. Identity-based permissions\nspecify what an IAM user or a role can do. Resource-based permissions specify what an\nAWS resource such as an S3 bucket or an SNS topic is allowed to do or who can have\nTrusted entities define which entities \nare allowed to assume the role.\nAn external ID prevents the confused \ndeputy problem, which is a form of \nprivilege escalation. It is needed if you \nhave configured access for a third party \nto gain entry to your AWS account.\nFigure B.4\nThis role grants CloudCheckr access to the AWS account to perform analysis of costs \nand recommend improvements.\n\n\n205\nAPPENDIX B\nSetting up your cloud\naccess to it. A resource-based policy often specifies who has access to the given\nresource. This allows trusted users to access the resource without having to assume a\nrole. The AWS user guide at http://mng.bz/VBJP states: \nCross-account access with a resource-based policy has an advantage over a role. With a\nresource that is accessed through a resource-based policy, the user still works in the trusted\naccount and does not have to give up his or her user permissions in place of the role\npermissions. In other words, the user continues to have access to resources in the trusted\naccount at the same time as he or she has access to the resource in the trusting account.\nNot all AWS services support resource-based policies (the user guide at http://\nmng.bz/xX8W lists all the services that do).\nB.1.5\nPermissions and policies\nWhen you initially create an IAM user, it’s not able to access or do anything in your\naccount. You need to grant the user permissions by creating a policy that describes\nwhat the user is allowed to do. The same goes for a new group or role. A new group or\na role needs to be assigned a policy to have any effect. \n The scope of any policy can vary. You can give your user or role administrator access\nto the whole account or specify individual actions. It’s better to be granular and specify\nonly permissions that are needed to get the job done (least privilege access). Start with\na minimum set of permissions and add additional permissions only if necessary. \n There are two types of policies: managed and inline. Managed policies apply to\nusers, groups, and roles but not to resources. Managed policies are standalone. Some\nmanaged policies are created and maintained by AWS. You also can also create and\nmaintain customer-managed policies. Managed policies are great for reusability and\nchange management. If you use a customer-managed policy and decide to modify it,\nall changes are automatically applied to all IAM users, roles, and groups that the pol-\nicy is attached to. Managed policies allow for easier versioning and rollbacks. \n Inline policies are created and attached directly to a specific user, group, or role.\nWhen an entity is deleted, the inline policies embedded within it are deleted also.\nResource-based policies are always inline. To add an inline or a managed policy, click\nthe required user, group, or role and then click the Permissions tab. You can attach,\nview, or detach a managed policy and similarly create, view, or remove an inline policy.\n A policy is specified using JSON notation. The following listing shows a managed\nAWSLambdaExecute policy. \n{  \n   \"Version\":\"2012-10-17\",   \n   \"Statement\":[                 \n      {  \n         \"Effect\":\"Allow\",\nListing B.1\nAWSLambdaExecute policy\nVersion specifies the policy language version; the current version is 2012-10-17. If you’re \ncreating a custom policy, make sure to include the version and set it to 2012-10-17.\nContains one or more statements that specify \nthe actual permissions that make up the policy\n\n\n206\nAPPENDIX B\nSetting up your cloud\n         \"Action\": \"logs:*\",\n         \"Resource\":\"arn:aws:logs:*:*:*\"\n      },\n      {  \n         \"Effect\":\"Allow\",      \n         \"Action\":[              \n            \"s3:GetObject\",\n            \"s3:PutObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::*\" \n      }\n   ]\n}\nMany IAM policies contain additional elements such as Principal, Sid, and Condi-\ntion. The Principal element specifies an IAM user, an account, or a service that’s\nallowed or denied access to a resource. The Principal element isn’t used in policies\nthat are attached to IAM users or groups. Instead, it’s used in roles to specify who can\nassume the role. It’s also common to resource-based policies. Statement ID (Sid) is\nrequired in policies for certain AWS services, such as SNS. A condition allows you to\nspecify rules that dictate when a policy should apply. An example of a condition is pre-\nsented in the next listing. \n\"Condition\": {\n   \"DateLessThan\": {     \n               \"aws:CurrentTime\": \"2020-09-12T12:00:00Z\"\n        },\n        \"IpAddress\": {\n               \"aws:SourceIp\": \"127.0.0.1\"   \n        }\n   }\n \nListing B.2\nPolicy condition\nThe Effect element is required and specifies \nwhether the statement allows or denies \naccess to the resource. The only two \navailable options are Allow and Deny.\nSpecifies the specific actions on the resource that should \nbe allowed or denied. The use of a wildcard (*) character \nis allowed (for example, “Action”: “s3:*”).\nThe Resource element identifies the object or objects \nthat the statement applies to. It can be specific or \ninclude a wildcard to refer to multiple entities.\nYou can use a number of conditional elements, which include DateEquals, DateLessThan, \nDateMoreThan, StringEquals, StringLike, StringNotEquals, and ArnEquals.\nThe condition keys represent values that \ncome from the request issued by a user. \nPossible keys include SourceIp, \nCurrentTime, Referer, SourceArn, userid, \nand username. The value can be either a \nspecific literal value such as “127.0.0.1” or \na policy variable.\nMultiple conditions\nThe AWS documentation at http://amzn.to/21UofNi states “If there are multiple con-\ndition operators, or if there are multiple keys attached to a single condition operator,\nthe conditions are evaluated using a logical AND. If a single condition operator\nincludes multiple values for one key, that condition operator is evaluated using a log-\nical OR.” See http://amzn.to/21UofNi for great examples you can follow and a whole\nheap of useful documentation.\n\n\n207\nAPPENDIX B\nSetting up your cloud\nAmazon recommends using conditions to the extent that is practical for security. The\nnext listing, for example, shows an S3 bucket policy that forces content to be served\nonly over HTTPS/SSL. This policy refuses connections over unencrypted HTTP.\n{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"123\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",                \n            \"Principal\": \"*\",\n            \"Action\": \"s3:*\",                    \n            \"Resource\": \"arn:aws:s3:::my-bucket/*\",\n            \"Condition\": {\n                \"Bool\": {\n                    \"aws:SecureTransport\": false  \n                }\n            }\n        }\n    ]\n}\nB.2\nCost\nReceiving an unpleasant surprise in the form of a large bill at the end of the month is\ndisappointing and stressful. Amazon CloudWatch can create billing alarms that send\nnotifications if total charges for the month exceed a predefined threshold. This is use-\nful not only to avoid unexpectedly large bills but also to catch potential misconfigura-\ntions of your system. \n For example, it’s easy to misconfigure a Lambda function and inadvertently allo-\ncate 3.0 GB of RAM to it. The function might not do anything useful except wait for\n15 s to receive a response from a database. In a heavy-duty environment, the system\nmight perform 2 M invocations of the function a month, costing a little over $1,462.\nThe same function with 128 MB of RAM would cost around $56 per month. If you per-\nform cost calculations up front and have a sensible billing alarm, you’ll quickly realize\nthat something is going on when billing alerts begin to come through. \nB.2.1\nCreating billing alerts\nFollow these steps to create a billing alert:\n1.\nIn the main AWS console, click your name (or the name of the IAM user that’s\nrepresenting you) and then click My Billing Dashboard. \n2.\nClick Billing Preferences in the navigation pane and then enable the check box\nnext to Receive Billing Alerts. \n3.\nClick Save preferences, then go back to the main AWS console and find the\nCloudWatch service. \nListing B.3\nPolicy to enforce HTTPS/SSL\nExplicitly denies access to \ns3 if the condition is met\nThe condition is met when requests \nare not sent using SSL. This forces \nthe policy to block access to the \nbucket if a user tries to access it \nover regular, unencrypted HTTP.\n\n\n208\nAPPENDIX B\nSetting up your cloud\n4.\nOpen the CloudWatch service, click Alarms, and select All Alarms in the naviga-\ntion pane. Click the Create alarm button and then click the Select metric button.\n5.\nUnder the Metrics heading, select Billing and click Total Estimated Charges. (If\nyou don’t see Billing it means you may not have enabled the Receive Billing\nAlerts option in step 2). \n6.\nTick the checkbox for EstimatedCharges and click Select metric to continue.\n7.\nMake sure that the Threshold type is set to Static and that Whenever Estimated-\nCharges is set to Greater. \n8.\nIn the Define the threshold value, enter the amount that you’d like to trigger\nthe alarm (for example, 200 as seen in figure B.5). \n9.\nClick Next to continue to the next page.\nHere you can set or create a new SNS topic to notify you when the alarm is\ntriggered. This is important! You need an SNS topic to receive emails to alert\nyou what is happening. \n10.\nClick the Add notification button.\n11.\nChoose Create new topic, enter a name for it, and then type in your email\naddress. Click Create topic button to save your SNS topic settings. When you\nare ready to proceed click Next. \n12.\nType in a name for your Alarm and click Next again.\n13.\nFinally, at the bottom, click the Create alarm button to finish. \nFigure B.5\nIt’s good practice to create multiple billing alarms to keep you informed of ongoing costs.\nThis key is to set the amount \nthat will let you know when \nyou go over your budget. \nYou want to stay in budget \nall of the time and not let \nthe costs blow out.\n",
      "page_number": 214
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 223-231)",
      "start_page": 223,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": "209\nAPPENDIX B\nSetting up your cloud\nB.2.2\nMonitoring and optimizing costs\nServices such as CloudCheckr (http://cloudcheckr.com) can help to track costs, send\nalerts, and even suggest savings by analyzing services and resources in use. CloudCheckr\ncomprises several different AWS services including S3, CloudSearch, SES, SNS, and\nDynamoDB. It’s richer in features and easier to use than some of the standard AWS fea-\ntures. It’s worth considering for its recommendations and daily notifications.\n AWS also has a service called Trusted Advisor that suggests improvements to per-\nformance, fault tolerance, security, and cost optimization. Unfortunately, the free ver-\nsion of Trusted Advisor is limited, so if you want to explore all of the features and\nrecommendations it has to offer, you must upgrade to a paid monthly plan or access it\nthrough an AWS enterprise account.\n Cost Explorer (figure B.6) is a useful, albeit high-level reporting and analytics tool\nbuilt into AWS. You must activate it first by clicking your name (or the IAM username)\nin the top-right corner of the AWS console, selecting My Billing Dashboard, then\nclicking Cost Explorer from the navigation pane and enabling it. Cost Explorer ana-\nlyzes your costs for the current month and the past four months. It then creates a fore-\ncast for the next three months. Initially, you may not see any information because it\ntakes 24 hours for AWS to process data for the current month. Processing data for pre-\nvious months make take even longer. More information about Cost Explorer is avail-\nable at http://amzn.to/1KvN0g2.\nFigure B.6\nCost Explorer allows you to review historical costs and estimate what future costs may be.\nYou have access to plenty of filters \nand you can create custom reports. \nHowever, becoming an expert at \nCost Explorer can take some time. \n\n\n210\nAPPENDIX B\nSetting up your cloud\nB.2.3\nUsing the Simple Monthly Calculator\nThe AWS Pricing Calculator (https://calculator.aws) is a web application developed\nby Amazon to help model costs for many of its services. This tool allows you to select a\nservice, enter information related to the consumption of that particular resource, and\nget an indicative cost. \nB.2.4\nCalculating Lambda and API Gateway costs\nThe cost of running serverless architecture often can be a lot less than running tradi-\ntional infrastructure. Naturally, the cost of each service you might use will be different,\nbut you can look at what it takes to run a serverless system with Lambda and the API\nGateway. \n Amazon’s pricing for Lambda (https://aws.amazon.com/lambda/pricing/) is\nbased on the number of requests, duration of execution, and the amount of memory\nallocated to the function. The first million requests are free with each subsequent mil-\nlion charged at $0.20. Duration is based on how long the function takes to execute\nmeasured to the millisecond (ms). Amazon charges in 1 ms increments, while also fac-\ntoring in the amount of memory reserved for the function. A function created with 1\nGB of memory will cost $0.000001667 per 100 ms of execution time, whereas a func-\ntion created with 128 MB of memory will cost $0.000000208 per 100 ms. \nNOTE\nAmazon prices may differ depending on the region and that they’re\nsubject to change at any time.\nAmazon provides a perpetual free tier with 1M free requests and 400,000 GB-seconds\nof compute time per month. This means that a user can perform a million requests\nand spend an equivalent of 400,000 seconds running a function created with 1 GB of\nmemory before they have to pay. As an example, consider a scenario where you have\nto run a 256 MB function, 5 million times a month. The function executes for 2 sec-\nonds each time. The cost calculation follows:\nMonthly request charge: \n– The free tier provides 1 million requests, which means that there are only 4\nmillion billable requests (5M requests – 1M free requests = 4M requests). \n– Each million is priced at $0.20, which makes the request charge $0.80 (4M\nrequests × $0.2/M = $0.80).\nMonthly compute charge:\n– The compute price for a function per GB-second is $0.00001667. The free\ntier provides 400,000 GB-seconds free. \n– In the compute price scenario, the function runs for 10 ms (5M × 2s). \n– 10M seconds at 256 MB of memory equates to 2,500,000 GB-seconds\n(10,000,000 × 256 MB / 1024 = 2,500,000). \n– The total billable amount of GB-seconds for the month is 2,100,000\n(2,500,000 GB-seconds – 400,000 free tier GB-seconds = 2,100,000). The\n\n\n211\nAPPENDIX B\nSetting up your cloud\ncompute charge is therefore $35.007 (2,100,000 GB-seconds × $0.00001667 =\n$35.007). \n– The total cost of running Lambda in this example is $35.807. \nThe API Gateway pricing is based on the number of API calls received and the\namount of data transferred out of AWS. In the eastern United States, Amazon charges\n$3.50 for each million API calls received and $0.09/GB for the first 10 TB transferred\nout. Given the previous example and assuming that monthly outbound data transfer is\n100 GB a month, the API Gateway pricing is as follows:\nMonthly API charge:\n– The free tier includes 1M API calls per month but is valid for only 12\nmonths. Given that it’s not a perpetual free tier, it won’t be included in this\ncalculation. \n– The total API cost is $17.50 (5M requests × $3.50/M = $17.50).\nThe monthly data charge is $9.00 (100 GB × $0.09/GB = $9).\nThe API Gateway cost in this example is $26.50. \nThe total cost of Lambda and the API Gateway is $62.307 per month. \nIt’s worthwhile to attempt to model how many requests and operations you may have\nto handle on an ongoing basis. If you expect 2M invocations of a Lambda function\nthat uses only 128 MB of memory and runs for 1 second, you’ll pay approximately\n$0.20 month. If you expect 2M invocations of a function with 512 MB of RAM that\nruns for 5 seconds, you’ll pay a little more than $75.00. With Lambda, you have an\nopportunity to assess costs, plan ahead, and pay for only what you actually use. Finally,\ndon’t forget to factor in other services such as S3 or SNS, no matter how insignificant\ntheir cost may seem to be.\n\n\n212\nappendix C\nDeployment frameworks\nAutomation and continuous delivery are important if you’re building anything on\na cloud platform such as AWS. If you take a serverless approach, it becomes even\nmore critical because you end up having more services, more functions, and more\nthings to configure. You need to be able to script your entire application, run tests,\nand deploy it automatically. The only time you should deploy Lambda functions\nmanually or self-configure API Gateway is while you learn. Once you begin working\non real serverless applications, you need to have a repeatable, automated, and\nrobust way of provisioning your system. Apart from Terraform, the other frame-\nworks discussed in this appendix do not provision resources on their own. Instead,\nthey rely on AWS CloudFormation (https://aws.amazon.com/cloudformation/) to\nprovision resources and are therefore bound by CloudFormation’s limitations.\nThese include the following:\nA CloudFormation template can have no more than 500 resources. To go\nbeyond this limit, you can use nested CloudFormation stacks.\nA CloudFormation template can have no more than 200 parameters or\noutputs.\nIt’s cumbersome to add existing resources to a CloudFormation stack.\nAlthough Terraform alleviates these limitations, it has shortcomings of its own. The\nmost notable of which is the lack of support for rollback. If there was a problem\nduring a deployment, then your application can end up in a broken state if some\nresources are updated but others are not.\n Some of the frameworks discussed in this appendix also provide additional utili-\nties, such as the ability to invoke Lambda functions locally or even simulate API\nGateway locally. With that said, let’s go through some of the most popular deploy-\nment frameworks for serverless applications.\n\n\n213\nAPPENDIX C\nDeployment frameworks\nC.1\nServerless Framework\nThe Serverless Framework (https://serverless.com) is an open source framework and\nis easily one of the most popular and mature deployment frameworks out there. At its\nessence, it allows users to define an entire serverless application (including Lambda\nfunctions, API Gateway APIs, SNS topics, and any other CloudFormation resources)\nand then deploy it using a command-line interface (CLI). It helps you organize and\nstructure serverless applications, which is of great benefit as you begin to build larger\nsystems, and it’s fully extensible via its plugin system.\nC.1.1\nGetting started\nThe Serverless Framework supports both JSON and YAML. It also lets you describe\nyour application in a manifest file like that shown in the following listing.\nservice: user-service  \nprovider:                            \n  name: aws\n  runtime: nodejs12.x\n  region: us-east-1\n  stage: dev\nfunctions:                       \n usersCreate:\n   events:                   \n     - http: \n         path: users/create\n         method: post\n usersDelete:\n   events:\n     - http: \n         path: users/delete\n         method: delete\nresource:                          \n  Resources:\n    UserTable:\n      Type: AWS::DynamoDB::Table\n      Properties:\n        BillingMode: PAY_PER_REQUEST\n        KeySchema:\n          ...\nListing C.1\nDescribing a service in serverless.yml\nThe name of the service. This would appear as part of the name for the \ngenerated CloudFormation stack as well as any provisioned API Gateway \nAPIs and Lambda functions.\nTop level configuration for the project: \nthe language runtime for the Lambda \nfunctions, the region, and the name of \nthe deployment stage.\nYour functions\nThe events that trigger \nthese functions\nThe resource your functions use. Raw \nAWS CloudFormation syntax goes here.\n\n\n214\nAPPENDIX C\nDeployment frameworks\nTo deploy the application, you only have to run a single command:\nserverless deploy\nThe Serverless Framework packages your code, uploads it to S3, and provisions the\nresources specified in the serverless.yml through CloudFormation. You can also over-\nride the default region and stage name with CLI options as the following shows\n(http://mng.bz/AOJz):\nserverless deploy -s prod -r eu-west-1\nC.1.2\nLanguage support\nThe Serverless Framework supports a number of language runtimes: Node.js, Python,\nJava, Golang, C#, and Scala to name a few. You have a lot of control over how the\nServerless Framework packages your functions. By default, it uses the same packaged\nartifact for all the functions you have configured in the serverless.yml. But you can\noptionally package each function separately and include or exclude specific folders or\nfiles.\n Through the serverless-webpack plugin (https://bit.ly/sls-webpack), you can\nalso incorporate webpack into the packaging process to tree shake and bundle Java-\nScript functions. Doing so can produce much smaller artifacts, which helps with both\ndeployment time as well as cold-start performance.\n For Python functions, it can be challenging to include third-party libraries into the\ndeployment artifact. The serverless-python-requirements plugin (http://bit.ly/\nsls-python-reqs) handles this for you transparently and lets you use your existing\nrequirements.txt file.\nC.1.3\nInvoking functions locally\nBesides packaging and deploying serverless applications, the Serverless Framework\nalso has a number of useful utilities. The most notable is the ability to invoke func-\ntions locally using the invoke local command:\nserverless invoke local -f functionName -d “{}”\nThe invoke local command is useful for quickly testing a function locally. It gives\nyou fast feedback without having to deploy the function to AWS first. You can also\nattach a debugger and step through the code line by line (for more information, see\nthis post http://bit.ly/sls-debug-vscode for how to do it with VS Code).\n But what if you want to emulate API Gateway locally? The serverless-offline\nplugin (http://bit.ly/sls-offline) lets you do exactly that and emulates API Gateway on\na localhost post. We find this useful when doing server-side rendering with Lambda.\nAlthough we can use invoke local to test a function locally and inspect its output, we\ncan’t render HTML in our heads! Having a local endpoint lets us point a browser to it\nand inspect the server-side rendered HTML in all its CSS glory.\n\n\n215\nAPPENDIX C\nDeployment frameworks\nC.1.4\nPlugins\nThe Serverless Framework has a rich ecosystem of plugins that extend its capability far\nand beyond what the framework is capable of out-of-the-box. Some plugins modify the\nCloudFormation template the Serverless Framework generates. For example, whereas\nthe Serverless Framework generates a shared identity and access management (IAM)\nrole for all the functions in a project, the popular serverless-iam-roles-per-function\nplugin lets you configure IAM roles for each function.\n Some plugins add support for services that the Serverless Framework does not sup-\nport natively. For example, the Serverless Framework does not support AppSync out\nof the box. You can still configure an AppSync API in the serverless.yml using raw\nCloudFormation syntax (in the resources section of the serverless.yml), but this is\ntedious and laborious. The serverless-appsync-plugin plugin extends the Server-\nless Framework to support AppSync and lets you configure AppSync APIs with a much\nmore succinct syntax. Similarly, the serverless-step-functions plugin adds support\nfor Step Functions.\n Some plugins can add additional commands to the Serverless Framework’s CLI.\nFor example, the serverless-offline plugin adds an offline command that starts a\nlocal instance of API Gateway. Similarly, the serverless-export-env plugin adds an\nexport-env command that captures the environment variables referenced by the\nLambda functions and exports them to a .env file. \n The Serverless Framework has a flexible plugin architecture and lets you custom-\nize just about everything the framework does. This flexibility allows you to disagree\nwith framework defaults and tailor its behavior to suit your needs. Its rich ecosystem of\navailable plugins is also what sets it apart from AWS SAM.\nC.2\nServerless Application Model (SAM)\nThe Serverless Application Model (https://aws.amazon.com/serverless/sam) (SAM),\nis AWS’s answer to the Serverless Framework and shares many similarities with the\nServerless Framework. Like the Serverless Framework, SAM uses CloudFormation to\nprovision resources and lets you use a simpler (compared with CloudFormation) syn-\ntax to define serverless applications in terms of Lambda functions, API Gateway, and\nso on. It also has a number of CLI commands that let you invoke Lambda functions\nlocally or start a local instance of API Gateway too. The biggest difference between\nSAM and the Serverless Framework is that SAM’s syntax is much closer to the raw\nCloudFormation syntax, and it doesn’t have a plugin system.\n The former is often held as a reason why one should favor SAM over the Serverless\nFramework, but it’s a question of personal preference. Ultimately, the CloudForma-\ntion syntax is verbose, and that's one reason why we prefer to use these frameworks\nthat offer a simpler syntax and more productive abstraction level to work with. So why\nshould one favor a framework because its syntax is closer to the thing that you try to\nget away from? It doesn’t make sense.\n\n\n216\nAPPENDIX C\nDeployment frameworks\n The lack of a plugin system, on the other hand, is often a deal breaker. It means\nyou’re limited by what the framework supports and have no easy way to override the\nframework defaults (unless the framework makes it a configurable option, of course).\nFor example, although SAM added support for Step Functions in May 2020 (which is\nmore than three years after the serverless-step-functions plugin did the same for\nthe Serverless Framework), it still has no support for AppSync at the time of writing\n(April 2021).\n And while the Serverless Framework’s plugin system offers an escape hatch for\nwhen you need to disagree with the framework's defaults, the lack of a plugin system\nrestricts you to what the framework allows you to configure with SAM. In order to dis-\nagree with the choices that SAM makes for you, you’d have to work around it with\nCloudFormation macros and use those macros to modify the SAM-generated Cloud-\nFormation template at deployment time. If this sounds like a tedious solution, it’s\nbecause it is as we learned the hard way two years ago (http://mng.bz/ZxJP).\n Having said that, SAM does certain things very well. For example, it lets you define\nIAM roles for individual functions out of the box. And the way it provisions API Gate-\nway resources is also more efficient (compared with the Serverless Framework) in\nterms of the number of CloudFormation resources. Whereas the Serverless Frame-\nwork would provision the API resources and methods as individual resources, SAM\nencodes all of them in the Body attribute of the AWS::ApiGateway::RestApi resource.\nThis approach minimizes the number of resources in the CloudFormation stack and\nhelps mitigate the risk of hitting the 500 resource limit in a CloudFormation stack.\nThis comes in handy in large API projects. With the Serverless Framework, these large\nprojects often have to rely on plugins such as the serverless-plugin-split-stacks\nplugin to work around the 500 resources limit.\nC.3\nTerraform\nTerraform (https://www.terraform.io) is a popular infrastructure-as-code (IaC) tool\nby HashiCorp. It is by far the least opinionated framework in this appendix. True to its\nmotto of “Write, Plan, and Create Infrastructure as Code,” Terraform has long been\nfavored by infrastructure engineers and is not designed with Lambda as its focus.\nInstead, it treats Lambda functions as AWS resources: nothing more, nothing less. As\nsuch, you have the utmost control and can configure Lambda, API Gateway, and any\nother resources however you like. But this exposes you to all the underlying complexi-\nties of those resources; complexities that the other tools try hard to manage for you. \n For example, you need to understand how API Gateway resources are organized,\nwhich we find is one of the most laborious aspects of using Terraform for Lambda. A\nsingle line of human-readable URL in the Serverless Framework or SAM can easily\ntranslate to 50 lines of Terraform code (figure C.1).\n Because Terraform is designed to give you a way to describe and create your infra-\nstructure, it doesn’t offer any value-add services for serverless applications. There’s no\n\n\n217\nAPPENDIX C\nDeployment frameworks\nlookup:\n   handler: functions/lookup.handler\n   description: handles the lookup/country/zipcode endpiont\n   events:\n      - http:\n           path: lookup/{country}/{zipcode}\n           method: get\nServerless framework\nresource \"aws_api_gateway_rest_api\" \"rest_api\" {\n   name = \"${var.stage}-${var.feature_name}\"\n   description = \"REST API for zipcode lookup\"\n}\nresource \"aws_api_gateway_resource\" \"lookup\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   parent_id = \"${aws_api_gateway_rest_api. rest_api.root_resource_id}\"\n   path_part = \"Lookup\"\n}\nresource \"aws_api_gateway_resource\" \"country\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   parent_id = \"${aws_api_gateway_resource.lookup.id}\"\n   path_part = \"{country}\"\n}\nresource \"aws_api_gateway_resource\" \"zipcode\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   parent_id = \"${aws_api_gateway_resource.country.id}\"\n   path_part = \"{zipcode}\"\n}\nresource \"aws_api_gateway_method\" \"get_zipcode\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   resource_id = \"${aws_api_gateway_resource.zipcode.id}\"\n   http_method = \"GET\"\n   authorization = \"NONE\"\n}\nresource \"aws_api_gateway_integration\" \"zipcode_lookup\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   resource_id = \"${aws_api_gateway_resource.zipcode.id}\"\n   type = “AWS_PROXY\"\n   http_method = \"${aws_api_gateway_method.get_zipcode.http_method}\"\n   integration_http_method = “POST\"\n   uri = “${aws_lambda_function.lookup.invoke_arn}\"\n}\nresource \"aws_api_gateway_deployment\" \"zipcode_api\" {\n   depends_on = [\n      \"aws_api_gateway_integration.zipcode_lookup\"\n   ]\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   stage_name = \"${var.stage}\"\n}\nTerraform\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n17\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\nFigure C.1\nConfiguring an API Gateway function with the Serverless Framework vs. Terraform\n",
      "page_number": 223
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 232-241)",
      "start_page": 232,
      "end_page": 241,
      "detection_method": "topic_boundary",
      "content": "218\nAPPENDIX C\nDeployment frameworks\nbuilt-in support for packaging your deployment artifact, nor is there any built-in sup-\nport for running functions locally.\n Whereas all the other tools in this list are built on top of CloudFormation, Terra-\nform does its own thing and relies on AWS APIs to create resources. This means Terra-\nform is not bound by CloudFormation limitations such as the aforementioned 500\nresources per stack, but it also lacks the capabilities that CloudFormation offers.\n For example, Terraform does not automatically rollback changes when a deploy-\nment fails halfway. Many HashiCorp fans would tell you that this is a feature, not a\nbug, but don’t let them fool you. You don’t want your application to be stuck in a half-\nway, broken state when a deployment fails.\n There are also other problems to consider when using Terraform in serverless\napplications. For instance, because Terraform uses the AWS APIs to create resources,\nit often runs into throttling limits that CloudFormation does not. A common example\nis the ResourceConflictException due to the number of concurrent updates to a\nLambda function. This can happen when you make certain changes to a Lambda\nfunction that requires multiple API calls to achieve. This has been a long-standing\nproblem (see this issue from 2018 at http://mng.bz/RqJK), and the only viable work-\naround is to daily-chain changes with depends_on clauses.\n Terraform keeps track of resource states and can persist them to data stores such as\nS3. However, it does not encrypt these state files, which means any sensitive informa-\ntion such as credentials and API keys are stored in plain text. It’s up to you to ensure\nthat the S3 bucket enables server-side encryption (SSE). To be even more secure, use\ncustomer-managed keys to ensure that only you can decrypt the data.\n Overall, the severe lack of productivity alone makes Terraform a bad choice when\nit comes to building serverless applications. We strongly recommend against it. How-\never, it’s taken a strong hold in the DevOps culture and many infrastructure teams\nmandate the use of Terraform within their organizations. If you’re struggling to con-\nvince your manager to let you use something other than Terraform in your serverless\nproject, then consider doing the following:\nShow them the difference in lines of code that you need to write for something as simple as\na single API endpoint. Translate this into development time and cost. For exam-\nple, “It’ll take a week to do with Terraform versus a couple of hours with Server-\nless Framework or SAM” is a convincing argument.\nExplain to the infrastructure team (they might be incorrectly labeled the DevOps team in\nyour organization) that there is an integration path between Terraform and the Serverless\nFramework or SAM. They can still use Terraform to provision shared infrastruc-\nture resources such as VPCs; they just need to share the ARNs or names of these\nresources as SSM parameters. Both the Serverless Framework and SAM can ref-\nerence these parameters. This way, both the infrastructure and feature teams\ncan use the right tool for the job and everyone’s happy.\n\n\n219\nAPPENDIX C\nDeployment frameworks\nC.4\nCloud Development Kit \nThe AWS Cloud Development Kit (CDK), available at https://aws.amazon.com/cdk,\nis a relatively new kid on the block but has received a lot of interest from the commu-\nnity. CDK differs from the aforementioned frameworks in that it does not use a\nmarkup language. Instead, CDK lets you describe the resources you want to provision\nusing a general-purpose programming language such as TypeScript or Python.\n It’s easy to see the appeal of using a general-purpose programming language in an\nIaC tool. Developers can use their favorite programming language to write their appli-\ncation as well as how it should be deployed. There’s no need to learn another lan-\nguage such as YAML or HCL (the JSON-based configuration language that Terraform\nuses). This doesn’t necessarily mean that CDK is a better IaC tool because it gives\ndevelopers what they want. After all, no matter how much we like eating cakes and\ncandies, it doesn’t change the fact that these sugary delights are bad for our health.\n For anyone who proclaims that YAML or HCL is not code, just remember that not\nlong ago, Java and .Net developers said the same thing about JavaScript and Python.\nThis kind of gatekeeping and putting others down to raise one’s standing happens in\nlots of places and have no place in our community. Configuration files are code. A Cloud-\nFormation template is a set of instructions to tell CloudFormation what resources to\nprovision and that is the dictionary definitions of code. Now that we got the common\nmisconceptions out of the way, let’s talk about where CDK really shines and the chal-\nlenges it faces.\nC.4.1\nWhere CDK shines\nGeneral-purpose programming languages give you much more expressive power com-\npared with configuration files like YAML. This makes CDK a fantastic choice when it\ncomes to templating some complex AWS environments. CloudFormation offers a\nrange of templating options with its intrinsic functions and conditionals, but these are\nlimited and often require complex YAML code to achieve basic branching logic or\nmapping input values against a dictionary. CDK makes these child’s play and can easily\nexpress them in a few lines of code in TypeScript or Python.\n Being able to use general-purpose programming languages like TypeScript and\nPython also means having access to the package managers for those languages. This\nmeans you can take common architectural patterns and create reusable constructs\nand share them as packages. The CDK Patterns (https://cdkpatterns.com) project is a\ngreat example of this. Instead of everyone taking the same recipe and implementing\nthese common patterns from scratch, you can download the relevant package from\nNPM and simply customize it. This is a great way to perpetuate and spread best prac-\ntices within a large organization. It makes it easy for teams to discover and share con-\nstructs that have those best practices and organizational norms baked in.\n\n\n220\nAPPENDIX C\nDeployment frameworks\nC.4.2\nCDK challenges\nSingle-page application (SPA) frameworks such as React and Vue.js have made a suc-\ncessful attempt at unifying HTML, CSS, and your application code into a cohesive and\nproductive JavaScript framework. CDK is doing something similar for infrastructure\ncode.\n However, whereas JavaScript is ubiquitous in the frontend world, the choice and\npreference for programming languages for backend applications are fractured and\ncontextualized around use case. One of the benefits of microservices is to allow teams\nto choose the best language for the job. For example, Node.js might be great for build-\ning REST APIs, but Python is better suited for machine learning (ML) workloads\nbecause most of the libraries are written in Python. The fact that different teams in the\norganization would prefer to use a different language can present a problem for CDK.\n If everyone agrees on using one programming language, then CDK makes it easy\nto share reusable constructs. But if teams want to use different languages, then you\nhave to maintain different versions of these constructs. You can even see this problem\nmanifest in the patterns on https://cdkpatterns.com, where some patterns support\nTypeScript, Python, Java, and C#, but most don’t support all four languages.\n Our other concern about CDK is that, whereas everyone must write the same\nYAML if they want to provision resources with the same configurations, that’s not the\ncase with a general-purpose programming language. Personal preferences and idioms\ncan come into play and suddenly it requires more cognitive energy to understand the\ninfrastructure code. It’s no longer configuration—the infrastructure code now con-\ntains business logic.\n This is especially problematic for those infrastructure teams that need to oversee\nan organization’s AWS environment and provide guidance and oversight for feature\nteams. Suddenly, they must work with infrastructure code that's written in multiple\nlanguages that they might not be familiar with. And because this infrastructure code\ncan contain ample business logic, it makes it doubly hard for infrastructure teams to\ndo their job. This is why we still prefer the declarative approach of YAML and think\nthe fact that it’s difficult to add complex logic into infrastructure code is actually a\nblessing.\nC.5\nAmplify\nAWS Amplify (https://aws.amazon.com/amplify) is a set of tools and services that can\nbe used together to build frontend web and mobile applications quickly. It consists of\nthe following:\nAmplify CLI—A CLI tool that lets you configure AWS resources.\nAmplify libraries—A set of open source libraries that helps you consume AWS\nresources such as Cognito and AppSync.\nAmplify UI components—A collection of drop-in UI components that works with\nAWS resources to provide authentication, storage, and interactions.\n\n\n221\nAPPENDIX C\nDeployment frameworks\nAmplify console—An AWS service that builds and hosts your single-page applica-\ntion (think AWS’s version of Netlify: https://www.netlify.com).\nAmplify Admin UI—A visual UI that lets you provision and configure AWS\nresources as well as manage the data in your application.\nYou can use each of these Amplify tools independently. For example, many teams\nwould use the Amplify libraries and UI components without using the Amplify CLI or\nAdmin UI to manage AWS resources. For this comparison, we’ll consider only the\nAmplify CLI.\n Whereas the other frameworks we have discussed so far take a resource centric\nview of serverless applications, the Amplify CLI takes a utility-centric approach.\nInstead of configuring a Cognito User Pool as a resource, you would run the com-\nmand amplify add auth. The Amplify CLI would then prompt you with a few ques-\ntions about what you want to do. This would bootstrap a CloudFormation template\nand configure a Cognito User Pool and maybe a Cognito Identity Pool too, depend-\ning on how you answer the questions from the CLI. \n Similarly, you can use a single command to bootstrap a brand-new AppSync API:\namplify add api. You can then focus on defining the model of your API, and the\nAmplify CLI can generate a lot of the underlying AWS resources for you, including the\nrelevant AppSync resolvers and even the DynamoDB tables.\n As you can see, the Amplify CLI can make a lot of decisions for you and get things\nwired up quickly. As such, it’s targeted at a slightly different demographic of develop-\ners. The other deployment frameworks in this list are typically used by backend teams\nwho work with AWS daily. Amplify on the other hand, targets frontend–focused teams\nwho are not as well versed with AWS and just want something that works.\n It’s a powerful tool and gives a lot of power to these frontend–focused teams to\nbuild something quickly, without having to spend many hours learning about each of\nthe AWS services they need to use and configure. But there also lies the pitfall, that\nteams are not aware of and do not understand the decisions that Amplify CLI makes for\nthem and are not able to debug problems when they arise. For example, the Amplify\nCLI defaults to using DynamoDB scans for list operations in a GraphQL schema.\nAlthough this works, it’s not an optimal solution and can become problematic as the\nsystem scales because DynamoDB scans are expensive and should be used sparingly.\n Amplify CLI automates a lot of things, and that’s what makes it a productive tool.\nBut it also limits your ability to customize how those AWS resources are configured.\nWhen you reach the limit of what you can achieve with Amplify CLI, there’s currently\nno escape hatch to move away from it. Many teams have had to rewrite their entire\napplication from scratch when they reached this point, which can be a struggle because\nmany teams don’t understand what Amplify CLI has done for them and have a hard\ntime replicating the setup because they know only how to do things with Amplify.\n In our opinion, the ideal users for tools like Amplify CLI are developers who under-\nstand AWS well and have experience working with and configuring those underlying\n\n\n222\nAPPENDIX C\nDeployment frameworks\nresources. You shouldn’t automate things that you don’t understand, which puts you in\na dangerous position of being over reliant on the tool and lets the tail wag the dog.\n The Amplify team is working hard to address the problems that we have brought\nup here and are looking into building escape hatches so teams can transition away\nfrom it when they need to. And we are excited to see where it goes, but for the time\nbeing, we think Amplify CLI should be confined to building proof of concepts or very\nsimple applications. For production applications that need to be maintained and iter-\nated over time, the risk of running into blockers and not being able to easily transition\naway from it is too great. However, it shouldn’t stop you from using other Amplify\ncomponents such as the Amplify libraries in your frontend project or the Amplify con-\nsole to build and host your SPA.\n In this appendix, we looked at five of the most popular ways people are provision-\ning and deploying their serverless applications. We looked at the Serverless Frame-\nwork and SAM, which provide a layer of abstraction over CloudFormation to make it\neasier to build serverless applications. Both support a set of value-add CLI commands,\nsuch as being able to invoke functions locally or run a local instance of API Gateway,\nthat aid you in your development workflow.\n The main difference between the two is that the Serverless Framework has a flexi-\nble plugin system and a rich ecosystem of existing plugins that can extend the frame-\nwork’s capabilities. Whereas with SAM, you’re limited by what it supports, and there is\nalso no easy way to change the framework’s default behavior beyond the available con-\nfigurations.\n We also looked at Terraform, which is a popular IaC tool and used by many infra-\nstructure teams. We explained the problems with using Terraform in serverless appli-\ncations and why we strongly recommend against it. It’s an unproductive tool when it\ncomes to building serverless applications.\n Both CDK and Amplify CLI are relative newcomers in this space, and both have\ngained a lot of momentum and are attracting many admirers. Whereas the Serverless\nFramework, SAM, and Terraform all use a markup language to describe the resources\nfor your serverless application, CDK and Amplify CLI take different approaches.\n CDK lets you use general-purpose programming languages to describe the\nresources you want to provision. It’s a double-edged sword, which offers the full flexi-\nbility that a general-purpose programming language offers as well as the package\nmanagement system that comes with that language. It lets developers use their favor-\nite programming language for both their application code as well as their infrastruc-\nture code and can easily share reusable patterns as packages. However, it can also be\nproblematic in organizations where teams use different programming languages in\ntheir application code. This limits the ability to share CDK constructs because the cre-\nators of these constructs have to support multiple languages. Letting developers add\nbusiness logic to their infrastructure code opens the door for extensive customization\nfor complex AWS environments, but it also makes infrastructure code harder to com-\nprehend and govern by infrastructure teams.\n\n\n223\nAPPENDIX C\nDeployment frameworks\n Finally, with Amplify CLI, you’re not configuring AWS resources so much as saying\nwhat capabilities you need in your application. Amplify CLI makes the magic happen\nand configures the necessary AWS resources with sensible defaults based on your\ninput. It’s a super-productive tool and can help you build a fully working application\nin no time. But it’s also a black box and has no escape hatch that lets you transition\naway from it when you reach the limit of what it can do. This puts you in a precarious\nposition, where you face the real possibility of having to rebuild the application from\nthe ground up if you ever hit a snag with Amplify CLI.\n Each of these tools has its strengths, but none is perfect. A good principle that will\nstand you in good stead, regardless of what tool you decide to use, is to understand how\nthe AWS services you need to work with operate and how the deployment mechanism\nworks before you try to automate it. Blindly automating what you don’t understand is\ndangerous. But once you understand the underlying machineries, then you should\nlook for tools that allow you to move up the abstraction levels and be more productive.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n225\nindex\nNumerics\n24-Hour Video project\nAmazon Web Services\ncosts 19–20\nElemental MediaConvert service 28\ntesting in 36\nusing 20–21\nevent-driven pipeline 19\nlogs 37–38\nServerless Framework 29–36\nbringing to project 31–33\ncreating Lambda functions 33–36\nsetting up 29–30\nsystem preparation 21–29\nAWS Elemental MediaConvert service 28–29\ncreating buckets 25\nIdentity and Access Management 22–27\nsetting up 22\n2FA (2 Factor Authentication) 22\nA\nA Cloud Guru 41, 70–83\noriginal architecture 71–82\nGraphQL 77–80\nmicroservices 75–77\nsecurity in BFF environment 82\nservice discovery 80–81\nremnants of legacy 82–83\nALB (Application Load Balancer) 68\nAlexa skills 44\nAlgolia 197\nAmazon API Gateway service 195, 210–211\nAmazon Athena 160–163\nAmazon Cognito 199\nAmazon DynamoDB. See DynamoDB\nAmazon Echo 44\nAmazon Kinesis Data Analytics 89–90\nAmazon Kinesis Data Firehose 88–89, 158–160\nAmazon Kinesis Data Streams 86–87, 198\nAmazon QuickSight 163–164\nAmazon RDS (Relational Database Service) 197\nAmazon S3 (Simple Storage Service) 196\nAmazon SES (Simple Email Service) 196\nAmazon SNS (Simple Notification Service)\n195–196\nAmplify 220–223\nAnalytics Service 157–164\nAWS Glue and Amazon Athena 160–163\nKinesis Firehose 158–160\nQuickSight 163–164\nAngularJS 73\nAPI Gateway 9, 29, 43–44, 75, 80\ncalculating costs 210–211\noverview 195\nApplication Load Balancer (ALB) 68\nAppSync 198\nARN (Amazon Resource Name) 201\nAthena service 198\nattributes 197\nAuth0 73, 199\nAWS (Amazon Web Services) 200–211\nAppSync 198\nAthena service 198\ncosts 19–20, 207–211\ncalculating Lambda and API Gateway \ncosts 210–211\ncreating billing alerts 207–208\n\n\nINDEX\n226\nAWS (Amazon Web Services) (continued)\nmonitoring and optimizing 209\nPricing Calculator 210\nElemental MediaConvert service\ncreating roles 29\nendpoint 32\noutputs 35–36\noverview 28\nspecifying ARN for role 33\nGlue ETL 160–163\nIdentity and Access Management 200–207\ncreating and managing users 22–25, \n201–203\ncreating roles 26–27\ngroups 203\nLambda role ARN 32\npermissions and policies 205–207\nresources 204–205\nroles 203–204\nMedia Services 197–198\nmultiple accounts 184–186\nbetter autonomy for teams 185–186\nbetter cost monitoring 185\neliminating contention for shared \nservice limits 185\ninfrastructure-as-code for AWS \nOrganizations 186\nisolating security breaches 184\ntesting in 36\nusing 20–21\nX-Ray 175\nAWS AppSync 79\nAWS Elemental MediaConvert 19, 25–26, 134\nAWS Glue 150, 157, 198\nAWS Pricing Calculator 210\nAWS Step Functions 9\nAWS WAF 76\nAZs (availability zones) 85, 197\nB\nbackends 41\nBFF (Backends for Frontends) environment, \nsecurity 82\nBI (Business Intelligence) service 163\nblueprints, Lambda 44\nbots 44\nbuckets\ncreating 25\ndefined 196\ntranscoded video bucket 32\nuploading 32\nC\ncanary deployment 68\ncanary pattern 68\nCDK (Cloud Development Kit) 219–220\nadvantages of 219\nchallenges of 220\nCDU (Code Developer University) project 164\nAnalytics Service 157–164\nAWS Glue and Amazon Athena 160–163\nKinesis Firehose 158–160\nQuickSight 163–164\nCode Scoring Service 150–153\nSubmissions Queue 152–153\nsummary of 153\nrequirements 147–148\ngeneral requirements 148\nleaderboards 148\nreports 148\nusers and experience points 148\nsolution architecture 148–150\nStudent Profile Service 153–157\nUpdate Student Scores function 155–157\nCloudAMQP 59\nCloudCheckr 209\nCloudFormation 32, 36, 74, 76, 212, 215–216\nCloudFront 73, 75\nCloudSearch 63\nCloudWatch 37–38, 93, 174–175\nCMKs (Customer Managed Keys) 189\nCode Scoring Service 150–153\nSubmissions Queue 152–153\nsummary of 153\nCognito 199\ncold latency 173\ncold start penalty 173\ncommand pattern\noverview 46–47\nuses for 47\ncompute layer 168\ncompute-as-glue architecture 51\nconcurrency 180–181\ncorrelation between requests, latency, and \nconcurrency 181\nmanaging 181\nconcurrent executions 85\ncontainers 8\ncontextual bandits model 89\ncron jobs 102–109\ncost 107\nprecision 105\nscalability (hotspots) 105–107\nscalability (number of open tasks) 105\ncustom code, minimizing 14\nCustomer Managed Keys (CMKs) 189\n\n\nINDEX\n227\nD\ndata layer 168\ndata processing and manipulation 42\nDAX in-memory cache 197\ndelegation 204\ndeployment frameworks 212–223\nAmplify 220–223\nCDK 219–220\nadvantages of 219\nchallenges of 220\nSAM 215–216\nServerless Framework 213–215\ninvoking functions locally 214\nlanguage support 214\noverview 213–214\nplugins 215\nTerraform 216–218\nDLQ (Dead Letter Queue) feature 86, 144\nDRM (Digital Right Management) 198\nDSL (domain-specific language) 7\nduration 175\nDynamoDB 9, 74, 76, 79, 81, 83, 109–113, \n137–139, 143, 150, 157, 197\ncombining with SQS 122–125\nprecision 124\nscalability (hotspots) 124\nscalability (number of open tasks) 124\ncost 113\nprecision 111\nscalability (hotspots) 111–112\nscalability (number of open tasks) 111\nE\ne2e (end-to-end) tests, temporary stacks for 188\nEFS (Elastic File System) 137\nElasticSearch 63\nElemental MediaConvert service\ncreating roles 29\nendpoint 32\noutputs 35–36\noverview 28\nspecifying ARN for role 33\nemerging practices 193\navoiding sensitive data in plain text in \nenvironment variables 188–190\nhandling sensitive data securely 189–190\nvulnerabilities 189\nEventBridge 190–193\narchiving and replaying events 191–192\ncontent-based filtering 190–191\nmore targets 192\nschema discovery 191\ntopology 192–193\nmultiple AWS accounts 184–186\nbetter autonomy for teams 185–186\nbetter cost monitoring 185\neliminating contention for shared service \nlimits 185\ninfrastructure-as-code for AWS \nOrganizations 186\nisolating security breaches 184\ntemporary stacks 186–188\ncommon AWS account structure 186–187\nfor e2e tests 188\nfor feature branches 187–188\nendpoints 168\nenvironment variables, avoiding sensitive data \nin plain text in 188–190\nhandling sensitive data securely 189–190\nvulnerabilities 189\nevent-driven pipeline 19\nEventBridge 149–150, 153, 157, 190–193\narchiving and replaying events 191–192\ncontent-based filtering 190–191\ncron jobs with 102–109\ncost 107\nprecision 105\nscalability (hotspots) 105–107\nscalability (number of open tasks) 105\nmore targets 192\nschema discovery 191\ntopology 192–193\nExtract Audio Lambda function 142–143\nF\nFaaS (Functions as a Service) 9\nfan-out pattern\noverview 50–51\nuses for 51\nFargate 85–86\ncost considerations 85\nperformance considerations 85–86\nfederation 204\nFFmpeg library 136\nFirebase 73–74, 83\nfolders, in S3 196\nFunctions as a Service (FaaS) 9\nG\nGlue ETL 160–163\nGraphQL\nmoving to 79–80\noverview 45–46, 77–79\nuses for 46\n\n\nINDEX\n228\nH\nhard limits 91–92, 115\nHello World! 30\nhybrid approach 44–45\nI\nIaaS (Infrastructure as a Service) 8\nIAM (Identity and Access Management)\n91, 200–207, 215\ncreating and managing users 22–25, 201–203\ncreating roles 26–27\ngroups 203\nLambda role ARN 32\npermissions and policies 205–207\nresources 204–205\nroles 203–204\niConsent 127–128\nidentity-based permissions, in AWS 204\nindices, in Algolia 197\nInfrastructure as a Service (IaaS) 8\ninline policies 205\ninvocations 174\nIoT (Internet of Things) 41\nitems, in DynamoDB 197\nJ\nJest JavaScript framework 188\nK\nkeys, in S3 (Simple Storage Service) 196\nKinesis Data Analytics 89–90\nKinesis Data Firehose 88–89, 158–160\nKinesis Data Streams 42–43, 86–87, 90, 198\nKinesis Firehose 42–43, 91, 149, 157\nKMS (Key Management Service), AWS 189\nL\nLambda service 182\ncalculating costs 210–211\nconcurrency 180–181\ncorrelation between requests, latency, and \nconcurrency 181\nmanaging 181\nLambda functions\ncreating 33–36\ndeployment 36\nExtract Audio Lambda function 142–143\nMediaConvert outputs 35–36\nMerge Video Lambda function 143\nrequest handling 169–173\nRouter 88\nSplit and Convert Video Lambda function 143\nTranscode Video Lambda function 140–141\nLambda role ARN 32\nlatency 176–180\nallocating sufficient resource to your \nexecution environment 178–179\ncold vs. warm 173\nminimizing deployment artifact size 176–178\noptimizing function logic 179–180\nload generation 173–174\noptimization 167–168\nperformance and availability tracking 174–176\nAWS X-Ray 175\nCloudWatch metrics 174–175\nthird-party tools 176\nlatency 176–180\nallocating sufficient resource to your execution \nenvironment 178–179\ncold vs. warm 173\nminimizing deployment artifact size 176–178\noptimizing function logic 179–180\nlegacy API proxy 43–44\nload generation 173–174\nload testing 92\nlogs 37–38\nM\nmachine learning (ML) workloads 220\nmanaged policies 205\nMapReduce model 133–137\narchitecture overview 135–137\ntranscoding video 134–135\nMedia Services 197–198\nMediaConvert 197\nMediaLive 197\nMediaPackage 198\nMediaStore 198\nMediaTailor 198\nMerge Video and Audio function 141\nMerge Video function 141\nMerge Video Lambda function 143\nmessaging pattern\noverview 47–48\nuses for 49\nmethods 195\nmicroservices\nCloud Guru 75–77\nmigrating to new 64–68\nML (machine learning) workloads 220\nN\nn^2 segments 138, 140\nNetlify 73\nnpm (Node Package Manager) 20–21\n",
      "page_number": 232
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 242-252)",
      "start_page": 242,
      "end_page": 252,
      "detection_method": "topic_boundary",
      "content": "INDEX\n229\nO\nobjects, in S3 (Simple Storage Service) 196\noperations 197\norchestrators 155\nP\nPaaS (Platform as a Service) 8\nparallel computing 145\nalternative architecture for 144\nmaintaining state 138–141\nTranscode Video Lambda function 140–141\nMapReduce model 133–137\narchitecture overview 135–137\ntranscoding video 134–135\nStep Functions 141–144\nExtract Audio Lambda function 142–143\nMerge Video Lambda function 143\nSplit and Convert Video Lambda function 143\npatterns 45–53\ncommand pattern\noverview 46–47\nuses for 47\ncompute-as-glue architecture 51\nfan-out pattern\noverview 50–51\nuses for 51\nGraphQL\noverview 45–46\nuses for 46\nmessaging pattern\noverview 47–48\nuses for 49\npipes and filters pattern\noverview 52–53\nuses for 53\npriority queue pattern\noverview 49–50\nuses for 50\nperformance and availability tracking 174–176\nAWS X-Ray 175\nCloudWatch metrics 174–175\nthird-party tools 176\npipes and filters pattern\noverview 52–53\nuses for 53\nPlatform as a Service (PaaS) 8\nPricing Calculator 210\npriority queue pattern\noverview 49–50\nuses for 50\npublic cloud-based architectures 15\nPython 148, 214, 219\nQ\nQuickSight 163–164\nR\nRabbitMQ 59\nRDS (Relational Database Service) 197\nreal-time analytics 42–43\nrecords, Algolia 197\nRedshift, Amazon 76, 160\nRemindMe 127\nresource-based permissions 204\nresources, in Amazon API Gateway 195\nretry storm 94\nReward Router Lambda function 89\nRouter Lambda function 88\nS\nS3 (Simple Storage Service) 20, 41, 136, 196\nSAM (Serverless Application Model) 215–216\nscheduled services 44\nscheduling services for ad hoc tasks 131\napplications 125–130\niConsent 127–128\nRemindMe 127\nTournamentsRUs 127\ncron jobs with EventBridge 102–109\ncost 107\nprecision 105\nscalability (hotspots) 105–107\nscalability (number of open tasks) 105\ndefining nonfunctional requirements 101–102\nDynamoDB TTL 109–113\ncombining with SQS 122–125\ncost 113\nprecision 111\nscalability (hotspots) 111–112\nscalability (number of open tasks) 111\nSQS 119–122\ncombining with DynamoDB TTL 122–125\nprecision 121\nscalability (hotspots) 121–122\nscalability (number of open tasks) 121\nStep Functions 113–119\ncost 116\nextend scheduled time beyond 1 year 117\nprecision 115\nscalability (hotspots) 116\nscalability (number of open tasks) 116\nscaling for hotspots 117–118\nSecrets Manager 189\nserial invocation 154\nserver-side request forgery (SSRF) 184\n\n\nINDEX\n230\nServerless Application Model (SAM) 215–216\nserverless architectures 17, 40, 53\nAWS\ncosts 19–20\nElemental MediaConvert service 28\ntesting in 36\nusing 20–21\nconventional implementation 7–9\ndeciding to use 11–14\nevent-driven pipeline 19\nlogs 37–38\noverview 5–6\npatterns 45–53\ncommand pattern 46–47\ncompute-as-glue architecture 51\nfan-out pattern 50–51\nGraphQL 45–46\nmessaging pattern 47–49\npipes and filters pattern 52–53\npriority queue pattern 49–50\npros and cons of 14–16\nServerless Framework 29–36\nbringing to project 31–33\ncreating Lambda functions 33–36\nsetting up 29–30\nserverless implementation 9–10\nserverless, defined 4–5\nservice-oriented architecture and \nmicroservices 7\nsystem preparation 21–29\nAWS Elemental MediaConvert service 28–29\ncreating buckets 25\nIdentity and Access Management 22–27\nsetting up 22\nuse cases 40–45\nAlexa skills 44\nbackends 41\nbots 44\ndata processing and manipulation 42\nhybrids 44–45\nInternet of Things 41\nlegacy API proxy 43–44\nreal-time analytics 42–43\nscheduled services 44\nServerless Framework 29–36, 213–215\nbringing to project 31–33\nLambda role ARN 32\nMediaConvert endpoint 32\nMediaConvert role 33\ntranscoded video bucket 32\nuploading bucket 32\ncreating Lambda functions 33–36\ndeployment 36\nMediaConvert outputs 35–36\ninvoking functions locally 214\nlanguage support 214\noverview 213–214\nplugins 215\nsetting up 29–30\ncredentials 29\nHello World! 30\nservice accounts 201\nservice-oriented architecture (SOA), \nmicroservices and 7\nservices 195–199\nAlgolia 197\nAPI Gateway 195\nAppSync 198\nAthena 198\nAuth0 199\nCognito 199\nDynamoDB 197\nKinesis Streams 198\nMedia Services 197–198\nRelational Database Service 197\nSimple Email Service 196\nSimple Notification Service 195–196\nSimple Queue Service 196\nSimple Storage Service 196\nSES (Simple Email Service) 196\nshards, in Kinesis Streams 198\nSimple Storage Service (S3) 20, 41, 136, 196\nskills, in Alexa 44\nSlack 44\nSNS (Simple Notification Service) 195–196\nSOA (service-oriented architecture), \nmicroservices and 7\nSOAP (Simple Object Access Protocol) 44\nsoft limits 91–92, 115\nSPA (single-page application) frameworks 220\nSplit and Convert Video Lambda function 143\nSputnik 80–81\nSQS (Simple Queue Service) 119–122, 196\ncombining with DynamoDB TTL 122–125\nprecision 124\nscalability (hotspots) 124\nscalability (number of open tasks) 124\nprecision 121\nscalability (hotspots) 121–122\nscalability (number of open tasks) 121\nSQS DLQ (dead-letter queue) 87\nSSM Parameter Store 189\nSSRF (server-side request forgery) 184\nstate\nmaintaining in parallel computing 138–141\nTranscode Video Lambda function 140–141\nstateful microservices 75\nstateless microservices 75\nStep Functions 113–119, 141–144\ncost 116\n\n\nINDEX\n231\nStep Functions (continued)\nextending scheduled time 117\nExtract Audio Lambda function 142–143\nMerge Video Lambda function 143\nprecision 115\nscalability (hotspots) 116\nscalability (number of open tasks) 116\nscaling for hotspots 117–118\nSplit and Convert Video Lambda function 143\nstrangler pattern 62\nStudent Profile Service 153–157\nSubmissions Queue 152–153\nsystem preparation 21–29\nAWS Elemental MediaConvert service\ncreating roles 29\noverview 28\ncreating buckets 25\nIdentity and Access Management\ncreating roles 26–27\ncreating users 22–25\nsetting up 22\nT\ntables, in DynamoDB 197\ntemporary stacks 186–188\ncommon AWS account structure 186–187\nfor e2e tests 188\nfor feature branches 187–188\nTerraform 216–218\nthrottles 175\nthundering herd 94\nTournamentsRUs 127\nTranscode Video Lambda function 140–141\ntranscoding video 134–135\nTTL (time-to-live) feature 109, 137\nTypeScript 219\nU\nUpdate Student Scores function 155–157\nuse cases 40–45\nAlexa skills 44\nbackends 41\nbots 44\ndata processing and manipulation 42\nhybrid approach 44–45\nInternet of Things 41\nlegacy API proxy 43–44\nreal-time analytics 42–43\nscheduled services 44\nuser pools 199\nV\nvisibility timeout, in SQS 119\nVogel, Werner 93\nVPCs (virtual private cloud) 76\nW\nwarm latency 173\nX\nX-Ray 175\nY\nYle 84–95\nbatching is good for cost and efficiency 94–95\nbuilding with failure in mind 93–94\neverything fails, all the time 93–94\npaying attention to retry configurations 94\ncost estimation is tricky 95\ningesting events at scale with Fargate 85–86\ncost considerations 85\nperformance considerations 85–86\nknowing service limits 91–93\nalways load testing 92\nCloudWatch metric granularity 93\nprojecting throughput at every point along \npipeline 92\nsoft vs. hard limits 91–92\nsome limits have bigger blast radiuses than \nothers 92–93\nprocessing events in real-time 86–90\nKinesis Data Analytics 89–90\nKinesis Data Firehose 88–89\nKinesis Data Streams 86–87\nRouter Lambda function 88\nSQS dead-letter queue 87\nYubl 57–69\nmigrating to new microservices 64–68\nnew serverless architecture 61–63\nrearchitecting and rewriting 62\nsearch API 62–63\noriginal architecture 58–60\nlong feature delivery cycles 59–60\nperformance problems 59\nreasons for serverless 60\nscalability problems 59\n\n\nSbarski ● Cui ● Nair\nISBN: 978-1-61729-542-3\nM\naintaining server hardware and software can cost a lot of \ntime and money. Unlike traditional data center infra-\nstructure, serverless architectures offl  oad core tasks like \ndata storage and hardware management to pre-built cloud \nservices. Better yet, you can combine your own custom AWS \nLambda functions with other serverless services to create \nfeatures that automatically start and scale on demand, reduce \nhosting cost, and simplify maintenance.\nIn Serverless Architectures with AWS, Second Edition you’ll learn \nhow to design serverless systems using Lambda and other ser-\nvices on the AWS platform. You’ll explore event-driven com-\nputing and discover how others have used serverless designs \nsuccessfully. Th is new edition off ers real-world use cases and \npractical insights from several large-scale serverless systems. \nChapters on innovative serverless design patterns and architec-\ntures will help you become a complete cloud professional. \nWhat’s Inside\n● First steps with serverless computing\n● Th e principles of serverless design\n● Important patterns and architectures\n● Real-world architectures and their tradeoff s\nFor server-side and full-stack software developers.\nPeter Sbarski is VP of Education and Research at A Cloud \nGuru. Yan Cui is an independent AWS consultant and \neducator. Ajay Nair is one of the founding members of the \nAWS Lambda team.\nRegister this print book to get free access to all ebook formats. \nVisit https://www.manning.com/freebook\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nServerless Architectures on AWS \nSecond Edition\nAWS/CLOUD COMPUTING\nM A N N I N G\n“\nA comprehensive and \npractical review of the AWS \ns erverless landscape.”\n \n—Eugene Serdiouk\nPrimex Family of Companies\n“\nFilled with indispensable \nadvice you can use to take \nyour AWS serverless architec-\ntures to the next level.”\n \n—Sal DiStefano\nTravelers Insurance\n“\nAn excellent book \nproviding an overview and \nexplanations of common \nserverless architectures. \nA must-read for every \ncloud developer.”\n \n—Mikołaj Graf\nCloudsail Digital Solutions\n“\nA clear path to exploring \nthe many services \n  off ered by AWS.”\n—Giampiero Granatell\nManyDesigns\nSee first page\n",
      "page_number": 242
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 253-257)",
      "start_page": 253,
      "end_page": 257,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 253
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "M A N N I N G\nPeter Sbarski\nYan Cui\nAjay Nair\nSECOND EDITION\n",
      "content_length": 61,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "Praise for the First Edition\n“A comprehensive, clear and very practical guide to making the best use of AWS\nthroughout an application’s lifecycle. Highly recommended for anyone wanting to\nuse AWS for real-life applications!”\n—Alain Couniot, Head of Enterprise Architecture, STIB-MIVB, Belgium\n“Peter’s tome not only dives deep on Lambda, it also covers all the AWS components\nyour apps will need to run serverless. A soup-to-nuts tour de force. Well done!”\n—Sean Hull, Founder, iHeavy, Inc.\n“A great introduction for those using AWS, who want to implement a serverless\narchitecture.”\n—John Huffman, Senior Technical Consultant, Summa Technologies\n“This book is a fantastic introduction to serverless architectures and AWS. I wish\nevery technical book was as well written and easy to read! The book walks you step-by-\nstep through building a video portal, including integrating AWS Lambda, API\nGateway, S3, auth0 and Firebase. By the end you feel confident not only that you\nunderstand all the pieces and how everything fits together, but also that you are ready\nto start building your own app.”\n—Kent R. Spillner, Sr. Software Engineer, DRW\n",
      "content_length": 1141,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Serverless Architectures on AWS\nSECOND EDITION\nPETER SBARSKI, YAN CUI, AJAY NAIR\nM A N N I N G\nSHELTER ISLAND\n",
      "content_length": 110,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2022 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end.\nRecognizing also our responsibility to conserve the resources of our planet, Manning books are \nprinted on paper that is at least 15 percent recycled and processed without the use of elemental \nchlorine.\nThe author and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The author and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \nof the information herein.\nManning Publications Co.\nDevelopment editor: Toni Arritola\n20 Baldwin Road\nTechnical development editor: Brent Stains\nPO Box 761\nReview editor: Aleksandar Dragosavljević\nShelter Island, NY 11964\nProduction editor: Andy Marinkovich\nCopy editor: Frances Buran\nProofreader: Jason Everett\nTechnical proofreader: Niek Palm\nTypesetter: Gordan Salinovic\nCover designer: Marija Tudor\nISBN 9781617295423\nPrinted in the United States of America\n",
      "content_length": 2132,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": " To my mum and dad, who always supported and\nencouraged my passion for computing. \n                                                                                       —Peter Sbarski\n \n \n To my wife, who always supports and encourages me, and\nputs up with all my late-night coding sessions.\n                                                                                       —Yan Cui\n \n \n To my wife, my kids, my brother, and my parents, thank you\nfor giving me the purpose and time to do this.\n                                                                                       —Ajay Nair\n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 620,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "v\nbrief contents\nPART 1\nFIRST STEPS.....................................................................1\n1\n■\nGoing serverless\n3\n2\n■\nFirst steps to serverless\n18\n3\n■\nArchitectures and patterns\n40\nPART 2\nUSE CASES.....................................................................55\n4\n■\nYubl: Architecture highlights, lessons learned\n57\n5\n■\nA Cloud Guru: Architecture highlights, lessons \nlearned\n70\n6\n■\nYle: Architecture highlights, lessons learned\n84\nPART 3\nPRACTICUM ...................................................................97\n7\n■\nBuilding a scheduling service for ad hoc tasks\n99\n8\n■\nArchitecting serverless parallel computing\n132\n9\n■\nCode Developer University\n146\nPART 4\nTHE FUTURE................................................................165\n10\n■\nBlackbelt Lambda\n167\n11\n■\nEmerging practices\n183\n",
      "content_length": 819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "vii\ncontents\npreface\nxiii\nacknowledgments\nxv\nabout this book\nxviii\nabout the authors\nxx\nabout the cover illustration\nxxii\nPART 1 FIRST STEPS ...........................................................1\n1 \nGoing serverless\n3\n1.1\nWhat’s in a name?\n4\n1.2\nUnderstanding serverless architectures\n5\nService-oriented architecture and microservices\n7\n■Implementing \narchitecture the conventional way\n7\n■Implementing architecture \nthe serverless way\n9\n1.3\nMaking the call to go serverless\n11\n1.4\nServerless pros and cons\n14\n1.5\nWhat’s new in this second edition?\n16\n2 \nFirst steps to serverless\n18\n2.1\nBuilding a video-encoding pipeline\n19\nA quick note on AWS costs\n19\n■Using Amazon Web Services (AWS)\n20\n",
      "content_length": 696,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "CONTENTS\nviii\n2.2\nPreparing your system\n21\nSetting up your system\n22\n■Working with Identity and Access \nManagement (IAM)\n22\n■Let’s make a bucket\n25\n■Creating \nan IAM role\n26\n■Using AWS Elemental MediaConvert\n28\nUsing MediaConvert Role\n29\n2.3\nStarting with the Serverless Framework\n29\nSetting up the Serverless Framework\n29\n■Bringing Serverless \nFramework to The 24-Hour Video\n31\n■Creating your first Lambda \nfunction\n33\n2.4\nTesting in AWS\n36\n2.5\nLooking at logs\n37\n3 \nArchitectures and patterns\n40\n3.1\nUse cases\n40\nBackend compute\n41\n■Internet of Things (IoT)\n41\n■Data \nprocessing and manipulation\n42\n■Real-time analytics\n42\nLegacy API proxy\n43\n■Scheduled services\n44\n■Bots and \nskills\n44\n■Hybrids\n44\n3.2\nPatterns\n45\nGraphQL\n45\n■Command pattern\n46\n■Messaging \npattern\n47\n■Priority queue pattern\n49\n■Fan-out pattern\n50\nCompute as glue\n51\n■Pipes and filters pattern\n52\nPART 2 USE CASES ...........................................................55\n4 \nYubl: Architecture highlights, lessons learned\n57\n4.1\nThe original Yubl architecture\n58\nScalability problems\n59\n■Performance problems\n59\n■Long \nfeature delivery cycles\n59\n■Why serverless?\n60\n4.2\nThe new serverless Yubl architecture\n61\nRearchitecting and rewriting\n62\n■The new search API\n62\n4.3\nMigrating to new microservices gracefully\n64\n5 \nA Cloud Guru: Architecture highlights, lessons learned\n70\n5.1\nThe original architecture\n71\nThe journey to 43 microservices\n75\n■What is GraphQL\n77\nMoving to GraphQL\n79\n■Service discovery\n80\n■Security in \nthe BFF world\n82\n5.2\nRemnants of the legacy\n82\n",
      "content_length": 1541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "CONTENTS\nix\n6 \nYle: Architecture highlights, lessons learned\n84\n6.1\nIngesting events at scale with Fargate\n85\nCost considerations\n85\n■Performance considerations\n85\n6.2\nProcessing events in real-time\n86\nKinesis Data Streams\n86\n■SQS dead-letter queue (DLQ)\n87\nThe Router Lambda function\n88\n■Kinesis Data Firehose\n88\nKinesis Data Analytics\n89\n■Putting it altogether\n90\n6.3\nLessons learned\n91\nKnow your service limits\n91\n■Build with failure in mind\n93\nBatching is good for cost and efficiency\n94\n■Cost estimation is \ntricky\n95\nPART 3 PRACTICUM .........................................................97\n7 \nBuilding a scheduling service for ad hoc tasks\n99\n7.1\nDefining nonfunctional requirements\n101\n7.2\nCron job with EventBridge\n102\nYour scores\n104\n■Our scores\n105\n■Tweaking the \nsolution\n107\n■Final thoughts\n109\n7.3\nDynamoDB TTL\n109\nYour scores\n110\n■Our scores\n111\n■Final \nthoughts\n113\n7.4\nStep Functions\n113\nYour scores\n115\n■Our scores\n115\n■Tweaking the \nsolution\n116\n■Final thoughts\n119\n7.5\nSQS\n119\nYour scores\n120\n■Our scores\n120\n■Final thoughts\n122\n7.6\nCombining DynamoDB TTL with SQS\n122\nYour scores\n123\n■Our scores\n124\n■Final thoughts\n125\n7.7\nChoosing the right solution for your application\n125\n7.8\nThe applications\n125\nYour weights\n126\n■Our weights\n126\n■Scoring the solutions \nfor each application\n128\n8 \nArchitecting serverless parallel computing\n132\n8.1\nIntroduction to MapReduce\n133\nHow to transcode a video\n134\n■Architecture overview\n135\n",
      "content_length": 1449,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "CONTENTS\nx\n8.2\nArchitecture deep dive\n137\nMaintaining state\n138\n■Step Functions\n141\n8.3\nAn alternative architecture\n144\n9 \nCode Developer University\n146\n9.1\nSolution overview\n147\nRequirements listed\n147\n■Solution architecture\n148\n9.2\nThe Code Scoring Service\n150\nSubmissions Queue\n152\n■Code Scoring Service summary\n153\n9.3\nStudent Profile Service\n153\nUpdate Student Scores function\n155\n9.4\nAnalytics Service\n157\nKinesis Firehose\n158\n■AWS Glue and Amazon Athena\n160\nQuickSight\n163\nPART 4 THE FUTURE ......................................................165\n10 \nBlackbelt Lambda\n167\n10.1\nWhere to optimize?\n167\n10.2\nBefore we get started\n169\nHow a Lambda function handles requests\n169\n■Latency: \nCold vs. warm\n173\n■Load generation on your function and \napplication\n173\n■Tracking performance and availability\n174\n10.3\nOptimizing latency\n176\nMinimize deployment artifact size\n176\n■Allocate sufficient \nresources to your execution environment\n178\n■Optimize function \nlogic\n179\n10.4\nConcurrency\n180\nCorrelation between requests, latency, and concurrency\n181\nManaging concurrency\n181\n11 \nEmerging practices\n183\n11.1\nUsing multiple AWS accounts\n184\nIsolate security breaches\n184\n■Eliminate contention for shared \nservice limits\n185\n■Better cost monitoring\n185\n■Better \nautonomy for your teams\n185\n■Infrastructure-as-code for \nAWS Organizations\n186\n",
      "content_length": 1340,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "CONTENTS\nxi\n11.2\nUsing temporary stacks\n186\nCommon AWS account structure\n186\n■Use temporary stacks for \nfeature branches\n187\n■Use temporary stacks for e2e tests\n188\n11.3\nAvoid sensitive data in plain text in environment variables\n188\nAttackers can still get in\n189\n■Handle sensitive data \nsecurely\n189\n11.4\nUse EventBridge in event-driven architectures\n190\nContent-based filtering\n190\n■Schema discovery\n191\n■Archive \nand replay events\n191\n■More targets\n192\n■Topology\n192\nappendix  A\nServices for your serverless architecture\n195\nappendix  B\nSetting up your cloud\n200\nappendix  C\nDeployment frameworks\n212\nindex\n225\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 669,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "xiii\npreface\nServerless technologies occupy an exciting space at the moment. Products like AWS\nLambda and DynamoDB have been around for a few years, yet they still feel new and\nthrilling, sometimes mysterious or puzzling. Many folks worldwide discuss, learn, and\nimplement systems with serverless architectures, yet we haven’t yet seen a mass level of\nadoption like that of containers. Cloud providers such as AWS continue to grow. How-\never, individuals and organizations still ask questions such as, is serverless right for me,\nand how do I architect a system correctly from the myriad of available components\nand options? \n We’ve written this book to address some of the more interesting questions we’ve\nseen across the industry and our technical community. We decided to look at use cases\nfor serverless and explore problems that usually wouldn’t seem like a good fit. More\nimportantly, we’ve tried to convey what it is to have a serverless-first mindset. Our rec-\nipe is simple: When you have a problem, offload as much of the undifferentiated\nheavy lifting onto AWS or another provider and apply the principles of serverless\narchitectures. And, if that doesn’t produce a satisfactory answer, only then go and look\nat other technologies that may help. It’s important to reiterate that you should always\nuse the right tool for the right job. However, having a set of principles and practices,\nlike viewing a potential solution through a serverless prism first, gives you a map and\nhelps make better, more robust decisions.\n This book shows a few examples of us doing it in practice. We discuss how to\napproach several problems using serverless architectures, what criteria to consider, and\nhow to deal with architectural trade-offs. We also present three real-world companies\n",
      "content_length": 1780,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "PREFACE\nxiv\nthat have built interesting systems using serverless architectures. These companies\ndealt with the same kinds of problems you might be solving right now, so it’s worth\nchecking out those chapters to see what potential solutions or ideas exist. \n If you are entirely new to serverless architectures, do not worry! The first three\nchapters introduce you to serverless and even get you building a small application. If\nyou are an expert already, you will enjoy the last two chapters that go deeper into AWS\nLambda and discuss emerging practices. And, before we let you go, one other thing:\nthe vast majority of this second edition is new. If you read our first edition, we think\nthat you will find this a very different book. We hope you find something interesting\nand helpful in this book and come with us on this exciting serverless journey.\n",
      "content_length": 853,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "xv\nacknowledgments\nThe second edition of Serverless Architectures on AWS couldn’t have been written with-\nout the encouragement and support from my peers, colleagues, family, and friends. I\nam lucky to be surrounded by passionate technologists who continuously encourage,\ngive feedback, and provide invaluable advice.\n First and foremost, I want to say thank you to my two co-authors: Yan Cui and Ajay\nNair. I am fortunate to know these two fantastic world-class experts to whom educa-\ntion and community is always foremost. I cannot describe how thankful I am to Yan\nand Ajay for helping to write this book and making it uniquely special among the\ntechnical literature available today. I am forever grateful to both of you for being\nthere through this journey, teaching me, and sharing the benefit of your experience.\n Second, I would like to thank our editor, Toni Arritola, who once again made the\nwriting of this book a great experience. Toni did a lot of work on the first edition of\nthis book, and she worked just as hard on the second edition. It bears repeating again\nthat Toni’s thoughtful feedback on the book’s structure, language, and narrative was\nextraordinarily helpful. And, after all these years of dealing with my slipping dead-\nlines, her attention to detail and enthusiasm kept the book and its authors going.\n It goes without saying that I want to thank Sam Kroonenburg too. Sam originally\nintroduced me to AWS Lambda and the serverless mindset. He co-founded A Cloud\nGuru, the first truly serverless startup, and gave me the opportunity to hone my skills.\nIf it wasn’t for Sam and my experience at A Cloud Guru, this book wouldn’t exist. I\nwould be amiss if I also didn’t thank Ryan Kroonenburg, the other co-founder of A\nCloud Guru and Sam’s brother. Both Sam and Ryan played a big part in the\n",
      "content_length": 1817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "ACKNOWLEDGMENTS\nxvi\npopularization of serverless technologies with A Cloud Guru, and also the founding of\nthe first technology conference focused entirely on serverless called Serverlessconf\n(ask me for stories over a drink!). Thank you, Sam and Ryan! \n I’d also like to thank a few others who for years have given me great feedback and\nencouragement. A big thank you to Tim Wagner, Drew Firment, Allan Brown, Nick\nTriantafillou, Tait Brown, Alicia Cheah, Forrest Brazeal, Peter Hanssens, Kim Bonilla,\nIlia Mogilevsky, as well as my fellow AWS serverless heroes and all my colleagues and\nfriends at A Cloud Guru/Pluralsight. I’d also like to thank Mike Stephens from Man-\nning for helping to bring this book to fruition. \n To all the reviewers: Aliaksandra Sankova, Bonnie Malec, Borko Djurkovic, Camal\nÇakar, Carl Nygard, Chris Kottmyer, Christopher Fry, Daniel Vásquez, Eugene Serdi-\nouk, Giampiero Granatella, Gregory Reshetniak, Javier Collado Cabeza, Jose San\nLeandro, Julien Pohie, Kelly E. Hair, Kirstie G. McKenzie, Lucian-Paul Torje, Matteo\nGildone, Michael Kumm, Michal Rutka, Miguel Montalvo, Mikołaj Wawrzyniak, Pat-\nrick Steger, Paul Mcilwaine, Robert Kulagowski, Sal DiStefano, Sau Fai Fong, Shaun\nHickson, Steve Hansen, Valeriy Arsentyev, Vignesh Muthuthurai, and William Dixon,\nyour suggestions and feedback made this a better book. \n Finally, I’d like to thank my family, including my dad, my brothers Igor and Dimi-\ntri, and their spouses Rita and Alexandra. They’ve had to find more strength to listen\nto me go on about the book for yet another year. And thank you to Durdana Masud,\nwho helped me greatly throughout my writing, with both the first edition and the sec-\nond edition.\n                    —Peter Sbarski\nI would like to thank Peter Sbarski for the opportunity to contribute to this book, and\nToni Arritola for her help and guidance every step of the way. It has truly been a plea-\nsure and honor to work with them over the past 12 months.\n I would also like to thank Anahit Pogosova for sharing details of the amazing work\nthat she and her team at Yle have done. The knowledge she shared with me was very\nvaluable and contained so many useful and actional tips for anyone building data\npipelines using serverless technologies. I hope I have done her work justice in chapter\n6, even though I had to leave out so much. We can easily fill a whole book with the\ninformation she shared with me.\n I would also like to thank a few friends and colleagues who have given me opportu-\nnities and guidance along the way. I wouldn’t be the man I am today without you, and\nyour friendship means everything to me; I can’t wait to catch up with you all in person\nsoon. Big thanks to Darryl Jennings, Tom Newton, Brett Johansen, Domas Lasauskas,\nScott Smethurst, Diana Ionita, Simon Coutts, Bruno Tavares, Heitor Lessa, Erez\nBerkner, Aviad Mor, John Earner, Simone Basso, and Alessandro Simi.\n Last, but not least, I would like to thank my wonderful wife, Yinan Xue, for all the\nsupport and encouragement she has given me and continues to give me over the\nyears. You are my best friend and the love of my life, and I look forward to growing old\nand wrinkly with you!\n",
      "content_length": 3182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "ACKNOWLEDGMENTS\nxvii\n Oh, I almost forgot, I would like to thank my cat, Ada, for bringing so much joy\ninto our lives and all the love she has given us. That scar you left on my thigh five years\nago is still visible to this day, I really . . . wait a minute. . . .\n             —Yan Cui\nI always hoped to create a lasting contribution to the developer community and am so\nexcited to see that finally happen with the second edition of Serverless Architectures on\nAWS. My biggest thanks to Peter Sbarski for making this happen and for the opportunity\nto create this work with Yan Cui and him. It has been an honor and a pleasure to be a\npart of the team with these serverless luminaries. Thank you to the crew at Manning, and\nour editor Toni Arritola, for their everlasting patience, thoroughness, and guidance.\n This book is dedicated to the serverless community. We at AWS and other provid-\ners may build the technology, but it is you, the community and the customers, that put\nit to work to the benefit of the world. I hope this book captures the passion, depth,\nand breadth that you deserve. Keep raising the bar and changing the world, one event\nat a time.\n Finally, a special shout out to Tim Wagner for getting the whole serverless universe\nstarted.\n             —Ajay Nair\n",
      "content_length": 1279,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "xviii\nabout this book\nServerless technologies and architectures are fascinating and unique. They present a\ndifferent way of building software in a cloud environment. This is because serverless is\nabout offloading the undifferentiated heavy lifting to others, reducing certain opera-\ntional concerns, moving toward event-driven computing, and giving yourself space to\nfocus on what’s important—the core goals of your business or project. This book\nteaches about the serverless approach to the design of systems. You will read how\nother companies have solved problems using a serverless approach on AWS and dive\ninto numerous discussions about architecture. \n Along the way, you will learn more about event-driven computing, useful design\npatterns, organizing and deploying your code, and security. This book isn’t a collec-\ntion of tutorials you can find online. Instead, it is an attempt to share our thinking\nand understanding of the future of cloud computing, which we think is serverless.\n This book is in four parts. The first part takes you through basic serverless princi-\nples as well as crucial architectures and patterns. You will also build a small serverless\napplication in AWS to get your hands dirty. It’ll be a fun one; your application will con-\nvert video files from one format to another without running a server.\n The second part focuses on three case studies from Yubl, A Cloud Guru, and Yle.\nYou will read how other companies have solved business and technical challenges with\na serverless approach. The third part is about architecture. Here you will learn how to\nadopt the serverless-first mindset, think about the pros and cons of different architec-\ntural implementations, and tackle unexpected challenges. The three examples we\n",
      "content_length": 1753,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "ABOUT THIS BOOK\nxix\npresent are all different, showing that a serverless approach to the design of systems is\nversatile and flexible. \n The fourth and final part of the book looks at the internals of AWS Lambda and\nemerging AWS practices. If you are already an expert on AWS and serverless, you may\nfind this section to be particularly fascinating. \n The second edition of Serverless Architectures on AWS is for serverless veterans and\nbeginners alike. No matter your experience, we think you will find something valuable\nin these pages. We hope that this book will inspire you to think serverless first. Now,\nlet’s read and build!\nAbout the code\nThis book provides many examples of code. These appear throughout the text and as\nseparate code listings. To accommodate long lines of code, listings include line-\ncontinuation markers (➥). Code appears in a fixed-width font just like this, so\nyou’ll know when you see it.\n This book is about architecture and, as such, it is not heavy on source code. Chap-\nter 2 is the only practical chapter. The source for chapter is available on GitHub at\nhttp://github.com/sbarski/serverless-architectures-aws-2. If you’d like to contribute,\nopen a pull request and we’d be happy to consider your changes. If you see a prob-\nlem, please file an issue. \nliveBook discussion forum\nPurchase of Serverless Architectures on AWS, Second Edition includes free access to liveBook,\nManning’s online reading platform. Using liveBook’s exclusive discussion features, you\ncan attach comments to the book globally or to specific sections or paragraphs. It’s a\nsnap to make notes for yourself, ask and answer technical questions, and receive\nhelp from the authors and other users. To access the forum, go to    https://livebook\n.manning.com/#!/book/serverless-architectures-on-aws-second-edition/discussion. You\ncan also learn more about Manning’s forums and the rules of conduct at https://livebook\n.manning.com/#!/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the authors can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe authors, whose contribution to the forum remains voluntary (and unpaid). We\nsuggest you try asking the authors some challenging questions lest their interest stray!\nThe forum and the archives of previous discussions will be accessible from the pub-\nlisher’s website as long as the book is in print.\n",
      "content_length": 2497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "xx\nabout the authors\nPETER SBARSKI is VP of Education & Research at A Cloud Guru, AWS Serverless Hero, and\nthe organizer of Serverlessconf, the world’s first conference dedicated entirely to server-\nless architectures and technologies. His work at A Cloud Guru allows him to research and\nwrite about serverless architectures, cloud computing and AWS. Peter is always happy to\ntalk about serverless technologies at conferences and meetups year round. His other pas-\nsions include technical education, and innovation in technology and cloud computing.\nPeter holds a Ph.D. in Computer Science from Monash University, Australia. He can be\nfound on Twitter (@sbarski) and LinkedIn (linkedin.com/in/petersbarski).\nYAN CUI is a developer advocate at Lumigo and an independent consultant who helps cli-\nents around the world go faster for less by successfully adopting serverless technologies.\nHe has over a decade of experience running production workloads at scale on AWS and\nhas worked as architect and principal engineer within a variety of industries including\nbanking, e-commerce, sports streaming, and mobile gaming. Yan is an AWS Serverless\nHero and a regular speaker at conferences internationally. He is the author of Production-\nReady Serverless (Manning, 2018) and co-author of F# Deep Dives (Manning, 2014), and he\nhas also self-published several popular courses such as the AppSync Masterclass. He can\nbe found on Twitter (@theburningmonk) and LinkedIn (linkedin.com/in/theburning-\nmonk) and writes regularly on his blog (theburningmonk.com).\nAJAY NAIR is a Director of Product and Engineering with Amazon Web Services. He is the\nfounding product leader for AWS Lambda and helped build the AWS serverless portfolio\nover the last several years. Ajay has spent his career focusing on cloud native platforms,\n",
      "content_length": 1812,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "ABOUT THE AUTHORS\nxxi\ndeveloper productivity, and big data systems. He loves spending his days helping\ndevelopers do more with less and delighting customers with the power of technology.\nAjay holds a Masters in Information Systems Management from Carnegie Mellon, USA,\nwith a Bachelors in Electrical and Electronics Engineering from Kerala University, India.\nYou can find Ajay sharing thoughts on everything from serverless to product\nmanagement on Twitter (@ajaynairthinks) or on LinkedIn (linkedin.com/in/ajnair).\n",
      "content_length": 516,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "xxii\nabout the cover illustration\nThe figure on the cover of Serverless Architectures on AWS, Second Edition is “Man from\nStupno/Sisak, Croatia,” from a book by Nikola Arsenović, published in 2003. The\nbook includes finely colored illustrations of figures from different regions of Croatia,\naccompanied by descriptions of the costumes and of everyday life.\n In those days, it was easy to identify where people lived and what their trade or sta-\ntion in life was just by their dress. Manning celebrates the inventiveness and initiative\nof today’s computer business with book covers based on the rich diversity of regional\nculture centuries ago, brought back to life by pictures from collections such as this\none.\n",
      "content_length": 712,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "Part 1\nFirst steps\nIf you are new to serverless architectures, you’ve come to the right place. The\nfirst three chapters of this book will give you an introduction to this exciting\ntechnology and even get you to build a small serverless application of your own.\nThe first chapter provides an overview of serverless technologies and a discus-\nsion about where we are today. The second chapter is more practical; it focuses\non giving you a hands-on experience with AWS and services such as AWS\nLambda. The third chapter describes popular and useful serverless patterns.\nLet’s get started!\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "3\nGoing serverless\nIf you ask software developers what software architecture is you might get answers\nranging from “it’s a blueprint or a plan” to “a conceptual model” to “the big pic-\nture.” This book is about an emerging architectural approach that has been\nadopted by developers and companies around the world to build their modern\napplications—serverless architectures. \n Serverless architectures have been described as somewhat of a “nirvana” for an\napplication architectural approach. It promises developers the ability to iterate as\nfast as possible while maintaining business critical latency, availability, security, and\nperformance guarantees, with minimal effort on the developers’ part.\n This book teaches you how to think about serverless systems that can scale and\nhandle demanding computational requirements without having to provision or\nThis chapter covers\nTraditional system and application architectures\nKey characteristics and benefits of serverless \narchitectures\nHow serverless architectures and microservices \nfit into the picture\nConsiderations when transitioning from server to \nserverless\nWhat’s new in this second edition?\n",
      "content_length": 1155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "4\nCHAPTER 1\nGoing serverless\nmanage a single server. Importantly, this book describes techniques that can help\ndevelopers quickly deliver products to market while maintaining a high level of quality\nand performance by using services and architectures offered by today’s cloud platforms.\n1.1\nWhat’s in a name?\nBefore going in any further, we think it’s important to come to terms with the word\nserverless. There are various attempts at this already, including an official one from\nAWS (https://aws.amazon.com/serverless/) and a community favorite from Martin\nFowler (https://martinfowler.com/articles/serverless.html). Here’s how we define it:\nDEFINITION\nServerless is a qualifier that can be applied to any software or ser-\nvice offering, which requires that it is consumed as a utility service and incurs\ncost only when used.\nSimple enough, right? But there’s a lot to unpack in that simple definition. Let’s dive\ninto each of the following two required criteria to call something serverless:\nConsumed as a utility service—The “software as a service” consumption model is\nwell understood. It means that anyone using the software uses a prescribed\napplication programming interface (API) or web interface to use the software\nand customize it, while staying within any published constraints for the software\nand usage policies for the API. Salesforce, Office365, and Google Maps are well-\nknown software packages delivered as a service. What’s key here is that the\nactual infrastructure (servers, networking, storage, etc.) hosting the software\nand powering the API is completely abstracted from you as the consumer; all\nthat is visible (and all that matter) is what the API permits. \nA service also typically comes with accompanying availability, reliability, and\nperformance guarantees from the service provider. A utility service, further, has\nthe billing characteristics that we’d expect from any utility computing offering;\nthat is, you pay for usage not for reservation, subscriptions, or provisioning. All\nexisting public cloud offerings have some form of utility billing associated with\nthem. For example, Amazon Elastic Compute Cloud (EC2) allows you to pay by\nthe second for the rent of virtual machines.\nIncurs cost only when used—This means there’s zero cost for having the software\ndeployed and ready to use. Think of this as the same cost model we expect from\nour public utilities like electricity and water. You, as the consumer, pay a per\ngranular usage unit cost if you use any, but you pay zero if you use nothing. This\naspect of pure usage-based pricing is a distinguishing criterion of serverless\nofferings from the other utility services that came before it.\nIn the rest of the book, we will use the “serverless” qualifier only for software that fits\nthese criteria. For example, software that requires you to provide a server to host a\nwebsite (like the Apache web server) would not qualify because it does not meet the\nfirst criterion. Software that is available as a service but requires you to pay by subscrip-\ntion (like Salesforce) would not qualify as well because it does not meet the second\n",
      "content_length": 3122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "5\nUnderstanding serverless architectures\ncriterion. A serverless architecture, by extension, is one composed entirely of serverless\ncomponents. But which components of an architecture need to be serverless for it to\nbe called as such? Let’s look at this next with an example.\n1.2\nUnderstanding serverless architectures\nLet’s take the example of a typical data-driven web application, not unlike the systems\npowering most of today’s web-enabled software. These typically consist of a backend\n(server) that accepts requests from a client and then processes the requests. \n The backend server performs various forms of computation, and the frontend cli-\nent provides an interface for users to operate via their browser, mobile, or desktop\ndevice. Data might travel through numerous application layers before being saved to a\ndatabase. The backend then generates a response that could be in the form of JSON\nor in fully rendered markup, which is sent back to the client (figure 1.1). These kinds\nof applications are conventionally architected as tiers (a presentation tier that controls\nhow the information is captured and provided to the user, an application tier that\ncontrols the business logic of the application, and a data tier with the database and\ncorresponding access controls).\nJust to clear up any misperceptions . . .\nOne of the common misunderstandings is that the “-less” in “serverless” implies\n“absence of or without” (think “sugarless,” “boneless,” and so on), which leads to\nsome colorful debates on social media on how any application architecture can claim\nto run without servers. We think “-less” here means “invisible in context of usage”\n(think “wireless,” “tasteless”). There obviously are servers somewhere! The differ-\nence is that these servers are hidden from you. There’s no infrastructure for you to\nthink about and no way to tweak the underlying operating system or virtual hardware\nconfiguration. Someone else takes care of the nitty-gritty details of infrastructure\nmanagement, freeing you from that operational overhead and giving back to you the\nmost expensive commodity there is—time.\n1. User performs an action \nthat requires data from a \ndatabase to be displayed.\n2. A request is formed \nand sent from the client \nto the web server.\n3. The request is \nprocessed and the \ndatabase is queried.\n4. Data is retrieved.\n5. An appropriate response \nis generated and sent back.\n6. Information is displayed \nto the user.\nApplication user\nWeb client\n(presentation tier)\nWeb server\n(application tier)\nDatabase\n(data tier)\nFigure 1.1\nA basic request-response (client-server) message-exchange pattern that most developers \nare familiar with. There’s only one web server and one database in this figure. Most systems are much \nmore complex.\n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "6\nCHAPTER 1\nGoing serverless\nSoftware architectures have evolved from the days of code running on a mainframe to\na multitier architecture where the presentation, data, and application/logic tiers are\ntraditionally separated. Within each tier, there may be multiple logical layers that deal\nwith the particular aspects of functionality or domain. There are also cross-cutting\ncomponents such as logging or exception handling systems that can span numerous\nlayers. The preference for layering is understandable. Layering allows developers to\ndecouple concerns and have more maintainable applications. Figure 1.2 shows an\nexample of a tiered architecture with multiple layers including the API, the business\nlogic, the user authentication component, and the database. \nApplication user\nUser interface components\nLayering helps to \nsegregate concerns, but \nmore layers can also \nmake changes harder \nand slower to implement.\nCross-cutting concerns \nspan numerous layers. \nA good example of this is \nlogging, which can happen \nat every layer.\nApplication tier\nCross-cutting\nconcerns\nPresentation\ntier\nData tier\nPresentation logic\nClient-side model\nClient-side service layer\nApplication programming interface\nServer-side service layer\nBusiness/domain layer\nBusiness entities/model\nData access/persistence layer\nException management\nCaching\nLogging\nCommunications\nSecurity\nDatabase\nFile storage\nFigure 1.2\nA typical three-tier application is usually made up of presentation, application, and data \ntiers. A tier can have multiple layers with specific responsibilities.\nTiers vs. layers\nThere is some confusion among developers about the difference between layers and\ntiers. A tier is a module boundary that provides isolation between major components\nof a system. For example, a presentation tier that’s visible to the user is separate\nfrom the application tier, which encompasses the business logic. In turn, a data tier\nis another separate system that manages, persists, and provides access to data.\nComponents grouped in a tier can physically reside on different infrastructures.\nLayers are logical slices that carry out specific responsibilities in an application. Each\ntier can have multiple layers that are responsible for different elements of function-\nality, such as domain services.\n",
      "content_length": 2286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "7\nUnderstanding serverless architectures\n1.2.1\nService-oriented architecture and microservices\nOne blunt approach would be to combine all the layers (the API, the business logic,\nthe user authentication) into one single, monolithic code base. This may sound like\nan antipattern today, but that was indeed the approach we adopted in the early days of\ncloud-based development. Most modern approaches, however, dictate that you archi-\ntect with reusability, autonomy, composability, and discoverability in mind. \n Among the veterans of our industry, service-oriented architecture (SOA) is a well-\nknown buzzword. SOA encourages an architectural approach in which developers cre-\nate autonomous services that communicate via message passing and often have a\nschema or a contract that defines how messages are created or exchanged. \n The modern incarnation of the service-oriented approach is often referred to as\nmicroservices architecture. Modern application architectures are composed of ser-\nvices communicating through events and APIs with business logic inserted as appro-\npriate. We define microservices as small, standalone, fully independent services built\naround a particular business purpose or capability. Ideally, microservices should be\neasy to replace, with each service written in an appropriate framework and language. \n The mere fact that microservices can be written in a different general-purpose lan-\nguage or a domain-specific language (DSL) is a drawing card for many developers.\nBenefits can be gained from using the right language or a specialized set of libraries\nfor the job. Each microservice can maintain state and store data. And if microservices\nare correctly decoupled, development teams can work and deploy microservices inde-\npendently from one another. This approach of building and deploying applications as\na collection of loosely coupled services is considered the default approach to develop-\nment in the cloud today (the “cloud native” approach, if you will).\n1.2.2\nImplementing architecture the conventional way\nOnce you have decided how your application is going to be architected, and all the\nsoftware required for each of the layers is ready to go, you would think the hardest\npart is done. The truth is, that’s when some of the more complex tasks begin. Devel-\noping your desired services traditionally requires servers running in data centers or in\nMicroservices all the time?\nMicroservice approaches aren’t all a bed of roses. Having a mix of languages and\nframeworks can be hard to support and, without strict discipline, can lead to confu-\nsion down the road. Eventual consistency, coordination, discovery, and complex error\nrecovery can make things difficult in a microservices universe. \nSoftware engineering is always a game of tradeoffs. Because something is in fashion\n(like microservices) doesn’t make it universally right for all problems and use cases.\nWhat matters is knowing about the different architectural options, understanding\ntheir pros and cons, and, importantly, understanding the requirements and needs of\nyour own problem. (And, yes, in some cases and situations, having a monolith is OK.)\n",
      "content_length": 3154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "8\nCHAPTER 1\nGoing serverless\nthe cloud that need to be managed, maintained, patched, and backed up. Today, you\nwould pick from a few options:\nDirectly build on VMs—The physical deployment of each service requires you to\nhave a set of instances with additional tasks to address required activities such as\nload balancing, transactions, clustering, caching, messaging, and data redun-\ndancy. Provisioning, managing, and patching of these servers is a time-consuming\ntask that often requires dedicated operations people. \nA non-trivial environment is hard to set up and operate effectively. Infra-\nstructure and hardware are necessary components of any IT system, but they’re\noften also a distraction from what should be the core focus—solving the busi-\nness problem. In our simple web application example, you would have to\nbecome an expert in building distributed systems and cloud infrastructure\nmanagement. In a cloud environment, this form of computing is often referred\nto as infrastructure as a service (IaaS).\nUse a PaaS—Over the past few years, technologies such as platform as a service\n(PaaS) and containers have appeared as potential solutions to the headache of\ninconsistent infrastructure environments, conflicts, and server management\noverhead. PaaS is a form of cloud computing that provides a platform for users\nto run their software while hiding some of the underlying infrastructure. \nTo make effective use of PaaS, developers need to write software that targets\nthe features and capabilities of the platform. Moving a legacy application\ndesigned to run on a standalone server to a PaaS service often leads to addi-\ntional development effort because of the ephemeral nature of most PaaS imple-\nmentations. Still, given a choice, many developers would understandably\nchoose to use PaaS rather than more traditional, manual solutions thanks to\nreduced maintenance and platform support requirements.\nUse containers—Containerization is considered ideal for microservices architec-\ntures because it is a way of isolating an application with its own environment.\nIt’s a lightweight alternative to full-blown virtualization that traditional cloud\nservers use. \nContainers are an excellent deployment and packaging solution especially\nwhen dependencies are in play (although they can come with their own house-\nkeeping challenges and complexities). Containers are isolated and lightweight,\nbut they need to be deployed to a server, whether in a public or private cloud or\non site. \nWhile each of these models are perfectly valid and offer varying degrees of simplicity\nand speed of development for your service, your costs are still driven by the lifecycle of\nthe infrastructure or servers you own, not to your application usage. If you purchase a\nrack at the data center, you pay for it 24/7. If you purchase a cloud instance (wrapped\nin a PaaS or running containers or otherwise), you pay for it when it runs, indepen-\ndent of whether it is serving traffic for your web app or not. \n",
      "content_length": 2996,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "9\nUnderstanding serverless architectures\n This leads to an entire discipline of engineers investing in improving server effi-\nciency or trying to match infrastructure lifecycle to application usage and server sizes\nto traffic patterns. This also means that all the effort spent on these tasks is time taken\naway from improving the functionality and differentiating aspects of your application.\nThis is equivalent to asking for a place to plug in your appliance and having to pay for\na share of the power generators at your utility company, as well as configuring the gen-\nerator to deliver the power in the phase, frequency, and wattage you desire no matter\nhow much you use. The actual outcome (plug in your appliance) is dwarfed by the\neffort and cost for the infrastructure required (the generators). This is where the\nserverless approach comes in. It aims for the moral equivalent of the utility approach\nwe know and love today—there when you need it, complexity abstracted away, and you\nonly pay for when you use it. \n1.2.3\nImplementing architecture the serverless way\nA serverless architecture for our sample application could be composed of different\nlayers. For example, to build the API, we would use a service that does not cost us any-\nthing if there are no API calls. To build the authentication service, we would use a ser-\nvice that does not cost us anything if there are no authentication calls. To build the\nstorage service, we would use . . . you get the picture. \n Much like the public cloud approach that offered virtual infrastructure Lego to\nassemble our cloud stack in the early days, a serverless architecture uses existing ser-\nvices from cloud providers like AWS to implement its architectural components. As an\nexample, AWS offers services to build our application primitives like APIs (Amazon\nAPI Gateway), workflows (AWS Step Functions), queues (Amazon Simple Queue Ser-\nvice), databases (Amazon DynamoDB and Amazon Aurora), and more.\n The idea of using off-the-shelf services to implement parts of our architecture is\nnot new; indeed, it’s been a best practice since the days of SOA. What’s changed in the\nlast few years is the capability to also implement the custom aspects of our applications\n(like the business logic) in a serverless manner. This ability to run arbitrary code with-\nout having to provision infrastructure to run it as a service or to pay for the infrastruc-\nture is referred to as functions as a service (FaaS). \n FaaS allows you to provide custom code, associated dependencies, and some con-\nfiguration to dictate your desired performance and access control characteristics.\nFaaS then executes this unit (referred to as a function) on an invisible compute fleet\nwith each execution of your code receiving an isolated environment with its own disk,\nmemory, and CPU allocation. You pay only for the time your code runs. A function is\nnot a lightweight instance; instead, think of it as akin to processes in an OS, where you\ncan spawn as many as needed by your application and then spin them down when\nyour application isn’t running. \n Serverless architectures are really the culmination of shifts that have been going\non for a long time: from monoliths to services and from managing infrastructure to\nincreasingly delegating the undifferentiating responsibilities. Serverless architectures\n",
      "content_length": 3339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "10\nCHAPTER 1\nGoing serverless\ncan help with the problem of layering and having to update too many things. There’s\nroom for developers to remove or minimize layering by breaking the system into func-\ntions and allowing the frontend to securely communicate with services and even the\ndatabase directly. A well-planned serverless architecture can make future changes eas-\nier, which is an important factor for any long-term application. \n To recap, a serverless architecture leverages a serverless implementation for each\nof its components, using FaaS (like AWS Lambda) for custom logic. This means each\ncomponent is built as a service, with utility pricing that incurs cost only when used.\nEach component is a service and exposes no configuration or cost related to the infra-\nstructure it is running on, which means these architectures don’t rely on direct access\nto a server to work. By making use of various powerful single-purpose APIs and web\nservices, developers can build loosely coupled, scalable, and efficient architectures\nquickly. Moving away from servers and infrastructure concerns, as well as allowing the\ndeveloper to primarily focus on code, is the ultimate goal behind serverless.\nMore on FaaS\nAWS’s FaaS offering is called AWS Lambda and is one of the first from the major\ncloud providers. Note that Lambda isn’t the only game in town. Microsoft Azure Func-\ntions (http://bit.ly/2DWx5Gn), IBM Cloud Functions (http://bit.ly/2l1PWbd), and\nGoogle Cloud Functions (http://bit.ly/2CbzOem) are other FaaS services you might\nwant to look at.\nMany developers conflate serverless with FaaS offerings like AWS Lambda, which\noften leads to confusing arguments around the adoption of containers or serverless\nwhen they really mean containers or functions. We like how TJ Hallowaychuk, the cre-\nator of the Apex framework, defines what serverless is about. He once tweeted,\n“serverless != functions, FaaS == functions, serverless == on-demand scaling and\npricing characteristics (not limited to functions).” We couldn’t agree more.\nAn emerging trend is that of serverless containers; that is, leveraging containers\ninstead of functions to implement the custom logic and using the container as a util-\nity service and incurring costs only when the container runs. Services like AWS Far-\ngate or Google Cloud Run offer this capability. The difference between the two\n(functions vs. containers) is just the degree to which developers want to shift the\nboundaries of shared responsibilities. Containers give you a bit more control over\nuser space libraries and network capabilities. Containers are an evolution of the\nexisting server-based/VM model, offering an easy packaging and deployment model\nfor your application stack. You are still required to define your operating system’s\nrequirements, your desired language stack, and dependencies to deploy code, which\nmeans you continue to carry some of the infrastructure complexity. For the purpose\nof this book, we are going to focus on using FaaS for our custom logic, though you\ncan explore the usage of serverless containers for the same as well.\n",
      "content_length": 3101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "11\nMaking the call to go serverless\n1.3\nMaking the call to go serverless\nThe web application example we went through is one of the simplest demonstrations\nof what can be achieved with serverless architectures. A serverless approach can also\nwork exceptionally well for organizations that want to innovate and move quickly. \n Functions and serverless architectures, in general, are versatile. You can use them\nto build backends for CRUD applications, e-commerce, back-office systems, complex\nweb apps, and all kinds of mobile and desktop software. Tasks that used to take weeks\ncan be done in days or hours as long as we chose the right combination of technolo-\ngies. Lambda functions are stateless and scalable, which makes them perfect for\nimplementing any logic that benefits from parallel processing. \n The most flexible and powerful serverless designs are event-driven, which means\neach component in the architecture reacts to a state change or notification of some\nkind rather than responding to a request or polling for information. In chapter 2, for\nexample, you’ll build an event-driven, push-based pipeline to see how quickly you can\nput together a system to encode video to different bit rates and formats. \nNOTE\nYou will find the use of events as a communication mechanism\nbetween components to be a recurring theme in serverless architectures;\nindeed, AWS Lambda’s initial launch was as an event-driven computing ser-\nvice. Building event-driven, push-based systems will often reduce cost and\ncomplexity (you won’t need to run extra code to poll for changes) and,\npotentially, make the overall user experience smoother. It goes without saying\nthat although event-driven, push-based models are a good goal, they might\nnot be appropriate or achievable in all circumstances. \nServerless architecture allows developers to focus on software design and code rather\nthan infrastructure. Scalability and high availability are easier to achieve, and the pric-\ning is often more fair because you pay only for what you use. More importantly, you\nhave the potential to reduce some of the complexity of the system by minimizing the\nnumber of layers and amount of code needed. \n Adopting a serverless approach to application development comes with significant\nagility, elasticity, and cost efficiency gains. However, it is easy to fall into the trap of try-\ning to adopt a serverless approach for all applications. We recommend keeping a few\nprinciples in mind as you start your serverless journey:\nAvoid lift-and-shift—In practice, serverless architectures are more suited for new\napplications rather than porting existing applications over. This is because exist-\ning application code bases have a lot of code that is made redundant by the\nserverless services. For example, porting a Java Spring app into Lambda brings\na heavy framework into a function, most of which exists to interact with a web\nserver (which doesn’t exist inside Lambda). \nAdopt a serverless first approach, not a serverless only approach—While there are\ncompanies like A Cloud Guru that have adopted a serverless only approach, where\n100% of their application runs as a serverless implementation, the more\n",
      "content_length": 3179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "12\nCHAPTER 1\nGoing serverless\nwidespread approach that companies like Expedia and T-Mobile have adopted is\nto go serverless first. What this means is that their developers attempt to first build\nany new application in the following priority order: build as much as possible using\nthird-party services, fall back to custom services built using AWS serverless primi-\ntives like AWS Lambda, and finally, fall back to custom services built using custom\nsoftware running on infrastructure like EC2. We talk about the reasons why you\nmay have to fall back beyond custom serverless services in the next section.\nIt doesn’t have to be all or nothing—One advantage of the serverless approach is\nthat existing applications can be gradually converted to serverless architecture.\nIf a developer is faced with a monolithic code base, they can gradually tease it\napart and convert individual components into a serverless implementation (the\nstrangler pattern). \nThe best approach is to initially create a prototype to test developer assump-\ntions about how the system would function if it is going to be partly or fully server-\nless. Legacy systems tend to have interesting constraints that require creative\nsolutions, and as with any architectural refactors at a large scale, compromises\nare inevitably going to be made. The system may end up being a hybrid (as in fig-\nure 1.3), but it may be better to have some of its components use Lambda and\nthird-party services rather than remain with an unchanged legacy architecture\nthat no longer scales or that requires expensive infrastructure to run.\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nIaaS\nPaaS\nContainers\nMonolithic application\nLambda\nfunction\nAnalytics\nservice\nPayment\nservice\nNotification\nservice\nSearch\nservice\nA monolithic application \ncan be deconstructed  \ninto Lambda functions, \nthird-party services, IaaS, \nPaaS, and containers.\nThe combination of \ntechnologies should depend \non your needs and constraints. \nHowever, more technologies \nrequire more overhead, time, \nand energy. \nContainers, PaaS, IaaS, Lambda functions, and services \ncan talk to one another. If you have designed a system \nusing a combination of these technologies, you must \nconsider how the orchestration of events take place.\nFigure 1.3\nServerless architecture is not an all-or-nothing proposition. If you currently have \na monolithic application, you can begin to gradually extract components and run them in \nisolated services or compute functions. You can decouple a monolithic application into an \nassortment of IaaS, PaaS, containers, functions, and third-party services if it helps.\n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "13\nMaking the call to go serverless\nThe transition from a legacy, server-based application to a scalable serverless architec-\nture may take time to get right. It needs to be approached carefully and slowly, and\ndevelopers need to have a good test plan and a great DevOps strategy in place before\nthey begin.\nPick applications suited for a service-oriented architecture—Serverless architectures\nare a natural extension of ideas raised in SOAs. In a serverless architecture, all\ncustom code is written and executed as isolated, independent, and often granu-\nlar functions that are run in a compute service such as AWS Lambda. Because\nevery component is a service, serverless architectures share a lot of advantages\nand complexities with event-driven microservices architectures. This also means\napplications likely need to be architected to meet the requirements of these\napproaches (like making the individual services stateless, for example). \nKeep in mind that the serverless approach is all about reducing the amount\nof code you have to own and maintain, so you can iterate and innovate faster.\nThis means you should strive to minimize the number of components that are\nrequired to build your application. For example, you may architect your web\napplication with a rich front end (in lieu of a complex backend) that can talk to\nthird-party services directly. That kind of architecture can be conducive to a bet-\nter user experience. Fewer hops between online resources and reduced latency\nwill result in a better perception of performance and usability of the applica-\ntion. In other words, you don’t have to route everything through a FaaS; your\nfrontend may be able to communicate directly with a search provider, a data-\nbase, or another useful API. \nAlso keep in mind that moving from a monolithic approach to a more\ndecentralized serverless approach doesn’t automatically reduce the complexity\nWhat about NoOps?\nEarly on, around the time of the first conference on serverless technologies and archi-\ntectures (https://serverlessconf.io) in 2016, there was talk that serverless technol-\nogies foreshadowed the era of NoOps. Some people believed that thanks to\nserverless, companies would no longer need to think about infrastructure operations.\nThe cloud vendor will take care of everything was the thought. That assumption, that\nNoOps was a real thing, proved not to be the case.\nWhen it comes to building and running serverless applications, DevOps engineers are\nessential, except now they have a different focus. Their attention is on deployment\nautomation, testing, and working with the operations/support teams of their pre-\nferred cloud provider (rather than tweaking servers and patching operating systems). \nCompanies can get away with smaller, more specialized DevOps teams; however,\nignoring operations entirely is a recipe for disaster (and don’t let anyone else tell you\notherwise). Remember, when your application fails, customers hold you accountable,\nnot your cloud provider, so be ready and have the right people and processes in place.\n",
      "content_length": 3060,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "14\nCHAPTER 1\nGoing serverless\nof the underlying system. The distributed nature of the solution can introduce\nits own challenges because of the need to make remote rather than in-process\ncalls and the need to handle failures and latency across a network, which your\napplication will need to be resilient to.\nMinimize custom code—The rise of serverless means many standard application\ncomponents like APIs, workflows, queues, and databases are available as server-\nless offerings from cloud providers and third parties. It’s far more useful for\ndevelopers to spend time solving a problem unique to their domain rather than\nrecreating functionality already implemented by someone else. Don’t build for\nthe sake of building if viable third-party services and APIs are available. Stand\non the shoulders of giants to reach new heights. \nAppendix A has a short list of Amazon Web Services and non-Amazon Web\nServices that we’ve found useful. We’ll look at most of those services in more\ndetail as we move through the book. However, it goes without saying that when\na third-party service is considered, factors such as price, capability, availability,\ndocumentation, and support must be carefully assessed. \nIf you have to build a piece of custom functionality, our advice is simple: try\nto solve your problem using functions first, and if that doesn’t work explore\ncontainers and more traditional server-based architectures second. Developers\ncan write functions to carry out almost any common task, such as reading and\nwriting to a data source, calling out to other functions, and performing calcula-\ntions. In more complex cases, developers can set up more elaborate pipelines\nand orchestrate invocations of multiple functions. \n1.4\nServerless pros and cons\nThe serverless approach of building applications by quickly assembling services pro-\nvides two significant advantages: less code to write and maintain per application and\nper activity pricing for our applications. This translates into a disruptive gain in agility\nand developer productivity, and a much more streamlined alignment between devel-\nopment and finance (because any application inefficiencies or optimizations show a\ndirect, tangible financial impact). Here are a few of the specific benefits you will real-\nize by adopting serverless architecture:\nHigh scale and reliability without server management—Building large scale, distrib-\nuted systems is hard. Tasks such as server configuration and management,\npatching, and maintenance are taken care of by the vendor, as is managing the\ninfrastructure architecture for high scale and reliability, which saves time and\nmoney. For example, Amazon looks after the health of its fleet of servers that\npower AWS Lambda. \nIf you don’t have specific requirements to manage or modify server\nresources, then having Amazon or another vendor look after them is a great\nsolution. You’re responsible only for your own code, leaving operational and\nadministrative tasks to a different set of capable hands. \n",
      "content_length": 3007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "15\nServerless pros and cons\nCompetitive pricing—Traditional server-based architecture requires servers that\ndon’t necessarily run at full capacity all of the time. Scaling, even with auto-\nmated systems, involves a new server, which is often wasted until there’s a tem-\nporary upsurge in traffic or new data. \nServerless systems are much more granular with regard to scaling and are cost-\neffective, especially when peak loads are uneven or unexpected. Because of their\nutility pay-per-use billing, serverless services can be extremely cost-effective; how-\never, they’re not cheaper than traditional (server and container) technologies in\nall circumstances. The best thing is to do some modeling before embarking on\na big project.\nLess code—We mentioned at the start of the chapter that serverless architecture\nprovides an opportunity to reduce some of the complexity and code, in com-\nparison to more traditional systems. Adopting a serverless approach eliminates\nundifferentiated code such as that required for orchestrating server fleets or\nrouting requests and events between components, which forms a surprisingly\nlarge part of modern code bases. \nServerless is not a silver bullet in all circumstances, however. Here are some reasons\nwhere you would want to avoid serverless architectures:\nYou are not comfortable with public cloud-based architectures. Serverless development\nis a natural extension of the move to cloud-based development, where more\nand more of the undifferentiated heavy lifting is moved to the providers. There\nare applications and business scenarios where you need to maintain your own\ndata center; in such cases, you cannot build a serverless architecture (though\nyou are welcome to host your own primitives on your infrastructure and use\nthose to build applications).\nThe services don’t meet the availability, performance, compliance, or scale needs of your\ncustomers. AWS serverless services offer an availability SLA, but their threshold\nmay be below what you need for your business. They also have a variety of com-\npliance certifications, but you must validate if they need what your business\nneeds. Services like AWS Lambda also do not offer a performance SLA, which\nmeans you may need to evaluate their performance against your desired levels.\nNon-AWS, third-party services are in the same boat. Some may have strong\nSLAs, whereas others may not have one at all. \nYour application and business needs more control or you need to customize the infrastruc-\nture. When it comes to Lambda, the efficiencies gained from having Amazon\nlook after the platform and scale functions come at the expense of being able to\ncustomize the operating system or tweak the underlying instance. You can mod-\nify the amount of RAM allocated to a function and change timeouts, but that’s\nabout it. Similarly, different third-party services will have varying levels of cus-\ntomization and flexibility. \n",
      "content_length": 2918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "16\nCHAPTER 1\nGoing serverless\nYour application and business needs require you to stay vendor agnostic. If a developer\ndecides to use third-party APIs and services, including AWS, there’s a chance\nthat architecture could become strongly coupled to the platform being used.\nThe implications of vendor lock-in and the risk of using third-party services—\nincluding company viability, data sovereignty and privacy, cost, support, docu-\nmentation, and available feature set—need to be thoroughly considered.\nIn this chapter, you learned what serverless architecture is, looked at its principles,\nand how it compares to traditional architectures. In the next chapter, we’ll get our\nhands dirty by creating a small serverless, event-driven application. This will help you\nget a good taste for serverless if this is your first time trying this approach. From there,\nwe’ll explore important architectures and patterns and discuss use cases where server-\nless architectures are used to solve a problem.\n1.5\nWhat’s new in this second edition?\nFor all intent and purposes, this is a completely different book from the first edition of\nServerless Architectures on AWS. Most of the chapters have been written from the ground\nup to provide a completely different experience from the first edition.\n When the first edition of this book came out in 2017, serverless was still new and\nmany of us were learning about serverless for the first time. As such, the first edition\ngave a gentle introduction to serverless and walked the reader through a build of a\nserverless application. Since then a lot of new educational content has crossed our\ndesks, including numerous books and video courses to help us get started with server-\nless technologies on AWS.\n If you’re looking for an introduction to serverless architectures on AWS, we have\nincluded some introductory content in chapter 2 and in appendices A and B. You can\nalso find the first edition of this book on the Manning website (https://www.manning\n.com/books/serverless-architectures-on-aws). Most of the content from the first edi-\ntion is still relevant today, and with that book, you will learn to build a serverless appli-\ncation from scratch.\n But, just as serverless technologies allow us to focus on the things that differentiate\nour business, we want to focus on things that can differentiate this book with this sec-\nond edition. Instead of yet another getting started guide to serverless, this book\nfocuses on serverless use cases and interesting architectures. It is aimed at developers\nwith some experience of serverless technologies already and answers the questions\nthat many of you have been asking us. Given the switch in focus, this book does not\nhave many actual code samples. Instead, we hope to challenge the way you think\nabout serverless architecture and help you get the most out of serverless technologies\non AWS.\n",
      "content_length": 2877,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "17\nSummary\nSummary\nThe cloud has been and continues to be a game changer for IT infrastructure\nand software development. \nSoftware developers need to think about the ways they can maximize the use of\ncloud platforms to gain a competitive advantage. \nServerless architectures are the latest step forward for developers and organiza-\ntions to think about, study, and adopt. This exciting shift in implementing\napplication architectures will grow quickly as software developers embrace com-\npute services such as AWS Lambda. \nIn many cases, serverless applications will be cheaper to run and faster to imple-\nment. There’s also a need to reduce complexity and costs associated with run-\nning infrastructure and carrying out development of traditional software\nsystems. \nThe reduction in cost and time spent on infrastructure maintenance and the\nbenefits of scalability are good reasons for organizations and developers to con-\nsider serverless architectures.\n",
      "content_length": 961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "18\nFirst steps to serverless\nTo give you an understanding of serverless architectures, you’re going to build a\nsmall, event-driven serverless application: specifically, a video-encoding pipeline.\nYour service will transcode videos, uploaded to an S3 bucket, from their existing\nformat, resolution or bit rate to a different format or bit rate (kind of like YouTube\nonly without the frontend website).\n To build this video-encoding pipeline, you will use AWS Lambda, S3, and Ele-\nmental MediaConvert. Later, if you so desire, you can build a frontend around it,\nbut we’ll leave that for you as an exercise. If you want to see how we’ve done it our-\nselves, you can refer to our first edition that covers the frontend in some detail. \nThis chapter covers\nWriting and deploying AWS Lambda functions\nAWS services such as Simple Storage Service \n(S3) and Elemental MediaConvert\nUsing the Serverless Framework to organize and \ndeploy services\n",
      "content_length": 940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "19\nBuilding a video-encoding pipeline\n2.1\nBuilding a video-encoding pipeline\nIn this section, you’ll begin to build a small, event-driven serverless application. At a\nhigh level, you’ll learn the following in this chapter:\nHow to construct a rudimentary serverless architecture using three AWS ser-\nvices including Lambda\nHow to use the Serverless Framework to organize and deploy a serverless\napplication\nHow to run, debug, and test the serverless pipeline that you built\nLet’s talk about the event-driven pipeline you are going to build (we’ll call it The 24-Hour\nVideo). Your pipeline will encode videos that were uploaded to a designated S3 bucket\ninto different formats, resolutions, and bit rates. Because the entire process is event-\ndriven, once a file is uploaded to S3, the system triggers automatically to process the file\nand create a new version with a different encoding in a separate bucket. And because\neverything is done automatically, there’s no need for any intervention on your behalf.\n2.1.1\nA quick note on AWS costs\nMost AWS services have a free tier. Following this example, you should stay within the\nfree tier of most AWS services. AWS Elemental MediaConvert, however, is one service\nthat may end up costing you a little bit of money. You’ll use MediaConvert to\ntranscode video files. This service is pay-as-you-go without any upfront cost. Pricing is\nbased solely on the duration of the new videos MediaConvert creates, and you are\ncharged in 10-second increments.\n MediaConvert offers two pricing tiers for on-demand services: Basic and Profes-\nsional. You will use the Basic tier in this book, although we invite you to investigate the\nProfessional tier if you are going to take your application to the next level (remember\nus if you end up building the next YouTube!). The Basic tier supports features such as\nFrameworks for serverless\nYou might have heard that there are several frameworks that you can use to organize\nand deploy serverless applications. These include the AWS Serverless Application\nModel (https:// github.com/awslabs/serverless-application-model), the Serverless\nFramework (https://serverless.com), Chalice (https://github.com/aws/chalice), and\na few others. In this chapter, you’ll use Serverless Framework to organize and auto-\nmate the deployment of your serverless application. \nOur advice is to always use a framework like the Serverless Framework or Serverless\nApplication Model (SAM). Once you understand the principles of serverless architec-\ntures, a framework accelerates everything you do by leaps and bounds. Appendix C\ncontains more information on the different frameworks we have found useful. There’s\neven an introduction and a bit of a primer on the Serverless Framework that you’ll\nuse in this chapter. Have a look at appendix C when you get a chance.\n",
      "content_length": 2820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "20\nCHAPTER 2\nFirst steps to serverless\nsingle-pass encoding, clipping, stitching, and overlays. The Professional tier supports\nquite a few more features.\n The per-minute rate in the Basic tier depends on the resolution and the frame rate\nof the desired output. It ranges from $0.0075 per minute for basic SD quality output to\n$0.0450 per minute for UHD output. This rate also differs based on the region you are\nin. US East 1 (North Virginia), for example, is cheaper than US West 1 (Northern Cal-\nifornia), but US East 1 is the region you’ll use throughout this book. You can see the\ntiers and the pricing information at https://aws.amazon.com/mediaconvert/pricing/.\nJust remember that there’s no free tier for MediaConvert, so you’ll start paying some-\nthing almost immediately.\n The S3 free tier allows users to store 5 GB of data with standard storage, issue\n20,000 GET requests and 2,000 PUT requests, and transfer 15 GB of data each month.\nLambda provides a free tier with 1 M free requests and 400,000 GB seconds of com-\npute time. You should be well within the free tier limitations of those services. The fol-\nlowing lists the high-level requirements for The 24-Hour Video: \nThe transcoding process converts uploaded source videos to three different res-\nolutions and bit rates: \n– 6 Mbps with a 16 × 9 aspect ratio and a resolution of 1920 × 1080\n– 4.5 Mbps with a 16 × 9 aspect ratio and a resolution of 1280 × 720\n– 1.5 Mbps with a 4 × 3 aspect ratio and a resolution of 640 × 480\nThere will be two S3 buckets: \n– Original files will go into the upload bucket. \n– Files created by AWS MediaConvert will be saved to the transcoded video\nbucket.\nTo make things simpler to manage, you’ll set up a build and deployment system using\nthe Node Package Manager (npm) and the Serverless Framework. First, here’s an\noverview on the AWS services we’ll use in this example.\n2.1.2\nUsing Amazon Web Services (AWS)\nTo create your serverless backend, you’ll use several services provided by AWS. These\ninclude Simple Storage Service (S3) for file storage, MediaConvert for video conver-\nsion, and Lambda for running custom code and orchestrating key parts of the system.\nIn this chapter, you’ll create your first Lambda function to kick off MediaConvert jobs.\nHere’s a brief description for each of the AWS services that we’ll use: \nS3 provides the storage service. Amazon S3 stores the uploaded and newly\ntranscoded videos.\nLambda handles parts of the system that require coordination or that can’t be done directly\nby other services. This function automatically runs when a file is uploaded to an S3\nbucket.\nMediaConvert encodes your videos to different resolutions and bit rates. Default presets\nremove the need to create custom encoding profiles.\n",
      "content_length": 2752,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "21\nPreparing your system\nFigure 2.1 shows a detailed flow of the proposed approach. Note that the only point\nwhere a user needs to interact with the system is at the initial upload stage. This figure\nand the architecture may look complex, but we’ll break the system into manageable\nchunks and tackle them one by one over the course of this chapter.\n2.2\nPreparing your system\nIt’s time to set up AWS services and install the software on your computer. Here’s what\nyou’ll install on your machine:\nNode.js and its package manager (npm) to help manage Lambda functions and\nkeep track of dependencies\nThe AWS command-line interface (CLI) to help with deployments and future\nuse cases and examples\nThe Serverless Framework (npm package) to help you organize and deploy\nyour application to AWS\nIn AWS, you’ll create\nAn Identity and Access Management (IAM) user and roles\nS3 buckets to store video files\nThe first Lambda function\nThis section may seem lengthy, but it explains a number of things that will help you\nthroughout the book. If you’ve already used AWS, you’ll be able to move through this\nsection quickly. \nUpload new\nvideo file\nTranscode\nvideo\n2. Trigger\nLambda\n4. Submit \njob\n1. S3 bucket\nCreate\ntranscode\njob\n3. Lambda\n5. AWS \nMediaConvert\n7. S3 bucket\n6. Save \nfile\nSave\ntranscoded\nvideo\nFigure 2.1\nThe 24-Hour Video backend is built with AWS S3, MediaConvert, and \nLambda. This pipeline may seem to have a lot of steps initially, but in this chapter, \nwe’ll break this down, and you’ll build a scalable serverless system in no time at all.\n",
      "content_length": 1554,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "22\nCHAPTER 2\nFirst steps to serverless\n2.2.1\nSetting up your system\nTo begin, you need to create an AWS account and install a number of software pack-\nages and tools on your computer. Let’s take these in order:\n1.\nCreate an AWS account. It’s free but you will need to provide your credit card\ndetails in case there are any charges if you go over the free tier allotment. \nYou can create your account at https://aws.amazon.com. We highly recom-\nmend that you set up 2 Factor Authentication (2FA) on your account as soon as\npossible. The instructions for 2FA are here: https://amzn.to/2ZASm33. \n2.\nAfter your account is created, download and install the appropriate version of\nthe AWS CLI for your system from here:\nhttp://docs.aws.amazon.com/cli/latest/userguide/installing.html \nThere are different ways to install the CLI, including an MSI installer if\nyou’re using Windows, pip (a Python-based tool), or a bundled installer if\nyou’re using Mac or Linux. \n3.\nInstall Node.js and npm. You can download Node.js from https://nodejs.org/\nen/download/ (npm comes bundled with Node.js). \nYou can install the latest version of Node.js but, at the time of writing, the\nmost up-to-date version supported by Lambda was 14. Node 14.x is what you will\ntarget when you deploy code. \nJust a heads up: in a short while you’ll need to install Serverless Framework. However,\nyou don’t need to do it now. We’ll cover that when it is time.\n2.2.2\nWorking with Identity and Access Management (IAM)\nHaving an AWS account is good, but you cannot do too much with it just yet. For\nexample, the AWS CLI you have just installed isn’t going to function. You will not be\nable to create resources, deploy, or do anything, really. To make AWS work, you’ll\nneed to create an IAM user, assign permissions to the user, and then configure the\nCLI to use the IAM user’s credentials. Let’s do that now: \n1.\nIn the AWS console, click IAM (Identity and Access Management), click Users,\nand then click Add User. \n2.\nGive your IAM user a name (in figure 2.2, we used lambda-upload for the\nname) and select the Programmatic Access check box. Selecting this check box\nallows you to generate an access key ID and a secret access key. (You’ll need\nthese keys to run aws configure in a few steps.) \n3.\nClick Next: Permissions to proceed.\n \n \n \n \n \n",
      "content_length": 2304,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "23\nPreparing your system\n4.\nSelect Attach Existing Policies Directly and then click the checkbox next to\nAdministratorAccess (figure 2.3). Choose Next: Tags to proceed.\nEnable Programmatic \nAccess to generate the \naccess key ID and the \nsecret access key.\nFigure 2.2\nCreating a new IAM user is straightforward when using the IAM console.\nYou are going to give \nAdministratorAccess to \nthis user because the \nServerless Framework \nwill require it at a \nlater stage. \nFigure 2.3\nMake sure to select the AdministratorAccess policy. You will need it to upload functions and \ndeploy other services.\n",
      "content_length": 594,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "24\nCHAPTER 2\nFirst steps to serverless\n5.\nTags are useful for keeping an inventory and metadata, but for this example,\nyou don’t need to do anything. Click Next: Review to go forward.\n6.\nOn the final page, you can review your user details and the permissions sum-\nmary. Choose Create User to proceed. \nYou should now see a table with the username, the access key ID, and the secret access\nkey. You can also download a CSV file with this information. Go ahead and download\nit now to retain a copy of the keys on your computer and click Close to exit (figure 2.4).\nRun aws configure from a terminal on your system. The AWS CLI prompts for several\nthings: \n1.\nAt the prompt for user credentials, enter the access and secret keys generated\nfor the lambda-upload username or the username that you selected previously. \n2.\nYou’ll also be prompted to enter a region. Type us-east-1 and press Enter. We\nrecommend that you use the same region for all services (you’ll find that it’s\ncheaper and makes things easier to configure). The N. Virginia (us-east-1)\nClick Show to see the \nsecret access key.\nDownload the CSV file to \nyour computers. It has \nthe access key ID and \nthe secret acess key.\nFigure 2.4\nRemember to save the access key ID and the secret access key. You won’t be able to \nget the secret access key again once you close this window.\n",
      "content_length": 1341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "25\nPreparing your system\nregion supports everything we’ll use for the duration of this book so make sure\nto use us-east-1 at all times.\n3.\nThere will be one more prompt asking you to select the default output format.\nSet it as json. \nYou are now done with the AWS CLI configuration. You created an IAM user and used\nthat user’s credentials to configure the CLI on your system. Good job! \n2.2.3\nLet’s make a bucket\nThe next step is to create a bucket in S3. This bucket will contain transcoded video\nput there by Elemental MediaConvert. All users of S3 share the same bucket name-\nspace, which means that you have to come up with bucket names that are not in use.\nIn this book, we’ll assume that this bucket is named something like serverless-video-\ntranscoded.\nTo create a bucket\n1.\nIn the AWS console, choose S3 and then click Create Bucket (figure 2.5). \n2.\nType in a name for the bucket and choose US East (N. Virginia) as the region. \n3.\nScroll to the bottom of the page and click Create Bucket to confirm. Your\nbucket should immediately appear in the console.\nGranular permissions\nThe best practice when it comes to permissions in AWS is to make them granular.\nThis means that your IAM users and roles should have only the specific permissions\nneeded to carry out their purpose. They shouldn’t have all administrator-level permis-\nsions, for example, unless there is a good reason for it. \nYou just created an IAM user that has administrator-level permissions. This flies in\nthe face of the advice we’ve just given. The reason for this is that the Serverless\nFramework, which you will use shortly, is going to need administrator-level access.\nThe Framework calls to a lot of APIs, and it’s difficult to configure an IAM user with\njust the right permissions. If you aren’t going to use the Serverless Framework and\nwant to deploy functions using the AWS CLI instead, then we’d recommend creating\nan IAM user and assigning a few specific permissions needed to upload your functions.\nBucket names\nBucket names must be unique throughout the S3 global resource space. We’ve\nalready taken serverless-video-transcoded, so you’ll need to come up with a different\nname. We suggest adding your initials (or a random string of characters) to these\nbucket names to help identify them throughout the book (for example, serverless-\nvideo-upload-ps and serverless-video-transcoded-ps).\n",
      "content_length": 2376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "26\nCHAPTER 2\nFirst steps to serverless\nNOTE\nYou are going to end up needing another S3 bucket to which you will\nupload videos in the first place. The Serverless Framework creates this bucket\nfor you automatically in the next section, so you don’t need to do anything\nyet. You can create the transcoded video bucket using CloudFormation inside\nthe Serverless Framework’s serverless.yml file too, but explaining that is out-\nside the scope of this chapter (good exercise, though).\n2.2.4\nCreating an IAM role\nNow you need to create an IAM role for your first Lambda function (you will create\nthis function in a little while). The role allows your function to interact with S3 and\nElemental MediaConvert. You’ll add two policies to this role: \n\nAWSLambdaExecute \n\nAWSElementalMediaConvertFullAccess\nThe AWSLambdaExecute policy allows Lambda to interact with S3 and CloudWatch.\nCloudWatch is an AWS service for collecting log files, tracking metrics, and setting\nSet the right region to reduce costs \nand minimize latency. Your Lambda \nfunctions should be in the same region. \nIf the bucket name is taken, AWS \nshows you an error message.\nFigure 2.5\nCreating a bucket from the AWS S3 console. Remember that bucket names are globally \nunique, so you’ll have to come up with your own new name.\n",
      "content_length": 1289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "27\nPreparing your system\nalarms. The AWSElementalMediaConvertFullAccess policy allows Lambda to submit\nnew transcoding jobs to Elemental MediaConvert.\n1.\nIn the AWS console, find and click IAM.\n2.\nChoose Roles. \n3.\nClick the Create Role button to begin. You will see a list of different AWS tech-\nnologies under AWS Service. Select Lambda and the Next: Permissions button. \n4.\nIn this view, you can search for and attach premade policies. Find and attach\n(by clicking the checkbox on the left) the following two policies: \n– AWSLambdaExecute\n– AWSElementalMediaConvertFullAccess \n5.\nClick Next: Tags to advance.\n6.\nClick Next: Review to proceed to the Review page.\n7.\nName your role transcode-video and click Create Role.\nAfter the role is created, you’ll see the list of your existing roles again. Choose\ntranscode-video to see what’s inside. It should look like figure 2.6.\nTwo policies have been added to the role. \nPermissions are embedded within policies.\nFigure 2.6\nTwo managed policies are needed for the transcode-video role to access S3 and \ncreate Elemental MediaConvert jobs.\n",
      "content_length": 1087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "28\nCHAPTER 2\nFirst steps to serverless\n2.2.5\nUsing AWS Elemental MediaConvert\nYou are going to use Elemental MediaConvert to convert uploaded video files from\none format to another. At a high level, MediaConvert works by taking a file uploaded\nto an S3 bucket, transcoding the file to one or more different versions, and then plac-\ning these versions in to another S3 bucket. \n When you create a MediaConvert job, you’ll have to specify this information,\nincluding input and output buckets, and what conversion you’d like to carry out. You\nwill also have to specify a MediaConvert endpoint. Each user has a custom endpoint,\nand you need to know where it is. Let’s find it now so that you’ll know where to look.\n1.\nIn the AWS console, select MediaConvert (it will be under the Media Services\ncategory).\n2.\nClick the hamburger icon in the top left corner (it looks like three parallel\nlines).\n3.\nChoose Account from the menu.\nYou should see the API endpoint that you will need to use in this chapter (figure 2.7).\nNote that you can always refer to these instructions to find the MediaConvert API end-\npoint if you forget where it is.\nYou are nearly there! There’s one more IAM role you need to create now to make\nthings easier later.\nThis API endpoint will be needed \nfor your first Lambda function.\nFigure 2.7\nViewing the API endpoint that you will use in this chapter\n",
      "content_length": 1368,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "29\nStarting with the Serverless Framework\n2.2.6\nUsing MediaConvert Role\nYou need to create a role for the MediaConvert service. MediaConvert needs to have\naccess to S3 as well as the API Gateway. Without this role, MediaConvert simply will\nnot run when you try to invoke it from Lambda. To create the role, follow these steps\n(or try it on your own!):\n1.\nIn the AWS console, click IAM and then click Roles. \n2.\nClick the Create Role button. You will see a list of different AWS technologies\nunder AWS Service. Select MediaConvert and the Next: Permissions button.\nAWS has already predefined what policies you need for this role. These are S3\nFull Access and API Gateway Full Invoke. \n3.\nChoose Next: Tags, then click Next: Review to proceed to the next page.\n4.\nName your role media-convert-role and click Create Role.\n5.\nCopy the ARN of the role to a notepad or someplace where it can be easily\nretrieved. You’ll need the role ARN as well as the API endpoint later (in listing 2.3).\n2.3\nStarting with the Serverless Framework\nThe Serverless Framework is going to help you organize your functions and deploy\nthem to AWS. This framework is powerful, so you will get a lot of flexibility in terms of\nhow to package code, what variables to use, and environments to deploy to. \n If you get stuck with the Serverless Framework, have a look at appendix C or check\nout https://serverless.com/framework/docs. Appendix C features a thorough walk-\nthrough of the Serverless Framework as well as useful hints and tips. However, if you\ndon’t find your answer there, then the online documentation is the way to go.\n2.3.1\nSetting up the Serverless Framework\nInstall the Serverless Framework by running npm install -g serverless from the ter-\nminal. At the time of writing, we used Serverless Framework 2.63. If you are using a\nlater version of the framework and something doesn’t work, you may have to apply\nyour best detective skills to figure out what’s wrong and fix it (or downgrade to 2.63).\nCREDENTIALS\nThe Serverless Framework needs access to your AWS account so that it can create and\nmanage resources on your behalf. By default, Serverless Framework uses the AWS pro-\nfile you have already configured on your machine using the AWS CLI.\nSource code on GitHub\nThe source code for this chapter can be found at https://github.com/sbarski/serverless\n-architectures-aws-2.\n",
      "content_length": 2362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "30\nCHAPTER 2\nFirst steps to serverless\nHELLO WORLD!\nHaving installed the Serverless Framework and configured credentials, let’s test that it\nworks. In your terminal, run the following command:\nserverless create --template aws-nodejs --path hello-world\nChange to the newly created directory by typing cd hello-world. You should see two\nfiles in this directory: serverless.yml and handler.js. The first file, serverless.yml, is a\nproject file (a service) that describes functions, events, and resources that the func-\ntion can use. The second file, handler.js, is an example Lambda function that you can\nchange! Open handler.js and modify the implementation of the function as the fol-\nlowing listing shows.\n'use strict';\nmodule.exports.hello = async (event) => {\n  return {\n    statusCode: 200,\n    body: JSON.stringify(\n      {\n        message: 'Hello Serverless World!',\n        input: event,\n      },\n      null,\n      2\n    ),\n  };\n};\nOnce you have finished modifying the function, remember to save the file. You are\nnow ready to deploy. Run serverless deploy from the terminal and press Enter\n(make sure you are in the same directory as the serverless.yml file before you deploy;\notherwise, you’ll get an error message). \n You’ll see the Serverless Framework package up files, prepare a CloudFormation\nstack, and deploy your function to AWS. As soon as the deployment finishes, you’ll see\na bit of useful information such as the stage used for the function (dev), the region\n(us-east-1), and the name of the service (hello-world). Your function will be called\nhello-world-dev-hello a combination of the service name, stage, and the function\nexport.\n You can finally check that the function was successfully deployed by opening the\nLambda console in AWS and running the function from there. Another option is to\ninvoke the deployed function from the terminal. \n To run the function in AWS and return a response, execute the following com-\nmand in the terminal: serverless invoke --function hello. The Serverless Frame-\nwork will know to invoke this function from the cloud environment.\nListing 2.1\nA new hello-world Lambda function\nBecause this is a basic function, you \ncan run it in AWS Lambda and see a \nmessage.\nThe only line of code that you need to \nmodify before running this example\n",
      "content_length": 2294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "31\nStarting with the Serverless Framework\n2.3.2\nBringing Serverless Framework to The 24-Hour Video\nNow that you have gotten the Serverless Framework to work, let’s get busy with The 24-\nHour Video. You are going to create a new function and reference it in serverless.yml.\nYou will be able to deploy the function (and then add additional functions) with a single\ncommand and, later, sustainably grow and organize your entire serverless application.\n1.\nIn a terminal window, run the following command: \nsls create --template aws-nodejs --path twentyfour-hour-video\nThe reason we used twentyfour instead of 24 is because a service name must\nbegin with an alphabetic character.\n2.\nChange to the new twentyfour-hour-video directory that was just created.\n3.\nDelete handler.js but leave serverless.yml intact.\n4.\nCreate a new subfolder called transcode-video.\nIn a moment, you’ll begin changing serverless.yml. You can stick to our implementa-\ntion (listing 2.2), however, there are five parameters that you must change to correctly\nreflect your environment. These parameters are bolded in listing 2.2:\nThe name of the upload bucket\nThe name of the transcoded video bucket\nThe video role ARN for your function\nThe MediaConvert endpoint\nThe MediaConvert role\nservice: twentyfour-hour-video \nprovider:\n  name: aws  \n  runtime: nodejs14.x  \n  region: us-east-1        \n    \ncustom:\n  upload-bucket: upload-video-bucket  \n  transcode-bucket: transcoded-video-bucket \n  transcode-video-role:\n  ➥ arn:aws:iam::038221756127:role/transcode-video \nListing 2.2\nChanging serverless.yml for your function\nServerless deploy\nYou don’t need to type serverless (as in serverless deploy) each time you want\nto deploy or do an operation. You can use the sls abbreviation. The following com-\nmands are entirely valid: sls deploy or sls invoke --function hello.\nThe provider is AWS, but Serverless Framework supports \nother Cloud providers like Azure and Google Cloud too.\nSet this to nodejs14.x if it’s not already set.\nDefines the region to deploy to. You can override \nthis setting and deploy to other regions.\nSet this custom variable to the \nname of your upload bucket.\nSet the transcode-bucket to the name \nof your transcoded video bucket. You \ncreated this bucket in section 2.2.3.\nSet the transcode-video role \nARN you created in section 2.2.4. \nUpdate the ARN to your value.\n",
      "content_length": 2364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "32\nCHAPTER 2\nFirst steps to serverless\n  media-endpoint:\n  ➥ https://u4ac0ytu.mediaconvert.us-east-1.amazonaws.com \n  media-role:\n  ➥ arn:aws:iam::038221756127:role/media-convert-role \nfunctions:\n  transcode-video: \n    handler: transcode-video/index.handler\n    role: ${self:custom.transcode-video-role}\n    package:\n      individually: true\n    environment:\n      MEDIA_ENDPOINT: ${self:custom.media-endpoint}\n      MEDIA_ROLE: ${self:custom.media-role}\n      TRANSCODED_VIDEO_BUCKET: ${self:custom.transcode-bucket}\n    events:\n      - s3: ${self:custom.upload-bucket}  \nHere’s a brief explanation of everything you need to update in listing 2.2 to make it\nwork for you. Let’s begin with the upload bucket.\nUPLOAD BUCKET\nIn listing 2.2, you must specify the name of the upload bucket. This is a new bucket\nthat doesn’t yet exist. Remember, you need to use a bucket name that is globally\nunique. One way to do this is to prefix or postfix your full name (unless you have a\ncommon name) or add a few random letters and numbers. \n Serverless Framework via CloudFormation creates the bucket for you automati-\ncally. You can go for something like upload-bucket-firstname-lastname. If the bucket\nname is already taken, Serverless Framework will tell you during deployment. You’ll\nbe able to change it and try again.\nTRANSCODED VIDEO BUCKET\nIn listing 2.2, there’s a custom property called transcode-bucket. This property con-\ntains the name of your transcoded video bucket. Update this property to the name of\nthe bucket you manually created in section 2.2.3. \nLAMBDA ROLE ARN\nYou must specify an IAM role for the function. Luckily, you created a role in section\n2.2.4. You need to find the ARN of that role, copy it, and then update the parameter\ncalled transcode-video-role. To get the role ARN and update serverless.yml, follow\nthese easy steps:\n1.\nIn the IAM console, select Roles.\n2.\nFind the transcode-video role and select it.\n3.\nCopy the value for the role ARN.\n4.\nPaste the value in to the serverless.yml file for the transcode-video-role.\nMEDIACONVERT ENDPOINT\nIn listing 2.2, you’ll find a line that creates a media-endpoint variable. To get this end-\npoint, refer to section 2.2.5 or follow these steps:\nSet your personal \nMediaConvert endpoint \nfor the service to work. \nYour URL will be different \nso be sure to change this.\nSet the MediaConvert role \nARN you created in section \n2.2.6. If you kept the same \nname, change the account \nnumber (e.g., 038221751234) \nto your account number and \neverything should work.\nSpecifies the event trigger for the Lambda \nfunction, which is the S3 upload bucket\n",
      "content_length": 2611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "33\nStarting with the Serverless Framework\n1.\nIn the AWS console, select MediaConvert (it will be under the Media Services\ncategory).\n2.\nClick the hamburger icon in the top left corner (it’s the button that looks like\nthree parallel lines).\n3.\nChoose Account from the menu. You’ll see the API endpoint that you should\ncopy into listing 2.2.\nMEDIACONVERT ROLE\nIn section 2.2.6, you created an IAM role for the Element MediaConvert service. In list-\ning 2.2, you needed to specify the ARN for that role. Make sure to look it up and copy\nit over correctly. Be careful not to confuse the two IAM roles that you have. The IAM\nrole created in section 2.2.4 is intended for the transcode-video Lambda function. The\nrole created in 2.2.6 is intended for MediaConvert and is the one you should use.\n2.3.3\nCreating your first Lambda function\nNow that you’ve created a serverless.yml file, change to the transcode-video folder,\nand in your terminal window, run npm init. Agree to all the options by pressing\nEnter. You can change anything you want; it will not affect your function. \n You’ll get a new file called package.json. This file can be used later if you want to\nadd additional dependencies or libraries into your function. Now, let’s discuss how\nyour new function will work and what it will do:\nThe function will invoke as soon as a new file is uploaded to an S3 bucket. \nInformation about the uploaded video will pass to the Lambda function via the\nevent object. It will include the bucket name and the name (key) of the file\nbeing uploaded. \nThe Lambda function will prepare a transcoding job for AWS MediaConvert.\nThe function will submit the job to MediaConvert and writes a message to an\nAmazon CloudWatch log stream. \nCreate a new file named index.js and open it in your favorite text editor. This file con-\ntains the first function. The important thing to note is that you must define a function\nhandler, which will be invoked by the Lambda runtime.\n Listing 2.3 shows this function’s implementation. Copy this listing into index.js.\nBefore you can deploy and run this code though, you’ll need to make a few small\nchanges as detailed in the text after the code listing.\n'use strict';\nconst AWS = require('aws-sdk');\nconst mediaConvert = new AWS.MediaConvert({\n    endpoint: process.env.MEDIA_ENDPOINT \n});\nListing 2.3\nCreating the transcode video Lambda\nGets the MediaConvert endpoint \nenvironment variable that’s set \nin serverless.yml (listing 2.2)\n",
      "content_length": 2459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "34\nCHAPTER 2\nFirst steps to serverless\nconst outputBucketName =\n➥ process.env.TRANSCODED_VIDEO_BUCKET;  \nexports.handler = async (event, context) => { \n    const key = event.Records[0].s3.object.key; \n    const sourceKey = decodeURIComponent(key.replace(/\\+/g, ' ')); \n    const outputKey = sourceKey.split('.')[0]; \n    const input = 's3://' + event.Records[0].s3.bucket.name + '/' +  \n    ➥ event.Records[0].s3.object.key; \n    const output = 's3://' + outputBucketName + '/' + outputKey + '/';\n    try {\n        const job = {\n            \"Role\": process.env.MEDIA_ROLE, \n            \"Settings\": {\n                \"Inputs\": [{\n                    \"FileInput\": input,     \n                    \"AudioSelectors\": {     \n                        \"Audio Selector 1\": {\n                            \"SelectorType\": \"TRACK\",\n                            \"Tracks\": [1]\n                        }\n                    }\n                }],\n                \"OutputGroups\": [{ \n                    \"Name\": \"File Group\",\n                    \"Outputs\": [{\n                        \"Preset\": \"System-\n                         ➥ Generic_Hd_Mp4_Avc_Aac_16x9_1920x1080p_24Hz_6Mbps\",\n                        \"Extension\": \"mp4\",\n                        \"NameModifier\": \"_16x9_1920x1080p_24Hz_6Mbps\"\n                    }, {\n                        \"Preset\": \"System-\n                         ➥ Generic_Hd_Mp4_Avc_Aac_16x9_1280x720p_24Hz_4.5Mbps\",\n                        \"Extension\": \"mp4\",\n                        \"NameModifier\": \"_16x9_1280x720p_24Hz_4.5Mbps\"\n                    }, {\n                        \"Preset\": \"System-\n                         ➥ Generic_Sd_Mp4_Avc_Aac_4x3_640x480p_24Hz_1.5Mbps\",\n                        \"Extension\": \"mp4\",\n                        \"NameModifier\": \"_4x3_640x480p_24Hz_1.5Mbps\"\n                    }],\n                    \"OutputGroupSettings\": {\n                        \"Type\": \"FILE_GROUP_SETTINGS\",\n                        \"FileGroupSettings\": {\n                            \"Destination\": output \n                        }\n                    }\n                }]\n            }\n        };\n        const mediaConvertResult = await \n        ➥ mediaConvert.createJob(job).promise();\nGets the transcoded video bucket name \nthat’s specified in serverless.yml\nGets the MediaConvert role ARN \nthat’s specified in serverless.yml\nSets the location of\nthe input video for\nthe MediaConvert\njob definition\nSpecifies the Audio Selector for the \nMediaConvert job definition. You’ll \ndefault to naming a single audio \ntrack in the video.\nSets the output bucket \nfor the new video files\n",
      "content_length": 2594,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "35\nStarting with the Serverless Framework\n        console.log(mediaConvertResult);\n    } catch (error) {\n        console.error(error);\n    }\n};\nMEDIACONVERT OUTPUTS\nThe function in listing 2.3 declares three new outputs that define the format for your\nnewly transcoded videos (this includes bit rate, resolution, and so forth). The tem-\nplates specified in listing 2.3 are generic templates built in to MediaConvert. Luckily,\nyou aren’t forced to use the ones we’ve selected; you can choose from different tem-\nplates or even create your own. To look at other available presets in MediaConvert do\nthe following:\n1.\nIn the AWS console, select MediaConvert.\n2.\nClick the hamburger icon in the top left corner. \n3.\nChoose Output Presets.\n4.\nFrom the dropdown that says Custom Presets, select System Presets. \nYou’ll see a grid of different presets you can use (figure 2.8). Note that the grid has mul-\ntiple pages and that you can choose to see different categories (MP4, HLS, Broadcast-\nXDCAM, and so on).\nFilter by category to see how \nmany different options you \ncan choose from.\nFigure 2.8\nThe MediaConvert Output Presets page lets you select system presets or configure your \nown.\n",
      "content_length": 1183,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "36\nCHAPTER 2\nFirst steps to serverless\nIf you want to create a different type of video using the code in listing 2.3, select the\nname of the desired preset, and copy it into the function as you did for the others\n(remember to specify the extension and a modified name). As an example, if you\nwant to add HLS output, you’d need to include something like this in the Outputs\narray in the function:\n{\n   \"Preset\": \"System-Ott_Hls_Ts_Avc_Aac_16x9_1280x720p_30Hz_3.5Mbps\",\n   \"Extension\": \"hls\",\n   \"NameModifier\": \"_Hls_Ts_Avc_Aac_16x9_1280x720p_30Hz_3.5Mbps \"\n}\nDEPLOYMENT\nDeploy your first function from the terminal by typing sls deploy (make sure you\nissue sls deploy from the directory where serverless.yml is located). The deployment\nshould succeed, and you should see your functions in AWS. The first function is going\nto be named something like twentyfour-hour-video-dev-transcode-video. Later, if\nyou want, you can remove this function from AWS by running sls remove from the\nterminal. \n One other note: the deployment process may create an additional bucket named\nsomething \nlike \ntwentyfour-hour-video-de-serverlessdeploymentbuck-sq06y6wjku9z.\nThis is normal. The Serverless Framework creates this bucket to upload the Cloud-\nFormation templates it generates. You can safely ignore this bucket, but do not manu-\nally delete! The sls remove command will remove it for you (along with the deployed\nLambda function). \n2.4\nTesting in AWS\nTo test your first function, upload a video to the upload bucket. Follow these steps: \n1.\nGo to the S3 console.\n2.\nClick into your upload bucket and then select Upload to open the Upload page\n(figure 2.9).\n3.\nClick Add Files, select a video file from your computer, and click the Upload\nbutton. All other settings can be left as is. If you don’t have any video files to\ntest, go to https://sample-videos.com and grab one of the MP4 videos. \nAfter a time, you should see three new videos in your transcoded video bucket. These\nfiles should appear in a folder rather than in the root of the bucket (figure 2.10). The\nlength of time to produce a new video depends on the duration of the file you’ve\nuploaded. It may take five minutes (or even longer) to produce a new file so grab a\ncup of tea while you wait. \n \n \n \n",
      "content_length": 2255,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "37\nLooking at logs\n2.5\nLooking at logs\nHaving performed a test in the previous section, you should see three new files in your\ntranscoded video bucket. But things may not always go as smoothly (although we hope\nthey do)! In case of problems, such as new files not appearing, you can check two dif-\nferent logs for errors. The first and most important one is Lambda’s log in Cloud-\nWatch. To view the log, perform the following steps: \n \n \nClick Add Files to bring \nup the dialog box.\nFigure 2.9\nTo test in ASW, it’s better to upload a small file initially because it makes the upload and \ntranscoding go a lot quicker.\nThese files are in an output \nfolder created in the root \nof the bucket. \nFigure 2.10\nMediaConvert generates three new files and places them in a folder in the transcoded \nvideo S3 bucket.\n",
      "content_length": 808,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "38\nCHAPTER 2\nFirst steps to serverless\n1.\nChoose Lambda in the AWS console and then click your function name.\n2.\nChoose the Monitor tab. You should see different graphs with numbers. One of\nthose graphs will be labeled Error Count and Success Rate. If there is a spike\n(that is, the count is more than 0), it means there is a problem. \n3.\nClick View Logs in CloudWatch to open CloudWatch. You’ll see all the log\nentries ordered by date. On the right, you’ll see which stream they belong to. \n4.\nClick each log entry to see more details including error messages. \n5.\nIf you previously saw that your Invocation error rate was more than 0, find the\nlog entry with the error and fix the problem. \nIf the Lambda logs reveal nothing out of the ordinary, take a look at the AWS Media-\nConvert logs. To view these logs: \n1.\nClick MediaConvert in the AWS console. \n2.\nChoose the hamburger icon on the left to open the sidebar. \n3.\nChoose Jobs from the menu. On the right you should see a list of jobs. \n4.\nClick a job (if it failed) to see more information (figure 2.11).  \n \n \n  \nChoose the job ID to view \ndetails about the error.\nFigure 2.11\nMediaConvert failures can occur for a variety of reasons including the source file being \ndeleted before the job started, an error with the code in the Lambda function, or a misconfiguration.\n",
      "content_length": 1328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "39\nSummary\nSummary\nThe best way to organize serverless applications is to use an Infrastructure as\nCode (IaC) framework like the Serverless Framework. \nDeploying functions and manually setting up services is great for learning, but\nit is not sustainable in the long term. The Serverless Framework can help to\norganize and deploy even the most complex serverless applications.\nServerless applications and pipelines usually consist of different services\nconnected together. In The 24-Hour Video example, we use AWS Lambda, S3,\nand Elemental MediaConvert. Most serverless applications use a combination\nof services.\nAWS CloudWatch is an important service for logging what happens within your\nAWS Lambda functions. It’s vital that you learn how to use it as you are most\ndefinitely going to need it.\nSecurity in AWS is controlled primarily via Identity and Access Manage-\nment (IAM), although there are some exceptions. If you want to become an\nexpert at AWS and serverless applications, knowing how IAM works is essential.\nEstimating cost in AWS can be tricky. A lot of services have generous free tiers\nbut can end up costing a lot if used incorrectly. Make sure you review the costs\nof all services you want to use and understand what the potential cost can be.\nWhen problems happen\nIn our experience, problems often occur because IAM permissions haven’t been con-\nfigured correctly or there was a typo somewhere in your function code. AWS doesn’t\nalways have the most descriptive error messages so, sometimes, a bit of digging\naround and investigative work with CloudWatch is required.\n",
      "content_length": 1592,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "40\nArchitectures\n and patterns\nWhat are the use cases for serverless architectures and what kinds of architectures\nand patterns are useful? We’re often asked these questions and queried about use\ncases as people learn about a serverless approach to designing systems. We find that\nit’s helpful to look at how others have applied this technology and what kinds of\nuse cases, designs, and architectures they’ve produced. \n This chapter gives you a solid introduction to where serverless architectures are\na good fit and how to think about the design of serverless systems. The rest of the\nbook focuses on real-world use cases and goes deep into a number of serverless\narchitectures that we’ve found particularly fascinating. \n3.1\nUse cases\nServerless technologies and architectures can be used to build entire systems, cre-\nate isolated components, or implement specific granular tasks. The scope for use of\nThis chapter covers\nUse cases for serverless architectures\nExamples of patterns and architectures\n",
      "content_length": 1006,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "41\nUse cases\nserverless design is broad, and one of its advantages is that it’s possible to use it for\nsmall as well as large tasks alike. We’ve designed serverless systems that power web and\nmobile applications for tens of thousands of users, and we’ve built simple systems to\nsolve specific minute problems. \n It’s worth remembering that serverless is not just about running code in a compute\nservice such as Lambda. It’s also about using third-party services and APIs to cut down\non the amount of work you must do. With this in mind, let’s look at some basic use cases.\n3.1.1\nBackend compute\nTechnologies such as AWS Lambda are a few years old, but we’ve already seen large\nserverless backends that power entire businesses. A Cloud Guru (https://acloudguru\n.com), for example, supports many thousands of users collaborating in real time and\nstreams hundreds of gigabytes of video. Another example is the insurance company,\nBranch, which from the start adopted a serverless-first approach (https://amzn.to/\n3vRumYU). \n Indeed, it is possible to create and run an entire business while having a serverless-\nfirst mindset. If you articulate that kind of philosophical approach to technology your-\nself, it will help you answer questions such as what services to adopt or how to best\nsolve a particular architectural problem. \n Startups are not the only organizations looking for agility and efficiencies from\nserverless. Established companies with long histories are also using serverless technol-\nogies and architectures to deliver value to their customers. Some of these bigger com-\npanies include well-known names like Comcast, Coinbase, Fender, Nordstrom, and\nNetflix (https://aws.amazon.com/serverless/customers/). \n3.1.2\nInternet of Things (IoT)\nPutting aside web and mobile applications, serverless is a great fit for the Internet of\nThings (IoT) applications. Amazon Web Services (AWS) has a useful IoT platform\n(https://aws.amazon.com/iot-platform/how-it-works/) that combines\nAuthentication and authorization\nCommunications gateway\nRegistry (a way to assign a unique identity to each device)\nDevice shadowing (to persist device state)\nRules engine (to transform and route device messages to AWS services)\nThe rules engine, for example, can save files to Amazon’s Simple Storage Service (S3),\npush data to an Amazon Simple Queue Service (SQS) queue, and invoke AWS\nLambda functions. Amazon’s IoT platform makes it easy to build scalable IoT back-\nends for devices without having to run a server. A serverless application backend is\nappealing because it removes a lot of infrastructure management, has granular and\npredictable billing (especially when a serverless compute service such as Lambda is\nused), and can scale well to meet uneven demands.\n",
      "content_length": 2762,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "42\nCHAPTER 3\nArchitectures and patterns\n3.1.3\nData processing and manipulation\nA common use for serverless technologies is data processing, conversion, manipula-\ntion, and transcoding. We’ve seen Lambda functions built by other developers for pro-\ncessing CSV, JSON, and XML files; collation and aggregation of data; image resizing;\nand format conversion. Lambda and AWS services are well suited for building event-\ndriven pipelines for data-processing tasks.\n In chapter 2, you built a powerful pipeline for converting videos from one format\nto another. This pipeline runs only when a new video file is added to a designated S3\nbucket, meaning that you only pay for the execution of Lambda when there’s some-\nthing to do and never while the system is idle. More broadly, however, we find data\nprocessing to be an excellent use case for serverless technologies, especially when we\nuse Lambda in concert with other services.\n3.1.4\nReal-time analytics\nIngestion of data such as logs, system events, transactions, or user clicks can be accom-\nplished using services such as Amazon Kinesis Data Streams and Amazon Kinesis Fire-\nhose. Kinesis Data Streams and Lambda functions are a good fit for applications that\ngenerate a lot of data that needs to be analyzed, aggregated, and stored. When it\ncomes to Kinesis, the number of functions spawned to process messages from a stream\nis the same as the number of shards (therefore, there’s one Lambda function per\nshard as figure 3.1 shows). \n If a Lambda function fails to process a batch, it retries the operation. This can\nkeep going for up to 24 hours (which is how long Kinesis will keep data around before\nit expires) each time processing fails. \nKinesis Streams can ingest a lot of messages \nthat can be processed with Lambda functions. \nData-intensive applications that perform real-time \nreporting and analytics can benefit from this architecture. \nFile\nstorage (S3)\nLambda\n(retrieve\nbatch of 100)\nDatabase\nLambda\n(retrieve\nbatch of 50)\nKinesis\nStreams\nKinesis\nStreams\nEvents/messages\nEvents/messages\nFigure 3.1\nLambda is a perfect tool to process data in near real time.\n",
      "content_length": 2122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "43\nUse cases\nAmazon Kinesis Firehose is another Kinesis service designed to ingest gigabytes of\nstreaming data and then push it into other services like S3, RedShift, or Elasticsearch\nfor further analytics. Firehose is a true serverless service because it is fully managed, it\nscales automatically depending on the volume of data coming in, and there’s no need\nto think about sharding as is the case with Kinesis Data Streams. \n A great feature of Kinesis Firehose is that a Lambda function can be added to the\nstream to seamlessly process data as it is added and before it is sent to its final destina-\ntion. You can use this to transform data while it’s in flight without having to provision\nany other infrastructure. We are not going to go into much more depth right now\nbecause chapter 6 and chapter 9 discuss use cases and applications for the Kinesis\nproducts in more detail. \n3.1.5\nLegacy API proxy\nOne innovative use case of the Amazon API Gateway and Lambda that we’ve seen a\nfew times is what we refer to as the legacy API proxy. Here, developers use API Gate-\nway and Lambda to create a new API layer over legacy APIs and services, which makes\nthem easier to use. \n The API Gateway creates a RESTful interface, and Lambda functions modify\nrequest/response and marshal data to formats that legacy services understand. The\nAPI Gateway and Lambda functions can transform requests made by clients and\ninvoke legacy services directly as figure 3.2 illustrates. This approach makes legacy ser-\nvices easier to consume for modern clients that may not support older protocols and\ndata formats. \nMost legacy services will \nrequire a Lambda function \nto convert data and to \ncorrectly invoke it.\nAPI\nGateway\nLambda \n(convert/\ninvoke)\nLambda \n(convert/\ninvoke)\nLambda \n(convert/\ninvoke)\nLegacy API\nLegacy API\nLegacy\nservice\n(SOAP)\nLegacy API\nLegacy\nservice\n(XML)\nFigure 3.2\nWe can use the API proxy architecture to build a modern API interface over old services \nand APIs.\n",
      "content_length": 1973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "44\nCHAPTER 3\nArchitectures and patterns\nIt’s important to note that the API Gateway can transform (to an extent) and issue\nrequests against other HTTP endpoints. But it works only in a number of fairly basic\nand limited use cases where JSON transformation is needed. In more complex scenar-\nios, however, a Lambda function is needed to convert data, issue requests, and process\nresponses. \n Take a Simple Object Access Protocol (SOAP) service as an example. You’d need\nto write a Lambda function to connect to a SOAP service and then map responses to\nJSON. Thankfully, there are libraries that can take care of much of the heavy lifting in\na Lambda function; for example, there are SOAP clients that can be downloaded\nfrom the npm registry for this purpose (see https://www.npmjs.com/package/soap).\n3.1.6\nScheduled services\nLambda functions can run on a schedule, which makes them effective for repetitive\ntasks like data backups, imports and exports, reminders, and alerts. We’ve seen devel-\nopers use Lambda functions on a schedule to periodically ping their websites to see if\nthey’re online and send an email or a text message if they’re not. You’ll find Lambda\nblueprints available for this (a blueprint is a template with sample code that can be\nselected when creating a new Lambda function). \n We’ve also seen developers write Lambda functions to perform nightly downloads\nof files off their servers and send daily account statements to users. Repetitive tasks\nsuch as file backup and file validation can also be done easily with Lambda thanks to\nthe scheduling capability that you can set and forget. Check out chapter 7 for an in-\ndepth analysis on how to go about thinking and building a scheduling service. \n3.1.7\nBots and skills\nAnother popular use of Lambda functions and serverless technologies is to build bots\n(a bot is an app or a script that runs automated tasks) for services such as Slack. A bot\nmade for Slack can respond to commands, carry out small tasks, and send reports and\nnotifications. We, for example, built a Slack bot in Lambda to report on the number\nof online sales made each day. And we’ve seen developers build bots for Telegram,\nSkype, and Facebook’s messenger platform. \n Similarly, developers write Lambda functions to power Alexa skills for Amazon\nEcho. Amazon Echo is a hands-free speaker that responds to voice commands. It runs\na virtual assistant called Alexa. Developers can implement skills to extend Alexa’s capa-\nbilities even further (a skill is essentially an app that can respond to a person’s voice;\nfor more information, see http://amzn.to/2b5NMFj). You can write a skill to order a\npizza or quiz yourself on geography. Alexa is driven entirely by voice, and skills are\npowered by Lambda.\n3.1.8\nHybrids\nAs we mentioned in chapter 1, serverless technologies and architectures are not an all-\nor-nothing proposition. They can be adopted and used alongside traditional systems.\nThis hybrid approach may work especially well if a part of the existing infrastructure is\nalready in AWS. We’ve also seen adoption of serverless technologies and architectures\n",
      "content_length": 3104,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "45\nPatterns\nin organizations with developers initially creating standalone components (often to\ndo additional data processing, database backups, and basic alerting) and, over time,\nintegrating these components into their main systems (figure 3.3).\n3.2\nPatterns\nPatterns are architectural solutions to problems in software design. They’re designed\nto address common problems found in software development. They’re also an excel-\nlent communications tool for developers working together on a solution. It’s far easier\nto find an answer to a problem if everyone in the room understands which patterns\nare applicable, how they work, their advantages, and their disadvantages. \n The patterns presented in this section are useful for solving design problems in\nserverless architectures. But these patterns aren’t exclusive to serverless. They were\nused in distributed systems long before serverless technologies became viable. \n Apart from the patterns presented in this chapter, we recommend that you\nbecome familiar with patterns relating to authentication, data management (e.g.,\nCQRS, event sourcing, materialized views), and error handling (e.g., Retry Pattern).\nLearning and applying these patterns will make you a better software engineer,\nregardless of the platform you choose to use. Let’s look at a few of these patterns.\n3.2.1\nGraphQL\nGraphQL (http://graphql.org) is a popular data query language developed by Face-\nbook in 2012 and released publicly in 2015. It was designed as an alternative to REST\n(Representational State Transfer) because of its perceived weaknesses (multiple\nround-trips, over-fetching, and problems with versioning). GraphQL attempts to solve\nAny legacy system can use functions and services. This \ncan allow you to slowly introduce serverless technologies \nwithout disturbing too much of the world order.\nAPI\nGateway\nLoad\nbalancer\nDatabase\nFile\nstorage\nLambda\n(calculate\ncost)\nServer\nServer\nLambda\n(save\nprofile)\nLambda\nfunction\nFigure 3.3\nThe hybrid approach is useful if you have a legacy system that uses servers.\n",
      "content_length": 2047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "46\nCHAPTER 3\nArchitectures and patterns\nthese problems by providing a hierarchical, declarative way of performing queries\nfrom a single endpoint (e.g., api/graphql). Figure 3.4 shows an example of a\nGraphQL and AWS Lambda implementation. \nGraphQL gives power to the client. Instead of specifying the structure of the response\non the server, it’s defined on the client (http://bit.ly/2aTjlh5). The client can specify\nwhat properties and relationships to return. GraphQL aggregates data from multiple\nsources and returns it to the client in a single round trip, which makes it an efficient\nsystem for retrieving data. According to Facebook, GraphQL serves millions of\nrequests per second from nearly 1,000 different versions of its application. \n A GraphQL library (server) can be hosted and run from a Lambda function. You’ll\nalso find managed solutions of GraphQL such as the ever-popular AWS AppSync at\nhttps://aws.amazon.com/appsync/.\nWHEN TO USE THIS\nGraphQL is a type of composite pattern that lets you aggregate data from multiple\nplaces. Reading and hydrating data from multiple data sources is common in web\napplications and especially so in those that adopt the microservices approach. There\nare other benefits too, including smaller payloads, avoiding the need to rebuild the\ndata model, and no more versioned APIs (as compared to REST). These are just some\nof the reasons why GraphQL has become so popular in the past few years. \n3.2.2\nCommand pattern\nIn the previous section, we mentioned the fact that a single endpoint can be used to\ncater to different requests with different data (a single GraphQL endpoint, for exam-\nple, can accept any combination of fields from a client and create a response that\nmatches the request). The same idea can be applied more generally. You can design a\nOnly a single GraphQL Lambda function is needed \nto query multiple data sources. It can be a viable \nalternative to building a full RESTful interface.\nAPI Gateway/\nGraphQL\nLambda\n(GraphQL)\nDatabase\nDatabase\nDatabase\nDatabase\nDatabase\nFigure 3.4\nThe GraphQL and Lambda architecture has become popular in the serverless community.\n",
      "content_length": 2129,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "47\nPatterns\nsystem in which a specific Lambda function controls and invokes other functions. You\ncan connect it to an API Gateway or invoke it manually and pass messages to it to\ninvoke other Lambda functions. \n In software engineering, the command pattern (figure 3.5) is used to “encapsulate a\nrequest as an object, thereby letting you parameterize clients with different requests,\nqueue or log requests, and support undoable operations” because of the “need to issue\nrequests to objects without knowing anything about the operation being requested or the\nreceiver of the request” (http://bit.ly/29ZaoWt). The command pattern lets you decou-\nple the caller of the operation from the entity that carries out the required processing.\nIn practice, this pattern can simplify an API Gateway implementation because you\nmay not want or need to create a RESTful URI for every request. It can also make ver-\nsioning simpler. The command Lambda function could work with different versions\nof your clients and invoke the right Lambda function that’s needed by the client.\nWHEN TO USE THIS\nThis pattern is useful if you want to decouple the caller and the receiver. Having a way\nto pass arguments as an object and allowing clients to be parametrized with different\nrequests can reduce coupling between components and help make the system more\nextensible. \n3.2.3\nMessaging pattern\nMessaging patterns (figure 3.6) are popular in distributed systems because they allow\ndevelopers to build scalable and robust systems by decoupling functions and services\nfrom direct dependence on one another and allowing storage of events/records/\nrequests in a queue. The reliability comes from the fact that if the consuming service\ngoes offline, the queue retains messages (for some period), which can still be pro-\ncessed at a later time.\nA command function is used to \ninvoke other functions and services. \nIt knows which functions to invoke \nin response to data/events and how \nto call those functions.\nLambda\nfunction\nLambda\nfunction\nFile\nstorage\nDatabase\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(command)\nAPI Gateway\nFigure 3.5\nThe command pattern invokes and controls functions and services from a single function.\n",
      "content_length": 2203,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "48\nCHAPTER 3\nArchitectures and patterns\nThis pattern features a message queue with a sender that can post to the queue and a\nreceiver that can retrieve messages from the queue. In terms of implementation in\nAWS, you can build this pattern on top of the SQS.\n Depending on how the system is designed, a message queue can have a single\nsender/receiver or multiple senders/receivers. SQS queues typically have one\nreceiver per queue. If you need to have multiple consumers, a straightforward way to\ndo it is to introduce multiple queues into the system (figure 3.7). A strategy you could\napply is to combine SQS with Amazon SNS. SQS queues can subscribe to an SNS topic\nso that pushing a message to the topic would automatically push the message to all of\nthe subscribed queues.\nSimilar to the command pattern, there \nis one function that reads messages \nfrom a queue. It invokes appropriate \nLambda functions based on the message.\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(dispatch)\nQueue (SQS) / \nstream (Kinesis)\nData source\nData source\nData source\nFigure 3.6\nThe messaging pattern and its many variations are popular in distributed environments.\nUse multiple queues/streams to decouple \nmultiple components in your system. \nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(dispatch)\nQueue (SQS) / \nstream (Kinesis)\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\n(dispatch)\nQueue #2 (SQS) / \nstream #2 (Kinesis)\nData source\nData source\nData source\nFigure 3.7\nYour system may have multiple queues or streams and Lambda functions to process all \nincoming data.\n",
      "content_length": 1599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "49\nPatterns\nWHEN TO USE THIS\nThe messaging pattern handles workloads and data processing. The queue serves as a\nbuffer, so if the consuming service crashes, data isn’t lost. It remains in the queue until\nthe service can restart and begin processing it again. \n A message queue can make future changes easier, too, because there’s less cou-\npling between functions. In an environment that has a lot of data processing, mes-\nsages, and requests, try to minimize the number of functions that are directly\ndependent on other functions and use the messaging pattern instead. \n3.2.4\nPriority queue pattern\nA great benefit of using a platform such as AWS and serverless architectures is that\ncapacity planning and scalability are more of a concern for Amazon’s engineers than\nfor you. But, in some cases, you may want to control how and when messages get dealt\nwith by your system. This is where you might need to have different queues, topics, or\nstreams to feed messages to your functions. \n Your system might go one step further, having entirely different workflows for mes-\nsages of different priority (the priority queue pattern). Messages that need immediate\nattention might go through a flow that expedites the process by using more expensive\nservices and APIs with more capacity. Messages that don’t need to be processed\nquickly can go through a different workflow as figure 3.8 shows.\nMessages with different priorities can \nbe dealt with by different workflows \nand different Lambda functions.\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nPriority 1\nPriority 2\nPriority 3\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nNotification service (SNS) / Queue (SQS)\nNotification service (SNS) / Queue (SQS)\nNotification service (SNS) / Queue (SQS)\nNotification service (SNS) / Queue (SQS)\nFigure 3.8\nThe priority queue pattern is an evolution of the messaging pattern.\n",
      "content_length": 1902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "50\nCHAPTER 3\nArchitectures and patterns\nThe priority queue pattern might involve the creation and use of entirely different\nSNS topics, SQS queues, Lambda functions, and even third-party services. Use this pat-\ntern sparingly, however, because additional components, dependencies, and work-\nflows result in more complexity. \nWHEN TO USE THIS\nThis pattern works when you need to have a different priority for processing mes-\nsages. Your system can implement workflows and use different services and APIs to\ncater to many types of needs and users (for example, paying versus nonpaying users).\n3.2.5\nFan-out pattern\nFan-out is a type of messaging pattern that’s familiar to many AWS users. Generally,\nthe fan-out pattern pushes a message to all listening/subscribed clients of a particular\nqueue or a message pipeline. In AWS, this pattern is usually implemented using SNS\ntopics that allow multiple subscribers to be invoked when a new message is added to a\ntopic. \n Take S3 as an example. When a new file is added to a bucket, S3 can invoke a sin-\ngle Lambda function with information about the file. But what if you need to invoke\ntwo, three, or more Lambda functions at the same time? The original function could\nbe modified to invoke other functions (like the command pattern), but that’s a lot of\nwork if all you need is to run functions in parallel. The solution is to use the fan-out\npattern with SNS (see figure 3.9).\nA message added to an SNS topic can force invocation \nof multiple Lambda functions in parallel.\nLambda\nfunction\nDatabase\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nNotification \nservice (SNS)\nLambda\nfunction\nLambda\nfunction\nLambda\nfunction\nNotification \nservice (SNS)\nFigure 3.9\nThe fan-out pattern is useful because many AWS services (such as S3) can’t invoke more \nthan one Lambda function at a time when an event takes place.\n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "51\nPatterns\nSNS topics are communications or messaging channels that can have multiple pub-\nlishers and subscribers (including Lambda functions). When a new message is added\nto a topic, it forces invocation of all the subscribers in parallel, thus causing the event\nto fan out. \n Going back to the S3 example discussed earlier, instead of invoking a single\nLambda function, you can configure S3 to push a message to an SNS topic, which\ninvokes all subscribed functions simultaneously. It’s an effective way to create event-\ndriven architectures and perform operations in parallel. Chapter 8 shows how to use\nthis pattern to perform video encoding at scale.\nWHEN TO USE THIS\nThis pattern is useful if you need to invoke multiple Lambda functions at the same\ntime. An SNS topic will retry, invoking your Lambda functions, if it fails to deliver the\nmessage or if the function fails to execute (see https://go.aws/3DTdCEK). \n Furthermore, the fan-out pattern can be used for more than just invocation of\nmultiple Lambda functions. SNS topics support other subscribers such as email and\nSQS queues. Adding a new message to a topic can invoke Lambda functions, send an\nemail, or push a message on to an SQS queue, all at the same time.\n3.2.6\nCompute as glue \nThe compute-as-glue architecture (figure 3.10) describes the idea that we can use\nLambda functions to create powerful execution pipelines and workflows. This often\ninvolves using Lambda as glue between different services, coordinating and invoking\nthem. With this style of architecture, the focus of the developer is on the design of\ntheir pipeline, coordination, and data flow. The parallelism of serverless compute ser-\nvices like Lambda helps to make these architectures appealing.\n \n \n \n \n \n \nSQS vs. SNS vs. EventBridge\nSometimes it’s hard to know which AWS service to use in which situation. When it\ncomes to event messaging, we’ve discussed SQS and SNS already, but there’s also\nAmazon EventBridge to round out the family. The following Lumigo blog post features\nan excellent summary and comparison of these services: https://bit.ly/3AYdJga. We\nhighly recommend that you take a look at it if you are trying to understand their dif-\nferences and use cases. Another good explanation comes from AWS themselves:\nhttps://go.aws/3phPOWW.\n",
      "content_length": 2293,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "52\nCHAPTER 3\nArchitectures and patterns\n3.2.7\nPipes and filters pattern\nThe purpose of the pipes and filters pattern is to decompose a complex processing\ntask into a series of manageable, discrete services organized in a pipeline (figure\n3.11). Components designed to transform data are traditionally referred to as filters,\nwhereas connectors that pass data from one component to the next component are\nreferred to as pipes. Serverless architecture lends itself well to this kind of pattern. This\nis useful for all kinds of tasks where multiple steps are required to achieve a result.\nLambda\n(create\nthumbnail) \nNotification\nservice\n(SNS)\nFile\nstorage\nDatabase\nFile\nstorage (S3)\nLambda\n(write log)\nLog service\n(CloudWatch)\nSearch\nservice\nNotification\nservice\n(SNS)\nLambda\n(update)\nFigure 3.10\nThe compute-as-glue architecture uses Lambda functions to connect different services \nand APIs to achieve a task. In this pipeline, a simple image transformation results in a new file, an \nupdate to a database, an update to a search service, and a new entry to a log service.\nFunctions and services \nare reused in pipelines.\nData source\nLambda \nfunction\nLambda \nfunction\nNotification\nservice (SNS)\nLambda \nfunction\nDatabase\nFile\nstorage\nData source\nLambda \nfunction\nSearch \nservice\nLambda \nfunction\nLambda \nfunction\nFigure 3.11\nThe pipes and filters pattern encourages the construction of pipelines to pass and \ntransform data from its origin (pump) to its destination (sink).\n",
      "content_length": 1471,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "53\nSummary\nWith this pattern, we recommend that every Lambda function be written as a granular\nservice or a task with the single-responsibility principle in mind. Inputs and outputs\nshould be clearly defined (there should be a clear interface) and any side effects min-\nimized. Following this advice will allow you to create functions that can be reused in\npipelines and, more broadly, within your serverless system. \n You might notice that this pattern is similar to the compute-as-glue architecture we\ndescribed previously. You are right, compute as glue and this pattern are closely\nrelated and are simply a variation of the same concept. \nWHEN TO USE THIS\nWhen you have a complex task, try to break it down into a series of functions (a pipe-\nline) and apply the following rules:\nMake sure your function follows the single-responsibility principle.\nClearly define an interface for the function. Make sure inputs and outputs are\nclearly stated.\nCreate a black box. Consumers of the function shouldn’t have to know how it\nworks, but they must know to use it and what kind of output to expect.\nThroughout the rest of this book, we’ll discuss and give more context to the patterns\nand architecture we explored here. With that in mind, let’s jump into the next chap-\nter and read a story about a social network called Yubl.\nSummary\nServerless architecture can support different use cases including building\nbackends for web, mobile, and IoT applications, as well as data processing and\nanalytics.\nServerless technologies like AWS Lambda are flexible. They can be combined\nwith containers or virtual machines into hybrid architectures. You don’t need to\nbe a serverless purist to achieve great outcomes.\nCertain patterns and approaches like GraphQL are well suited to serverless\narchitectures because AWS services such as AppSync are on hand and can inte-\ngrate nicely with the rest of your architecture. \nClassic software engineering patterns like messaging patterns work exception-\nally well with serverless architectures and AWS products such as SQS.\nThe fan-out pattern is one of the more common patterns. Knowing how to set it\nup using Amazon SNS is important to be effective with AWS.\nAWS has a lot of different services and products that overlap. Having a thorough\nunderstanding of when to use each service will help you make better decisions.\n \n \n \n \n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "Part 2\nUse cases\nYou’ve read through part 1 and now, we hope, you have a good understand-\ning of what serverless is all about. It’s time to take a look at how three companies\nuse serverless architectures to solve problems and delight their customers. In\npart 2, we present three use-case studies from Yubl, A Cloud Guru, and Yle. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "57\nYubl: Architecture\n highlights, lessons learned\nIn April 2016, I joined a social network based in London called Yubl. There I inher-\nited a monolithic backend system written in Node.js and running on a handful of\nElastic Compute Cloud (EC2) instances. The original system took 2.5 years to imple-\nment and had a long list of performance and scalability issues once it went live. With\na small team of six engineers, we managed to move the platform to serverless over the\ncourse of six months. Along the way, we added many new features and addressed the\nexisting performance and scalability issues. We reduced feature delivery time from\nmonths to days, and in some cases, hours. Although cost was not the main motivation\nfor undertaking this transformation, we made a 95% savings on our AWS bill in the\nprocess. Let’s take a peek at the original Yubl architecture.\nThis chapter covers\nThe original Yubl architecture and its problems\nThe new serverless architecture and the \ndecisions behind it\nStrategies and patterns for moving monolith \napplications to serverless\nLessons learned from this migration\n",
      "content_length": 1107,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "58\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\n4.1\nThe original Yubl architecture\nYubl (short for “your social bubble”) was a mobile-first, social network designed for\nthe 17 to 25-year-old demographic. The user-generated posts (called yubls) contained\nvideos as well as animated and interactive elements. The app had all the social fea-\ntures you’d find in other social networks: follow users, private and group chat, liking\nand resharing content, and others.\n The original architecture (figure 4.1) consisted of the following:\nA monolithic REST API written in Node.js and running on EC2\nA WebSockets API written in Node.js and running on EC2\nA monolithic MongoDB database hosted in MongoLab\nA CloudAMQP message queue\nA cluster of background workers written in Node.js and running on EC2   \nMongoLab\nRoute53\nELB\nAPI\nRoute53\nELB\nWebSockets\nWorkers\nFigure 4.1\nA high-level overview of the original Yubl architecture\nWhat is MongoDB and MongoLab?\nMongoDB is a popular document-oriented NoSQL database that allows you to store\nJSON documents. You can learn more about it at https://www.mongodb.com.\nMongoLab is an online service that provides MongoDB hosting as a service. You can\ncreate a MongoDB cluster with a few clicks, and MongoLab takes care of the under-\nlying infrastructure for you. You can learn more about it at https://mlab.com. Back\nin 2016, MongoLab was a viable service for running MongoDB without having to man-\nage the underlying infrastructure yourself.\n",
      "content_length": 1488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "59\nThe original Yubl architecture\n4.1.1\nScalability problems\nBeing an early stage social network, the baseline traffic at Yubl was low, but they man-\naged to attract several high-profile Instagram influencers to the platform. These influ-\nencers brought many of their Instagram followers along, and with tens of thousands of\nfollowers, these influencers drove unpredictable and spiky traffic through the system\nwhenever they posted new content. \n We often saw 100x spikes in traffic as thousands of users flooded in all at once to\nsee their favorite influencer’s new content. These traffic spikes were usually short-\nlived, which was problematic for the EC2-based system because EC2 autoscaling\ncouldn’t react fast enough. It typically takes EC2 instances a few minutes to spin up. By\nthe time they are ready to serve user requests, it’s too late. The traffic spikes have\ncome and gone, and many users would have left after having experienced a laggy\nresponse time.\n As a workaround, we ran a much larger EC2 cluster, scaling up much earlier than\nwe wanted. This resulted in a lot of wasted cost because we had to pay for lots of EC2\nresources that we were not using. Our cluster of API web servers had an average utili-\nzation of from 2% to 5%.\n4.1.2\nPerformance problems\nThe monolithic MongoDB database was also a constant source of performance and\nscalability problems. Every read and write operation hit the database directly; some\nAPI operations can take a heavy toll on a MongoDB server. One example of this is a\nuser search, which is a frequently used API call and executes a complex regex query\nagainst MongoDB. Another example included user recommendations, which exe-\ncuted a complex query to find second- and third-degree connections to the current\nuser (those who follow your followers or those followed by users you follow).\n4.1.3\nLong feature delivery cycles\nThe codebase was complex, and many features were intertwined through shared\nMongoDB collections and implicit coupling through shared libraries. Although there\nwere plenty of unit tests with a reasonable code coverage, these did not prove useful\nbecause code changes often passed all the tests, only to fail when deployed to the AWS\nWhat is RabbitMQ and CloudAMQP?\nAdvanced Message Queueing Protocol (AMQP) is an application protocol for mes-\nsage-oriented middlewares. It supports message queueing and routing and is often\nused in publish-and-subscribe systems.\nRabbitMQ is an open-source message broker that implements the AMQP proto-\ncol. You can learn more abo9ut RabbitMQ at https://www.rabbitmq.com/.\nCloudAMQP is an online service that provides RabbitMQ hosting. You can learn\nmore about it at https://cloudamqp.com.\n",
      "content_length": 2698,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "60\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\nenvironment. The interaction with external services was mocked thoroughly and,\ntherefore, not covered by the tests. In many cases, the tests simply confirmed the\nmocks were working and returned what was requested even if the MongoDB query\ncontained syntax errors. We had little faith in the tests because they gave us too many\nfalse-positives.\n To make matters worse, every deployment required taking the whole system down\nfor 30 minutes or more, during which time users received no feedback and the app\njust appeared broken. Features used to take months to go to production. Even simple\nchanges often took weeks to complete, which was frustrating to everyone involved.\n4.1.4\nWhy serverless?\nBased on the requirements for our system and the problems the current implementa-\ntion experienced, serverless was a great fit for the following reasons:\nAWS Lambda autoscales the number of concurrent executions based on load. This\nhappens instantly and handles those unpredictable spikes we experience\neffortlessly.\nAWS Lambda deploys functions to three availability zones by default, which provides sig-\nnificant redundancy without incurring extra costs. We pay only when a function\nruns, whereas with EC2, we paid for the redundancy in a multi-AZ setup, which\nalso dilutes the traffic and reduces the resource utilization even further.\nAWS manages the underlying physical infrastructure as well as the operating system that\nour code runs on. AWS applies patches and security updates regularly and does a\nmuch better job of keeping the operating system secure than we could. This\nremoves a whole class of vulnerabilities that plague so many software systems\naround the world.\nWith tools such as the Serverless framework, the deployment pipeline for our application is\ndrastically simplified. A typical deployment takes less than a minute and has no\ndowntime because AWS Lambda automatically routes requests to the new code.\nWhen using serverless technologies such as API Gateway, Lambda, and DynamoDB, we\ndon’t have to worry about the underlying infrastructure. This lets us focus on address-\ning core business needs. Almost every line of our code is business logic! And it\nallows the development team to move quickly, knowing that what we build is\nscalable and resilient by default.\nThe number of production deployments went from four to six per month to averaging more\nthan 80 per month with the same sized team. We didn’t have to hire more people to\ngo faster, we allowed each developer to be more productive instead.\nAs we migrated more and more of the system to serverless, scalability, cost and reliability all\nimproved. There were far fewer production issues, and we were spending a frac-\ntion of what we spent on EC2 previously.\n",
      "content_length": 2798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "61\nThe new serverless Yubl architecture\n4.2\nThe new serverless Yubl architecture\nBy November 2016, less than 8 months after I joined the company and started us on\nthe journey to serverless, almost the entire backend system was migrated to serverless;\nthis using a combination of services such as API Gateway, Lambda, DynamoDB, Kine-\nsis, and so much more. Along the way, we enhanced existing features and imple-\nmented countless new features. We also addressed many security issues with the\nprevious system. \n Overall, the system’s reliability increased drastically. We experienced only one\nminor outage to our production environment because of a brief Simple Storage Ser-\nvice (S3) outage. The following points are some key highlights of the new serverless\narchitecture on AWS:\nThe monolith was broken up into many microservices.\nEvery microservice has its own GitHub repository and one Continuous Integration/\nContinuous Delivery (CI/CD) pipeline. All the components that make up this\nmicroservice (API Gateway, Lambda functions, DynamoDB tables, etc.) are\ndeployed together as one CloudFormation stack using the Serverless Framework.\nMost microservices have an API Gateway REST API running under its own sub-\ndomain, such as search.yubl.com.\nEvery microservice has its own database for the data it needs. Most use\nDynamoDB, but it’s not universal because different microservices have different\ndata needs.\nEvery state change in the system is captured as an event and published to a\nKinesis Data Stream (for example, a user created new content, a user posted\nnew content, and so on).\nMost of the time, we prefer to synchronize data between microservices through\nevents rather than synchronous API calls at run time. This helps prevent cas-\ncade failures when one microservice experiences an outage in production.\nInstead, microservices subscribe to the relevant Kinesis Data Stream and copy\nneeded data from the appropriate events.\nThis diagram (https://d2qt42rcwzspd6.cloudfront.net/overall.png) shows a birds-eye\nview of this new architecture. Don’t worry about making sense of everything in the fig-\nure. It merely demonstrates the fact that you can build even complex systems using\nserverless components.\n It’s worth mentioning that the move to serverless was not one of our goals. The goal\nwas to deliver a better user experience with less downtime, more responsiveness, and\nmore scalability. Serverless technologies like Lambda, API Gateway, and DynamoDB\nhappen to be a great way to achieve our goals while also making our lives a lot easier and\nallowing us to ship features faster.\n",
      "content_length": 2597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "62\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\n4.2.1\nRearchitecting and rewriting \nTo fully realize our goals, we had to rearchitect and rewrite large parts of the system.\nBut we didn’t want to migrate everything to serverless for the sake of migrating them.\nWe wanted to accelerate feature development and deliver value to our users faster\nthan before. For this, we took a pragmatic approach, whereby we made a case-by-case\ndecision on whether to rearchitect a feature when we needed to work on it. \n To mitigate our risks, we rearchitected and migrated features that had the least\nbusiness impact first. Business critical features such as timelines (which is the first\nthing you see in the app) were tackled only when we had gained sufficient confidence\nand know-how. This approach of migrating a large system piece-by-piece is commonly\nreferred to as the strangler pattern.\n In the Yubl app, you could search other users by first name, last name, and user-\nname. This was a simple feature, but it caused crippling performance issues with the\nmonolith as the number of users grew. This was because a search was implemented\nwith regex queries against MongoDB. The old implementation also didn’t allow for\nmore sophisticated ranking, so users often couldn’t find who they were looking for.\nThere was a push from the marketing team to surface influencers further up the\nsearch results as many users had followed these influencers onto the platform.\n This was the first feature that we rearchitected and migrated to serverless because\nit was both low-risk and could have a high impact. Let’s dive into how we extracted the\nsearch feature out of the monolith and built a microservice around it with its own\nREST API.\n4.2.2\nThe new search API\nOne of the first and most important steps was to ensure that our legacy monolith\nwould publish its state changes to Kinesis Data Streams. This gave us a foundation to\nbuild the new microservices by building on top of these events. To extract the search\ncapability out of the monolith, we created a new search microservice. Figure 4.2 shows\nthe high-level architecture of this search microservice.\nEC2\nKinesis Data Stream\nLambda\nCloudSearch\nLambda\nAPI Gateway\nRoute53\nsearch.yubl.com\nThe new search microservice\n1\n2\n3\n4\n5\n6\nFigure 4.2\nA high-level overview of the new architecture running on \nserverless components\n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "63\nThe new serverless Yubl architecture\nIf you follow the numbered arrows in figure 4.2, this is how all the pieces fit together:\n1.\nThe legacy monolith publishes all user-related events to a Kinesis Data Stream\ncalled users. These include the user-created and user-profile-updated events\nthat tell us when a new user joins or a user has updated their profile.\n2.\nA Lambda function subscribes to the users stream.\n3.\nThe Lambda function uses these events to insert, update, or delete user docu-\nments in the users index in Amazon CloudSearch.\n4.\nA new API in API Gateway with a POST /?query={string} endpoint proxies to\nanother Lambda function to handle the HTTP request.\n5.\nThe Lambda function translates a user’s query string into a search request\nagainst the users index in Amazon CloudSearch.\n6.\nTo create a user-friendly subdomain for the new REST API, a custom domain\nname in API Gateway for search.yubl.com is registered in Route53.\nFor this microservice, we chose Amazon CloudSearch instead of Amazon Elastic-\nSearch because, at the time, Amazon ElasticSearch didn’t allow you to change the\nnumber of write nodes in an ElasticSearch cluster, which is a scalability concern for\nthe write throughput. But Amazon CloudSearch was not without its problems. \n Although you can autoscale the read and write nodes independently, scaling up a\nCloudSearch cluster takes as long as 30 minutes. This did not match well with our\nspiky workload, and we had to overprovision the read cluster as a result. If I imple-\nmented this service again today, I would definitely use Amazon ElasticSearch or a\nthird-party service such as Algolia (https://algolia.com) instead.\n Before we launched the new service, we also needed to ensure all existing user\ndata was available in the CloudSearch index. To do this, we ran a one-off task to copy\nall existing user data (~800,000 users) from MongoDB to CloudSearch, while tracking\nthe most recent user profile update. Only after this was complete, did we enable the\nfunction at step 2 (figure 4.2) to start processing user updates. \n Another important detail to note here is that when we enabled the function’s Kine-\nsis subscription, we processed events from when the one-off task started. With Kinesis,\nyou are able to specify the StartingPosition of the subscription. You can configure\nthis to AT_TIMESTAMP to start processing events from a specific timestamp. Processing\nevents from when a one-off task started ensured that we didn’t miss any updates that\nhappened while Yubl was running.\n Once live, performance of the new search service was significantly improved over\nthe old search. It also removed a lot of the load on the monolith MongoDB database\nin the process, which had a positive impact on the general responsiveness of the app.\nIt also gave us a template on how to build other microservices using serverless technol-\nogies such as API Gateway and Lambda.\n",
      "content_length": 2900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "64\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\n4.3\nMigrating to new microservices gracefully\nBuilding the new microservices was the easy part. The difficult part was how to\nmigrate them safely and be able to roll back quickly if there were any unforeseen\nissues. Another concern was how to do it gracefully without downtime and impact on\nour users. Suppose your starting position is a monolith where all the features are\naccessing directly a shared database (figure 4.3). Where will you begin?\nWe started to break apart this monolith into microservices built with serverless compo-\nnents such as API Gateway, Lambda, and DynamoDB. As we moved a feature out of\nthe monolith into its own microservice, we wanted the microservice to be the author-\nity over some part of the system, be it user profiles or product catalogue or customer\norders. The microservice has its own database, and other microservices (or the mono-\nlith) should not be able to reach into its database and access or manipulate data\ndirectly.\n Instead, to instigate some change in state, other microservices need to communi-\ncate with this microservice through its API. This can be HTTP-based in the form of a\nREST API call or message-based in the form of publishing an event/message to a\nqueue. The important thing is to cut off direct access to and manipulation of data that\nthe microservice is supposed to be the authority of (figure 4.4). How do you do this\ngracefully without causing significant disruption to your users?\n The challenge here is that it’s risky to do a big-bang migration because it usually\nrequires downtime. That is not to say that you should never entertain the idea of a big-\nbang migration. If you’re a small startup and have few users on your current platform,\nthen a big-bang migration with downtime is quite possibly the fastest and most effi-\ncient approach for you. But for many organizations that are undergoing such migra-\ntion, it’s important to minimize the risk and disruption caused by moving to a new\nmicroservice.\n \n \n \n \nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature E\nFeature F\nFigure 4.3\nA monolithic system where \neverything has direct access to a shared \ndatabase\n",
      "content_length": 2210,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "65\nMigrating to new microservices gracefully\nA common strategy is to perform the migration in multiple steps to maximize safety.\nFor example, the following process describes some likely steps as figure 4.5 illustrates:\n1.\nMove the business logic for a particular feature into a separate service and cre-\nate its own API. The new service will still use the monolith database until it has\nauthority over the data.\n2.\nFind the places where the monolith accesses this feature’s data directly and\nredirect those access points to go through the new service’s API instead. Start\nwith the least critical component first to minimize the blast radius of any\nunforeseen problems or impacts.\n3.\nMove all other direct access points to the new service’s data to go through its\nAPI (probably, one at a time).\n4.\nNow that the new service is the authority over its data, you can plan a course to\nmigrate the data out of the monolith database into its own database. You might\nuse a different database, based on your requirements for this new service. If\nyour access pattern is simple and mostly key lookups, then DynamoDB is proba-\nbly a good choice.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\nFigure 4.4\nA monolithic system where everything has direct access to a shared database\n",
      "content_length": 1340,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "66\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\n3\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\n1\nService\nFeature E\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\n2\nMigrate the least\ncritical component first.\nService\nFeature E\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\n4\nFigure 4.5\nGradually cut off the direct access to the shared monolith database by moving \naccess to go through the new microservice’s API instead.\n",
      "content_length": 618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "67\nMigrating to new microservices gracefully\n5.\nOnce you have created the new database, you need to migrate data from the\nmonolith database. To do so without downtime, you can treat the new database\nas a read-through and write-through cache: any updates and inserts are written\nto the monolith database and then copied to the new database (figure 4.6). \nWhen attempting to read, you will read from the new database first. If the\ndata is not found, then read from the monolith database and save the data in\nthe new database.\n6.\nRun a one-off task in the background to copy over all existing data (figure 4.6).\nTake care to ensure that you don’t overwrite newer updates. (With DynamoDB,\nhttps://amzn.to/2IbE818, this can be done using conditional writes.)\nThis is a useful pattern for extracting features from a monolith and moving them into\nmicroservices that can scale and fail independently. There is more you can do to ensure\nthat you do so safely and gracefully to minimize the potential impact on your users. For\nexample, you can route only a small percentage of traffic to the new microservice when\nit first goes live. This limits the blast radius of any unforeseen problems with the new\nmicroservice. It is especially important for microservices that are user-facing and that\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\n5\nTreat new DB as read-through/\nwrite-through cache.\nread\nwrite\nMonolith DB\nMonolith\nFeature A\nFeature B\nFeature C\nFeature D\nFeature F\nService\nFeature E\nDB\n6\nAlso run one-off migration \njob in the background.\nRead\nWrite\nMigration\nFigure 4.6\nMigrate data to the new database gradually, without downtime.\n",
      "content_length": 1677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "68\nCHAPTER 4\nYubl: Architecture highlights, lessons learned\nhandle requests from the mobile/web client directly because they can have a big impact\non user experience. \n This approach is commonly known as the canary pattern and is not limited to sys-\ntem migrations. The term canary deployment refers to a deployment strategy where the\npattern is used for every deployment when a small percentage of traffic is directed at\nthe new version of the application, which limits the blast radius of any unforeseen\nproblems. If you’re using the Application Load Balancer (ALB) in front of your appli-\ncation, then you can configure this routing behavior there (figure 4.7). \nWhere this approach is not possible (or in the case of Yubl where ALB didn’t exist at\nthe time), you can also proxy requests from the monolith for a configurable percent-\nage of requests. Figure 4.8 shows this approach.\nSummary\nYou need to re-architect most applications to reap the full benefit of a serverless\narchitecture. Although there are solutions to lift and shift existing applications\ninto serverless, these don’t deliver optimal performance and scalability.\nTo get the full benefit of a serverless architecture, you need small, autonomous\nteams who are capable of making their own architectural decisions. Developers\nshould be responsible for more than just the code and empowered to own their\nsystem. As Amazon’s motto goes, “You build it, you run it.” \n90%\n10%\nMonolith DB\nMonolith\nFeature E\nFeature F\nService\nFeature E\nDB\nALB\nFigure 4.7\nYou can use the Application Load Balancer (ALB) to distribute traffic between \nthe monolith and the new microservices. This allows you to minimize impact of unforeseen \nproblems to a subset of users.\n10%\nMonolith DB\nMonolith\nFeature E\nFeature F\nService\nFeature E\nDB\nFigure 4.8\nEven without ALBs, you can still proxy requests by modifying the monolith.\n",
      "content_length": 1871,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "69\nSummary\nDevOps is simpler with serverless. You get a lot of automation out of the box,\nand tools such as Serverless Framework takes care of the rest. You still need to\nknow what metrics to pay attention to and what alerts to add, however, as opera-\ntional experience of running a production system is still valuable.\nUnit tests have a low return on investment when it comes to serverless architec-\ntures. Most functions are simple and often integrate with other services such as\nAmazon’s DynamoDB and Simple Queuing Service (SQS). Unit tests that mock\nthese integration points do not test those service interactions and give you a\nfalse sense of security.\nPrefer integration tests that exercise the real AWS services for the happy paths\nand use mocks only for failure cases that are difficult to simulate otherwise. For\nexample, execute the function code locally but have it talk to the real DynamoDB\ntables. Then use mocks when you need to test your error handling for\nDynamoDB’s throughput exceeded errors.\nServices often have to call each other in a microservices architecture. For inter-\nnal APIs that are more prone to breaking in the development environments\n(compared to AWS services), use mocks to isolate the failures. The last thing\nyou want is for an error in one service to fail the tests for all other services that\ndepend on it.\nSimulating AWS services (for example, DynamoDB, SNS, SQS) locally is not\nworth the effort. It’s easier and quicker to deploy a temporary stack, than using\nlocal simulation tools.\nWhen dealing with batched event sources like Kinesis and SQS, you need to\nthink about how to handle partial failures. You either have to make sure that\nthe operations are idempotent and can be retried without problem, or you\nneed to ensure that successfully processed items in a failed batch are not pro-\ncessed again when the batch is retried.\n",
      "content_length": 1876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "70\nA Cloud Guru:\n Architecture highlights,\n lessons learned\nIn the first edition of this book, we described a serverless LMS (Learning Manage-\nment System) built by A Cloud Guru (https://acloudguru.com). At that time, A\nCloud Guru built a RESTful API backend using Amazon API Gateway, AWS\nLambda, and Google’s Firebase as its primary database. Since we published our first\nedition, A Cloud Guru has gone through a major transformation. The company\nmoved from a RESTful monolithic design to a GraphQL-driven microservices\narchitecture. This chapter describes this journey. We’ll look at the original RESTful\ndesign, the transition to microservices, how GraphQL plays a major part, and the\nlessons learned along the way. \nThis chapter covers\nA Cloud Guru’s original REST architecture\nThe reasons the team decided to migrate from \nREST to microservices and GraphQL\nLessons learned through the migration\n",
      "content_length": 903,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "71\nThe original architecture\n One thing is clear though, serverless technologies allowed A Cloud Guru to re-\narchitect their platform rapidly and with minimal fuss. As a developer, you can be\nmore agile with a serverless application than with a traditional three-tier behemoth.\nThis is because, in a serverless approach, your primary focus is on the architecture of\nthe system, your data, and the code. A Cloud Guru developers didn’t need to spend\ntime and energy worrying about provisioning servers, updating server software, or\nmanaging Kubernetes clusters. That alone saved them time and gave them the oppor-\ntunity to focus on the platform elements that were critical to the business.\n5.1\nThe original architecture\nA Cloud Guru is an online educational platform for anyone wanting to learn Amazon\nWeb Services (AWS), Microsoft Azure, and Google Cloud Platform, as well as Cloud-\nrelated technologies. The core features of the platform include the following: \nOn-demand video courses \nPractice exams and quizzes\nA real-time discussion forum\nDashboards and reporting\nUser profiles and gamification\nEducational features like learning paths\nInteractive sandbox environments for students wanting to test their skills\nA Cloud Guru is also an ecommerce platform that allows students to pay for a monthly\nor yearly subscription and have access to content and features. The training architects\nwho create courses for A Cloud Guru can upload videos directly to S3. These videos\nare immediately transcoded to a variety of formats and resolutions (1080p, 720p, HLS,\nand so on).\n In 2017–2018, the A Cloud Guru platform used Firebase as its primary database. A\nnice feature of this database is that it allows client devices (the browser on your com-\nputer or phone) to receive updates in near real time without refreshing or polling.\n(Firebase uses web sockets to push updates to all connected devices at the same time.)\nThe other main components were API Gateway and AWS Lambda. Figure 5.1 shows a\nbasic high-level view of how that initial REST architecture looked. \n In the first edition of this book, we described how to build a serverless system with\na RESTful interface. We wanted to illustrate the fact that you can create sophisticated,\nscalable, and highly available platforms using functions and services provided by AWS\nand Google Cloud Platform. The A Cloud Guru team was able to do that and go far\nbeyond. They built a system that would go on to serve tens of thousands of concurrent\nusers. Figure 5.2 shows a slightly more advanced version of the same architecture as\nwas presented in the first edition of this book.\n \n \n \n",
      "content_length": 2634,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "72\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nAPI\nGateway\nLambda\nfunction\nFirebase\nClient Web Browser connects \nto Firebase directly.\nThere are multiple Lambda \nfunctions connected to the \nAPI Gateway.\nFigure 5.1\nA basic high-level view of the initial \nA Cloud Guru architecture. Their system was \nmore complex (there were many more Lambda \nfunctions), but in a nutshell, this is how it worked.\nStudents are given \npermission to read files \nfrom S3 via CloudFront.\nLecturers are given \npermission to upload \nto S3.\nAPI\nGateway\nAuth0\n(auth)\nFirebase\n(database)\nS3 \n(file storage)\nLambda\n(transcode\nstart)\nS3 \n(file storage)\nLambda\n(transcode\nfinish)\nFirebase\nFirebase\nMedia transcoding pipeline\nFirebase\nS3 \n(file storage)\nCloudFront\nLambda\n(forum\nanswer)\nLambda\n(answer \nsubmit)\nLambda\n(read file)\nLambda\n(upload file)\nFigure 5.2\nThis is a slightly more advanced version of the A Cloud Guru architecture. The actual \nproduction architecture had Lambda functions and services for performing payments, managing \nadministration, gaming, reporting, and analytics.\n",
      "content_length": 1082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "73\nThe original architecture\nThe original system worked well and scaled as the development team expected. It was\nalso inexpensive to run with the AWS bill being just a few thousand dollars (the\nLambda and the API Gateway bill was under $1,000). Note the following about the\noriginal A Cloud Guru architecture (figure 5.2):\nThe frontend was built using AngularJS and was hosted by Netlify (https://\nnetlify.com).\nAuth0 was used to provide registration and authentication. It creates delegation\ntokens that allow an AngularJS website to directly and securely communicate\nwith other services such as Firebase.\nEvery client created a connection to Firebase using web sockets and received\nupdates from it in near real time. This meant that clients received updates as\nthey happened without having to poll (which led to a nicer user experience). \nThe training architects who created content for the platform uploaded files\n(usually videos) straight to an S3 bucket via their browser. \nFor this to work, the web application invoked a Lambda function to first\nrequest the necessary upload credentials. As soon as the credentials were\nretrieved, the client web application uploaded the file to S3 via HTTP. All of this\nhappened behind the scenes and was invisible to the training architects.\nOnce a file was uploaded to S3, the system automatically kicked off a chain of\nevents that transcoded the videos, saved the new files in another bucket,\nupdated the database, and immediately made the transcoded videos available to\nother users.\nTo view the videos, students were given permission by another Lambda func-\ntion. Permissions were valid for 24 hours, after which they were to be renewed. \nFiles were accessed via CloudFront. CloudFront ensures that users have low-\nlatency access to videos wherever they may be.\nOver time, the A Cloud Guru development team began considering the future of\ntheir serverless REST architecture. The company wanted to further accelerate the\ndevelopment of the platform, reduce blockers, and allow independent teams to focus\non different high-value features. The following were some of the considerations that\ndrove the decision to change the architecture:\nThe existing architecture that was created was, in a sense, a serverless monolith.\nThere were a large number of Lambda functions, but they connected to the\nsame Firebase database. Making a change to the database would affect nearly\nevery Lambda function and the developers working on them. This made it easy\nin the existing system for developers to step on each other’s toes.\nThe business wanted to have separate development teams owning different\nparts of the product. For example, the student-experience team would need to\nbe able to update a database and deploy a Lambda function without affecting\nthe team responsible for billing and reporting. \n",
      "content_length": 2838,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "74\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nTransitioning to a true microservices approach (where each microservice owns\nits data and its own view of the world) would allow teams to develop the plat-\nform in parallel. Each development team would look after a number of micros-\nervices and iterate on them as needed. This would mean moving away from a\nsingle Firebase database to multiple databases and yet still provide a way to read\nand hydrate data as needed.\nMoving to a microservices approach would give the teams a greater level of iso-\nlation. This would mean that different subsystems and components within the\ncode base would have clearer boundaries in terms of ownership and a looser\ncoupling. \nThe team wanted to find a way to minimize round trips to the backend and\nfetch only the data that was needed. Devs also wanted to be able to serve multi-\nple clients like mobile and web. While this can be accomplished with REST, the\nteam determined that GraphQL was a better fit.\nFinally, the company felt that Firebase was getting a little bit too expensive.\nGiven the platform’s access usage patterns, Amazon’s DynamoDB looked like the right\ndatabase to move to. Migrating to DynamoDB would allow teams to better manage\ninfrastructure using CloudFormation and use built-in DynamoDB features like event\ntriggers. And it would allow teams to stay entirely within the AWS environment.\n Refactoring to a proper microservices approach and moving to DynamoDB as the\nprimary database necessitated a rethink of the entire architecture. One of the main\nquestions to consider was how to get data from disparate microservices and do it as\neffectively as possible (without multiple round trips or data hydration on the client)\nwhen a user made a request. This is where GraphQL entered the picture and became\nthe focus of the new architecture. But before we get to GraphQL, let’s see how the A\nCloud Guru team split up their monolith and created their microservices first.\nA serverless monolith\nThe RESTful API design that A Cloud Guru originally created was a serverless mono-\nlith. There was a single database and functions that needed to save or load data con-\nnected to it. There is nothing wrong with building a serverless monolith. For A Cloud\nGuru, it scaled well for a long time and helped build the company. The core reason\nfor the move to a microservices design was the need for multiple teams to work in\nparallel. \nIf you are starting out today, know that it is OK to go with a monolithic approach.\nWhen you need to, you can migrate to microservices. Remember, you don’t have to\nfollow the trend and do the microservices road if it isn’t right for you.\n",
      "content_length": 2685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "75\nThe original architecture\n5.1.1\nThe journey to 43 microservices\nLet’s take a look at the serverless microservices approach the company came up with.\nFirst, here are some stats of the new GraphQL re-architected A Cloud Guru platform\nat the start of 2020:\n240 million Lambda invocations per month (100 per second)\n180 million API Gateway calls per month (70 per second)\n90 TB of data transferred from CloudFront per month (274 MB per second)\nThe team began to break apart the monolith and move to a microservices architec-\nture during 2018. API Gateway and Lambdas were separated into discrete microser-\nvices, each with their own responsibilities and view of the world. In the new world of\nmicroservices, each service could be as simple as a single DynamoDB table, a couple\nof Lambda functions, and an API Gateway. Figure 5.3 shows an example of how a cou-\nple of basic microservices could look.\nThe packaging of the microservices is also interesting to note. A Cloud Guru uses\nServerless Framework and CloudFormation to organize and deploy microservices.\nSome services in a microservice are stateful, whereas others are stateless. A Lambda\nTwo basic examples of how simple microservices could be \nstructured with Lambda, API Gateway, DynamoDB, and S3.\nLambda\nfunction\nLambda\nfunction\nAPI Gateway\nLambda\nfunction\nDynamoDB\nDynamoDB\nS3 bucket\nLambda\nfunction\nAPI Gateway\nLambda\nfunction\nSample Microservice #1\nSample Microservice #2\nFigure 5.3\nThe two microservices here are akin to the simplified RESTful architecture we \ndiscussed before.\n",
      "content_length": 1544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "76\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nfunction is stateless, meaning that it can be overwritten on each deployment. It’s\nalways ephemeral. A DynamoDB table or an S3 bucket is stateful; you must be careful\nto preserve the data that is already there. Your deployment process cannot overwrite\nit. Also, there can be global services and resources that don’t belong to any specific\nmicroservice. How do you think they should be deployed and managed?\n The A Cloud Guru team designed their microservice so they would have different\nCloudFormation stacks for stateless and stateful resources, as well as a stack for config-\nuration and core dependencies. Figure 5.4 shows what that looks like. \nThis approach to different CloudFormation stacks for different kinds of resources\nallows the development team to deploy the stack with the stateless resources when they\nneed to be updated without having to touch stateful resources. The same goes for the\nconfiguration and the core-dependencies stack. They can be updated without modify-\ning anything else within the microservices. This kind of separation of concerns is\nadvantageous because it can help to avoid accidental modification of stateful resources.\n There are also a few other global dependencies that exist as well, but these are not\nwithin any microservice. They include infrastructure components such as Amazon\nRedShift (data warehouse), AWS WAF (firewall), and VPCs (virtual private cloud).\nMicroservices were designed to avoid having a hard dependency on these global\nresources. In fact, there is quite a loose coupling between them. \n For example, if a microservice needs to push data into RedShift, it doesn’t do it\ndirectly. Instead, a regular ETL job pulls data out of microservices and writes it to Red-\nShift. That means microservices don’t have to know about RedShift. A microservice\ncan live and breathe on its own while a separate ETL task does its own job. Figure 5.5\nshows that it’s necessary for some resources to live outside specific microservices.\nStateless \nresources\nStateful \nresources\nConfiguration \nand core \ndependencies \nMicroservice 1\nMicroservice 2\nMicroservice 3\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nFigure 5.4\nEach microservice is in its own CloudFormation stack, which makes it easy to deploy.\n",
      "content_length": 2743,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "77\nThe original architecture\nThe A Cloud Guru team gradually teased apart the serverless monolith that was cre-\nated in the first place and re-implemented it with microservices. Moving from one\narchitecture to a another always takes time but a nice advantage here was that the\nteam could primarily focus on code. There was no hardware, servers, or a container\norchestration engine (like Kubernetes) to worry about. The team carefully and gradu-\nally reimplemented various components making sure that no users were affected\nduring the change.\n As part of the move to microservices, GraphQL became the solution to the ques-\ntion of how to pull the right data from different microservices when a client makes a\nrequest. After all, each microservice may have its own database and its own view of the\nworld. When a user needs to get data, how does it all happen? Which microservice has\nasked for it? And, what if multiple microservices have the required information, and\nthe client needs an aggregate response? GraphQL became the tool to query microser-\nvices and, with schema-stitching, create responses needed for the clients.\n5.1.2\nWhat is GraphQL\nWe already mentioned GraphQL in chapter 3, but let’s do a quick recap about what it\nis. GraphQL is a popular data query language developed by Facebook in 2012 and\nThese resources exist outside \nof any individual microservice. \nAmazon \nRedshift\nAWS WAF\nAmazon VPC\nMicroservice 1\nMicroservice 2\nMicroservice 3\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nCloudFormation stack \nCloudFormation stack \nCloudFormation stack \nAmazon API \nGateway\nAWS Lambda\nAmazon \nDynamoDB\nAmazon S3\nParameter \nstore\nCustom \nresources\nFigure 5.5\nGlobal dependencies reside outside of each individual microservice. Not everything has to exist \nwithin a microservice.\n",
      "content_length": 2062,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "78\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nreleased publicly in 2015. It was designed as an alternative to REST because of REST’s\nperceived weaknesses (multiple round-trips, over-fetching, and problems with version-\ning). GraphQL attempted to solve these problems by providing a hierarchical, declar-\native way of performing queries from a single endpoint (for example, api/graphql).\nFigure 5.6 provides an illustration of how this looks.\nGraphQL gives power to the client. Instead of specifying the structure of the response\non the server, it’s defined on the client. The client can specify which properties and\nrelationships to return. GraphQL aggregates data from multiple sources and returns it\nto the client in a single round trip, which makes it an efficient system for retrieving data. \n According to Facebook, GraphQL serves millions of requests per second from\nnearly 1,000 different versions of its application. To further illustrate what GraphQL\nlooks like, here’s a simple query taken from https://graphql.org/learn/queries/:\n{\n  hero {\n    name\n  }\n}\nAnd one possible response to that query:\n{\n  \"data\": {\n    \"hero\": {\n      \"name\": \"R2-D2\"\n    }\n  }\n}\nGraphQL allows you to query multiple \ndatabases from a single endpoint.  \nAPI Gateway/\nGraphQL\nLambda\n(GraphQL)\nDatabase\nDatabase\nDatabase\nDatabase\nDatabase\nGraphQL\nFigure 5.6\nA GraphQL library running in a Lambda function can query multiple databases and, \nusing schema stitching, produce a result relevant for each individual client.\n",
      "content_length": 1528,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "79\nThe original architecture\nIn a serverless architecture, GraphQL can be run from a single Lambda function con-\nnected to the API Gateway (this is what A Cloud Guru did) or used through a service\nlike AWS AppSync. GraphQL can query and write to multiple data sources such as\nDynamoDB tables and, using schema-stitching, assemble a response that matches the\nrequest.\n5.1.3\nMoving to GraphQL\nWhen A Cloud Guru began moving to GraphQL, AWS AppSync wasn’t yet released or\neven announced for that matter. The team had one option, which was to run GraphQL\nfrom a Lambda function using the Apollo GraphQL library (https://www.apollo\ngraphql.com). \n Initially, getting the Apollo GraphQL implementation to work in a Lambda func-\ntion presented a few interesting challenges. Apollo was originally designed for long-\nrunning processes on servers and containers. The team had to make a certain number\nof tweaks to optimize it for Lambda.\n The A Cloud Guru team began using GraphQL and a design pattern called Back-\nends for Frontends (BFF). The idea behind BFF is that each client has its own API or\nendpoint that services its specific needs (for example, there’s a dedicated endpoint\nfor mobile and another for the web). Each of the endpoints can query the appropri-\nate microservices to save or load data as needed. The client doesn’t need to know\nabout the different microservices. It only needs to know which endpoint to query.\nThis pattern solves the decoupling issue present in many systems. Figure 5.7 shows an\nexample of the BFF architecture and what the A Cloud Guru team is driving toward.\nSeparate endpoints for different\nclients like mobile or web\nAPI Gateway/\nGraphQL\nWeb app\n(GraphQL)\nMicroservice 1\n(Payments)\nMobile\n(GraphQL)\nInternal\ndashboards\n(GraphQL)\nDifferent endpoints can query different\nmicroservices and databases to get the\ndata needed for the given client.\nMicroservice 3\n(Reporting)\nMicroservice 2\n(Video)\nBFF\nFigure 5.7\nAn example of the \nBFF pattern that can be applied to \nmicroservices and multiple clients\n",
      "content_length": 2030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "80\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\nThroughout the chapter, we’ve called the Lambda function that contains the\nGraphQL JavaScript library a GraphQL endpoint. But it’s probably better to call it a\nBFF endpoint as we now understand this pattern. Here’s how the A Cloud Guru’s imple-\nmentation works at a high level (excluding a few details like service discovery):\nA request from the user reaches the API Gateway.\nThe API Gateway invokes a Lambda function with the Apollo GraphQL library.\nThis is the BFF endpoint we’ve discussed.\nThe Apollo GraphQL library queries the microservices it knows about (more on\nhow it knows about which microservices to target in the service discovery section).\nEach microservice has an endpoint that is an API Gateway with a Lambda func-\ntion (there is a /graphql endpoint in each microservice). \nThe Lambda function runs the GraphQL library with a number of thin schema\nresolvers. It queries the databases contained within the microservice and pro-\nduces a result, which is sent back to the BFF endpoint.\nThe BFF endpoint receives responses from the different microservices it que-\nried. Using schema stitching, it assembles the final response.\nThis final response is sent back to the client via the API Gateway.\nThe GraphQL Lambda function is aware of the multitude of microservices (more on\nthis in a moment) and is able to query those when it receives a client request. Figure\n5.8 shows a high-level overview of this architecture.\n5.1.4\nService discovery\nWith 43 microservices in the system, how does GraphQL know which services to query\nwhen a client request comes in? The A Cloud Guru team built an internal service-\ndiscovery service called Sputnik (note, this is an in-house, proprietary service that’s\n4. Responses are combined together\n    using schema stitching. \nAPI Gateway\nLambda\n(GraphQL)\nMicroservice 3\nMicroservice 2\nMicroservice 1\nGraphQL\n5. Final response is sent \n    back to the client.\n1. User makes a request.\n3. GraphQL queries the \nmicroservices it knows about.\n2. GraphQL function/\n    library is invoked.\nFigure 5.8\nThe GraphQL endpoint serves as the central point for clients that need data.\n",
      "content_length": 2188,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "81\nThe original architecture\nnot available publicly). Sputnik consists of a database with API/URI definitions and\ndatabase schemas. The microservices know when to update Sputnik. Additionally, the\nGraphQL Lambda function knows when to query Sputnik to get schemas for each\nmicroservice and figure out where to route requests. \nDEFINITION\nService discovery is a standard technique in microservices archi-\ntecture, which solves the problem of knowing what services are available, how\nto access them, and what their interface looks like.\nSputnik is made up of Lambda functions and DynamoDB tables that contain schemas\nand URIs of different microservices. It is really a microservice that facilitates the com-\nmunication of BFF with other microservices in the system. Figure 5.9 shows how Sput-\nnik helps the BFF endpoint know where to make a query.\nTIP\nAWS has a service called Cloud Map, which is AWS’ own service discovery\nproduct. It even has a tagline that simply says, “Service discovery for cloud\nresources.” If you are looking for something like Sputnik, check out Cloud\nMap. It may work for you. You can find Cloud Map at https://aws.amazon\n.com/cloud-map/. \nThe metadata about each microservice (URI and schema) is cached at the BFF endpoint\ntoo, negating the need for the function to query Sputnik on every request. However,\nSputnik can invalidate the cache and force the Lambda function to requery it again. \nMicroservices update \nSputnik whenever their \nschema or URI changes. \nAPI Gateway\nLambda\n(GraphQL)\nMicroservice 3\nMicroservice 2\nMicroservice 1\nService discovery\nFunction queries Sputnik \nto find out URIs of all \nmicroservices and their \nschemas. \nService \ndiscovery\n(Sputnik)\nFigure 5.9\nThe service discovery (Sputnik) mechanism for A Cloud Guru. There’s an AWS service \ncalled Cloud Map that you may want to check out if you are looking for something similar.\n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "82\nCHAPTER 5\nA Cloud Guru: Architecture highlights, lessons learned\n5.1.5\nSecurity in the BFF world\nA Cloud Guru practices security in depth with multiple layers of security built into the\nsystem. Let’s talk about the two main components: user authentication/authorization\nand BFF to microservice security. \n In the A Cloud Guru platform, students are authenticated using the Auth0 service\nthat generates a unique JWT token for each user. All requests to AWS are made with\nthat JWT token, which is validated using a custom authorizer at the API Gateway. If\nthe JWT token is valid, the request is allowed to continue to the BFF endpoint. If it’s\nnot, then a response is generated and sent back to the client telling it that it is unau-\nthorized. This is a simple mechanism, which was also used in the original REST design\nof the platform.\n The second interesting element is how to authenticate a request made by the BFF\nto the microservice. In this scenario, A Cloud Guru uses API keys to authenticate\nrequests. Each microservice has a unique API key that the BFF includes in its request\nin the header (using the X-API-Key parameter). Microservices check the included key\nand authorize requests if everything is OK.\n5.2\nRemnants of the legacy\nThe migration from REST to GraphQL took some time because the teams were care-\nful not to cause issues for users. An interesting side effect of this was the way the sys-\ntem looked midway through the re-architecture. The team implemented new\nmicroservices and a BFF, but the old Firebase database was still in use because it was\npowering some of the elements of the user interface on the A Cloud Guru website.\nFigure 5.10 shows how that mid-way architecture looked.\nFigure 5.10\nA high-level overview of the A Cloud Guru architecture as it was going through a transition\nMicroservices #A\nMicroservices #B\nAPI Gateway\nGraphQL endpoint\nDynamoDB\nDynamoDB\nCloudFront\nS3\nClient\nFirebase\nAlgolia\nThese are two non-AWS services \nat this stage in the architecture.\n",
      "content_length": 1998,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "83\nSummary\nAn interesting note about figure 5.10: you can see that Firebase is still used to drive\nsome of the client-facing user interface elements. To keep the data in Firebase up to\ndate, the team used DynamoDB event streams and Lambdas to ensure that happened.\nWhen a table is updated in any microservice, that change is pushed by DynamoDB to\nthe DynamoDB event stream, which in turn invokes a Lambda function. That Lambda\nfunction analyses the change and then updates Firebase (and any other services like\nAlgolia as needed). \n Now, Firebase becomes basically a materialized view that is used to drive some parts\nof the interface. It is never directly queried, but it is there for older components that\ndepend on it. This is one of the creative decisions made by the team as they transi-\ntioned from the old serverless architecture to the new one. They were able to use Fire-\nbase while they introduced DynamoDB and microservices and move everything\nacross. \n There’s also an important lesson here in migration. You can gradually implement a\nnew architecture while keeping the old one going by splitting things into smaller\npieces and moving them one by one.\nSummary\nTeams can work on a platform without affecting each other. Different teams are\nresponsible for different microservices, and they can work on those without\naffecting anyone else.\nThere has been a substantial improvement in performance for A Cloud Guru.\nFor example, a BFF pattern fetches only the data that’s needed (great for\nmobile) and needs only one roundtrip to make that happen. This is an optimi-\nzation on what was there previously.\nThe BFF pattern helps to support multiple client types and devices. These can\nbe different depending on the requirements of the client.\nThe team also had to do additional re-engineering to make Apollo work well\nwith Lambda. These days, it shouldn’t be much of a problem, but that’s the\npain when you are an early adopter.\nAs always, security is a number one concern. More microservices create a larger\nsurface area for attacks. It’s therefore critical that microservices and endpoints\nare secured. The use of machine keys to secure communications between back-\nend components is an example of one good practice you should know about.\n",
      "content_length": 2251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "84\nYle: Architecture\n highlights, lessons learned\nYle is the national broadcaster for Finland and operates their own popular stream-\ning service called Yle Areena, which is used by millions of households. For a num-\nber of years now, Yle has used serverless technologies at scale in their architecture.\nThey use a combination of AWS Fargate (https://aws.amazon.com/fargate),\nLambda, and Kinesis to process more than 500 million user-interaction events per\nday. These events feed Yle’s machine learning (ML) algorithm and help them pro-\nvide better content recommendations, image personalization, smart notifications,\nand more.1\nThis chapter covers\nYle’s big data architecture \nScalability and resilience, lessons learned\n1 I want to take this opportunity to thank Anahit Pogosova for sharing details of this architecture and the les-\nsons she and her team learned along the way.\n",
      "content_length": 881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "85\nIngesting events at scale with Fargate\n6.1\nIngesting events at scale with Fargate\nTo provide better content recommendations, Yle needs to know which content the vis-\nitors interact with the most. Yle ingests user-interaction data from streaming services\nas well as mobile and TV apps via an HTTP API. The challenge with this API is that the\ntraffic can be spiky, such as during live sporting events. And sometimes events overlap\n(for example, when the election results coverage was on at the same time as a hockey\ngame, which is the most popular sport in Finland)!\n As mentioned, Yle’s API ingests more than 500 million user-interaction events per\nday with more than 600,000 requests per minute during peak time. Live sporting events\nor special events (such as the election results) can cause peak traffic to go even higher.\nThe maximum traffic throughput they have observed is 2,500,000 requests per minute.\n Because the traffic is so spiky, the Yle team decided to use Fargate instead of AWS’s\nAPI Gateway and Lambda. Fargate, also an AWS service, lets you run containers with-\nout having to worry about underlying virtual machines. It’s part of an emerging trend\nfor serverless containers, where you use containers as a utility service.\n6.1.1\nCost considerations\nIn general, AWS services that charge you based on up time tend to be orders of magni-\ntude cheaper when running at scale, compared with those that charge based on\nrequest count. With API Gateway and Lambda, you pay for individual API requests.\nFargate, on the other hand, charges a per-hour amount based on the vCPU, memory,\nand storage resources that your containers use. You incur costs for as long as the con-\ntainers run, even if they don’t serve any user traffic.\n Paying for up time can be inefficient for APIs that don’t receive a lot of requests.\nFor example, an API that receives a few thousand requests a day would cost signifi-\ncantly less using API Gateway and Lambda. This is especially true when you consider\nthat you need some redundancy to ensure that your API stays up and running even if\na container fails or if one of the AWS availability zones (AZs) hosting your containers\nexperiences an outage. However, for high throughput APIs like the Yle API, which\nhandles hundreds of millions of requests per day, running the API in Fargate can be\nmore economical than using API Gateway and Lambda.\n6.1.2\nPerformance considerations\nA more important consideration for the Yle team was that, given how spikey their traf-\nfic can be, they would likely run into throttling limits with API Gateway and Lambda.\nA Lambda function’s concurrency is the number of instances of that function that\nserve requests at a given time. This is known as the number of concurrent executions. \n Most AWS regions have a default limit of 1,000 concurrent executions across all\nyour Lambda functions in that region. This is a soft limit, however, and can be raised by\na support request. Even though Lambda does not impose a hard limit on the maximum\nnumber of concurrent executions, how quickly you reach the required number of con-\ncurrent executions is limited by two factors:\n",
      "content_length": 3135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "86\nCHAPTER 6\nYle: Architecture highlights, lessons learned\nThe initial burst limit, which ranges from 500 to 3,000 depending on the\nregion.\nAfter the initial burst limit, your functions’ concurrencies can increase by 500\ninstances per minute. This continues until there are enough instances to serve\nall requests or until a concurrency limit is reached.\nAPI traffic is often measured in requests per second (or RPS). It’s worth noting that\nRPS is not equivalent to Lambda’s concurrent executions. For example, if an API\nrequest takes an average of 100 ms to process, then a single instance of a Lambda\nfunction can process up to 10 requests per second. If this API needs to handle 100\nRPS at peak, then you will likely need around 10 Lambda concurrent executions at\npeak to handle this throughput.\n If, however, an API’s throughput jumps from 100 RPS to 20,000 RPS in the span of\n30 seconds, then you will likely exhaust the initial burst limit and the subsequent scal-\ning limit of 500 instances per minute. Eventually Lambda would be able to scale\nenough instances of your API functions to handle this peak load, but in the mean-\ntime, many API requests would have been throttled.\n Another caveat to consider is that because live events are scheduled ahead of time,\nthe Yle team can use a broadcast schedule to prescale their infrastructure in advance.\nThere is no easy way to do this with Lambda except for using provisioned concurrency\n(https://amzn.to/3faBkCU). But you’d need to allocate provisioned concurrency to\nevery Lambda function that needs to be prescaled; that would consume the available\nconcurrencies in the region. \n When used broadly like this, it can significantly impact your ability to absorb fur-\nther spikes in traffic because there might not be enough concurrency left in the\nregion if most of it is taken up by provision concurrency. Because of these scaling lim-\nits, AWS API Gateway and Lambda are not a\ngood fit for APIs with extremely spiky traffic.\nIt’s the main reason why the Yle team opted to\nbuild their API with Fargate, and that was a sen-\nsible decision.\n6.2\nProcessing events in real-time\nOnce Yle’s API ingested the user-interaction\nevents, it published them to Amazon Kinesis\nData Stream in batches of 500 records at a time\nwith an Amazon Simple Queue Service (SQS)\nqueue as the dead-letter queue (DLQ). Figure\n6.1 illustrates this process.\n6.2.1\nKinesis Data Streams\nAmazon’s Kinesis Data Streams is a fully man-\naged and massively scalable service that lets you\nFargate\nKinesis\nSQS\nFigure 6.1\nHigh-level architecture of \nYle’s ingestion API, which assimilates \nmore than 500 million events per day at \na peak throughput of more than 600,000 \nevents per minute. The events are \nforwarded to Kinesis Data Stream in \nbatches of 500 records. If the Kinesis \ndata stream is unavailable, the events \nare sent to an SQS dead-letter queue \n(DLQ) to be reprocessed later.\n",
      "content_length": 2908,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "87\nProcessing events in real-time\nstream data and process it in real time. Data is available to the consumers of the\nstream in milliseconds and is stored in the stream for 24 hours, by default, but that\ncan be extended to a whole year, based on your configuration. (Keep in mind that\nextra charges apply when you extend the retention period for your stream.)\n Within a Kinesis stream, the basic unit of parallelism is a shard. When you send\ndata to a Kinesis stream, the data is sent to one of its shards based on the partition key\nyou send in the request. Each shard can ingest 1 MB of data per second or up to 1,000\nrecords per second and supports an egress throughput of up to 2 MB per second. The\nmore shards a stream has, the more throughput it can handle. \n There is no upper limit to the number of shards you can have in a stream so, theo-\nretically, you can scale a Kinesis stream indefinitely by adding more shards to it. But\nthere are cost implications that you have to consider when deciding how many shards\nyou will need for your workload.\n Kinesis charges based on two core dimensions: shard hours and PUT payload\nunits. One PUT payload unit equates one request to send a record with up to 25 KB to\na Kinesis stream. If you send a piece of data that is 45 KB in size, for example, then\nthat counts as two PUT payload units. It works the same way as Amazon’s DynamoDB’s\nread and write request units.\n A Kinesis shard costs $0.015 per hour and $0.014 per million PUT payload units.\nThere are also additional charges if you enable optional features such as extending\nthe data retention period. Some of these additional costs are also charged at a per\nhour rate, such as the cost for extended data retention and enhanced fan-out.\n Because of the hourly cost, it’s not economically efficient to over-provision the\nnumber of shards you’ll need. Given the amount of throughput each shard supports,\nyou don’t need many shards to support even a high throughput system like Yle’s data\ningestion pipeline.\n Based on Yle’s prime-time traffic of 600,000 requests per minute, if we assume the\ntraffic is uniformly distributed across 1 minute, then we arrive at 10,000 requests per\nsecond. And assuming that each event is less than 25 KB in size, then Yle needs about\n10 shards to accommodate this traffic pattern. However, as we discussed, their traffic is\nspiky and, because Kinesis doesn’t support autoscaling, the Yle team over-provisions\ntheir stream, running 40 shards all the time. This gives the team plenty of headroom\nto handle unexpected spikes and to minimize the risk of data loss.\n6.2.2\nSQS dead-letter queue (DLQ)\nBecause data is the blood supply for Yle’s ML algorithms, the team wants to ensure\nthat it’s not lost when the Kinesis service experiences an outage in Yle’s region. In the\nevent the Kinesis service is out of commission, the API sends the events to the SQS\nDLQ so they can be captured and reprocessed later.\n",
      "content_length": 2935,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "88\nCHAPTER 6\nYle: Architecture highlights, lessons learned\n6.2.3\nThe Router Lambda function\nTo process the constant stream of events, a Lambda function called Router subscribes\nto the Kinesis data stream. This function routes events to different Kinesis Firehose\nstreams that the other microservices use.\n To make storing and querying the data more efficient, the Yle team stores the\nevents in Apache Parquet format. To do this, they use Amazon Kinesis Data Firehose\n(to batch data into large files and deliver them to S3) with AWS Glue Data Catalog (to\nprovide the schema). Figure 6.2 shows this arrangement.\n6.2.4\nKinesis Data Firehose\nKinesis Data Firehose is another member of the Amazon Kinesis family of services. It is\na fully managed service to load streaming data to a destination. Kinesis Firehose can\nsend data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service (Amazon\nES), and any HTTP endpoint owned by you or by external service providers such as\nDatadog, New Relic, and Splunk.\n A Firehose stream allows you to load streaming data with zero lines of code. Unlike\nKinesis Data Streams, a Kinesis Firehose stream scales automatically, and you pay for\nonly the volume of data you ingest into the stream. The cost for ingesting data into\nKinesis Data Firehose starts at $0.029 per GB for the first 500 TB of data per month. It\ngets cheaper the more data you ingest.\n In addition to the automated scaling, a Firehose stream can batch the incoming\ndata, compress it and, optionally, transform it using Lambda functions. It can also\nconvert the input data from JSON to Apache Parquet or to Apache ORC formats\nbefore loading it into the destination.\n Like Kinesis Data Streams, it stores data in the stream for only up to 24 hours. You\ncan configure the batch size by the maximum number of records or for a certain period\nof time. For example, you can ask the Firehose stream to batch the data into 128 MB\nfiles or 5 minutes’ worth of data, whichever limit is reached first. It’s a convenient\nFargate\nKinesis\nSQS\nLambda\nRouter\nKinesis Firehose\nKinesis Firehose\nFigure 6.2\nThe Lambda Router function routes events to different Kinesis Firehose \nstreams so they can be aggregated and converted to Apache Parquet files.\n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "89\nProcessing events in real-time\nservice with no management overhead for scaling, and you don’t have to write any\ncustom code to transport data to the intended target.\n To convert the data from JSON format to Apache Parquet or Apache ORC, you\nneed to use the AWS Glue Data Catalog service. A Kinesis Firehose stream uses the\nschema captured in the Glue Data Catalog before sending it to a destination.\n The Yle team uses S3 as the data lake and the destination for the Kinesis Firehose\nstreams (figure 6.3). Once the data is delivered to S3, it is further processed and con-\nsumed by a number of microservices to perform several ML tasks such as demo-\ngraphic predictions.\n6.2.5\nKinesis Data Analytics\nTo personalize the icon image for videos, the Yle team uses the contextual bandits model,\nwhich is a form of an unsupervised ML model. They use the user-interaction events to\nreward the model so it can learn what the user likes. To do that, the team uses Kinesis\nData Analytics to filter and aggregate the data from the Firehose stream and deliver it\nto a Lambda function called Reward Router. This function then calls several Reward\nAPIs to reward the personalization models the Yle team maintains (figure 6.4).\n Kinesis Data Analytics lets you run queries against streaming data using SQL or\nJava and the Apache Flink framework. Using an SQL approach, you can join, filter,\nand aggregate data across several streams without writing any custom code or running\nKinesis\nLambda\nRouter\nKinesis Firehose\nKinesis Firehose\nS3\nFigure 6.3\nThe Router function \nroutes incoming events to a number \nof Kinesis Firehose streams, one for \neach type of event. The streams then \nbatch, transform, and convert the \ndata into Apache Parque format and \nwrite it to S3.\nKinesis Firehose\nS3\nKinesis Analytics\nLambda\nReward\nRouter\n...\nReward APIs\nFigure 6.4\nThe Yle team uses Kinesis Data \nAnalytics and Lambda to reward different \npersonalization models in real time.\n",
      "content_length": 1951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "90\nCHAPTER 6\nYle: Architecture highlights, lessons learned\nany infrastructure. The Java approach, however, gives you the most control over how\nyour application runs and how it processes the data.\n You can output the result of your queries to Kinesis Data Stream, Kinesis Firehose,\nor a Lambda function. Having a Lambda function as a destination gives you a lot of\nflexibility, however. You can process the results further, forward the results to any-\nwhere you want, or both. In Yle’s case, they use the Reward Router function as the des-\ntination for the Kinesis Data Analytics application and reward the relevant\npersonalization models.\n6.2.6\nPutting it altogether\nTaking a step back, you can see in figure 6.5 what Yle’s data pipeline looks like from a\nhigh level. We have omitted some minor details, such as the fact that the Kinesis Fire-\nhose streams also use Lambda functions to transform and format the data and the fact\nthat this is just the start of the journey for many of these user events. Once the data is\nsaved into S3 in Apache Parquet format, many microservices ingest the data, process\nit, and use it to enrich their ML models.\nWhat I would like to highlight in this architecture is the prevalent use of Kinesis and\nits data analytics capabilities. This includes\nKinesis Data Streams for ingesting large amounts of user events.\nFargate\nKinesis\nSQS\nLambda\nRouter\nKinesis Firehose\nKinesis Firehose\nS3\nKinesis Analytics\nLambda\nReward\nRouter\n...\nReward APIs\nFigure 6.5\nYle’s data pipeline architecture. They use Fargate to run the ingestion API because of cost and \nperformance considerations and then process the ingested events in real time using Kinesis Data Streams, \nKinesis Firehose, and Lambda. The data is transformed, compressed, and converted to Apache Parquet format\nand stored in S3 as the data lake. At the same time, they also use Kinesis Data Analytics to perform real-time\naggregations and use Lambda to reward the relevant personalization ML models.\n",
      "content_length": 1982,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "91\nLessons learned\nKinesis Firehose Streams for batching, formatting, and outputting data into\nlarge compressed files that are more easily consumable by the downstream ML\nmodels.\nKinesis Data Analytics for running aggregations over live streams of data in real\ntime and using a Lambda function as a destination to reward personalization\nmodels.\n6.3\nLessons learned\nThe use of these Kinesis capabilities and how they are combined is a common sight in\ndata analytics applications. However, Yle is processing events at a much higher scale\nthan most! Operating at such high scale comes with unique challenges, and the Yle\nteam has learned some valuable lessons along the way, including those that follow.\n6.3.1\nKnow your service limits\nEvery service in AWS comes with service limits. These generally fall into three categories:\nResource limits—How many of X can you have in a region. For example, Kinesis\nData Streams has a default quota of 500 shards per region in us-east-1, us-west-1,\nand eu-west-1, and 200 shards per region in all other regions. Similarly, AWS\nIdentity and Access Management (IAM) has a default quota of 1,000 roles per\nregion.\nControl-plane API limits—How many requests per second you can send to a con-\ntrol plane API to manage your resources. For example, Kinesis Data Streams\nlimits you to five requests per second to the CreateStream API.\nData-plane API limits—How many requests per second you can send to a data\nplane API to act on your data. For example, Kinesis Data Streams limits you to\nfive GetRecords requests per second per shard.\nThese limits are published in the AWS Service Quotas console. In the console, you can\nview your current limits and whether you can raise the limit.\nSOFT VS. HARD LIMITS\nLimits that can be raised are considered soft limits, and those that can’t be raised are\nconsidered hard limits. You can ask for a soft limits raise via a support ticket, or you can\ndo it in the AWS Service Quotas console. But it’s worth keeping in mind that some-\ntimes there is a limit to how far you can raise those soft limits. For example, the num-\nber of IAM roles in a region is a soft limit, but you can raise that limit to only 5,000\nroles per region. If your approach relies on raising these soft limits indefinitely, then\nthere’s a good chance that you’re using the service in a way that it’s not designed for,\nand you might have to reconsider your approach.\n Keeping an eye on your usage levels and your current limits is something that\nevery AWS user should do but is especially important when you need to operate at\nscale and you run the risk of running into those limits. For the Yle team, one of the\nimportant lessons they learned is that you need to raise the limit on the number of\n",
      "content_length": 2736,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "92\nCHAPTER 6\nYle: Architecture highlights, lessons learned\nFargate tasks you can run and give yourself plenty of headroom because it can take a\nfew days for AWS to raise the limit in your account. \n At present, the default limit is 1,000 concurrent Fargate tasks per region. When\nthe Yle team started out, however, the default limit was only 100, and it took the team\nthree days to raise that limit to 200.\nPROJECT THROUGHPUT AT EVERY POINT ALONG THE PIPELINE\nTo understand which service limits affect your application, look at every service along\nthe way and build a projection of how throughput changes with user traffic. Take Yle’s\ncase: as the number of concurrent users goes up, there’s more traffic going through\nthe ingestion API running in Fargate. \nHow does this increase affect the throughput that needs to be processed by\nKinesis and, therefore, the number of shards that need to be provisioned? \nBased on the current BatchSize and ParallelizationFactor configurations,\nhow many concurrent Lambda executions would be required to process the\nevents at peak load? \nGiven that many concurrent Lambda executions, how many events would be\nsent to each Kinesis Firehose stream? \nDoes your current throughput limit for Kinesis Data Firehose support that\nmany events per second?\nALWAYS LOAD TEST, DON’T ASSUME\nEvery service in the pipeline can become a bottleneck, and the best way to know that\nyour application can handle the desired throughput is to load test it. The services you\nbuild your application on might be scalable, but it doesn’t mean that your application\nis, not without the proper adjustment to its service limits. \n If your target is to handle 100,000 concurrent users, then load test it to at least\n200,000 concurrent users. Who knows, maybe your application will be successful!\nThat’s what you’re hoping for, right? Even if your application already comfortably\nhandles 50,000 concurrent users, load test it to 200,000 concurrent users anyway. You\ncan’t assume the system is infinitely scalable and that its performance characteristics\nare perfectly consistent as throughput goes up. Don’t assume anything; find out.\nSOME LIMITS HAVE A BIGGER BLAST RADIUS THAN OTHERS\nIt’s also worth mentioning that some service limits have a bigger blast radius than oth-\ners. Lambda’s regional concurrency limit is a great example of this.\n Whereas exhausting the write throughput limit on a Kinesis shard affects only\nputRecord operations against that shard, the impact is localized to a single shard in a\nsingle Kinesis stream and will not affect your application in a big way. On the other\nhand, exhausting the Lambda concurrent executions limit can have a wide-reaching\nimpact on your application because you’re likely using Lambda functions to handle a\nvariety of different workloads: APIs, real-time event processing, transforming data for\nKinesis Firehose, and so on. \n",
      "content_length": 2887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "93\nLessons learned\n This is why you need to pay even more attention to those service limits that have a\nbig blast radius. In the case of Lambda, you can also use the ReservedConcurrency\nconfiguration to restrain the maximum number of concurrent executions a function\ncan have in cases where it’s appropriate and necessary.\nMIND CLOUDWATCH’S METRIC GRANULARITY\nYou should monitor your usage level and be proactive about raising service limits. One\nway to do that is by setting up CloudWatch alarms against the relevant metrics. One\ncaveat to keep in mind here is that CloudWatch often reports usage metrics at a per-\nminute granularity but the limits are per second, which applies to both Kinesis Data\nStreams and DynamoDB’s throughput metrics. In these cases, when you set up those\nCloudWatch alarms, make sure that you set up the thresholds correctly. For example,\nif the per-second throughput limit is 1, then the corresponding per-minute threshold\nshould be 60.\n6.3.2\nBuild with failure in mind\nNotice that in figure 6.1, there is a SQS DLQ? It’s there as a backup for when there is\na problem with the Kinesis Data Streams service. Kinesis Data Streams is a robust and\nhighly scalable service, but it’s not infallible.\nEVERYTHING FAILS, ALL THE TIME\nAWS CTO, Werner Vogel, famously said, “Everything fails, all the time.” It’s a fact of life\nthat even the most reliable and robust service can have a hiccup from time to time.\nRemember when S3 was down (https://aws.amazon.com/message/41926) for a few\nhours in 2017? Or that time when Kinesis Data Streams had an outage and affected\nCloudWatch and EventBridge as well (https://aws.amazon.com/message/11201)? Or\nwhen Gmail, Google Drive, and YouTube went down (http://mng.bz/ExlO)?\n At the machine level, individual disk drives or CPU cores or memory chips con-\nstantly fail and are replaced. Cloud providers such as AWS and Google have invested\nheavily into their physical infrastructure as well as their software infrastructure to\nensure that such failures do not affect their customers. In fact, by using serverless\ntechnologies such as API Gateway, Lambda, and DynamoDB, your application is\nalready protected from data center-wide failures because your code and data are\nstored in multiple availability zones (data centers) within a given AWS region. How-\never, there are occasional region-wide disruptions that affect one or more services in\nan entire AWS region, such as the S3 and Kinesis outages mentioned previously.\n What this means is that you need to build your application with failure in mind\nand have a plan B (and maybe even a plan C, D, and E) in case your primary service\nhas a bad day at the office. DLQs are a good way to capture traffic that can’t be deliv-\nered to the primary target when first asked. Many AWS services now offer DLQ sup-\nport out of the box. For example, SNS, EventBridge, and SQS all support DLQs\nnatively in case the events they capture cannot be delivered to the intended target\nafter retries. If you process events from a Kinesis Data Stream with a Lambda function,\nthen you can also use the OnFailure configuration to specify a DQL. \n",
      "content_length": 3130,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "94\nCHAPTER 6\nYle: Architecture highlights, lessons learned\n The more throughput your system has to process, the more failures you will\nencounter, and the more important these DLQs become. Remember, even a one-in-a-\nmillion event would occur five times a minute if you have to process 5,000,000\nrequests a minute!\nPAY ATTENTION TO RETRY CONFIGURATIONS\nAs you introduce more moving parts into your architecture and process more\nthroughput, you should also pay more attention to your timeout and retry configura-\ntions. There are two problems that often plague applications that operate at scale:\nThundering herd—A large number of processes waiting for an event are awaken\nat the same time, but there aren’t enough resources to handle the requests\nfrom all these newly awaken processes. This creates a lot of resource conten-\ntion, potentially causing the system to grind to a halt or fail over.\nRetry storm—An anti-pattern in client-server communications. When a server\nbecomes unhealthy, the client retries aggressively, which multiplies the volume\nof requests to the remaining healthy servers and, in turn, causes them to time-\nout or fail. This triggers even more retries and exacerbates the problem even\nfurther.\nRetries are a simple and effective way to handle most intermittent problems, but set-\nting these needs to be done with care. A good practice is to use exponential backoff\nbetween retry attempts and the circuit breaker pattern to mitigate the risk of retry\nstorms (https://martinfowler.com/bliki/CircuitBreaker.html).\n6.3.3\nBatching is good for cost and efficiency\nCost is one of those things that developers often don’t think about when they’re mak-\ning architectural decisions, but this can come back and bite them in a big way later.\nThis is especially true when you need to operate at scale and process large volumes of\nevents. As we discussed in section 6.1.1, one of the reasons why the Yle team decided\nto use Fargate for ingesting user-interactions events instead of API Gateway and\nLambda was cost and efficiency. \n In general, AWS services that charge you based on up time tend to be orders of\nmagnitude cheaper when running at scale, compared with those that charge based on\nrequest count. And the bigger the scale, the more you need to batch events for cost\nand efficiency. After all, processing 1,000 events with a single Lambda invocation is far\ncheaper and more efficient than processing those with 1,000 Lambda invocations. \n Processing events in a batch also reduces the number of concurrent Lambda exe-\ncutions you need to run and minimizes the risk of exhausting the regional concurrent\nexecutions limit. However, with batch processing comes the potential for partial failures.\n If you process one event at a time and that event fails enough times, then you put it\ninto the DLQ and move on to the next event. But when you process 1,000 events in a\nsingle invocation and one event fails, what do you do about the other 999 events? Do\nyou throw an error and let the invocation be retried, potentially reprocessing the 999\n",
      "content_length": 3055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "95\nSummary\nsuccessful events? Do you put the failed event into a DLQ and process it later? These\nare the sort of questions that you have to answer.\n6.3.4\nCost estimation is tricky\nIf you don’t pay attention to cost, then it can pile up quickly and catch you by surprise.\nBut trying to accurately predict your cost ahead of time is also difficult; there are a lot\nof factors that can affect your cost in production. For example, looking at the architec-\nture diagram in figure 6.5, you might be focusing on the cost of Fargate, Lambda, and\nthe Kinesis family of services. There are also other peripheral services to consider,\nsuch as the cost for CloudWatch, X-Ray, and data transfer costs.\n The cost of Lambda is usually a small part of the overall cost of a serverless applica-\ntion. In fact, in most production systems, the cost of Lambda often pales in compari-\nson with the cost of CloudWatch metrics, logs, and alarms.\nSummary\nYle’s ingestion API processes more than 500 million events per day and more\nthan 600,000 events per minute at peak times. The traffic is spiky and heavily\ninfluenced by real-world events such as a live hockey match or the election\nresults.\nThe Yle team uses Fargate for the ingestion API because of cost and perfor-\nmance considerations.\nIn general, AWS services that charge you based on up time are significantly\ncheaper to use at scale compared to those services that charge you based on\nusage (number of requests, volume of data processed, etc.).\nThe Yle team uses Kinesis Data Stream, Kinesis Data Firehose, and Lambda to\nprocess, transform, and convert the ingested events to Apache Parquet format.\nThe ingested data is stored in S3 as the data lake.\nThe Yle team uses Kinesis Data Analytics to perform real-time aggregation on\nthe ingested events.\nThe aggregated events reward the relevant personalization ML models.\n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 1886,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "Part 3\nPracticum\nIt’s time to take a look at three interesting problems and discuss how we\nwould tackle them using serverless architectures. We will not be providing\nsource code, but we will show sample architectures and discuss how to go about\ndesigning three different and unique systems. Let’s sink our teeth into these\ndelicious serverless architectures.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "99\nBuilding a scheduling\n service for ad hoc tasks\nWith serverless technologies, you can build scalable and resilient applications\nquickly by offloading infrastructure responsibilities to AWS. Doing so allows you to\nfocus on the needs of your customers and your business. Ideally, all the code you\nwrite is directly attributed to features that differentiate your business and add value\nfor your customers.\n What this means in practice is that you use many managed services instead of\nbuilding and running your own. For example, instead of running a cluster of\nRabbitMQ servers on EC2, you use Amazon Simple Queue Service (SQS). Through-\nout the course of this book, you have also read about other AWS services such as\nDynamoDB and Step Functions.\nThis chapter covers\nApproaching architectural decisions when faced \nwith a novel problem\nDefining nonfunctional requirements\nChoosing the right AWS service to satisfy \nnonfunctional requirements\nCombining different AWS services \n",
      "content_length": 980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "100\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n Therefore, an important skill is to be able to analyze the nonfunctional require-\nments of a system and choose the correct AWS service to work with. But the AWS eco-\nsystem is enormous and consists of a huge number of different services. Many of these\nservices overlap in their use cases but have different operational constraints and scal-\ning characteristics. For example, to add a queue between two Lambda functions to\ndecouple them, you can use any of the following services:\nAmazon SQS\nAmazon Simple Notification Service (SNS)\nAmazon Kinesis Data Streams\nAmazon DynamoDB Streams\nAmazon EventBridge\nAWS IOT Core\nThese services have different characteristics when it comes to their scaling behavior,\ncost, service limits, and how they integrate with Lambda. Depending on your require-\nments, some might be a better fit for you than others.\n Although AWS gives you a lot of different services to architect your system, it doesn’t\noffer any guidance or opinion on when to use which. As a developer or architect working\nwith AWS, one of the most challenging tasks is figuring this out for yourself.\n This chapter shines a light on the problem by taking you through the design pro-\ncess for a scheduling service for ad hoc tasks. It’s a common need for applications,\nand AWS does not yet offer a managed service to solve this problem. The closest thing\nin AWS is the scheduled events in EventBridge, but scheduling repeated tasks (e.g., do\nX every Y seconds) is different than scheduling ad hoc tasks (e.g., do X at 2021-08-\n30T23:59:59Z, do Y at 2021-08-20T08:05:00Z).\n The functional requirement for such a scheduling service is simple: you schedule\nan ad hoc task to be run at a specified date and time (for example, “Remind me to call\nmum on Monday, at 9:00”). What’s interesting about this is that it has to deal with dif-\nferent nonfunctional requirements depending on the application (for example, “It\nneeds to handle a million open tasks that are scheduled but not yet run”).\n For the rest of this chapter, you will see five different solutions for this scheduling\nservice using different AWS services and learn how to evaluate them. But first, let’s\ndefine the nonfunctional requirements that we will evaluate the solutions against.\nHere is the plan for this chapter:\nDefine nonfunctional requirements. The four nonfunctional requirements we will\nconsider are precision, scalability (number of open tasks), scalability (hotspots),\nand cost. All the following solutions will be evaluated against these requirements:\n– Cron with EventBridge—A simple solution using cron jobs to find open tasks\nand run them.\n– DynamoDB TTL—A creative use of DynamoDB’s time-to-live (TTL) mecha-\nnism to trigger and run the scheduled ad hoc tasks.\n– Step Functions—A solution that uses Step Function’s Wait state to schedule\nand run tasks.\n",
      "content_length": 2897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "101\nDefining nonfunctional requirements\n– SQS—A solution that uses SQS’s DelaySeconds and VisibilityTimeout set-\ntings to hide tasks until their scheduled execution time.\n– Combining DynamoDB TTL and SQS—A solution that combines DynamoDB\nTTL with SQS to compensate for each other’s shortcomings.\nChoose the right solution for your application. Different applications have different\nneeds, and some nonfunctional requirements may be more important than oth-\ners. In this section, you will see three different applications, understand their\nneeds, and pick the most appropriate solution for them.\n7.1\nDefining nonfunctional requirements\nThe ad hoc scheduling service is an interesting problem that often shows up in differ-\nent contexts and has different nonfunctional requirements. For example, a dating\napp may require ad hoc tasks to remind users a date is coming up. A multiplayer game\nmay need to schedule ad hoc tasks to start or stop a tournament. A news site might use\nad hoc tasks to cancel expired subscriptions.\n User behaviors and traffic patterns differ between these contexts, which in turn\ncreate different nonfunctional requirements the service needs to meet. It’s important\nfor you to define these requirements up front to prevent unconscious biases (such as a\nconfirmation bias) from creeping in. \n Too often, we subconsciously put more weight behind characteristics that align\nwith our solution, even if they aren’t as important to our application. Defining\nrequirements up front helps us maintain our objectivity. For a service that allows you\nto schedule ad hoc tasks to run at a specific time, the following lists some nonfunc-\ntional requirements you need to consider:\nPrecision—How close to the scheduled time is the task run?\nScalability (number of open tasks)—Can the service support millions of tasks that\nare scheduled but not yet processed?\nScalability (hotspots)—Can the service run millions of tasks at the same time?\nCost\nThroughout this chapter, you will evaluate five different solutions against this set of\nnonfunctional requirements. And remember, there are no wrong answers! The goal\nof this chapter is to help you hone the skill of thinking through solutions and evaluat-\ning them. We’ll spend the rest of the chapter looking at these different solutions. Each\nprovides a different approach and utilizes different AWS services. However, every solu-\ntion uses only serverless components, and there is no infrastructure to manage. The\nfive solutions include\nA cron job with EventBridge\nDynamoDB Time to Live (TTL)\nStep Functions\nSQS\nCombining DynamoDB TTL with SQS\n",
      "content_length": 2609,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "102\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nAfter each solution, we’ll ask you to score the solution against the aforementioned\nnonfunctional requirements. You can compare your scores against ours and see the\nrationale for our scores. Let’s start with the solution for a cron job with EventBridge.\n7.2\nCron job with EventBridge\nThis solution uses a cron job in EventBridge to invoke a Lambda function every cou-\nple of minutes (figure 7.1). With this solution, you will need the following:\nA database (such as DynamoDB) to store all the scheduled tasks, including\nwhen they should run\nAn EventBridge schedule that runs every X minutes\nA Lambda function that reads overdue tasks from the database and runs them\nThere are a few things to note about this solution:\nThe lowest granularity for an EventBridge schedule is 1 minute. Assuming the\nservice is able to keep up with the rate of scheduled tasks that need to run, the\nprecision of this solution is within 1 minute.\nThe Lambda function can run for up to 15 minutes. If the Lambda function\nfetches more scheduled tasks than it can process in 1 minute, then it can keep\nrunning until it completes the batch. In the meantime, the cron job can start\nanother concurrent execution of this function. Therefore, you need to take\ncare to avoid the same scheduled tasks being fetched and run twice.\nThe precision of individual tasks within the batch can vary, depending on their\nrelative position in the batch and when they are actually processed. In the case\nof a large batch that cannot be processed within 1 minute, the precision for\nsome tasks may be longer than 1 minute (figure 7.2).\nIt’s possible to increase the throughput of this solution by adding a Lambda\nfunction as the target multiple times (figure 7.3).\nEventBridge\nLambda\nDynamoDB\nInvokes Lambda\nfunction every X minutes.\nDefines cron job as a schedule\n(e.g., every X minutes).\nQueries DynamoDB for\noverdue tasks to execute.\nAll scheduled tasks are\nstored in DynamoDB.\nFigure 7.1\nHigh-level architecture showing an EventBridge cron job with \nLambda to run ad hoc scheduled tasks.\n",
      "content_length": 2112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "103\nCron job with EventBridge\nTime\n23:00 UTC\n23:01 UTC\n23:02 UTC\nTask 1’s scheduled time\nTask 10001’s scheduled time\nCron job runs\nTask 1 executed\nTask 10001 executed\nTask 1’s precision (~1 min)\nTask 10001’s precision (> 1 min)\nFigure 7.2\nThe precision of individual tasks inside a batch can vary greatly \ndepending on their position inside the batch.\nThe same function can be configured as a target more than once. \nEach time the cron job runs, the function would be invoked \nmultiple times, one for every time it's configured as a target.\nFigure 7.3\nYou can add the same Lambda function as a target for an EventBridge rule multiple times.\n",
      "content_length": 641,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "104\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nBecause EventBridge has a limit of five targets per rule, you can use this tech-\nnique to increase the throughput fivefold. This means every time the cron job\nruns, it creates five concurrent executions of this Lambda function. To avoid\nthem all picking up and running the same tasks, you can configure different\ninputs for each target as figure 7.4 shows.\n7.2.1\nYour scores\nWhat do you think of this solution? How would you rate it on a scale of 1 (worst) to 10\n(best) against each of the nonfunctional requirements? Write down your scores in the\nempty spaces in the tables provided for this (see table 7.1 as an example). And\nremember, there are no right or wrong answers. Just use your best judgement based\non the information available.\nTable 7.1\nYour solution scores for a cron job\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nYou can configure a different input for \neach target, then each concurrent \nexecution of the Lambda function \npicks up a different segment of the \nopen tasks that need to be run.\nFigure 7.4\nYou can configure a \ndifferent input for each target to \nhave them fetch different subsets \nof scheduled tasks. Then the tasks \nare not processed multiple times.\n",
      "content_length": 1279,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "105\nCron job with EventBridge\n7.2.2\nOur scores\nThe biggest advantage of this solution is that it’s really simple to implement. The com-\nplexity of a solution is an important consideration in real-world projects because we’re\nalways bound by resource and time constraints. However, for the purpose of this book,\nwe will ignore these real-world constraints and only consider the nonfunctional require-\nments outlined in section 7.1. With that said, here are our scores for this solution (table\n7.2). We’ll then explain our reasons for these scores in the following subsections.\nPRECISION\nWe gave this solution a 6 for precision because EventBridge cron jobs can run at most\nonce per minute. That’s the best precision we can hope for with this solution. Further-\nmore, this solution is also constrained by the number of tasks that can be processed in\neach iteration. When there are too many tasks that need to be run simultaneously,\nthey can stack up and cause delays. These delays are a symptom of the biggest chal-\nlenge with this solution—dealing with hotspots. More on that next.\nSCALABILITY (NUMBER OF OPEN TASKS)\nProvided that the open tasks do not cluster together (hotspots), this solution would\nhave no problem scaling to millions and millions of open tasks. Each time the\nLambda function runs, it only cares about the tasks that are now overdue. Because of\nthis, we gave this solution a perfect 10 for scalability (number of open tasks).\nSCALABILITY (HOTSPOTS)\nWe gave this solution a lowly 2 for this criteria because a cron job doesn’t handle\nhotspots well at all. When there are more tasks than the Lambda function can handle\nin one invocation, this solution runs into all kinds of trouble and forces us into diffi-\ncult trade-offs.\n For example, do we allow the function to run for more than 1 minute? If we don’t,\nthen the function would time out, and there’s a strong possibility that some tasks\nmight be processed but not marked as so because the invocation was interrupted mid-\nway through. We need to either make sure the scheduled tasks are idempotent or we\nhave to choose between:\nTable 7.2\nOur solution scores for a cron job with EventBridge\nScore\nPrecision\n6\nScalability (number of open tasks)\n10\nScalability (hotspots)\n2\nCost\n7\n",
      "content_length": 2249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "106\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nExecuting some tasks twice if we mark them as processed in the database after\nsuccessfully processing.\nNot executing some tasks at all if we mark them as processed in the database\nbefore we finish processing them.\nEmploying a mechanism such as the Saga pattern (http://mng.bz/AOEp) for\nmanaging the transaction and reliably updating the database record after the\ntask is successfully processed. (This can add a lot of complexity and cost to the\nsolution.)\nOn the other hand, if we allow the function to run for more than 1 minute, then we\nare less likely to experience this problem until we see a large enough hotspot that the\nLambda function can’t process in 15 minutes! Also, now there can be more than one\nconcurrent execution of this function running at the same time. To avoid the same\ntask being run more than once, we can set the function’s Reserved Concurrency set-\nting to 1. This ensures that at any moment, only one concurrent execution of the\nLambda function is running (see figure 7.5). However, this severely limits the poten-\ntial throughput of the system.\n Imagine 1,000,000 tasks that need to be run at 00:00 UTC, but the Lambda func-\ntion can process only 10,000 tasks per minute. If we do nothing, then the function\nwould timeout, be retried, and would take at least 100 invocations to finish all the\n00:00\n00:01\n00:02\n00:03\n00:04\n00:05\ntime\nmissed\nmissed\nmissed\nOn the next iteration of the cron \njob, it will try to invoke the function \nagain. These invocations would be \nthrottled by Lambda, and the cron \njob would miss those iterations.\nWhen Reserved Concurrency is set \nto 1, only one concurrent execution \nof the function is allowed to run at \nthe same time.\nTasks that are scheduled to execute \nat 00:01, 00:02, and 00:03 would \nnot have been picked up by the \ninvocation that started at 00:00 \nand instead picked up here.\nFigure 7.5\nIf we limit Reserve Concurrency to 1, then there will be only one concurrent execution of the \nLambda function running at any moment. This means some cron job cycles will be skipped.\n",
      "content_length": 2108,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "107\nCron job with EventBridge\ntasks. In the meantime, other tasks are also delayed, further exasperating the impact\non user experience. This is the Achille’s heel of this solution. But we can tweak the\nsolution to increase its throughput and help it cope with hotspots better. More on this\nlater.\nCOST\nWith EventBridge, cron jobs are free, but we have to pay for the Lambda invocations\neven when there are no tasks to run. You can minimize the Lambda cost if you use a mod-\nerate memory size for the cron job. After all, it’s not doing anything CPU-intensive and\nshouldn’t need a lot of memory (and therefore CPU).\n In our scenario, the main cost for this solution is the DynamoDB read and write\nrequests. For every task, you need one write request (when scheduling the task) and\none read request (when the cron job retrieves it). This access pattern makes it a good\nfit for DynamoDB’s on-demand pricing and allows the cost of the solution to grow lin-\nearly with its scale. At $1.25 per million write units and $0.25 per million read units,\nthe cost per million scheduled tasks can be as low as $1.50. That’s just the DynamoDB\ncost, and even that depends on the size of the items you need to store for each task as\nDynamoDB read/write units are calculated based on payload size. You also have to fac-\ntor in the Lambda costs too, which also depend on a number of factors such as mem-\nory size and execution duration.\n Nonetheless, this is still a cost-effective solution, even when you scale to millions of\nscheduled tasks per day. And, hence, why we gave it a score of 7. Overall, this is a good\nsolution for applications that don’t have to deal with hotspots, and it is also easy to\nimplement. As we mentioned earlier, we can also tweak the architecture slightly to\naddress its problem with hotspots.\n7.2.3\nTweaking the solution  \nEarlier, we mentioned that we can increase the throughput of this solution by allowing\nmultiple concurrent executions of the Lambda to run in parallel. We can do this by\nduplicating the Lambda function target in the EventBridge rule. Because there’s a limit\nof five targets per EventBridge rule, we can only hope for a fivefold increase at best.\nBeyond that, we can also duplicate the EventBridge rule itself as many times as we need.\n But even with these tricks, Lambda’s 15 minutes execution time limit is still loom-\ning over our head. We also have to shard the reads so that the concurrent executions\ndon’t process the same tasks. Doing that, we also incur higher operational cost and\ncomplexity as well. There are more resources to configure and manage, and there are\nmore Lambda invocations and database reads, even though most of the time they’re\nnot necessary. Essentially, we have “provisioned” (for lack of a better term) our appli-\ncation for peak throughput all the time.\n Increasing throughput this way is ineffective. A much better alternative is to fan-out\nthe processing logic based on the number of tasks that need to run. Lambda’s burst\ncapacity limit allows up to 3,000 concurrent executions to be created instantly (see\nhttps://amzn.to/2BxRuVG). This allows for a huge potential for parallel processing\n",
      "content_length": 3157,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "108\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\neven if we use just a fraction of it. For this to work, we need to move the business logic\nto fetch and run tasks into another Lambda function. From here, we can invoke as\nmany instances of this new function as we deem necessary when faced with a large batch\nof tasks (figure 7.6 shows this approach).\nOnce we know the number of tasks that needs to run, we can calculate the number of\nconcurrent executions we need. To alleviate the time pressure and minimize the dan-\nger of timeouts, we can add some headroom into our calculation.\n For example, if the throughput for the processing function is 10,000 tasks per min-\nute, then we can start a new concurrent execution for every 5,000 tasks. If there are\n1,000,000 tasks, then we need 200 concurrent executions. This is well below the burst\ncapacity limit of 3,000 concurrent executions in the region. \nCount\nFetch tasks\nInvoke\n(async)\nSQS\nDLQ\nSet InvocationType to Event to\nmake the invocation asynchronous.\ncron-runner\ntask-runner\nTo prevent data loss, use a dead-\nletter queue (DLQ) to capture\nevents that can’t be processed\ndue to persistent failures.\nInstead of a DLQ, you can also configure\nan On-Failure destination. The advantage\nof using Lambda Destinations instead of\nDLQ is that it captures the invocation error\ntoo, not just the invocation event.\nInstead of fetching and running the tasks,\nthe Lambda function triggered by the cron\njob now only gets a count of the number\nof tasks and invokes another Lambda\nfunction (the task-runner function).\nThe number of tasks in a given minute slot\nshould be recorded as an atomic counter.\nThis avoids expensive SCAN or QUERY \nrequests, where you would pay for every\nitem and have to page through the results.\nEventBridge\nLambda\nDynamoDB\nLambda\nFigure 7.6\nAn alternative architecture as a solution to our cron job. The solution fans out the \nprocessing logic to another function.\nExercise: Score the modified solution\nConsider how the proposed changes would affect the nonfunctional requirements of\nprecision, scalability (number of open tasks), scalability (hotspots), and costs. How\nwould you score this modified solution?\n",
      "content_length": 2186,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "109\nDynamoDB TTL\n7.2.4\nFinal thoughts\nCron jobs can be a simple and yet effective solution. As you saw, with some small tweaks\nit can also be scaled to support even large hotspots. However, it tends to push a lot of\nthe load onto the database. In the aforementioned scenario of 1,000,000 tasks that need\nto be run in a single minute, it would require 1,000,000 reads from DynamoDB. Luckily\nfor us, DynamoDB can handle this level of traffic, although we need to be careful with\nthe throughput limits that are in place. For example, DynamoDB has a default limit of\n40,000 read units per table for on-demand tables (https://amzn.to/3eH0THZ).\n What if there’s a way to implement the scheduling service without having to read\nfrom the DynamoDB table at all? It turns out we can do that by taking advantage of\nDynamoDB’s time-to-live (TTL) feature (https://amzn.to/2NRgARU).\n7.3\nDynamoDB TTL\nDynamoDB lets you specify a TTL value on items, and it deletes the items after the\nTTL has passed. This is a fully managed process, so you don’t have to do anything\nyourself besides specifying a TTL value for each item.\n You can use the TTL value to schedule a task that needs to run at a specific time.\nWhen the item is deleted from the table, a REMOVE event is published to the corre-\nsponding DynamoDB stream. You can subscribe a Lambda function to this stream and\nrun the task when it’s removed from the table (figure 7.7).\nExercise: Other alternatives\nWhile keeping to the same general approach of using cron jobs, are there any modi-\nfications to the basic design that can compensate for its shortcomings in precision\nand scaling for hotspots?\nLambda\nDynamoDB\nDynamoDB Streams\nLambda\nScheduler\nscheduled_tasks\nREMOVE events\nExecute\nThis table holds all the tasks that have\nbeen scheduled. The scheduled time for\nthe tasks are used as their TTL, then \nwhen their schedule time is up, they will \nbe deleted by DynamoDB TTL.\nThis function writes the scheduled task\ninto the scheduled_tasks table with the\nTTL set to the scheduled execution time.\nWhen an item is deleted by DynamoDB\nTTL, a corresponding REMOVE event\nwill be recorded in the table’s stream.\nThe Execute function listens\nfor the REMOVE event and\nruns the corresponding task.\nFigure 7.7\nHigh-level architecture using DynamoDB TTL to run ad hoc scheduled tasks.\n",
      "content_length": 2313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "110\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nThere are a couple of things to keep in mind about this solution. The first, and most\nimportant, is that DynamoDB TTL doesn’t offer any guarantee on how quickly it deletes\nexpired items from the table. In fact, the official documentation (https://amzn.to/\n2NRgARU) only goes as far as to say, “TTL typically deletes expired items within 48 hours\nof expiration” (see figure 7.8). In practice, the actual timing is usually not as bleak.\nBased on empirical data that we collected, items are usually deleted within 30 minutes\nof expiration. But as figure 7.8 shows, it can vary greatly depending on the size and activ-\nity level of the table.\nThe second thing to consider is that the throughput of the DynamoDB stream is con-\nstrained by the number of shards in the stream. The number of shards is, in turn, deter-\nmined by the number of partitions in the DynamoDB table. However, there’s no way for\nyou to directly control the number of partitions. It’s entirely managed by DynamoDB,\nbased on the number of items in the table and its read and write throughputs.\n We know we’re throwing a lot of information at you about DynamoDB, including\nsome of its internal mechanics such as how it partitions data. Don’t worry if these are\nall new to you, you can learn a lot about how DynamoDB works under the hood by\nwatching this session from AWS re:invent 2018: https://www.youtube.com/watch?v=\nyvBR71D0nAQ.\n7.3.1\nYour scores\nWhat do you think of this solution? How would you rate it on a scale of 1 to 10 for\neach of the nonfunctional requirements? As before, write your scores in the empty\nspaces in table 7.3.\nTable 7.3\nYour scores for DynamoDB TTL\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nFigure 7.8\nDynamoDB TLL’s notification regarding its ability to delete expired items in tables\n",
      "content_length": 1876,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "111\nDynamoDB TTL\n7.3.2\nOur scores\nThe biggest problem with this solution is that DynamoDB TTL does not delete the\nscheduled items reliably. This limitation means it’s not suitable for any application\nthat is remotely time sensitive. With that said, here are our scores in table 7.4. Again,\nwe present how we arrived at these scores in the following subsections.\nPRECISION\nScheduled tasks would be run within 48\nhours of their scheduled time. A score of 1\nmight be considered a flattering score here.\nSCALABILITY (NUMBER OF OPEN TASKS)\nWe gave this solution a perfect 10 because\nthe number of open tasks equals the num-\nber of items in the scheduled_tasks table.\nBecause DynamoDB has no limit on the\ntotal number of items you can have in a\ntable, this solution can scale to millions of\nopen tasks. Unlike relational databases,\nwhose performance can degrade quickly\nas the database gets bigger, DynamoDB\noffers consistent and fast performance no\nmatter how big it gets. Figure 7.9 provides\na testimony to its performance.\nSCALABILITY (HOTSPOTS)\nWe gave this solution a 6 because it can still\nface throughput-related problems because\nit’s constrained by the throughput of the\nDynamoDB stream. But the tasks would be\nsimply queued up in the stream and would\nrun slightly later than scheduled.\n Let’s drill into this throughput constraint some more as that is useful for you to\nunderstand. As mentioned previously, the number of shards in the DynamoDB stream\nis managed by DynamoDB. For every shard the Lambda service would have a dedicated\nTable 7.4\nOur scores for DynamoDB TTL\nScore\nPrecision\n1\nScalability (number of open tasks)\n10\nScalability (hotspots)\n6\nCost\n10\nFigure 7.9\nA satisfied customer’s statement \nregarding DynamoDB’s scalability and number \nof opened tasks\n",
      "content_length": 1769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "112\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nconcurrent execution of the execute function. You can read more about how Lambda\nworks with DynamoDB and Kinesis streams in the official documentation at\nhttps://amzn.to/2ZIu3Cx.\n When a large number of tasks are deleted from DynamoDB at the same time, the\nREMOVE events are queued in the DynamoDB stream for the execute function to\nprocess. These events stay in the stream for up to 24 hours. As long as the execute\nfunction is able to eventually catch up, then we won’t lose any data. \n Although there is no scalability concern with hotspots per se, we do need to con-\nsider the factors that affect the throughput of this solution. Ultimately, these through-\nput limitations will affect the precision of this solution:\nHow quickly the hotspots are processed depends on how quickly DynamoDB TTL deletes\nthose items. DynamoDB TTL deletes items in batches, and we have no control\nover how often it runs and how many items are deleted in each batch.\nHow quickly the execute function processes all the tasks in a hotspot is constrained by\nhow many instances of it runs in parallel. Unfortunately, we can’t control the num-\nber of partitions in the scheduled_tasks table, which ultimately determines the\nnumber of concurrent executions of the execute function. However, we can\noverride the Concurrent Batches Per Shard configuration setting (https://\namzn.to/2YUGE59), which allows us to increase the parallelism factor tenfold\n(see figure 7.10). \nFigure 7.10\nYou can find the Concurrent Batches Per Shard setting under Additional Settings for Kinesis \nand DynamoDB Stream functions.\n",
      "content_length": 1643,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "113\nStep Functions\nCOST\nThis solution requires no DynamoDB reads. The deleted item is included in the\nREMOVE events in the DynamoDB stream. Because events in the DynamoDB stream\nare received in batches, they are efficient to process and require fewer Lambda invo-\ncations. Furthermore, DynamoDB Streams are usually charged by the number of read\nrequests, but it’s free when you process events with Lambda. Because of these charac-\nteristics, this solution is extremely cost effective even when it’s scaled to many millions\nof scheduled tasks. Hence, this is why we gave it a perfect 10 for Cost.\n7.3.3\nFinal thoughts\nThis solution makes creative use of the TTL feature in DynamoDB and gives you an\nextremely cost-effective solution for running scheduled tasks. However, because\nDynamoDB TTL doesn’t offer any reasonable guarantee on how quickly tasks are\ndeleted, it’s ill-fitted for many applications. In fact, neither cron jobs nor DynamoDB\nTTL are well-suited for applications where tasks need to be run within a few seconds of\ntheir scheduled time. For these applications, our next solution might be the best fit as\nit offers unparalleled precision at the expense of other nonfunctional requirements.\n7.4\nStep Functions\nStep Functions is an orchestration service that lets you model complex workflows as state\nmachines. It can invoke Lambda functions or integrate directly with other AWS services\nsuch as DynamoDB, SNS, and SQS when the state machine transitions to a new state.\n One of the understated superpowers of Step Functions is the Wait state (https://\namzn.to/38po884). It lets you pause a workflow for up to an entire year! Normally,\nidle waiting is difficult to do with Lambda. But with Step Functions, it’s as easy as a few\nlines of JSON:\n\"wait_ten_seconds\": {\n  \"Type\": \"Wait\",\n  \"Seconds\": 10,\n  \"Next\": \"NextState\"\n}\nYou can also wait until a specific UTC timestamp:\n\"wait_until\": {\n  \"Type\": \"Wait\",\n  \"Timestamp\": \"2016-03-14T01:59:00Z\",\n  \"Next\": \"NextState\"\n}\nAnd using TimestampPath, you can parameterize the Timestamp value using data that\nis passed into the execution:\n\"wait_until\": {\n  \"Type\": \"Wait\",\n  \"TimestampPath\": \"$.scheduledTime\",\n  \"Next\": \"NextState\"\n}\n",
      "content_length": 2191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "114\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nTo schedule an ad hoc task, you can start a state machine execution and use a Wait\nstate to pause the workflow until the specified date and time. This solution is precise.\nBased on the data we have collected, tasks are run within 0.01 second of the scheduled\ntime in the 90th percentile. However, there are several service limits to keep in mind\n(see https://amzn.to/2C4fGPD):\nThere are limits to the StartExecution API. This API limits the rate at which you can\nschedule new tasks because every task has its own state machine execution (see\nfigure 7.11).\nThere are limits to the number of state transitions per second. When the Wait state\nexpires, the scheduled task runs. However, when there are large hotspots where\nmany tasks all run simultaneously, these can be throttled because of this limita-\ntion (see figure 7.12).\nFigure 7.11\nStartExecution API \nlimit for AWS Step Functions\nFigure 7.12\nState transition limit for AWS Step Functions\n",
      "content_length": 1007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "115\nStep Functions\nThere is a default limit of 1,000,000 open executions. Because there is one open exe-\ncution per scheduled task, this is the maximum number of open tasks the sys-\ntem can support.\nThankfully, all of these limits are soft limits, which means you can increase them with a\nservice limit raise. However, given that the default limits for some of these are pretty\nlow, it might not be possible to raise to a level that can support running a million\nscheduled tasks in a single hotspot.\n There is also the hard limit on how long an execution can run, which is one year.\nThis limits the system to schedule tasks that are no further than a year away. For most\nuse cases, this would likely be sufficient. If not, we can tweak the solution to support\ntasks that are scheduled for more than a year away (more on this later).\n7.4.1\nYour scores\nWhat do you think of this solution? How would you rate it on a scale of 1 to 10 against\neach of the nonfunctional requirements? As before, write down your scores in the\nempty spaces provided by table 7.5.\n7.4.2\nOur scores\nStep Functions gives us a simple and elegant solution for the problem at hand. How-\never, it’s hampered by several service limits that makes it difficult to scale. We will dive\ninto these limitations, but first, table 7.6 shows our scores for this solution.\nPRECISION\nAs we mentioned before, Step Functions is able to run tasks within 0.01 s precision at\nthe 90th percentile. It just doesn’t get more precise than that!\nTable 7.5\nYour solution scores for Step Functions\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nTable 7.6\nOur scores for Step Functions\nScore\nPrecision\n10\nScalability (number of open tasks)\n7\nScalability (hotspots)\n4\nCost\n2\n",
      "content_length": 1751,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "116\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nSCALABILITY (NUMBER OF OPEN TASKS)\nWe gave this solution a 7 because the StartExecution API limit restricts how many\nscheduled tasks we can create per second. Whereas solutions that store scheduled\ntasks in DynamoDB can easily scale to scheduling tens of thousands of tasks per sec-\nond, here we have to contend with a default refill rate of just 300 per second in the\nlarger AWS regions. Luckily, it is a soft limit, so technically we can raise it to whatever\nwe need. But the onus is on us to constantly monitor its usage against the current limit\nto prevent us from being throttled.\n The same applies to the limit on the number of open executions. While the\ndefault limit of 1,000,000 is generous, we still need to keep an eye on the usage level.\nOnce we reach the limit, no new tasks can be scheduled until existing tasks are run.\nUser behavior would have a big impact here. The more uniformly the tasks are distrib-\nuted over time, the less likely this limit would be an issue.\nSCALABILITY (HOTSPOTS)\nWe gave this solution a 4 because the limit on StateTransition per second is problem-\natic if a large cluster of tasks needs to run during the same time. Because the limit\napplies to all state transitions, even the initial Wait states could be throttled and affect\nour ability to schedule new tasks.\n We can increase both the bucket size (think of it as the burst limit) as well as the\nrefill rate per second. But raising these limits alone might not be enough to scale this\nsolution to support large hotspots with, say, 1,000,000 tasks. Thankfully, there are\ntweaks we can make to this solution to help it handle large hotspots better, but we\nneed to trade off some precision (more on this later).\nCOST\nWe gave this solution a 2 because Step Functions is one of the\nmost expensive services on AWS. We are charged based on\nthe number of state transitions. For a state machine that waits\nuntil the scheduled time and runs the task, there are four\nstates (see figure 7.13), and every execution charges for these\nstate transitions (http://mng.bz/ZxGm).\n At $0.025 per 1,000 state transitions, the cost for schedul-\ning 1,000,000 tasks would be $100 plus the Lambda cost asso-\nciated with executing the tasks. This is nearly two orders of\nmagnitude higher than the other solutions considered so far. \n7.4.3\nTweaking the solution\nSo far, we have discussed several problems with this solution:\nnot being able to schedule tasks for more than a year and hav-\ning trouble with hotspots. Fortunately, there are simple modi-\nfications we can make to address these problems.\nWait\nStart\nExecute\nEnd\nFigure 7.13\nA simple \nstate machine that waits \nuntil the scheduled time \nto run its task\n",
      "content_length": 2743,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "117\nStep Functions\nWait\nStart\nExecute\nEnd\nIs it time to run?\nRecurse\nFigure 7.14\nA revised state \nmachine design that can \nsupport scheduled tasks that \nare more than one year away\nEXTEND THE SCHEDULED TIME BEYOND ONE YEAR\nThe maximum time a state machine execution can run for\nis one year. As such, the maximum amount of time a Wait\nstate can wait for is also one year. However, we can extend\nthis limitation by borrowing the idea of tail recursion\n(https://www.geeksforgeeks.org/tail-recursion/) \nfrom\nfunctional programming. Essentially, at the end of a Wait\nstate, we can check if we need to wait for even more time.\nIf so, the state machine starts another execution of itself\nand waits for another year, and so on. Until eventually, we\narrive at the task’s scheduled time and run the task.\n This is similar to a tail recursion because the first exe-\ncution does not need to wait for the recursion to finish.\nIt simply starts the second execution and then proceeds\nto complete itself. See figure 7.14 for how this revised\nstate machine might look.\nSCALING FOR HOTSPOTS\nSometimes, just raising the soft limits on the number of\nStateTransitions per second alone is not going to be\nenough. Because the default limits have a bucket size of\n5,000 (the initial burst limit) and a refill rate of 1,500\nper second, if we are to support running 1,000,000 tasks\naround the same time, we will need to raise these limits by multiple orders of magni-\ntude. AWS will be unlikely to oblige such a request, and we will be politely reminded\nthat Step Functions is not designed for such use cases.\n Fortunately, we can make small tweaks to the solution to make it far more scalable\nwhen it comes to dealing with hotspots. Unfortunately, we will need to trade off some\nof the precision of this solution for the new found scalability.\n For example, instead of running every task scheduled for 00:00 UTC at exactly\n00:00 UTC, we can spread them across a 1-minute window. We can do this by adding\nsome random delay to the scheduled time. Following this simple change, some of the\naforementioned tasks would be run at 00:00:12 UTC, and some would be run at\n00:00:47 UTC, for instance. This allows us to make the most of the available through-\nput. With the default limit of 5,000 bucket size and refill rate of 1,500 per second, the\nmaximum number of state transitions per minute is 93,500:\nUses all 5,000 state transitions in the first second\nUses the 1,500 refill per second for the remaining 59 seconds\nDoing this would reduce the precision to “run within a minute,” but we wouldn’t need\nto raise the default limits by nearly as much. It’ll be a trivial change to inject a variable\namount of delay (0–59 s) to the scheduled time so that tasks are uniformly distributed\nacross the minute window. With this simple tweak, Step Functions is no longer the\n",
      "content_length": 2837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "118\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nscalability bottleneck. Instead, we will need to worry about the rate limits on the\nLambda function that will run the task. \n Another alternative would be to have each state machine execution run all the\ntasks that are scheduled for the same minute in batches and in parallel. For example,\nwhen scheduling a task, add the task with the scheduled time in a DynamoDB table as\nthe HASH key and a unique task ID as the RANGE key. At the same time, atomically\nincrement a counter for the number of tasks scheduled for this timestamp. Both of\nthese updates can be performed in a single DynamoDB transaction. Figure 7.15 shows\nhow the table might look.\nWe would start a state machine execution with the timestamp as the execution name.\nBecause execution names have to be unique, the StartExe-\ncution request will fail if there’s an existing execution\nalready. This ensures that only one execution is responsi-\nble for running all the tasks scheduled for that minute\n(2020-07-04T21:53:22).\n Instead of executing the scheduled tasks immediately\nafter the Wait state, we could get a count of the number of\ntasks that need to run. From here, we would use a Map\nstate to dynamically generate parallel branches to run\nthese tasks in parallel. See figure 7.16 for how this alterna-\ntive design might look.\n Making these changes would not affect the precision\nby too much, but it would reduce the number of state\nmachine executions and Lambda invocations required.\nEssentially, we would need one state machine execution\nfor every minute when we need to run some scheduled\ntasks. There is a total of 525,600 minutes in a 365 days cal-\nendar year, so this also removes the need to increase the\nlimit on the number of open executions (again, the\ndefault limit is 1,000,000). That’s the beauty of these com-\nposable architecture components! Because there are so\nmany ways to compose them, it gives you lots of different\noptions and trade-offs.\nFigure 7.15\nSet the scheduled task as well as the count in the same DynamoDB table.\nWait\nStart\nExecute\nEnd\nGetCount\nFigure 7.16\nAn alternative \ndesign for the state machine \nthat can run tasks in batches \nin parallel\n",
      "content_length": 2203,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "119\nSQS\n7.4.4\nFinal thoughts\nStep Functions offers a simple and elegant solution that can run tasks at great preci-\nsion. The big drawback are its costs and the various service limits that you need to look\nout for, which hampers its scalability. But as you can see, if we are willing to make\ntradeoffs against precision, we can modify the solution to make it much more scalable. \n We looked at a couple of possible modifications, including taking some elements\nof the cron job solution and turning this solution into a more flexible cron job that\nonly runs when there are tasks that need to run. We also looked at a modification that\nallows us to work around the 1-year limit by applying tail recursion to the state\nmachine design. In the next solution, we’ll apply the same technique to SQS as it is\nbound by an even tighter constraint on how long a task can stay open.\n7.5\nSQS\nThe Amazon Simple Queue Service (SQS) is a fully managed queuing service. You can\nsend messages to and receive messages from the queue. Once a message has been\nreceived by a consumer, the message is then hidden from all other consumers for a\nperiod of time, which is known as the visibility timeout. You can configure the visibility time-\nout value on the queue, but the setting can also be overridden for individual messages.\n When you send a message to SQS, you can also use the DelaySeconds attribute to\nmake the message become visible at the right time. You can implement the scheduling\nservice by using these two settings to hide a message until its scheduled time. However,\nthe maximum DelaySeconds is a measly 15 minutes, and the maximum visibility time-\nout is only 12 hours. But all is not lost. \n When the execute function receives the message after the initial DelaySeconds, it\ncan inspect the message and see if it’s time to run the task (see figure 7.17). If not, it\ncan call ChangeMessageVisibility on the message to hide the message for up to\nanother 12 hours (https://amzn.to/3e1GVY6). It can do this repeatedly until it’s\nfinally time to run the scheduled task.\n Before you score this solution, consider that there is a limit of 120,000 inflight mes-\nsages. Unfortunately, this is a hard limit and cannot be raised. This limit has a pro-\nfound implication that can mean it’s not suitable for some use cases at all!\n Once a message is inflight, this solution would keep it inflight by continuously\nextending its visibility timeout until its scheduled time. In this case, the number of\ninflight messages equates to the number of open tasks. However, once you reach the\nExercise: Score the modified solutions\nRepeat the same scoring exercise against the modified solutions we proposed. \nHow much would it impact the system’s ability to handle a large number of open\ntasks or hotspots? \nIs there any additional cost impact (e.g., one of the proposed tweaks uses a\nDynamoDB table) that needs to be considered?\n",
      "content_length": 2903,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "120\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n120,000 inflight messages limit, then newer messages would stay in the queue’s back-\nlog, even if some of the newer messages might need to run sooner than the messages\nthat are already inflight. Priority is given to tasks based on when they were scheduled,\nnot by their execution time.\n This is not a desirable characteristic for a scheduling service. In fact, it’s the oppo-\nsite of what we want. Tasks that are scheduled to execute soon should be given the pri-\nority to ensure they’re executed on time. That being said, this is a problem that would\nonly arise when you have reached the 120,000 inflight messages limit. The further\naway tasks can be scheduled, the more open tasks you would have, and the more likely\nyou would run into this problem.\n7.5.1\nYour scores\nWith this severe limitation in mind, how would you score this solution? Write down\nyour scores in the empty spaces in table 7.7.\n7.5.2\nOur scores\nThis solution is best suited for scenarios where tasks are not scheduled too far away in\ntime. Otherwise, we face the prospect of accumulating a large number of open tasks\nand running into the limit on inflight messages. Also, we would need to call Change-\nMessageVisibility on the message every 12 hours for a long time. If a task is sched-\nuled to execute in a year, then that’s a total of 730 times. Multiplied that by 1,000,000\nTable 7.7\nYour solution scores for SQS\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nsendMessage with\nDelaySeconds\nChangeMessageVisibility\nMessages are hidden in the \nqueue for up to 15 minutes \nusing the DelaySeconds attribute.\nWhen the Execute function\nreceives a message, it can check\nif it’s time to run the scheduled\ntask. If so, it runs the task and\ndeletes the message from SQS.\nIf it’s not yet time to run the \nscheduled task, then the message\ncan be put back into the queue and\nhidden for up to another 12 hours.\nLambda\nScheduler\nLambda\nExecute\ntask_queue\nSQS\nFigure 7.17\nHigh-level architecture of using SQS to schedule ad hoc tasks\n",
      "content_length": 2085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "121\nSQS\ntasks and that’s a total of 730 million API requests or $292 for keeping 1,000,000 tasks\nopen for a whole year. With these in mind, table 7.8 shows our scores.\nPRECISION\nUnder normal conditions, SQS messages that are delayed or hidden are run no more\nthan a few seconds after their scheduled times. Not as precise as Step Functions, but\nstill very good. This is why we gave this solution a score of 9.\nSCALABILITY (NUMBER OF OPEN TASKS)\nWe gave this solution a low score because the hard limit of 120,000 inflight messages\nseverely limits this solution’s ability to support a large number of open tasks. Even\nthough the tasks can still be scheduled, they cannot run until the number of inflight\nmessages drops below 120,000. This is a serious hinderance and, in the worst cases,\ncan render the system completely unusable. For example, if 120,000 tasks are sched-\nuled to run in one year, then nothing else that’s scheduled after that can run until\nthose first 120,000 tasks have been run.\nSCALABILITY (HOTSPOTS)\nThe Lambda service uses long polling to poll SQS queues and only invokes our func-\ntion when there are messages (http://mng.bz/Rqyj). These pollers are an invisible\nlayer between SQS and our function, and we do not pay for them. But we do pay for\nthe SQS ReceiveMessage requests they make. According to this blog post by Randall\nHunt (https://amzn.to/31MfVtl)\nThe Lambda service monitors the number of inflight messages, and when it detects that this\nnumber is trending up, it will increase the polling frequency by 20 ReceiveMessage requests\nper minute and the function concurrency by 60 calls per minute. As long as the queue\nremains busy it will continue to scale until it hits the function concurrency limits. As the\nnumber of inflight messages trends down Lambda will reduce the polling frequency by 10\nReceiveMessage requests per minute and decrease the concurrency used to invoke our\nfunction by 30 calls per-minute.\nBy keeping the queue artificially busy with a high number of inflight messages, we are\nartificially raising Lambda’s polling frequency and function concurrency. This is use-\nful for dealing with hotspots. \n Because of the way this solution works, all open tasks are kept as inflight messages.\nThis means the Lambda service would likely be running a high number of concurrent\nTable 7.8\nOur solution scores for SQS\nScore\nPrecision\n9\nScalability (number of open tasks)\n2\nScalability (hotspots)\n8\nCost\n5\n",
      "content_length": 2444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "122\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\npollers all the time. When a cluster of messages become available at the same time, they\nwill likely be processed by the execute function with a high degree of parallelism. And\nLambda scales up the number of concurrent executions as more messages become\navailable. We can, therefore, use the autoscaling capability that Lambda offers.\n Because of this, we gave this solution a really good score. But, on the other hand,\nthis behavior generates a lot of redundant SQS ReceiveMessage requests, which can\nhave a noticeable impact on cost when running at scale.\nCOST\nBetween the many ReceiveMessage requests Lambda makes on our behalf and the\ncost of calling ChangeMessageVisibility on every message every 12 hours, most of\nthe cost for this solution will likely be attributed to SQS. While SQS is not an expen-\nsive service, at $0.40 per million API requests, the cost can accumulate quickly because\nthis solution is capable of generating many millions of requests at scale. As such, we\ngave this solution a 5, which is to say that it’s not great but also unlikely to cause you\ntoo much trouble.\n7.5.3\nFinal thoughts\nIf you put the scores for this solution side-by-side with DynamoDB TTL, you can see\nthat they perfectly complement each other. Where one is strong, the other is weak.\nTable 7.9 shows the ratings for both services.\nWhat if we can combine these two solutions to create a solution that offers the best of\nboth worlds? Let’s look at that next.\n7.6\nCombining DynamoDB TTL with SQS\nSo far, we have seen that the DynamoDB TTL solution is great at dealing with a large\nnumber of open tasks, but lacks the precision required for most use cases. Conversely,\nthe SQS solution is great at providing good precision and dealing with hotspots but\ncan’t handle a large number of open tasks. The two rather complement each other\nand can be combined to great effect.\n For example, what if long-term tasks are stored in DynamoDB until two days before\ntheir scheduled time? Why two days? Because it’s the only soft guarantee that DynamoDB\nTTL gives: \nTable 7.9\nOur ratings for SQS vs. DynamoDB TTL\nScore (SQS)\nScore (DynamoDB TTL)\nPrecision\n9\n1\nScalability (number of open tasks)\n2\n10\nScalability (hotspots)\n8\n6\nCost\n5\n10\n",
      "content_length": 2277,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "123\nCombining DynamoDB TTL with SQS\nTTL typically deletes expired items within 48 hours of expiration (https://amzn.to/\n2NRgARU).\nOnce the tasks are deleted from the DynamoDB table, they are moved to SQS where\nthey are kept inflight until the scheduled time (using the ChangeMessageVisibility\nAPI as discussed earlier). For tasks that are scheduled to execute in less than two days,\nthey are added to the SQS queue straight away. See figure 7.18 for how this solution\nmight look.\n7.6.1\nYour scores\nHow would you score this solution? Again, write your scores in the empty spaces in\ntable 7.10.\nTable 7.10\nYour solution scores for DynamoDB TTL with SQS\nScore\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\nShort-term task\nLong-term task\nOnly long-term tasks are stored in\nthis table. Their TTL is set to be two\ndays before their scheduled time.\nShort-term tasks (to be run in less\nthan two days) are pushed to SQS\nright away, bypassing DynamoDB.\nUnlike in the DynamoDB TTL\nsolution, this function doesn’t run \nthe task but forwards it to SQS.\nLike the SQS solution, this\nfunction checks each message\nto see if it should run the task \nor hide it again using the\nChangeMessageVisibility API.\nDynamoDB\nDynamoDB Streams\nLambda\nscheduled_tasks\nREMOVE events\nRescheduler\nLambda\nScheduler\nChangeMessageVisibility\nLambda\nExecute\ntask_queue\nSQS\nFigure 7.18\nHigh-level architecture of combining DynamoDB TTL with SQS\n",
      "content_length": 1431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "124\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n7.6.2\nOur scores\nAccording to “The Fundamental theorem of software engineering” (https://dzone\n.com/articles/why-fundamental-theorem):\nWe can solve any problem by introducing an extra level of indirection.\nLike the other alternative solutions we saw earlier in this chapter, this solution solves\nthe problems with an existing solution by introducing an extra level of indirection. It\ndoes so by composing different services together in order to make up for the short-\ncomings of each. Take a look at table 7.11 for our scores for this solution, then we’ll\ndiscuss how we arrived at these scores.\nPRECISION\nAs all the executions go through SQS, this solution has the same level of precision as\nthe SQS-only solution, 9.\nSCALABILITY (NUMBER OF OPEN TASKS)\nStoring long-term tasks in DynamoDB largely solves SQS’s problem with scaling the\nnumber of open tasks. However, it is still possible to run into the 120,000 inflight mes-\nsages limit with just the short-term tasks. It’s far less likely, but it is still a possibility that\nneeds to be considered. Hence, we marked this solution as an 8.\nSCALABILITY (HOTSPOTS)\nAs all the executions go through SQS, this solution has the same score as the SQS-only\nsolution, 8.\nCOST\nThis solution eliminates most of the ChangeMessageVisibility requests because all the\nlong-term tasks are stored in DynamoDB. This cuts out a large chunk of the cost\nassociated with the SQS solution. However, in return, it adds additional costs for\nDynamoDB usage and Lambda invocations for the reschedule function. Overall, the\ncosts this solution takes away are greater than the new costs it adds. Hence, we gave it\na 7, improving on the original score of 5 for the SQS solution.\nTable 7.11\nOur solution scores for DynamoDB TTL with SQS\nScore\nPrecision\n9\nScalability (number of open tasks)\n8\nScalability (hotspots)\n8\nCost\n7\n",
      "content_length": 1906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "125\nThe applications\n7.6.3\nFinal thoughts\nThis is just one example of how different solutions or aspects of them can be com-\nbined to make a more effective answer. This combinatory effect is one of the things\nthat makes cloud architectures so interesting and fascinating, but also, so complex\nand confusing at times. There are so many different ways to achieve the same goal,\nand depending on what your application needs, there’s usually no one-size-fits-all solu-\ntion that offers the best results for all applications.\n So far, we have only looked at the supply side of the equation and what each solu-\ntion can offer. We haven’t looked at the demand side yet or what application needs\nwhat. After all, depending on the application, you might put a different weight\nbehind each of the nonfunctional requirements. Let’s try to match our solutions to\nthe right application next.\n7.7\nChoosing the right solution for your application\nTable 7.12 shows our scores for the five solutions that we considered in this chapter.\nThe solutions in this table do not include the proposed tweaks. Depending on the\napplication, some of these requirements might be more important than others.\n7.8\nThe applications\nLet’s consider three applications that might use the ad hoc scheduling service:\nApplication 1 is a reminder app, which we’ll call RemindMe. \nApplication 2 is a multi-player app for a mobile game, which we’ll call Tourna-\nmentsRUs.\nApplication 3 is a healthcare app that digitizes and manages your consent for\nsharing your medical data with care providers, which we’ll call iConsent.\nIn the reminder app, RemindMe, users can create reminders for future events, and\nthe system will send SMS/push notifications to the users 10 minutes before the event.\nWhile reminders are usually distributed evenly over time, there are hotspots around\npublic holidays and major sporting events such as the Super Bowl. During these\nhotspots, the application might need to notify millions of users. Fortunately, because\nTable 7.12\nOur scores for all five solutions\nCron job\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\n6\n1\n10\n9\n9\nScalability (number of \nopen tasks)\n10\n10\n7\n2\n8\nScalability (hotspots)\n2\n6\n4\n8\n8\nCost\n7\n10\n2\n5\n7\n",
      "content_length": 2225,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "126\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nthe reminders are sent 10 minutes before the event, the system gives us some slack in\nterms of timing.\n In application 2, the multi-player mobile app called TournamentsRUs, players\ncompete in user-generated tournaments that are 15–30 minutes long. As soon as the\nfinal whistle blows, the tournament ends and all participants wait for a winner to be\nannounced via an in-game popup. TournamentsRUs currently has 1.5 million daily\nactive users (DAU) and hopes to double that number in 12 months’ time. At peak, the\nnumber of concurrent users is around 5% of its DAU, and tournaments typically con-\nsist of 10–15 players each.\n In application 3, the healthcare app iConsent, users fill in digital forms that allow\ncare providers to access their medical data. Each consent has an expiration date, and the\napp needs to change its status to expired when the expiration date passes. iConsent cur-\nrently has millions of registered users, and users have an average of three consents.\n Each of these applications need to use a scheduling service to run ad hoc tasks at spe-\ncific times, but their use cases are drastically different. Some deal with tasks that are\nshort-lived, while others allow tasks to be scheduled for any future point in time. Some\nare prone to hotspots around real-world events; others can accumulate large numbers\nof open tasks because there is no limit to how far away tasks can be scheduled.\n To help us better understand which solution is the best for each application, we\ncan apply a weight against each of the nonfunctional requirements. For example,\nTournamentsRUs cares a lot about precision because users will be waiting for their\nresults at the end of a tournament. If the tasks to finalize tournaments are delayed,\nthen it can negatively impact the users’ experience with the app.\n7.8.1\nYour weights\nFor each of the applications, write a weight between 1 (“I don’t care”) and 10 (“This is\ncritical”) for each of the nonfunctional requirements in table 7.13. Remember, there\nare no right or wrong answers here! Use your best judgement based on the limited\namount of information you know about each app.\n7.8.2\nOur weights\nTable 7.14 shows our weightings. Are these scores similar to yours? We’ll go through\neach application and talk about how we arrived at these weights in the sections follow-\ning the table.\nTable 7.13\nYour ratings for RemindMe, TournamentsRUs, and iConsent\nRemindMe\nTournamentsRUs\niConsent\nPrecision\nScalability (number of open tasks)\nScalability (hotspots)\nCost\n",
      "content_length": 2565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "127\nThe applications\nREMINDME\nWe gave Precision a weight of 5 for this app because reminders are sent 10 minutes\nbefore the event. This gives us a lot of slack. Even if the reminder is sent 5 minutes\nlate, it’s still OK.\n Scalability (number of open tasks) gets a weight of 10 because there are no upper\nbounds on how far out the events can be scheduled. At any moment, there can be mil-\nlions of open reminders. This makes scaling the number of open tasks an absolutely\ncritical requirement for this application. For Scalability (hotspots), we gave a weight of\n8 because large hotspots would likely form around public holidays (for example,\nmother’s day) and sporting events (for example, the Super Bowl or the Olympics).\n Finally, for Cost, we gave it a weight of 3. This perhaps reflects our general attitude\ntowards the cost of serverless technologies. Their pay-per-use pricing allows our cost to\ngrow linearly when scaling. Generally speaking, we don’t want to optimize for cost\nunless the solution is going to burn down the house!\nTOURNAMENTSRUS\nFor TournamentsRUs, Precision gets a weight of 10 because when a tournament fin-\nishes, players will all be waiting for the announcement of the winner. If the scheduled\ntask (to finalize the tournament and calculate the winner) is delayed for even a few\nseconds, it would provide a bad user experience.\n We gave Scalability (number of open tasks) a weight of 6 because only a small per-\ncentage of its DAUs are online at once and because of the short duration of its tourna-\nments. At 1.5 M DAU, 5% concurrent users at peak and an average of 10–15 players in\neach tournament, these numbers translate to approximately 5,000–7,500 open tour-\nnaments during peak times.\n For Scalability (hotspots), it received a lowly 3 because the tournaments are user-\ngenerated and have different lengths of time (between 15–30 minutes). It’s unlikely\nfor large hotspots to form under these conditions. And, as with RemindMe, we gave\nCost a weight of 3 (just don’t burn my house down!).\nICONSENT\nLastly, for iConsent, Precision received a weight of 4. When a consent expires, it\nshould be shown in the UI with the correct status. However, because the user is proba-\nbly not going to check the app every few minutes for updates, it’s OK if the status is\nupdated a few minutes (or maybe even an hour) later.\nTable 7.14\nOur scores for RemindMe, TournamentsRUs, and iConsent\nRemindMe\nTournamentsRUs\niConsent\nPrecision\n5\n10\n4\nScalability (number of open tasks)\n10\n6\n10\nScalability (hotspots)\n8\n3\n1\nCost\n3\n3\n3\n",
      "content_length": 2541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "128\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\n We gave a weight of 10 for Scalability (number of open tasks). This is because med-\nical consents can be valid for a year or sometimes even longer: all of these active con-\nsents are open tasks, so the system would have millions of open tasks at any moment.\nFor Scalability (hotspots) on the other hand, we gave it a weight of 1 because there is\njust no natural clustering that can lead to hotspots. And finally, cost gets a weight of 3\nbecause that’s just how we generally feel about cost for serverless applications.\n7.8.3\nScoring the solutions for each application\nSo far, we have scored each solution based on its own merits. But this says nothing about\nhow well suited a solution is to an application because, as we have seen, applications have\ndifferent requirements. By combining a solution’s scores with an application’s weights,\nwe can arrive at something of an indicative score of how well they are suited for each\nother. Let’s show you how this can be done and then you can do this exercise yourself.\nIf you recall, the following table shows our scores for the cron job solution:\nFor RemindMe, we gave the following weights:\nNow, if we multiple the score with the weight for each nonfunctional requirement, we\nwill arrive at the scores in the following table:\nScore (cron job)\nPrecision\n6\nScalability (number of open tasks)\n10\nScalability (hotspots)\n2\nCost\n7\nWeight (RemindMe)\nPrecision\n5\nScalability (number of open tasks)\n10\nScalability (hotspots)\n8\nCost\n3\nWeighted Score (Cron job × RemindMe)\nPrecision\n6 × 5 = 30\nScalability (number of open tasks)\n10 × 10 = 100\nScalability (hotspots)\n2 × 8 = 16\nCost\n7 × 3 = 21\n",
      "content_length": 1689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "129\nThe applications\nThis adds up to a grand total of 30 + 100 + 16 + 21 = 167. On its own, this score means\nvery little. But if we repeat the exercise and score each of the solutions for RemindMe,\nthen we can see how well they compare with each other. This would help us pick the\nmost appropriate solution for RemindMe, which might be different than the solution\nyou would use for TournamentsRUs or iConsent. \n With that in mind, use the whitespace in table 7.15 to calculate your weighted\nscores for each of the five solutions that we have discussed so far for RemindMe. Then\ndo the same for TournamentsRUs and IConsent in tables 7.16 and 7.17, respectively.   \nTable 7.15\nWeighted scores for the app RemindMe\nCron\njob\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\nScalability (number of \nopen tasks)\nScalability (hotspots)\nCost\nTotal score\nTable 7.16\nWeighted scores for the app TournamentsRUs\nCron\njob\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\nScalability (number of \nopen tasks)\nScalability (hotspots)\nCost\nTotal score\nTable 7.17\nWeighted scores for the app iConsent\nCron\njob\nDynamoDB\nTTL\nStep\nFunctions\nSQS\nSQS + \nDynamoDB TTL\nPrecision\nScalability (number of \nopen tasks)\nScalability (hotspots)\nCost\nTotal score\n",
      "content_length": 1255,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "130\nCHAPTER 7\nBuilding a scheduling service for ad hoc tasks\nDid the scores align with your gut instinct for which solution is best for each applica-\ntion? Did you find something unexpected in the process? Did some solutions not fare\nas well as you thought they might?\n If you find any uncomfortable outcomes as a result of these exercises, then they\nhave done their job. The purpose of defining requirements up front and putting a\nweight against each requirement is to help us remain objective and combat cognitive\nbiases. Table 7.18 shows our total weighted scores for each solution and application.\nThese scores give you a sense as to which solutions are best suited for each applica-\ntion. But they don’t give you definitive answers and you shouldn’t follow them blindly.\nFor instance, there are often factors that aren’t included in the scoring scheme but\nneed to be taken into account nonetheless. Factors such as complexity, resource con-\nstraints, and familiarity with the technologies are usually important for real-world\nprojects.\nSummary\nIn this chapter, we analyzed five different ways to implement a service for executing ad\nhoc tasks, and we judged these solutions on the nonfunctional requirements we set\nout at the start of the chapter. Throughout the chapter we asked you to think critically\nabout how well each solution would perform for these nonfunctional requirements\nand asked you to rate them. And we shared our scores with you and our rationale for\nthese scores. We hope through these exercises you have gained some insights into\nhow we approach problem solving and the considerations that goes into evaluating a\npotential solution:\nWhat are the relevant service limits and how do they affect the scalability\nrequirements of the application?\nWhat are the performance characteristics of the services in question and do\nthey match up with the application’s needs?\nHow are the services charged? Project the cost of the application by thinking\nthrough how the application would need to use these AWS services and apply-\ning the services’ billing model to that usage pattern.\nTable 7.18\nOur total weighted scores for RemindMe, TournamentsRUs, and iConsent\nRemindMe\nTournamentsRUs\niConsent\nCron job\n167\n147\n147\nDynamoDB TTL\n180\n115\n137\nStep Functions\n158\n160\n120\nSQS\n144\n141\n79\nDynamoDB TTL with SQS\n210\n183\n145\n",
      "content_length": 2333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "131\nSummary\nThese points are a lot easier said than done and it takes practice to become proficient\nat them. The AWS services are always evolving and new services and features become\navailable all the time so you also have to continuously educate yourself as new options\nand techniques emerge. Despite having worked with AWS for over a decade, we are\nstill learning ourselves and having to constantly update our own understanding of\nhow different AWS services operate.\n Furthermore, AWS do not publish the performance characteristics for many of its\nservices. For example, how soon does Step Functions execute a Wait state after the\nspecific timestamp. If your solution depends on assumptions about these unknown\nperformance characteristics, then you should design small experiments to test your\nassumptions. In the course of writing this chapter, we conducted several experiments\nto find out how soon Step Functions and SQS executes delayed tasks. Failing to vali-\ndate these assumptions early can have devastating consequences. Months of engineer-\ning work might go to waste if an entire solution was built on false assumptions.\n At the end of the chapter we also asked you to do an exercise of finding the best\nsolution for a given problem and gave you three example applications, each with a dif-\nferent set of needs. The scoring method we asked you to apply is not fool-proof but\nhelps you make objective decisions and combat confirmation biases.\n As you brainstormed and evaluated the solutions that have been put in front of you\nin this chapter, I hope you picked up on the even more important lessons here: that\nall architecture decisions have inherit tradeoffs and not all application requirements\nare created equally. The fact that different applications care about the characteristics\nof its architecture to different degrees gives us a lot of room to make smart tradeoffs.\n There are so many different AWS services to choose from, each offering a different\nset of characteristics and tradeoffs. When you use different services together, they can\noften create interesting synergies. All of these give us a wealth of options to mix and\nmatch different architectural approaches and to engage in a creative problem-solving\nprocess, and that’s beautiful!\n",
      "content_length": 2262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "132\nArchitecting serverless\n parallel computing\nThere’s a secret about AWS Lambda that we like to tell people: it’s a supercomputer\nthat can perform faster than the largest EC2 instance. The trick is to think about\nLambda in terms of parallel computation. If you can divide your problem into\nhundreds or thousands of smaller problems and solve them in parallel, you will get\nto a result faster than if you try to solve the same problem by moving through it\nsequentially. \n Parallel computing is an important topic in computer science and is often talked\nabout in the undergraduate computer science curriculum. Interestingly, Lambda,\nby its very nature, predisposes us to think and apply concepts from parallel comput-\ning. Services like Step Functions and DynamoDB make it easier to build parallel\napplications. \n In this chapter, we’ll illustrate how to build a serverless video transcoder in\nLambda that outperforms bigger and more expensive EC2 servers.\nThis chapter covers\nPrinciples of MapReduce\nServerless solution with Step Functions and EFS\n",
      "content_length": 1051,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "133\nIntroduction to MapReduce\n8.1\nIntroduction to MapReduce\nMapReduce is a popular and well-known programming model that’s often used to\nprocess large data sets. It was originally created at Google by developers who were\nthemselves inspired by two well-known functional programming primitives (higher-\norder functions): map and reduce. MapReduce works by splitting up a large data set\ninto many smaller subsets, performing an operation on each subset and then combin-\ning or summing up to get the result. \n Imagine that you want to find out how many times a character’s name is men-\ntioned in Tolstoy’s War and Peace. You can sequentially look through every page, one\nby one, and count the occurrences (but that’s slow). If you apply a MapReduce\napproach, however, you can do it much quicker:\n1.\nYou split the data into many independent subsets. In the case of War and Peace,\nit could be individual pages or paragraphs. \n2.\nYou apply the map function to each subset. The map function in this case scans\nthe page (or paragraph) and emits how many times a character’s name is\nmentioned. \n3.\nThere could be an optional step here to combine some of the data. That can\nhelp make the computation a little easier to perform in the next step. \n4.\nThe reduce function performs a summary operation. It counts the number of\ntimes the map function has emitted the character’s name and produces the\noverall result.\nNOTE\nIt’s important to understand that the power of MapReduce in the War\nand Peace example comes from the fact that the map step can run in parallel\non thousands of pages or paragraphs. If this wasn’t the case, then this pro-\ngram would be no different from a sequential count. \nFigure 8.1 shows what a theoretical MapReduce architecture looks like. We’ll build\nsomething like this soon.\n As you may have already guessed, real-world MapReduce applications are often\nmore complex. In a lot of cases, there are intermediary steps between map and reduce\nthat combine or simplify data, and considerations such as locality of data become\nimportant in order to minimize overhead. Nevertheless, we can take the idea of split-\nting a problem into smaller chunks, processing them in parallel, and then combining\nand reducing them to achieve the outcome you need, and we can do that with\nLambda. \n \n \n \n \n \n \n",
      "content_length": 2301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "134\nCHAPTER 8\nArchitecting serverless parallel computing\n8.1.1\nHow to transcode a video\nIn the second chapter of this book, you built a serverless pipeline that converted video\nfrom one format to another. To do this, you used an AWS service called AWS Elemen-\ntal MediaConvert. This service takes a video uploaded to an S3 bucket and transcodes\nit to a range of different formats specified by you. Our goal in this chapter is to do\nsomething crazy and implement our own video transcoder service using Lambda.\nNote that this is just an experiment and an opportunity to explore highly parallelized\nserverless architectures. Our major requirements for our serverless encoding service\nare as follows:\nBuild a transcoder that takes a video file and produces a transcoded version in a\ndifferent format or bit rate. We want complete control over the transcoding\nprocess.\nUse only serverless services in AWS, such as Lambda and S3. Obviously, we are\nnot allowed to use an EC2 or a managed service to do transcoding for us.\nBuild a product that is robust and fast. It should be able to beat a large EC2\ninstance most of the time. \nLearn about parallel computing and how to think about solving these problems. \nThe solution we are about to present works, but it’s not something we recommend\nrunning in a production environment. Unless your core business is video transcoding,\nA split procedure splits the data \nset into multiple chunks.\nA map procedure performs\nfiltering and sorting.\nA combine procedure (optional)\nreduces data to a simplified form.\nA reduce procedure performs \nsummary options.\nSplit\nReduce\nMap\nMap\nMap\nMap\nMap\nProblem\nCombine\nCombine\nCombine\nFigure 8.1\nThese are the steps a fictional MapReduce algorithm could take.\n",
      "content_length": 1731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "135\nIntroduction to MapReduce\nyou should outsource as much of the undifferentiated heavy lifting as possible in\norder to focus on your own unique problem. In most cases, a managed service like\nAWS Elemental MediaConvert is better; you don’t have to worry about keeping it run-\nning. Take this as just an exercise and an opportunity to learn about MapReduce and\nparallel computation (you never know when you might face a big problem that\nrequires the skill you may pick up here).\n8.1.2\nArchitecture overview\nTo transcode a file using Lambda, we need to apply principles of MapReduce. We are\nnot implementing classical MapReduce here; instead, we are taking inspiration from\nthis algorithm to build our transcoder. \n An interesting thing about Lambda (that we’ve mentioned before) is that it forces\nus to think parallel. It’s impossible to process large video files in a Lambda function if\nyou treat a single function like a traditional computer. You’d run out of memory and\ntimeout quickly. If the video file is large enough, the function would either stop after\n15 minutes of processing or exhaust all available RAM and crash. \n To get around this, we decompose the problem into smaller problems that can be\nprocessed within Lambda’s constraints. The implication is that we can try the follow-\ning to process a large video file:\n1.\nDivide the video file into a lot of tiny segments. \n2.\nTranscode these segments in parallel.\n3.\nCombine these small segments together to produce a new video file.\nWe need to parallelize as much as possible to get the most out of our serverless super-\ncomputer. For example, if some segments are ready to be combined while others are\nstill processing, we should combine the ones that are ready. Performance is the name\nof the game here. So, with that in mind, here’s an outline for our serverless transcod-\ning algorithm that, let’s say, is designed to transcode a video from one bit rate to\nanother:\n1.\nA user uploads a video file to S3 that invokes a Lambda function.\n2.\nThe Lambda function analyzes the file and figures out how to cut up the source\nfile to produce smaller video files (segments) for transcoding.\n3.\nTo make things go a little bit faster, we strip away the audio from video and save\nit to another file. Not having to worry about the audio makes executing steps\n4–6 faster. \nHow would you do video transcoding in Lambda?\nTake a moment and think about how you would build a video transcoder using AWS\nLambda. All guesses are good, and we’d love to know how you would approach the\nproblem (twitter.com/sbarski). Can you build it yourself without reading the rest of the\nchapter?\n",
      "content_length": 2626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "136\nCHAPTER 8\nArchitecting serverless parallel computing\n4.\nThis step performs the split process that creates small video segments for\ntranscoding.\n5.\nNow comes the map process that transcodes segments to the desired format or\nbit rate. The system can transcode a bunch of these segments in parallel.\n6.\nThe map process is followed by a combine step that begins to merge these small\nvideo files together.\n7.\nThe final step merges audio and video together and presents the file to the\nuser. We have reduced our work to its final output. \n8.\nAs the kids say, the real final step is profit.\nHere are the main AWS services and software that we will use to build the transcoder:\nFFmpeg—In the first edition of our book, we briefly used FFmpeg, a cross-platform\nlibrary/application created for recording, converting, and streaming audio and\nvideo. This is a powerhouse of an application that is used by everyone and anyone\nranging from hobbyists to commercial TV channels. \nWe’ll use ffmpeg in this chapter to do the transcoding, splitting, and merg-\ning of video files. We’ll also use a utility called ffprobe to analyze the video file\nand figure out how to cut it up on keyframes. The ffmpeg and ffprobe binaries\nare shipped as a Lambda layer (http://mng.bz/N4MN), which allow other\nLambda functions to access and run them. You don’t necessarily have to use\nLambda layers (you can upload the ffmpeg binary with each function that will\nuse it), but that is redundant, inconvenient, and takes a long time to deploy.\nTherefore, making ffmpeg available via a Lambda layer is the recommended\nand preferred approach.\nDEFINITION\nA keyframe stores the complete image in the video stream, while a\nregular frame stores an incremental change from the previous frame. Cutting\non keyframes prevents us from losing information in the video. \nAWS Lambda—It goes without saying that we’ll use Lambda for nearly every-\nthing. Lambda runs ffmpeg and executes most of the logic. We’ll write six func-\ntions to analyze the video, extract audio, split the original file, convert\nsegments, merge video segments, and then merge video and audio in the final\nstep. \nStep Functions—To help us orchestrate this workflow, we’ll rely on Step Func-\ntions. This service helps us to define how and in what order our execution steps\nhappen, makes the entire workflow robust, and provides additional visibility\ninto the execution. \nS3—We’ll use Simple Storage Service (S3) for storing the initial and the final\nvideo. We could also use it to store the temporary video chunks, but we’ll use\nEFS for that. The reason why we chose EFS is because it is easy to mount and\naccess as a filesystem from Lambda. We’ll provide an alternative implementa-\ntion that uses S3, but it is slightly harder to get right.\n",
      "content_length": 2769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "137\nArchitecture deep dive\nEFS—We’ll use the Elastic File System (EFS) for our serverless transcoder to\nstore the temporary files that we generate. There will be a lot of small video\nfiles. Luckily, EFS can grow and shrink as needed.\nDynamoDB—Although Step Functions help to manage the overall workflow and\nexecution of Lambda functions, we still need to maintain some state. We need\nto know whether certain video chunks were created or can be merged. We’ll use\nDynamoDB to store this information. Everything that’s stored is ephemeral and\nwill be deleted using DynamoDB’s Time to Live (TTL) feature.\nFigure 8.2 shows a high-level overview of the system we are about to build. We will\njump into individual components in the next section. \n8.2\nArchitecture deep dive\nLet’s explore the architecture in more detail. There’s nuance to the implementation\nand how things work. To avoid dealing with some pain later, let’s plan how we will\ndesign, build, and deploy the serverless transcoder. Before going any further, recall\nthat the entire idea is to split a giant video file into many small segments, transcode\nthese segments in parallel, and then merge them together into a new file.\nSimple Storage Service (S3)\n(source file)\nAWS Lambda\n(Transcode Video)\nAWS Lambda \n(Merge Video and Audio)\nSimple Storage Service\n(new File)\nDynamoDB\nEFS\nAWS Lambda\n(Merge Video)\nMultiple Lambda’s run here in \nparallel to split the file using the \nDynamic Parallelism feature of \nStep Functions.\nAWS Cloud\nAWS Step Functions workflow\nAWS Lambda\n(Split Audio)\nEFS\nDynamoDB\nEFS\nDynamoDB\nAWS Lambda \n(Split and Convert Video)\nEFS\nDynamoDB\nStart\nFigure 8.2\nThis is a simplified view of the serverless transcoder. There are a few more components to \nit, but we’ve avoided including them in this figure to focus on the essential elements of the architecture.\n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "138\nCHAPTER 8\nArchitecting serverless parallel computing\n8.2.1\nMaintaining state\nWe’ll use DynamoDB to maintain state across the entire operation of the serverless\npipeline. It’ll keep track of which videos have been created and which haven’t. To sim-\nplify the pipeline and, in particular, to simplify the code that monitors which segments\nhave been created or transcoded, we are going to use a trick. (Before going any fur-\nther, think about how you would keep track of all small video segments that have been\ncreated, transcoded, and merged given that segments will be created and processed in\nparallel.) \n The trick is to create n^2 smaller video segments. Out of one large video file, we\nshould generate 2, or 4, or 8, or 256, or 512, or more segments. Just remember to\nkeep the number of segments at n^2. Why is this? The idea is that once we’ve created\nand transcoded our video segments, we can begin merging them in any order. Having\nn^2 segments easily allows us to identify which two neighbor segments can be merged.\nAnd, we can keep track of this information in the database. \n We’ll create a basic binary tree that helps to make the logic around this algorithm\na little easier to manage. Let’s imagine that we have 8 segments. Here’s how the pro-\ncess could take place: \n1.\nSegments 3 and 4 are transcoded quicker than the rest and can be merged\ntogether (they are neighbors) into a new segment called 3-4.\n2.\nThen segment 7 is created, but segment 8 is not yet available. The system waits\nfor segment 8 to become ready before merging 7 and 8 together. \n3.\nSegment 8 is created and segments 7 and 8 can be merged together into a new\nsegment 7-8.\n4.\nThen segments 1 and 2 finish transcoding and are merged into a segment 1-2. \n5.\nThe good news is that a neighboring segment 3-4 is already available. Therefore,\nsegments 3-4 and 1-2 can be merged together into a new segment called 1-4.\n6.\nSegments 5 and 6 are transcoded and are merged into a segment 5-6. \n7.\nSegment 5-6 has a neighboring segment 7-8. These two segments are merged\ntogether to create segment 5-8. \n8.\nFinally, segments 1-4 and 5-8 can be merged together to create the final video\nthat consists of segments 1 to 8. \nBecause we have n^2 segments, we can keep track of neighboring segments and\nmerge them as soon as both neighbors (the left and the right) become available.\nAnother interesting aspect is that segments themselves can figure out who their neigh-\nbors are for merging. A segment with an index that is cleanly divisible by 2 is always on\nthe right, whereas a segment that is not cleanly divisible by 2 is on the left. For exam-\nple, a segment with an index of 6 is divisible by 2, therefore, we can figure that it’s on\nthe right, and the neighbor it needs to merge with (when it becomes available) has an\nindex of 5. Figure 8.3 illustrates how blocks can be merged together.\n DynamoDB is an excellent tool for keeping track of which segments have been cre-\nated and merged. In fact, we will precompute all possible segments and add them to\n",
      "content_length": 3029,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "139\nArchitecture deep dive\nDynamoDB. Then we will atomically increment counters in DynamoDB to have a\nrecord of when segments have been created and merged. This allows the processing\nengine to figure out which blocks haven’t been merged yet and which need to be\ndone next. \n This is an important part of the algorithm, so it’s worth restating it again. Each\nrecord in DynamoDB represents two neighboring segments (for example, segment 1\nand segment 2). The split-and-convert operation increments a confirmation counter\neach time a segment is created. When the confirmation counter equals 2, our system\nknows that the two neighboring segments exist and that they can be merged together. \n This information and logic are used in the Split and Convert function and in the\nMerge Video function. Our algorithm continues to merge segments and increment\nthe confirmation counter in DynamoDB until there’s nothing left to merge.\nNeighboring segments\nare merged as soon as\nthey are ready.\nFinal video\nSegment 1-8\nMerge\nSegment 1-4\nMerge\nSegment 5-8\nSegment 1\nSegment 2\nSegment 3\nSegment 4\nSegment 5\nSegment 6\nSegment 7\nSegment 8\nMerge\nSegment 1-2\nMerge\nSegment 3-4\nMerge\nSegment 5-6\nMerge\nSegment 7-8\nFigure 8.3\nSegments are merged together into a new video. The beauty of our engine is that neighboring \nsegments can be merged as soon as they are ready. There’s no need to wait for other, nonrelated, segments \nto finish processing.\nThere are more ways than one to do it\nOur use of a binary tree is just one approach to solving this problem, keeping track\nof segments and ultimately merging them together. There are myriad other ways this\ncan be done. How would you do it if you had to come up with a different approach?\n",
      "content_length": 1713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "140\nCHAPTER 8\nArchitecting serverless parallel computing\nTRANSCODE VIDEO\nOur serverless transcoder kicks off once we upload a file into an S3 bucket. An S3\nevent invokes the Transcode Video Lambda and the process begins. Figure 8.4 shows\nwhat this looks like.\n The Transcode Video function performs the following steps:\n1.\nDownloads the file from S3 to a local directory on EFS.\n2.\nAnalyzes the downloaded video file and extracts metadata from it. Video key-\nframes are provided in this metadata.\n3.\nCreates the necessary directories in EFS for all future segments that are going\nto be created.\n4.\nWorks out how many segments need to be created based on the number of\nkeyframes. \nRemember that we are always creating n^2 segments. This means that we\nmay have to create some fake segments in DynamoDB. These will not really do\nanything. They are considered as segments that have already been created, so\nthey don’t need to be processed.\n5.\nCreates the necessary records in DynamoDB including any fake records that are\nneeded.\n6.\nRuns a Step Functions workflow with two different inputs. The first parameter\ntells Step Functions to run a Lambda to extract and save the audio to EFS. The\nAWS Cloud\nThe event that invokes Lambda \ncarries information about the \nvideo file uploaded to S3.\nSimple Storage Service (S3)\n(source file)\nAWS Lambda\n(Transcode Video)\nFigure 8.4\nThis is a basic and common serverless architecture. Invoking code from S3 is the bread and \nbutter of Lambda functions.\n",
      "content_length": 1486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "141\nArchitecture deep dive\nsecond parameter is an array of objects that specify the start and end times of all\nsegments that need to be created. Step Functions takes this array and applies\nthe Map procedure. It fans out and creates a Lambda function for each object\nin the array, thus causing the original file to be split up by many Lambda func-\ntions in parallel.\nThe Transcode Video function is an example of a monolithic or “fat” Lambda func-\ntion because it does a lot of different things. It may not be a bad idea to split it up, but\nthat also comes with its own set of tradeoffs. In the end, whether you think this func-\ntion should be kept together or not may depend on your personal taste and philoso-\nphy. We think that this function is a pragmatic solution for what we need to do, but we\nwouldn’t be aghast if you decided to split it.\n8.2.2\nStep Functions\nStep Functions plays a central role in our system. This service orchestrates and runs\nthe main workflow that splits the video file into segments, transcodes them, and then\nmerges them. Step Functions also run a function that extracts the audio from the\nvideo file and saves it to EFS for safekeeping. The Lambda functions that Step Func-\ntions invoke include:\nSplit Audio—Extracts the audio from the video and saves it as a separate file in\nEFS.\nSplit and Convert Video—Splits the video file from a particular start point (for\nexample, 5 minutes and 25 seconds) to an end point (such as 6 minutes and 25\nseconds) and then encodes the new segment to a different format or bit rate.\nMerge Video—Merges segments together after they have been transcoded. Multi-\nple Merge Video functions will run to merge segments until one final video file\nis produced.\nMerge Video and Audio—Merges the newly created video file and the audio file to\ncreate the final output. This function uploads the new file back to S3.\nTIP\nYou don’t have to extract the audio from the video and then transcode\njust the video file separately. We decided to do that because in our tests, our sys-\ntem ran a bit faster when the video was processed on its own and then recom-\nbined with the audio. However, your mileage may vary, so we recommend that\nyou test video transcoding with and without extracting the audio first. \nStep Functions is a workflow engine that is fairly customizable. It supports different\nstates like Task (this invokes a Lambda function or passes a parameter to the API of\nanother service) or Choice (this adds branching logic). \n The one important state that we’ll use is Map. This state takes in an array and exe-\ncutes the same steps for each entry in that array. Therefore, if we pass in an array with\ninformation on how to cut up a video into segments, Map runs enough Lambda func-\ntions to process all of those arguments in parallel. This is exactly what we are going to\nbuild. We will pass an array to a Split and Convert Lambda function using the Map\n",
      "content_length": 2916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "142\nCHAPTER 8\nArchitecting serverless parallel computing\ntype. Step Functions will create as many functions as necessary to cut the original\nvideo into segments. \n Here comes the more interesting part. As soon as the segments are created, Step\nFunctions begins calling the Merge Video function until all segments are merged into\na new video. We’ll add some logic to the Step Functions execution workflow to figure\nout if Merge Video needs to be called. Once all Merge Video tasks are called and pro-\ncessed, Step Functions will take the result from Split Audio and from Merge Video\nand invoke the final Merge Video and Audio function. Figure 8.5 shows what this pro-\ncess looks like.\nNow that you know what the Step Functions workflow does, let’s discuss each of the\nLambda functions in more detail.\nSPLIT AUDIO\nStep Functions runs the Split Audio Lambda function to extract audio from the video\nfile. As we’ve mentioned, this step is done to accelerate the overall workflow because,\nfrom there on, the audio portion of the file isn’t considered, and only the video por-\ntion is transcoded to another bit rate. We don’t have to do this. We can leave audio\nAWS Step Functions workflow\nThis is the only tricky bit about \nStep Functions. The Merge Video \nfunction must repeat until all \nvideo segments have been merged.\nMerge\nvideo\nchoice\nTask: Choice\nTask: Pass\nAWS Lambda (Merge Video)\nThis logic is running \nusing the Map state.\nAWS Cloud\nAWS Lambda (Split and Convert Video)\nAWS Lambda\n(Split Audio)\nMerge\naudio\ntask\nAWS Lambda \n(Merge Video and Audio)\nStart\nFigure 8.5\nThe Step Functions execution workflow does all the work in our transcoder. The video is split, \nconverted, and merged again using two main functions and a bit of logic.\n",
      "content_length": 1740,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "143\nArchitecture deep dive\nand video together, but in our case, our testing showed that doing this improved the\noverall performance. The Split Audio function executes the following steps:\n1.\nExtracts audio using ffmpeg and saves it to a folder in EFS.\n2.\nUpdates the relevant DynamoDB record to record that this was done.\n3.\nReturns a Success message and additional parameters (like the location of the\naudio file) to the Step Functions orchestrator.\nAt a later stage, Step Functions invokes the Merge Video Audio function with the\nparameters that were returned by the Split Audio and Merge Video functions.\nSPLIT AND CONVERT VIDEO\nThe Split and Convert Video function splits the original video file into a segment and\nconverts that segment to a new bit rate or encoding. The original video file doesn’t get\nchanged in this process; instead, the function merely extracts a segment between a\nstart time and an end time, specified in the parameters that are passed to it. These\nparameters are worked out by the Transcode Video function. \n Many hundreds of Split and Convert Video functions can run in parallel. Here are\nthe main actions that it performs:\n1.\nUsing ffmpeg, the function creates a new video file from the original one.\n2.\nIt increments a confirmation counter in the appropriate DynamoDB record to\nspecify that the segment exists.\n3.\nIf the confirmation counter is equal to 2, it then returns to the Step Functions\nworkflow with a Merge message. Otherwise, it returns with a Success message,\nwhich stops the execution of that particular Step Functions parallel execution. \nYou may recall from the previous section that, with DynamoDB, each record rep-\nresents two neighboring segments. When a record counter is incremented to 2, the\nfunction knows that the two neighboring segments exist. The function returns a\nMerge message to Step Functions, and Step Functions knows that it can begin calling\nthe Merge Video for these segments. \nMERGE VIDEO\nStep Functions calls the Merge Video function when two neighboring segments are\nready to be merged into a new single segment. The merge operation happens using\nffmpeg, and the new segment is saved to EFS. Here’s what happens in a little more detail:\n1.\nThe Merge Video function is invoked with a number of parameters passed to it\nby Step Functions. These parameters include the left and the right segments.\n2.\nUsing ffmpeg, the left and right segments are merged to create a new segment.\nThis new segment is saved to EFS.\n3.\nDynamoDB confirmation is incremented. If there are two confirmations, then\nthe function returns to the workflow with a Merge message. \n4.\nHowever, if there are two confirmations and the last two remaining segments\nhave been merged, the function returns with a MergeAudio message to the\nworkflow.\n",
      "content_length": 2777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "144\nCHAPTER 8\nArchitecting serverless parallel computing\nAs you can see, the Merge Video function creates a bit of a loop. It continues to merge\nsegments, returns the Merge message, and causes Step Functions to invoke itself\nagain. This happens until the last two segments are merged, then the return type is\nchanged to MergeAudio. This is when Step Functions knows that it’s time to combine\naudio and video and invokes the Merge Video and Audio function.\nMERGE VIDEO AND AUDIO\nThe final function is Merge Video and Audio. It takes input from the Split Audio and\nMerge Video functions and merges the audio and the new video files together using\nffmpeg. The new file is saved somewhere else (in another directory) on EFS. The\nfunction can also upload the new file to an S3 bucket for easier access.\n8.3\nAn alternative architecture\nYou can build this serverless transcoder without using EFS (or Step Functions for that\nmatter). In fact, our first iteration used only S3 and SNS to perform fan-out. We\nwanted to present you with an alternative architecture that shows that you don’t neces-\nsarily have to use Step Functions or EFS if you don’t want to. This section demon-\nstrates that you can use SNS and S3 instead to achieve the same outcome. It’s nice that\nAWS provides so many building blocks that we can build our desired architecture in\ndifferent ways. \nTIP\nOne reason for adopting a different architecture could be because you\ndon’t want to pay for Step Functions and EFS. That is a reasonable concern.\nUsing S3 is likely going to be much cheaper than using EFS and will probably\nperform just as well. Once you get the code working, using S3 is straightfor-\nward, and we don’t have a reason not to recommend it. Whether you should\nuse SNS instead of Step Functions is a tougher proposition. SNS is cheaper,\nbut you will lose a lot of the robustness and observability that you get with\nStep Functions. Perhaps the best solution is to use Step Functions with S3?\nWe’ll leave it to you as an exercise to achieve. \nThis alternative implementation closely resembles what we created in the previous sec-\ntion except, as we mentioned, we’ll replace Step Functions with SNS and EFS with S3.\nFigure 8.6 shows what this architecture looks like. \n This architecture works well, but there are some improvements that can be made\nto it. For one, the implementation should be improved in case of errors such as the\nsplit or merge operation failing. Luckily, there is the Dead Letter Queue (DLQ) fea-\nture of Lambda that allows us to save, review, and even replay failed invocations. If you\nwant a challenge, we invite you to implement DLQ for this architecture to make it\nmore resilient to errors. \n The second issue is observability and knowing what’s happening with the system.\nStep Functions provides some level of visibility, but things get a little bit harder with\nSNS. One tool you can use to help yourself is AWS X-Ray. This AWS service can help\nyou understand the interactions of different services within your system. It goes with-\nout saying that CloudWatch is essential too.\n",
      "content_length": 3076,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "145\nSummary\nSummary\nMapReduce can work really well with a serverless approach. Lambda invites you\nto think about parallelization from the start so take advantage of that.\nYou can solve a lot of problems in Lambda and process vast amounts of data by\nsplitting it up into smaller chunks and parallelizing the operations.\nStep Functions is an excellent service for defining workflows. It allows you to\nfan-out and fan-in operations. \nEFS for Lambda is an endless local disk—it grows as much as you need. You can\nrun applications with EFS and Lambda that you couldn’t have run before. Hav-\ning said that, S3 is still likely to be cheaper so make sure to do your calculations\nand analysis before choosing EFS.\nYou can solve problems in different ways: \n– You don’t have to use Step Functions because you can use SNS (although\nStep Functions adds an additional level of robustness and visibility). \n– You don’t need to use EFS because you can use S3. \nWhen coming up with an architecture for your system, explore the available\noptions because there will be different alternatives with different tradeoffs. \nAmazon Simple \nNotification\nService (fan-out)\nAmazon Simple \nNotification\nService\nAWS Lambda\n(Split Audio)\nAWS Lambda\n(Split and Covert\nSegments)\nAWS Lambda (final \nVideo and Audio Merge)\nSimple Storage \nService (S3)\n(final file)\nAWS Lambda\n(Merge Segments)\nS3 temp\nS3 temp\nStart here\nThis is DynamoDB hiding in the \nback. It’s used to maintain state.\nAWS Cloud\nSimple Storage Service (S3)\n(source file)\nAWS Lambda\n(Analyze Video)\nS3 temp\nAmazon Simple \nNotification\nService (fan-out)\nAmazon Simple \nNotification\nService\nS3 temp\nFigure 8.6\nThe SNS and S3 architecture for the serverless transcoder\n",
      "content_length": 1705,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "146\nCode Developer University\nOne idea that we’ve been mulling for a while has been a web app designed to help\ndevelopers learn programming skills in a fun way with gamification and useful ana-\nlytics. Our idea, let’s call it Code Developer University (CDU), evolved into a proof-\nof-concept website with a collection of interesting programming challenges for\nbudding developers to solve and to build skills. \n Each challenge would pose a problem. The student would have space to type in\ntheir solution and then submit it to our system for processing. The system would\nrun the solution through a battery of tests and decide whether the solution passed\nor failed. If the solution failed, the user would have a chance to update their code\nand resubmit again. If the solution passed, the user would advance to the next chal-\nlenge, receiving between 50 and 500 experience points (XP) based on the difficulty\nof the problem. \nThis chapter covers\nAWS Glue and Amazon Athena\nUsing EventBridge to connect system \ncomponents\nUsing Kinesis Firehose and Lambda for at-scale \ndata processing\n",
      "content_length": 1084,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "147\nSolution overview\n To make the entire experience more interesting and exciting, there would be ele-\nments of gamification baked-in throughout the system. For instance, experience\npoints would be used to create various leaderboards. That way, users interested in a\nfriendly competition would be able to compete for a top 10 position. The more chal-\nlenges solved, the higher the score. These leaderboards would show the overall top 10\nperformers and then the best performers for each language like Python or JavaScript. \n If a student wanted to dig into more data and perhaps see, search, and filter more\nadvanced reports, that would be supported too. A student could, for example, look at\nthe most common mistakes that other users make (anonymized, of course) and learn\nfrom that as well. \n The original idea was lofty but doable. The key to building this project would be to\nlean on as many different AWS services as possible. That way we could focus on the\nunique aspects of the system and leave the rest of the undifferentiated heavy lifting,\nlike authentication, to AWS. At the end, and as you will see, we used the following ser-\nvices to put everything together: \nEventBridge (messaging) \nGlue (data preparation and transformation) \nAthena (data querying) \nDynamoDB (database) \nQuickSight (reporting) \nS3 (storage) \nLambda and API Gateway \nIn this chapter, we focus specifically on data, leaderboards, and reporting for CDU.\nIt’s a fascinating part of the system because it uses so many parts of the AWS ecosystem\nand because you can build something similar just as rapidly yourself. Other features of\nCDU are quite standard for a web app. There are user accounts, an HTML5 user inter-\nface, and all the basic bolts and bits you would expect. If you want to learn how to\nbuild such a system yourself, take a look at the first edition of this book, which\ndescribes a similar, albeit video-focused web application.\n9.1\nSolution overview\nThe leaderboard and reporting aspect of CDU is interesting because it is serverless,\nscalable, and, frankly, fun to implement. There are many serverless AWS services that\nmake data collection, aggregation, and analysis possible without resorting to tradi-\ntional reporting and data-warehousing products of yesteryear. Let’s take a look first at\nthe requirements and then the overall solution.\n9.1.1\nRequirements listed\nCDU is a website with user registration and account features, and the ability for users\nto access and try code exercises and receive points if they are successful in implement-\ning and solving a coding challenge. To that end, the following sections provide a list of\nhigh-level requirements for CDU.\n",
      "content_length": 2671,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "148\nCHAPTER 9\nCode Developer University\nGENERAL\nThe user must be able to run their code solution and determine if it passes or\nfails the tests.\nIf the tests pass, then the solution is considered to be correct.\nA correct solution awards the user some number of points, which are saved to\nthe user’s profile. \nThere should be leaderboards and advanced reports for users to view.\nThe entire system must be serverless, event-driven, and as automated as much\nas possible (no intervention from the administrator should be needed to\nupdate leaderboards and reports).\nUSERS AND EXPERIENCE POINTS\nPoints are awarded for the programming language that is used to solve the chal-\nlenge. For example, if the user codes in Python, then they get points allocated\ntoward Python. If they use JavaScript, then points are allocated to their Java-\nScript score.\nThe user’s profile should show the overall score (sum of all previous points for\nall programming languages) and scores for each programming language indi-\nvidually. The user’s profile and scores should be updated in near real time.\nThe user shouldn’t receive points for the same challenge more than once.\nLEADERBOARDS\nCDU should feature a leaderboard that shows the top scorers across different\nprogramming languages (e.g., Python and JavaScript).\nAn overall leaderboard should show the top performers (regardless of the pro-\ngramming language) for last month, last year, and all time.\nLeaderboards don’t need to be updated in real time, however, but they should\nrefresh at least every 60 minutes. There should also be a way to refresh them on\ndemand by the administrator.\nREPORTS\nApart from the leaderboard, users should also have access to more in-depth\nreports that they can search and filter. \nThe exact implementation of the reports can be left to the data team; however,\na basic report could show the best performers, similar to the leaderboard.\nReports should be refreshed at least every 60 minutes but could also be\nrefreshed sooner (if needed) by the administrator.\nAny user, not just the administrator, should have access to the leaderboard.\n9.1.2\nSolution architecture\nLet’s now take a look at a possible architecture that ought to address our major\nrequirements. Figure 9.1 shows most of the major architectural pieces. These include\nthe following three main microservices:\n",
      "content_length": 2343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "149\nSolution overview\nCode Scoring Service \nStudent Profile Service \nAnalytics Service\nWe will break down the solution in the coming sections, but let’s take a look at the\nhigh-level architecture shown in figure 9.1. The Code Scoring Service runs a Lambda\nfunction that processes submitted code. If it passes the test, information is sent across\nto the EventBridge, which invokes two other microservices: \nThe Student Profile Service updates the student’s profile in the database and\nadds to the student’s overall score. \nThe Analytics Service processes and stores the user’s test data in S3, which later\nenables the creation of the QuickSight dashboards.\nThere’s actually quite a bit that happens in the Analytics Service. It is covered in detail\nin section 9.4, but here’s a high-level overview of what actually takes place in this\nmicroservice: \nThe message (with the user’s solution) is pushed into Kinesis Firehose, which\nuses a Lambda function to modify the format of the message so that it can be\nprocessed later by other AWS services.\nRun Unit \nTest Lambda\nProcess Submission\nLambda\nSubmissions\nqueue\nResults EventBridge\nUpdate Student \nScore Lambda\nStudent database\nStudent scores \nKinesis Firehose\nProcess Firehose \nSubmission Lambda\nStudent scores\nbucket\nProcess results and\nupdate summary\ntables with Glue\nQuery summaries \nin Athena\n \nQuery \nLeaderBoard \nSummary Lambda\nProcess \nLeaderboard \nSummary Lambda\nQuickSight\ndashboard\nLeaderboard\ndatabase\nCode scoring service\nAnalytics service\nStudent profile service\nTests bucket\nAWS Cloud\nStudent database\nLesson database\nSchedule\n(run every \nhour)\nFigure 9.1\nThe architecture of Code Developer University (CDU) that’s responsible for scoring and \nleaderboards\n",
      "content_length": 1725,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "150\nCHAPTER 9\nCode Developer University\nKinesis then stores the newly processed message (as a JSON file) in an S3 bucket.\nAWS Glue runs on schedule, which is set to trigger every 60 minutes. When that\nhappens, Glue processes the aforementioned S3 bucket and updates a Glue\nData Catalog (think: a table with metadata) that points to the data stored in S3.\nGlue then triggers a Lambda function, which uses Amazon Athena to query the\ndata stored in S3 via the Glue Data Catalog.\nOnce Athena finishes, it triggers another Lambda function that gets the result\nof the query and updates the appropriate leaderboards saved in DynamoDB.\nFinally, there’s an Amazon QuickSight report that uses Athena to query the data\nin the S3 bucket when a user wants to see more information.\nThere’s a little more detail to all the services, and you may have other questions,\nwhich should be cleared up in coming sections. Read on!\n9.2\nThe Code Scoring Service\nThe purpose of the Code Scoring Service is to receive submitted code from the user and\nrun it against a set of tests. If tests pass, the Run Unit Test Lambda creates a submission,\nwhich it puts into the submissions queue. The submission is picked up from the queue\nby the Process Submission Lambda and is enriched with data from a couple of Dyna-\nmoDB tables. Finally, the Process Submission Lambda pushes the newly enriched mes-\nsage on to Amazon EventBridge for consumption by other services in our system. \n The actual design of the Code Scoring Service is fairly straightforward, but let’s\ntake a look at its design in more detail. Figure 9.2 shows a closeup of the architecture\nbeginning with the Run Unit Test Lambda. This Run Unit Test Lambda function is\ninvoked via HTTPS (via the API Gateway) and receives a zip payload as part of the\nQuickSight vs. DynamoDB\nOne question you may be asking yourself is why are we using Amazon QuickSight for\nreporting and also storing leaderboards in DynamoDB? Isn’t that redundant? The\nreason is that QuickSight is heavy, powerful, but also slow. You can integrate it into\nyour website (in an iFrame), but it takes a long time to load. If you are committed to\nusing QuickSight to explore data in detail, then you’ll wait for 10 or 20 seconds. But\nif you want to see results quickly, waiting for it can be unbearable (AWS, please look\nat performance!). \nThis is why we store important leaderboard results in DynamoDB, which can be\nloaded and displayed to the user quickly. Then it’s up to the user to choose to see\nthe QuickSight version of this data, especially if they need more detail. You can think\nof our DynamoDB leaderboards as an informal cache for QuickSight. \nThe negative aspect of this implementation is that the DynamoDB table and the data\nin QuickSight must be synchronized. If the Student Profile Service updates the stu-\ndent’s score, but the Analytics Service fails, DynamoDB may end up showing some-\nthing different to QuickSight. There are ways to fix this though. What would you do?\nWe will discuss this in section 9.3.\n",
      "content_length": 3024,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "151\nThe Code Scoring Service\nrequest body. The zip payload contains the user’s code submission and metadata, such\nas what challenge the user is attempting and what programming language is being\nused. The Lambda function looks up the appropriate test in the Tests bucket (it knows\nwhich test to grab based on the lesson name) and downloads that test file from S3.\nNow the Lambda function can execute the appropriate interpreter or compiler, run\nthe unit test, and test the user’s submission.\nPrevent tight coupling of Lambda\nfunctions by putting in a queue\nbetween two Lambda functions.\nEventBridge will push out a \nmessage to two other \nmicroservices.\nAWS Cloud\nRun Unit Test \nLambda\nProcess Submission\nLambda\nSubmissions\nqueue\nResults EventBridge\nCode Scoring Service\nTests bucket\nStudent database\nLesson database\nFigure 9.2\nThe Code Scoring Service runs the user’s code and, if it’s successful, kicks of \nthe rest of the chain of events in our system.\nLambda layers\nIf you want to support multiple languages like Python, JavaScript, C++, C#, Java, and\nso on, use Lambda layers. A layer is a zip file that can contain additional libraries or\ncustom runtimes. \nYou can have a Lambda layer with a Python interpreter or a layer with a C compiler.\nMoreover, you deploy layers separately from Lambda functions, thus keeping your\nactual Lambda functions small. At run time, as long as it’s configured correctly, your\nLambda functions can access the contents of your layers (which are extracted to the\n/opt directory in the function execution environment). You can deploy as many layers\nas you like, but know that Lambda can only use up to five layers at a time. You can read\nmore about them at http://mng.bz/doJO.\n",
      "content_length": 1709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "152\nCHAPTER 9\nCode Developer University\nThe Run Unit Test Lambda by itself is not particularly complex. It needs to know how\nto run a unit test and then parse the result to figure out if it passed successfully or not.\nIf the test failed, then the function sends back an HTTP response with the output\nfrom the interpreter or the compiler. Thus, the user can see the error message, fix the\ncode issue, and resubmit. Otherwise, if it passes, the function sends back a celebratory\nmessage to the user and places a message containing the user’s submission on the\nSubmissions SQS queue for further processing.\n9.2.1\nSubmissions Queue\nThe Submissions Queue is an SQS queue that sits between the only two Lambda func-\ntions in this service. When a message is placed in the queue, it leads to an invocation\nof the Process Submission Lambda that retrieves the message and enriches it with\nmore data before pushing it to the EventBridge. There are a few reasons we do this,\nincluding the following:\nOne of the requirements is to prevent the user from receiving points for the\nsame challenge multiple times. The Process Submission Lambda needs to look\nup the Student DynamoDB table to figure out whether the user has already\ncompleted this challenge. If the user has already completed that challenge,\nthen that is noted, and no points are earned.\nAssuming that the student has solved the challenge for the first time and is sup-\nposed to receive points, the Process Submission Lambda also looks up how\nmany points should be awarded from the Lesson database.\nAll of this information, including the message that came from the queue, is\ncombined and pushed to Amazon EventBridge.\nBy now you might be thinking, “Why not do everything in the initial Run Unit Test\nLambda?” The reason is to separate responsibility. The Run Unit Test function is\nintended to run code and figure out if it passes a test. The second Process Submission\nLambda function has to perform database lookups and evaluate whether the student\nshould be awarded points. As a rule of thumb, you should use multiple Lambda functions\nwhen you are dealing with different concerns rather than having everything lumped into\none. Hence, this is the reason we created two functions and introduced a message queue\nbetween them.\n Another question you may have is why we used SQS rather than have functions call\none another directly. Our recommendation is never to have functions call each other\ndirectly unless you are using a feature called Lambda Destinations (which adds a hid-\nden queue between two functions anyway). Lambda Destinations, however, only works\nfor asynchronous invocations, so it wouldn’t have been possible in our case. The Run\nUnit Test Lambda was invoked synchronously via HTTP. The reason for having a queue\nbetween two functions is to reduce coupling (e.g., the two functions have no direct\nknowledge of one another) and to have an easier time handling errors and retries.\n",
      "content_length": 2942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "153\nStudent Profile Service\n We also could have chosen to use Amazon EventBridge instead, but SQS was\nacceptable in this scenario. And, if we ever wanted to enable First-In First-Out (FIFO)\nqueues at a later stage, we’d need to use SQS because EventBridge doesn’t support\nthis feature, so that further weighed our decision. \n The last action performed by the Process Submission Lambda is to push the mes-\nsage to Amazon EventBridge. As you may recall, this message contains the original\nsubmission made by the user together with additional details that consists of informa-\ntion on whether the experience points should be awarded to the user and the amount\nof those points (this information was obtained by looking up a couple of tables in the\nProcess Submission Lambda function). \n9.2.2\nCode Scoring Service summary\nThe Code Scoring Service is a relatively trivial service apart, perhaps, from running\nthe code provided by the user. Even then it’s not too difficult to unzip a file and run\nan interpreter (or a compiler) within Lambda. One important thing to mention is\nsecurity. If you are running someone else’s code in a function, you must be prepared\nthat someone will try to subvert it, find a vulnerability to exploit, and do something\nbad. Therefore, you must follow the principle of least privilege and disallow anything\nthat isn’t critical to the running of your function. This should be a rule for all Lambda\nfunctions, but in this instance, you should be doubly careful and vigilant.\n9.3\nStudent Profile Service\nThe Student Profile Service is small. Its purpose is to increment the number of experi-\nence points in the student record in the DynamoDB table. That way, the student can\nimmediately see the cumulative score added to their tally and feel good about their\nachievement. This service consists of a single Lambda function that communicates\nwith DynamoDB. This function receives an event from EventBridge, reads it, and\nupdates the user profile if the user has received any points. Figure 9.3 shows what this\nbasic service looks like.\n You may remember that earlier (in section 9.1), we posed a question about keep-\ning different tables in sync. Given that the Student Profile Service and the Analytics\nAmazon EventBridge\nAmazon EventBridge is a serverless event bus that can connect different AWS (and\nnon-AWS) services. It has a few great features that services like SQS, SNS, and Kine-\nsis do not possess. Chief among them is the ability to use more than 90 AWS ser-\nvices as event sources and 17 services as targets, automated scaling, content-\nbased filtering, schema discovery, and input transformation. But like any other tech-\nnology, it has certain deficiencies like no guarantee on ordering of events or buffering.\nAs always, what you end up choosing should depend on your requirements and the\ncapabilities of the product you are using.\n",
      "content_length": 2866,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "154\nCHAPTER 9\nCode Developer University\nService store similar data (namely the user’s score), what happens if one of the ser-\nvices goes down and falls out of sync with the other service? In other words, if there’s a\nfault in a service that causes a data mismatch, what can we do about it? There are a\nnumber of solutions you can think about implementing to address this problem:\nSerial invocation—One approach is to make the Analytics Service and the Stu-\ndent Profile Service run in serial rather than parallel. That way your system\nwould update the Student Profile Service first and then run through the update\nprocedure in the Analytics Service (invoking it via another EventBridge). If the\nAnalytics Service fails, the system would roll back the change in the Student\nProfile Service, and both services would continue operating in sync.\nOne source of truth—Alternatively, you could make the Analytics Service your\nsource of truth and then simply copy the data over to the Student Profile Ser-\nvice. That way you could even delete all data in the Student Profile Service and\nregenerate it as many times as necessary from the Analytics Service.\nShare the database—Both services could read and write to the same database. That\nwould avoid some problems, but then, we no longer have a microservices archi-\ntecture in which each service is responsible for its own view of the world. We\nwould end up with a distributed monolith. It must be mentioned that in many cir-\ncumstances having a distributed monolith is a fine and acceptable solution.\nThis is a basic service with one Lambda\nupdating a DynamoDB table. The Lambda\nfunction is invoked by the EventBridge.\nStudent Profile Service\nResults EventBridge\nStudent database\nUpdate Student Score Lambda\nAWS Cloud\nFigure 9.3\nThe Student Profile Service is the simplest one in this entire architecture. It’s a Lambda \nfunction that writes to a Dynamo table.\n",
      "content_length": 1906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "155\nStudent Profile Service\nOrchestrator—Another approach is to have an orchestrator sit above the two services\nand monitor what is happening. If there is an error, the orchestrator could run addi-\ntional actions to compensate for the issue (for example, retry or roll back). \nQuite frankly, this is a common situation with a microservices-based approach. How do\nyou keep services in sync without having all microservices coupled to a central data-\nbase? There are different solutions to this problem but, as with anything in software\nengineering, they all have different trade-offs. In the case of CDU, we decided to update\nboth services in parallel. If an issue were to occur, we would use the Analytics Service as\nour source of truth and regenerate the data needed by the Student Profile Service.\n9.3.1\nUpdate Student Scores function\nThe Update Student Score function is shown in listing 9.1. It performs three primary\nactions:\nIt parses the event received from the EventBridge that has the scores/data.\nUpdates the amount of XP gained for the topic like JavaScript or Python.\nUpdates the total amount of XP earned by the user.\n'use strict';\nconst AWS = require('aws-sdk');\nconst sns = new AWS.SNS();\nconst dynamoDB = require('aws-sdk/clients/dynamodb');\nconst doc = new dynamoDB.DocumentClient();\nconst updateTotalXP = (record, lessons) => {\n    const date = new Date(Date.now()).toISOString();\n    const xp = lessons.filter(m => m.xp)\n    ➥ .map(m => m.xp)\n    ➥ .reduce((a, b) => a+b); \n    const params = {    \n        TableName: process.env.USER_DATABASE,\n        Key: {\n           userId: record.username\n        },\n        UpdateExpression: `set \\\n                           modified = :date, \\\n                           xp.#total = :xp`, \n        ExpressionAttributeNames: {\n           '#total': 'total'\n        },\n        ExpressionAttributeValues: {\n           ':date': date,\n           ':xp': xp\n        },\nListing 9.1\nUpdating the Student Score Lambda\nCalculates the total XP for a user by summing \nup the XP for all of the lessons. This is woefully \ninefficient to do each time but OK for an example. \nCan you think of a better way?\nThis params object has all the necessary \nattributes needed to update the relevant \nDynamoDB table. Note the ‘total’ in the \nExpressionAttributeNames. It’s a reserved \nkeyword so it has to be specified using \nExpressionAttributeNames.\n",
      "content_length": 2389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "156\nCHAPTER 9\nCode Developer University\n        ReturnValues: 'ALL_NEW'\n    };\n    return doc.update(params).promise(); \n}\nconst updateTopicXP = (record) => {\n    const date = new Date(Date.now()).toISOString();\n    const lesson = {\n        lesson: record.lesson,\n        topic: record.topic,\n        modified: date,\n        xp: record.xp,\n        isCompleted: record.isCompleted\n    };\n    const params = {\n        TableName: process.env.USER_DATABASE,\n        Key: {\n            userId: record.username\n        },\n        UpdateExpression: `set \\\n                          modified = :date, \\\n                          lessons = list_append(if_not_exists(lessons,\n    ➥ :empty_list), :lesson), \\     \n                          xp.${record.topic} = \n    ➥ if_not_exists(xp.${record.topic}, :zero) + :xp`,\n        ExpressionAttributeValues: {\n            ':lesson': [ lesson ],   \n            ':empty_list': [],       \n            ':zero': 0,\n            ':date': date,\n            ':xp': parseInt(record.xp, 10)\n        },\n        ReturnValues: 'ALL_NEW'\n    };\n    return doc.update(params).promise();\n}\nexports.handler = async (event, context) => {\n    try {\n        const record = event.detail; \n        if (record.isCompleted) {\n           const user = await updateTopicXP(record);\n           if (user.Attributes.lessons.length > 0) {\n               await updateTotalXP(record, user.Attributes.lessons);\n           }\n        }\n    } catch (error) {\n         console.log(error);\n    }\n}\nThis update expression appends \na lesson to a list of lessons in \nDynamoDB. Otherwise, if a list \ndoesn’t exist, a new and empty \none is created.\nThis function is invoked via the \nEventBridge. The parameter event.detail \ncontains the information that was sent \nover from the Process Submission Lambda \nfunction in the previous section.\n",
      "content_length": 1827,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "157\nAnalytics Service\nThe Student Profile Service is a small microservice with a single Lambda function. Its\npurpose is to update a DynamoDB table and that’s pretty much as basic as you can get.\nThe next service, however, is not as straightforward. Let’s take a look at it now.\n9.4\nAnalytics Service\nThis is going to be a big one, so grab yourself a tea or a coffee before jumping in. If\nyou recall, the purpose of the Analytics Service is twofold:\nEnable the creation and display of QuickSight dashboards.\nMaintain leaderboards in DynamoDB that could be quickly accessed and read.\nThe data collected and processed by the Analytics Service must enable us to achieve\nthose two aims. Let’s take a look at the architecture in figure 9.4. The steps that the\nAnalytics Service takes are as follows:\n1.\nThe EventBridge service pushes a message from the Code Scoring Service on to\nthe Student Scores Kinesis Firehose.\n2.\nThe Firehose runs a Lambda that processes and transforms each incoming mes-\nsage into a format that is palatable for Amazon Glue to work on later.\n3.\nAfter the message is transformed by Lambda, Firehose stores it in an S3 bucket.\n4.\nEvery hour (or on demand) AWS Glue runs and crawls the messages stored in\nthe S3 bucket. It updates a table within the AWS Glue Data Catalog with the\nmetadata based on the crawl.\n5.\nOnce Glue is finished processing, the Query Leaderboard Summary function is\nrun. The Lambda function invokes Athena that runs a query to work out the\nleaderboard.\n6.\nAthena accesses Glue and S3 and extracts the relevant data for the query. Once\nthe query is complete, the Process Leaderboard Summary Lambda is invoked.\n7.\nThis Process Leaderboard Summary Lambda function receives the result of the\nquery from Athena, reads it, and updates the Leaderboard DynamoDB table.\n8.\nFinally, the QuickSight Dashboard component uses Athena to execute queries\nbased on what the user is trying to see in a QuickSight report.\nYou may agree that this is quite a lot to take in one go, so let’s break down the most\ninteresting components. We’ll do that in the following sections.\n \n \n \n \n \n \n \n \n \n",
      "content_length": 2114,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "158\nCHAPTER 9\nCode Developer University\n9.4.1\nKinesis Firehose\nKinesis Firehose provides a way to capture and stream data into Elasticsearch, Red-\nshift, and S3. AWS says that it’s the “. . . easiest way to reliably load streaming data into\ndata lakes, data stores, and analytics services” (https://aws.amazon.com/kinesis/data\n-firehose/), which sounds perfect for our use case. Kinesis Firehose, unlike other\nKinesis services, is serverless, meaning that you don’t need to worry about scaling par-\ntitions or sharding as is the case with, say, Kinesis Data Stream. It is all done for you\nautomatically. Another nice feature of Firehose is that it can run Lambda for messages\nas they are ingested. Lambda can be used to convert raw streaming data to other,\nmore useful, formats and this is exactly what we would do. In our use case, we can use\na Lambda function to convert the messages to a JSON format that would later be read\nby the AWS Glue service before storing them in S3.\n Listing 9.2 shows a Kinesis Firehose processing function that takes a message, pro-\ncesses it, creates a new record with a different set of fields, and pushes it back to Fire-\nhose for storage in S3. In this listing, we are extracting only a few properties from the\noriginal message because we don’t want to keep everything. For example, we may dis-\ncard the user’s submitted source code because we care only if they’ve passed the test\nor not. There are a few things to keep in mind in this listing:\nKinesis stores data in S3.\nGlue reads from the bucket\nand creates a data catalog\nthat can be queried by\nAthena.\nRun Glue \nOn-Demand Lambda\nResults EventBridge\nStudent scores \nKinesis Firehose\nProcess Firehose \nSubmission Lambda\nStudent scores\nbucket\nProcess results and\nupdate summary\ntables with Glue\nQuery summaries \nin Athena\n \nQuery \nLeaderboard \nSummary Lambda\nProcess \nLeaderboard \nSummary Lambda\nQuickSight\ndashboard\nLeaderboard\ndatabase\nAWS Cloud\nSchedule\n(run every \nhour)\nFigure 9.4\nThe Analytics Service architecture includes Glue, Athena, DynamoDB, Kinesis Firehose, \nQuickSight, and Lambda.\n",
      "content_length": 2084,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "159\nAnalytics Service\nAll transformed records must contain a recordId, result, and data. Other-\nwise, Kinesis Firehose rejects the entire record and treats it as a “transformation\nfailure.”\nThe property called recordId is passed from Firehose to Lambda. The trans-\nformed record has to contain the same recordId as the original. A mismatch\nresults in transformation failure (so, don’t make your own or append anything\nto it).\nThe property result must either be Ok (record transformed) or Dropped (record\nwas dropped intentionally). The only other allowed value is ProcessingFailed\nif you want to flag that the transformation couldn’t take place.\nThe property data is your base-64 encoded transformed record. \n'use strict';\nexports.handler = (event, context) => {\n    let records = [];\n    \n    for (let i = 0; i < event.records.length; i++) {\n        const payload = Buffer.from(\n        ➥ event.records[i].data, 'base64')\n        ➥ .toString('utf-8');                 \n        const data = JSON.parse(payload);      \n        const record = {   \n          username: data.detail.username,\n          name: data.detail.user.name,\n          lesson: data.detail.lesson,\n          topic: data.detail.topic,\n          xp: data.detail.xp,\n          hasPassedTests: (data.detail.hasPassedTests || false),\n          runTests: (data.detail.runTests || false),\n          isCompleted: (data.detail.isCompleted || false),\n          time: data.time,\n        };\n        records.push({  \n            recordId: event.records[i].recordId,\n            result: 'Ok',\n            data: Buffer.from(JSON.stringify(record)).toString('base64')\n        });\n    }\n    console.log(`Return: ${ JSON.stringify({records}) }`);\n    return Promise.resolve({\n        records\n    });\n};\nListing 9.2\nKinesis Firehose processing function\nThe original message that was \npushed to Firehose. You can \nnow extract the relevant bits \nyou might want to save in S3.\nThe record you create here \nand store in S3 will be JSON.\nAll transformed records must contain a property \ncalled recordId, result, and data. Transformation \nis the ultimate goal for this Lambda.\n",
      "content_length": 2122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "160\nCHAPTER 9\nCode Developer University\nFinally, you must ensure that your response doesn’t exceed 6 MB. Otherwise, Firehose\nwill refuse to play along.\n9.4.2\nAWS Glue and Amazon Athena\nAWS Glue is a serverless ETL (extract, transform, and load) service that can scour an\nS3 bucket with a crawler and update a central metadata repository called the Glue\nData Catalog. You and other services can then use this metadata repository to quickly\nsearch for relevant information among the records scattered in S3. Glue never actu-\nally moves or copies any data. The tables with metadata it creates in the Glue Data Cat-\nalog point to the data in S3 (or other sources like Amazon Redshift or RDS). This\nmeans that the Data Catalog can be recreated from the original data if necessary.\n Amazon Athena is a serverless query service that can analyze data in S3 using stan-\ndard SQL. If you haven’t tried Athena, you have to give it a go. You simply point it to\nS3, define the schema, and begin querying using SQL. What’s even nicer is that it inte-\ngrates closely with Glue and its Data Catalog (which takes care of the schema). Once\nyou have AWS Glue configured and the Data Catalog created, you can begin querying\nAthena immediately.\n Listing 9.3 shows how to perform a query. An important thing to note is that the\nquery is asynchronous. You will not get a response once you’ve run it. You have to start\nthe query execution and then, using CloudWatch events, react to when you get the\nresult. Luckily everything can be accomplished with two Lambda functions. Listing 9.3\nshows how to execute a query and listing 9.4 shows how to process it if you have\nhooked up CloudWatch events to respond.\n'use strict';\nconst AWS = require('aws-sdk');\nconst athena = new AWS.Athena();\nconst runQuery = (view) => {\n    const params = {\n        QueryString: `SELECT * FROM \"${view}\"`, \n        QueryExecutionContext: {\n            Catalog: process.env.ATHENA_DATA_SOURCE,  \n            Database: process.env.ATHENA_DATABASE    \n        },\n        WorkGroup: process.env.ATHENA_WORKGROUP       \n    };\n    return athena.startQueryExecution(params).promise();\n}\nexports.handler = async (event) => {\n    let promises = [];\nListing 9.3\nQuery Leaderboard Summary Lambda\nViews are supported by Athena \nand are as useful as regular SQL.\nParameters such as the Catalog, \nDatabase and WorkGroup are \nset up in Athena when you \nconfigure it.\n",
      "content_length": 2407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "161\nAnalytics Service\n    promises.push(runQuery(process.env\n    ➥ .ATHENA_LEADERBOARD_VIEW_TOPICS));  \n    promises.push(runQuery(process.env\n    ➥ .ATHENA_LEADERBOARD_VIEW_OVERALL)); \n    const query = await Promise.all(promises);\n    console.log('Athena Query Id', query);\n}\nListing 9.4 shows a Process Leaderboard Lambda function that responds to a Cloud-\nWatch event that contains information about the query performed in listing 9.3. Note\nthat the actual result (meaning the data itself) must be retrieved from Athena using\nthe GetQueryResults API call. When the Process Leaderboard Summary function is\ninvoked, only queryExecutionId is passed into it, but that’s enough to perform the\nGetQueryResults API call to get the data. The code in the following listing is quite\nlengthy because, apart from showing how to get a result out of Athena, it demon-\nstrates how to update a DynamoDB table.\n'use strict';\nconst AWS = require('aws-sdk');\nconst athena = new AWS.Athena();\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\nconst getQueryResults = (queryExecutionId) => {\n    const params = {\n        QueryExecutionId: queryExecutionId\n    };\n    return athena.getQueryResults(params).promise();\n}\nconst updateDynamoLeaderboard = (rows, index) => {\n    let transactItems = [];\n    const date = new Date(Date.now()).toISOString();\n    //\n    // Skip the first row because it's the label\n    // Data: [\n    //   { VarCharValue: 'topic' },\n    //   { VarCharValue: 'username' },\n    //   { VarCharValue: 'name' },\n    //   { VarCharValue: 'score' },\n    //   { VarCharValue: 'rn' }\n    // ]\n    //\n    for (let i = 0; i < rows.length; i++) {\n        const row = rows[i].Data;\nListing 9.4\nProcess Leaderboard Summary Lambda\nViews are supported by Athena \nand are as useful as regular SQL.\nYou need the \nQueryExecutionId to run \nGetQueryResults, then the \nresult of the query is yours.\n",
      "content_length": 1886,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "162\nCHAPTER 9\nCode Developer University\n        const params = {\n            TableName: process.env.LEADERBOARD_DATABASE,\n            Key: {\n                uniqueId: row[1].VarCharValue, //username\n                type: row[0].VarCharValue //topic\n            },\n            UpdateExpression: `set \\\n                                #name = :name, \\\n                                modified = :date, \\\n                                #rank = :rank,\n                                score = :score`,\n            ExpressionAttributeNames: {\n                '#name': 'name',\n                '#rank': 'rank'\n            },\n            ExpressionAttributeValues: {\n                ':date': date,\n                ':name': row[2].VarCharValue,\n                ':score': parseInt(row[3].VarCharValue, 10),\n                ':rank': parseInt(row[4].VarCharValue, 10)\n            },\n            ReturnValues: 'ALL_NEW'\n        }\n        transactItems.push({Update: params});\n    }\n    return dynamodb.transactWrite({TransactItems:transactItems}).promise();\n}\nexports.handler = async (event) => {\n    try {\n        if (event.detail\n            ➥ .currentState === 'SUCCEEDED') { \n            const queryExecutionId = \n            ➥ event.detail.queryExecutionId;\n            const result = \n            ➥ await getQueryResults(queryExecutionId);\n            result.ResultSet.Rows.shift();  \n            if (result.ResultSet.Rows.length > 0) {\n                const maxItemsPerTransaction = 20;  \n                for (let i = 0; i < \n    ➥ result.ResultSet.Rows.length/maxItemsPerTransaction; i++) {\n                    const factor = \n    ➥ result.ResultSet.Rows.length/maxItemsPerTransaction;\n                    const remainder = \n    ➥ result.ResultSet.Rows.length%maxItemsPerTransaction;\nWe only ever want to retrieve \nthe results and save them if \nthe query executes successfully. \nLuckily, this parameter checks \nif it’s all good.\nThe first row in the array \ncontains labels for the \ncolumns (e.g., topic, score, \netc.). Shifting that row \nremoves it because we are \nonly interested in the values.\nUpdates in chunks of 20 items. \nDynamoDB can handle 25 items \nin a transaction, but we only do \n20 here instead.\n",
      "content_length": 2204,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "163\nAnalytics Service\n                    let data = \n    ➥ result.ResultSet.Rows.slice(i*maxItemsPerTransaction, \n    ➥ i*maxItemsPerTransaction + Math.max(maxItemsPerTransaction, \n    ➥ remainder));  \n                    const update = await updateDynamoLeaderboard(data, \n    ➥ i*maxItemsPerTransaction);\n                    \n                }\n            }\n        } else {\n            console.log('Query Unsuccessful');\n        }\n    } catch (error) {\n        console.log(error);\n    }\n}\nServerless architectures are typically push-based and event-driven. You should try to\navoid polling whenever you can. We could have polled for the status of the query and\nthen called the Lambda function to process the result, but it would have been more\ncomplex and error prone. Instead, we rely on CloudWatch events to get notified\nabout the query state transition. Interestingly, this feature wasn’t always available, and\npeople had to poll. There really was no other option, so it’s good to see AWS adding\nthe necessary support and enabling our serverless dream to continue.\n9.4.3\nQuickSight\nAmazon QuickSight is AWS’s Business Intelligence (BI) service in the vein of Tableau.\nYou can use it to build dashboards of all kinds and embed them into your website.\nQuickSight has some really interesting features, like its ability to formulate answers\nusing natural language (this is underpinned by machine learning). \n Truth be told, however, at the time of writing, QuickSight is an underwhelming\nAWS service. It’s slow, reasonably pricey, and weirdly different enough from other\nAWS services to necessitate a steeper learning curve. Nevertheless, it is also serverless,\nand it allows us to stay within the AWS environment, which is an advantage. We hope\nthat AWS substantially improves QuickSight over the coming months and years. If you\nare looking for a BI solution, you should have a look at QuickSight but evaluate other\noptions too. \n We used QuickSight to create dashboards that read data straight from S3 via Athena\nfor the CDU. Describing how to use QuickSight is out of scope for this chapter, but it\ndoes have a fairly intuitive interface that you can click through. QuickSight isn’t\nsupported by CloudFormation (at least at the time of writing this in the second half of\n2021), so creating consistent, repeatable dashboards is challenging and that’s a\nbummer. However, if your data is in S3 and can be queried with Athena, you can always\nWe use a little bit of math to retrieve the\nnecessary records (slice) from the array.\nThis formula gets 20 or fewer rows to\nstore in DynamoDB at a time.\n",
      "content_length": 2596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "164\nCHAPTER 9\nCode Developer University\nrecreate your dashboards. The main thing is having the data in the right format and\nplace, which you will have with the tools described in this chapter.\n In summary, to build an Analytics Service, AWS services such as Kinesis Firehose,\nAthena, and Glue can be what you need. These are serverless services, meaning that\nyou don’t have to think about scaling or managing them the same way that you’d need\nto think about Amazon Redshift. Nevertheless, if you decide to embark on a serverless\njourney with these services make sure to do your evaluation first. \nAre they capable of meeting all of your requirements? \nIs there a situation where, in your case, Amazon Redshift may be better? \nAthena’s charges are based on the amount of data scanned in each query; Redshift is\npriced based on the size of the instance. There could be circumstances where Athena\nis cheaper, but Redshift is faster, so you should spend a little bit of time with Excel pro-\njecting cost. Nevertheless, in many cases, especially for smaller data sets, the combina-\ntion of Athena and Glue is more than enough for most needs.\nSummary\nAWS has a variety of services and ways to capture, transform, analyze, and report\non data relevant to your application. \nCapturing, processing, and reporting on data using services such as Event-\nBridge, DynamoDB, Amazon Glue, Amazon Athena, and Amazon QuickSight\nto build a web application with three microservices leaves us with a few take-\naways, including the following: \n– Amazon QuickSight is slow (and it can be expensive). If you need to show\nleaderboards, cache them in something like DynamoDB for quick retrieval.\n– Glue and Athena are fantastic tools. Glue can index the data stored in S3,\nand Athena can search across it using standard SQL. The result is less “lift-\ning” and coding for you.\n– Kinesis Firehose has a fantastic feature that allows you to modify records\nbefore they get to whatever destination they are going to. This is a fantastic\nfeature that’s worth the price of admission.\n– Do not have Lambda functions call each directly unless you are using\nLambda Destinations. Always use a queue like SQS or EventBridge if Lambda\nDestinations is not available.\n– EventBridge is an excellent message bus for use within AWS. Apart from not\nhaving FIFO functionality (this could change by the time you read this), it\nhas a ton of excellent features, and we highly recommended it.\n",
      "content_length": 2446,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "Part 4\nThe future\nThe last two chapters of this book are really fun. The next chapter is on the\nAWS Lambda internals and is fascinating for anyone wanting to know how\nLambda works. The last chapter of this book is about emerging practices. It cov-\ners the usage of multiple AWS accounts, temporary CloudFormation stacks,\nmanagement of sensitive data, and the use of EventBridge in event-driven archi-\ntectures. These two chapters are some of our favorites. We hope you like reading\nthem as much as we loved working on them.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 558,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "167\nBlackbelt Lambda\nPerformance (how fast your application responds) and availability (whether or not\nyour application provides a valid response) are critical aspects of your end user expe-\nrience. When using serverless architectures, your performance also has a direct\nimpact on your costs; for example, AWS Lambda bills you for the duration your func-\ntion runs, weighted by the memory you assign to it. Serverless architectures elimi-\nnate many of the common surface areas for performance optimizations, like scaling\navailable servers or tweaking server configurations, which can make it challenging\nfor new users to understand how to go about making these optimizations. \n This chapter introduces you to key tools and approaches available to you to\nimprove performance across the various services that make up your serverless appli-\ncation. We’ll use relevant examples to demonstrate how these techniques work.\n10.1\nWhere to optimize?\nBefore we delve into how we optimize serverless architectures, let’s quickly recap\nhow to think about them. Serverless architectures have multiple conceptual layers\nThis chapter covers\nMonitoring latency, request per second, and \nconcurrency for serverless applications\nTechniques for optimizing latency\n",
      "content_length": 1246,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "168\nCHAPTER 10\nBlackbelt Lambda\nas figure 10.1 illustrates. Endpoints are responsible for secure interactions with your\nend users and devices, and for the ingress of requests or events to your application\nfrom the end user. Examples of endpoints you can use in your AWS Serverless archi-\ntecture include API Gateway, AWS IoT, Amazon Alexa (if you were building an Alexa\nskill), or even just the AWS SDK. \n The compute layer of your workload manages requests from external systems\n(received through the endpoints), while controlling access and ensuring requests are\nappropriately authorized. It contains the run-time environment that deploys and runs\nyour business logic embodied as Lambda functions (we’ll delve into this shortly). \n The data layer of your workload manages persistent storage from within a system. It\nprovides a secure mechanism to store states that your business logic will need. It also\nprovides a mechanism to trigger events in response to data changes, which in turn can\nfeed into other parts of your business logic. As you can imagine, this is a broad surface\narea to discuss optimizations across, so we’ll focus on the following points, highlighted\nin figure 10.1:\nFunctions\nInvocations of these functions (either via requests from endpoints or events\nfrom backend systems)\nInteractions the functions have with downstream resources\nNow that you have a conceptual understanding of the various points of optimization,\nlet’s look at the tools available to do so. We’ll discuss those in the following sections.\nExample:\nAmazon API Gateway\nAWS IoT Amazon Alexa\nExample:\nAny public API\nService running on EC2\nExample:\nAmazon SQS\nAWS Step Functions\nAmazon SNS\nExample:\nAmazon S3\nAmazon DynamoDB\nMongo on EC2\nEndpoint services\nBusiness logic\nFunctions\nFunctions\nOther services and APIs\nMessaging and workflow\ncomponents\nData services\nFigure 10.1\nConceptual architecture of a serverless application\n",
      "content_length": 1916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "169\nBefore we get started\n10.2\nBefore we get started\nTo effectively optimize applications, there are certain tools and concepts we must be\nfamiliar with. In this section, we will recap what happens when a Lambda function\nexecutes and how it impacts latency, how to observe the latency and contributors to it,\nand how to generate load to a function to get enough sample data.\n10.2.1 How a Lambda function handles requests\nTo understand how to optimize functions, we need to have a shared understanding of\nhow Lambda goes about executing our functions. Let’s use an example to illustrate what\nhappens when a function is deployed. We’ll use the image-resizer-service application\nfrom the Serverless Application Repository (http://mng.bz/WBy4) for reference. This\nserverless application deploys a Lambda function (written in Node.js) and an API Gate-\nway to your AWS account in the US East (N. Virginia)east-1 region that reads images\nfrom a S3 bucket (whose name is defined at deployment) and serves them through the\nAPI Gateway. The function uses the ImageMagick library to process the image. \nNOTE\nYou need to specify a new bucket name for the application to use. Use\nthe name “image-resizer-service-demo” for this example. \nOnce deployed, click the Test App button on the page, and it will take you to the\nApplications list view on the Lambda console, where you’ll see the newly deployed\napplication. In figure 10.2 these are marked as (1) and (2), respectively.\nTo test the application, you need to navigate to the main function. Click the applica-\ntion and on the detailed view (figure 10.3); click the image’s ResizeFunction (1) to\naccess the function. \nFigure 10.2\nThe Applications view shows all deployed services and applications.\n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "170\nCHAPTER 10\nBlackbelt Lambda\nOnce you select the function, you are taken to the Function Overview page (figure\n10.4). Here you can test the function by selecting Test (1), but you need to configure\na sample event to supply to the function first (2).\nFigure 10.3\nApplication detail view of the image resizer service\nFigure 10.4\nThe Function Overview page lets you customize and execute the function.\n",
      "content_length": 402,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "171\nBefore we get started\nYou can use the test event in listing 10.1 to test the function. However, first upload a\nfile from https://commons.wikimedia.org/wiki/File:Happy_smiley_face.png to the\nimage-resizer-service-demo bucket. If you chose to upload a different image, be sure\nto change the object name in the key field in this listing. You need to do this so that\nthe function doesn’t error out looking for an object that doesn’t exist!\n {\n\"Records\": [\n  {\n   \"eventVersion\": \"2.0\",\n   \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n   \"requestParameters\": {\n    \"sourceIPAddress\": \"127.0.0.1\"\n   },\n   \"s3\": {\n    \"configurationId\": \"testConfigRule\",\n    \"object\": {\n     \"eTag\": \"0123456789abcdef0123456789abcdef\",\n     \"sequencer\": \"0A1B2C3D4E5F678901\",\n     \"key\": \"Happy_smiley_face.png\",\n     \"size\": 1024\n    },\n    \"bucket\": {\n     \"arn\": \"arn:aws:s3::: image-resizer-service-demo \",\n     \"name\": \" image-resizer-service-demo \",\n     \"ownerIdentity\": {\n      \"principalId\": \"EXAMPLE\"\n     }\n    },\n    \"s3SchemaVersion\": \"1.0\"\n   },\n   \"responseElements\": {\n    \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuv\n    ➥ wxyzABCDEFGH\",\n    \"x-amz-request-id\": \"EXAMPLE123456789\"\n   },\n   \"awsRegion\": \"us-east-1\",\n   \"eventName\": \"ObjectCreated:Put\",\n   \"userIdentity\": {\n    \"principalId\": \"EXAMPLE\"\n   },\n   \"eventSource\": \"aws:s3\"\n  }\n ]\n}\nInvoke the function a few times to evaluate the behavior; we will use this function to\ndiscuss the various optimizations that follow. When you invoke this function, there are\ndifferent layers in play—the Lambda compute substrate, the execution environment,\nand the function code (figure 10.5). The substrate is invisible to you; the execution\nListing 10.1\nAdding a sample event\n",
      "content_length": 1741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "172\nCHAPTER 10\nBlackbelt Lambda\nenvironment is instantiated on demand for scale events (like a burst of requests); the\nfunction code is instantiated for every request.\n When the first request or event arrives for your function, the AWS Lambda service\nperforms a series of steps. Once the environment exists, Lambda runs the code inside\nyour function handler. Figure 10.6 shows the steps as follows:\n1.\nDownloads your Lambda function Node.js code onto the part of the compute\nsubstrate where your code will run.\n2.\nInstantiates a new execution environment (size is based on your function allo-\ncation) with a Node.js runtime.\n3.\nInstantiates your nonfunction dependencies (in this case, ImageMagick).\n4.\nRuns the parts of your function written outside the handler (we don’t have any\nin this example).\nFunction code\nLanguage \nruntime\nFunction \nexecution \nenvironment\nCompute\nsubstrate \nFigure 10.5\nLayers involved \nin executing a function\nInstantiate\nruntime and\ndependencies\n3\nDownload\nfunction code\n1\nInstantiate new \nexecution\nenvironment\n2\nInstantiate \nnonhandler \ncode\n4\nCache execution\nenvironment\nExecute handler\ncode to\ncompletion\nUnassign\nexecution\nenvironment\nNew\nInvoke\nAssign execution\nenvironment to\nrequest\nUnassigned,\ncached \nexecution\nenvironment?\nNo\nYes\nFigure 10.6\nThe Lambda \nrequest lifecycle\n",
      "content_length": 1311,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "173\nBefore we get started\nIn our ResizeFunction example, when your function handler runs, it processes the\nimage and returns the image metadata. Lambda considers the function as done pro-\ncessing the request when the handler logic (and any threads spawned from within the\nfunction handler) finishes executing. When the request is complete, however, AWS\nLambda does not discard the execution environment (with the run time and code ini-\ntialized). Instead, it caches the execution environment, where all processes inside the\nexecution environment are paused. AWS does not publish any official guidance on how\nlong the environment is retained in this state, but various published experiments\n(https://www.usenix.org/conference/atc18/presentation/wang-liang) show this rang-\ning from 5 to 20 minutes. \n When a subsequent request arrives during this time and a cached execution envi-\nronment is available, AWS Lambda will reuse that execution environment to service\nthe request. On the other hand, if a cached execution environment is not available,\nAWS Lambda will repeat all the steps to serve the request. This has significant implica-\ntions to both the performance of your function and how you write your function; we’ll\ndiscuss this further later in this chapter. One important behavior to remember is that\nAWS Lambda always runs only one request per execution environment. This means\nthat if all execution environments are processing requests and a new one comes in,\nAWS Lambda will instantiate a new execution environment. \n10.2.2 Latency: Cold vs. warm\nThe latency incurred due to steps 1 through 4 in this example (figure 10.6) is referred\nto commonly as the cold start penalty. We refer to the request latency for a request\nincurring a cold start as cold latency, and we refer to the actual function execution\nlatency as the warm latency. As a reminder, you incur the cold start penalty only in two\nsituations. First, you’ll see cold starts if your function has never been invoked before\nor is being invoked after an extended period (such that all cached execution environ-\nments are removed). Second, you’ll see cold starts if there is an increase in the incom-\ning request rate such that AWS Lambda needs to spawn new execution environments\nbecause all available ones are servicing requests.\n For most production scenarios, cold starts impact less than 0.5% of requests, but\ncold starts disproportionally impact functions that are invoked infrequently and func-\ntions having a burst of traffic (specifically for the requests that first lead to the\nincreased traffic). Requests that experience cold starts may also experience timeouts\nbecause the AWS Lambda timeout setting is applied to the total request latency.\n10.2.3 Load generation on your function and application\nAs you go about optimizing your application, you want to do so at a load representative\nof real-life usage. As you can see, your latency characteristics may vary based on load as\nwell. Serverless-artillery is a Nordstrom open source project. It builds on artillery.io and\nserverless.com by using the horizontal scalability and pay-as-you-go nature of AWS\nLambda to instantly and inexpensively throw arbitrary load at your services and report\nresults to an InfluxDB time-series database (other reporting plugins are available). This\n",
      "content_length": 3310,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "174\nCHAPTER 10\nBlackbelt Lambda\ncapability gives you performance and load testing on every commit early in your CI/CD\npipeline, so performance bugs can be caught and fixed immediately. https://\ngithub.com/Nordstrom/serverless-artillery-workshop presents a detailed walk through\non using and setting up the tool.\n10.2.4 Tracking performance and availability\nYou can’t optimize what you can’t measure. Before you go about figuring out how to\nreduce the latency and improve the availability of your serverless application, you\nmust have a consistent approach to monitor this information. AWS offers a variety of\nboth native and third-party tools for this task. To see what’s available, pick any of your\nfunctions on the AWS Lambda console, click the function in the function list in the\nAWS Lambda console, and navigate to the Monitor tab (figure 10.7). You’ll see three\ntools available to you out of the box: CloudWatch metrics (1) on the selected page,\nCloudWatch logs (2), and AWS X-Ray (3).\nIn this chapter, we’ll use CloudWatch metrics and X-Ray as the two primary tools to\nobserve the latency characteristics of the application.\nCLOUDWATCH METRICS \nEach serverless service (like AWS Lambda and API Gateway) emits standard metrics\nthat help you understand the performance and availability characteristics. For\nLambda, AWS offers the following metrics among others:\nInvocations—Total number of requests received by the given function. This is\ninclusive of all requests, independent of whether they were processed success-\nfully, throttled, or resulted in an error. This also includes any requests that were\nretried due to Lambda’s built-in retry policy (more on this later).\nThe Monitor tab has access to all the \nmetrics and insights you will need. You’ll  \nalso be able to access relevant CloudWatch  \nlogs and X-Ray traces from here.\n1\n2\n3\nFigure 10.7\nMonitoring tab for AWS Lambda functions showing three tools for monitoring\n",
      "content_length": 1932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "175\nBefore we get started\nDuration—Measures the elapsed wall clock time from when the function code\nstarts executing (because of an invocation) to when it stops executing. This is a\nreasonable proxy for what your function will be billed, although not exact,\nbecause AWS Lambda rounds your billed duration to the nearest 1 milliseconds.\nErrors—Measures the number of invocations that failed due to errors in the\nfunction. Note that this does not measure errors due to problems in the AWS\nLambda service or due to throttling.\nThrottles—Measures the number of invocations that did not result in your func-\ntion code executing because your function hit either its concurrency limit or\ncaused the account to hit its concurrency limit (1,000 concurrent executions is\nthe default limit but it can be raised by contacting AWS).\nAWS X-RAY\nAWS X-Ray is a service that allows you to detect, analyze, and optimize performance\nissues with your AWS Lambda functions and trace requests across multiple services\nwithin your serverless architecture. X-Ray generates traces for a sample of requests\nthat each function receives, where a trace consists of segments for each service that\nthe request traverses. A segment may further contain subsegments that detail what\nparticular aspect of the service added to the latency of the request. To turn on X-Ray,\nyou must enable Active tracing under the Monitoring and Operations tools on the\nfunction’s Configuration tab. \n As an example, figure 10.8 shows the trace for a simple sample application. You\ncan see the total time spent in Lambda (1), the time your function took to execute\n(2), as well as the time spent in a cold start (3). X-Ray can be a useful tool to deter-\nmine where the bottlenecks in your function execution are, including whether the\ntop contributor is a cold start.\n1\n2\n3\nAWS X-Ray is great for discovering performance \nissues and improving application performance. \nYou get to see a lot of useful information to help \nyou optimize your serverless applications.\nFigure 10.8\nX-Ray trace for a sample application\n",
      "content_length": 2063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "176\nCHAPTER 10\nBlackbelt Lambda\nTHIRD-PARTY TOOLS\nThere’s a growing ecosystem of non-AWS tools that can also be used for performance\nand availability monitoring from well established companies like NewRelic (https://\nnewrelic.com/) and serverless-first companies like Epsagon (https://epsagon.com/).\nWe won’t dive deep into these tools in this chapter, but we encourage you to explore\nall options and choose what works best for you from https://aws.amazon.com/\nlambda/partners/ (the AWS Lambda partner page). \n10.3\nOptimizing latency\nYou now have an understanding of what contributes to your application latency, how\nto generate load to your application to observe the latency, and what tools to observe\nthe latency. In this section, we’ll discuss how to improve it. \n Your best return on effort at optimizing latency is within individual functions. As\nthe core glue and logic component of your application, any changes made to the\nfunction can have direct and immediate impacts to the latency that your customers\nexperience and to your overall application costs. For example, reducing function exe-\ncution time by 10% reduces the cost of the function by 10%, which can be significant\nat high scale. The decision on what percentile and number to optimize for is your\nchoice, depending on your customers. For example, if you are building a website, you\nwant your response time to be less than 2 seconds at the 99th percentile; if you are\nrunning a backend API, you may be able to tolerate 10’s of seconds of response time\nat the 99th percentile.\n10.3.1 Minimize deployment artifact size\nThe size of your deployment package directly impacts your cold start penalty in two\nways. As a reminder, one of the steps that AWS Lambda undertakes on a “cold” invoke\nis downloading your code (step 1 in figure 10.9). \n The larger your function, the longer this step takes—it’s that simple! AWS Lambda\nenforces a limit of 250 MB for your functions’ deployment package, so there’s a natural\nA note on CloudWatch logs\nAs discussed in earlier chapters, CloudWatch logs capture any log activity specified\nwithin a Lambda function. CloudWatch logs can also be used in two additional ways:\nAs a data source for custom metrics—For example, you can emit data points for\nthe time spent within a specific method of your Lambda function and visualize\nand alarm on that information as a custom metric in CloudWatch metrics.\nAs a bridge to surface data to third-party tools—CloudWatch logs makes it\neasy to send data to third-party tools like NewRelic, which in turn can provide\nadditional visualization and tracing. While Lambda does support the inclusion\nof third-party agents directly via AWS Lambda Extensions, CloudWatch logs\nremains an easy way to surface operational information to other services.\n",
      "content_length": 2780,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "177\nOptimizing latency\n“worst case” impact for your deployment package. Second, for functions written in com-\npiled languages like Java and C#, larger deployment packages with many dependencies\ntake longer to instantiate when there are many classes to load into the CLASSPATH. As\nan example, a simple “hello world” on Java loads only 429 classes in the JVM in about\n0.1 seconds, while doing the same “hello world” using Clojure loads 1,988 classes: three\ntimes as much and taking about 1 second. \n A best practice to follow is to audit any function dependencies. Are there any\nheavyweight library dependencies that could be removed or lightweight versions that\ncan be used? Especially look for libraries that act as HTTP servers or agents; they have\nno use inside Lambda functions because Lambda acts as the server for you. For exam-\nple, instead of using the default Java Spring library, you can use the streamlined\nhttps://github.com/awslabs/aws-serverless-java-container library, which is approxi-\nmately 30% faster in experiments. In our example, instead of packaging the entire\nAWS SDK, you could include only the SDK required for accessing S3. You can audit\nyour dependencies for Node.js using tools like https://npm.anvaka.com/, for Python\nusing https://pypi.org/project/modulegraph/, or for Java using the Maven depen-\ndency tree.\n Languages also offer specific tools to reduce deployment package sizes. For exam-\nple, you can use minify for Node.js (https://www.npmjs.com/package/node-minify)\nto reduce the overall size of your Node.js function package. You can also use\nInstantiate\nruntime and\ndependencies\n3\nDownload\nfunction code\n1\nInstantiate new \nexecution\nenvironment\n2\nInstantiate \nnonhandler\ncode\n4\nCache execution\nenvironment\nExecute handler\ncode to\ncompletion\nUnassign\nexecution\nenvironment\nNew\nInvoke\nAssign execution\nenvironment to\nrequest\nUnassigned,\ncached \nexecution\nenvironment?\nNo\nYes\nFigure 10.9\nLambda execution request lifecycle\n",
      "content_length": 1958,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "178\nCHAPTER 10\nBlackbelt Lambda\nProGuard (https://www.guardsquare.com/en/products/proguard) to reduce the\nsize of your Java deployment package (JAR files).\n10.3.2 Allocate sufficient resources to your execution environment\nYour code requires compute resources (CPU, memory) to run. AWS Lambda provides\na single dial to set the resources required by your function: the memory setting. You\ncan change this setting by opening a Lambda function in the AWS console, selecting\nthe Configuration tab, and then selecting Edit next to General Configuration. You\ncan then experiment with different memory allocations (1 in figure 10.10). You can\nalso set the same value via the API and CLI. \nAWS Lambda allocates CPU power proportional to the memory by using the same\nratio as a general-purpose Amazon EC2 instance type such as an M3 type. For exam-\nple, if you allocate 256 MB memory, your Lambda function will receive twice the CPU\nshare than if you allocate only 128 MB. You can update the configuration and request\nadditional memory in 64 MB increments from 128 MB to 10240 MB. This change is\nnot free: AWS Lambda pricing weights the billed duration for your function by its\nmemory setting: 1 second of function execution time at 1024 MB costs the same as 8\nseconds of execution at 128 MB.\nAround December of 2020, AWS Lambda began supporting 10,240 MB of \nmemory (and 6 vCPUs) for new existing Lambda functions.\nAccording to AWS, Lambda functions with 10 GB of memory and 6 vCPUs \ncan be particularly useful for machine learning, modeling, genomics, as well \nas more traditional ETL and media-processing applications. \n1\nFigure 10.10\nIn Edit Basic Settings, you can adjust the amount of memory allocated to the function.\n",
      "content_length": 1716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "179\nOptimizing latency\n Let’s experiment with the memory setting on the image-resizer-service function\nyou created so you can see the impact (if you haven’t, see section 10.2.1 earlier in this\nchapter). Set the memory to 128 MB, 256 MB, 512 MB, and 1024 MB and run a few\ntest invokes using the console (we recommend at least 10). Now note the average exe-\ncution time for those invocations from CloudWatch metrics. You should see results\nsimilar to that in table 10.1. The estimated costs are based on AWS Lambda public\npricing for 1,000 requests to the function.\nWe see that increasing the memory in this case keeps the cost relatively flat, while\nincreasing the performance ~10x. You’ll typically see these kind of gains for CPU-\nbound functions like image processing; more resources can help the function run\nfaster without changing the costs. For I/O-bound operations (such as those waiting\nfor a downstream service to respond), you’ll see no benefit in increasing the resource\nallocation. For lightweight run times like Node.js and Go, you may be able to reduce\nthe setting to the lowest (128 MB); for run times like Java and C#, going lower than\n256 MB can have detrimental effects to how the run time loads your function code.\n Finding the right resource allocation for your function requires some experimen-\ntation. The easiest path is to start with a high setting and reduce it until you see a change\nin performance characteristics. You can use the popular tuning tool at https://\ngithub.com/alexcasalboni/aws-lambda-power-tuning to help estimate your function’s\nresource usage.\n10.3.3 Optimize function logic\nAWS Lambda bills your usage based on the time your function starts executing to the\ntime it stops executing, not by CPU cycles spent or any other time-based metrics. This\nimplies that what your function does during that time is important. Consider the\nTable 10.1\nEstimated costs for 1,000 requests\nMemory\nDuration\nEstimated cost for 1,000 requests\n128 MB\n11.722965s\n$0.024628\n256 MB \n6.678945s\n$0.028035\n512 MB \n3.194954s\n$0.026830 \n1024 MB\n1.465984s\n$0.024638\nResource allocation during cold starts\nAWS Lambda respects the resource allocation while executing your function but will\nattempt to “boost” the CPU available while loading and initializing your function\ndependencies. This means that increasing the resource allocation will not really\nmake a difference to your cold starts.\n",
      "content_length": 2404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "180\nCHAPTER 10\nBlackbelt Lambda\nimage-resizer-service function. When you are downloading the S3 object, your code is\nsimply waiting for S3 service to respond, and you are paying for that wait time. In this\nfunction’s case, the time spent is negligible, but this wait time can get excessive for ser-\nvices that have long response times (for example, waiting on an EC2 instance being\nprovisioned) or wait times (such as downloading a very large file). There are two\noptions to minimize this idle time:\nMinimize orchestration in code—Instead of waiting on an operation inside your\nfunction, use AWS Step functions to separate the “before” and “after” logic as\ntwo separate functions. For example, if you have logic that needs to run before\nand after an API call is made, sequence them as two separate functions and use\nan AWS Step function to orchestrate between them. \nUse threads for I/O intensive operations—You can use multiple threads within a\nLambda function (if the programming language supports it), just like code run-\nning in any compute environment. However, unlike conventional programs,\nthe best use for multi-threading isn’t for parallelizing computations. This is\nbecause Lambda does not allocate multiple cores to Lambda functions running\nwith memory less than 1.8 GB, so you need to allocate more resources to get the\nparallelization benefit. Instead, you can use threads as a way to parallelize I/O\noperations. For example, a Python version of the image_resizer function could\nact on multiple functions by executing the S3 download on a separate thread to\nthumbnailing.\nBy following these best practices, you can significantly reduce the latency (and cost!)\nof your serverless application. Finally, let’s look at concurrency, and we’ll do that in\nthe following section.\n10.4\nConcurrency \nAnother important concept to understand for AWS Lambda functions is concurrency.\nConcurrency is the unit of scale for a Lambda function. Underneath the covers, it maps\nto the number of execution environments assigned to requests. You can estimate the\nconcurrency of your function at any time with the following formula:\nConcurrency = Requests per second (TPS) * Function duration\nUsing peak values will give you peak concurrency; using average values will give you\naverage concurrency. You can monitor the concurrency for any given function (and\nfor the overall account) using the ConcurrentExecutions CloudWatch metric. AWS\nLambda enforces two limits to the concurrency of a function: \nThere is an account-wide soft limit on the total concurrent executions of all functions within\nthe account. This is set by default to 1,000 at the time of writing, and it can be raised\nto desired values through a support case. You can view the account-level setting\nby using the GetAccountSettings API and viewing the AccountLimit object.\n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "181\nConcurrency\nThere is also an account-wide limit on the rate at which you can scale up your concurrent\nexecutions. In larger AWS regions, you are allowed to scale instantly to 3,000 con-\ncurrent and then add 500 concurrent executions every subsequent minute; this\nlimit is lower in smaller regions. These limits may change, so be sure to refer to\nthe latest values listed in http://mng.bz/80PZ.\nThis makes it important to always estimate what your peak and average concurrency\nneeds will be, how quickly you’ll need to ramp up, and to file a request to raise limits\nas needed. \n10.4.1 Correlation between requests, latency, and concurrency\nFor most functions, concurrency increases as a function of requests and function\nduration, subject to the concurrency limits on the function and account. However, for\nfunctions used to process stream data (Kinesis and DynamoDB streams), the concur-\nrency is determined by the number of shards on the stream being processed. Given\nthat latency is determined by the function itself, this means for stream-processing\nfunctions, you may see variable request rate or throughput. To put it another way, \nEffective processing rate = Effective concurrency / average duration (events \n➥ per second)\nConsider a function that takes 1 second to process a stream with 5 shards and with a\nbatch size of 100. This means the maximum number of requests (each with 100\nrecords) that the function can process would be 5, and the maximum number of\nrecords processed at any given time would be 5 * 100 = 500. On the other hand, if the\nsame stream had 10 shards, the throughput would double as well.\n10.4.2 Managing concurrency\nAWS offers two settings for managing concurrency. The first one is the account level\nconcurrency limit that is enforced on the total concurrency across all functions within\nyour account. This limit is set to 1,000 by default and can be raised through a service\nlimit increase ticket: you cannot “self-service” this increase at the time of writing. The\nsecond is a per function concurrency control, which you can use to control the con-\ncurrency of an individual function. You only use the per function concurrency control\nif you have a function that you want to “reserve” concurrency for or a function that\nneeds to be limited in its concurrency (because of a downstream resource). \n For example, you may want to restrict how high a Lambda function scales because\nit calls an API that can only handle a certain load. If this was left unchecked, then your\nfunction could cause the downstream API to be overloaded, causing an availability for\nyour overall application. This makes monitoring concurrency and managing it an\nimportant step to follow. You can learn more about the limits and the controls here:\nhttp://mng.bz/v4mq.\n",
      "content_length": 2775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "182\nCHAPTER 10\nBlackbelt Lambda\nSummary\nServerless applications do not require conventional application performance\nmonitoring steps. Instead, optimizing the performance of your function code\ngives you the most gain.\nUse the toolsets (like X-Ray) and configurations (like the memory setting) to\neasily locate and optimize performance.\nConcurrency for Lambda functions can affect your function latency (and vice\nversa), so ensure you monitor and manage it for your critical functions.\n",
      "content_length": 487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "183\nEmerging practices\nThe term serverless came about after AWS released the Lambda service back in 2014.\nIn that sense, the serverless paradigm (building applications using managed ser-\nvices, including for all your compute needs) is something of a new kid on the\nblock.\n New paradigms give us new ways to look at problems and solve them differently,\nperhaps more efficiently. This should be obvious by now as we have discussed sev-\neral serverless architectures in this book, and you must admit they look very differ-\nent than the equivalent serverful architectures; they are more event-driven, and\nthey often involve many different services working together.\n New paradigms also require us to think and work differently. For example,\ninstead of thinking about cost as a function of the size of a fleet of virtual machines\nand how long you need them for, we need to think about cost in terms of request\nThis chapter covers\nUsing multiple AWS accounts\nUsing temporary stacks\nAvoiding keeping sensitive data in plain text in \nenvironment variables\nUsing EventBridge in event-driven architectures\n",
      "content_length": 1100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "184\nCHAPTER 11\nEmerging practices\ncount and execution duration. The code we write and the way we deploy and monitor\nour applications also need to change to take full advantage of this new paradigm and\nmitigate some of its limitations.\n The following emerging practices are used by teams that have successfully adopted\nserverless technologies in their organization. Many are useful outside of the context of\nserverless, such as using multiple AWS accounts and using EventBridge in an event-\ndriven architecture. Although none of them are silver bullets (is anything?), they are\nuseful in the right contexts and are ideas worth considering.\n11.1\nUsing multiple AWS accounts\nEvery AWS employee you speak to nowadays will tell you that you should have multiple\nAWS accounts and manage them with AWS Organizations (https://aws.amazon\n.com/organizations). At the minimum, you should have at least one AWS account per\nenvironment. For larger organizations, you should go further and have at least one\nAWS account per team per environment. There are many reasons why this is considered\na best practice—regardless of whether you’re working with serverless technologies—\nincluding those discussed in the following sections.\n11.1.1 Isolate security breaches\nImagine the nightmare scenario where an attacker has gained access into your AWS\nenvironment and is then able to access and steal your users’ data. This nightmare sce-\nnario can happen in many ways, and here are three that jump to mind right away:\nAn EC2 instance is exposed publicly and the attacker is able to SSH into the instance\nusing brute force. Once inside, they can use the instance’s IAM role to access\nother AWS resources.\nA misconfigured web application firewall (WAF) allows the attacker to execute a server-\nside request forgery (SSRF) attack and trick the WAF to relay requests to the EC2 metadata\nservice. This allows the attacker to find out the temporary AWS credentials used\nby the WAF server. From here, the attacker is able to access other AWS\nresources in the account. This is what happened in the Capital One data breach\nin 2019.\nAn employee accidentally includes their AWS credentials in a Git commit in a public\nGitHub repo. The attacker scans public GitHub repos for AWS credentials and\nfinds this commit. The attacker is then able to access all the AWS resources that\nthe employee had access to. AWS also scans public GitHub repos for active AWS\ncredentials and warns its customers when it finds them. But the damage is often\ndone already by the time the customer realizes it.\nUsing multiple accounts doesn’t stop these attack vectors, but it limits the blast radius\nof a security breach to a single account (and hopefully not your production account!).\n",
      "content_length": 2731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "185\nUsing multiple AWS accounts\n11.1.2 Eliminate contention for shared service limits\nThroughout this book, we have talked about AWS service limits several times already.\nAs your organization and your system grow, more engineers need to work on the sys-\ntem, and you will likely create more and more services that take care of specific\ndomains within the larger system (think microservices). As this happens, you will\nlikely run into those pesky service limits more frequently because there is more con-\ntention for the shared-service limits. \n It gets worse from here. Because service limits apply at the region level and affect\nall the resources in a region, it means that one team or one service can exhaust all the\navailable throughput (for example, Lambda concurrent executions) in the region\nand throttle everything else. \n What’s more, if all the environments are run from the same AWS account, then\nsomething happening in a non-production environment can also impact users in pro-\nduction. For example, a load test in staging can consume too many Lambda concur-\nrent executions so that users are not able to access your APIs in production because\nthose API functions are throttled.\n Having separate accounts for each team and each environment eliminates the con-\ntention altogether. If a team makes mistakes or experiences a sudden traffic spike in\ntheir services, the extra throughput they consume will not impact other services. Any\nservice limit-related throttling would be contained to that account and limit the blast\nradius of these incidents. Equally, you can safely run load tests in non-production envi-\nronments knowing that they won’t affect your users in production.\n What if, within a team, the same contention exists between different services?\nMaybe one of the team’s services handles much more traffic than the rest and occa-\nsionally causes other services to be throttled. Well, then you want to move that service\ninto its own set of accounts of dev, test, staging, and production. This technique of\nusing AWS accounts as bulkheads to isolate and contain the blast radius can go as far\nas you need. You don’t have to stop at one account per team per environment. Make\nthe techniques work for you, not the other way around.\n11.1.3 Better cost monitoring\nIf everything runs from the same AWS account, then you will have a hard time attrib-\nuting your AWS costs to different environments or teams or services. Having multiple\naccounts lets you see the cost for those accounts easily.\n11.1.4 Better autonomy for your teams\nFrom a security and access control point of view, if each team has its own set of AWS\naccounts, then you can afford to give them more autonomy and control of their own\nAWS accounts. If everyone shares the same AWS account and that account is used for\nboth non-production as well as production environments, then the stakes are high.\nMistakes have a large blast radius and teams can accidentally delete or update other\nteams’ resources, or even delete users’ data in production. That is why you need to be\n",
      "content_length": 3049,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "186\nCHAPTER 11\nEmerging practices\ncareful in terms of managing access. It creates a lot of complexity and stress for whom-\never must manage access (typically the security team or a cloud platform team).\n In my experience, the high stakes and complexity invite gatekeeping and create fric-\ntion between the various disciplines. Feature teams often have to suffer delays as they\nwait for an over-worked platform team to grant them the access they need. Resentment\nbuilds and harmony erodes, and soon it becomes an “us versus them” situation.\n Giving every team their own AWS accounts limits the blast radius of any issues and\nlowers the stakes. You can then afford to give your teams more autonomy within their\nown accounts. The platform team/security team can instead focus on setting up\nguardrails and governance infrastructure so they can identify problems quickly. And\nthey should work with the feature teams to ensure they follow organizational best\npractices and meet your security requirements.\n11.1.5 Infrastructure-as-code for AWS Organizations\nHaving multiple AWS accounts means you need to have some way to manage them,\nespecially as you scale your organization. The number of AWS accounts can grow, and\nas more engineers join the organization, it becomes more important to have strong\ngovernance and oversight of your AWS environment.\n One of the shortcomings of AWS Organizations is that you can’t update the config-\nurations of your organization using infrastructure as code (IaC). For example, Cloud-\nFormation is a regional service and is limited to provisioning resources within a single\naccount and region. At the time of writing, the only tool that allows you to apply IaC\nto AWS Organizations is org-formation (https://github.com/org-formation/org\n-formation-cli). It’s an open source tool that lets you capture the configuration of your\nAWS accounts and the entire AWS organization using IaC. I have used it with several\nprojects and I can’t recommend it highly enough! \n A topic related to using multiple AWS accounts is the use of temporary Cloud-\nFormation stacks for temporary environments, such as those for feature branches or\nto carry out end-to-end (e2e) tests. We discuss temporary stacks next.\n11.2\nUsing temporary stacks\nOne of the benefits of serverless technologies is that you pay for them only when peo-\nple use your application. When your code is not running, you aren’t charged. Com-\nbine this with the fact that it’s easy to deploy a serverless application using tools such\nas the Serverless Framework. Because it’s so easy to create new environments and\nthere is no uptime cost for having these environments, many teams create temporary\nenvironments for when they work on feature branches or to run their e2e tests.\n11.2.1 Common AWS account structure\nIt’s common for teams to have multiple AWS accounts, one for each environment.\nThough there doesn’t seem to be a consensus on how to use these environments, we\ntend to follow these conventions:\n",
      "content_length": 2984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "187\nUsing temporary stacks\nThe dev environment is shared by the team. This is where the latest development\nchanges are deployed to and tested end to end. This environment is unstable by\nnature and shouldn’t be used by other teams.\nThe test environment is where other teams can integrate with your team’s work. This envi-\nronment should be stable so it doesn’t slow down other teams.\nThe staging environment should closely resemble the production environment and may\noften contain dumps of production data. This is where you can stress test your\nrelease candidate in a production-like environment.\nAnd then there’s the production environment.\nAs discussed earlier in this chapter, it’s best practice to have multiple AWS accounts—\nat least one account per team per environment. In the dev account, you can also have\nmore than one environment—one for each developer or each feature branch.\n11.2.2 Use temporary stacks for feature branches\nWhen we start work on a new feature, we still feel our way toward the best solution for\nthe problem. The codebase is unstable and many bugs haven’t been ironed out yet.\nDeploying our half-baked changes to the dev environment can be quite disruptive:\nIt risks destabilizing the team’s shared environment.\nIt overwrites other features the team is working on.\nTeam members may fight over who gets to deploy their feature branch to the\nshared environment.\nInstead, we can deploy the feature branch to a temporary environment. Using the\nServerless Framework is as easy as running the command sls deploy -s my-feature,\nwhere my-feature is both the name of the environment and the name of the Cloud-\nFormation stack. This deploys all the Lambda functions, API Gateway, and any other\nrelated resources such as DynamoDB tables in their own CloudFormation stack. We\nare able to test our work-in-progress feature in an AWS account without affecting\nother team members’ work. \n Having these temporary CloudFormation stacks for each feature branch has negli-\ngible cost overhead. When the developer is done with the feature, the temporary stack\ncan be easily removed by running the command sls remove -s my-feature. However,\nbecause these temporary stacks are an extension of your feature branch, they exhibit the\nsame problems when you have long-lived feature branches. Namely, they get out of sync\nwith other systems they need to integrate with. This applies to the incoming events that\ntrigger your Lambda functions (such as the payloads from SQS/SNS/Kinesis), as well\nas data your function depends on (such as the data schema in DynamoDB tables). We\nfind teams that use serverless technologies tend to move faster, which makes the prob-\nlems with long-lived feature branches more prominent and noticeable.\n As a rule of thumb, don’t leave feature branches hanging around for more than a\nweek. If the work is large and takes longer to implement, then break it up into smaller\n",
      "content_length": 2910,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "188\nCHAPTER 11\nEmerging practices\nfeatures. When you’re working on a feature branch, you should also integrate from\nthe main development branch regularly—no less than once per day.\n11.2.3 Use temporary stacks for e2e tests\nAnother common use of temporary CloudFormation stacks is for running e2e tests.\nOne of the common problems with these tests is that you need to insert test data into\na shared AWS environment. Over time, this adds a lot of junk data in those environ-\nments and can make it difficult for other team members. For example, testers often\nhave to do manual tests on the mobile or web app, and all the test data left by your\nautomated tests can create confusion and make their job more difficult than it needs\nto be. As a rule of thumb, we always do the following:\nInsert the data a test case needs before the test.\nDelete the data after the test finishes.\nUsing the Jest (https://jestjs.io) JavaScript framework, you can capture the before\nand after steps as part of your test suite. They help keep our tests robust and self-\ncontained because they don’t implicitly rely on data to exist. They also help reduce\nthe amount of junk data in the shared dev environment.\n But despite our best intentions, mistakes happen, and sometimes we deliberately\ncut corners to gain agility in the short term. Over time, these shared environments still\nend up with tons of test data. As a countermeasure, many teams employ cron jobs to\nwipe these environments from time to time.\n An emerging practice to combat these challenges is to create a temporary Cloud-\nFormation stack during the CI/CD pipeline. The temporary stack is used to run the e2e\ntests and destroyed afterwards. This way, there is no need to clean up test data, either\nas part of your test fixture or with cron jobs. The drawbacks include the following:\nThe CI/CD pipeline takes longer to run.\nYou still leave test data in external systems, so it’s not a complete solution.\nYou should weigh the benefits of this approach against the delay it adds to your\nCI/CD pipeline. Personally, we think it’s a great approach, and we see more teams\nstarting to adopt it. To make CI/CD pipelines go faster, some teams keep a number of\nthese temporary stacks around and reuse them in a round-robin fashion. This way, you\nstill enjoy the benefit of being able to run e2e tests against a temporary environment\nbut shorten the time it takes to deploy the temporary environment (updating an exist-\ning CloudFormation stack is significantly faster than creating a new stack).\n11.3\nAvoid sensitive data in plain text in environment variables\nOne common mistake we have seen for both serverful and serverless applications is that\nsensitive data (such as API keys and credentials) is left in plain text in environment vari-\nables. When it comes to security, serverless applications are more secure because AWS\ntakes care of the security of the operational environment of our application. This\n",
      "content_length": 2942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "189\nAvoid sensitive data in plain text in environment variables\nincludes securing the virtual machines our code runs on as well as their network con-\nfigurations, and it includes the security of the operating system itself.\n Our Lambda functions run on bare-metal EC2 instances that AWS manages, and\nthe EC2 instances reside in AWS-managed VPCs. There’s no easy way for an attacker to\nfind out information about the virtual machine itself, and there’s no way for attackers\nto SSH into these virtual machines.\n The operating systems are constantly updated and patched with the latest security\npatches, sometimes before the patch is even available to the general public. Such was\nthe case during the Meltdown and Spectre debacle when all EC2 instances behind\nLambda and Fargate were quickly patched against the vulnerabilities long before the\nrest of us were able to patch our container and EC2 images. Having AWS manage the\noperational environment of our code removes a huge class of attack vectors from our\nplate, but we are still responsible for the security of our application and its data.\n11.3.1 Attackers can still get in\nEven though the operational environment of our code is secured by AWS, it’s still pos-\nsible for attackers to get inside the execution environment of our functions via other\nmeans, including the following:\nAttacker successfully executes a code injection attack. For example, if your application\nor any of its dependencies use JavaScript’s eval() function against a piece of\nuser input, then you’re vulnerable to these attacks.\nAttacker compromises one of your dependencies and publishes a malicious version of the\ndependency that steals information from your application at run time. Remember that\ntime when a security researcher gained publish access to 14% of NPM packages\n(http://mng.bz/N4PN)? Or that time an attacker compromised the NPM\naccount for one of EsLint’s maintainers and published a malicious version of\neslint-scope and eslint-config-eslint (http://mng.bz/DKPn)?\nAttacker publishes a malicious NPM package with similar names to popular NPM pack-\nages and steals information from your application on initialization. An example is the\ntime when an attacker published a malicious package called crossenv using the\npopular NPM package cross-env as bait (http://mng.bz/l9d6).\nOnce inside, attackers often steal information from common, easily accessible places\nsuch as environment variables. This is why it’s so important that we avoid putting sen-\nsitive data in plain text in environment variables.\n11.3.2 Handle sensitive data securely\nSensitive data should be encrypted both in transit and at rest. This means it should be\nstored in an encrypted form; within AWS, you can use both the SSM Parameter Store\nand the Secrets Manager to store it. Both services support encryption at rest, integrate\ndirectly with AWS Key Management Service (KMS), and allow you to use Customer\nManaged Keys (CMKs). The same encrypted at-rest principle should be applied to\n",
      "content_length": 2996,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "190\nCHAPTER 11\nEmerging practices\nhow sensitive data is stored in your application. There are multiple ways to achieve\nthis; for example:\nStore the sensitive data in encrypted form in environment variables and decrypt\nit using KMS during cold start.\nKeep the sensitive data in SSM Parameter Store or Secrets Manager, and during\nthe Lambda function cold start, fetch it from SSM Parameter Store/Secrets\nManager.\nOnce decrypted, the data can be kept in an application variable or closure where it\ncan be easily accessed by your code. The important thing is that sensitive data should\nnever be placed back into the environment variables in unencrypted form. Our per-\nsonal preference is to fetch sensitive data from the SSM Parameter Store/Secrets Man-\nager during cold start. We would use middy’s SSM middleware (https://github.com/\nmiddyjs/middy/tree/main/packages/ssm) to inject the decrypted data into the\ncontext variable and cache it for some time.\n This way, we can rotate these secrets at the source without having to redeploy the\napplication. Once the cache expires, the middleware fetches the new values on the\nnext Lambda invocation. It also makes it easier to manage shared secrets where multi-\nple services need to access the same secret. Finally, this approach allows more granu-\nlar control of permissions because the Lambda function requires permissions to\naccess the secrets in SSM Parameter Store/Secrets Manager.\n There are other variants of these two approaches; for example, instead of storing\nencrypted secrets in environment variables, you can store them in an encrypted file\nthat is deployed as part of the application. During Lambda cold start, this file is\ndecrypted with KMS, and the secrets it contains are then extracted and stored away\nfrom the environment variables.\n11.4\nUse EventBridge in event-driven architectures\nAmazon SNS and SQS have long been the go-to option for AWS developers when it\ncomes to service integration. However, since its rebranding, Amazon EventBridge\n(formerly Amazon CloudWatch Events) has become a popular alternative, and I\nwould argue that it’s actually a much better option as the event bus in an event-driven\narchitecture.\n11.4.1 Content-based filtering\nSNS lets you filter messages via filtering policies. But you can’t filter messages by their\ncontent, you can only filter by message attributes, and you can only have up to 10 attri-\nbutes per message. If you require content-based filtering, then it has to be done in\ncode. EventBridge, on the other hand, supports content-based filtering and lets you\npattern match against an event’s content. In addition, it supports advanced filtering\nrules such as these:\n",
      "content_length": 2673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "191\nUse EventBridge in event-driven architectures\nNumeric comparison\nPrefix matching\nIP address matching\nExistence matching\nAnything-but matching\nNOTE\nCheck out the blog post at http://mng.bz/B1w0 on EventBridge’s\ncontent-based filtering for more details on these advanced rules.\nIn an event-driven architecture, it’s often desirable to have a centralized event bus. It\nmakes it easy for subsystems to subscribe to events triggered by any other subsystem\nand for you to create an archive that captures everything happening in the whole\napplication (for both audit and replay purposes).\n With content-based filtering, it’s possible to have a centralized event bus in Event-\nBridge. Subscribers can freely subscribe to the exact events they want without having\nto negotiate with the event publishers on what attributes to include. This is usually not\nfeasible with SNS, and you have to use multiple SNS topics.\n11.4.2 Schema discovery\nA common challenge with event-driven architectures is identifying and versioning\nevent schemas. EventBridge deals with this challenge with its schema registry and pro-\nvides a built-in mechanism for schema discovery.\n EventBridge captures a wide range of events from AWS services (such as when an\nEC2 instance’s state has changed) in the default event bus. It provides the schema for\nthese AWS events in the default schema registry. You also can enable schema discovery\non any event bus, and EventBridge samples the ingested events and generates and ver-\nsions schema definitions for these events.\n If you’re programmatically generating schema definitions for your application\nevents already, then you can also create a custom schema registry and publish your\nschema definitions there as part of your CI/CD pipeline. That way, your developers\nalways have an up-to-date list of the events in circulation and what information they\ncan find on these events.\n Open-source tools such as the evb-cli (https://www.npmjs.com/package/\n@mhlabs/evb-cli) even let you generate EventBridge patterns using the schema defi-\nnitions in a schema registry. This is handy, especially if you’re new to EventBridge’s\npattern language!\n11.4.3 Archive and replay events\nAnother common requirement for event-driven architectures is to be able to archive\nthe ingested events and replay them later. The archive requirement is often part of a\nlarger set of audit or compliance requirements and is therefore a must-have in many\nsystems. Luckily, EventBridge offers archive and replay capabilities out of the box.\nWhen you create an archive, you can configure the retention period, which can be set\n",
      "content_length": 2608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "192\nCHAPTER 11\nEmerging practices\nto indefinite. You can optionally configure a filter so that only matching events are\nincluded in the archive.\n When you need to replay events from the archive, you can choose a start and end\ntime so that only the events captured in the specified time range will be replayed. One\nthing to keep in mind about event replays is that EventBridge does not preserve the\noriginal order of the events as they were received. Instead, EventBridge looks to replay\nthese events as quickly as possible, which means you can expect a lot of concurrency\nand that most events will be replayed out of sequence.\n If ordering is important to you when replaying events, then you should check out\nthe evb-cli project mentioned earlier. Its evb replay command supports paced\nreplays, which retains the ordering of events and lets you control how quickly events\nare replayed. For example, using a replay speed of 100 replays events in real time\nmeans replaying an hour’s worth of events would take an hour.\n11.4.4 More targets\nWhereas SNS supports a handful of targets (such as HTTP, Email, SQS, Lambda, and\nSMS), EventBridge supports more than 15 AWS services (including SNS, SQS, Kinesis,\nand Lambda), and you can forward events to another EventBridge bus in another\naccount.\n This extensive reach helps to remove a lot of unnecessary glue code. For example,\nto start a Step Functions state machine, you would have needed a Lambda function\nbetween SNS and Step Functions. With EventBridge, you can connect the rule to the\nstate machine directly.\n11.4.5 Topology\nThere are different ways to arrange event buses in EventBridge. For example, you can\nhave a centralized event bus, every service can publish events to their own event bus,\nor maybe you have a few domain-specific event buses that are shared by related ser-\nvices. There is no clear consensus on which approach is the best because everyone’s\ncontext is different, and each approach has its pros and cons. However, we personally\nfavor the centralized event bus approach because it has some great advantages includ-\ning the following:\nYou can implement an archive and a schema registry in one place.\nYou can manage access and permissions in one place.\nAll the events you need are available in one event bus.\nThere are fewer resources to manage.\nBut it also has some shortcomings that you need to consider:\nThere is a single point of failure. Having said that, EventBridge is already highly\navailable, and the infrastructure that ingests, filters, and forwards events to con-\nfigured targets is distributed across multiple availability zones.\nService teams have less autonomy as they all depend on the centralized event\nbus.\n",
      "content_length": 2700,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "193\nSummary\nThere is also the question of AWS account topology. That is, which account do you\ndeploy the event bus to if a given environment consists of multiple AWS accounts\n(such as when you have one account per team)? Should you deploy the centralized\nevent bus in its own account or in the account that perhaps make the most sense?\nThat is a wider topic that is outside the scope of this chapter, but I recommend you\ncheck this re:Invent 2020 session by Stephen Liedig: https://www.youtube.com/\nwatch?v=Wk0FoXTUEjo. It goes into detail about the different configurations and the\npros and cons of each.\nSummary\nAnd that’s it for a list of emerging practices that you should seriously consider adopt-\ning in your projects. We call these emerging practices because they are not adopted ubiq-\nuitously but are gaining traction in the AWS community. As the AWS ecosystem and\nserverless technologies develop and mature, more practices emerge and take root. It’s\nworth remembering that no practice should be considered best in its own right, and\nyou must always consider the context and environment a practice is applied in.\n As technology and your organization change, your context changes too. Many of the\nthings that you might once consider as best practice can easily become anti-patterns.\nFor example, monorepos work great when you are a small team, but by the time you\ngrow to hundreds or perhaps thousands of engineers, monorepos present many chal-\nlenges that require complex solutions to address. \n The same goes for how we build, test, deploy, and operate software. What worked\ngreat in private data centers and server farms might not translate well to the cloud.\nAnd practices that serve us well when we have to manage both the infrastructure our\ncode runs on as well as the code itself might work against us as we build applications\nwith serverless technologies.\n Best practices and design patterns should be the start of the conversation, not the\nend. After all, these so-called best practices and design patterns are collective docu-\nmentations of things that others have done that worked for them to some degree at\nsome time. There’s no guarantee that they’ll work for you today. And it’s easy to see\nparallels from other industries. For example, did you know that lobotomies were part\nof mainstream mental healthcare from 1930s to 1950s before they were outlawed in\nthe 1970s and considered outright barbaric by today’s standards?\n",
      "content_length": 2444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "195\nappendix A\nServices for your\n serverless architecture\nAWS is a giant playground of different services and products you can use to build\nserverless applications. Lambda is a key service that we discussed in this book, but\nother services and products can be just as useful, if not crucial, for solving certain\nproblems. There are many excellent non-AWS products too, so don’t feel obligated\nto use only what Amazon has to offer. Have a look at the offerings from Microsoft\nand Google too. The following sections provide a sample of services that we’ve\nfound useful. You can use this appendix as a guide to various services and products\nwe’ll discuss throughout the book.\nA.1\nAPI Gateway\nThe Amazon API Gateway is a service that you can use to create an API layer\nbetween the frontend and backend services. The lifecycle management of the API\nGateway allows multiple versions of the API to be run at the same time, and it sup-\nports multiple release stages such as development, staging, and production. API\nGateway also comes with useful features like caching and throttling requests.\n The API is defined around resources and methods. A resource is a logical entity\nsuch as a user or product. A method is a combination of an HTTP verb (such as GET,\nPOST, PUT, or DELETE) and the resource path. API Gateway integrates with\nLambda and other AWS services. It can be used as a proxy service and forward\nrequests to regular HTTP endpoints.\nA.2\nSimple Notification Service (SNS)\nAmazon Simple Notification Service (SNS) is a scalable pub/sub service designed\nto deliver messages. Producers or publishers create and send messages to a topic.\nSubscribers or consumers subscribe to a topic and receive messages over one of the\n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "196\nAPPENDIX A\nServices for your serverless architecture\nsupported protocols. SNS stores messages across multiple servers and data centers for\nredundancy and guarantees at-least-once delivery. At-least-once delivery stipulates that\na message will be delivered at least once to a subscriber, but on rare occasions, due to\nthe distributed nature of SNS, it may be delivered multiple times.\n In cases where a message can’t be delivered by SNS to HTTP endpoints, it can be\nconfigured to retry deliveries at a later time. SNS can also retry failed deliveries to\nLambda when throttling is applied. SNS supports message payloads of up to 256 KB.\nA.3\nSimple Storage Service (S3) \nSimple Storage Service (S3) is Amazon’s scalable storage solution. Data in S3 is stored\nredundantly across multiple facilities and servers. The event notifications system\nallows S3 to send events to SNS, SQS, or Lambda when objects are created or deleted.\nS3 is secure, by default, with only owners having access to the resources they create,\nbut it’s possible to set more granular and flexible access permissions using access con-\ntrol lists and bucket policies.\n S3 uses the concept of buckets and objects. Buckets are high-level directories or\ncontainers for objects. Objects are a combination of data, metadata, and a key. A key is a\nunique identifier for an object in a bucket. \n S3 also supports the concept of a folder as a means of grouping objects in the S3\nconsole. Folders work by using key name prefixes. A forward slash character (/) in the\nkey name delineates a folder. For example, an object with the key name documents/\npersonal/myfile.txt is represented as a folder called documents, containing a folder\ncalled personal, containing the file myfile.txt in the S3 console.\nA.4\nSimple Queue Service (SQS)\nSimple Queue Service (SQS) is Amazon’s distributed and fault-tolerant queuing ser-\nvice. It ensures at-least-once delivery of messages similar to SNS and supports message\npayloads of up to 256 KB. SQS allows multiple publishers and consumers to interact\nwith the same queue, and it has a built-in message lifecycle that automatically expires\nand deletes messages after a preset retention period. As with most AWS products,\nthere are access controls to help control access to the queue. SQS integrates with SNS\nto automatically receive and queue messages.\nA.5\nSimple Email Service (SES)\nSimple Email Service (SES) is a service designed to send and receive email. SES han-\ndles email-receiving operations such as scanning for spam and viruses and rejection of\nemail from untrusted sources. Incoming email can be delivered to an S3 bucket or\nused to invoke a Lambda notification, or create an SNS notification. These actions\ncan be configured as part of the receipt rule, which tells SES what to do with the email\nonce it arrives.\n Sending emails with SES is straightforward, but there are limits that are in place to\nregulate the rate and the number of messages sent. SES automatically increases the\nquota as long as high-quality email, and not spam, is sent.\n",
      "content_length": 3049,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "197\nAPPENDIX A\nServices for your serverless architecture\nA.6\nRelational Database Service (RDS)\nAmazon Relational Database Service (RDS) is a web service that helps with the setup\nand operation of a relational database in the AWS infrastructure. RDS supports the\nAmazon Aurora, MySQL, MariaDB, Oracle, MS-SQL, and PostgreSQL database\nengines. It takes care of routine tasks such as provisioning, backup, patching, recovery,\nrepair, and failure detection. Monitoring and metrics, database snapshots, and multiple\navailability zone (AZ) support are provided out of the box. RDS uses SNS to deliver noti-\nfications when an event occurs. This makes it easy to respond to database events such\nas creation, deletion, failover, recovery, and restoration when they happen.\nA.7\nDynamoDB\nDynamoDB is Amazon’s NoSQL database. Tables, items, and attributes are Dynamo’s\nmain concepts. A table stores a collection of items. An item is made up of a collection of\nattributes. Each attribute is a simple piece of data such as a person’s name or phone\nnumber. Every item is uniquely identifiable. Lambda integrates with DynamoDB\ntables and can be triggered by a table update. Global tables is a notable feature of\nDynamo that seamlessly replicates tables across different AWS regions and resolves any\ndata conflicts (using “last writer wins” reconciliation to handle concurrent updates). It\nmakes DynamoDB a good database for scalable, global applications. Finally, an in-\nmemory cache (DAX) is available for DynamoDB. It shortens the response time but\ncomes at a price.\nA.8\nAlgolia\nAlgolia is a (non-AWS) managed search engine API. It can search through semi-\nstructured data and has APIs to allow developers to integrate search directly into their\nwebsites and mobile applications. One of Algolia’s outstanding capabilities is its speed.\nAlgolia can distribute and synchronize data across 15 regions around the world and\ndirect queries to the closest data center. \n Algolia has a concept of indices (“. . . an entity where you import the data you want\nto search . . . analogous to a table within a database . . .”), records (“. . . a JSON schema-\nless object that you want to be searchable . . .”) and operations (which are essentially\natomic actions such as update or delete). These concepts are straightforward and\nmake Algolia one of the easier search platforms to use. Paid plans begin from about\n$35 per month but can quickly grow in cost, depending on the number of records and\noperations performed by your application and users.\nA.9\nMedia Services\nAWS Media Services is a new product designed for developers to build video work-\nflows. Media Services consist of the following products:\nMediaConvert is designed to transcode between different video formats at scale.\nMediaLive is a live video-processing service. It takes a live video source and com-\npresses it into smaller versions for distribution.\n",
      "content_length": 2894,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "198\nAPPENDIX A\nServices for your serverless architecture\nMediaPackage enables developers to implement video features such as pause\nand rewind. It can also be used to add Digital Right Management (DRM) to\ncontent.\nMediaStore is a storage service optimized for media. Its aim is to provide a low-\nlatency storage system for live and on-demand video content.\nMediaTailor enables developers to insert individually targeted ads in to the\nvideo stream.\nMedia Services provide an advanced suite of services that are superior to Elastic\nTranscoder. Nevertheless, Elastic Transcoder has a few features (such as the ability to\ncreate WebM files and animated GIFs) that Media Services is missing.\nA.10\nKinesis Streams\nKinesis Streams is a service for real-time processing of streaming big data. It’s typically\nused for quick log and data intake, metrics, analytics, and reporting. It’s different\nfrom SQS in that Amazon recommends that Kinesis Streams be used primarily for\nstreaming big data, whereas SQS is used as a reliable hosted queue, especially if more\nfine-grained control over messages such as visibility timeouts or individual delays is\nrequired. \n In Kinesis Streams, shards specify the throughput capacity of a stream. The number\nof shards needs to be stipulated when the stream is created, but resharding is possible\nif throughput needs to be increased or reduced. In comparison, SQS makes scaling\nmuch more transparent. Lambda can integrate with Kinesis to read batches of records\nfrom a stream as soon as they’re detected.\nA.11\nAthena\nAWS bills Athena as a serverless interactive query service. Essentially, this service allows\nyou to query data placed into S3 using standard SQL. In a lot of cases, there’s no need\nto run ETL (extract, transform, and load) jobs to transform your data before querying\ncan take place (although you can combine Athena with AWS Glue if you needed to\ntransform your data a certain way). As a user, you upload data to S3, prepare a\nschema, and begin querying almost immediately. \nA.12\nAppSync\nAppSync is billed as allowing developers to create “ . . . data driven apps with real-time\nand offline capabilities.” In reality, AppSync is a managed GraphQL endpoint pro-\nvided by AWS. It integrates with DynamoDB, Lambda, and Amazon Elasticsearch. If\nyou are familiar with GraphQL and GraphQL schemas, you can get started with\nAppSync straight away. If you are not familiar with GraphQL, we recommend doing a\nbit of reading beforehand (http://graphql.org/learn/). GraphQL has certainly been\nfinding its share of acclaim over the past few years, particularly among adopters of\nserverless technologies.\n",
      "content_length": 2633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "199\nAPPENDIX A\nServices for your serverless architecture\nA.13\nCognito\nAmazon Cognito is an identity management service. It integrates with public identity\nproviders such as Google, Facebook, Twitter, and Amazon or with your own system.\nCognito supports user pools, which allow you to create your own user directory. This\nlets you register and authenticate users without having to run a separate user database\nand authentication service. Cognito supports synchronization of user application data\nacross different devices and has offline support that allows mobile devices to function\neven when there’s no internet access.\nA.14\nAuth0\nAuth0 (recently acquired by Okta) is a non-AWS identity management product that\nhas a few features that Cognito doesn’t. Auth0 integrates with more than 30 identity\nproviders including Google, Facebook, Twitter, Amazon, LinkedIn, and Windows\nLive. It provides a way to register new users through the use of its own user database,\nwithout having to integrate with an identity provider. In addition, it has a facility to\nimport users from other databases. As expected, Auth0 supports standard industry\nprotocols including SAML, OpenID Connect, OAuth 2.0, OAuth 1.0, and JSON Web\nToken (JWT). It’s simple to integrate with AWS Identity, Access Management, and\nCognito.\nA.15\nOther services\nThe list of services provided in this section is a short sample of the different products\nyou can use to build your application. There are many more services, including those\nprovided by large cloud-focused companies such as Google and Microsoft and smaller,\nindependent companies like Auth0. There are also auxiliary services that you need to\nbe aware of. These can help you be more efficient and build software faster, improve\nperformance, or achieve other goals. When building software, consider the following\nproducts and services: \nContent Delivery Networks (CloudFront, CloudFlare)\nDNS management (Route 53) \nCaching (ElastiCache)\nSource control (GitHub, GitLab) \nContinuous integration and deployment (GitHub Actions)\nFor every service suggestion, you can find alternatives that may be just as good or even\nbetter, depending on your circumstances. We urge you to do more research and\nexplore the various services that are currently available. \n",
      "content_length": 2274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "200\nappendix B\nSetting up your cloud\nMost of the architecture described in this book is built on top of AWS. This means\nyou need a clear understanding of AWS from the perspectives of security, alerting,\nand costs. It doesn’t matter whether you use Lambda alone or have a large mix of\nservices. Being able to configure security, knowing how to set up alerts, and con-\ntrolling cost are important. This appendix is designed so that you can understand\nthese concerns and learn where to look for important information in AWS. \n AWS security is a complex subject, but this appendix gives you an overview of\nthe difference between users and roles and shows you how to create policies. This\ninformation is needed to configure a system in which services can communicate\neffectively and securely. Some of the time, you will not need to create or configure\npolicies directly; tools like Serverless framework will do it for you. But it’s still\nimportant to understand how the pieces fit together and where to look for help if\nthings go wrong. \n Cost is an important consideration when using a platform such as AWS and\nimplementing serverless architecture. It’s essential to understand the cost calcula-\ntion of the services you’re going to use. This is useful not only for avoiding bill\nshock but also for predicting next month’s bill and beyond. We look at estimating\nthe cost of services and discuss strategies for tracking costs and keeping them\nunder control. This appendix is not an exhaustive guide to AWS. If you have fur-\nther questions after reading this appendix, take a look at AWS documentation\n(https://aws.amazon.com/documentation).\nB.1\nSecurity model and identity management\nIn chapter 2, you created an Identity and Access Management (IAM) user and a\nnumber of roles in order to use Lambda, S3, and MediaConvert. In this section,\nyou’ll take your new-found knowledge and develop it further by learning about\nusers, groups, roles, and policies in more detail. \n",
      "content_length": 1965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "201\nAPPENDIX B\nSetting up your cloud\nB.1.1\nCreating and managing IAM users\nAs you’ll recall, an IAM user is an entity in AWS that identifies a human user, an appli-\ncation, or a service. A user normally has a set of credentials and permissions that can\nbe used to access resources and services across AWS.\n An IAM user typically has a friendly name to help you identify the user and an\nAmazon Resource Name (ARN) that uniquely identifies it across AWS. Figure B.1\nshows a summary page and an ARN for a fictional user named Alfred. You can get to\nthis summary in the AWS console by clicking IAM, clicking Users in the navigation\npane, and then clicking the name of the user you want to view.\nYou can create IAM users to represent human users, applications, or services. IAM\nusers created to work on behalf of an application or a service sometimes are referred\nto as service accounts. These types of IAM users can access AWS service APIs using an\naccess key. An access key for an IAM user can be generated when the user is initially\ncreated, or you can create it later by clicking Users in the IAM console, clicking the\nrequired user name, selecting Security Credentials, and then clicking the Create\nAccess Key button.\n The two components of an access key are the Access Key ID and the Secret Access\nKey. The Access Key ID can be shared publicly, but the Secret Access Key must be kept\nhidden. If the Secret Access Key is revealed, the whole key must be immediately invali-\ndated and recreated. An IAM user can have, at most, two active access keys.\n If an IAM user is created for a real person, then that user should be assigned a\npassword. This password allows a human user to log into the AWS console and use ser-\nvices and APIs directly. To create a password for an IAM user, follow these steps:\n1.\nIn the IAM console, click Users in the navigation pane.\n2.\nClick the required username to open the user’s settings. \n3.\nClick the Security Credentials tab and then click Manage next to Console pass-\nword (figure B.2).\nThe ARN of the user Alfred\nFigure B.1\nThe IAM console shows metadata such as the ARN, groups, and creation time \nfor every IAM user in your account.\n",
      "content_length": 2169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "202\nAPPENDIX B\nSetting up your cloud\n4.\nIn the popup, choose whether to enable or disable console access, type in a new\ncustom password, or let the system autogenerate one. You can also force the\nuser to create a new password at the next sign-in (figure B.3).\nThe Manage option is \navailable for any IAM user. \nUsers with passwords can \nlog into the AWS Console. \nFigure B.2\nIAM users have a number of options including being able to set a password, change \naccess keys, and enable multifactor authentication.\nAsking the user to set a new password is good practice, \nas long as a good password policy is established.\nFigure B.3\nMake sure to create a good password policy with a high degree of complexity if you allow \nusers to log into the AWS console. Password policy can be set up in Account Settings of the IAM console.\n",
      "content_length": 823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "203\nAPPENDIX B\nSetting up your cloud\nAfter a user is assigned a password, they can log into the AWS console by navigating to\nhttps://<Account-ID>.signin.aws.amazon.com/console. To get the account ID, click\nSupport in the upper-right navigation bar, and then click Support Center. The\naccount ID (or account number) is shown at the top of the console. You may want to\nset up an alias for the account ID also so that your users don’t have to remember it\n(for more information about aliases, see http://amzn.to/1MgvWvf). \nB.1.2\nGroups\nGroups represent a collection of IAM users. They provide an easy way to specify per-\nmissions for multiple users at once. For example, you may want to create a group for\ndevelopers or testers in your organization or have a group called Lambda to allow all\nmembers of that group to execute Lambda functions. Amazon recommends using\ngroups to assign permissions to IAM users rather than defining permissions individu-\nally. Any user who joins a group inherits permissions assigned to the group. Similarly,\nif a user leaves a group, the group’s permissions are removed from the user. Further-\nmore, groups can contain only users, not other groups or entities such as roles.\nB.1.3\nRoles\nA role is a set of permissions that a user, application, or a service can assume for a\nperiod of time. A role is not uniquely coupled to a specific user, nor does it have asso-\nciated credentials such as passwords or access keys. It’s designed to grant permissions\nto a user or a service that typically doesn’t have access to the required resource.\nMulti-factor authentication\nMulti-factor authentication (MFA) adds another layer of security by prompting users\nto enter an authentication code from their MFA device when they try to sign into the\nconsole (this is in addition to the usual username and password). It makes it more\ndifficult for an attacker to compromise an account. Any modern smartphone can act\nas a virtual MFA appliance using an application such as Google Authenticator or AWS\nVirtual MFA. It’s recommended that you enable MFA for any user who might use the\nAWS console. You’ll find the option Assign MFA Device in the Security Credentials tab\nwhen you click an IAM user in the console.\nTemporary security credentials\nAt this time, there’s a limit of 5,000 users per AWS account, but you can raise the\nlimit if needed. An alternative to increasing the number of users is to use temporary\nsecurity credentials. Temporary security credentials can be set up to expire after a\nshort while and can be generated dynamically. See Amazon’s online documentation\nat http://mng.bz/drnN for more information on temporary security credentials. You\ncan find more information about IAM users at http://mng.bz/r6zB.\n",
      "content_length": 2733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "204\nAPPENDIX B\nSetting up your cloud\n Delegation is an important concept associated with roles. Put simply, delegation is\nconcerned with the granting of permissions to a third party to allow access to a partic-\nular resource. It involves establishing a trust relationship between a trusting account\nthat owns the resource and a trusted account that contains the users or applications\nthat need to access the resource. Figure B.4 shows a role with a trust relationship\nestablished for a service called CloudCheckr.\nFederation is another concept that’s discussed often in the context of roles. Federation\nis the process of creating a trust relationship between an external identity provider\nsuch as Facebook, Google, or an enterprise identity system that supports Security\nAssertion Markup Language (SAML) 2.0 and AWS. It enables users to log in via one of\nthose external identity providers and assume an IAM role with temporary credentials.\nB.1.4\nResources\nPermissions in AWS are either identity-based or resource-based. Identity-based permissions\nspecify what an IAM user or a role can do. Resource-based permissions specify what an\nAWS resource such as an S3 bucket or an SNS topic is allowed to do or who can have\nTrusted entities define which entities \nare allowed to assume the role.\nAn external ID prevents the confused \ndeputy problem, which is a form of \nprivilege escalation. It is needed if you \nhave configured access for a third party \nto gain entry to your AWS account.\nFigure B.4\nThis role grants CloudCheckr access to the AWS account to perform analysis of costs \nand recommend improvements.\n",
      "content_length": 1606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "205\nAPPENDIX B\nSetting up your cloud\naccess to it. A resource-based policy often specifies who has access to the given\nresource. This allows trusted users to access the resource without having to assume a\nrole. The AWS user guide at http://mng.bz/VBJP states: \nCross-account access with a resource-based policy has an advantage over a role. With a\nresource that is accessed through a resource-based policy, the user still works in the trusted\naccount and does not have to give up his or her user permissions in place of the role\npermissions. In other words, the user continues to have access to resources in the trusted\naccount at the same time as he or she has access to the resource in the trusting account.\nNot all AWS services support resource-based policies (the user guide at http://\nmng.bz/xX8W lists all the services that do).\nB.1.5\nPermissions and policies\nWhen you initially create an IAM user, it’s not able to access or do anything in your\naccount. You need to grant the user permissions by creating a policy that describes\nwhat the user is allowed to do. The same goes for a new group or role. A new group or\na role needs to be assigned a policy to have any effect. \n The scope of any policy can vary. You can give your user or role administrator access\nto the whole account or specify individual actions. It’s better to be granular and specify\nonly permissions that are needed to get the job done (least privilege access). Start with\na minimum set of permissions and add additional permissions only if necessary. \n There are two types of policies: managed and inline. Managed policies apply to\nusers, groups, and roles but not to resources. Managed policies are standalone. Some\nmanaged policies are created and maintained by AWS. You also can also create and\nmaintain customer-managed policies. Managed policies are great for reusability and\nchange management. If you use a customer-managed policy and decide to modify it,\nall changes are automatically applied to all IAM users, roles, and groups that the pol-\nicy is attached to. Managed policies allow for easier versioning and rollbacks. \n Inline policies are created and attached directly to a specific user, group, or role.\nWhen an entity is deleted, the inline policies embedded within it are deleted also.\nResource-based policies are always inline. To add an inline or a managed policy, click\nthe required user, group, or role and then click the Permissions tab. You can attach,\nview, or detach a managed policy and similarly create, view, or remove an inline policy.\n A policy is specified using JSON notation. The following listing shows a managed\nAWSLambdaExecute policy. \n{  \n   \"Version\":\"2012-10-17\",   \n   \"Statement\":[                 \n      {  \n         \"Effect\":\"Allow\",\nListing B.1\nAWSLambdaExecute policy\nVersion specifies the policy language version; the current version is 2012-10-17. If you’re \ncreating a custom policy, make sure to include the version and set it to 2012-10-17.\nContains one or more statements that specify \nthe actual permissions that make up the policy\n",
      "content_length": 3060,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "206\nAPPENDIX B\nSetting up your cloud\n         \"Action\": \"logs:*\",\n         \"Resource\":\"arn:aws:logs:*:*:*\"\n      },\n      {  \n         \"Effect\":\"Allow\",      \n         \"Action\":[              \n            \"s3:GetObject\",\n            \"s3:PutObject\"\n         ],\n         \"Resource\":\"arn:aws:s3:::*\" \n      }\n   ]\n}\nMany IAM policies contain additional elements such as Principal, Sid, and Condi-\ntion. The Principal element specifies an IAM user, an account, or a service that’s\nallowed or denied access to a resource. The Principal element isn’t used in policies\nthat are attached to IAM users or groups. Instead, it’s used in roles to specify who can\nassume the role. It’s also common to resource-based policies. Statement ID (Sid) is\nrequired in policies for certain AWS services, such as SNS. A condition allows you to\nspecify rules that dictate when a policy should apply. An example of a condition is pre-\nsented in the next listing. \n\"Condition\": {\n   \"DateLessThan\": {     \n               \"aws:CurrentTime\": \"2020-09-12T12:00:00Z\"\n        },\n        \"IpAddress\": {\n               \"aws:SourceIp\": \"127.0.0.1\"   \n        }\n   }\n \nListing B.2\nPolicy condition\nThe Effect element is required and specifies \nwhether the statement allows or denies \naccess to the resource. The only two \navailable options are Allow and Deny.\nSpecifies the specific actions on the resource that should \nbe allowed or denied. The use of a wildcard (*) character \nis allowed (for example, “Action”: “s3:*”).\nThe Resource element identifies the object or objects \nthat the statement applies to. It can be specific or \ninclude a wildcard to refer to multiple entities.\nYou can use a number of conditional elements, which include DateEquals, DateLessThan, \nDateMoreThan, StringEquals, StringLike, StringNotEquals, and ArnEquals.\nThe condition keys represent values that \ncome from the request issued by a user. \nPossible keys include SourceIp, \nCurrentTime, Referer, SourceArn, userid, \nand username. The value can be either a \nspecific literal value such as “127.0.0.1” or \na policy variable.\nMultiple conditions\nThe AWS documentation at http://amzn.to/21UofNi states “If there are multiple con-\ndition operators, or if there are multiple keys attached to a single condition operator,\nthe conditions are evaluated using a logical AND. If a single condition operator\nincludes multiple values for one key, that condition operator is evaluated using a log-\nical OR.” See http://amzn.to/21UofNi for great examples you can follow and a whole\nheap of useful documentation.\n",
      "content_length": 2545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "207\nAPPENDIX B\nSetting up your cloud\nAmazon recommends using conditions to the extent that is practical for security. The\nnext listing, for example, shows an S3 bucket policy that forces content to be served\nonly over HTTPS/SSL. This policy refuses connections over unencrypted HTTP.\n{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"123\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Deny\",                \n            \"Principal\": \"*\",\n            \"Action\": \"s3:*\",                    \n            \"Resource\": \"arn:aws:s3:::my-bucket/*\",\n            \"Condition\": {\n                \"Bool\": {\n                    \"aws:SecureTransport\": false  \n                }\n            }\n        }\n    ]\n}\nB.2\nCost\nReceiving an unpleasant surprise in the form of a large bill at the end of the month is\ndisappointing and stressful. Amazon CloudWatch can create billing alarms that send\nnotifications if total charges for the month exceed a predefined threshold. This is use-\nful not only to avoid unexpectedly large bills but also to catch potential misconfigura-\ntions of your system. \n For example, it’s easy to misconfigure a Lambda function and inadvertently allo-\ncate 3.0 GB of RAM to it. The function might not do anything useful except wait for\n15 s to receive a response from a database. In a heavy-duty environment, the system\nmight perform 2 M invocations of the function a month, costing a little over $1,462.\nThe same function with 128 MB of RAM would cost around $56 per month. If you per-\nform cost calculations up front and have a sensible billing alarm, you’ll quickly realize\nthat something is going on when billing alerts begin to come through. \nB.2.1\nCreating billing alerts\nFollow these steps to create a billing alert:\n1.\nIn the main AWS console, click your name (or the name of the IAM user that’s\nrepresenting you) and then click My Billing Dashboard. \n2.\nClick Billing Preferences in the navigation pane and then enable the check box\nnext to Receive Billing Alerts. \n3.\nClick Save preferences, then go back to the main AWS console and find the\nCloudWatch service. \nListing B.3\nPolicy to enforce HTTPS/SSL\nExplicitly denies access to \ns3 if the condition is met\nThe condition is met when requests \nare not sent using SSL. This forces \nthe policy to block access to the \nbucket if a user tries to access it \nover regular, unencrypted HTTP.\n",
      "content_length": 2350,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "208\nAPPENDIX B\nSetting up your cloud\n4.\nOpen the CloudWatch service, click Alarms, and select All Alarms in the naviga-\ntion pane. Click the Create alarm button and then click the Select metric button.\n5.\nUnder the Metrics heading, select Billing and click Total Estimated Charges. (If\nyou don’t see Billing it means you may not have enabled the Receive Billing\nAlerts option in step 2). \n6.\nTick the checkbox for EstimatedCharges and click Select metric to continue.\n7.\nMake sure that the Threshold type is set to Static and that Whenever Estimated-\nCharges is set to Greater. \n8.\nIn the Define the threshold value, enter the amount that you’d like to trigger\nthe alarm (for example, 200 as seen in figure B.5). \n9.\nClick Next to continue to the next page.\nHere you can set or create a new SNS topic to notify you when the alarm is\ntriggered. This is important! You need an SNS topic to receive emails to alert\nyou what is happening. \n10.\nClick the Add notification button.\n11.\nChoose Create new topic, enter a name for it, and then type in your email\naddress. Click Create topic button to save your SNS topic settings. When you\nare ready to proceed click Next. \n12.\nType in a name for your Alarm and click Next again.\n13.\nFinally, at the bottom, click the Create alarm button to finish. \nFigure B.5\nIt’s good practice to create multiple billing alarms to keep you informed of ongoing costs.\nThis key is to set the amount \nthat will let you know when \nyou go over your budget. \nYou want to stay in budget \nall of the time and not let \nthe costs blow out.\n",
      "content_length": 1556,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "209\nAPPENDIX B\nSetting up your cloud\nB.2.2\nMonitoring and optimizing costs\nServices such as CloudCheckr (http://cloudcheckr.com) can help to track costs, send\nalerts, and even suggest savings by analyzing services and resources in use. CloudCheckr\ncomprises several different AWS services including S3, CloudSearch, SES, SNS, and\nDynamoDB. It’s richer in features and easier to use than some of the standard AWS fea-\ntures. It’s worth considering for its recommendations and daily notifications.\n AWS also has a service called Trusted Advisor that suggests improvements to per-\nformance, fault tolerance, security, and cost optimization. Unfortunately, the free ver-\nsion of Trusted Advisor is limited, so if you want to explore all of the features and\nrecommendations it has to offer, you must upgrade to a paid monthly plan or access it\nthrough an AWS enterprise account.\n Cost Explorer (figure B.6) is a useful, albeit high-level reporting and analytics tool\nbuilt into AWS. You must activate it first by clicking your name (or the IAM username)\nin the top-right corner of the AWS console, selecting My Billing Dashboard, then\nclicking Cost Explorer from the navigation pane and enabling it. Cost Explorer ana-\nlyzes your costs for the current month and the past four months. It then creates a fore-\ncast for the next three months. Initially, you may not see any information because it\ntakes 24 hours for AWS to process data for the current month. Processing data for pre-\nvious months make take even longer. More information about Cost Explorer is avail-\nable at http://amzn.to/1KvN0g2.\nFigure B.6\nCost Explorer allows you to review historical costs and estimate what future costs may be.\nYou have access to plenty of filters \nand you can create custom reports. \nHowever, becoming an expert at \nCost Explorer can take some time. \n",
      "content_length": 1834,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "210\nAPPENDIX B\nSetting up your cloud\nB.2.3\nUsing the Simple Monthly Calculator\nThe AWS Pricing Calculator (https://calculator.aws) is a web application developed\nby Amazon to help model costs for many of its services. This tool allows you to select a\nservice, enter information related to the consumption of that particular resource, and\nget an indicative cost. \nB.2.4\nCalculating Lambda and API Gateway costs\nThe cost of running serverless architecture often can be a lot less than running tradi-\ntional infrastructure. Naturally, the cost of each service you might use will be different,\nbut you can look at what it takes to run a serverless system with Lambda and the API\nGateway. \n Amazon’s pricing for Lambda (https://aws.amazon.com/lambda/pricing/) is\nbased on the number of requests, duration of execution, and the amount of memory\nallocated to the function. The first million requests are free with each subsequent mil-\nlion charged at $0.20. Duration is based on how long the function takes to execute\nmeasured to the millisecond (ms). Amazon charges in 1 ms increments, while also fac-\ntoring in the amount of memory reserved for the function. A function created with 1\nGB of memory will cost $0.000001667 per 100 ms of execution time, whereas a func-\ntion created with 128 MB of memory will cost $0.000000208 per 100 ms. \nNOTE\nAmazon prices may differ depending on the region and that they’re\nsubject to change at any time.\nAmazon provides a perpetual free tier with 1M free requests and 400,000 GB-seconds\nof compute time per month. This means that a user can perform a million requests\nand spend an equivalent of 400,000 seconds running a function created with 1 GB of\nmemory before they have to pay. As an example, consider a scenario where you have\nto run a 256 MB function, 5 million times a month. The function executes for 2 sec-\nonds each time. The cost calculation follows:\nMonthly request charge: \n– The free tier provides 1 million requests, which means that there are only 4\nmillion billable requests (5M requests – 1M free requests = 4M requests). \n– Each million is priced at $0.20, which makes the request charge $0.80 (4M\nrequests × $0.2/M = $0.80).\nMonthly compute charge:\n– The compute price for a function per GB-second is $0.00001667. The free\ntier provides 400,000 GB-seconds free. \n– In the compute price scenario, the function runs for 10 ms (5M × 2s). \n– 10M seconds at 256 MB of memory equates to 2,500,000 GB-seconds\n(10,000,000 × 256 MB / 1024 = 2,500,000). \n– The total billable amount of GB-seconds for the month is 2,100,000\n(2,500,000 GB-seconds – 400,000 free tier GB-seconds = 2,100,000). The\n",
      "content_length": 2639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "211\nAPPENDIX B\nSetting up your cloud\ncompute charge is therefore $35.007 (2,100,000 GB-seconds × $0.00001667 =\n$35.007). \n– The total cost of running Lambda in this example is $35.807. \nThe API Gateway pricing is based on the number of API calls received and the\namount of data transferred out of AWS. In the eastern United States, Amazon charges\n$3.50 for each million API calls received and $0.09/GB for the first 10 TB transferred\nout. Given the previous example and assuming that monthly outbound data transfer is\n100 GB a month, the API Gateway pricing is as follows:\nMonthly API charge:\n– The free tier includes 1M API calls per month but is valid for only 12\nmonths. Given that it’s not a perpetual free tier, it won’t be included in this\ncalculation. \n– The total API cost is $17.50 (5M requests × $3.50/M = $17.50).\nThe monthly data charge is $9.00 (100 GB × $0.09/GB = $9).\nThe API Gateway cost in this example is $26.50. \nThe total cost of Lambda and the API Gateway is $62.307 per month. \nIt’s worthwhile to attempt to model how many requests and operations you may have\nto handle on an ongoing basis. If you expect 2M invocations of a Lambda function\nthat uses only 128 MB of memory and runs for 1 second, you’ll pay approximately\n$0.20 month. If you expect 2M invocations of a function with 512 MB of RAM that\nruns for 5 seconds, you’ll pay a little more than $75.00. With Lambda, you have an\nopportunity to assess costs, plan ahead, and pay for only what you actually use. Finally,\ndon’t forget to factor in other services such as S3 or SNS, no matter how insignificant\ntheir cost may seem to be.\n",
      "content_length": 1616,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "212\nappendix C\nDeployment frameworks\nAutomation and continuous delivery are important if you’re building anything on\na cloud platform such as AWS. If you take a serverless approach, it becomes even\nmore critical because you end up having more services, more functions, and more\nthings to configure. You need to be able to script your entire application, run tests,\nand deploy it automatically. The only time you should deploy Lambda functions\nmanually or self-configure API Gateway is while you learn. Once you begin working\non real serverless applications, you need to have a repeatable, automated, and\nrobust way of provisioning your system. Apart from Terraform, the other frame-\nworks discussed in this appendix do not provision resources on their own. Instead,\nthey rely on AWS CloudFormation (https://aws.amazon.com/cloudformation/) to\nprovision resources and are therefore bound by CloudFormation’s limitations.\nThese include the following:\nA CloudFormation template can have no more than 500 resources. To go\nbeyond this limit, you can use nested CloudFormation stacks.\nA CloudFormation template can have no more than 200 parameters or\noutputs.\nIt’s cumbersome to add existing resources to a CloudFormation stack.\nAlthough Terraform alleviates these limitations, it has shortcomings of its own. The\nmost notable of which is the lack of support for rollback. If there was a problem\nduring a deployment, then your application can end up in a broken state if some\nresources are updated but others are not.\n Some of the frameworks discussed in this appendix also provide additional utili-\nties, such as the ability to invoke Lambda functions locally or even simulate API\nGateway locally. With that said, let’s go through some of the most popular deploy-\nment frameworks for serverless applications.\n",
      "content_length": 1806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "213\nAPPENDIX C\nDeployment frameworks\nC.1\nServerless Framework\nThe Serverless Framework (https://serverless.com) is an open source framework and\nis easily one of the most popular and mature deployment frameworks out there. At its\nessence, it allows users to define an entire serverless application (including Lambda\nfunctions, API Gateway APIs, SNS topics, and any other CloudFormation resources)\nand then deploy it using a command-line interface (CLI). It helps you organize and\nstructure serverless applications, which is of great benefit as you begin to build larger\nsystems, and it’s fully extensible via its plugin system.\nC.1.1\nGetting started\nThe Serverless Framework supports both JSON and YAML. It also lets you describe\nyour application in a manifest file like that shown in the following listing.\nservice: user-service  \nprovider:                            \n  name: aws\n  runtime: nodejs12.x\n  region: us-east-1\n  stage: dev\nfunctions:                       \n usersCreate:\n   events:                   \n     - http: \n         path: users/create\n         method: post\n usersDelete:\n   events:\n     - http: \n         path: users/delete\n         method: delete\nresource:                          \n  Resources:\n    UserTable:\n      Type: AWS::DynamoDB::Table\n      Properties:\n        BillingMode: PAY_PER_REQUEST\n        KeySchema:\n          ...\nListing C.1\nDescribing a service in serverless.yml\nThe name of the service. This would appear as part of the name for the \ngenerated CloudFormation stack as well as any provisioned API Gateway \nAPIs and Lambda functions.\nTop level configuration for the project: \nthe language runtime for the Lambda \nfunctions, the region, and the name of \nthe deployment stage.\nYour functions\nThe events that trigger \nthese functions\nThe resource your functions use. Raw \nAWS CloudFormation syntax goes here.\n",
      "content_length": 1847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "214\nAPPENDIX C\nDeployment frameworks\nTo deploy the application, you only have to run a single command:\nserverless deploy\nThe Serverless Framework packages your code, uploads it to S3, and provisions the\nresources specified in the serverless.yml through CloudFormation. You can also over-\nride the default region and stage name with CLI options as the following shows\n(http://mng.bz/AOJz):\nserverless deploy -s prod -r eu-west-1\nC.1.2\nLanguage support\nThe Serverless Framework supports a number of language runtimes: Node.js, Python,\nJava, Golang, C#, and Scala to name a few. You have a lot of control over how the\nServerless Framework packages your functions. By default, it uses the same packaged\nartifact for all the functions you have configured in the serverless.yml. But you can\noptionally package each function separately and include or exclude specific folders or\nfiles.\n Through the serverless-webpack plugin (https://bit.ly/sls-webpack), you can\nalso incorporate webpack into the packaging process to tree shake and bundle Java-\nScript functions. Doing so can produce much smaller artifacts, which helps with both\ndeployment time as well as cold-start performance.\n For Python functions, it can be challenging to include third-party libraries into the\ndeployment artifact. The serverless-python-requirements plugin (http://bit.ly/\nsls-python-reqs) handles this for you transparently and lets you use your existing\nrequirements.txt file.\nC.1.3\nInvoking functions locally\nBesides packaging and deploying serverless applications, the Serverless Framework\nalso has a number of useful utilities. The most notable is the ability to invoke func-\ntions locally using the invoke local command:\nserverless invoke local -f functionName -d “{}”\nThe invoke local command is useful for quickly testing a function locally. It gives\nyou fast feedback without having to deploy the function to AWS first. You can also\nattach a debugger and step through the code line by line (for more information, see\nthis post http://bit.ly/sls-debug-vscode for how to do it with VS Code).\n But what if you want to emulate API Gateway locally? The serverless-offline\nplugin (http://bit.ly/sls-offline) lets you do exactly that and emulates API Gateway on\na localhost post. We find this useful when doing server-side rendering with Lambda.\nAlthough we can use invoke local to test a function locally and inspect its output, we\ncan’t render HTML in our heads! Having a local endpoint lets us point a browser to it\nand inspect the server-side rendered HTML in all its CSS glory.\n",
      "content_length": 2553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "215\nAPPENDIX C\nDeployment frameworks\nC.1.4\nPlugins\nThe Serverless Framework has a rich ecosystem of plugins that extend its capability far\nand beyond what the framework is capable of out-of-the-box. Some plugins modify the\nCloudFormation template the Serverless Framework generates. For example, whereas\nthe Serverless Framework generates a shared identity and access management (IAM)\nrole for all the functions in a project, the popular serverless-iam-roles-per-function\nplugin lets you configure IAM roles for each function.\n Some plugins add support for services that the Serverless Framework does not sup-\nport natively. For example, the Serverless Framework does not support AppSync out\nof the box. You can still configure an AppSync API in the serverless.yml using raw\nCloudFormation syntax (in the resources section of the serverless.yml), but this is\ntedious and laborious. The serverless-appsync-plugin plugin extends the Server-\nless Framework to support AppSync and lets you configure AppSync APIs with a much\nmore succinct syntax. Similarly, the serverless-step-functions plugin adds support\nfor Step Functions.\n Some plugins can add additional commands to the Serverless Framework’s CLI.\nFor example, the serverless-offline plugin adds an offline command that starts a\nlocal instance of API Gateway. Similarly, the serverless-export-env plugin adds an\nexport-env command that captures the environment variables referenced by the\nLambda functions and exports them to a .env file. \n The Serverless Framework has a flexible plugin architecture and lets you custom-\nize just about everything the framework does. This flexibility allows you to disagree\nwith framework defaults and tailor its behavior to suit your needs. Its rich ecosystem of\navailable plugins is also what sets it apart from AWS SAM.\nC.2\nServerless Application Model (SAM)\nThe Serverless Application Model (https://aws.amazon.com/serverless/sam) (SAM),\nis AWS’s answer to the Serverless Framework and shares many similarities with the\nServerless Framework. Like the Serverless Framework, SAM uses CloudFormation to\nprovision resources and lets you use a simpler (compared with CloudFormation) syn-\ntax to define serverless applications in terms of Lambda functions, API Gateway, and\nso on. It also has a number of CLI commands that let you invoke Lambda functions\nlocally or start a local instance of API Gateway too. The biggest difference between\nSAM and the Serverless Framework is that SAM’s syntax is much closer to the raw\nCloudFormation syntax, and it doesn’t have a plugin system.\n The former is often held as a reason why one should favor SAM over the Serverless\nFramework, but it’s a question of personal preference. Ultimately, the CloudForma-\ntion syntax is verbose, and that's one reason why we prefer to use these frameworks\nthat offer a simpler syntax and more productive abstraction level to work with. So why\nshould one favor a framework because its syntax is closer to the thing that you try to\nget away from? It doesn’t make sense.\n",
      "content_length": 3027,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "216\nAPPENDIX C\nDeployment frameworks\n The lack of a plugin system, on the other hand, is often a deal breaker. It means\nyou’re limited by what the framework supports and have no easy way to override the\nframework defaults (unless the framework makes it a configurable option, of course).\nFor example, although SAM added support for Step Functions in May 2020 (which is\nmore than three years after the serverless-step-functions plugin did the same for\nthe Serverless Framework), it still has no support for AppSync at the time of writing\n(April 2021).\n And while the Serverless Framework’s plugin system offers an escape hatch for\nwhen you need to disagree with the framework's defaults, the lack of a plugin system\nrestricts you to what the framework allows you to configure with SAM. In order to dis-\nagree with the choices that SAM makes for you, you’d have to work around it with\nCloudFormation macros and use those macros to modify the SAM-generated Cloud-\nFormation template at deployment time. If this sounds like a tedious solution, it’s\nbecause it is as we learned the hard way two years ago (http://mng.bz/ZxJP).\n Having said that, SAM does certain things very well. For example, it lets you define\nIAM roles for individual functions out of the box. And the way it provisions API Gate-\nway resources is also more efficient (compared with the Serverless Framework) in\nterms of the number of CloudFormation resources. Whereas the Serverless Frame-\nwork would provision the API resources and methods as individual resources, SAM\nencodes all of them in the Body attribute of the AWS::ApiGateway::RestApi resource.\nThis approach minimizes the number of resources in the CloudFormation stack and\nhelps mitigate the risk of hitting the 500 resource limit in a CloudFormation stack.\nThis comes in handy in large API projects. With the Serverless Framework, these large\nprojects often have to rely on plugins such as the serverless-plugin-split-stacks\nplugin to work around the 500 resources limit.\nC.3\nTerraform\nTerraform (https://www.terraform.io) is a popular infrastructure-as-code (IaC) tool\nby HashiCorp. It is by far the least opinionated framework in this appendix. True to its\nmotto of “Write, Plan, and Create Infrastructure as Code,” Terraform has long been\nfavored by infrastructure engineers and is not designed with Lambda as its focus.\nInstead, it treats Lambda functions as AWS resources: nothing more, nothing less. As\nsuch, you have the utmost control and can configure Lambda, API Gateway, and any\nother resources however you like. But this exposes you to all the underlying complexi-\nties of those resources; complexities that the other tools try hard to manage for you. \n For example, you need to understand how API Gateway resources are organized,\nwhich we find is one of the most laborious aspects of using Terraform for Lambda. A\nsingle line of human-readable URL in the Serverless Framework or SAM can easily\ntranslate to 50 lines of Terraform code (figure C.1).\n Because Terraform is designed to give you a way to describe and create your infra-\nstructure, it doesn’t offer any value-add services for serverless applications. There’s no\n",
      "content_length": 3163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "217\nAPPENDIX C\nDeployment frameworks\nlookup:\n   handler: functions/lookup.handler\n   description: handles the lookup/country/zipcode endpiont\n   events:\n      - http:\n           path: lookup/{country}/{zipcode}\n           method: get\nServerless framework\nresource \"aws_api_gateway_rest_api\" \"rest_api\" {\n   name = \"${var.stage}-${var.feature_name}\"\n   description = \"REST API for zipcode lookup\"\n}\nresource \"aws_api_gateway_resource\" \"lookup\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   parent_id = \"${aws_api_gateway_rest_api. rest_api.root_resource_id}\"\n   path_part = \"Lookup\"\n}\nresource \"aws_api_gateway_resource\" \"country\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   parent_id = \"${aws_api_gateway_resource.lookup.id}\"\n   path_part = \"{country}\"\n}\nresource \"aws_api_gateway_resource\" \"zipcode\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   parent_id = \"${aws_api_gateway_resource.country.id}\"\n   path_part = \"{zipcode}\"\n}\nresource \"aws_api_gateway_method\" \"get_zipcode\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   resource_id = \"${aws_api_gateway_resource.zipcode.id}\"\n   http_method = \"GET\"\n   authorization = \"NONE\"\n}\nresource \"aws_api_gateway_integration\" \"zipcode_lookup\" {\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   resource_id = \"${aws_api_gateway_resource.zipcode.id}\"\n   type = “AWS_PROXY\"\n   http_method = \"${aws_api_gateway_method.get_zipcode.http_method}\"\n   integration_http_method = “POST\"\n   uri = “${aws_lambda_function.lookup.invoke_arn}\"\n}\nresource \"aws_api_gateway_deployment\" \"zipcode_api\" {\n   depends_on = [\n      \"aws_api_gateway_integration.zipcode_lookup\"\n   ]\n   rest_api_id = \"${aws_api_gateway_rest_api.rest_api.id}\"\n   stage_name = \"${var.stage}\"\n}\nTerraform\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n17\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\nFigure C.1\nConfiguring an API Gateway function with the Serverless Framework vs. Terraform\n",
      "content_length": 2018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "218\nAPPENDIX C\nDeployment frameworks\nbuilt-in support for packaging your deployment artifact, nor is there any built-in sup-\nport for running functions locally.\n Whereas all the other tools in this list are built on top of CloudFormation, Terra-\nform does its own thing and relies on AWS APIs to create resources. This means Terra-\nform is not bound by CloudFormation limitations such as the aforementioned 500\nresources per stack, but it also lacks the capabilities that CloudFormation offers.\n For example, Terraform does not automatically rollback changes when a deploy-\nment fails halfway. Many HashiCorp fans would tell you that this is a feature, not a\nbug, but don’t let them fool you. You don’t want your application to be stuck in a half-\nway, broken state when a deployment fails.\n There are also other problems to consider when using Terraform in serverless\napplications. For instance, because Terraform uses the AWS APIs to create resources,\nit often runs into throttling limits that CloudFormation does not. A common example\nis the ResourceConflictException due to the number of concurrent updates to a\nLambda function. This can happen when you make certain changes to a Lambda\nfunction that requires multiple API calls to achieve. This has been a long-standing\nproblem (see this issue from 2018 at http://mng.bz/RqJK), and the only viable work-\naround is to daily-chain changes with depends_on clauses.\n Terraform keeps track of resource states and can persist them to data stores such as\nS3. However, it does not encrypt these state files, which means any sensitive informa-\ntion such as credentials and API keys are stored in plain text. It’s up to you to ensure\nthat the S3 bucket enables server-side encryption (SSE). To be even more secure, use\ncustomer-managed keys to ensure that only you can decrypt the data.\n Overall, the severe lack of productivity alone makes Terraform a bad choice when\nit comes to building serverless applications. We strongly recommend against it. How-\never, it’s taken a strong hold in the DevOps culture and many infrastructure teams\nmandate the use of Terraform within their organizations. If you’re struggling to con-\nvince your manager to let you use something other than Terraform in your serverless\nproject, then consider doing the following:\nShow them the difference in lines of code that you need to write for something as simple as\na single API endpoint. Translate this into development time and cost. For exam-\nple, “It’ll take a week to do with Terraform versus a couple of hours with Server-\nless Framework or SAM” is a convincing argument.\nExplain to the infrastructure team (they might be incorrectly labeled the DevOps team in\nyour organization) that there is an integration path between Terraform and the Serverless\nFramework or SAM. They can still use Terraform to provision shared infrastruc-\nture resources such as VPCs; they just need to share the ARNs or names of these\nresources as SSM parameters. Both the Serverless Framework and SAM can ref-\nerence these parameters. This way, both the infrastructure and feature teams\ncan use the right tool for the job and everyone’s happy.\n",
      "content_length": 3150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "219\nAPPENDIX C\nDeployment frameworks\nC.4\nCloud Development Kit \nThe AWS Cloud Development Kit (CDK), available at https://aws.amazon.com/cdk,\nis a relatively new kid on the block but has received a lot of interest from the commu-\nnity. CDK differs from the aforementioned frameworks in that it does not use a\nmarkup language. Instead, CDK lets you describe the resources you want to provision\nusing a general-purpose programming language such as TypeScript or Python.\n It’s easy to see the appeal of using a general-purpose programming language in an\nIaC tool. Developers can use their favorite programming language to write their appli-\ncation as well as how it should be deployed. There’s no need to learn another lan-\nguage such as YAML or HCL (the JSON-based configuration language that Terraform\nuses). This doesn’t necessarily mean that CDK is a better IaC tool because it gives\ndevelopers what they want. After all, no matter how much we like eating cakes and\ncandies, it doesn’t change the fact that these sugary delights are bad for our health.\n For anyone who proclaims that YAML or HCL is not code, just remember that not\nlong ago, Java and .Net developers said the same thing about JavaScript and Python.\nThis kind of gatekeeping and putting others down to raise one’s standing happens in\nlots of places and have no place in our community. Configuration files are code. A Cloud-\nFormation template is a set of instructions to tell CloudFormation what resources to\nprovision and that is the dictionary definitions of code. Now that we got the common\nmisconceptions out of the way, let’s talk about where CDK really shines and the chal-\nlenges it faces.\nC.4.1\nWhere CDK shines\nGeneral-purpose programming languages give you much more expressive power com-\npared with configuration files like YAML. This makes CDK a fantastic choice when it\ncomes to templating some complex AWS environments. CloudFormation offers a\nrange of templating options with its intrinsic functions and conditionals, but these are\nlimited and often require complex YAML code to achieve basic branching logic or\nmapping input values against a dictionary. CDK makes these child’s play and can easily\nexpress them in a few lines of code in TypeScript or Python.\n Being able to use general-purpose programming languages like TypeScript and\nPython also means having access to the package managers for those languages. This\nmeans you can take common architectural patterns and create reusable constructs\nand share them as packages. The CDK Patterns (https://cdkpatterns.com) project is a\ngreat example of this. Instead of everyone taking the same recipe and implementing\nthese common patterns from scratch, you can download the relevant package from\nNPM and simply customize it. This is a great way to perpetuate and spread best prac-\ntices within a large organization. It makes it easy for teams to discover and share con-\nstructs that have those best practices and organizational norms baked in.\n",
      "content_length": 2975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "220\nAPPENDIX C\nDeployment frameworks\nC.4.2\nCDK challenges\nSingle-page application (SPA) frameworks such as React and Vue.js have made a suc-\ncessful attempt at unifying HTML, CSS, and your application code into a cohesive and\nproductive JavaScript framework. CDK is doing something similar for infrastructure\ncode.\n However, whereas JavaScript is ubiquitous in the frontend world, the choice and\npreference for programming languages for backend applications are fractured and\ncontextualized around use case. One of the benefits of microservices is to allow teams\nto choose the best language for the job. For example, Node.js might be great for build-\ning REST APIs, but Python is better suited for machine learning (ML) workloads\nbecause most of the libraries are written in Python. The fact that different teams in the\norganization would prefer to use a different language can present a problem for CDK.\n If everyone agrees on using one programming language, then CDK makes it easy\nto share reusable constructs. But if teams want to use different languages, then you\nhave to maintain different versions of these constructs. You can even see this problem\nmanifest in the patterns on https://cdkpatterns.com, where some patterns support\nTypeScript, Python, Java, and C#, but most don’t support all four languages.\n Our other concern about CDK is that, whereas everyone must write the same\nYAML if they want to provision resources with the same configurations, that’s not the\ncase with a general-purpose programming language. Personal preferences and idioms\ncan come into play and suddenly it requires more cognitive energy to understand the\ninfrastructure code. It’s no longer configuration—the infrastructure code now con-\ntains business logic.\n This is especially problematic for those infrastructure teams that need to oversee\nan organization’s AWS environment and provide guidance and oversight for feature\nteams. Suddenly, they must work with infrastructure code that's written in multiple\nlanguages that they might not be familiar with. And because this infrastructure code\ncan contain ample business logic, it makes it doubly hard for infrastructure teams to\ndo their job. This is why we still prefer the declarative approach of YAML and think\nthe fact that it’s difficult to add complex logic into infrastructure code is actually a\nblessing.\nC.5\nAmplify\nAWS Amplify (https://aws.amazon.com/amplify) is a set of tools and services that can\nbe used together to build frontend web and mobile applications quickly. It consists of\nthe following:\nAmplify CLI—A CLI tool that lets you configure AWS resources.\nAmplify libraries—A set of open source libraries that helps you consume AWS\nresources such as Cognito and AppSync.\nAmplify UI components—A collection of drop-in UI components that works with\nAWS resources to provide authentication, storage, and interactions.\n",
      "content_length": 2872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "221\nAPPENDIX C\nDeployment frameworks\nAmplify console—An AWS service that builds and hosts your single-page applica-\ntion (think AWS’s version of Netlify: https://www.netlify.com).\nAmplify Admin UI—A visual UI that lets you provision and configure AWS\nresources as well as manage the data in your application.\nYou can use each of these Amplify tools independently. For example, many teams\nwould use the Amplify libraries and UI components without using the Amplify CLI or\nAdmin UI to manage AWS resources. For this comparison, we’ll consider only the\nAmplify CLI.\n Whereas the other frameworks we have discussed so far take a resource centric\nview of serverless applications, the Amplify CLI takes a utility-centric approach.\nInstead of configuring a Cognito User Pool as a resource, you would run the com-\nmand amplify add auth. The Amplify CLI would then prompt you with a few ques-\ntions about what you want to do. This would bootstrap a CloudFormation template\nand configure a Cognito User Pool and maybe a Cognito Identity Pool too, depend-\ning on how you answer the questions from the CLI. \n Similarly, you can use a single command to bootstrap a brand-new AppSync API:\namplify add api. You can then focus on defining the model of your API, and the\nAmplify CLI can generate a lot of the underlying AWS resources for you, including the\nrelevant AppSync resolvers and even the DynamoDB tables.\n As you can see, the Amplify CLI can make a lot of decisions for you and get things\nwired up quickly. As such, it’s targeted at a slightly different demographic of develop-\ners. The other deployment frameworks in this list are typically used by backend teams\nwho work with AWS daily. Amplify on the other hand, targets frontend–focused teams\nwho are not as well versed with AWS and just want something that works.\n It’s a powerful tool and gives a lot of power to these frontend–focused teams to\nbuild something quickly, without having to spend many hours learning about each of\nthe AWS services they need to use and configure. But there also lies the pitfall, that\nteams are not aware of and do not understand the decisions that Amplify CLI makes for\nthem and are not able to debug problems when they arise. For example, the Amplify\nCLI defaults to using DynamoDB scans for list operations in a GraphQL schema.\nAlthough this works, it’s not an optimal solution and can become problematic as the\nsystem scales because DynamoDB scans are expensive and should be used sparingly.\n Amplify CLI automates a lot of things, and that’s what makes it a productive tool.\nBut it also limits your ability to customize how those AWS resources are configured.\nWhen you reach the limit of what you can achieve with Amplify CLI, there’s currently\nno escape hatch to move away from it. Many teams have had to rewrite their entire\napplication from scratch when they reached this point, which can be a struggle because\nmany teams don’t understand what Amplify CLI has done for them and have a hard\ntime replicating the setup because they know only how to do things with Amplify.\n In our opinion, the ideal users for tools like Amplify CLI are developers who under-\nstand AWS well and have experience working with and configuring those underlying\n",
      "content_length": 3226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "222\nAPPENDIX C\nDeployment frameworks\nresources. You shouldn’t automate things that you don’t understand, which puts you in\na dangerous position of being over reliant on the tool and lets the tail wag the dog.\n The Amplify team is working hard to address the problems that we have brought\nup here and are looking into building escape hatches so teams can transition away\nfrom it when they need to. And we are excited to see where it goes, but for the time\nbeing, we think Amplify CLI should be confined to building proof of concepts or very\nsimple applications. For production applications that need to be maintained and iter-\nated over time, the risk of running into blockers and not being able to easily transition\naway from it is too great. However, it shouldn’t stop you from using other Amplify\ncomponents such as the Amplify libraries in your frontend project or the Amplify con-\nsole to build and host your SPA.\n In this appendix, we looked at five of the most popular ways people are provision-\ning and deploying their serverless applications. We looked at the Serverless Frame-\nwork and SAM, which provide a layer of abstraction over CloudFormation to make it\neasier to build serverless applications. Both support a set of value-add CLI commands,\nsuch as being able to invoke functions locally or run a local instance of API Gateway,\nthat aid you in your development workflow.\n The main difference between the two is that the Serverless Framework has a flexi-\nble plugin system and a rich ecosystem of existing plugins that can extend the frame-\nwork’s capabilities. Whereas with SAM, you’re limited by what it supports, and there is\nalso no easy way to change the framework’s default behavior beyond the available con-\nfigurations.\n We also looked at Terraform, which is a popular IaC tool and used by many infra-\nstructure teams. We explained the problems with using Terraform in serverless appli-\ncations and why we strongly recommend against it. It’s an unproductive tool when it\ncomes to building serverless applications.\n Both CDK and Amplify CLI are relative newcomers in this space, and both have\ngained a lot of momentum and are attracting many admirers. Whereas the Serverless\nFramework, SAM, and Terraform all use a markup language to describe the resources\nfor your serverless application, CDK and Amplify CLI take different approaches.\n CDK lets you use general-purpose programming languages to describe the\nresources you want to provision. It’s a double-edged sword, which offers the full flexi-\nbility that a general-purpose programming language offers as well as the package\nmanagement system that comes with that language. It lets developers use their favor-\nite programming language for both their application code as well as their infrastruc-\nture code and can easily share reusable patterns as packages. However, it can also be\nproblematic in organizations where teams use different programming languages in\ntheir application code. This limits the ability to share CDK constructs because the cre-\nators of these constructs have to support multiple languages. Letting developers add\nbusiness logic to their infrastructure code opens the door for extensive customization\nfor complex AWS environments, but it also makes infrastructure code harder to com-\nprehend and govern by infrastructure teams.\n",
      "content_length": 3324,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "223\nAPPENDIX C\nDeployment frameworks\n Finally, with Amplify CLI, you’re not configuring AWS resources so much as saying\nwhat capabilities you need in your application. Amplify CLI makes the magic happen\nand configures the necessary AWS resources with sensible defaults based on your\ninput. It’s a super-productive tool and can help you build a fully working application\nin no time. But it’s also a black box and has no escape hatch that lets you transition\naway from it when you reach the limit of what it can do. This puts you in a precarious\nposition, where you face the real possibility of having to rebuild the application from\nthe ground up if you ever hit a snag with Amplify CLI.\n Each of these tools has its strengths, but none is perfect. A good principle that will\nstand you in good stead, regardless of what tool you decide to use, is to understand how\nthe AWS services you need to work with operate and how the deployment mechanism\nworks before you try to automate it. Blindly automating what you don’t understand is\ndangerous. But once you understand the underlying machineries, then you should\nlook for tools that allow you to move up the abstraction levels and be more productive.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 1252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "225\nindex\nNumerics\n24-Hour Video project\nAmazon Web Services\ncosts 19–20\nElemental MediaConvert service 28\ntesting in 36\nusing 20–21\nevent-driven pipeline 19\nlogs 37–38\nServerless Framework 29–36\nbringing to project 31–33\ncreating Lambda functions 33–36\nsetting up 29–30\nsystem preparation 21–29\nAWS Elemental MediaConvert service 28–29\ncreating buckets 25\nIdentity and Access Management 22–27\nsetting up 22\n2FA (2 Factor Authentication) 22\nA\nA Cloud Guru 41, 70–83\noriginal architecture 71–82\nGraphQL 77–80\nmicroservices 75–77\nsecurity in BFF environment 82\nservice discovery 80–81\nremnants of legacy 82–83\nALB (Application Load Balancer) 68\nAlexa skills 44\nAlgolia 197\nAmazon API Gateway service 195, 210–211\nAmazon Athena 160–163\nAmazon Cognito 199\nAmazon DynamoDB. See DynamoDB\nAmazon Echo 44\nAmazon Kinesis Data Analytics 89–90\nAmazon Kinesis Data Firehose 88–89, 158–160\nAmazon Kinesis Data Streams 86–87, 198\nAmazon QuickSight 163–164\nAmazon RDS (Relational Database Service) 197\nAmazon S3 (Simple Storage Service) 196\nAmazon SES (Simple Email Service) 196\nAmazon SNS (Simple Notification Service)\n195–196\nAmplify 220–223\nAnalytics Service 157–164\nAWS Glue and Amazon Athena 160–163\nKinesis Firehose 158–160\nQuickSight 163–164\nAngularJS 73\nAPI Gateway 9, 29, 43–44, 75, 80\ncalculating costs 210–211\noverview 195\nApplication Load Balancer (ALB) 68\nAppSync 198\nARN (Amazon Resource Name) 201\nAthena service 198\nattributes 197\nAuth0 73, 199\nAWS (Amazon Web Services) 200–211\nAppSync 198\nAthena service 198\ncosts 19–20, 207–211\ncalculating Lambda and API Gateway \ncosts 210–211\ncreating billing alerts 207–208\n",
      "content_length": 1613,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "INDEX\n226\nAWS (Amazon Web Services) (continued)\nmonitoring and optimizing 209\nPricing Calculator 210\nElemental MediaConvert service\ncreating roles 29\nendpoint 32\noutputs 35–36\noverview 28\nspecifying ARN for role 33\nGlue ETL 160–163\nIdentity and Access Management 200–207\ncreating and managing users 22–25, \n201–203\ncreating roles 26–27\ngroups 203\nLambda role ARN 32\npermissions and policies 205–207\nresources 204–205\nroles 203–204\nMedia Services 197–198\nmultiple accounts 184–186\nbetter autonomy for teams 185–186\nbetter cost monitoring 185\neliminating contention for shared \nservice limits 185\ninfrastructure-as-code for AWS \nOrganizations 186\nisolating security breaches 184\ntesting in 36\nusing 20–21\nX-Ray 175\nAWS AppSync 79\nAWS Elemental MediaConvert 19, 25–26, 134\nAWS Glue 150, 157, 198\nAWS Pricing Calculator 210\nAWS Step Functions 9\nAWS WAF 76\nAZs (availability zones) 85, 197\nB\nbackends 41\nBFF (Backends for Frontends) environment, \nsecurity 82\nBI (Business Intelligence) service 163\nblueprints, Lambda 44\nbots 44\nbuckets\ncreating 25\ndefined 196\ntranscoded video bucket 32\nuploading 32\nC\ncanary deployment 68\ncanary pattern 68\nCDK (Cloud Development Kit) 219–220\nadvantages of 219\nchallenges of 220\nCDU (Code Developer University) project 164\nAnalytics Service 157–164\nAWS Glue and Amazon Athena 160–163\nKinesis Firehose 158–160\nQuickSight 163–164\nCode Scoring Service 150–153\nSubmissions Queue 152–153\nsummary of 153\nrequirements 147–148\ngeneral requirements 148\nleaderboards 148\nreports 148\nusers and experience points 148\nsolution architecture 148–150\nStudent Profile Service 153–157\nUpdate Student Scores function 155–157\nCloudAMQP 59\nCloudCheckr 209\nCloudFormation 32, 36, 74, 76, 212, 215–216\nCloudFront 73, 75\nCloudSearch 63\nCloudWatch 37–38, 93, 174–175\nCMKs (Customer Managed Keys) 189\nCode Scoring Service 150–153\nSubmissions Queue 152–153\nsummary of 153\nCognito 199\ncold latency 173\ncold start penalty 173\ncommand pattern\noverview 46–47\nuses for 47\ncompute layer 168\ncompute-as-glue architecture 51\nconcurrency 180–181\ncorrelation between requests, latency, and \nconcurrency 181\nmanaging 181\nconcurrent executions 85\ncontainers 8\ncontextual bandits model 89\ncron jobs 102–109\ncost 107\nprecision 105\nscalability (hotspots) 105–107\nscalability (number of open tasks) 105\ncustom code, minimizing 14\nCustomer Managed Keys (CMKs) 189\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "INDEX\n227\nD\ndata layer 168\ndata processing and manipulation 42\nDAX in-memory cache 197\ndelegation 204\ndeployment frameworks 212–223\nAmplify 220–223\nCDK 219–220\nadvantages of 219\nchallenges of 220\nSAM 215–216\nServerless Framework 213–215\ninvoking functions locally 214\nlanguage support 214\noverview 213–214\nplugins 215\nTerraform 216–218\nDLQ (Dead Letter Queue) feature 86, 144\nDRM (Digital Right Management) 198\nDSL (domain-specific language) 7\nduration 175\nDynamoDB 9, 74, 76, 79, 81, 83, 109–113, \n137–139, 143, 150, 157, 197\ncombining with SQS 122–125\nprecision 124\nscalability (hotspots) 124\nscalability (number of open tasks) 124\ncost 113\nprecision 111\nscalability (hotspots) 111–112\nscalability (number of open tasks) 111\nE\ne2e (end-to-end) tests, temporary stacks for 188\nEFS (Elastic File System) 137\nElasticSearch 63\nElemental MediaConvert service\ncreating roles 29\nendpoint 32\noutputs 35–36\noverview 28\nspecifying ARN for role 33\nemerging practices 193\navoiding sensitive data in plain text in \nenvironment variables 188–190\nhandling sensitive data securely 189–190\nvulnerabilities 189\nEventBridge 190–193\narchiving and replaying events 191–192\ncontent-based filtering 190–191\nmore targets 192\nschema discovery 191\ntopology 192–193\nmultiple AWS accounts 184–186\nbetter autonomy for teams 185–186\nbetter cost monitoring 185\neliminating contention for shared service \nlimits 185\ninfrastructure-as-code for AWS \nOrganizations 186\nisolating security breaches 184\ntemporary stacks 186–188\ncommon AWS account structure 186–187\nfor e2e tests 188\nfor feature branches 187–188\nendpoints 168\nenvironment variables, avoiding sensitive data \nin plain text in 188–190\nhandling sensitive data securely 189–190\nvulnerabilities 189\nevent-driven pipeline 19\nEventBridge 149–150, 153, 157, 190–193\narchiving and replaying events 191–192\ncontent-based filtering 190–191\ncron jobs with 102–109\ncost 107\nprecision 105\nscalability (hotspots) 105–107\nscalability (number of open tasks) 105\nmore targets 192\nschema discovery 191\ntopology 192–193\nExtract Audio Lambda function 142–143\nF\nFaaS (Functions as a Service) 9\nfan-out pattern\noverview 50–51\nuses for 51\nFargate 85–86\ncost considerations 85\nperformance considerations 85–86\nfederation 204\nFFmpeg library 136\nFirebase 73–74, 83\nfolders, in S3 196\nFunctions as a Service (FaaS) 9\nG\nGlue ETL 160–163\nGraphQL\nmoving to 79–80\noverview 45–46, 77–79\nuses for 46\n",
      "content_length": 2397,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "INDEX\n228\nH\nhard limits 91–92, 115\nHello World! 30\nhybrid approach 44–45\nI\nIaaS (Infrastructure as a Service) 8\nIAM (Identity and Access Management)\n91, 200–207, 215\ncreating and managing users 22–25, 201–203\ncreating roles 26–27\ngroups 203\nLambda role ARN 32\npermissions and policies 205–207\nresources 204–205\nroles 203–204\niConsent 127–128\nidentity-based permissions, in AWS 204\nindices, in Algolia 197\nInfrastructure as a Service (IaaS) 8\ninline policies 205\ninvocations 174\nIoT (Internet of Things) 41\nitems, in DynamoDB 197\nJ\nJest JavaScript framework 188\nK\nkeys, in S3 (Simple Storage Service) 196\nKinesis Data Analytics 89–90\nKinesis Data Firehose 88–89, 158–160\nKinesis Data Streams 42–43, 86–87, 90, 198\nKinesis Firehose 42–43, 91, 149, 157\nKMS (Key Management Service), AWS 189\nL\nLambda service 182\ncalculating costs 210–211\nconcurrency 180–181\ncorrelation between requests, latency, and \nconcurrency 181\nmanaging 181\nLambda functions\ncreating 33–36\ndeployment 36\nExtract Audio Lambda function 142–143\nMediaConvert outputs 35–36\nMerge Video Lambda function 143\nrequest handling 169–173\nRouter 88\nSplit and Convert Video Lambda function 143\nTranscode Video Lambda function 140–141\nLambda role ARN 32\nlatency 176–180\nallocating sufficient resource to your \nexecution environment 178–179\ncold vs. warm 173\nminimizing deployment artifact size 176–178\noptimizing function logic 179–180\nload generation 173–174\noptimization 167–168\nperformance and availability tracking 174–176\nAWS X-Ray 175\nCloudWatch metrics 174–175\nthird-party tools 176\nlatency 176–180\nallocating sufficient resource to your execution \nenvironment 178–179\ncold vs. warm 173\nminimizing deployment artifact size 176–178\noptimizing function logic 179–180\nlegacy API proxy 43–44\nload generation 173–174\nload testing 92\nlogs 37–38\nM\nmachine learning (ML) workloads 220\nmanaged policies 205\nMapReduce model 133–137\narchitecture overview 135–137\ntranscoding video 134–135\nMedia Services 197–198\nMediaConvert 197\nMediaLive 197\nMediaPackage 198\nMediaStore 198\nMediaTailor 198\nMerge Video and Audio function 141\nMerge Video function 141\nMerge Video Lambda function 143\nmessaging pattern\noverview 47–48\nuses for 49\nmethods 195\nmicroservices\nCloud Guru 75–77\nmigrating to new 64–68\nML (machine learning) workloads 220\nN\nn^2 segments 138, 140\nNetlify 73\nnpm (Node Package Manager) 20–21\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "INDEX\n229\nO\nobjects, in S3 (Simple Storage Service) 196\noperations 197\norchestrators 155\nP\nPaaS (Platform as a Service) 8\nparallel computing 145\nalternative architecture for 144\nmaintaining state 138–141\nTranscode Video Lambda function 140–141\nMapReduce model 133–137\narchitecture overview 135–137\ntranscoding video 134–135\nStep Functions 141–144\nExtract Audio Lambda function 142–143\nMerge Video Lambda function 143\nSplit and Convert Video Lambda function 143\npatterns 45–53\ncommand pattern\noverview 46–47\nuses for 47\ncompute-as-glue architecture 51\nfan-out pattern\noverview 50–51\nuses for 51\nGraphQL\noverview 45–46\nuses for 46\nmessaging pattern\noverview 47–48\nuses for 49\npipes and filters pattern\noverview 52–53\nuses for 53\npriority queue pattern\noverview 49–50\nuses for 50\nperformance and availability tracking 174–176\nAWS X-Ray 175\nCloudWatch metrics 174–175\nthird-party tools 176\npipes and filters pattern\noverview 52–53\nuses for 53\nPlatform as a Service (PaaS) 8\nPricing Calculator 210\npriority queue pattern\noverview 49–50\nuses for 50\npublic cloud-based architectures 15\nPython 148, 214, 219\nQ\nQuickSight 163–164\nR\nRabbitMQ 59\nRDS (Relational Database Service) 197\nreal-time analytics 42–43\nrecords, Algolia 197\nRedshift, Amazon 76, 160\nRemindMe 127\nresource-based permissions 204\nresources, in Amazon API Gateway 195\nretry storm 94\nReward Router Lambda function 89\nRouter Lambda function 88\nS\nS3 (Simple Storage Service) 20, 41, 136, 196\nSAM (Serverless Application Model) 215–216\nscheduled services 44\nscheduling services for ad hoc tasks 131\napplications 125–130\niConsent 127–128\nRemindMe 127\nTournamentsRUs 127\ncron jobs with EventBridge 102–109\ncost 107\nprecision 105\nscalability (hotspots) 105–107\nscalability (number of open tasks) 105\ndefining nonfunctional requirements 101–102\nDynamoDB TTL 109–113\ncombining with SQS 122–125\ncost 113\nprecision 111\nscalability (hotspots) 111–112\nscalability (number of open tasks) 111\nSQS 119–122\ncombining with DynamoDB TTL 122–125\nprecision 121\nscalability (hotspots) 121–122\nscalability (number of open tasks) 121\nStep Functions 113–119\ncost 116\nextend scheduled time beyond 1 year 117\nprecision 115\nscalability (hotspots) 116\nscalability (number of open tasks) 116\nscaling for hotspots 117–118\nSecrets Manager 189\nserial invocation 154\nserver-side request forgery (SSRF) 184\n",
      "content_length": 2330,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "INDEX\n230\nServerless Application Model (SAM) 215–216\nserverless architectures 17, 40, 53\nAWS\ncosts 19–20\nElemental MediaConvert service 28\ntesting in 36\nusing 20–21\nconventional implementation 7–9\ndeciding to use 11–14\nevent-driven pipeline 19\nlogs 37–38\noverview 5–6\npatterns 45–53\ncommand pattern 46–47\ncompute-as-glue architecture 51\nfan-out pattern 50–51\nGraphQL 45–46\nmessaging pattern 47–49\npipes and filters pattern 52–53\npriority queue pattern 49–50\npros and cons of 14–16\nServerless Framework 29–36\nbringing to project 31–33\ncreating Lambda functions 33–36\nsetting up 29–30\nserverless implementation 9–10\nserverless, defined 4–5\nservice-oriented architecture and \nmicroservices 7\nsystem preparation 21–29\nAWS Elemental MediaConvert service 28–29\ncreating buckets 25\nIdentity and Access Management 22–27\nsetting up 22\nuse cases 40–45\nAlexa skills 44\nbackends 41\nbots 44\ndata processing and manipulation 42\nhybrids 44–45\nInternet of Things 41\nlegacy API proxy 43–44\nreal-time analytics 42–43\nscheduled services 44\nServerless Framework 29–36, 213–215\nbringing to project 31–33\nLambda role ARN 32\nMediaConvert endpoint 32\nMediaConvert role 33\ntranscoded video bucket 32\nuploading bucket 32\ncreating Lambda functions 33–36\ndeployment 36\nMediaConvert outputs 35–36\ninvoking functions locally 214\nlanguage support 214\noverview 213–214\nplugins 215\nsetting up 29–30\ncredentials 29\nHello World! 30\nservice accounts 201\nservice-oriented architecture (SOA), \nmicroservices and 7\nservices 195–199\nAlgolia 197\nAPI Gateway 195\nAppSync 198\nAthena 198\nAuth0 199\nCognito 199\nDynamoDB 197\nKinesis Streams 198\nMedia Services 197–198\nRelational Database Service 197\nSimple Email Service 196\nSimple Notification Service 195–196\nSimple Queue Service 196\nSimple Storage Service 196\nSES (Simple Email Service) 196\nshards, in Kinesis Streams 198\nSimple Storage Service (S3) 20, 41, 136, 196\nskills, in Alexa 44\nSlack 44\nSNS (Simple Notification Service) 195–196\nSOA (service-oriented architecture), \nmicroservices and 7\nSOAP (Simple Object Access Protocol) 44\nsoft limits 91–92, 115\nSPA (single-page application) frameworks 220\nSplit and Convert Video Lambda function 143\nSputnik 80–81\nSQS (Simple Queue Service) 119–122, 196\ncombining with DynamoDB TTL 122–125\nprecision 124\nscalability (hotspots) 124\nscalability (number of open tasks) 124\nprecision 121\nscalability (hotspots) 121–122\nscalability (number of open tasks) 121\nSQS DLQ (dead-letter queue) 87\nSSM Parameter Store 189\nSSRF (server-side request forgery) 184\nstate\nmaintaining in parallel computing 138–141\nTranscode Video Lambda function 140–141\nstateful microservices 75\nstateless microservices 75\nStep Functions 113–119, 141–144\ncost 116\n",
      "content_length": 2685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "INDEX\n231\nStep Functions (continued)\nextending scheduled time 117\nExtract Audio Lambda function 142–143\nMerge Video Lambda function 143\nprecision 115\nscalability (hotspots) 116\nscalability (number of open tasks) 116\nscaling for hotspots 117–118\nSplit and Convert Video Lambda function 143\nstrangler pattern 62\nStudent Profile Service 153–157\nSubmissions Queue 152–153\nsystem preparation 21–29\nAWS Elemental MediaConvert service\ncreating roles 29\noverview 28\ncreating buckets 25\nIdentity and Access Management\ncreating roles 26–27\ncreating users 22–25\nsetting up 22\nT\ntables, in DynamoDB 197\ntemporary stacks 186–188\ncommon AWS account structure 186–187\nfor e2e tests 188\nfor feature branches 187–188\nTerraform 216–218\nthrottles 175\nthundering herd 94\nTournamentsRUs 127\nTranscode Video Lambda function 140–141\ntranscoding video 134–135\nTTL (time-to-live) feature 109, 137\nTypeScript 219\nU\nUpdate Student Scores function 155–157\nuse cases 40–45\nAlexa skills 44\nbackends 41\nbots 44\ndata processing and manipulation 42\nhybrid approach 44–45\nInternet of Things 41\nlegacy API proxy 43–44\nreal-time analytics 42–43\nscheduled services 44\nuser pools 199\nV\nvisibility timeout, in SQS 119\nVogel, Werner 93\nVPCs (virtual private cloud) 76\nW\nwarm latency 173\nX\nX-Ray 175\nY\nYle 84–95\nbatching is good for cost and efficiency 94–95\nbuilding with failure in mind 93–94\neverything fails, all the time 93–94\npaying attention to retry configurations 94\ncost estimation is tricky 95\ningesting events at scale with Fargate 85–86\ncost considerations 85\nperformance considerations 85–86\nknowing service limits 91–93\nalways load testing 92\nCloudWatch metric granularity 93\nprojecting throughput at every point along \npipeline 92\nsoft vs. hard limits 91–92\nsome limits have bigger blast radiuses than \nothers 92–93\nprocessing events in real-time 86–90\nKinesis Data Analytics 89–90\nKinesis Data Firehose 88–89\nKinesis Data Streams 86–87\nRouter Lambda function 88\nSQS dead-letter queue 87\nYubl 57–69\nmigrating to new microservices 64–68\nnew serverless architecture 61–63\nrearchitecting and rewriting 62\nsearch API 62–63\noriginal architecture 58–60\nlong feature delivery cycles 59–60\nperformance problems 59\nreasons for serverless 60\nscalability problems 59\n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "Sbarski ● Cui ● Nair\nISBN: 978-1-61729-542-3\nM\naintaining server hardware and software can cost a lot of \ntime and money. Unlike traditional data center infra-\nstructure, serverless architectures offl  oad core tasks like \ndata storage and hardware management to pre-built cloud \nservices. Better yet, you can combine your own custom AWS \nLambda functions with other serverless services to create \nfeatures that automatically start and scale on demand, reduce \nhosting cost, and simplify maintenance.\nIn Serverless Architectures with AWS, Second Edition you’ll learn \nhow to design serverless systems using Lambda and other ser-\nvices on the AWS platform. You’ll explore event-driven com-\nputing and discover how others have used serverless designs \nsuccessfully. Th is new edition off ers real-world use cases and \npractical insights from several large-scale serverless systems. \nChapters on innovative serverless design patterns and architec-\ntures will help you become a complete cloud professional. \nWhat’s Inside\n● First steps with serverless computing\n● Th e principles of serverless design\n● Important patterns and architectures\n● Real-world architectures and their tradeoff s\nFor server-side and full-stack software developers.\nPeter Sbarski is VP of Education and Research at A Cloud \nGuru. Yan Cui is an independent AWS consultant and \neducator. Ajay Nair is one of the founding members of the \nAWS Lambda team.\nRegister this print book to get free access to all ebook formats. \nVisit https://www.manning.com/freebook\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nServerless Architectures on AWS \nSecond Edition\nAWS/CLOUD COMPUTING\nM A N N I N G\n“\nA comprehensive and \npractical review of the AWS \ns erverless landscape.”\n \n—Eugene Serdiouk\nPrimex Family of Companies\n“\nFilled with indispensable \nadvice you can use to take \nyour AWS serverless architec-\ntures to the next level.”\n \n—Sal DiStefano\nTravelers Insurance\n“\nAn excellent book \nproviding an overview and \nexplanations of common \nserverless architectures. \nA must-read for every \ncloud developer.”\n \n—Mikołaj Graf\nCloudsail Digital Solutions\n“\nA clear path to exploring \nthe many services \n  off ered by AWS.”\n—Giampiero Granatell\nManyDesigns\nSee first page\n",
      "content_length": 2221,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}