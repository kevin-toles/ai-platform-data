{
  "metadata": {
    "title": "Python Cookbook 3rd",
    "author": "David Beazley and Brian K. Jones",
    "publisher": "O'Reilly Media",
    "edition": "3rd Edition",
    "isbn": "978-1-449-34037-7",
    "total_pages": 706,
    "conversion_date": "2025-11-07T19:10:19.184322",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Python Cookbook.pdf"
  },
  "chapters": [
    {
      "chapter_number": 1,
      "title": "Data Structures and Algorithms",
      "start_page": 1,
      "end_page": 60
    },
    {
      "chapter_number": 2,
      "title": "Strings and Text",
      "start_page": 61,
      "end_page": 120
    },
    {
      "chapter_number": 3,
      "title": "Numbers, Dates, and Times",
      "start_page": 121,
      "end_page": 160
    },
    {
      "chapter_number": 4,
      "title": "Iterators and Generators",
      "start_page": 161,
      "end_page": 200
    },
    {
      "chapter_number": 5,
      "title": "Files and I/O",
      "start_page": 201,
      "end_page": 240
    },
    {
      "chapter_number": 6,
      "title": "Data Encoding and Processing",
      "start_page": 241,
      "end_page": 280
    },
    {
      "chapter_number": 7,
      "title": "Functions",
      "start_page": 281,
      "end_page": 320
    },
    {
      "chapter_number": 8,
      "title": "Classes and Objects",
      "start_page": 321,
      "end_page": 380
    },
    {
      "chapter_number": 9,
      "title": "Metaprogramming",
      "start_page": 381,
      "end_page": 440
    },
    {
      "chapter_number": 10,
      "title": "Modules and Packages",
      "start_page": 441,
      "end_page": 480
    },
    {
      "chapter_number": 11,
      "title": "Network and Web Programming",
      "start_page": 481,
      "end_page": 520
    },
    {
      "chapter_number": 12,
      "title": "Concurrency",
      "start_page": 521,
      "end_page": 560
    },
    {
      "chapter_number": 13,
      "title": "Utility Scripting and System Administration",
      "start_page": 561,
      "end_page": 600
    },
    {
      "chapter_number": 14,
      "title": "Testing, Debugging, and Exceptions",
      "start_page": 601,
      "end_page": 640
    },
    {
      "chapter_number": 15,
      "title": "C Extensions",
      "start_page": 641,
      "end_page": 680
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "chapter": 1,
      "content": "Recipes for Mastering Python 3\n\nO’REILLY° David Beazley & Brian K. Jones",
      "content_length": 72,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": 1,
      "content": "Programming Languages/Python\n\nPython Cookbook\n\nIf you need help writing programs in Python 3, or want to update David Beazley, an independent\nolder Python 2 code, this book is just the ticket. Packed with software developer, teaches\npractical recipes written and tested with Python 3.3, this unique programming courses for\ncookbook is for experienced Python programmers who want to developers, scientists, and\n\nthe author of\nntial Reference\n\nfocus on modern tools and idioms. engineers. Fie’\n\nthe Python E.\n\nInside, you'll find complete recipes for more than a dozen topics, (Addison-Wesley), and has\ncovering the core Python language as well as tasks common to a created several open-source\nwide variety of application domains. Each recipe contains code Python packages.\n\nsamples you can use in your projects right away, along with a Brian K. Jones is a system\ndiscussion about how and why the solution works. administrator in the\n\nDepartment of Computer\nropes nelice! Science at Princeton\nUniversity.\n\nData Structures and Algorithms.\nStrings and Text\n\nNumbers, Dates, and Times\nIterators and Generators\n\nFiles and /O\n\nData Encoding and Processing\nFunctions\n\nClasses and Objects\nMetaprogramming\n\nModules and Packages\n\nNetwork and Web Programming\nConcurrency\n\nUtility Scripting and System Administration\nTesting, Debugging, and Exceptions\nC Extensions\n\nTwitter: @oreillymedia\nfacebook.com/oreilly\n\nO’REILLY*\n\noreilly.com\n\nUS $49.99 CAN $52.99\nISBN: 978-1-449-34037-7\n\nHUN) ivi\nwii\n\n781449\"340377",
      "content_length": 1495,
      "extraction_method": "OCR"
    },
    {
      "page_number": 3,
      "chapter": 1,
      "content": "David Beazley and Brian K. Jones\nTHIRD EDITION\nPython Cookbook",
      "content_length": 62,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": 1,
      "content": "Python Cookbook, Third Edition\nby David Beazley and Brian K. Jones\nCopyright © 2013 David Beazley and Brian Jones. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://my.safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditors: Meghan Blanchette and Rachel Roumeliotis\nProduction Editor: Kristen Borg\nCopyeditor: Jasmine Kwityn\nProofreader: BIM Proofreading Services\nIndexer: WordCo Indexing Services\nCover Designer: Karen Montgomery\nInterior Designer: David Futato\nIllustrator: Robert Romano\nMay 2013:\nThird Edition\nRevision History for the Third Edition:\n2013-05-08: First release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449340377 for release details.\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of O’Reilly\nMedia, Inc. Python Cookbook, the image of a springhaas, and related trade dress are trademarks of O’Reilly\nMedia, Inc.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trade‐\nmark claim, the designations have been printed in caps or initial caps.\nWhile every precaution has been taken in the preparation of this book, the publisher and authors assume\nno responsibility for errors or omissions, or for damages resulting from the use of the information contained\nherein.\nISBN: 978-1-449-34037-7\n[LSI]",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": 1,
      "content": "Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\n1. Data Structures and Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n1.1. Unpacking a Sequence into Separate Variables                                                      1\n1.2. Unpacking Elements from Iterables of Arbitrary Length                                     3\n1.3. Keeping the Last N Items                                                                                           5\n1.4. Finding the Largest or Smallest N Items                                                                 7\n1.5. Implementing a Priority Queue                                                                                8\n1.6. Mapping Keys to Multiple Values in a Dictionary                                               11\n1.7. Keeping Dictionaries in Order                                                                               12\n1.8. Calculating with Dictionaries                                                                                 13\n1.9. Finding Commonalities in Two Dictionaries                                                       15\n1.10. Removing Duplicates from a Sequence while Maintaining Order                  17\n1.11. Naming a Slice                                                                                                         18\n1.12. Determining the Most Frequently Occurring Items in a Sequence                20\n1.13. Sorting a List of Dictionaries by a Common Key                                              21\n1.14. Sorting Objects Without Native Comparison Support                                     23\n1.15. Grouping Records Together Based on a Field                                                    24\n1.16. Filtering Sequence Elements                                                                                 26\n1.17. Extracting a Subset of a Dictionary                                                                     28\n1.18. Mapping Names to Sequence Elements                                                              29\n1.19. Transforming and Reducing Data at the Same Time                                        32\n1.20. Combining Multiple Mappings into a Single Mapping                                    33\n2. Strings and Text. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\n2.1. Splitting Strings on Any of Multiple Delimiters                                                  37\n2.2. Matching Text at the Start or End of a String                                                       38\n2.3. Matching Strings Using Shell Wildcard Patterns                                                 40\n2.4. Matching and Searching for Text Patterns                                                            42\niii",
      "content_length": 2986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": 1,
      "content": "2.5. Searching and Replacing Text                                                                                 45\n2.6. Searching and Replacing Case-Insensitive Text                                                   46\n2.7. Specifying a Regular Expression for the Shortest Match                                    47\n2.8. Writing a Regular Expression for Multiline Patterns                                          48\n2.9. Normalizing Unicode Text to a Standard Representation                                  50\n2.10. Working with Unicode Characters in Regular Expressions                             52\n2.11. Stripping Unwanted Characters from Strings                                                    53\n2.12. Sanitizing and Cleaning Up Text                                                                          54\n2.13. Aligning Text Strings                                                                                              57\n2.14. Combining and Concatenating Strings                                                               58\n2.15. Interpolating Variables in Strings                                                                         61\n2.16. Reformatting Text to a Fixed Number of Columns                                           64\n2.17. Handling HTML and XML Entities in Text                                                       65\n2.18. Tokenizing Text                                                                                                       66\n2.19. Writing a Simple Recursive Descent Parser                                                        69\n2.20. Performing Text Operations on Byte Strings                                                     78\n3. Numbers, Dates, and Times. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  83\n3.1. Rounding Numerical Values                                                                                   83\n3.2. Performing Accurate Decimal Calculations                                                         84\n3.3. Formatting Numbers for Output                                                                            87\n3.4. Working with Binary, Octal, and Hexadecimal Integers                                    89\n3.5. Packing and Unpacking Large Integers from Bytes                                             90\n3.6. Performing Complex-Valued Math                                                                       92\n3.7. Working with Infinity and NaNs                                                                            94\n3.8. Calculating with Fractions                                                                                      96\n3.9. Calculating with Large Numerical Arrays                                                             97\n3.10. Performing Matrix and Linear Algebra Calculations                                      100\n3.11. Picking Things at Random                                                                                  102\n3.12. Converting Days to Seconds, and Other Basic Time Conversions               104\n3.13. Determining Last Friday’s Date                                                                          106\n3.14. Finding the Date Range for the Current Month                                              107\n3.15. Converting Strings into Datetimes                                                                    109\n3.16. Manipulating Dates Involving Time Zones                                                      110\n4. Iterators and Generators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\n4.1. Manually Consuming an Iterator                                                                         113\n4.2. Delegating Iteration                                                                                                114\n4.3. Creating New Iteration Patterns with Generators                                             115\n4.4. Implementing the Iterator Protocol                                                                     117\n4.5. Iterating in Reverse                                                                                                 119\n4.6. Defining Generator Functions with Extra State                                                 120\niv \n| \nTable of Contents",
      "content_length": 4384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": 1,
      "content": "4.7. Taking a Slice of an Iterator                                                                                   122\n4.8. Skipping the First Part of an Iterable                                                                   123\n4.9. Iterating Over All Possible Combinations or Permutations                             125\n4.10. Iterating Over the Index-Value Pairs of a Sequence                                        127\n4.11. Iterating Over Multiple Sequences Simultaneously                                        129\n4.12. Iterating on Items in Separate Containers                                                        131\n4.13. Creating Data Processing Pipelines                                                                   132\n4.14. Flattening a Nested Sequence                                                                             135\n4.15. Iterating in Sorted Order Over Merged Sorted Iterables                                136\n4.16. Replacing Infinite while Loops with an Iterator                                              138\n5. Files and I/O. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141\n5.1. Reading and Writing Text Data                                                                            141\n5.2. Printing to a File                                                                                                     144\n5.3. Printing with a Different Separator or Line Ending                                          144\n5.4. Reading and Writing Binary Data                                                                        145\n5.5. Writing to a File That Doesn’t Already Exist                                                      147\n5.6. Performing I/O Operations on a String                                                              148\n5.7. Reading and Writing Compressed Datafiles                                                       149\n5.8. Iterating Over Fixed-Sized Records                                                                     151\n5.9. Reading Binary Data into a Mutable Buffer                                                       152\n5.10. Memory Mapping Binary Files                                                                          153\n5.11. Manipulating Pathnames                                                                                    156\n5.12. Testing for the Existence of a File                                                                       157\n5.13. Getting a Directory Listing                                                                                 158\n5.14. Bypassing Filename Encoding                                                                            160\n5.15. Printing Bad Filenames                                                                                       161\n5.16. Adding or Changing the Encoding of an Already Open File                         163\n5.17. Writing Bytes to a Text File                                                                                 165\n5.18. Wrapping an Existing File Descriptor As a File Object                                  166\n5.19. Making Temporary Files and Directories                                                         167\n5.20. Communicating with Serial Ports                                                                      170\n5.21. Serializing Python Objects                                                                                  171\n6. Data Encoding and Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  175\n6.1. Reading and Writing CSV Data                                                                            175\n6.2. Reading and Writing JSON Data                                                                          179\n6.3. Parsing Simple XML Data                                                                                     183\n6.4. Parsing Huge XML Files Incrementally                                                              186\n6.5. Turning a Dictionary into XML                                                                           189\n6.6. Parsing, Modifying, and Rewriting XML                                                            191\n6.7. Parsing XML Documents with Namespaces                                                      193\nTable of Contents \n| \nv",
      "content_length": 4449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": 1,
      "content": "6.8. Interacting with a Relational Database                                                                195\n6.9. Decoding and Encoding Hexadecimal Digits                                                    197\n6.10. Decoding and Encoding Base64                                                                         199\n6.11. Reading and Writing Binary Arrays of Structures                                           199\n6.12. Reading Nested and Variable-Sized Binary Structures                                   203\n6.13. Summarizing Data and Performing Statistics                                                  214\n7. Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  217\n7.1. Writing Functions That Accept Any Number of Arguments                           217\n7.2. Writing Functions That Only Accept Keyword Arguments                            219\n7.3. Attaching Informational Metadata to Function Arguments                            220\n7.4. Returning Multiple Values from a Function                                                       221\n7.5. Defining Functions with Default Arguments                                                     222\n7.6. Defining Anonymous or Inline Functions                                                         224\n7.7. Capturing Variables in Anonymous Functions                                                  225\n7.8. Making an N-Argument Callable Work As a Callable with Fewer\nArguments                                                                                                                      227\n7.9. Replacing Single Method Classes with Functions                                              231\n7.10. Carrying Extra State with Callback Functions                                                 232\n7.11. Inlining Callback Functions                                                                                235\n7.12. Accessing Variables Defined Inside a Closure                                                 238\n8. Classes and Objects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  243\n8.1. Changing the String Representation of Instances                                              243\n8.2. Customizing String Formatting                                                                            245\n8.3. Making Objects Support the Context-Management Protocol                         246\n8.4. Saving Memory When Creating a Large Number of Instances                       248\n8.5. Encapsulating Names in a Class                                                                           250\n8.6. Creating Managed Attributes                                                                               251\n8.7. Calling a Method on a Parent Class                                                                     256\n8.8. Extending a Property in a Subclass                                                                      260\n8.9. Creating a New Kind of Class or Instance Attribute                                         264\n8.10. Using Lazily Computed Properties                                                                    267\n8.11. Simplifying the Initialization of Data Structures                                             270\n8.12. Defining an Interface or Abstract Base Class                                                   274\n8.13. Implementing a Data Model or Type System                                                   277\n8.14. Implementing Custom Containers                                                                    283\n8.15. Delegating Attribute Access                                                                                287\n8.16. Defining More Than One Constructor in a Class                                           291\n8.17. Creating an Instance Without Invoking init                                                    293\n8.18. Extending Classes with Mixins                                                                           294\n8.19. Implementing Stateful Objects or State Machines                                           299\nvi \n| \nTable of Contents",
      "content_length": 4237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": 1,
      "content": "8.20. Calling a Method on an Object Given the Name As a String                        305\n8.21. Implementing the Visitor Pattern                                                                      306\n8.22. Implementing the Visitor Pattern Without Recursion                                   311\n8.23. Managing Memory in Cyclic Data Structures                                                  317\n8.24. Making Classes Support Comparison Operations                                          321\n8.25. Creating Cached Instances                                                                                  323\n9. Metaprogramming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  329\n9.1. Putting a Wrapper Around a Function                                                               329\n9.2. Preserving Function Metadata When Writing Decorators                              331\n9.3. Unwrapping a Decorator                                                                                       333\n9.4. Defining a Decorator That Takes Arguments                                                    334\n9.5. Defining a Decorator with User Adjustable Attributes                                     336\n9.6. Defining a Decorator That Takes an Optional Argument                                339\n9.7. Enforcing Type Checking on a Function Using a Decorator                           341\n9.8. Defining Decorators As Part of a Class                                                               345\n9.9. Defining Decorators As Classes                                                                           347\n9.10. Applying Decorators to Class and Static Methods                                          350\n9.11. Writing Decorators That Add Arguments to Wrapped Functions               352\n9.12. Using Decorators to Patch Class Definitions                                                   355\n9.13. Using a Metaclass to Control Instance Creation                                              356\n9.14. Capturing Class Attribute Definition Order                                                    359\n9.15. Defining a Metaclass That Takes Optional Arguments                                  362\n9.16. Enforcing an Argument Signature on *args and **kwargs                            364\n9.17. Enforcing Coding Conventions in Classes                                                       367\n9.18. Defining Classes Programmatically                                                                   370\n9.19. Initializing Class Members at Definition Time                                                374\n9.20. Implementing Multiple Dispatch with Function Annotations                      376\n9.21. Avoiding Repetitive Property Methods                                                             382\n9.22. Defining Context Managers the Easy Way                                                       384\n9.23. Executing Code with Local Side Effects                                                            386\n9.24. Parsing and Analyzing Python Source                                                              388\n9.25. Disassembling Python Byte Code                                                                      392\n10. Modules and Packages. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  397\n10.1. Making a Hierarchical Package of Modules                                                     397\n10.2. Controlling the Import of Everything                                                               398\n10.3. Importing Package Submodules Using Relative Names                                 399\n10.4. Splitting a Module into Multiple Files                                                               401\n10.5. Making Separate Directories of Code Import Under a Common\nNamespace                                                                                                                      404\n10.6. Reloading Modules                                                                                               406\nTable of Contents \n| \nvii",
      "content_length": 4179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": 1,
      "content": "10.7. Making a Directory or Zip File Runnable As a Main Script                          407\n10.8. Reading Datafiles Within a Package                                                                  408\n10.9. Adding Directories to sys.path                                                                           409\n10.10. Importing Modules Using a Name Given in a String                                   411\n10.11. Loading Modules from a Remote Machine Using Import Hooks               412\n10.12. Patching Modules on Import                                                                            428\n10.13. Installing Packages Just for Yourself                                                                431\n10.14. Creating a New Python Environment                                                             432\n10.15. Distributing Packages                                                                                        433\n11. Network and Web Programming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  437\n11.1. Interacting with HTTP Services As a Client                                                    437\n11.2. Creating a TCP Server                                                                                         441\n11.3. Creating a UDP Server                                                                                        445\n11.4. Generating a Range of IP Addresses from a CIDR Address                          447\n11.5. Creating a Simple REST-Based Interface                                                          449\n11.6. Implementing a Simple Remote Procedure Call with XML-RPC                 454\n11.7. Communicating Simply Between Interpreters                                                 456\n11.8. Implementing Remote Procedure Calls                                                            458\n11.9. Authenticating Clients Simply                                                                            461\n11.10. Adding SSL to Network Services                                                                      464\n11.11. Passing a Socket File Descriptor Between Processes                                     470\n11.12. Understanding Event-Driven I/O                                                                    475\n11.13. Sending and Receiving Large Arrays                                                               481\n12. Concurrency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  485\n12.1. Starting and Stopping Threads                                                                           485\n12.2. Determining If a Thread Has Started                                                                488\n12.3. Communicating Between Threads                                                                    491\n12.4. Locking Critical Sections                                                                                     497\n12.5. Locking with Deadlock Avoidance                                                                    500\n12.6. Storing Thread-Specific State                                                                             504\n12.7. Creating a Thread Pool                                                                                        505\n12.8. Performing Simple Parallel Programming                                                       509\n12.9. Dealing with the GIL (and How to Stop Worrying About It)                        513\n12.10. Defining an Actor Task                                                                                      516\n12.11. Implementing Publish/Subscribe Messaging                                                 520\n12.12. Using Generators As an Alternative to Threads                                            524\n12.13. Polling Multiple Thread Queues                                                                      531\n12.14. Launching a Daemon Process on Unix                                                           534\n13. Utility Scripting and System Administration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  539\nviii \n| \nTable of Contents",
      "content_length": 4251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": 1,
      "content": "13.1. Accepting Script Input via Redirection, Pipes, or Input Files                        539\n13.2. Terminating a Program with an Error Message                                               540\n13.3. Parsing Command-Line Options                                                                       541\n13.4. Prompting for a Password at Runtime                                                              544\n13.5. Getting the Terminal Size                                                                                    545\n13.6. Executing an External Command and Getting Its Output                             545\n13.7. Copying or Moving Files and Directories                                                         547\n13.8. Creating and Unpacking Archives                                                                     549\n13.9. Finding Files by Name                                                                                         550\n13.10. Reading Configuration Files                                                                             552\n13.11. Adding Logging to Simple Scripts                                                                   555\n13.12. Adding Logging to Libraries                                                                             558\n13.13. Making a Stopwatch Timer                                                                               559\n13.14. Putting Limits on Memory and CPU Usage                                                  561\n13.15. Launching a Web Browser                                                                                563\n14. Testing, Debugging, and Exceptions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  565\n14.1. Testing Output Sent to stdout                                                                             565\n14.2. Patching Objects in Unit Tests                                                                            567\n14.3. Testing for Exceptional Conditions in Unit Tests                                            570\n14.4. Logging Test Output to a File                                                                             572\n14.5. Skipping or Anticipating Test Failures                                                              573\n14.6. Handling Multiple Exceptions                                                                            574\n14.7. Catching All Exceptions                                                                                      576\n14.8. Creating Custom Exceptions                                                                              578\n14.9. Raising an Exception in Response to Another Exception                              580\n14.10. Reraising the Last Exception                                                                            582\n14.11. Issuing Warning Messages                                                                                583\n14.12. Debugging Basic Program Crashes                                                                 585\n14.13. Profiling and Timing Your Program                                                               587\n14.14. Making Your Programs Run Faster                                                                 590\n15. C Extensions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  597\n15.1. Accessing C Code Using ctypes                                                                          599\n15.2. Writing a Simple C Extension Module                                                              605\n15.3. Writing an Extension Function That Operates on Arrays                              609\n15.4. Managing Opaque Pointers in C Extension Modules                                     612\n15.5. Defining and Exporting C APIs from Extension Modules                            614\n15.6. Calling Python from C                                                                                        619\n15.7. Releasing the GIL in C Extensions                                                                     625\n15.8. Mixing Threads from C and Python                                                                 625\n15.9. Wrapping C Code with Swig                                                                               627\nTable of Contents \n| \nix",
      "content_length": 4420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": 1,
      "content": "15.10. Wrapping Existing C Code with Cython                                                        632\n15.11. Using Cython to Write High-Performance Array Operations                    638\n15.12. Turning a Function Pointer into a Callable                                                    643\n15.13. Passing NULL-Terminated Strings to C Libraries                                         644\n15.14. Passing Unicode Strings to C Libraries                                                           648\n15.15. Converting C Strings to Python                                                                       653\n15.16. Working with C Strings of Dubious Encoding                                              654\n15.17. Passing Filenames to C Extensions                                                                  657\n15.18. Passing Open Files to C Extensions                                                                 658\n15.19. Reading File-Like Objects from C                                                                   659\n15.20. Consuming an Iterable from C                                                                        662\n15.21. Diagnosing Segmentation Faults                                                                      663\nA. Further Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  665\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  667\nx \n| \nTable of Contents",
      "content_length": 1579,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": 1,
      "content": "Preface\nSince 2008, the Python world has been watching the slow evolution of Python 3. It was\nalways known that the adoption of Python 3 would likely take a long time. In fact, even\nat the time of this writing (2013), most working Python programmers continue to use\nPython 2 in production. A lot has been made about the fact that Python 3 is not backward\ncompatible with past versions. To be sure, backward compatibility is an issue for anyone\nwith an existing code base. However, if you shift your view toward the future, you’ll find\nthat Python 3 offers much more than meets the eye.\nJust as Python 3 is about the future, this edition of the Python Cookbook represents a\nmajor change over past editions. First and foremost, this is meant to be a very forward\nlooking book. All of the recipes have been written and tested with Python 3.3 without\nregard to past Python versions or the “old way” of doing things. In fact, many of the\nrecipes will only work with Python 3.3 and above. Doing so may be a calculated risk,\nbut the ultimate goal is to write a book of recipes based on the most modern tools and\nidioms possible. It is hoped that the recipes can serve as a guide for people writing new\ncode in Python 3 or those who hope to modernize existing code.\nNeedless to say, writing a book of recipes in this style presents a certain editorial chal‐\nlenge. An online search for Python recipes returns literally thousands of useful recipes\non sites such as ActiveState’s Python recipes or Stack Overflow. However, most of these\nrecipes are steeped in history and the past. Besides being written almost exclusively for\nPython 2, they often contain workarounds and hacks related to differences between old\nversions of Python (e.g., version 2.3 versus 2.4). Moreover, they often use outdated\ntechniques that have simply become a built-in feature of Python 3.3. Finding recipes\nexclusively focused on Python 3 can be a bit more difficult.\nRather than attempting to seek out Python 3-specific recipes, the topics of this book are\nmerely inspired by existing code and techniques. Using these ideas as a springboard,\nthe writing is an original work that has been deliberately written with the most modern\nPython programming techniques possible. Thus, it can serve as a reference for anyone\nwho wants to write their code in a modern style.\nxi",
      "content_length": 2333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": 1,
      "content": "In choosing which recipes to include, there is a certain realization that it is simply\nimpossible to write a book that covers every possible thing that someone might do with\nPython. Thus, a priority has been given to topics that focus on the core Python language\nas well as tasks that are common to a wide variety of application domains. In addition,\nmany of the recipes aim to illustrate features that are new to Python 3 and more likely\nto be unknown to even experienced programmers using older versions. There is also a\ncertain preference to recipes that illustrate a generally applicable programming tech‐\nnique (i.e., programming patterns) as opposed to those that narrowly try to address a\nvery specific practical problem. Although certain third-party packages get coverage, a\nmajority of the recipes focus on the core language and standard library.\nWho This Book Is For\nThis book is aimed at more experienced Python programmers who are looking to\ndeepen their understanding of the language and modern programming idioms. Much\nof the material focuses on some of the more advanced techniques used by libraries,\nframeworks, and applications. Throughout the book, the recipes generally assume that\nthe reader already has the necessary background to understand the topic at hand (e.g.,\ngeneral knowledge of computer science, data structures, complexity, systems program‐\nming, concurrency, C programming, etc.). Moreover, the recipes are often just skeletons\nthat aim to provide essential information for getting started, but which require the\nreader to do more research to fill in the details. As such, it is assumed that the reader\nknows how to use search engines and Python’s excellent online documentation.\nMany of the more advanced recipes will reward the reader’s patience with a much greater\ninsight into how Python actually works under the covers. You will learn new tricks and\ntechniques that can be applied to your own code.\nWho This Book Is Not For\nThis is not a book designed for beginners trying to learn Python for the first time. In\nfact, it already assumes that you know the basics that might be taught in a Python tutorial\nor more introductory book. This book is also not designed to serve as a quick reference\nmanual (e.g., quickly looking up the functions in a specific module). Instead, the book\naims to focus on specific programming topics, show possible solutions, and serve as a\nspringboard for jumping into more advanced material you might find online or in a\nreference.\nxii \n| \nPreface",
      "content_length": 2512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": 1,
      "content": "Conventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program elements\nsuch as variable or function names, databases, data types, environment variables,\nstatements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nThis icon signifies a tip, suggestion, or general note.\nThis icon indicates a warning or caution.\nOnline Code Examples\nAlmost all of the code examples in this book are available online at http://github.com/\ndabeaz/python-cookbook. The authors welcome bug fixes, improvements, and com‐\nments.\nUsing Code Examples\nThis book is here to help you get your job done. In general, if this book includes code\nexamples, you may use the code in this book in your programs and documentation. You\ndo not need to contact us for permission unless you’re reproducing a significant portion\nof the code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing a CD-ROM of examples from\nO’Reilly books does require permission. Answering a question by citing this book and\nquoting example code does not require permission. Incorporating a significant amount\nPreface \n| \nxiii",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": 1,
      "content": "of example code from this book into your product’s documentation does require per‐\nmission.\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\nauthor, publisher, and ISBN. For example: Python Cookbook, 3rd edition, by David\nBeazley and Brian K. Jones (O’Reilly). Copyright 2013 David Beazley and Brian Jones,\n978-1-449-34037-7.\nIf you feel your use of code examples falls outside fair use or the permission given here,\nfeel free to contact us at permissions@oreilly.com.\nSafari® Books Online\nSafari Books Online is an on-demand digital library that delivers ex‐\npert content in both book and video form from the world’s leading\nauthors in technology and business.\nTechnology professionals, software developers, web designers, and business and crea‐\ntive professionals use Safari Books Online as their primary resource for research, prob‐\nlem solving, learning, and certification training.\nSafari Books Online offers a range of product mixes and pricing programs for organi‐\nzations, government agencies, and individuals. Subscribers have access to thousands of\nbooks, training videos, and prepublication manuscripts in one fully searchable database\nfrom publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley Pro‐\nfessional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John\nWiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT\nPress, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course Technol‐\nogy, and dozens more. For more information about Safari Books Online, please visit us\nonline.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at http://oreil.ly/python_cookbook_3e.\nxiv \n| \nPreface",
      "content_length": 2063,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": 1,
      "content": "To comment or ask technical questions about this book, send email to bookques\ntions@oreilly.com.\nFor more information about our books, courses, conferences, and news, see our website\nat http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nWe would like to acknowledge the technical reviewers, Jake Vanderplas, Robert Kern,\nand Andrea Crotti, for their very helpful comments, as well as the general Python com‐\nmunity for their support and encouragement. We would also like to thank the editors\nof the prior edition, Alex Martelli, Anna Ravenscroft, and David Ascher. Although this\nedition is newly written, the previous edition provided an initial framework for selecting\nthe topics and recipes of interest. Last, but not least, we would like to thank readers of\nthe early release editions for their comments and suggestions for improvement.\nDavid Beazley’s Acknowledgments\nWriting a book is no small task. As such, I would like to thank my wife Paula and my\ntwo boys for their patience and support during this project. Much of the material in this\nbook was derived from content I developed teaching Python-related training classes\nover the last six years. Thus, I’d like to thank all of the students who have taken my\ncourses and ultimately made this book possible. I’d also like to thank Ned Batchelder,\nTravis Oliphant, Peter Wang, Brian Van de Ven, Hugo Shi, Raymond Hettinger, Michael\nFoord, and Daniel Klein for traveling to the four corners of the world to teach these\ncourses while I stayed home in Chicago to work on this project. Meghan Blanchette and\nRachel Roumeliotis of O’Reilly were also instrumental in seeing this project through to\ncompletion despite the drama of several false starts and unforeseen delays. Last, but not\nleast, I’d like to thank the Python community for their continued support and putting\nup with my flights of diabolical fancy.\nDavid M. Beazley\nhttp://www.dabeaz.com\nhttps://twitter.com/dabeaz\nPreface \n| \nxv",
      "content_length": 2093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": 1,
      "content": "Brian Jones’ Acknowledgments\nI would like to thank both my coauthor, David Beazley, as well as Meghan Blanchette\nand Rachel Roumeliotis of O’Reilly, for working with me on this project. I would also\nlike to thank my amazing wife, Natasha, for her patience and encouragement in this\nproject, and her support in all of my ambitions. Most of all, I’d like to thank the Python\ncommunity at large. Though I have contributed to the support of various open source\nprojects, languages, clubs, and the like, no work has been so gratifying and rewarding\nas that which has been in the service of the Python community.\nBrian K. Jones\nhttp://www.protocolostomy.com\nhttps://twitter.com/bkjones\nxvi \n| \nPreface",
      "content_length": 695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": 1,
      "content": "CHAPTER 1\nData Structures and Algorithms\nPython provides a variety of useful built-in data structures, such as lists, sets, and dic‐\ntionaries. For the most part, the use of these structures is straightforward. However,\ncommon questions concerning searching, sorting, ordering, and filtering often arise.\nThus, the goal of this chapter is to discuss common data structures and algorithms\ninvolving data. In addition, treatment is given to the various data structures contained\nin the collections module.\n1.1. Unpacking a Sequence into Separate Variables\nProblem\nYou have an N-element tuple or sequence that you would like to unpack into a collection\nof N variables.\nSolution\nAny sequence (or iterable) can be unpacked into variables using a simple assignment\noperation. The only requirement is that the number of variables and structure match\nthe sequence. For example:\n>>> p = (4, 5)\n>>> x, y = p\n>>> x\n4\n>>> y\n5\n>>>\n>>> data = [ 'ACME', 50, 91.1, (2012, 12, 21) ]\n>>> name, shares, price, date = data\n>>> name\n1",
      "content_length": 1013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": 1,
      "content": "'ACME'\n>>> date\n(2012, 12, 21)\n>>> name, shares, price, (year, mon, day) = data\n>>> name\n'ACME'\n>>> year\n2012\n>>> mon\n12\n>>> day\n21\n>>>\nIf there is a mismatch in the number of elements, you’ll get an error. For example:\n>>> p = (4, 5)\n>>> x, y, z = p\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: need more than 2 values to unpack\n>>>\nDiscussion\nUnpacking actually works with any object that happens to be iterable, not just tuples or\nlists. This includes strings, files, iterators, and generators. For example:\n>>> s = 'Hello'\n>>> a, b, c, d, e = s\n>>> a\n'H'\n>>> b\n'e'\n>>> e\n'o'\n>>>\nWhen unpacking, you may sometimes want to discard certain values. Python has no\nspecial syntax for this, but you can often just pick a throwaway variable name for it. For\nexample:\n>>> data = [ 'ACME', 50, 91.1, (2012, 12, 21) ]\n>>> _, shares, price, _ = data\n>>> shares\n50\n>>> price\n91.1\n>>>\nHowever, make sure that the variable name you pick isn’t being used for something else\nalready.\n2 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1058,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": 1,
      "content": "1.2. Unpacking Elements from Iterables of Arbitrary\nLength\nProblem\nYou need to unpack N elements from an iterable, but the iterable may be longer than N\nelements, causing a “too many values to unpack” exception.\nSolution\nPython “star expressions” can be used to address this problem. For example, suppose\nyou run a course and decide at the end of the semester that you’re going to drop the first\nand last homework grades, and only average the rest of them. If there are only four\nassignments, maybe you simply unpack all four, but what if there are 24? A star expres‐\nsion makes it easy:\ndef drop_first_last(grades):\n    first, *middle, last = grades\n    return avg(middle)\nAs another use case, suppose you have user records that consist of a name and email\naddress, followed by an arbitrary number of phone numbers. You could unpack the\nrecords like this:\n>>> record = ('Dave', 'dave@example.com', '773-555-1212', '847-555-1212')\n>>> name, email, *phone_numbers = user_record\n>>> name\n'Dave'\n>>> email\n'dave@example.com'\n>>> phone_numbers\n['773-555-1212', '847-555-1212']\n>>>\nIt’s worth noting that the phone_numbers variable will always be a list, regardless of how\nmany phone numbers are unpacked (including none). Thus, any code that uses\nphone_numbers won’t have to account for the possibility that it might not be a list or\nperform any kind of additional type checking.\nThe starred variable can also be the first one in the list. For example, say you have a\nsequence of values representing your company’s sales figures for the last eight quarters.\nIf you want to see how the most recent quarter stacks up to the average of the first seven,\nyou could do something like this:\n*trailing_qtrs, current_qtr = sales_record\ntrailing_avg = sum(trailing_qtrs) / len(trailing_qtrs)\nreturn avg_comparison(trailing_avg, current_qtr)\nHere’s a view of the operation from the Python interpreter:\n1.2. Unpacking Elements from Iterables of Arbitrary Length \n| \n3",
      "content_length": 1951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": 1,
      "content": ">>> *trailing, current = [10, 8, 7, 1, 9, 5, 10, 3]\n>>> trailing\n[10, 8, 7, 1, 9, 5, 10]\n>>> current\n3\nDiscussion\nExtended iterable unpacking is tailor-made for unpacking iterables of unknown or ar‐\nbitrary length. Oftentimes, these iterables have some known component or pattern in\ntheir construction (e.g. “everything after element 1 is a phone number”), and star un‐\npacking lets the developer leverage those patterns easily instead of performing acro‐\nbatics to get at the relevant elements in the iterable.\nIt is worth noting that the star syntax can be especially useful when iterating over a\nsequence of tuples of varying length. For example, perhaps a sequence of tagged tuples:\nrecords = [\n     ('foo', 1, 2),\n     ('bar', 'hello'),\n     ('foo', 3, 4),\n]\ndef do_foo(x, y):\n    print('foo', x, y)\ndef do_bar(s):\n    print('bar', s)\nfor tag, *args in records:\n    if tag == 'foo':\n        do_foo(*args)\n    elif tag == 'bar':\n        do_bar(*args)\nStar unpacking can also be useful when combined with certain kinds of string processing\noperations, such as splitting. For example:\n>>> line = 'nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false'\n>>> uname, *fields, homedir, sh = line.split(':')\n>>> uname\n'nobody'\n>>> homedir\n'/var/empty'\n>>> sh\n'/usr/bin/false'\n>>>\nSometimes you might want to unpack values and throw them away. You can’t just specify\na bare * when unpacking, but you could use a common throwaway variable name, such\nas _ or ign (ignored). For example:\n4 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": 1,
      "content": ">>> record = ('ACME', 50, 123.45, (12, 18, 2012))\n>>> name, *_, (*_, year) = record\n>>> name\n'ACME'\n>>> year\n2012\n>>>\nThere is a certain similarity between star unpacking and list-processing features of var‐\nious functional languages. For example, if you have a list, you can easily split it into head\nand tail components like this:\n>>> items = [1, 10, 7, 4, 5, 9]\n>>> head, *tail = items\n>>> head\n1\n>>> tail\n[10, 7, 4, 5, 9]\n>>>\nOne could imagine writing functions that perform such splitting in order to carry out\nsome kind of clever recursive algorithm. For example:\n>>> def sum(items):\n...     head, *tail = items\n...     return head + sum(tail) if tail else head\n...\n>>> sum(items)\n36\n>>>\nHowever, be aware that recursion really isn’t a strong Python feature due to the inherent\nrecursion limit. Thus, this last example might be nothing more than an academic cu‐\nriosity in practice.\n1.3. Keeping the Last N Items\nProblem\nYou want to keep a limited history of the last few items seen during iteration or during\nsome other kind of processing.\nSolution\nKeeping a limited history is a perfect use for a collections.deque. For example, the\nfollowing code performs a simple text match on a sequence of lines and yields the\nmatching line along with the previous N lines of context when found:\n1.3. Keeping the Last N Items \n| \n5",
      "content_length": 1327,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": 1,
      "content": "from collections import deque\ndef search(lines, pattern, history=5):\n    previous_lines = deque(maxlen=history)\n    for line in lines:\n        if pattern in line:\n            yield line, previous_lines\n        previous_lines.append(line)\n# Example use on a file\nif __name__ == '__main__':\n    with open('somefile.txt') as f:\n        for line, prevlines in search(f, 'python', 5):\n            for pline in prevlines:\n                print(pline, end='')\n            print(line, end='')\n            print('-'*20)\nDiscussion\nWhen writing code to search for items, it is common to use a generator function in‐\nvolving yield, as shown in this recipe’s solution. This decouples the process of searching\nfrom the code that uses the results. If you’re new to generators, see Recipe 4.3.\nUsing deque(maxlen=N) creates a fixed-sized queue. When new items are added and\nthe queue is full, the oldest item is automatically removed. For example:\n>>> q = deque(maxlen=3)\n>>> q.append(1)\n>>> q.append(2)\n>>> q.append(3)\n>>> q\ndeque([1, 2, 3], maxlen=3)\n>>> q.append(4)\n>>> q\ndeque([2, 3, 4], maxlen=3)\n>>> q.append(5)\n>>> q\ndeque([3, 4, 5], maxlen=3)\nAlthough you could manually perform such operations on a list (e.g., appending, de‐\nleting, etc.), the queue solution is far more elegant and runs a lot faster.\nMore generally, a deque can be used whenever you need a simple queue structure. If\nyou don’t give it a maximum size, you get an unbounded queue that lets you append\nand pop items on either end. For example:\n>>> q = deque()\n>>> q.append(1)\n>>> q.append(2)\n>>> q.append(3)\n>>> q\n6 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": 1,
      "content": "deque([1, 2, 3])\n>>> q.appendleft(4)\n>>> q\ndeque([4, 1, 2, 3])\n>>> q.pop()\n3\n>>> q\ndeque([4, 1, 2])\n>>> q.popleft()\n4\nAdding or popping items from either end of a queue has O(1) complexity. This is unlike\na list where inserting or removing items from the front of the list is O(N).\n1.4. Finding the Largest or Smallest N Items\nProblem\nYou want to make a list of the largest or smallest N items in a collection.\nSolution\nThe heapq module has two functions—nlargest() and nsmallest()—that do exactly\nwhat you want. For example:\nimport heapq\nnums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]\nprint(heapq.nlargest(3, nums))  # Prints [42, 37, 23]\nprint(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2]\nBoth functions also accept a key parameter that allows them to be used with more\ncomplicated data structures. For example:\nportfolio = [\n   {'name': 'IBM', 'shares': 100, 'price': 91.1},\n   {'name': 'AAPL', 'shares': 50, 'price': 543.22},\n   {'name': 'FB', 'shares': 200, 'price': 21.09},\n   {'name': 'HPQ', 'shares': 35, 'price': 31.75},\n   {'name': 'YHOO', 'shares': 45, 'price': 16.35},\n   {'name': 'ACME', 'shares': 75, 'price': 115.65}\n]\ncheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price'])\nexpensive = heapq.nlargest(3, portfolio, key=lambda s: s['price'])\nDiscussion\nIf you are looking for the N smallest or largest items and N is small compared to the\noverall size of the collection, these functions provide superior performance. Underneath\n1.4. Finding the Largest or Smallest N Items \n| \n7",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": 1,
      "content": "the covers, they work by first converting the data into a list where items are ordered as\na heap. For example:\n>>> nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]\n>>> import heapq\n>>> heap = list(nums)\n>>> heapq.heapify(heap)\n>>> heap\n[-4, 2, 1, 23, 7, 2, 18, 23, 42, 37, 8]\n>>>\nThe most important feature of a heap is that heap[0] is always the smallest item. More‐\nover, subsequent items can be easily found using the heapq.heappop() method, which\npops off the first item and replaces it with the next smallest item (an operation that\nrequires O(log N) operations where N is the size of the heap). For example, to find the\nthree smallest items, you would do this:\n>>> heapq.heappop(heap)\n-4\n>>> heapq.heappop(heap)\n1\n>>> heapq.heappop(heap)\n2\nThe nlargest() and nsmallest() functions are most appropriate if you are trying to\nfind a relatively small number of items. If you are simply trying to find the single smallest\nor largest item (N=1), it is faster to use min() and max(). Similarly, if N is about the\nsame size as the collection itself, it is usually faster to sort it first and take a slice (i.e.,\nuse sorted(items)[:N] or sorted(items)[-N:]). It should be noted that the actual\nimplementation of nlargest() and nsmallest() is adaptive in how it operates and will\ncarry out some of these optimizations on your behalf (e.g., using sorting if N is close to\nthe same size as the input).\nAlthough it’s not necessary to use this recipe, the implementation of a heap is an inter‐\nesting and worthwhile subject of study. This can usually be found in any decent book\non algorithms and data structures. The documentation for the heapq module also dis‐\ncusses the underlying implementation details.\n1.5. Implementing a Priority Queue\nProblem\nYou want to implement a queue that sorts items by a given priority and always returns\nthe item with the highest priority on each pop operation.\n8 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": 1,
      "content": "Solution\nThe following class uses the heapq module to implement a simple priority queue:\nimport heapq\nclass PriorityQueue:\n    def __init__(self):\n        self._queue = []\n        self._index = 0\n    def push(self, item, priority):\n        heapq.heappush(self._queue, (-priority, self._index, item))\n        self._index += 1\n    def pop(self):\n        return heapq.heappop(self._queue)[-1]\nHere is an example of how it might be used:\n>>> class Item:\n...     def __init__(self, name):\n...         self.name = name\n...     def __repr__(self):\n...         return 'Item({!r})'.format(self.name)\n...\n>>> q = PriorityQueue()\n>>> q.push(Item('foo'), 1)\n>>> q.push(Item('bar'), 5)\n>>> q.push(Item('spam'), 4)\n>>> q.push(Item('grok'), 1)\n>>> q.pop()\nItem('bar')\n>>> q.pop()\nItem('spam')\n>>> q.pop()\nItem('foo')\n>>> q.pop()\nItem('grok')\n>>>\nObserve how the first pop() operation returned the item with the highest priority. Also\nobserve how the two items with the same priority (foo and grok) were returned in the\nsame order in which they were inserted into the queue.\nDiscussion\nThe core of this recipe concerns the use of the heapq module. The functions heapq.heap\npush() and heapq.heappop() insert and remove items from a list _queue in a way such\nthat the first item in the list has the smallest priority (as discussed in Recipe 1.4). The\nheappop() method always returns the “smallest” item, so that is the key to making the\n1.5. Implementing a Priority Queue \n| \n9",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": 1,
      "content": "queue pop the correct items. Moreover, since the push and pop operations have O(log\nN) complexity where N is the number of items in the heap, they are fairly efficient even\nfor fairly large values of N.\nIn this recipe, the queue consists of tuples of the form (-priority, index, item). The\npriority value is negated to get the queue to sort items from highest priority to lowest\npriority. This is opposite of the normal heap ordering, which sorts from lowest to highest\nvalue.\nThe role of the index variable is to properly order items with the same priority level.\nBy keeping a constantly increasing index, the items will be sorted according to the order\nin which they were inserted. However, the index also serves an important role in making\nthe comparison operations work for items that have the same priority level.\nTo elaborate on that, instances of Item in the example can’t be ordered. For example:\n>>> a = Item('foo')\n>>> b = Item('bar')\n>>> a < b\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unorderable types: Item() < Item()\n>>>\nIf you make (priority, item) tuples, they can be compared as long as the priorities\nare different. However, if two tuples with equal priorities are compared, the comparison\nfails as before. For example:\n>>> a = (1, Item('foo'))\n>>> b = (5, Item('bar'))\n>>> a < b\nTrue\n>>> c = (1, Item('grok'))\n>>> a < c\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unorderable types: Item() < Item()\n>>>\nBy introducing the extra index and making (priority, index, item) tuples, you avoid\nthis problem entirely since no two tuples will ever have the same value for index (and\nPython never bothers to compare the remaining tuple values once the result of com‐\nparison can be determined):\n>>> a = (1, 0, Item('foo'))\n>>> b = (5, 1, Item('bar'))\n>>> c = (1, 2, Item('grok'))\n>>> a < b\nTrue\n>>> a < c\n10 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": 1,
      "content": "True\n>>>\nIf you want to use this queue for communication between threads, you need to add\nappropriate locking and signaling. See Recipe 12.3 for an example of how to do this.\nThe documentation for the heapq module has further examples and discussion con‐\ncerning the theory and implementation of heaps.\n1.6. Mapping Keys to Multiple Values in a Dictionary\nProblem\nYou want to make a dictionary that maps keys to more than one value (a so-called\n“multidict”).\nSolution\nA dictionary is a mapping where each key is mapped to a single value. If you want to\nmap keys to multiple values, you need to store the multiple values in another container\nsuch as a list or set. For example, you might make dictionaries like this:\nd = {\n   'a' : [1, 2, 3],\n   'b' : [4, 5]\n}\ne = {\n   'a' : {1, 2, 3},\n   'b' : {4, 5}\n}\nThe choice of whether or not to use lists or sets depends on intended use. Use a list if\nyou want to preserve the insertion order of the items. Use a set if you want to eliminate\nduplicates (and don’t care about the order).\nTo easily construct such dictionaries, you can use defaultdict in the collections\nmodule. A feature of defaultdict is that it automatically initializes the first value so\nyou can simply focus on adding items. For example:\nfrom collections import defaultdict\nd = defaultdict(list)\nd['a'].append(1)\nd['a'].append(2)\nd['b'].append(4)\n...\nd = defaultdict(set)\n1.6. Mapping Keys to Multiple Values in a Dictionary \n| \n11",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": 1,
      "content": "d['a'].add(1)\nd['a'].add(2)\nd['b'].add(4)\n...\nOne caution with defaultdict is that it will automatically create dictionary entries for\nkeys accessed later on (even if they aren’t currently found in the dictionary). If you don’t\nwant this behavior, you might use setdefault() on an ordinary dictionary instead. For\nexample:\nd = {}    # A regular dictionary\nd.setdefault('a', []).append(1)\nd.setdefault('a', []).append(2)\nd.setdefault('b', []).append(4)\n...\nHowever, many programmers find setdefault() to be a little unnatural—not to men‐\ntion the fact that it always creates a new instance of the initial value on each invocation\n(the empty list [] in the example).\nDiscussion\nIn principle, constructing a multivalued dictionary is simple. However, initialization of\nthe first value can be messy if you try to do it yourself. For example, you might have\ncode that looks like this:\nd = {}\nfor key, value in pairs:\n    if key not in d:\n         d[key] = []\n    d[key].append(value)\nUsing a defaultdict simply leads to much cleaner code:\nd = defaultdict(list)\nfor key, value in pairs:\n    d[key].append(value)\nThis recipe is strongly related to the problem of grouping records together in data pro‐\ncessing problems. See Recipe 1.15 for an example.\n1.7. Keeping Dictionaries in Order\nProblem\nYou want to create a dictionary, and you also want to control the order of items when\niterating or serializing.\n12 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1448,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": 1,
      "content": "Solution\nTo control the order of items in a dictionary, you can use an OrderedDict from the\ncollections module. It exactly preserves the original insertion order of data when\niterating. For example:\nfrom collections import OrderedDict\nd = OrderedDict()\nd['foo'] = 1\nd['bar'] = 2\nd['spam'] = 3\nd['grok'] = 4\n# Outputs \"foo 1\", \"bar 2\", \"spam 3\", \"grok 4\"\nfor key in d:\n    print(key, d[key])\nAn OrderedDict can be particularly useful when you want to build a mapping that you\nmay want to later serialize or encode into a different format. For example, if you want\nto precisely control the order of fields appearing in a JSON encoding, first building the\ndata in an OrderedDict will do the trick:\n>>> import json\n>>> json.dumps(d)\n'{\"foo\": 1, \"bar\": 2, \"spam\": 3, \"grok\": 4}'\n>>>\nDiscussion\nAn OrderedDict internally maintains a doubly linked list that orders the keys according\nto insertion order. When a new item is first inserted, it is placed at the end of this list.\nSubsequent reassignment of an existing key doesn’t change the order.\nBe aware that the size of an OrderedDict is more than twice as large as a normal dic‐\ntionary due to the extra linked list that’s created. Thus, if you are going to build a data\nstructure involving a large number of OrderedDict instances (e.g., reading 100,000 lines\nof a CSV file into a list of OrderedDict instances), you would need to study the re‐\nquirements of your application to determine if the benefits of using an OrderedDict\noutweighed the extra memory overhead.\n1.8. Calculating with Dictionaries\nProblem\nYou want to perform various calculations (e.g., minimum value, maximum value, sort‐\ning, etc.) on a dictionary of data.\n1.8. Calculating with Dictionaries \n| \n13",
      "content_length": 1717,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": 1,
      "content": "Solution\nConsider a dictionary that maps stock names to prices:\nprices = {\n   'ACME': 45.23,\n   'AAPL': 612.78,\n   'IBM': 205.55,\n   'HPQ': 37.20,\n   'FB': 10.75\n}\nIn order to perform useful calculations on the dictionary contents, it is often useful to\ninvert the keys and values of the dictionary using zip(). For example, here is how to\nfind the minimum and maximum price and stock name:\nmin_price = min(zip(prices.values(), prices.keys()))\n# min_price is (10.75, 'FB')\nmax_price = max(zip(prices.values(), prices.keys()))\n# max_price is (612.78, 'AAPL')\nSimilarly, to rank the data, use zip() with sorted(), as in the following:\nprices_sorted = sorted(zip(prices.values(), prices.keys()))\n# prices_sorted is [(10.75, 'FB'), (37.2, 'HPQ'),\n#                   (45.23, 'ACME'), (205.55, 'IBM'),\n#                   (612.78, 'AAPL')]\nWhen doing these calculations, be aware that zip() creates an iterator that can only be\nconsumed once. For example, the following code is an error:\nprices_and_names = zip(prices.values(), prices.keys())\nprint(min(prices_and_names))   # OK\nprint(max(prices_and_names))   # ValueError: max() arg is an empty sequence\nDiscussion\nIf you try to perform common data reductions on a dictionary, you’ll find that they only\nprocess the keys, not the values. For example:\nmin(prices)   # Returns 'AAPL'\nmax(prices)   # Returns 'IBM'\nThis is probably not what you want because you’re actually trying to perform a calcu‐\nlation involving the dictionary values. You might try to fix this using the values()\nmethod of a dictionary:\nmin(prices.values())  # Returns 10.75\nmax(prices.values())  # Returns 612.78\n14 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": 1,
      "content": "Unfortunately, this is often not exactly what you want either. For example, you may want\nto know information about the corresponding keys (e.g., which stock has the lowest\nprice?).\nYou can get the key corresponding to the min or max value if you supply a key function\nto min() and max(). For example:\nmin(prices, key=lambda k: prices[k])  # Returns 'FB'\nmax(prices, key=lambda k: prices[k])  # Returns 'AAPL'\nHowever, to get the minimum value, you’ll need to perform an extra lookup step. For\nexample:\nmin_value = prices[min(prices, key=lambda k: prices[k])]\nThe solution involving zip() solves the problem by “inverting” the dictionary into a\nsequence of (value, key) pairs. When performing comparisons on such tuples, the\nvalue element is compared first, followed by the key. This gives you exactly the behavior\nthat you want and allows reductions and sorting to be easily performed on the dictionary\ncontents using a single statement.\nIt should be noted that in calculations involving (value, key) pairs, the key will be\nused to determine the result in instances where multiple entries happen to have the same\nvalue. For instance, in calculations such as min() and max(), the entry with the smallest\nor largest key will be returned if there happen to be duplicate values. For example:\n>>> prices = { 'AAA' : 45.23, 'ZZZ': 45.23 }\n>>> min(zip(prices.values(), prices.keys()))\n(45.23, 'AAA')\n>>> max(zip(prices.values(), prices.keys()))\n(45.23, 'ZZZ')\n>>>\n1.9. Finding Commonalities in Two Dictionaries\nProblem\nYou have two dictionaries and want to find out what they might have in common (same\nkeys, same values, etc.).\nSolution\nConsider two dictionaries:\na = {\n   'x' : 1,\n   'y' : 2,\n   'z' : 3\n}\n1.9. Finding Commonalities in Two Dictionaries \n| \n15",
      "content_length": 1754,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": 1,
      "content": "b = {\n   'w' : 10,\n   'x' : 11,\n   'y' : 2\n}\nTo find out what the two dictionaries have in common, simply perform common set\noperations using the keys() or items() methods. For example:\n# Find keys in common\na.keys() & b.keys()   # { 'x', 'y' }\n# Find keys in a that are not in b\na.keys() - b.keys()   # { 'z' }\n# Find (key,value) pairs in common\na.items() & b.items() # { ('y', 2) }\nThese kinds of operations can also be used to alter or filter dictionary contents. For\nexample, suppose you want to make a new dictionary with selected keys removed. Here\nis some sample code using a dictionary comprehension:\n# Make a new dictionary with certain keys removed\nc = {key:a[key] for key in a.keys() - {'z', 'w'}}\n# c is {'x': 1, 'y': 2}\nDiscussion\nA dictionary is a mapping between a set of keys and values. The keys() method of a\ndictionary returns a keys-view object that exposes the keys. A little-known feature of\nkeys views is that they also support common set operations such as unions, intersections,\nand differences. Thus, if you need to perform common set operations with dictionary\nkeys, you can often just use the keys-view objects directly without first converting them\ninto a set.\nThe items() method of a dictionary returns an items-view object consisting of (key,\nvalue) pairs. This object supports similar set operations and can be used to perform\noperations such as finding out which key-value pairs two dictionaries have in common.\nAlthough similar, the values() method of a dictionary does not support the set oper‐\nations described in this recipe. In part, this is due to the fact that unlike keys, the items\ncontained in a values view aren’t guaranteed to be unique. This alone makes certain set\noperations of questionable utility. However, if you must perform such calculations, they\ncan be accomplished by simply converting the values to a set first.\n16 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": 1,
      "content": "1.10. Removing Duplicates from a Sequence while\nMaintaining Order\nProblem\nYou want to eliminate the duplicate values in a sequence, but preserve the order of the\nremaining items.\nSolution\nIf the values in the sequence are hashable, the problem can be easily solved using a set\nand a generator. For example:\ndef dedupe(items):\n    seen = set()\n    for item in items:\n        if item not in seen:\n            yield item\n            seen.add(item)\nHere is an example of how to use your function:\n>>> a = [1, 5, 2, 1, 9, 1, 5, 10]\n>>> list(dedupe(a))\n[1, 5, 2, 9, 10]\n>>>\nThis only works if the items in the sequence are hashable. If you are trying to eliminate\nduplicates in a sequence of unhashable types (such as dicts), you can make a slight\nchange to this recipe, as follows:\ndef dedupe(items, key=None):\n    seen = set()\n    for item in items:\n        val = item if key is None else key(item)\n        if val not in seen:\n            yield item\n            seen.add(val)\nHere, the purpose of the key argument is to specify a function that converts sequence\nitems into a hashable type for the purposes of duplicate detection. Here’s how it works:\n>>> a = [ {'x':1, 'y':2}, {'x':1, 'y':3}, {'x':1, 'y':2}, {'x':2, 'y':4}]\n>>> list(dedupe(a, key=lambda d: (d['x'],d['y'])))\n[{'x': 1, 'y': 2}, {'x': 1, 'y': 3}, {'x': 2, 'y': 4}]\n>>> list(dedupe(a, key=lambda d: d['x']))\n[{'x': 1, 'y': 2}, {'x': 2, 'y': 4}]\n>>>\nThis latter solution also works nicely if you want to eliminate duplicates based on the\nvalue of a single field or attribute or a larger data structure.\n1.10. Removing Duplicates from a Sequence while Maintaining Order \n| \n17",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": 1,
      "content": "Discussion\nIf all you want to do is eliminate duplicates, it is often easy enough to make a set. For\nexample:\n>>> a\n[1, 5, 2, 1, 9, 1, 5, 10]\n>>> set(a)\n{1, 2, 10, 5, 9}\n>>>\nHowever, this approach doesn’t preserve any kind of ordering. So, the resulting data will\nbe scrambled afterward. The solution shown avoids this.\nThe use of a generator function in this recipe reflects the fact that you might want the\nfunction to be extremely general purpose—not necessarily tied directly to list process‐\ning. For example, if you want to read a file, eliminating duplicate lines, you could simply\ndo this:\nwith open(somefile,'r') as f:\n    for line in dedupe(f):\n        ...\nThe specification of a key function mimics similar functionality in built-in functions\nsuch as sorted(), min(), and max(). For instance, see Recipes 1.8 and 1.13.\n1.11. Naming a Slice\nProblem\nYour program has become an unreadable mess of hardcoded slice indices and you want\nto clean it up.\nSolution\nSuppose you have some code that is pulling specific data fields out of a record string\nwith fixed fields (e.g., from a flat file or similar format):\n######    0123456789012345678901234567890123456789012345678901234567890'\nrecord = '....................100          .......513.25     ..........'\ncost = int(record[20:32]) * float(record[40:48])\nInstead of doing that, why not name the slices like this?\nSHARES = slice(20,32)\nPRICE  = slice(40,48)\ncost = int(record[SHARES]) * float(record[PRICE])\n18 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1511,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": 1,
      "content": "In the latter version, you avoid having a lot of mysterious hardcoded indices, and what\nyou’re doing becomes much clearer.\nDiscussion\nAs a general rule, writing code with a lot of hardcoded index values leads to a readability\nand maintenance mess. For example, if you come back to the code a year later, you’ll\nlook at it and wonder what you were thinking when you wrote it. The solution shown\nis simply a way of more clearly stating what your code is actually doing.\nIn general, the built-in slice() creates a slice object that can be used anywhere a slice\nis allowed. For example:\n>>> items = [0, 1, 2, 3, 4, 5, 6]\n>>> a = slice(2, 4)\n>>> items[2:4]\n[2, 3]\n>>> items[a]\n[2, 3]\n>>> items[a] = [10,11]\n>>> items\n[0, 1, 10, 11, 4, 5, 6]\n>>> del items[a]\n>>> items\n[0, 1, 4, 5, 6]\nIf you have a slice instance s, you can get more information about it by looking at its\ns.start, s.stop, and s.step attributes, respectively. For example:\n>>> a = slice(10, 50, 2)\n>>> a.start\n10\n>>> a.stop\n50\n>>> a.step\n2\n>>>\nIn addition, you can map a slice onto a sequence of a specific size by using its indi\nces(size) method. This returns a tuple (start, stop, step) where all values have\nbeen suitably limited to fit within bounds (as to avoid IndexError exceptions when\nindexing). For example:\n>>> s = 'HelloWorld'\n>>> a.indices(len(s))\n(5, 10, 2)\n>>> for i in range(*a.indices(len(s))):\n...     print(s[i])\n...\nW\nr\n1.11. Naming a Slice \n| \n19",
      "content_length": 1428,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": 1,
      "content": "d\n>>>\n1.12. Determining the Most Frequently Occurring Items in\na Sequence\nProblem\nYou have a sequence of items, and you’d like to determine the most frequently occurring\nitems in the sequence.\nSolution\nThe collections.Counter class is designed for just such a problem. It even comes with\na handy most_common() method that will give you the answer.\nTo illustrate, let’s say you have a list of words and you want to find out which words\noccur most often. Here’s how you would do it:\nwords = [\n   'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes',\n   'the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the',\n   'eyes', \"don't\", 'look', 'around', 'the', 'eyes', 'look', 'into',\n   'my', 'eyes', \"you're\", 'under'\n]\nfrom collections import Counter\nword_counts = Counter(words)\ntop_three = word_counts.most_common(3)\nprint(top_three)\n# Outputs [('eyes', 8), ('the', 5), ('look', 4)]\nDiscussion\nAs input, Counter objects can be fed any sequence of hashable input items. Under the\ncovers, a Counter is a dictionary that maps the items to the number of occurrences. For\nexample:\n>>> word_counts['not']\n1\n>>> word_counts['eyes']\n8\n>>>\nIf you want to increment the count manually, simply use addition:\n>>> morewords = ['why','are','you','not','looking','in','my','eyes']\n>>> for word in morewords:\n...     word_counts[word] += 1\n20 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": 1,
      "content": "...\n>>> word_counts['eyes']\n9\n>>>\nOr, alternatively, you could use the update() method:\n>>> word_counts.update(morewords)\n>>>\nA little-known feature of Counter instances is that they can be easily combined using\nvarious mathematical operations. For example:\n>>> a = Counter(words)\n>>> b = Counter(morewords)\n>>> a\nCounter({'eyes': 8, 'the': 5, 'look': 4, 'into': 3, 'my': 3, 'around': 2,\n         \"you're\": 1, \"don't\": 1, 'under': 1, 'not': 1})\n>>> b\nCounter({'eyes': 1, 'looking': 1, 'are': 1, 'in': 1, 'not': 1, 'you': 1,\n         'my': 1, 'why': 1})\n>>> # Combine counts\n>>> c = a + b\n>>> c\nCounter({'eyes': 9, 'the': 5, 'look': 4, 'my': 4, 'into': 3, 'not': 2,\n         'around': 2, \"you're\": 1, \"don't\": 1, 'in': 1, 'why': 1,\n         'looking': 1, 'are': 1, 'under': 1, 'you': 1})\n>>> # Subtract counts\n>>> d = a - b\n>>> d\nCounter({'eyes': 7, 'the': 5, 'look': 4, 'into': 3, 'my': 2, 'around': 2,\n         \"you're\": 1, \"don't\": 1, 'under': 1})\n>>>\nNeedless to say, Counter objects are a tremendously useful tool for almost any kind of\nproblem where you need to tabulate and count data. You should prefer this over man‐\nually written solutions involving dictionaries.\n1.13. Sorting a List of Dictionaries by a Common Key\nProblem\nYou have a list of dictionaries and you would like to sort the entries according to one\nor more of the dictionary values.\n1.13. Sorting a List of Dictionaries by a Common Key \n| \n21",
      "content_length": 1415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": 1,
      "content": "Solution\nSorting this type of structure is easy using the operator module’s itemgetter function.\nLet’s say you’ve queried a database table to get a listing of the members on your website,\nand you receive the following data structure in return:\nrows = [\n    {'fname': 'Brian', 'lname': 'Jones', 'uid': 1003},\n    {'fname': 'David', 'lname': 'Beazley', 'uid': 1002},\n    {'fname': 'John', 'lname': 'Cleese', 'uid': 1001},\n    {'fname': 'Big', 'lname': 'Jones', 'uid': 1004}\n]\nIt’s fairly easy to output these rows ordered by any of the fields common to all of the\ndictionaries. For example:\nfrom operator import itemgetter\nrows_by_fname = sorted(rows, key=itemgetter('fname'))\nrows_by_uid = sorted(rows, key=itemgetter('uid'))\nprint(rows_by_fname)\nprint(rows_by_uid)\nThe preceding code would output the following:\n[{'fname': 'Big', 'uid': 1004, 'lname': 'Jones'},\n {'fname': 'Brian', 'uid': 1003, 'lname': 'Jones'},\n {'fname': 'David', 'uid': 1002, 'lname': 'Beazley'},\n {'fname': 'John', 'uid': 1001, 'lname': 'Cleese'}]\n[{'fname': 'John', 'uid': 1001, 'lname': 'Cleese'},\n {'fname': 'David', 'uid': 1002, 'lname': 'Beazley'},\n {'fname': 'Brian', 'uid': 1003, 'lname': 'Jones'},\n {'fname': 'Big', 'uid': 1004, 'lname': 'Jones'}]\nThe itemgetter() function can also accept multiple keys. For example, this code\nrows_by_lfname = sorted(rows, key=itemgetter('lname','fname'))\nprint(rows_by_lfname)\nProduces output like this:\n[{'fname': 'David', 'uid': 1002, 'lname': 'Beazley'},\n {'fname': 'John', 'uid': 1001, 'lname': 'Cleese'},\n {'fname': 'Big', 'uid': 1004, 'lname': 'Jones'},\n {'fname': 'Brian', 'uid': 1003, 'lname': 'Jones'}]\nDiscussion\nIn this example, rows is passed to the built-in sorted() function, which accepts a key‐\nword argument key. This argument is expected to be a callable that accepts a single item\n22 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1864,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": 1,
      "content": "from rows as input and returns a value that will be used as the basis for sorting. The\nitemgetter() function creates just such a callable.\nThe operator.itemgetter() function takes as arguments the lookup indices used to\nextract the desired values from the records in rows. It can be a dictionary key name, a\nnumeric list element, or any value that can be fed to an object’s __getitem__() method.\nIf you give multiple indices to itemgetter(), the callable it produces will return a tuple\nwith all of the elements in it, and sorted() will order the output according to the sorted\norder of the tuples. This can be useful if you want to simultaneously sort on multiple\nfields (such as last and first name, as shown in the example).\nThe functionality of itemgetter() is sometimes replaced by lambda expressions. For\nexample:\nrows_by_fname = sorted(rows, key=lambda r: r['fname'])\nrows_by_lfname = sorted(rows, key=lambda r: (r['lname'],r['fname']))\nThis solution often works just fine. However, the solution involving itemgetter()\ntypically runs a bit faster. Thus, you might prefer it if performance is a concern.\nLast, but not least, don’t forget that the technique shown in this recipe can be applied\nto functions such as min() and max(). For example:\n>>> min(rows, key=itemgetter('uid'))\n{'fname': 'John', 'lname': 'Cleese', 'uid': 1001}\n>>> max(rows, key=itemgetter('uid'))\n{'fname': 'Big', 'lname': 'Jones', 'uid': 1004}\n>>>\n1.14. Sorting Objects Without Native Comparison Support\nProblem\nYou want to sort objects of the same class, but they don’t natively support comparison\noperations.\nSolution\nThe built-in sorted() function takes a key argument that can be passed a callable that\nwill return some value in the object that sorted will use to compare the objects. For\nexample, if you have a sequence of User instances in your application, and you want to\nsort them by their user_id attribute, you would supply a callable that takes a User\ninstance as input and returns the user_id. For example:\n>>> class User:\n...     def __init__(self, user_id):\n...         self.user_id = user_id\n1.14. Sorting Objects Without Native Comparison Support \n| \n23",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": 1,
      "content": "...     def __repr__(self):\n...         return 'User({})'.format(self.user_id)\n...\n>>> users = [User(23), User(3), User(99)]\n>>> users\n[User(23), User(3), User(99)]\n>>> sorted(users, key=lambda u: u.user_id)\n[User(3), User(23), User(99)]\n>>>\nInstead of using lambda, an alternative approach is to use operator.attrgetter():\n>>> from operator import attrgetter\n>>> sorted(users, key=attrgetter('user_id'))\n[User(3), User(23), User(99)]\n>>>\nDiscussion\nThe choice of whether or not to use lambda or attrgetter() may be one of personal\npreference. However, attrgetter() is often a tad bit faster and also has the added\nfeature of allowing multiple fields to be extracted simultaneously. This is analogous to\nthe use of operator.itemgetter() for dictionaries (see Recipe 1.13). For example, if\nUser instances also had a first_name and last_name attribute, you could perform a\nsort like this:\nby_name = sorted(users, key=attrgetter('last_name', 'first_name'))\nIt is also worth noting that the technique used in this recipe can be applied to functions\nsuch as min() and max(). For example:\n>>> min(users, key=attrgetter('user_id')\nUser(3)\n>>> max(users, key=attrgetter('user_id')\nUser(99)\n>>>\n1.15. Grouping Records Together Based on a Field\nProblem\nYou have a sequence of dictionaries or instances and you want to iterate over the data\nin groups based on the value of a particular field, such as date.\nSolution\nThe itertools.groupby() function is particularly useful for grouping data together\nlike this. To illustrate, suppose you have the following list of dictionaries:\n24 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": 1,
      "content": "rows = [\n    {'address': '5412 N CLARK', 'date': '07/01/2012'},\n    {'address': '5148 N CLARK', 'date': '07/04/2012'},\n    {'address': '5800 E 58TH', 'date': '07/02/2012'},\n    {'address': '2122 N CLARK', 'date': '07/03/2012'},\n    {'address': '5645 N RAVENSWOOD', 'date': '07/02/2012'},\n    {'address': '1060 W ADDISON', 'date': '07/02/2012'},\n    {'address': '4801 N BROADWAY', 'date': '07/01/2012'},\n    {'address': '1039 W GRANVILLE', 'date': '07/04/2012'},\n]\nNow suppose you want to iterate over the data in chunks grouped by date. To do it, first\nsort by the desired field (in this case, date) and then use itertools.groupby():\nfrom operator import itemgetter\nfrom itertools import groupby\n# Sort by the desired field first\nrows.sort(key=itemgetter('date'))\n# Iterate in groups\nfor date, items in groupby(rows, key=itemgetter('date')):\n    print(date)\n    for i in items:\n        print('    ', i)\nThis produces the following output:\n    07/01/2012\n         {'date': '07/01/2012', 'address': '5412 N CLARK'}\n         {'date': '07/01/2012', 'address': '4801 N BROADWAY'}\n    07/02/2012\n         {'date': '07/02/2012', 'address': '5800 E 58TH'}\n         {'date': '07/02/2012', 'address': '5645 N RAVENSWOOD'}\n         {'date': '07/02/2012', 'address': '1060 W ADDISON'}\n    07/03/2012\n         {'date': '07/03/2012', 'address': '2122 N CLARK'}\n    07/04/2012\n         {'date': '07/04/2012', 'address': '5148 N CLARK'}\n         {'date': '07/04/2012', 'address': '1039 W GRANVILLE'}\nDiscussion\nThe groupby() function works by scanning a sequence and finding sequential “runs”\nof identical values (or values returned by the given key function). On each iteration, it\nreturns the value along with an iterator that produces all of the items in a group with\nthe same value.\nAn important preliminary step is sorting the data according to the field of interest. Since\ngroupby() only examines consecutive items, failing to sort first won’t group the records\nas you want.\n1.15. Grouping Records Together Based on a Field \n| \n25",
      "content_length": 2020,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": 1,
      "content": "If your goal is to simply group the data together by dates into a large data structure that\nallows random access, you may have better luck using defaultdict() to build a\nmultidict, as described in Recipe 1.6. For example:\nfrom collections import defaultdict\nrows_by_date = defaultdict(list)\nfor row in rows:\n    rows_by_date[row['date']].append(row)\nThis allows the records for each date to be accessed easily like this:\n>>> for r in rows_by_date['07/01/2012']:\n...     print(r)\n...\n{'date': '07/01/2012', 'address': '5412 N CLARK'}\n{'date': '07/01/2012', 'address': '4801 N BROADWAY'}\n>>>\nFor this latter example, it’s not necessary to sort the records first. Thus, if memory is no\nconcern, it may be faster to do this than to first sort the records and iterate using\ngroupby(). \n1.16. Filtering Sequence Elements\nProblem\nYou have data inside of a sequence, and need to extract values or reduce the sequence\nusing some criteria.\nSolution\nThe easiest way to filter sequence data is often to use a list comprehension. For example:\n>>> mylist = [1, 4, -5, 10, -7, 2, 3, -1]\n>>> [n for n in mylist if n > 0]\n[1, 4, 10, 2, 3]\n>>> [n for n in mylist if n < 0]\n[-5, -7, -1]\n>>>\nOne potential downside of using a list comprehension is that it might produce a large\nresult if the original input is large. If this is a concern, you can use generator expressions\nto produce the filtered values iteratively. For example:\n>>> pos = (n for n in mylist if n > 0)\n>>> pos\n<generator object <genexpr> at 0x1006a0eb0>\n>>> for x in pos:\n...     print(x)\n...\n26 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": 1,
      "content": "1\n4\n10\n2\n3\n>>>\nSometimes, the filtering criteria cannot be easily expressed in a list comprehension or\ngenerator expression. For example, suppose that the filtering process involves exception\nhandling or some other complicated detail. For this, put the filtering code into its own\nfunction and use the built-in filter() function. For example:\nvalues = ['1', '2', '-3', '-', '4', 'N/A', '5']\ndef is_int(val):\n    try:\n        x = int(val)\n        return True\n    except ValueError:\n        return False\nivals = list(filter(is_int, values))\nprint(ivals)\n# Outputs ['1', '2', '-3', '4', '5']\nfilter() creates an iterator, so if you want to create a list of results, make sure you also\nuse list() as shown.\nDiscussion\nList comprehensions and generator expressions are often the easiest and most straight‐\nforward ways to filter simple data. They also have the added power to transform the\ndata at the same time. For example:\n>>> mylist = [1, 4, -5, 10, -7, 2, 3, -1]\n>>> import math\n>>> [math.sqrt(n) for n in mylist if n > 0]\n[1.0, 2.0, 3.1622776601683795, 1.4142135623730951, 1.7320508075688772]\n>>>\nOne variation on filtering involves replacing the values that don’t meet the criteria with\na new value instead of discarding them. For example, perhaps instead of just finding\npositive values, you want to also clip bad values to fit within a specified range. This is\noften easily accomplished by moving the filter criterion into a conditional expression\nlike this:\n>>> clip_neg = [n if n > 0 else 0 for n in mylist]\n>>> clip_neg\n[1, 4, 0, 10, 0, 2, 3, 0]\n>>> clip_pos = [n if n < 0 else 0 for n in mylist]\n>>> clip_pos\n1.16. Filtering Sequence Elements \n| \n27",
      "content_length": 1657,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": 1,
      "content": "[0, 0, -5, 0, -7, 0, 0, -1]\n>>>\nAnother notable filtering tool is itertools.compress(), which takes an iterable and\nan accompanying Boolean selector sequence as input. As output, it gives you all of the\nitems in the iterable where the corresponding element in the selector is True. This can\nbe useful if you’re trying to apply the results of filtering one sequence to another related\nsequence. For example, suppose you have the following two columns of data:\naddresses = [\n    '5412 N CLARK',\n    '5148 N CLARK',\n    '5800 E 58TH',\n    '2122 N CLARK'\n    '5645 N RAVENSWOOD',\n    '1060 W ADDISON',\n    '4801 N BROADWAY',\n    '1039 W GRANVILLE',\n]\ncounts = [ 0, 3, 10, 4, 1, 7, 6, 1]\nNow suppose you want to make a list of all addresses where the corresponding count\nvalue was greater than 5. Here’s how you could do it:\n>>> from itertools import compress\n>>> more5 = [n > 5 for n in counts]\n>>> more5\n[False, False, True, False, False, True, True, False]\n>>> list(compress(addresses, more5))\n['5800 E 58TH', '4801 N BROADWAY', '1039 W GRANVILLE']\n>>>\nThe key here is to first create a sequence of Booleans that indicates which elements\nsatisfy the desired condition. The compress() function then picks out the items corre‐\nsponding to True values.\nLike filter(), compress() normally returns an iterator. Thus, you need to use list()\nto turn the results into a list if desired.\n1.17. Extracting a Subset of a Dictionary\nProblem\nYou want to make a dictionary that is a subset of another dictionary.\n28 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": 1,
      "content": "Solution\nThis is easily accomplished using a dictionary comprehension. For example:\nprices = {\n   'ACME': 45.23,\n   'AAPL': 612.78,\n   'IBM': 205.55,\n   'HPQ': 37.20,\n   'FB': 10.75\n}\n# Make a dictionary of all prices over 200\np1 = { key:value for key, value in prices.items() if value > 200 }\n# Make a dictionary of tech stocks\ntech_names = { 'AAPL', 'IBM', 'HPQ', 'MSFT' }\np2 = { key:value for key,value in prices.items() if key in tech_names }\nDiscussion\nMuch of what can be accomplished with a dictionary comprehension might also be done\nby creating a sequence of tuples and passing them to the dict() function. For example:\np1 = dict((key, value) for key, value in prices.items() if value > 200)\nHowever, the dictionary comprehension solution is a bit clearer and actually runs quite\na bit faster (over twice as fast when tested on the prices dictionary used in the example).\nSometimes there are multiple ways of accomplishing the same thing. For instance, the\nsecond example could be rewritten as:\n# Make a dictionary of tech stocks\ntech_names = { 'AAPL', 'IBM', 'HPQ', 'MSFT' }\np2 = { key:prices[key] for key in prices.keys() & tech_names }\nHowever, a timing study reveals that this solution is almost 1.6 times slower than the\nfirst solution. If performance matters, it usually pays to spend a bit of time studying it.\nSee Recipe 14.13 for specific information about timing and profiling.\n1.18. Mapping Names to Sequence Elements\nProblem\nYou have code that accesses list or tuple elements by position, but this makes the code\nsomewhat difficult to read at times. You’d also like to be less dependent on position in\nthe structure, by accessing the elements by name.\n1.18. Mapping Names to Sequence Elements \n| \n29",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": 1,
      "content": "Solution\ncollections.namedtuple() provides these benefits, while adding minimal overhead\nover using a normal tuple object. collections.namedtuple() is actually a factory\nmethod that returns a subclass of the standard Python tuple type. You feed it a type\nname, and the fields it should have, and it returns a class that you can instantiate, passing\nin values for the fields you’ve defined, and so on. For example:\n>>> from collections import namedtuple\n>>> Subscriber = namedtuple('Subscriber', ['addr', 'joined'])\n>>> sub = Subscriber('jonesy@example.com', '2012-10-19')\n>>> sub\nSubscriber(addr='jonesy@example.com', joined='2012-10-19')\n>>> sub.addr\n'jonesy@example.com'\n>>> sub.joined\n'2012-10-19'\n>>>\nAlthough an instance of a namedtuple looks like a normal class instance, it is inter‐\nchangeable with a tuple and supports all of the usual tuple operations such as indexing\nand unpacking. For example:\n>>> len(sub)\n2\n>>> addr, joined = sub\n>>> addr\n'jonesy@example.com'\n>>> joined\n'2012-10-19'\n>>>\nA major use case for named tuples is decoupling your code from the position of the\nelements it manipulates. So, if you get back a large list of tuples from a database call,\nthen manipulate them by accessing the positional elements, your code could break if,\nsay, you added a new column to your table. Not so if you first cast the returned tuples\nto namedtuples.\nTo illustrate, here is some code using ordinary tuples:\ndef compute_cost(records):\n    total = 0.0\n    for rec in records:\n        total += rec[1] * rec[2]\n    return total\nReferences to positional elements often make the code a bit less expressive and more\ndependent on the structure of the records. Here is a version that uses a namedtuple:\nfrom collections import namedtuple\nStock = namedtuple('Stock', ['name', 'shares', 'price'])\n30 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1848,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": 1,
      "content": "def compute_cost(records):\n    total = 0.0\n    for rec in records:\n        s = Stock(*rec)\n        total += s.shares * s.price\n    return total\nNaturally, you can avoid the explicit conversion to the Stock namedtuple if the records\nsequence in the example already contained such instances.\nDiscussion\nOne possible use of a namedtuple is as a replacement for a dictionary, which requires\nmore space to store. Thus, if you are building large data structures involving dictionaries,\nuse of a namedtuple will be more efficient. However, be aware that unlike a dictionary,\na namedtuple is immutable. For example:\n>>> s = Stock('ACME', 100, 123.45)\n>>> s\nStock(name='ACME', shares=100, price=123.45)\n>>> s.shares = 75\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: can't set attribute\n>>>\nIf you need to change any of the attributes, it can be done using the _replace() method\nof a namedtuple instance, which makes an entirely new namedtuple with specified val‐\nues replaced. For example:\n>>> s = s._replace(shares=75)\n>>> s\nStock(name='ACME', shares=75, price=123.45)\n>>>\nA subtle use of the _replace() method is that it can be a convenient way to populate\nnamed tuples that have optional or missing fields. To do this, you make a prototype\ntuple containing the default values and then use _replace() to create new instances\nwith values replaced. For example:\nfrom collections import namedtuple\nStock = namedtuple('Stock', ['name', 'shares', 'price', 'date', 'time'])\n# Create a prototype instance\nstock_prototype = Stock('', 0, 0.0, None, None)\n# Function to convert a dictionary to a Stock\ndef dict_to_stock(s):\n    return stock_prototype._replace(**s)\n1.18. Mapping Names to Sequence Elements \n| \n31",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": 1,
      "content": "Here is an example of how this code would work:\n>>> a = {'name': 'ACME', 'shares': 100, 'price': 123.45}\n>>> dict_to_stock(a)\nStock(name='ACME', shares=100, price=123.45, date=None, time=None)\n>>> b = {'name': 'ACME', 'shares': 100, 'price': 123.45, 'date': '12/17/2012'}\n>>> dict_to_stock(b)\nStock(name='ACME', shares=100, price=123.45, date='12/17/2012', time=None)\n>>>\nLast, but not least, it should be noted that if your goal is to define an efficient data\nstructure where you will be changing various instance attributes, using namedtuple is\nnot your best choice. Instead, consider defining a class using __slots__ instead (see\nRecipe 8.4).\n1.19. Transforming and Reducing Data at the Same Time\nProblem\nYou need to execute a reduction function (e.g., sum(), min(), max()), but first need to\ntransform or filter the data.\nSolution\nA very elegant way to combine a data reduction and a transformation is to use a \ngenerator-expression argument. For example, if you want to calculate the sum of\nsquares, do the following:\nnums = [1, 2, 3, 4, 5]\ns = sum(x * x for x in nums)\nHere are a few other examples:\n# Determine if any .py files exist in a directory\nimport os\nfiles = os.listdir('dirname')\nif any(name.endswith('.py') for name in files):\n    print('There be python!')\nelse:\n    print('Sorry, no python.')\n# Output a tuple as CSV\ns = ('ACME', 50, 123.45)\nprint(','.join(str(x) for x in s))\n# Data reduction across fields of a data structure\nportfolio = [\n   {'name':'GOOG', 'shares': 50},\n   {'name':'YHOO', 'shares': 75},\n   {'name':'AOL', 'shares': 20},\n32 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": 1,
      "content": "{'name':'SCOX', 'shares': 65}\n]\nmin_shares = min(s['shares'] for s in portfolio)\nDiscussion\nThe solution shows a subtle syntactic aspect of generator expressions when supplied as\nthe single argument to a function (i.e., you don’t need repeated parentheses). For ex‐\nample, these statements are the same:\ns = sum((x * x for x in nums))    # Pass generator-expr as argument\ns = sum(x * x for x in nums)      # More elegant syntax\nUsing a generator argument is often a more efficient and elegant approach than first\ncreating a temporary list. For example, if you didn’t use a generator expression, you\nmight consider this alternative implementation:\nnums = [1, 2, 3, 4, 5]\ns = sum([x * x for x in nums])\nThis works, but it introduces an extra step and creates an extra list. For such a small list,\nit might not matter, but if nums was huge, you would end up creating a large temporary\ndata structure to only be used once and discarded. The generator solution transforms\nthe data iteratively and is therefore much more memory-efficient.\nCertain reduction functions such as min() and max() accept a key argument that might\nbe useful in situations where you might be inclined to use a generator. For example, in\nthe portfolio example, you might consider this alternative:\n# Original: Returns 20\nmin_shares = min(s['shares'] for s in portfolio)\n# Alternative: Returns {'name': 'AOL', 'shares': 20}\nmin_shares = min(portfolio, key=lambda s: s['shares'])\n1.20. Combining Multiple Mappings into a Single\nMapping\nProblem\nYou have multiple dictionaries or mappings that you want to logically combine into a\nsingle mapping to perform certain operations, such as looking up values or checking\nfor the existence of keys.\nSolution\nSuppose you have two dictionaries:\n1.20. Combining Multiple Mappings into a Single Mapping \n| \n33",
      "content_length": 1812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": 1,
      "content": "a = {'x': 1, 'z': 3 }\nb = {'y': 2, 'z': 4 }\nNow suppose you want to perform lookups where you have to check both dictionaries\n(e.g., first checking in a and then in b if not found). An easy way to do this is to use the\nChainMap class from the collections module. For example:\nfrom collections import ChainMap\nc = ChainMap(a,b)\nprint(c['x'])      # Outputs 1  (from a)\nprint(c['y'])      # Outputs 2  (from b)\nprint(c['z'])      # Outputs 3  (from a)\nDiscussion\nA ChainMap takes multiple mappings and makes them logically appear as one. However,\nthe mappings are not literally merged together. Instead, a ChainMap simply keeps a list\nof the underlying mappings and redefines common dictionary operations to scan the\nlist. Most operations will work. For example:\n>>> len(c)\n3\n>>> list(c.keys())\n['x', 'y', 'z']\n>>> list(c.values())\n[1, 2, 3]\n>>>\nIf there are duplicate keys, the values from the first mapping get used. Thus, the entry\nc['z'] in the example would always refer to the value in dictionary a, not the value in\ndictionary b.\nOperations that mutate the mapping always affect the first mapping listed. For example:\n>>> c['z'] = 10\n>>> c['w'] = 40\n>>> del c['x']\n>>> a\n{'w': 40, 'z': 10}\n>>> del c['y']\nTraceback (most recent call last):\n...\nKeyError: \"Key not found in the first mapping: 'y'\"\n>>>\nA ChainMap is particularly useful when working with scoped values such as variables in\na programming language (i.e., globals, locals, etc.). In fact, there are methods that make\nthis easy:\n34 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 1542,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": 1,
      "content": ">>> values = ChainMap()\n>>> values['x'] = 1\n>>> # Add a new mapping\n>>> values = values.new_child()\n>>> values['x'] = 2\n>>> # Add a new mapping\n>>> values = values.new_child()\n>>> values['x'] = 3\n>>> values\nChainMap({'x': 3}, {'x': 2}, {'x': 1})\n>>> values['x']\n3\n>>> # Discard last mapping\n>>> values = values.parents\n>>> values['x']\n2\n>>> # Discard last mapping\n>>> values = values.parents\n>>> values['x']\n1\n>>> values\nChainMap({'x': 1})\n>>>\nAs an alternative to ChainMap, you might consider merging dictionaries together using\nthe update() method. For example:\n>>> a = {'x': 1, 'z': 3 }\n>>> b = {'y': 2, 'z': 4 }\n>>> merged = dict(b)\n>>> merged.update(a)\n>>> merged['x']\n1\n>>> merged['y']\n2\n>>> merged['z']\n3\n>>>\nThis works, but it requires you to make a completely separate dictionary object (or\ndestructively alter one of the existing dictionaries). Also, if any of the original diction‐\naries mutate, the changes don’t get reflected in the merged dictionary. For example:\n>>> a['x'] = 13\n>>> merged['x']\n1\nA ChainMap uses the original dictionaries, so it doesn’t have this behavior. For example:\n1.20. Combining Multiple Mappings into a Single Mapping \n| \n35",
      "content_length": 1164,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": 1,
      "content": ">>> a = {'x': 1, 'z': 3 }\n>>> b = {'y': 2, 'z': 4 }\n>>> merged = ChainMap(a, b)\n>>> merged['x']\n1\n>>> a['x'] = 42\n>>> merged['x']   # Notice change to merged dicts\n42\n>>>\n36 \n| \nChapter 1: Data Structures and Algorithms",
      "content_length": 219,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": 1,
      "content": "CHAPTER 2\nStrings and Text\nAlmost every useful program involves some kind of text processing, whether it is parsing\ndata or generating output. This chapter focuses on common problems involving text\nmanipulation, such as pulling apart strings, searching, substitution, lexing, and parsing.\nMany of these tasks can be easily solved using built-in methods of strings. However,\nmore complicated operations might require the use of regular expressions or the cre‐\nation of a full-fledged parser. All of these topics are covered. In addition, a few tricky\naspects of working with Unicode are addressed.\n2.1. Splitting Strings on Any of Multiple Delimiters\nProblem\nYou need to split a string into fields, but the delimiters (and spacing around them) aren’t\nconsistent throughout the string.\nSolution\nThe split() method of string objects is really meant for very simple cases, and does\nnot allow for multiple delimiters or account for possible whitespace around the delim‐\niters. In cases when you need a bit more flexibility, use the re.split() method:\n>>> line = 'asdf fjdk; afed, fjek,asdf,      foo'\n>>> import re\n>>> re.split(r'[;,\\s]\\s*', line)\n['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']\nDiscussion\nThe re.split() function is useful because you can specify multiple patterns for the\nseparator. For example, as shown in the solution, the separator is either a comma (,),\n37",
      "content_length": 1375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": 1,
      "content": "semicolon (;), or whitespace followed by any amount of extra whitespace. Whenever\nthat pattern is found, the entire match becomes the delimiter between whatever fields\nlie on either side of the match. The result is a list of fields, just as with str.split().\nWhen using re.split(), you need to be a bit careful should the regular expression\npattern involve a capture group enclosed in parentheses. If capture groups are used,\nthen the matched text is also included in the result. For example, watch what happens\nhere:\n>>> fields = re.split(r'(;|,|\\s)\\s*', line)\n>>> fields\n['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']\n>>>\nGetting the split characters might be useful in certain contexts. For example, maybe you\nneed the split characters later on to reform an output string:\n>>> values = fields[::2]\n>>> delimiters = fields[1::2] + ['']\n>>> values\n['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']\n>>> delimiters\n[' ', ';', ',', ',', ',', '']\n>>> # Reform the line using the same delimiters\n>>> ''.join(v+d for v,d in zip(values, delimiters))\n'asdf fjdk;afed,fjek,asdf,foo'\n>>>\nIf you don’t want the separator characters in the result, but still need to use parentheses\nto group parts of the regular expression pattern, make sure you use a noncapture group,\nspecified as (?:...). For example:\n>>> re.split(r'(?:,|;|\\s)\\s*', line)\n['asdf', 'fjdk', 'afed', 'fjek', 'asdf', 'foo']\n>>>\n2.2. Matching Text at the Start or End of a String\nProblem\nYou need to check the start or end of a string for specific text patterns, such as filename\nextensions, URL schemes, and so on.\nSolution\nA simple way to check the beginning or end of a string is to use the str.starts\nwith() or str.endswith() methods. For example:\n38 \n| \nChapter 2: Strings and Text",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": 1,
      "content": ">>> filename = 'spam.txt'\n>>> filename.endswith('.txt')\nTrue\n>>> filename.startswith('file:')\nFalse\n>>> url = 'http://www.python.org'\n>>> url.startswith('http:')\nTrue\n>>>\nIf you need to check against multiple choices, simply provide a tuple of possibilities to\nstartswith() or endswith():\n>>> import os\n>>> filenames = os.listdir('.')\n>>> filenames\n[ 'Makefile', 'foo.c', 'bar.py', 'spam.c', 'spam.h' ]\n>>> [name for name in filenames if name.endswith(('.c', '.h')) ]\n['foo.c', 'spam.c', 'spam.h'\n>>> any(name.endswith('.py') for name in filenames)\nTrue\n>>>\nHere is another example:\nfrom urllib.request import urlopen\ndef read_data(name):\n    if name.startswith(('http:', 'https:', 'ftp:')):\n        return urlopen(name).read()\n    else:\n        with open(name) as f:\n             return f.read()\nOddly, this is one part of Python where a tuple is actually required as input. If you happen\nto have the choices specified in a list or set, just make sure you convert them using\ntuple() first. For example:\n>>> choices = ['http:', 'ftp:']\n>>> url = 'http://www.python.org'\n>>> url.startswith(choices)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: startswith first arg must be str or a tuple of str, not list\n>>> url.startswith(tuple(choices))\nTrue\n>>>\n2.2. Matching Text at the Start or End of a String \n| \n39",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": 1,
      "content": "Discussion\nThe startswith() and endswith() methods provide a very convenient way to perform\nbasic prefix and suffix checking. Similar operations can be performed with slices, but\nare far less elegant. For example:\n>>> filename = 'spam.txt'\n>>> filename[-4:] == '.txt'\nTrue\n>>> url = 'http://www.python.org'\n>>> url[:5] == 'http:' or url[:6] == 'https:' or url[:4] == 'ftp:'\nTrue\n>>>\nYou might also be inclined to use regular expressions as an alternative. For example:\n>>> import re\n>>> url = 'http://www.python.org'\n>>> re.match('http:|https:|ftp:', url)\n<_sre.SRE_Match object at 0x101253098>\n>>>\nThis works, but is often overkill for simple matching. Using this recipe is simpler and\nruns faster.\nLast, but not least, the startswith() and endswith() methods look nice when com‐\nbined with other operations, such as common data reductions. For example, this state‐\nment that checks a directory for the presence of certain kinds of files:\nif any(name.endswith(('.c', '.h')) for name in listdir(dirname)):\n   ...\n2.3. Matching Strings Using Shell Wildcard Patterns\nProblem\nYou want to match text using the same wildcard patterns as are commonly used when\nworking in Unix shells (e.g., *.py, Dat[0-9]*.csv, etc.).\nSolution\nThe fnmatch module provides two functions—fnmatch() and fnmatchcase()—that\ncan be used to perform such matching. The usage is simple:\n>>> from fnmatch import fnmatch, fnmatchcase\n>>> fnmatch('foo.txt', '*.txt')\nTrue\n>>> fnmatch('foo.txt', '?oo.txt')\nTrue\n>>> fnmatch('Dat45.csv', 'Dat[0-9]*')\n40 \n| \nChapter 2: Strings and Text",
      "content_length": 1549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": 1,
      "content": "True\n>>> names = ['Dat1.csv', 'Dat2.csv', 'config.ini', 'foo.py']\n>>> [name for name in names if fnmatch(name, 'Dat*.csv')]\n['Dat1.csv', 'Dat2.csv']\n>>>\nNormally, fnmatch() matches patterns using the same case-sensitivity rules as the sys‐\ntem’s underlying filesystem (which varies based on operating system). For example:\n>>> # On OS X (Mac)\n>>> fnmatch('foo.txt', '*.TXT')\nFalse\n>>> # On Windows\n>>> fnmatch('foo.txt', '*.TXT')\nTrue\n>>>\nIf this distinction matters, use fnmatchcase() instead. It matches exactly based on the\nlower- and uppercase conventions that you supply:\n>>> fnmatchcase('foo.txt', '*.TXT')\nFalse\n>>>\nAn often overlooked feature of these functions is their potential use with data processing\nof nonfilename strings. For example, suppose you have a list of street addresses like this:\naddresses = [\n    '5412 N CLARK ST',\n    '1060 W ADDISON ST',\n    '1039 W GRANVILLE AVE',\n    '2122 N CLARK ST',\n    '4802 N BROADWAY',\n]\nYou could write list comprehensions like this:\n>>> from fnmatch import fnmatchcase\n>>> [addr for addr in addresses if fnmatchcase(addr, '* ST')]\n['5412 N CLARK ST', '1060 W ADDISON ST', '2122 N CLARK ST']\n>>> [addr for addr in addresses if fnmatchcase(addr, '54[0-9][0-9] *CLARK*')]\n['5412 N CLARK ST']\n>>>\nDiscussion\nThe matching performed by fnmatch sits somewhere between the functionality of sim‐\nple string methods and the full power of regular expressions. If you’re just trying to\nprovide a simple mechanism for allowing wildcards in data processing operations, it’s\noften a reasonable solution.\n2.3. Matching Strings Using Shell Wildcard Patterns \n| \n41",
      "content_length": 1605,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": 1,
      "content": "If you’re actually trying to write code that matches filenames, use the glob module\ninstead. See Recipe 5.13.\n2.4. Matching and Searching for Text Patterns\nProblem\nYou want to match or search text for a specific pattern.\nSolution\nIf the text you’re trying to match is a simple literal, you can often just use the basic string\nmethods, such as str.find(), str.endswith(), str.startswith(), or similar. For\nexample:\n>>> text = 'yeah, but no, but yeah, but no, but yeah'\n>>> # Exact match\n>>> text == 'yeah'\nFalse\n>>> # Match at start or end\n>>> text.startswith('yeah')\nTrue\n>>> text.endswith('no')\nFalse\n>>> # Search for the location of the first occurrence\n>>> text.find('no')\n10\n>>>\nFor more complicated matching, use regular expressions and the re module. To illus‐\ntrate the basic mechanics of using regular expressions, suppose you want to match dates\nspecified as digits, such as “11/27/2012.” Here is a sample of how you would do it:\n>>> text1 = '11/27/2012'\n>>> text2 = 'Nov 27, 2012'\n>>>\n>>> import re\n>>> # Simple matching: \\d+ means match one or more digits\n>>> if re.match(r'\\d+/\\d+/\\d+', text1):\n...     print('yes')\n... else:\n...     print('no')\n...\nyes\n>>> if re.match(r'\\d+/\\d+/\\d+', text2):\n...     print('yes')\n... else:\n42 \n| \nChapter 2: Strings and Text",
      "content_length": 1271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": 2,
      "content": "...     print('no')\n...\nno\n>>>\nIf you’re going to perform a lot of matches using the same pattern, it usually pays to\nprecompile the regular expression pattern into a pattern object first. For example:\n>>> datepat = re.compile(r'\\d+/\\d+/\\d+')\n>>> if datepat.match(text1):\n...     print('yes')\n... else:\n...     print('no')\n...\nyes\n>>> if datepat.match(text2):\n...     print('yes')\n... else:\n...     print('no')\n...\nno\n>>>\nmatch() always tries to find the match at the start of a string. If you want to search text\nfor all occurrences of a pattern, use the findall() method instead. For example:\n>>> text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'\n>>> datepat.findall(text)\n['11/27/2012', '3/13/2013']\n>>>\nWhen defining regular expressions, it is common to introduce capture groups by en‐\nclosing parts of the pattern in parentheses. For example:\n>>> datepat = re.compile(r'(\\d+)/(\\d+)/(\\d+)')\n>>>\nCapture groups often simplify subsequent processing of the matched text because the\ncontents of each group can be extracted individually. For example:\n>>> m = datepat.match('11/27/2012')\n>>> m\n<_sre.SRE_Match object at 0x1005d2750>\n>>> # Extract the contents of each group\n>>> m.group(0)\n'11/27/2012'\n>>> m.group(1)\n'11'\n>>> m.group(2)\n'27'\n>>> m.group(3)\n'2012'\n>>> m.groups()\n2.4. Matching and Searching for Text Patterns \n| \n43",
      "content_length": 1333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": 2,
      "content": "('11', '27', '2012')\n>>> month, day, year = m.groups()\n>>>\n>>> # Find all matches (notice splitting into tuples)\n>>> text\n'Today is 11/27/2012. PyCon starts 3/13/2013.'\n>>> datepat.findall(text)\n[('11', '27', '2012'), ('3', '13', '2013')]\n>>> for month, day, year in datepat.findall(text):\n...     print('{}-{}-{}'.format(year, month, day))\n...\n2012-11-27\n2013-3-13\n>>>\nThe findall() method searches the text and finds all matches, returning them as a list.\nIf you want to find matches iteratively, use the finditer() method instead. For example:\n>>> for m in datepat.finditer(text):\n...     print(m.groups())\n...\n('11', '27', '2012')\n('3', '13', '2013')\n>>>\nDiscussion\nA basic tutorial on the theory of regular expressions is beyond the scope of this book.\nHowever, this recipe illustrates the absolute basics of using the re module to match and\nsearch for text. The essential functionality is first compiling a pattern using\nre.compile() and then using methods such as match(), findall(), or finditer().\nWhen specifying patterns, it is relatively common to use raw strings such as\nr'(\\d+)/(\\d+)/(\\d+)'. Such strings leave the backslash character uninterpreted,\nwhich can be useful in the context of regular expressions. Otherwise, you need to use\ndouble backslashes such as '(\\\\d+)/(\\\\d+)/(\\\\d+)'.\nBe aware that the match() method only checks the beginning of a string. It’s possible\nthat it will match things you aren’t expecting. For example:\n>>> m = datepat.match('11/27/2012abcdef')\n>>> m\n<_sre.SRE_Match object at 0x1005d27e8>\n>>> m.group()\n'11/27/2012'\n>>>\nIf you want an exact match, make sure the pattern includes the end-marker ($), as in\nthe following:\n44 \n| \nChapter 2: Strings and Text",
      "content_length": 1699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": 2,
      "content": ">>> datepat = re.compile(r'(\\d+)/(\\d+)/(\\d+)$')\n>>> datepat.match('11/27/2012abcdef')\n>>> datepat.match('11/27/2012')\n<_sre.SRE_Match object at 0x1005d2750>\n>>>\nLast, if you’re just doing a simple text matching/searching operation, you can often skip\nthe compilation step and use module-level functions in the re module instead. For\nexample:\n>>> re.findall(r'(\\d+)/(\\d+)/(\\d+)', text)\n[('11', '27', '2012'), ('3', '13', '2013')]\n>>>\nBe aware, though, that if you’re going to perform a lot of matching or searching, it usually\npays to compile the pattern first and use it over and over again. The module-level func‐\ntions keep a cache of recently compiled patterns, so there isn’t a huge performance hit,\nbut you’ll save a few lookups and extra processing by using your own compiled pattern.\n2.5. Searching and Replacing Text\nProblem\nYou want to search for and replace a text pattern in a string.\nSolution\nFor simple literal patterns, use the str.replace() method. For example:\n>>> text = 'yeah, but no, but yeah, but no, but yeah'\n>>> text.replace('yeah', 'yep')\n'yep, but no, but yep, but no, but yep'\n>>>\nFor more complicated patterns, use the sub() functions/methods in the re module. To\nillustrate, suppose you want to rewrite dates of the form “11/27/2012” as “2012-11-27.”\nHere is a sample of how to do it:\n>>> text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'\n>>> import re\n>>> re.sub(r'(\\d+)/(\\d+)/(\\d+)', r'\\3-\\1-\\2', text)\n'Today is 2012-11-27. PyCon starts 2013-3-13.'\n>>>\nThe first argument to sub() is the pattern to match and the second argument is the\nreplacement pattern. Backslashed digits such as \\3 refer to capture group numbers in\nthe pattern.\n2.5. Searching and Replacing Text \n| \n45",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": 2,
      "content": "If you’re going to perform repeated substitutions of the same pattern, consider compil‐\ning it first for better performance. For example:\n>>> import re\n>>> datepat = re.compile(r'(\\d+)/(\\d+)/(\\d+)')\n>>> datepat.sub(r'\\3-\\1-\\2', text)\n'Today is 2012-11-27. PyCon starts 2013-3-13.'\n>>>\nFor more complicated substitutions, it’s possible to specify a substitution callback func‐\ntion instead. For example:\n>>> from calendar import month_abbr\n>>> def change_date(m):\n...     mon_name = month_abbr[int(m.group(1))]\n...     return '{} {} {}'.format(m.group(2), mon_name, m.group(3))\n...\n>>> datepat.sub(change_date, text)\n'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.'\n>>>\nAs input, the argument to the substitution callback is a match object, as returned by \nmatch() or find(). Use the .group() method to extract specific parts of the match. The\nfunction should return the replacement text.\nIf you want to know how many substitutions were made in addition to getting the\nreplacement text, use re.subn() instead. For example:\n>>> newtext, n = datepat.subn(r'\\3-\\1-\\2', text)\n>>> newtext\n'Today is 2012-11-27. PyCon starts 2013-3-13.'\n>>> n\n2\n>>>\nDiscussion\nThere isn’t much more to regular expression search and replace than the sub() method\nshown. The trickiest part is specifying the regular expression pattern—something that’s\nbest left as an exercise to the reader. \n2.6. Searching and Replacing Case-Insensitive Text\nProblem\nYou need to search for and possibly replace text in a case-insensitive manner.\n46 \n| \nChapter 2: Strings and Text",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": 2,
      "content": "Solution\nTo perform case-insensitive text operations, you need to use the re module and supply\nthe re.IGNORECASE flag to various operations. For example:\n>>> text = 'UPPER PYTHON, lower python, Mixed Python'\n>>> re.findall('python', text, flags=re.IGNORECASE)\n['PYTHON', 'python', 'Python']\n>>> re.sub('python', 'snake', text, flags=re.IGNORECASE)\n'UPPER snake, lower snake, Mixed snake'\n>>>\nThe last example reveals a limitation that replacing text won’t match the case of the\nmatched text. If you need to fix this, you might have to use a support function, as in the\nfollowing:\ndef matchcase(word):\n    def replace(m):\n        text = m.group()\n        if text.isupper():\n            return word.upper()\n        elif text.islower():\n            return word.lower()\n        elif text[0].isupper():\n            return word.capitalize()\n        else:\n            return word\n    return replace\nHere is an example of using this last function:\n>>> re.sub('python', matchcase('snake'), text, flags=re.IGNORECASE)\n'UPPER SNAKE, lower snake, Mixed Snake'\n>>>\nDiscussion\nFor simple cases, simply providing the re.IGNORECASE is enough to perform case-\ninsensitive matching. However, be aware that this may not be enough for certain kinds\nof Unicode matching involving case folding. See Recipe 2.10 for more details.\n2.7. Specifying a Regular Expression for the Shortest\nMatch\nProblem\nYou’re trying to match a text pattern using regular expressions, but it is identifying the\nlongest possible matches of a pattern. Instead, you would like to change it to find the\nshortest possible match.\n2.7. Specifying a Regular Expression for the Shortest Match \n| \n47",
      "content_length": 1645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": 2,
      "content": "Solution\nThis problem often arises in patterns that try to match text enclosed inside a pair of\nstarting and ending delimiters (e.g., a quoted string). To illustrate, consider this example:\n>>> str_pat = re.compile(r'\\\"(.*)\\\"')\n>>> text1 = 'Computer says \"no.\"'\n>>> str_pat.findall(text1)\n['no.']\n>>> text2 = 'Computer says \"no.\" Phone says \"yes.\"'\n>>> str_pat.findall(text2)\n['no.\" Phone says \"yes.']\n>>>\nIn this example, the pattern r'\\\"(.*)\\\"' is attempting to match text enclosed inside\nquotes. However, the * operator in a regular expression is greedy, so matching is based\non finding the longest possible match. Thus, in the second example involving text2, it\nincorrectly matches the two quoted strings.\nTo fix this, add the ? modifier after the * operator in the pattern, like this:\n>>> str_pat = re.compile(r'\\\"(.*?)\\\"')\n>>> str_pat.findall(text2)\n['no.', 'yes.']\n>>>\nThis makes the matching nongreedy, and produces the shortest match instead.\nDiscussion\nThis recipe addresses one of the more common problems encountered when writing\nregular expressions involving the dot (.) character. In a pattern, the dot matches any\ncharacter except a newline. However, if you bracket the dot with starting and ending\ntext (such as a quote), matching will try to find the longest possible match to the pattern.\nThis causes multiple occurrences of the starting or ending text to be skipped altogether\nand included in the results of the longer match. Adding the ? right after operators such\nas * or + forces the matching algorithm to look for the shortest possible match instead.\n2.8. Writing a Regular Expression for Multiline Patterns\nProblem\nYou’re trying to match a block of text using a regular expression, but you need the match\nto span multiple lines.\n48 \n| \nChapter 2: Strings and Text",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": 2,
      "content": "Solution\nThis problem typically arises in patterns that use the dot (.) to match any character but\nforget to account for the fact that it doesn’t match newlines. For example, suppose you\nare trying to match C-style comments:\n>>> comment = re.compile(r'/\\*(.*?)\\*/')\n>>> text1 = '/* this is a comment */'\n>>> text2 = '''/* this is a\n...               multiline comment */\n... '''\n>>>\n>>> comment.findall(text1)\n[' this is a comment ']\n>>> comment.findall(text2)\n[]\n>>>\nTo fix the problem, you can add support for newlines. For example:\n>>> comment = re.compile(r'/\\*((?:.|\\n)*?)\\*/')\n>>> comment.findall(text2)\n[' this is a\\n              multiline comment ']\n>>>\nIn this pattern, (?:.|\\n) specifies a noncapture group (i.e., it defines a group for the\npurposes of matching, but that group is not captured separately or numbered).\nDiscussion\nThe re.compile() function accepts a flag, re.DOTALL, which is useful here. It makes\nthe . in a regular expression match all characters, including newlines. For example:\n>>> comment = re.compile(r'/\\*(.*?)\\*/', re.DOTALL)\n>>> comment.findall(text2)\n[' this is a\\n              multiline comment ']\nUsing the re.DOTALL flag works fine for simple cases, but might be problematic if you’re\nworking with extremely complicated patterns or a mix of separate regular expressions\nthat have been combined together for the purpose of tokenizing, as described in\nRecipe 2.18. If given a choice, it’s usually better to define your regular expression pattern\nso that it works correctly without the need for extra flags.\n2.8. Writing a Regular Expression for Multiline Patterns \n| \n49",
      "content_length": 1610,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": 2,
      "content": "2.9. Normalizing Unicode Text to a Standard\nRepresentation\nProblem\nYou’re working with Unicode strings, but need to make sure that all of the strings have\nthe same underlying representation.\nSolution\nIn Unicode, certain characters can be represented by more than one valid sequence of\ncode points. To illustrate, consider the following example:\n>>> s1 = 'Spicy Jalape\\u00f1o'\n>>> s2 = 'Spicy Jalapen\\u0303o'\n>>> s1\n'Spicy Jalapeño'\n>>> s2\n'Spicy Jalapeño'\n>>> s1 == s2\nFalse\n>>> len(s1)\n14\n>>> len(s2)\n15\n>>>\nHere the text “Spicy Jalapeño” has been presented in two forms. The first uses the fully\ncomposed “ñ” character (U+00F1). The second uses the Latin letter “n” followed by a\n“~” combining character (U+0303).\nHaving multiple representations is a problem for programs that compare strings. In\norder to fix this, you should first normalize the text into a standard representation using\nthe unicodedata module:\n>>> import unicodedata\n>>> t1 = unicodedata.normalize('NFC', s1)\n>>> t2 = unicodedata.normalize('NFC', s2)\n>>> t1 == t2\nTrue\n>>> print(ascii(t1))\n'Spicy Jalape\\xf1o'\n>>> t3 = unicodedata.normalize('NFD', s1)\n>>> t4 = unicodedata.normalize('NFD', s2)\n>>> t3 == t4\nTrue\n>>> print(ascii(t3))\n'Spicy Jalapen\\u0303o'\n>>>\n50 \n| \nChapter 2: Strings and Text",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": 2,
      "content": "The first argument to normalize() specifies how you want the string normalized. NFC\nmeans that characters should be fully composed (i.e., use a single code point if possible).\nNFD means that characters should be fully decomposed with the use of combining char‐\nacters.\nPython also supports the normalization forms NFKC and NFKD, which add extra com‐\npatibility features for dealing with certain kinds of characters. For example:\n>>> s = '\\ufb01'   # A single character\n>>> s\n'ﬁ'\n>>> unicodedata.normalize('NFD', s)\n'ﬁ'\n# Notice how the combined letters are broken apart here\n>>> unicodedata.normalize('NFKD', s)\n'fi'\n>>> unicodedata.normalize('NFKC', s)\n'fi'\n>>>\nDiscussion\nNormalization is an important part of any code that needs to ensure that it processes\nUnicode text in a sane and consistent way. This is especially true when processing strings\nreceived as part of user input where you have little control of the encoding.\nNormalization can also be an important part of sanitizing and filtering text. For example,\nsuppose you want to remove all diacritical marks from some text (possibly for the pur‐\nposes of searching or matching):\n>>> t1 = unicodedata.normalize('NFD', s1)\n>>> ''.join(c for c in t1 if not unicodedata.combining(c))\n'Spicy Jalapeno'\n>>>\nThis last example shows another important aspect of the unicodedata module—namely,\nutility functions for testing characters against character classes. The combining() func‐\ntion tests a character to see if it is a combining character. There are other functions in\nthe module for finding character categories, testing digits, and so forth.\nUnicode is obviously a large topic. For more detailed reference information about nor‐\nmalization, visit Unicode’s page on the subject. Ned Batchelder has also given an excel‐\nlent presentation on Python Unicode handling issues at his website. \n2.9. Normalizing Unicode Text to a Standard Representation \n| \n51",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": 2,
      "content": "2.10. Working with Unicode Characters in Regular\nExpressions\nProblem\nYou are using regular expressions to process text, but are concerned about the handling\nof Unicode characters.\nSolution\nBy default, the re module is already programmed with rudimentary knowledge of cer‐\ntain Unicode character classes. For example, \\d already matches any unicode digit\ncharacter:\n>>> import re\n>>> num = re.compile('\\d+')\n>>> # ASCII digits\n>>> num.match('123')\n<_sre.SRE_Match object at 0x1007d9ed0>\n>>> # Arabic digits\n>>> num.match('\\u0661\\u0662\\u0663')\n<_sre.SRE_Match object at 0x101234030>\n>>>\nIf you need to include specific Unicode characters in patterns, you can use the usual\nescape sequence for Unicode characters (e.g., \\uFFFF or \\UFFFFFFF). For example, here\nis a regex that matches all characters in a few different Arabic code pages:\n>>> arabic = re.compile('[\\u0600-\\u06ff\\u0750-\\u077f\\u08a0-\\u08ff]+')\n>>>\nWhen performing matching and searching operations, it’s a good idea to normalize and\npossibly sanitize all text to a standard form first (see Recipe 2.9). However, it’s also\nimportant to be aware of special cases. For example, consider the behavior of case-\ninsensitive matching combined with case folding:\n>>> pat = re.compile('stra\\u00dfe', re.IGNORECASE)\n>>> s = 'straße'\n>>> pat.match(s)              # Matches\n<_sre.SRE_Match object at 0x10069d370>\n>>> pat.match(s.upper())      # Doesn't match\n>>> s.upper()                 # Case folds\n'STRASSE'\n>>>\n52 \n| \nChapter 2: Strings and Text",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": 2,
      "content": "Discussion\nMixing Unicode and regular expressions is often a good way to make your head explode.\nIf you’re going to do it seriously, you should consider installing the third-party regex\nlibrary, which provides full support for Unicode case folding, as well as a variety of other\ninteresting features, including approximate matching.\n2.11. Stripping Unwanted Characters from Strings\nProblem\nYou want to strip unwanted characters, such as whitespace, from the beginning, end, or\nmiddle of a text string.\nSolution\nThe strip() method can be used to strip characters from the beginning or end of a\nstring. lstrip() and rstrip() perform stripping from the left or right side, respectively.\nBy default, these methods strip whitespace, but other characters can be given. For\nexample:\n>>> # Whitespace stripping\n>>> s = '   hello world  \\n'\n>>> s.strip()\n'hello world'\n>>> s.lstrip()\n'hello world  \\n'\n>>> s.rstrip()\n'   hello world'\n>>>\n>>> # Character stripping\n>>> t = '-----hello====='\n>>> t.lstrip('-')\n'hello====='\n>>> t.strip('-=')\n'hello'\n>>>\nDiscussion\nThe various strip() methods are commonly used when reading and cleaning up data\nfor later processing. For example, you can use them to get rid of whitespace, remove\nquotations, and other tasks.\nBe aware that stripping does not apply to any text in the middle of a string. For example:\n2.11. Stripping Unwanted Characters from Strings \n| \n53",
      "content_length": 1393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": 2,
      "content": ">>> s = '  hello       world   \\n'\n>>> s = s.strip()\n>>> s\n'hello       world'\n>>>\nIf you needed to do something to the inner space, you would need to use another tech‐\nnique, such as using the replace() method or a regular expression substitution. For\nexample:\n>>> s.replace(' ', '')\n'helloworld'\n>>> import re\n>>> re.sub('\\s+', ' ', s)\n'hello world'\n>>>\nIt is often the case that you want to combine string stripping operations with some other\nkind of iterative processing, such as reading lines of data from a file. If so, this is one\narea where a generator expression can be useful. For example:\nwith open(filename) as f:\n    lines = (line.strip() for line in f)\n    for line in lines:\n        ...\nHere, the expression lines = (line.strip() for line in f) acts as a kind of data\ntransform. It’s efficient because it doesn’t actually read the data into any kind of tem‐\nporary list first. It just creates an iterator where all of the lines produced have the strip‐\nping operation applied to them.\nFor even more advanced stripping, you might turn to the translate() method. See the\nnext recipe on sanitizing strings for further details.\n2.12. Sanitizing and Cleaning Up Text\nProblem\nSome bored script kiddie has entered the text “pýtĥöñ” into a form on your web page\nand you’d like to clean it up somehow.\nSolution\nThe problem of sanitizing and cleaning up text applies to a wide variety of problems\ninvolving text parsing and data handling. At a very simple level, you might use basic\nstring functions (e.g., str.upper() and str.lower()) to convert text to a standard case.\nSimple replacements using str.replace() or re.sub() can focus on removing or\n54 \n| \nChapter 2: Strings and Text",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": 2,
      "content": "changing very specific character sequences. You can also normalize text using unicode\ndata.normalize(), as shown in Recipe 2.9.\nHowever, you might want to take the sanitation process a step further. Perhaps, for\nexample, you want to eliminate whole ranges of characters or strip diacritical marks. To\ndo so, you can turn to the often overlooked str.translate() method. To illustrate,\nsuppose you’ve got a messy string such as the following:\n>>> s = 'pýtĥöñ\\fis\\tawesome\\r\\n'\n>>> s\n'pýtĥöñ\\x0cis\\tawesome\\r\\n'\n>>>\nThe first step is to clean up the whitespace. To do this, make a small translation table\nand use translate():\n>>> remap = {\n...     ord('\\t') : ' ',\n...     ord('\\f') : ' ',\n...     ord('\\r') : None      # Deleted\n... }\n>>> a = s.translate(remap)\n>>> a\n'pýtĥöñ is awesome\\n'\n>>>\nAs you can see here, whitespace characters such as \\t and \\f have been remapped to a\nsingle space. The carriage return \\r has been deleted entirely.\nYou can take this remapping idea a step further and build much bigger tables. For ex‐\nample, let’s remove all combining characters:\n>>> import unicodedata\n>>> import sys\n>>> cmb_chrs = dict.fromkeys(c for c in range(sys.maxunicode)\n...                          if unicodedata.combining(chr(c)))\n...\n>>> b = unicodedata.normalize('NFD', a)\n>>> b\n'pýtĥöñ is awesome\\n'\n>>> b.translate(cmb_chrs)\n'python is awesome\\n'\n>>>\nIn this last example, a dictionary mapping every Unicode combining character to None\nis created using the dict.fromkeys().\nThe original input is then normalized into a decomposed form using unicodedata.nor\nmalize(). From there, the translate function is used to delete all of the accents. Similar\ntechniques can be used to remove other kinds of characters (e.g., control characters,\netc.).\n2.12. Sanitizing and Cleaning Up Text \n| \n55",
      "content_length": 1794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": 2,
      "content": "As another example, here is a translation table that maps all Unicode decimal digit\ncharacters to their equivalent in ASCII:\n>>> digitmap = { c: ord('0') + unicodedata.digit(chr(c))\n...             for c in range(sys.maxunicode)\n...             if unicodedata.category(chr(c)) == 'Nd' }\n...\n>>> len(digitmap)\n460\n>>> # Arabic digits\n>>> x = '\\u0661\\u0662\\u0663'\n>>> x.translate(digitmap)\n'123'\n>>>\nYet another technique for cleaning up text involves I/O decoding and encoding func‐\ntions. The idea here is to first do some preliminary cleanup of the text, and then run it\nthrough a combination of encode() or decode() operations to strip or alter it. For\nexample:\n>>> a\n'pýtĥöñ is awesome\\n'\n>>> b = unicodedata.normalize('NFD', a)\n>>> b.encode('ascii', 'ignore').decode('ascii')\n'python is awesome\\n'\n>>>\nHere the normalization process decomposed the original text into characters along with\nseparate combining characters. The subsequent ASCII encoding/decoding simply dis‐\ncarded all of those characters in one fell swoop. Naturally, this would only work if getting\nan ASCII representation was the final goal.\nDiscussion\nA major issue with sanitizing text can be runtime performance. As a general rule, the\nsimpler it is, the faster it will run. For simple replacements, the str.replace() method\nis often the fastest approach—even if you have to call it multiple times. For instance, to\nclean up whitespace, you could use code like this:\ndef clean_spaces(s):\n    s = s.replace('\\r', '')\n    s = s.replace('\\t', ' ')\n    s = s.replace('\\f', ' ')\n    return s\nIf you try it, you’ll find that it’s quite a bit faster than using translate() or an approach\nusing a regular expression.\nOn the other hand, the translate() method is very fast if you need to perform any\nkind of nontrivial character-to-character remapping or deletion.\n56 \n| \nChapter 2: Strings and Text",
      "content_length": 1863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": 2,
      "content": "In the big picture, performance is something you will have to study further in your\nparticular application. Unfortunately, it’s impossible to suggest one specific technique\nthat works best for all cases, so try different approaches and measure it.\nAlthough the focus of this recipe has been text, similar techniques can be applied to\nbytes, including simple replacements, translation, and regular expressions.\n2.13. Aligning Text Strings\nProblem\nYou need to format text with some sort of alignment applied.\nSolution\nFor basic alignment of strings, the ljust(), rjust(), and center() methods of strings\ncan be used. For example:\n>>> text = 'Hello World'\n>>> text.ljust(20)\n'Hello World         '\n>>> text.rjust(20)\n'         Hello World'\n>>> text.center(20)\n'    Hello World     '\n>>>\nAll of these methods accept an optional &&65.180&&fill character as well. For example:\n>>> text.rjust(20,'=')\n'=========Hello World'\n>>> text.center(20,'*')\n'****Hello World*****'\n>>>\nThe format() function can also be used to easily align things. All you need to do is use\nthe <, >, or ^ characters along with a desired width. For example:\n>>> format(text, '>20')\n'         Hello World'\n>>> format(text, '<20')\n'Hello World         '\n>>> format(text, '^20')\n'    Hello World     '\n>>>\nIf you want to include a fill character other than a space, specify it before the alignment\ncharacter:\n>>> format(text, '=>20s')\n'=========Hello World'\n2.13. Aligning Text Strings \n| \n57",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": 2,
      "content": ">>> format(text, '*^20s')\n'****Hello World*****'\n>>>\nThese format codes can also be used in the format() method when formatting multiple\nvalues. For example:\n>>> '{:>10s} {:>10s}'.format('Hello', 'World')\n'     Hello      World'\n>>>\nOne benefit of format() is that it is not specific to strings. It works with any value,\nmaking it more general purpose. For instance, you can use it with numbers:\n>>> x = 1.2345\n>>> format(x, '>10')\n'    1.2345'\n>>> format(x, '^10.2f')\n'   1.23   '\n>>>\nDiscussion\nIn older code, you will also see the % operator used to format text. For example:\n>>> '%-20s' % text\n'Hello World         '\n>>> '%20s' % text\n'         Hello World'\n>>>\nHowever, in new code, you should probably prefer the use of the format() function or\nmethod. format() is a lot more powerful than what is provided with the % operator.\nMoreover, format() is more general purpose than using the jlust(), rjust(), or\ncenter() method of strings in that it works with any kind of object.\nFor a complete list of features available with the format() function, consult the online\nPython documentation. \n2.14. Combining and Concatenating Strings\nProblem\nYou want to combine many small strings together into a larger string.\nSolution\nIf the strings you wish to combine are found in a sequence or iterable, the fastest way\nto combine them is to use the join() method. For example:\n58 \n| \nChapter 2: Strings and Text",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": 2,
      "content": ">>> parts = ['Is', 'Chicago', 'Not', 'Chicago?']\n>>> ' '.join(parts)\n'Is Chicago Not Chicago?'\n>>> ','.join(parts)\n'Is,Chicago,Not,Chicago?'\n>>> ''.join(parts)\n'IsChicagoNotChicago?'\n>>>\nAt first glance, this syntax might look really odd, but the join() operation is specified\nas a method on strings. Partly this is because the objects you want to join could come\nfrom any number of different data sequences (e.g., lists, tuples, dicts, files, sets, or gen‐\nerators), and it would be redundant to have join() implemented as a method on all of\nthose objects separately. So you just specify the separator string that you want and use\nthe join() method on it to glue text fragments together.\nIf you’re only combining a few strings, using + usually works well enough:\n>>> a = 'Is Chicago'\n>>> b = 'Not Chicago?'\n>>> a + ' ' + b\n'Is Chicago Not Chicago?'\n>>>\nThe + operator also works fine as a substitute for more complicated string formatting\noperations. For example:\n>>> print('{} {}'.format(a,b))\nIs Chicago Not Chicago?\n>>> print(a + ' ' + b)\nIs Chicago Not Chicago?\n>>>\nIf you’re trying to combine string literals together in source code, you can simply place\nthem adjacent to each other with no + operator. For example:\n>>> a = 'Hello' 'World'\n>>> a\n'HelloWorld'\n>>>\nDiscussion\nJoining strings together might not seem advanced enough to warrant an entire recipe,\nbut it’s often an area where programmers make programming choices that severely\nimpact the performance of their code.\nThe most important thing to know is that using the + operator to join a lot of strings\ntogether is grossly inefficient due to the memory copies and garbage collection that\noccurs. In particular, you never want to write code that joins strings together like this:\n2.14. Combining and Concatenating Strings \n| \n59",
      "content_length": 1794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": 2,
      "content": "s = ''\nfor p in parts:\n    s += p\nThis runs quite a bit slower than using the join() method, mainly because each +=\noperation creates a new string object. You’re better off just collecting all of the parts first\nand then joining them together at the end.\nOne related (and pretty neat) trick is the conversion of data to strings and concatenation\nat the same time using a generator expression, as described in Recipe 1.19. For example:\n>>> data = ['ACME', 50, 91.1]\n>>> ','.join(str(d) for d in data)\n'ACME,50,91.1'\n>>>\nAlso be on the lookout for unnecessary string concatenations. Sometimes programmers\nget carried away with concatenation when it’s really not technically necessary. For ex‐\nample, when printing:\nprint(a + ':' + b + ':' + c)       # Ugly\nprint(':'.join([a, b, c]))         # Still ugly\nprint(a, b, c, sep=':')            # Better\nMixing I/O operations and string concatenation is something that might require study\nin your application. For example, consider the following two code fragments:\n# Version 1 (string concatenation)\nf.write(chunk1 + chunk2)\n# Version 2 (separate I/O operations)\nf.write(chunk1)\nf.write(chunk2)\nIf the two strings are small, the first version might offer much better performance due\nto the inherent expense of carrying out an I/O system call. On the other hand, if the two\nstrings are large, the second version may be more efficient, since it avoids making a large\ntemporary result and copying large blocks of memory around. Again, it must be stressed\nthat this is something you would have to study in relation to your own data in order to\ndetermine which performs best.\nLast, but not least, if you’re writing code that is building output from lots of small strings,\nyou might consider writing that code as a generator function, using yield to emit frag‐\nments. For example:\ndef sample():\n    yield 'Is'\n    yield 'Chicago'\n    yield 'Not'\n    yield 'Chicago?'\n60 \n| \nChapter 2: Strings and Text",
      "content_length": 1939,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": 2,
      "content": "The interesting thing about this approach is that it makes no assumption about how the\nfragments are to be assembled together. For example, you could simply join the frag‐\nments using join():\ntext = ''.join(sample())\nOr you could redirect the fragments to I/O:\nfor part in sample():\n    f.write(part)\nOr you could come up with some kind of hybrid scheme that’s smart about combining\nI/O operations:\ndef combine(source, maxsize):\n    parts = []\n    size = 0\n    for part in source:\n        parts.append(part)\n        size += len(part)\n        if size > maxsize:\n            yield ''.join(parts)\n            parts = []\n            size = 0\n    yield ''.join(parts)\nfor part in combine(sample(), 32768):\n    f.write(part)\nThe key point is that the original generator function doesn’t have to know the precise\ndetails. It just yields the parts.\n2.15. Interpolating Variables in Strings\nProblem\nYou want to create a string in which embedded variable names are substituted with a\nstring representation of a variable’s value.\nSolution\nPython has no direct support for simply substituting variable values in strings. However,\nthis feature can be approximated using the format() method of strings. For example:\n>>> s = '{name} has {n} messages.'\n>>> s.format(name='Guido', n=37)\n'Guido has 37 messages.'\n>>>\n2.15. Interpolating Variables in Strings \n| \n61",
      "content_length": 1346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": 2,
      "content": "Alternatively, if the values to be substituted are truly found in variables, you can use the\ncombination of format_map() and vars(), as in the following:\n>>> name = 'Guido'\n>>> n = 37\n>>> s.format_map(vars())\n'Guido has 37 messages.'\n>>>\nOne subtle feature of vars() is that it also works with instances. For example:\n>>> class Info:\n...     def __init__(self, name, n):\n...         self.name = name\n...         self.n = n\n...\n>>> a = Info('Guido',37)\n>>> s.format_map(vars(a))\n'Guido has 37 messages.'\n>>>\nOne downside of format() and format_map() is that they do not deal gracefully with\nmissing values. For example:\n>>> s.format(name='Guido')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'n'\n>>>\nOne way to avoid this is to define an alternative dictionary class with a __miss\ning__() method, as in the following:\nclass safesub(dict):\n    def __missing__(self, key):\n        return '{' + key + '}'\nNow use this class to wrap the inputs to format_map():\n>>> del n     # Make sure n is undefined\n>>> s.format_map(safesub(vars()))\n'Guido has {n} messages.'\n>>>\nIf you find yourself frequently performing these steps in your code, you could hide the\nvariable substitution process behind a small utility function that employs a so-called\n“frame hack.” For example:\nimport sys\ndef sub(text):\n    return text.format_map(safesub(sys._getframe(1).f_locals))\nNow you can type things like this:\n62 \n| \nChapter 2: Strings and Text",
      "content_length": 1459,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": 2,
      "content": ">>> name = 'Guido'\n>>> n = 37\n>>> print(sub('Hello {name}'))\nHello Guido\n>>> print(sub('You have {n} messages.'))\nYou have 37 messages.\n>>> print(sub('Your favorite color is {color}'))\nYour favorite color is {color}\n>>>\nDiscussion\nThe lack of true variable interpolation in Python has led to a variety of solutions over\nthe years. As an alternative to the solution presented in this recipe, you will sometimes\nsee string formatting like this:\n>>> name = 'Guido'\n>>> n = 37\n>>> '%(name) has %(n) messages.' % vars()\n'Guido has 37 messages.'\n>>>\nYou may also see the use of template strings:\n>>> import string\n>>> s = string.Template('$name has $n messages.')\n>>> s.substitute(vars())\n'Guido has 37 messages.'\n>>>\nHowever, the format() and format_map() methods are more modern than either of\nthese alternatives, and should be preferred. One benefit of using format() is that you\nalso get all of the features related to string formatting (alignment, padding, numerical\nformatting, etc.), which is simply not possible with alternatives such as Template string\nobjects.\nParts of this recipe also illustrate a few interesting advanced features. The little-known\n__missing__() method of mapping/dict classes is a method that you can define to\nhandle missing values. In the safesub class, this method has been defined to return\nmissing values back as a placeholder. Instead of getting a KeyError exception, you\nwould see the missing values appearing in the resulting string (potentially useful for\ndebugging).\nThe sub() function uses sys._getframe(1) to return the stack frame of the caller. From\nthat, the f_locals attribute is accessed to get the local variables. It goes without saying\nthat messing around with stack frames should probably be avoided in most code. How‐\never, for utility functions such as a string substitution feature, it can be useful. As an\naside, it’s probably worth noting that f_locals is a dictionary that is a copy of the local\nvariables in the calling function. Although you can modify the contents of f_locals,\n2.15. Interpolating Variables in Strings \n| \n63",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": 2,
      "content": "the modifications don’t actually have any lasting effect. Thus, even though accessing a\ndifferent stack frame might look evil, it’s not possible to accidentally overwrite variables\nor change the local environment of the caller.\n2.16. Reformatting Text to a Fixed Number of Columns\nProblem\nYou have long strings that you want to reformat so that they fill a user-specified number\nof columns.\nSolution\nUse the textwrap module to reformat text for output. For example, suppose you have\nthe following long string:\ns = \"Look into my eyes, look into my eyes, the eyes, the eyes, \\\nthe eyes, not around the eyes, don't look around the eyes, \\\nlook into my eyes, you're under.\"\nHere’s how you can use the textwrap module to reformat it in various ways:\n>>> import textwrap\n>>> print(textwrap.fill(s, 70))\nLook into my eyes, look into my eyes, the eyes, the eyes, the eyes,\nnot around the eyes, don't look around the eyes, look into my eyes,\nyou're under.\n>>> print(textwrap.fill(s, 40))\nLook into my eyes, look into my eyes,\nthe eyes, the eyes, the eyes, not around\nthe eyes, don't look around the eyes,\nlook into my eyes, you're under.\n>>> print(textwrap.fill(s, 40, initial_indent='    '))\n    Look into my eyes, look into my\neyes, the eyes, the eyes, the eyes, not\naround the eyes, don't look around the\neyes, look into my eyes, you're under.\n>>> print(textwrap.fill(s, 40, subsequent_indent='    '))\nLook into my eyes, look into my eyes,\n    the eyes, the eyes, the eyes, not\n    around the eyes, don't look around\n    the eyes, look into my eyes, you're\n    under.\n64 \n| \nChapter 2: Strings and Text",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": 2,
      "content": "Discussion\nThe textwrap module is a straightforward way to clean up text for printing—especially\nif you want the output to fit nicely on the terminal. On the subject of the terminal size,\nyou can obtain it using os.get_terminal_size(). For example:\n>>> import os\n>>> os.get_terminal_size().columns\n80\n>>>\nThe fill() method has a few additional options that control how it handles tabs, sen‐\ntence endings, and so on. Look at the documentation for the textwrap.TextWrapper\nclass for further details.\n2.17. Handling HTML and XML Entities in Text\nProblem\nYou want to replace HTML or XML entities such as &entity; or &#code; with their\ncorresponding text. Alternatively, you need to produce text, but escape certain charac‐\nters (e.g., <, >, or &).\nSolution\nIf you are producing text, replacing special characters such as < or > is relatively easy if\nyou use the html.escape() function. For example:\n>>> s = 'Elements are written as \"<tag>text</tag>\".'\n>>> import html\n>>> print(s)\nElements are written as \"<tag>text</tag>\".\n>>> print(html.escape(s))\nElements are written as &quot;&lt;tag&gt;text&lt;/tag&gt;&quot;.\n>>> # Disable escaping of quotes\n>>> print(html.escape(s, quote=False))\nElements are written as \"&lt;tag&gt;text&lt;/tag&gt;\".\n>>>\nIf you’re trying to emit text as ASCII and want to embed character code entities for non-\nASCII characters, you can use the errors='xmlcharrefreplace' argument to various\nI/O-related functions to do it. For example:\n>>> s = 'Spicy Jalapeño'\n>>> s.encode('ascii', errors='xmlcharrefreplace')\nb'Spicy Jalape&#241;o'\n>>>\n2.17. Handling HTML and XML Entities in Text \n| \n65",
      "content_length": 1612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": 2,
      "content": "To replace entities in text, a different approach is needed. If you’re actually processing\nHTML or XML, try using a proper HTML or XML parser first. Normally, these tools\nwill automatically take care of replacing the values for you during parsing and you don’t\nneed to worry about it.\nIf, for some reason, you’ve received bare text with some entities in it and you want them\nreplaced manually, you can usually do it using various utility functions/methods asso‐\nciated with HTML or XML parsers. For example:\n>>> s = 'Spicy &quot;Jalape&#241;o&quot.'\n>>> from html.parser import HTMLParser\n>>> p = HTMLParser()\n>>> p.unescape(s)\n'Spicy \"Jalapeño\".'\n>>>\n>>> t = 'The prompt is &gt;&gt;&gt;'\n>>> from xml.sax.saxutils import unescape\n>>> unescape(t)\n'The prompt is >>>'\n>>>\nDiscussion\nProper escaping of special characters is an easily overlooked detail of generating HTML\nor XML. This is especially true if you’re generating such output yourself using print()\nor other basic string formatting features. Using a utility function such as html.es\ncape() is an easy solution.\nIf you need to process text in the other direction, various utility functions, such as\nxml.sax.saxutils.unescape(), can help. However, you really need to investigate the\nuse of a proper parser. For example, if processing HTML or XML, using a parsing mod‐\nule such as html.parser or xml.etree.ElementTree should already take care of details\nrelated to replacing entities in the input text for you.\n2.18. Tokenizing Text\nProblem\nYou have a string that you want to parse left to right into a stream of tokens.\nSolution\nSuppose you have a string of text such as this:\ntext = 'foo = 23 + 42 * 10'\n66 \n| \nChapter 2: Strings and Text",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": 2,
      "content": "To tokenize the string, you need to do more than merely match patterns. You need to\nhave some way to identify the kind of pattern as well. For instance, you might want to\nturn the string into a sequence of pairs like this:\ntokens = [('NAME', 'foo'), ('EQ','='), ('NUM', '23'), ('PLUS','+'),\n          ('NUM', '42'), ('TIMES', '*'), ('NUM', 10')]\nTo do this kind of splitting, the first step is to define all of the possible tokens, including\nwhitespace, by regular expression patterns using named capture groups such as this:\nimport re\nNAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'\nNUM  = r'(?P<NUM>\\d+)'\nPLUS = r'(?P<PLUS>\\+)'\nTIMES = r'(?P<TIMES>\\*)'\nEQ    = r'(?P<EQ>=)'\nWS    = r'(?P<WS>\\s+)'\nmaster_pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))\nIn these re patterns, the ?P<TOKENNAME> convention is used to assign a name to the\npattern. This will be used later.\nNext, to tokenize, use the little-known scanner() method of pattern objects. This\nmethod creates a scanner object in which repeated calls to match() step through the\nsupplied text one match at a time. Here is an interactive example of how a scanner object\nworks:\n>>> scanner = master_pat.scanner('foo = 42')\n>>> scanner.match()\n<_sre.SRE_Match object at 0x100677738>\n>>> _.lastgroup, _.group()\n('NAME', 'foo')\n>>> scanner.match()\n<_sre.SRE_Match object at 0x100677738>\n>>> _.lastgroup, _.group()\n('WS', ' ')\n>>> scanner.match()\n<_sre.SRE_Match object at 0x100677738>\n>>> _.lastgroup, _.group()\n('EQ', '=')\n>>> scanner.match()\n<_sre.SRE_Match object at 0x100677738>\n>>> _.lastgroup, _.group()\n('WS', ' ')\n>>> scanner.match()\n<_sre.SRE_Match object at 0x100677738>\n>>> _.lastgroup, _.group()\n('NUM', '42')\n>>> scanner.match()\n>>>\n2.18. Tokenizing Text \n| \n67",
      "content_length": 1735,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": 2,
      "content": "To take this technique and put it into code, it can be cleaned up and easily packaged\ninto a generator like this:\nfrom collections import namedtuple\nToken = namedtuple('Token', ['type','value'])\ndef generate_tokens(pat, text):\n    scanner = pat.scanner(text)\n    for m in iter(scanner.match, None):\n        yield Token(m.lastgroup, m.group())\n# Example use\nfor tok in generate_tokens(master_pat, 'foo = 42'):\n    print(tok)\n# Produces output\n# Token(type='NAME', value='foo')\n# Token(type='WS', value=' ')\n# Token(type='EQ', value='=')\n# Token(type='WS', value=' ')\n# Token(type='NUM', value='42')\nIf you want to filter the token stream in some way, you can either define more generator\nfunctions or use a generator expression. For example, here is how you might filter out\nall whitespace tokens.\ntokens = (tok for tok in generate_tokens(master_pat, text)\n          if tok.type != 'WS')\nfor tok in tokens:\n    print(tok)\nDiscussion\nTokenizing is often the first step for more advanced kinds of text parsing and handling.\nTo use the scanning technique shown, there are a few important details to keep in mind.\nFirst, you must make sure that you identify every possible text sequence that might\nappear in the input with a correponding re pattern. If any nonmatching text is found,\nscanning simply stops. This is why it was necessary to specify the whitespace (WS) token\nin the example.\nThe order of tokens in the master regular expression also matters. When matching, re\ntries to match pattens in the order specified. Thus, if a pattern happens to be a substring\nof a longer pattern, you need to make sure the longer pattern goes first. For example:\nLT = r'(?P<LT><)'\nLE = r'(?P<LE><=)'\nEQ = r'(?P<EQ>=)'\nmaster_pat = re.compile('|'.join([LE, LT, EQ]))    # Correct\n# master_pat = re.compile('|'.join([LT, LE, EQ]))  # Incorrect\n68 \n| \nChapter 2: Strings and Text",
      "content_length": 1861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": 2,
      "content": "The second pattern is wrong because it would match the text <= as the token LT followed\nby the token EQ, not the single token LE, as was probably desired.\nLast, but not least, you need to watch out for patterns that form substrings. For example,\nsuppose you have two pattens like this:\nPRINT = r'(P<PRINT>print)'\nNAME  = r'(P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'\nmaster_pat = re.compile('|'.join([PRINT, NAME]))\nfor tok in generate_tokens(master_pat, 'printer'):\n    print(tok)\n# Outputs :\n#  Token(type='PRINT', value='print')\n#  Token(type='NAME', value='er')\nFor more advanced kinds of tokenizing, you may want to check out packages such as \nPyParsing or PLY. An example involving PLY appears in the next recipe.\n2.19. Writing a Simple Recursive Descent Parser\nProblem\nYou need to parse text according to a set of grammar rules and perform actions or build\nan abstract syntax tree representing the input. The grammar is small, so you’d prefer to\njust write the parser yourself as opposed to using some kind of framework.\nSolution\nIn this problem, we’re focused on the problem of parsing text according to a particular\ngrammar. In order to do this, you should probably start by having a formal specification\nof the grammar in the form of a BNF or EBNF. For example, a grammar for simple\narithmetic expressions might look like this:\n    expr ::= expr + term\n         |   expr - term\n         |   term\n    term ::= term * factor\n         |   term / factor\n         |   factor\n    factor ::= ( expr )\n           |   NUM\nOr, alternatively, in EBNF form:\n2.19. Writing a Simple Recursive Descent Parser \n| \n69",
      "content_length": 1599,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": 2,
      "content": "expr ::= term { (+|-) term }*\n    term ::= factor { (*|/) factor }*\n    factor ::= ( expr )\n           |   NUM\nIn an EBNF, parts of a rule enclosed in { ... }* are optional. The * means zero or more\nrepetitions (the same meaning as in a regular expression).\nNow, if you’re not familiar with the mechanics of working with a BNF, think of it as a\nspecification of substitution or replacement rules where symbols on the left side can be\nreplaced by the symbols on the right (or vice versa). Generally, what happens during\nparsing is that you try to match the input text to the grammar by making various sub‐\nstitutions and expansions using the BNF. To illustrate, suppose you are parsing an ex‐\npression such as 3 + 4 * 5. This expression would first need to be broken down into\na token stream, using the techniques described in Recipe 2.18. The result might be a\nsequence of tokens like this:\n    NUM + NUM * NUM\nFrom there, parsing involves trying to match the grammar to input tokens by making\nsubstitutions:\n    expr\n    expr ::= term { (+|-) term }*\n    expr ::= factor { (*|/) factor }* { (+|-) term }*\n    expr ::= NUM { (*|/) factor }* { (+|-) term }*\n    expr ::= NUM { (+|-) term }*\n    expr ::= NUM + term { (+|-) term }*\n    expr ::= NUM + factor { (*|/) factor }* { (+|-) term }*\n    expr ::= NUM + NUM { (*|/) factor}* { (+|-) term }*\n    expr ::= NUM + NUM * factor { (*|/) factor }* { (+|-) term }*\n    expr ::= NUM + NUM * NUM { (*|/) factor }* { (+|-) term }*\n    expr ::= NUM + NUM * NUM { (+|-) term }*\n    expr ::= NUM + NUM * NUM\nFollowing all of the substitution steps takes a bit of coffee, but they’re driven by looking\nat the input and trying to match it to grammar rules. The first input token is a NUM, so\nsubstitutions first focus on matching that part. Once matched, attention moves to the\nnext token of + and so on. Certain parts of the righthand side (e.g., { (*/) fac\ntor }*) disappear when it’s determined that they can’t match the next token. In a suc‐\ncessful parse, the entire righthand side is expanded completely to match the input token\nstream.\nWith all of the preceding background in place, here is a simple recipe that shows how\nto build a recursive descent expression evaluator:\nimport re\nimport collections\n70 \n| \nChapter 2: Strings and Text",
      "content_length": 2282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": 2,
      "content": "# Token specification\nNUM    = r'(?P<NUM>\\d+)'\nPLUS   = r'(?P<PLUS>\\+)'\nMINUS  = r'(?P<MINUS>-)'\nTIMES  = r'(?P<TIMES>\\*)'\nDIVIDE = r'(?P<DIVIDE>/)'\nLPAREN = r'(?P<LPAREN>\\()'\nRPAREN = r'(?P<RPAREN>\\))'\nWS     = r'(?P<WS>\\s+)'\nmaster_pat = re.compile('|'.join([NUM, PLUS, MINUS, TIMES,\n                                  DIVIDE, LPAREN, RPAREN, WS]))\n# Tokenizer\nToken = collections.namedtuple('Token', ['type','value'])\ndef generate_tokens(text):\n    scanner = master_pat.scanner(text)\n    for m in iter(scanner.match, None):\n        tok = Token(m.lastgroup, m.group())\n        if tok.type != 'WS':\n            yield tok\n# Parser\nclass ExpressionEvaluator:\n    '''\n    Implementation of a recursive descent parser.   Each method\n    implements a single grammar rule.  Use the ._accept() method\n    to test and accept the current lookahead token.  Use the ._expect()\n    method to exactly match and discard the next token on on the input\n    (or raise a SyntaxError if it doesn't match).\n    '''\n    def parse(self,text):\n        self.tokens = generate_tokens(text)\n        self.tok = None             # Last symbol consumed\n        self.nexttok = None         # Next symbol tokenized\n        self._advance()             # Load first lookahead token\n        return self.expr()\n    def _advance(self):\n        'Advance one token ahead'\n        self.tok, self.nexttok = self.nexttok, next(self.tokens, None)\n    def _accept(self,toktype):\n        'Test and consume the next token if it matches toktype'\n        if self.nexttok and self.nexttok.type == toktype:\n            self._advance()\n            return True\n        else:\n            return False\n2.19. Writing a Simple Recursive Descent Parser \n| \n71",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": 2,
      "content": "def _expect(self,toktype):\n        'Consume next token if it matches toktype or raise SyntaxError'\n        if not self._accept(toktype):\n            raise SyntaxError('Expected ' + toktype)\n    # Grammar rules follow\n    def expr(self):\n        \"expression ::= term { ('+'|'-') term }*\"\n        exprval = self.term()\n        while self._accept('PLUS') or self._accept('MINUS'):\n            op = self.tok.type\n            right = self.term()\n            if op == 'PLUS':\n                exprval += right\n            elif op == 'MINUS':\n                exprval -= right\n        return exprval\n    def term(self):\n        \"term ::= factor { ('*'|'/') factor }*\"\n        termval = self.factor()\n        while self._accept('TIMES') or self._accept('DIVIDE'):\n            op = self.tok.type\n            right = self.factor()\n            if op == 'TIMES':\n                termval *= right\n            elif op == 'DIVIDE':\n                termval /= right\n        return termval\n    def factor(self):\n        \"factor ::= NUM | ( expr )\"\n        if self._accept('NUM'):\n            return int(self.tok.value)\n        elif self._accept('LPAREN'):\n            exprval = self.expr()\n            self._expect('RPAREN')\n            return exprval\n        else:\n            raise SyntaxError('Expected NUMBER or LPAREN')\nHere is an example of using the ExpressionEvaluator class interactively:\n>>> e = ExpressionEvaluator()\n>>> e.parse('2')\n2\n>>> e.parse('2 + 3')\n5\n72 \n| \nChapter 2: Strings and Text",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": 2,
      "content": ">>> e.parse('2 + 3 * 4')\n14\n>>> e.parse('2 + (3 + 4) * 5')\n37\n>>> e.parse('2 + (3 + * 4)')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"exprparse.py\", line 40, in parse\n    return self.expr()\n  File \"exprparse.py\", line 67, in expr\n    right = self.term()\n  File \"exprparse.py\", line 77, in term\n    termval = self.factor()\n  File \"exprparse.py\", line 93, in factor\n    exprval = self.expr()\n  File \"exprparse.py\", line 67, in expr\n    right = self.term()\n  File \"exprparse.py\", line 77, in term\n    termval = self.factor()\n  File \"exprparse.py\", line 97, in factor\n    raise SyntaxError(\"Expected NUMBER or LPAREN\")\nSyntaxError: Expected NUMBER or LPAREN\n>>>\nIf you want to do something other than pure evaluation, you need to change the\nExpressionEvaluator class to do something else. For example, here is an alternative\nimplementation that constructs a simple parse tree:\nclass ExpressionTreeBuilder(ExpressionEvaluator):\n    def expr(self):\n        \"expression ::= term { ('+'|'-') term }\"\n        exprval = self.term()\n        while self._accept('PLUS') or self._accept('MINUS'):\n            op = self.tok.type\n            right = self.term()\n            if op == 'PLUS':\n                exprval = ('+', exprval, right)\n            elif op == 'MINUS':\n                exprval = ('-', exprval, right)\n        return exprval\n    def term(self):\n        \"term ::= factor { ('*'|'/') factor }\"\n        termval = self.factor()\n        while self._accept('TIMES') or self._accept('DIVIDE'):\n            op = self.tok.type\n            right = self.factor()\n            if op == 'TIMES':\n                termval = ('*', termval, right)\n            elif op == 'DIVIDE':\n2.19. Writing a Simple Recursive Descent Parser \n| \n73",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": 2,
      "content": "termval = ('/', termval, right)\n        return termval\n    def factor(self):\n        'factor ::= NUM | ( expr )'\n        if self._accept('NUM'):\n            return int(self.tok.value)\n        elif self._accept('LPAREN'):\n            exprval = self.expr()\n            self._expect('RPAREN')\n            return exprval\n        else:\n            raise SyntaxError('Expected NUMBER or LPAREN')\nThe following example shows how it works:\n>>> e = ExpressionTreeBuilder()\n>>> e.parse('2 + 3')\n('+', 2, 3)\n>>> e.parse('2 + 3 * 4')\n('+', 2, ('*', 3, 4))\n>>> e.parse('2 + (3 + 4) * 5')\n('+', 2, ('*', ('+', 3, 4), 5))\n>>> e.parse('2 + 3 + 4')\n('+', ('+', 2, 3), 4)\n>>>\nDiscussion\nParsing is a huge topic that generally occupies students for the first three weeks of a\ncompilers course. If you are seeking background knowledge about grammars, parsing\nalgorithms, and other information, a compilers book is where you should turn. Needless\nto say, all of that can’t be repeated here.\nNevertheless, the overall idea of writing a recursive descent parser is generally simple.\nTo start, you take every grammar rule and you turn it into a function or method. Thus,\nif your grammar looks like this:\n    expr ::= term { ('+'|'-') term }*\n    term ::= factor { ('*'|'/') factor }*\n    factor ::= '(' expr ')'\n           |   NUM\nYou start by turning it into a set of methods like this:\nclass ExpressionEvaluator:\n    ...\n    def expr(self):\n        ...\n74 \n| \nChapter 2: Strings and Text",
      "content_length": 1465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": 2,
      "content": "def term(self):\n        ...\n    def factor(self):\n        ...\nThe task of each method is simple—it must walk from left to right over each part of the\ngrammar rule, consuming tokens in the process. In a sense, the goal of the method is\nto either consume the rule or generate a syntax error if it gets stuck. To do this, the\nfollowing implementation techniques are applied:\n• If the next symbol in the rule is the name of another grammar rule (e.g., term or\nfactor), you simply call the method with the same name. This is the “descent” part\nof the algorithm—control descends into another grammar rule. Sometimes rules\nwill involve calls to methods that are already executing (e.g., the call to expr in the\nfactor ::= '(' expr ')' rule). This is the “recursive” part of the algorithm.\n• If the next symbol in the rule has to be a specific symbol (e.g., (), you look at the\nnext token and check for an exact match. If it doesn’t match, it’s a syntax error. The\n_expect() method in this recipe is used to perform these steps.\n• If the next symbol in the rule could be a few possible choices (e.g., + or -), you have\nto check the next token for each possibility and advance only if a match is made.\nThis is the purpose of the _accept() method in this recipe. It’s kind of like a weaker\nversion of the _expect() method in that it will advance if a match is made, but if\nnot, it simply backs off without raising an error (thus allowing further checks to be\nmade).\n• For grammar rules where there are repeated parts (e.g., such as in the rule expr ::=\nterm { ('+'|'-') term }*), the repetition gets implemented by a while loop.\nThe body of the loop will generally collect or process all of the repeated items until\nno more are found.\n• Once an entire grammar rule has been consumed, each method returns some kind\nof result back to the caller. This is how values propagate during parsing. For ex‐\nample, in the expression evaluator, return values will represent partial results of the\nexpression being parsed. Eventually they all get combined together in the topmost\ngrammar rule method that executes.\nAlthough a simple example has been shown, recursive descent parsers can be used to\nimplement rather complicated parsers. For example, Python code itself is interpreted\nby a recursive descent parser. If you’re so inclined, you can look at the underlying gram‐\nmar by inspecting the file Grammar/Grammar in the Python source. That said, there\nare still numerous pitfalls and limitations with making a parser by hand.\n2.19. Writing a Simple Recursive Descent Parser \n| \n75",
      "content_length": 2561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": 2,
      "content": "One such limitation of recursive descent parsers is that they can’t be written for grammar\nrules involving any kind of left recursion. For example, suppose you need to translate a\nrule like this:\n    items ::= items ',' item\n           |  item\nTo do it, you might try to use the items() method like this:\ndef items(self):\n    itemsval = self.items()\n    if itemsval and self._accept(','):\n         itemsval.append(self.item())\n    else:\n         itemsval = [ self.item() ]\nThe only problem is that it doesn’t work. In fact, it blows up with an infinite recursion\nerror.\nYou can also run into tricky issues concerning the grammar rules themselves. For ex‐\nample, you might have wondered whether or not expressions could have been described\nby this more simple grammar:\n    expr ::= factor { ('+'|'-'|'*'|'/') factor }*\n    factor ::= '(' expression ')'\n           |   NUM\nThis grammar technically “works,” but it doesn’t observe the standard arithmetic rules\nconcerning order of evaluation. For example, the expression “3 + 4 * 5” would get eval‐\nuated as “35” instead of the expected result of “23.” The use of separate “expr” and “term”\nrules is there to make evaluation work correctly.\nFor really complicated grammars, you are often better off using parsing tools such as \nPyParsing or PLY. This is what the expression evaluator code looks like using PLY:\nfrom ply.lex import lex\nfrom ply.yacc import yacc\n# Token list\ntokens = [ 'NUM', 'PLUS', 'MINUS', 'TIMES', 'DIVIDE', 'LPAREN', 'RPAREN' ]\n# Ignored characters\nt_ignore = ' \\t\\n'\n# Token specifications (as regexs)\nt_PLUS   = r'\\+'\nt_MINUS  = r'-'\nt_TIMES  = r'\\*'\nt_DIVIDE = r'/'\nt_LPAREN = r'\\('\n76 \n| \nChapter 2: Strings and Text",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": 2,
      "content": "t_RPAREN = r'\\)'\n# Token processing functions\ndef t_NUM(t):\n    r'\\d+'\n    t.value = int(t.value)\n    return t\n# Error handler\ndef t_error(t):\n    print('Bad character: {!r}'.format(t.value[0]))\n    t.skip(1)\n# Build the lexer\nlexer = lex()\n# Grammar rules and handler functions\ndef p_expr(p):\n    '''\n    expr : expr PLUS term\n         | expr MINUS term\n    '''\n    if p[2] == '+':\n        p[0] = p[1] + p[3]\n    elif p[2] == '-':\n        p[0] = p[1] - p[3]\ndef p_expr_term(p):\n    '''\n    expr : term\n    '''\n    p[0] = p[1]\ndef p_term(p):\n    '''\n    term : term TIMES factor\n         | term DIVIDE factor\n    '''\n    if p[2] == '*':\n        p[0] = p[1] * p[3]\n    elif p[2] == '/':\n        p[0] = p[1] / p[3]\ndef p_term_factor(p):\n    '''\n    term : factor\n    '''\n    p[0] = p[1]\ndef p_factor(p):\n    '''\n    factor : NUM\n2.19. Writing a Simple Recursive Descent Parser \n| \n77",
      "content_length": 881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": 2,
      "content": "'''\n    p[0] = p[1]\ndef p_factor_group(p):\n    '''\n    factor : LPAREN expr RPAREN\n    '''\n    p[0] = p[2]\ndef p_error(p):\n    print('Syntax error')\nparser = yacc()\nIn this code, you’ll find that everything is specified at a much higher level. You simply\nwrite regular expressions for the tokens and high-level handling functions that execute\nwhen various grammar rules are matched. The actual mechanics of running the parser,\naccepting tokens, and so forth is implemented entirely by the library.\nHere is an example of how the resulting parser object gets used:\n>>> parser.parse('2')\n2\n>>> parser.parse('2+3')\n5\n>>> parser.parse('2+(3+4)*5')\n37\n>>>\nIf you need a bit more excitement in your programming, writing parsers and compilers\ncan be a fun project. Again, a compilers textbook will have a lot of low-level details\nunderlying theory. However, many fine resources can also be found online. Python’s\nown ast module is also worth a look.\n2.20. Performing Text Operations on Byte Strings\nProblem\nYou want to perform common text operations (e.g., stripping, searching, and replace‐\nment) on byte strings.\nSolution\nByte strings already support most of the same built-in operations as text strings. For\nexample:\n78 \n| \nChapter 2: Strings and Text",
      "content_length": 1246,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": 2,
      "content": ">>> data = b'Hello World'\n>>> data[0:5]\nb'Hello'\n>>> data.startswith(b'Hello')\nTrue\n>>> data.split()\n[b'Hello', b'World']\n>>> data.replace(b'Hello', b'Hello Cruel')\nb'Hello Cruel World'\n>>>\nSuch operations also work with byte arrays. For example:\n>>> data = bytearray(b'Hello World')\n>>> data[0:5]\nbytearray(b'Hello')\n>>> data.startswith(b'Hello')\nTrue\n>>> data.split()\n[bytearray(b'Hello'), bytearray(b'World')]\n>>> data.replace(b'Hello', b'Hello Cruel')\nbytearray(b'Hello Cruel World')\n>>>\nYou can apply regular expression pattern matching to byte strings, but the patterns\nthemselves need to be specified as bytes. For example:\n>>>\n>>> data = b'FOO:BAR,SPAM'\n>>> import re\n>>> re.split('[:,]',data)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.3/re.py\", line 191, in split\n    return _compile(pattern, flags).split(string, maxsplit)\nTypeError: can't use a string pattern on a bytes-like object\n>>> re.split(b'[:,]',data)     # Notice: pattern as bytes\n[b'FOO', b'BAR', b'SPAM']\n>>>\nDiscussion\nFor the most part, almost all of the operations available on text strings will work on byte\nstrings. However, there are a few notable differences to be aware of. First, indexing of\nbyte strings produces integers, not individual characters. For example:\n>>> a = 'Hello World'     # Text string\n>>> a[0]\n'H'\n>>> a[1]\n'e'\n>>> b = b'Hello World'    # Byte string\n2.20. Performing Text Operations on Byte Strings \n| \n79",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": 2,
      "content": ">>> b[0]\n72\n>>> b[1]\n101\n>>>\nThis difference in semantics can affect programs that try to process byte-oriented data\non a character-by-character basis.\nSecond, byte strings don’t provide a nice string representation and don’t print cleanly\nunless first decoded into a text string. For example:\n>>> s = b'Hello World'\n>>> print(s)\nb'Hello World'               # Observe b'...'\n>>> print(s.decode('ascii'))\nHello World\n>>>\nSimilarly, there are no string formatting operations available to byte strings.\n>>> b'%10s %10d %10.2f' % (b'ACME', 100, 490.1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for %: 'bytes' and 'tuple'\n>>> b'{} {} {}'.format(b'ACME', 100, 490.1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'bytes' object has no attribute 'format'\n>>>\nIf you want to do any kind of formatting applied to byte strings, it should be done using\nnormal text strings and encoding. For example:\n>>> '{:10s} {:10d} {:10.2f}'.format('ACME', 100, 490.1).encode('ascii')\nb'ACME              100     490.10'\n>>>\nFinally, you need to be aware that using a byte string can change the semantics of certain\noperations—especially those related to the filesystem. For example, if you supply a file‐\nname encoded as bytes instead of a text string, it usually disables filename encoding/\ndecoding. For example:\n>>> # Write a UTF-8 filename\n>>> with open('jalape\\xf1o.txt', 'w') as f:\n...     f.write('spicy')\n...\n>>> # Get a directory listing\n>>> import os\n>>> os.listdir('.')          # Text string (names are decoded)\n['jalapeño.txt']\n80 \n| \nChapter 2: Strings and Text",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": 2,
      "content": ">>> os.listdir(b'.')         # Byte string (names left as bytes)\n[b'jalapen\\xcc\\x83o.txt']\n>>>\nNotice in the last part of this example how giving a byte string as the directory name\ncaused the resulting filenames to be returned as undecoded bytes. The filename shown\nin the directory listing contains raw UTF-8 encoding. See Recipe 5.15 for some related\nissues concerning filenames.\nAs a final comment, some programmers might be inclined to use byte strings as an\nalternative to text strings due to a possible performance improvement. Although it’s\ntrue that manipulating bytes tends to be slightly more efficient than text (due to the\ninherent overhead related to Unicode), doing so usually leads to very messy and noni‐\ndiomatic code. You’ll often find that byte strings don’t play well with a lot of other parts\nof Python, and that you end up having to perform all sorts of manual encoding/decoding\noperations yourself to get things to work right. Frankly, if you’re working with text, use\nnormal text strings in your program, not byte strings.\n2.20. Performing Text Operations on Byte Strings \n| \n81",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": 2,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 101,
      "chapter": 2,
      "content": "CHAPTER 3\nNumbers, Dates, and Times\nPerforming mathematical calculations with integers and floating-point numbers is easy\nin Python. However, if you need to perform calculations with fractions, arrays, or dates\nand times, a bit more work is required. The focus of this chapter is on such topics.\n3.1. Rounding Numerical Values\nProblem\nYou want to round a floating-point number to a fixed number of decimal places.\nSolution\nFor simple rounding, use the built-in round(value, ndigits) function. For example:\n>>> round(1.23, 1)\n1.2\n>>> round(1.27, 1)\n1.3\n>>> round(-1.27, 1)\n-1.3\n>>> round(1.25361,3)\n1.254\n>>>\nWhen a value is exactly halfway between two choices, the behavior of round is to round\nto the nearest even digit. That is, values such as 1.5 or 2.5 both get rounded to 2.\nThe number of digits given to round() can be negative, in which case rounding takes\nplace for tens, hundreds, thousands, and so on. For example:\n>>> a = 1627731\n>>> round(a, -1)\n1627730\n83",
      "content_length": 968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": 2,
      "content": ">>> round(a, -2)\n1627700\n>>> round(a, -3)\n1628000\n>>>\nDiscussion\nDon’t confuse rounding with formatting a value for output. If your goal is simply to\noutput a numerical value with a certain number of decimal places, you don’t typically\nneed to use round(). Instead, just specify the desired precision when formatting. For\nexample:\n>>> x = 1.23456\n>>> format(x, '0.2f')\n'1.23'\n>>> format(x, '0.3f')\n'1.235'\n>>> 'value is {:0.3f}'.format(x)\n'value is 1.235'\n>>>\nAlso, resist the urge to round floating-point numbers to “fix” perceived accuracy prob‐\nlems. For example, you might be inclined to do this:\n>>> a = 2.1\n>>> b = 4.2\n>>> c = a + b\n>>> c\n6.300000000000001\n>>> c = round(c, 2)      # \"Fix\" result (???)\n>>> c\n6.3\n>>>\nFor most applications involving floating point, it’s simply not necessary (or recom‐\nmended) to do this. Although there are small errors introduced into calculations, the\nbehavior of those errors are understood and tolerated. If avoiding such errors is im‐\nportant (e.g., in financial applications, perhaps), consider the use of the decimal module,\nwhich is discussed in the next recipe.\n3.2. Performing Accurate Decimal Calculations\nProblem\nYou need to perform accurate calculations with decimal numbers, and don’t want the\nsmall errors that naturally occur with floats.\n84 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": 2,
      "content": "Solution\nA well-known issue with floating-point numbers is that they can’t accurately represent\nall base-10 decimals. Moreover, even simple mathematical calculations introduce small\nerrors. For example:\n>>> a = 4.2\n>>> b = 2.1\n>>> a + b\n6.300000000000001\n>>> (a + b) == 6.3\nFalse\n>>>\nThese errors are a “feature” of the underlying CPU and the IEEE 754 arithmetic per‐\nformed by its floating-point unit. Since Python’s float data type stores data using the\nnative representation, there’s nothing you can do to avoid such errors if you write your\ncode using float instances.\nIf you want more accuracy (and are willing to give up some performance), you can use\nthe decimal module:\n>>> from decimal import Decimal\n>>> a = Decimal('4.2')\n>>> b = Decimal('2.1')\n>>> a + b\nDecimal('6.3')\n>>> print(a + b)\n6.3\n>>> (a + b) == Decimal('6.3')\nTrue\n>>>\nAt first glance, it might look a little weird (i.e., specifying numbers as strings). However,\nDecimal objects work in every way that you would expect them to (supporting all of the\nusual math operations, etc.). If you print them or use them in string formatting func‐\ntions, they look like normal numbers.\nA major feature of decimal is that it allows you to control different aspects of calcula‐\ntions, including number of digits and rounding. To do this, you create a local context\nand change its settings. For example:\n>>> from decimal import localcontext\n>>> a = Decimal('1.3')\n>>> b = Decimal('1.7')\n>>> print(a / b)\n0.7647058823529411764705882353\n>>> with localcontext() as ctx:\n...     ctx.prec = 3\n...     print(a / b)\n...\n3.2. Performing Accurate Decimal Calculations \n| \n85",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": 2,
      "content": "0.765\n>>> with localcontext() as ctx:\n...     ctx.prec = 50\n...     print(a / b)\n...\n0.76470588235294117647058823529411764705882352941176\n>>>\nDiscussion\nThe decimal module implements IBM’s “General Decimal Arithmetic Specification.”\nNeedless to say, there are a huge number of configuration options that are beyond the\nscope of this book.\nNewcomers to Python might be inclined to use the decimal module to work around\nperceived accuracy problems with the float data type. However, it’s really important\nto understand your application domain. If you’re working with science or engineering\nproblems, computer graphics, or most things of a scientific nature, it’s simply more\ncommon to use the normal floating-point type. For one, very few things in the real world\nare measured to the 17 digits of accuracy that floats provide. Thus, tiny errors introduced\nin calculations just don’t matter. Second, the performance of native floats is significantly\nfaster—something that’s important if you’re performing a large number of calculations.\nThat said, you can’t ignore the errors completely. Mathematicians have spent a lot of\ntime studying various algorithms, and some handle errors better than others. You also\nhave to be a little careful with effects due to things such as subtractive cancellation and\nadding large and small numbers together. For example:\n>>> nums = [1.23e+18, 1, -1.23e+18]\n>>> sum(nums)    # Notice how 1 disappears\n0.0\n>>>\nThis latter example can be addressed by using a more accurate implementation in \nmath.fsum():\n>>> import math\n>>> math.fsum(nums)\n1.0\n>>>\nHowever, for other algorithms, you really need to study the algorithm and understand\nits error propagation properties.\nAll of this said, the main use of the decimal module is in programs involving things\nsuch as finance. In such programs, it is extremely annoying to have small errors creep\ninto the calculation. Thus, decimal provides a way to avoid that. It is also common to\nencounter Decimal objects when Python interfaces with databases—again, especially\nwhen accessing financial data.\n86 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 2111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": 2,
      "content": "3.3. Formatting Numbers for Output\nProblem\nYou need to format a number for output, controlling the number of digits, alignment,\ninclusion of a thousands separator, and other details.\nSolution\nTo format a single number for output, use the built-in format() function. For example:\n>>> x = 1234.56789\n>>> # Two decimal places of accuracy\n>>> format(x, '0.2f')\n'1234.57'\n>>> # Right justified in 10 chars, one-digit accuracy\n>>> format(x, '>10.1f')\n'    1234.6'\n>>> # Left justified\n>>> format(x, '<10.1f')\n'1234.6    '\n>>> # Centered\n>>> format(x, '^10.1f')\n'  1234.6  '\n>>> # Inclusion of thousands separator\n>>> format(x, ',')\n'1,234.56789'\n>>> format(x, '0,.1f')\n'1,234.6'\n>>>\nIf you want to use exponential notation, change the f to an e or E, depending on the\ncase you want used for the exponential specifier. For example:\n>>> format(x, 'e')\n'1.234568e+03'\n>>> format(x, '0.2E')\n'1.23E+03'\n>>>\nThe general form of the width and precision in both cases is '[<>^]?width[,]?(.dig\nits)?' where width and digits are integers and ? signifies optional parts. The same\nformat codes are also used in the .format() method of strings. For example:\n3.3. Formatting Numbers for Output \n| \n87",
      "content_length": 1180,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": 2,
      "content": ">>> 'The value is {:0,.2f}'.format(x)\n'The value is 1,234.57'\n>>>\nDiscussion\nFormatting numbers for output is usually straightforward. The technique shown works\nfor both floating-point numbers and Decimal numbers in the decimal module.\nWhen the number of digits is restricted, values are rounded away according to the same\nrules of the round() function. For example:\n>>> x\n1234.56789\n>>> format(x, '0.1f')\n'1234.6'\n>>> format(-x, '0.1f')\n'-1234.6'\n>>>\nFormatting of values with a thousands separator is not locale aware. If you need to take\nthat into account, you might investigate functions in the locale module. You can also\nswap separator characters using the translate() method of strings. For example:\n>>> swap_separators = { ord('.'):',', ord(','):'.' }\n>>> format(x, ',').translate(swap_separators)\n'1.234,56789'\n>>>\nIn a lot of Python code, numbers are formatted using the % operator. For example:\n>>> '%0.2f' % x\n'1234.57'\n>>> '%10.1f' % x\n'    1234.6'\n>>> '%-10.1f' % x\n'1234.6    '\n>>>\nThis formatting is still acceptable, but less powerful than the more modern format()\nmethod. For example, some features (e.g., adding thousands separators) aren’t sup‐\nported when using the % operator to format numbers.\n88 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1260,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": 2,
      "content": "3.4. Working with Binary, Octal, and Hexadecimal\nIntegers\nProblem\nYou need to convert or output integers represented by binary, octal, or hexadecimal\ndigits.\nSolution\nTo convert an integer into a binary, octal, or hexadecimal text string, use the bin(),\noct(), or hex() functions, respectively:\n>>> x = 1234\n>>> bin(x)\n'0b10011010010'\n>>> oct(x)\n'0o2322'\n>>> hex(x)\n'0x4d2'\n>>>\nAlternatively, you can use the format() function if you don’t want the 0b, 0o, or 0x\nprefixes to appear. For example:\n>>> format(x, 'b')\n'10011010010'\n>>> format(x, 'o')\n'2322'\n>>> format(x, 'x')\n'4d2'\n>>>\nIntegers are signed, so if you are working with negative numbers, the output will also\ninclude a sign. For example:\n>>> x = -1234\n>>> format(x, 'b')\n'-10011010010'\n>>> format(x, 'x')\n'-4d2'\n>>>\nIf you need to produce an unsigned value instead, you’ll need to add in the maximum\nvalue to set the bit length. For example, to show a 32-bit value, use the following:\n>>> x = -1234\n>>> format(2**32 + x, 'b')\n'11111111111111111111101100101110'\n>>> format(2**32 + x, 'x')\n3.4. Working with Binary, Octal, and Hexadecimal Integers \n| \n89",
      "content_length": 1114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": 2,
      "content": "'fffffb2e'\n>>>\nTo convert integer strings in different bases, simply use the int() function with an\nappropriate base. For example:\n>>> int('4d2', 16)\n1234\n>>> int('10011010010', 2)\n1234\n>>>\nDiscussion\nFor the most part, working with binary, octal, and hexadecimal integers is straightfor‐\nward. Just remember that these conversions only pertain to the conversion of integers\nto and from a textual representation. Under the covers, there’s just one integer type.\nFinally, there is one caution for programmers who use octal. The Python syntax for\nspecifying octal values is slightly different than many other languages. For example, if\nyou try something like this, you’ll get a syntax error:\n>>> import os\n>>> os.chmod('script.py', 0755)\n  File \"<stdin>\", line 1\n    os.chmod('script.py', 0755)\n                           ^\nSyntaxError: invalid token\n>>>\nMake sure you prefix the octal value with 0o, as shown here:\n>>> os.chmod('script.py', 0o755)\n>>>\n3.5. Packing and Unpacking Large Integers from Bytes\nProblem\nYou have a byte string and you need to unpack it into an integer value. Alternatively,\nyou need to convert a large integer back into a byte string.\nSolution\nSuppose your program needs to work with a 16-element byte string that holds a 128-\nbit integer value. For example:\ndata = b'\\x00\\x124V\\x00x\\x90\\xab\\x00\\xcd\\xef\\x01\\x00#\\x004'\n90 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": 2,
      "content": "To interpret the bytes as an integer, use int.from_bytes(), and specify the byte ordering\nlike this:\n>>> len(data)\n16\n>>> int.from_bytes(data, 'little')\n69120565665751139577663547927094891008\n>>> int.from_bytes(data, 'big')\n94522842520747284487117727783387188\n>>>\nTo convert a large integer value back into a byte string, use the int.to_bytes() method,\nspecifying the number of bytes and the byte order. For example:\n>>> x = 94522842520747284487117727783387188\n>>> x.to_bytes(16, 'big')\nb'\\x00\\x124V\\x00x\\x90\\xab\\x00\\xcd\\xef\\x01\\x00#\\x004'\n>>> x.to_bytes(16, 'little')\nb'4\\x00#\\x00\\x01\\xef\\xcd\\x00\\xab\\x90x\\x00V4\\x12\\x00'\n>>>\nDiscussion\nConverting large integer values to and from byte strings is not a common operation.\nHowever, it sometimes arises in certain application domains, such as cryptography or\nnetworking. For instance, IPv6 network addresses are represented as 128-bit integers.\nIf you are writing code that needs to pull such values out of a data record, you might\nface this problem.\nAs an alternative to this recipe, you might be inclined to unpack values using the struct\nmodule, as described in Recipe 6.11. This works, but the size of integers that can be\nunpacked with struct is limited. Thus, you would need to unpack multiple values and\ncombine them to create the final value. For example:\n>>> data\nb'\\x00\\x124V\\x00x\\x90\\xab\\x00\\xcd\\xef\\x01\\x00#\\x004'\n>>> import struct\n>>> hi, lo = struct.unpack('>QQ', data)\n>>> (hi << 64) + lo\n94522842520747284487117727783387188\n>>>\nThe specification of the byte order (little or big) just indicates whether the bytes that\nmake up the integer value are listed from the least to most significant or the other way\naround. This is easy to view using a carefully crafted hexadecimal value:\n>>> x = 0x01020304\n>>> x.to_bytes(4, 'big')\nb'\\x01\\x02\\x03\\x04'\n>>> x.to_bytes(4, 'little')\n3.5. Packing and Unpacking Large Integers from Bytes \n| \n91",
      "content_length": 1895,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": 2,
      "content": "b'\\x04\\x03\\x02\\x01'\n>>>\nIf you try to pack an integer into a byte string, but it won’t fit, you’ll get an error. You\ncan use the int.bit_length() method to determine how many bits are required to\nstore a value if needed:\n>>> x = 523 ** 23\n>>> x\n335381300113661875107536852714019056160355655333978849017944067\n>>> x.to_bytes(16, 'little')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nOverflowError: int too big to convert\n>>> x.bit_length()\n208\n>>> nbytes, rem = divmod(x.bit_length(), 8)\n>>> if rem:\n...     nbytes += 1\n...\n>>>\n>>> x.to_bytes(nbytes, 'little')\nb'\\x03X\\xf1\\x82iT\\x96\\xac\\xc7c\\x16\\xf3\\xb9\\xcf...\\xd0'\n>>>\n3.6. Performing Complex-Valued Math\nProblem\nYour code for interacting with the latest web authentication scheme has encountered a\nsingularity and your only solution is to go around it in the complex plane. Or maybe\nyou just need to perform some calculations using complex numbers.\nSolution\nComplex numbers can be specified using the complex(real, imag) function or by\nfloating-point numbers with a j suffix. For example:\n>>> a = complex(2, 4)\n>>> b = 3 - 5j\n>>> a\n(2+4j)\n>>> b\n(3-5j)\n>>>\nThe real, imaginary, and conjugate values are easy to obtain, as shown here:\n>>> a.real\n2.0\n92 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": 2,
      "content": ">>> a.imag\n4.0\n>>> a.conjugate()\n(2-4j)\n>>>\nIn addition, all of the usual mathematical operators work:\n>>> a + b\n(5-1j)\n>>> a * b\n(26+2j)\n>>> a / b\n(-0.4117647058823529+0.6470588235294118j)\n>>> abs(a)\n4.47213595499958\n>>>\nTo perform additional complex-valued functions such as sines, cosines, or square roots,\nuse the cmath module:\n>>> import cmath\n>>> cmath.sin(a)\n(24.83130584894638-11.356612711218174j)\n>>> cmath.cos(a)\n(-11.36423470640106-24.814651485634187j)\n>>> cmath.exp(a)\n(-4.829809383269385-5.5920560936409816j)\n>>>\nDiscussion\nMost of Python’s math-related modules are aware of complex values. For example, if\nyou use numpy, it is straightforward to make arrays of complex values and perform\noperations on them:\n>>> import numpy as np\n>>> a = np.array([2+3j, 4+5j, 6-7j, 8+9j])\n>>> a\narray([ 2.+3.j,  4.+5.j,  6.-7.j,  8.+9.j])\n>>> a + 2\narray([  4.+3.j,   6.+5.j,   8.-7.j,  10.+9.j])\n>>> np.sin(a)\narray([    9.15449915  -4.16890696j,   -56.16227422 -48.50245524j,\n        -153.20827755-526.47684926j,  4008.42651446-589.49948373j])\n>>>\nPython’s standard mathematical functions do not produce complex values by default,\nso it is unlikely that such a value would accidentally show up in your code. For example:\n>>> import math\n>>> math.sqrt(-1)\nTraceback (most recent call last):\n3.6. Performing Complex-Valued Math \n| \n93",
      "content_length": 1333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": 2,
      "content": "File \"<stdin>\", line 1, in <module>\nValueError: math domain error\n>>>\nIf you want complex numbers to be produced as a result, you have to explicitly use cmath\nor declare the use of a complex type in libraries that know about them. For example:\n>>> import cmath\n>>> cmath.sqrt(-1)\n1j\n>>>\n3.7. Working with Infinity and NaNs\nProblem\nYou need to create or test for the floating-point values of infinity, negative infinity, or\nNaN (not a number).\nSolution\nPython has no special syntax to represent these special floating-point values, but they\ncan be created using float(). For example:\n>>> a = float('inf')\n>>> b = float('-inf')\n>>> c = float('nan')\n>>> a\ninf\n>>> b\n-inf\n>>> c\nnan\n>>>\nTo test for the presence of these values, use the math.isinf() and math.isnan() func‐\ntions. For example:\n>>> math.isinf(a)\nTrue\n>>> math.isnan(c)\nTrue\n>>>\nDiscussion\nFor more detailed information about these special floating-point values, you should\nrefer to the IEEE 754 specification. However, there are a few tricky details to be aware\nof, especially related to comparisons and operators.\n94 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": 2,
      "content": "Infinite values will propagate in calculations in a mathematical manner. For example:\n>>> a = float('inf')\n>>> a + 45\ninf\n>>> a * 10\ninf\n>>> 10 / a\n0.0\n>>>\nHowever, certain operations are undefined and will result in a NaN result. For example:\n>>> a = float('inf')\n>>> a/a\nnan\n>>> b = float('-inf')\n>>> a + b\nnan\n>>>\nNaN values propagate through all operations without raising an exception. For example:\n>>> c = float('nan')\n>>> c + 23\nnan\n>>> c / 2\nnan\n>>> c * 2\nnan\n>>> math.sqrt(c)\nnan\n>>>\nA subtle feature of NaN values is that they never compare as equal. For example:\n>>> c = float('nan')\n>>> d = float('nan')\n>>> c == d\nFalse\n>>> c is d\nFalse\n>>>\nBecause of this, the only safe way to test for a NaN value is to use math.isnan(), as\nshown in this recipe.\nSometimes programmers want to change Python’s behavior to raise exceptions when\noperations result in an infinite or NaN result. The fpectl module can be used to adjust\nthis behavior, but it is not enabled in a standard Python build, it’s platform-dependent,\nand really only intended for expert-level programmers. See the online Python docu‐\nmentation for further details.\n3.7. Working with Infinity and NaNs \n| \n95",
      "content_length": 1176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": 2,
      "content": "3.8. Calculating with Fractions\nProblem\nYou have entered a time machine and suddenly find yourself working on elementary-\nlevel homework problems involving fractions. Or perhaps you’re writing code to make\ncalculations involving measurements made in your wood shop.\nSolution\nThe fractions module can be used to perform mathematical calculations involving\nfractions. For example:\n>>> from fractions import Fraction\n>>> a = Fraction(5, 4)\n>>> b = Fraction(7, 16)\n>>> print(a + b)\n27/16\n>>> print(a * b)\n35/64\n>>> # Getting numerator/denominator\n>>> c = a * b\n>>> c.numerator\n35\n>>> c.denominator\n64\n>>> # Converting to a float\n>>> float(c)\n0.546875\n>>> # Limiting the denominator of a value\n>>> print(c.limit_denominator(8))\n4/7\n>>> # Converting a float to a fraction\n>>> x = 3.75\n>>> y = Fraction(*x.as_integer_ratio())\n>>> y\nFraction(15, 4)\n>>>\nDiscussion\nCalculating with fractions doesn’t arise often in most programs, but there are situations\nwhere it might make sense to use them. For example, allowing a program to accept units\nof measurement in fractions and performing calculations with them in that form might\nalleviate the need for a user to manually make conversions to decimals or floats.\n96 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": 2,
      "content": "3.9. Calculating with Large Numerical Arrays\nProblem\nYou need to perform calculations on large numerical datasets, such as arrays or grids.\nSolution\nFor any heavy computation involving arrays, use the NumPy library. The major feature\nof NumPy is that it gives Python an array object that is much more efficient and better\nsuited for mathematical calculation than a standard Python list. Here is a short example\nillustrating important behavioral differences between lists and NumPy arrays:\n>>> # Python lists\n>>> x = [1, 2, 3, 4]\n>>> y = [5, 6, 7, 8]\n>>> x * 2\n[1, 2, 3, 4, 1, 2, 3, 4]\n>>> x + 10\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can only concatenate list (not \"int\") to list\n>>> x + y\n[1, 2, 3, 4, 5, 6, 7, 8]\n>>> # Numpy arrays\n>>> import numpy as np\n>>> ax = np.array([1, 2, 3, 4])\n>>> ay = np.array([5, 6, 7, 8])\n>>> ax * 2\narray([2, 4, 6, 8])\n>>> ax + 10\narray([11, 12, 13, 14])\n>>> ax + ay\narray([ 6,  8, 10, 12])\n>>> ax * ay\narray([ 5, 12, 21, 32])\n>>>\nAs you can see, basic mathematical operations involving arrays behave differently.\nSpecifically, scalar operations (e.g., ax * 2 or ax + 10) apply the operation on an\nelement-by-element basis. In addition, performing math operations when both\noperands are arrays applies the operation to all elements and produces a new array.\nThe fact that math operations apply to all of the elements simultaneously makes it very\neasy and fast to compute functions across an entire array. For example, if you want to\ncompute the value of a polynomial:\n>>> def f(x):\n...     return 3*x**2 - 2*x + 7\n3.9. Calculating with Large Numerical Arrays \n| \n97",
      "content_length": 1644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": 2,
      "content": "...\n>>> f(ax)\narray([ 8, 15, 28, 47])\n>>>\nNumPy provides a collection of “universal functions” that also allow for array opera‐\ntions. These are replacements for similar functions normally found in the math module.\nFor example:\n>>> np.sqrt(ax)\narray([ 1.        ,  1.41421356,  1.73205081,  2.        ])\n>>> np.cos(ax)\narray([ 0.54030231, -0.41614684, -0.9899925 , -0.65364362])\n>>>\nUsing universal functions can be hundreds of times faster than looping over the array\nelements one at a time and performing calculations using functions in the math module.\nThus, you should prefer their use whenever possible.\nUnder the covers, NumPy arrays are allocated in the same manner as in C or Fortran.\nNamely, they are large, contiguous memory regions consisting of a homogenous data\ntype. Because of this, it’s possible to make arrays much larger than anything you would\nnormally put into a Python list. For example, if you want to make a two-dimensional\ngrid of 10,000 by 10,000 floats, it’s not an issue:\n>>> grid = np.zeros(shape=(10000,10000), dtype=float)\n>>> grid\narray([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n>>>\nAll of the usual operations still apply to all of the elements simultaneously:\n>>> grid += 10\n>>> grid\narray([[ 10.,  10.,  10., ...,  10.,  10.,  10.],\n       [ 10.,  10.,  10., ...,  10.,  10.,  10.],\n       [ 10.,  10.,  10., ...,  10.,  10.,  10.],\n       ...,\n       [ 10.,  10.,  10., ...,  10.,  10.,  10.],\n       [ 10.,  10.,  10., ...,  10.,  10.,  10.],\n       [ 10.,  10.,  10., ...,  10.,  10.,  10.]])\n>>> np.sin(grid)\narray([[-0.54402111, -0.54402111, -0.54402111, ..., -0.54402111,\n        -0.54402111, -0.54402111],\n       [-0.54402111, -0.54402111, -0.54402111, ..., -0.54402111,\n        -0.54402111, -0.54402111],\n98 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": 2,
      "content": "[-0.54402111, -0.54402111, -0.54402111, ..., -0.54402111,\n        -0.54402111, -0.54402111],\n       ...,\n       [-0.54402111, -0.54402111, -0.54402111, ..., -0.54402111,\n        -0.54402111, -0.54402111],\n       [-0.54402111, -0.54402111, -0.54402111, ..., -0.54402111,\n        -0.54402111, -0.54402111],\n       [-0.54402111, -0.54402111, -0.54402111, ..., -0.54402111,\n        -0.54402111, -0.54402111]])\n>>>\nOne extremely notable aspect of NumPy is the manner in which it extends Python’s list\nindexing functionality—especially with multidimensional arrays. To illustrate, make a\nsimple two-dimensional array and try some experiments:\n>>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n>>> a\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n>>> # Select row 1\n>>> a[1]\narray([5, 6, 7, 8])\n>>> # Select column 1\n>>> a[:,1]\narray([ 2,  6, 10])\n>>> # Select a subregion and change it\n>>> a[1:3, 1:3]\narray([[ 6,  7],\n       [10, 11]])\n>>> a[1:3, 1:3] += 10\n>>> a\narray([[ 1,  2,  3,  4],\n       [ 5, 16, 17,  8],\n       [ 9, 20, 21, 12]])\n>>> # Broadcast a row vector across an operation on all rows\n>>> a + [100, 101, 102, 103]\narray([[101, 103, 105, 107],\n       [105, 117, 119, 111],\n       [109, 121, 123, 115]])\n>>> a\narray([[ 1,  2,  3,  4],\n       [ 5, 16, 17,  8],\n       [ 9, 20, 21, 12]])\n>>> # Conditional assignment on an array\n>>> np.where(a < 10, a, 10)\narray([[ 1,  2,  3,  4],\n3.9. Calculating with Large Numerical Arrays \n| \n99",
      "content_length": 1483,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": 2,
      "content": "[ 5, 10, 10,  8],\n       [ 9, 10, 10, 10]])\n>>>\nDiscussion\nNumPy is the foundation for a huge number of science and engineering libraries in\nPython. It is also one of the largest and most complicated modules in widespread use.\nThat said, it’s still possible to accomplish useful things with NumPy by starting with\nsimple examples and playing around.\nOne note about usage is that it is relatively common to use the statement import numpy\nas np, as shown in the solution. This simply shortens the name to something that’s more\nconvenient to type over and over again in your program.\nFor more information, you definitely need to visit http://www.numpy.org. \n3.10. Performing Matrix and Linear Algebra Calculations\nProblem\nYou need to perform matrix and linear algebra operations, such as matrix multiplication,\nfinding determinants, solving linear equations, and so on.\nSolution\nThe NumPy library has a matrix object that can be used for this purpose. Matrices are\nsomewhat similar to the array objects described in Recipe 3.9, but follow linear algebra\nrules for computation. Here is an example that illustrates a few essential features:\n>>> import numpy as np\n>>> m = np.matrix([[1,-2,3],[0,4,5],[7,8,-9]])\n>>> m\nmatrix([[ 1, -2,  3],\n        [ 0,  4,  5],\n        [ 7,  8, -9]])\n>>> # Return transpose\n>>> m.T\nmatrix([[ 1,  0,  7],\n        [-2,  4,  8],\n        [ 3,  5, -9]])\n>>> # Return inverse\n>>> m.I\nmatrix([[ 0.33043478, -0.02608696,  0.09565217],\n        [-0.15217391,  0.13043478,  0.02173913],\n        [ 0.12173913,  0.09565217, -0.0173913 ]])\n100 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": 2,
      "content": ">>> # Create a vector and multiply\n>>> v = np.matrix([[2],[3],[4]])\n>>> v\nmatrix([[2],\n        [3],\n        [4]])\n>>> m * v\nmatrix([[ 8],\n        [32],\n        [ 2]])\n>>>\nMore operations can be found in the numpy.linalg subpackage. For example:\n>>> import numpy.linalg\n>>> # Determinant\n>>> numpy.linalg.det(m)\n-229.99999999999983\n>>> # Eigenvalues\n>>> numpy.linalg.eigvals(m)\narray([-13.11474312,   2.75956154,   6.35518158])\n>>> # Solve for x in mx = v\n>>> x = numpy.linalg.solve(m, v)\n>>> x\nmatrix([[ 0.96521739],\n        [ 0.17391304],\n        [ 0.46086957]])\n>>> m * x\nmatrix([[ 2.],\n        [ 3.],\n        [ 4.]])\n>>> v\nmatrix([[2],\n        [3],\n        [4]])\n>>>\nDiscussion\nLinear algebra is obviously a huge topic that’s far beyond the scope of this cookbook.\nHowever, if you need to manipulate matrices and vectors, NumPy is a good starting\npoint. Visit http://www.numpy.org for more detailed information.\n3.10. Performing Matrix and Linear Algebra Calculations \n| \n101",
      "content_length": 978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": 2,
      "content": "3.11. Picking Things at Random\nProblem\nYou want to pick random items out of a sequence or generate random numbers.\nSolution\nThe random module has various functions for random numbers and picking random\nitems. For example, to pick a random item out of a sequence, use random.choice():\n>>> import random\n>>> values = [1, 2, 3, 4, 5, 6]\n>>> random.choice(values)\n2\n>>> random.choice(values)\n3\n>>> random.choice(values)\n1\n>>> random.choice(values)\n4\n>>> random.choice(values)\n6\n>>>\nTo take a sampling of N items where selected items are removed from further consid‐\neration, use random.sample() instead:\n>>> random.sample(values, 2)\n[6, 2]\n>>> random.sample(values, 2)\n[4, 3]\n>>> random.sample(values, 3)\n[4, 3, 1]\n>>> random.sample(values, 3)\n[5, 4, 1]\n>>>\nIf you simply want to shuffle items in a sequence in place, use random.shuffle():\n>>> random.shuffle(values)\n>>> values\n[2, 4, 6, 5, 3, 1]\n>>> random.shuffle(values)\n>>> values\n[3, 5, 2, 1, 6, 4]\n>>>\nTo produce random integers, use random.randint():\n>>> random.randint(0,10)\n2\n102 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": 3,
      "content": ">>> random.randint(0,10)\n5\n>>> random.randint(0,10)\n0\n>>> random.randint(0,10)\n7\n>>> random.randint(0,10)\n10\n>>> random.randint(0,10)\n3\n>>>\nTo produce uniform floating-point values in the range 0 to 1, use random.random():\n>>> random.random()\n0.9406677561675867\n>>> random.random()\n0.133129581343897\n>>> random.random()\n0.4144991136919316\n>>>\nTo get N random-bits expressed as an integer, use random.getrandbits():\n>>> random.getrandbits(200)\n335837000776573622800628485064121869519521710558559406913275\n>>>\nDiscussion\nThe random module computes random numbers using the Mersenne Twister algorithm.\nThis is a deterministic algorithm, but you can alter the initial seed by using the\nrandom.seed() function. For example:\nrandom.seed()            # Seed based on system time or os.urandom()\nrandom.seed(12345)       # Seed based on integer given\nrandom.seed(b'bytedata') # Seed based on byte data\nIn addition to the functionality shown, random() includes functions for uniform, Gaus‐\nsian, and other probabality distributions. For example, random.uniform() computes\nuniformly distributed numbers, and random.gauss() computes normally distributed\nnumbers. Consult the documentation for information on other supported distributions.\nFunctions in random() should not be used in programs related to cryptography. If you\nneed such functionality, consider using functions in the ssl module instead. For ex‐\nample, ssl.RAND_bytes() can be used to generate a cryptographically secure sequence\nof random bytes.\n3.11. Picking Things at Random \n| \n103",
      "content_length": 1537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": 3,
      "content": "3.12. Converting Days to Seconds, and Other Basic Time\nConversions\nProblem\nYou have code that needs to perform simple time conversions, like days to seconds,\nhours to minutes, and so on.\nSolution\nTo perform conversions and arithmetic involving different units of time, use the date\ntime module. For example, to represent an interval of time, create a timedelta instance,\nlike this:\n>>> from datetime import timedelta\n>>> a = timedelta(days=2, hours=6)\n>>> b = timedelta(hours=4.5)\n>>> c = a + b\n>>> c.days\n2\n>>> c.seconds\n37800\n>>> c.seconds / 3600\n10.5\n>>> c.total_seconds() / 3600\n58.5\n>>>\nIf you need to represent specific dates and times, create datetime instances and use the\nstandard mathematical operations to manipulate them. For example:\n>>> from datetime import datetime\n>>> a = datetime(2012, 9, 23)\n>>> print(a + timedelta(days=10))\n2012-10-03 00:00:00\n>>>\n>>> b = datetime(2012, 12, 21)\n>>> d = b - a\n>>> d.days\n89\n>>> now = datetime.today()\n>>> print(now)\n2012-12-21 14:54:43.094063\n>>> print(now + timedelta(minutes=10))\n2012-12-21 15:04:43.094063\n>>>\nWhen making calculations, it should be noted that datetime is aware of leap years. For\nexample:\n104 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": 3,
      "content": ">>> a = datetime(2012, 3, 1)\n>>> b = datetime(2012, 2, 28)\n>>> a - b\ndatetime.timedelta(2)\n>>> (a - b).days\n2\n>>> c = datetime(2013, 3, 1)\n>>> d = datetime(2013, 2, 28)\n>>> (c - d).days\n1\n>>>\nDiscussion\nFor most basic date and time manipulation problems, the datetime module will suffice.\nIf you need to perform more complex date manipulations, such as dealing with time\nzones, fuzzy time ranges, calculating the dates of holidays, and so forth, look at the\ndateutil module.\nTo illustrate, many similar time calculations can be performed with the dateutil.rel\nativedelta() function. However, one notable feature is that it fills in some gaps per‐\ntaining to the handling of months (and their differing number of days). For instance:\n>>> a = datetime(2012, 9, 23)\n>>> a + timedelta(months=1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'months' is an invalid keyword argument for this function\n>>>\n>>> from dateutil.relativedelta import relativedelta\n>>> a + relativedelta(months=+1)\ndatetime.datetime(2012, 10, 23, 0, 0)\n>>> a + relativedelta(months=+4)\ndatetime.datetime(2013, 1, 23, 0, 0)\n>>>\n>>> # Time between two dates\n>>> b = datetime(2012, 12, 21)\n>>> d = b - a\n>>> d\ndatetime.timedelta(89)\n>>> d = relativedelta(b, a)\n>>> d\nrelativedelta(months=+2, days=+28)\n>>> d.months\n2\n>>> d.days\n28\n>>>\n3.12. Converting Days to Seconds, and Other Basic Time Conversions \n| \n105",
      "content_length": 1414,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": 3,
      "content": "3.13. Determining Last Friday’s Date\nProblem\nYou want a general solution for finding a date for the last occurrence of a day of the\nweek. Last Friday, for example.\nSolution\nPython’s datetime module has utility functions and classes to help perform calculations\nlike this. A decent, generic solution to this problem looks like this:\nfrom datetime import datetime, timedelta\nweekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday',\n            'Friday', 'Saturday', 'Sunday']\ndef get_previous_byday(dayname, start_date=None):\n    if start_date is None:\n        start_date = datetime.today()\n    day_num = start_date.weekday()\n    day_num_target = weekdays.index(dayname)\n    days_ago = (7 + day_num - day_num_target) % 7\n    if days_ago == 0:\n        days_ago = 7\n    target_date = start_date - timedelta(days=days_ago)\n    return target_date\nUsing this in an interpreter session would look like this:\n>>> datetime.today()  # For reference\ndatetime.datetime(2012, 8, 28, 22, 4, 30, 263076)\n>>> get_previous_byday('Monday')\ndatetime.datetime(2012, 8, 27, 22, 3, 57, 29045)\n>>> get_previous_byday('Tuesday') # Previous week, not today\ndatetime.datetime(2012, 8, 21, 22, 4, 12, 629771)\n>>> get_previous_byday('Friday')\ndatetime.datetime(2012, 8, 24, 22, 5, 9, 911393)\n>>>\nThe optional start_date can be supplied using another datetime instance. For\nexample:\n>>> get_previous_byday('Sunday', datetime(2012, 12, 21))\ndatetime.datetime(2012, 12, 16, 0, 0)\n>>>\n106 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1497,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": 3,
      "content": "Discussion\nThis recipe works by mapping the start date and the target date to their numeric position\nin the week (with Monday as day 0). Modular arithmetic is then used to figure out how\nmany days ago the target date last occurred. From there, the desired date is calculated\nfrom the start date by subtracting an appropriate timedelta instance.\nIf you’re performing a lot of date calculations like this, you may be better off installing\nthe python-dateutil package instead. For example, here is an example of performing\nthe same calculation using the relativedelta() function from dateutil:\n>>> from datetime import datetime\n>>> from dateutil.relativedelta import relativedelta\n>>> from dateutil.rrule import *\n>>> d = datetime.now()\n>>> print(d)\n2012-12-23 16:31:52.718111\n>>> # Next Friday\n>>> print(d + relativedelta(weekday=FR))\n2012-12-28 16:31:52.718111\n>>>\n>>> # Last Friday\n>>> print(d + relativedelta(weekday=FR(-1)))\n2012-12-21 16:31:52.718111\n>>>\n3.14. Finding the Date Range for the Current Month\nProblem\nYou have some code that needs to loop over each date in the current month, and want\nan efficient way to calculate that date range.\nSolution\nLooping over the dates doesn’t require building a list of all the dates ahead of time. You\ncan just calculate the starting and stopping date in the range, then use datetime.time\ndelta objects to increment the date as you go.\nHere’s a function that takes any datetime object, and returns a tuple containing the first\ndate of the month and the starting date of the next month:\nfrom datetime import datetime, date, timedelta\nimport calendar\ndef get_month_range(start_date=None):\n    if start_date is None:\n3.14. Finding the Date Range for the Current Month \n| \n107",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": 3,
      "content": "start_date = date.today().replace(day=1)\n    _, days_in_month = calendar.monthrange(start_date.year, start_date.month)\n    end_date = start_date + timedelta(days=days_in_month)\n    return (start_date, end_date)\nWith this in place, it’s pretty simple to loop over the date range:\n>>> a_day = timedelta(days=1)\n>>> first_day, last_day = get_month_range()\n>>> while first_day < last_day:\n...     print(first_day)\n...     first_day += a_day\n...\n2012-08-01\n2012-08-02\n2012-08-03\n2012-08-04\n2012-08-05\n2012-08-06\n2012-08-07\n2012-08-08\n2012-08-09\n#... and so on...\nDiscussion\nThis recipe works by first calculating a date correponding to the first day of the month.\nA quick way to do this is to use the replace() method of a date or datetime object to\nsimply set the days attribute to 1. One nice thing about the replace() method is that\nit creates the same kind of object that you started with. Thus, if the input was a date\ninstance, the result is a date. Likewise, if the input was a datetime instance, you get a\ndatetime instance.\nAfter that, the calendar.monthrange() function is used to find out how many days are\nin the month in question. Any time you need to get basic information about calendars,\nthe calendar module can be useful. monthrange() is only one such function that returns\na tuple containing the day of the week along with the number of days in the month.\nOnce the number of days in the month is known, the ending date is calculated by adding\nan appropriate timedelta to the starting date. It’s subtle, but an important aspect of this\nrecipe is that the ending date is not to be included in the range (it is actually the first\nday of the next month). This mirrors the behavior of Python’s slices and range opera‐\ntions, which also never include the end point.\nTo loop over the date range, standard math and comparison operators are used. For\nexample, timedelta instances can be used to increment the date. The < operator is used\nto check whether a date comes before the ending date.\n108 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": 3,
      "content": "Ideally, it would be nice to create a function that works like the built-in range() function,\nbut for dates. Fortunately, this is extremely easy to implement using a generator:\ndef date_range(start, stop, step):\n    while start < stop:\n        yield start\n        start += step\nHere is an example of it in use:\n>>> for d in date_range(datetime(2012, 9, 1), datetime(2012,10,1),\n                        timedelta(hours=6)):\n...     print(d)\n...\n2012-09-01 00:00:00\n2012-09-01 06:00:00\n2012-09-01 12:00:00\n2012-09-01 18:00:00\n2012-09-02 00:00:00\n2012-09-02 06:00:00\n...\n>>>\nAgain, a major part of the ease of implementation is that dates and times can be ma‐\nnipulated using standard math and comparison operators.\n3.15. Converting Strings into Datetimes\nProblem\nYour application receives temporal data in string format, but you want to convert those\nstrings into datetime objects in order to perform nonstring operations on them.\nSolution\nPython’s standard datetime module is typically the easy solution for this. For example:\n>>> from datetime import datetime\n>>> text = '2012-09-20'\n>>> y = datetime.strptime(text, '%Y-%m-%d')\n>>> z = datetime.now()\n>>> diff = z - y\n>>> diff\ndatetime.timedelta(3, 77824, 177393)\n>>>\nDiscussion\nThe datetime.strptime() method supports a host of formatting codes, like %Y for the\nfour-digit year and %m for the two-digit month. It’s also worth noting that these format‐\n3.15. Converting Strings into Datetimes \n| \n109",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": 3,
      "content": "ting placeholders also work in reverse, in case you need to represent a datetime object\nin string output and make it look nice.\nFor example, let’s say you have some code that generates a datetime object, but you need\nto format a nice, human-readable date to put in the header of an auto-generated letter\nor report:\n>>> z\ndatetime.datetime(2012, 9, 23, 21, 37, 4, 177393)\n>>> nice_z = datetime.strftime(z, '%A %B %d, %Y')\n>>> nice_z\n'Sunday September 23, 2012'\n>>>\nIt’s worth noting that the performance of strptime() is often much worse than you\nmight expect, due to the fact that it’s written in pure Python and it has to deal with all\nsorts of system locale settings. If you are parsing a lot of dates in your code and you\nknow the precise format, you will probably get much better performance by cooking\nup a custom solution instead. For example, if you knew that the dates were of the form\n“YYYY-MM-DD,” you could write a function like this:\nfrom datetime import datetime\ndef parse_ymd(s):\n    year_s, mon_s, day_s = s.split('-')\n    return datetime(int(year_s), int(mon_s), int(day_s))\nWhen tested, this function runs over seven times faster than datetime.strptime().\nThis is probably something to consider if you’re processing large amounts of data in‐\nvolving dates.\n3.16. Manipulating Dates Involving Time Zones\nProblem\nYou had a conference call scheduled for December 21, 2012, at 9:30 a.m. in Chicago. At\nwhat local time did your friend in Bangalore, India, have to show up to attend?\nSolution\nFor almost any problem involving time zones, you should use the pytz module. This\npackage provides the Olson time zone database, which is the de facto standard for time\nzone information found in many languages and operating systems.\nA major use of pytz is in localizing simple dates created with the datetime library. For\nexample, here is how you would represent a date in Chicago time:\n>>> from datetime import datetime\n>>> from pytz import timezone\n110 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": 3,
      "content": ">>> d = datetime(2012, 12, 21, 9, 30, 0)\n>>> print(d)\n2012-12-21 09:30:00\n>>>\n>>> # Localize the date for Chicago\n>>> central = timezone('US/Central')\n>>> loc_d = central.localize(d)\n>>> print(loc_d)\n2012-12-21 09:30:00-06:00\n>>>\nOnce the date has been localized, it can be converted to other time zones. To find the\nsame time in Bangalore, you would do this:\n>>> # Convert to Bangalore time\n>>> bang_d = loc_d.astimezone(timezone('Asia/Kolkata'))\n>>> print(bang_d)\n2012-12-21 21:00:00+05:30\n>>>\nIf you are going to perform arithmetic with localized dates, you need to be particularly\naware of daylight saving transitions and other details. For example, in 2013, U.S. stan‐\ndard daylight saving time started on March 13, 2:00 a.m. local time (at which point, time\nskipped ahead one hour). If you’re performing naive arithmetic, you’ll get it wrong. For\nexample:\n>>> d = datetime(2013, 3, 10, 1, 45)\n>>> loc_d = central.localize(d)\n>>> print(loc_d)\n2013-03-10 01:45:00-06:00\n>>> later = loc_d + timedelta(minutes=30)\n>>> print(later)\n2013-03-10 02:15:00-06:00       # WRONG! WRONG!\n>>>\nThe answer is wrong because it doesn’t account for the one-hour skip in the local time.\nTo fix this, use the normalize() method of the time zone. For example:\n>>> from datetime import timedelta\n>>> later = central.normalize(loc_d + timedelta(minutes=30))\n>>> print(later)\n2013-03-10 03:15:00-05:00\n>>>\nDiscussion\nTo keep your head from completely exploding, a common strategy for localized date\nhandling is to convert all dates to UTC time and to use that for all internal storage and\nmanipulation. For example:\n3.16. Manipulating Dates Involving Time Zones \n| \n111",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": 3,
      "content": ">>> print(loc_d)\n2013-03-10 01:45:00-06:00\n>>> utc_d = loc_d.astimezone(pytz.utc)\n>>> print(utc_d)\n2013-03-10 07:45:00+00:00\n>>>\nOnce in UTC, you don’t have to worry about issues related to daylight saving time and\nother matters. Thus, you can simply perform normal date arithmetic as before. Should\nyou want to output the date in localized time, just convert it to the appropriate time\nzone afterward. For example:\n>>> later_utc = utc_d + timedelta(minutes=30)\n>>> print(later_utc.astimezone(central))\n2013-03-10 03:15:00-05:00\n>>>\nOne issue in working with time zones is simply figuring out what time zone names to\nuse. For example, in this recipe, how was it known that “Asia/Kolkata” was the correct\ntime zone name for India? To find out, you can consult the pytz.country_timezones\ndictionary using the ISO 3166 country code as a key. For example:\n>>> pytz.country_timezones['IN']\n['Asia/Kolkata']\n>>>\nBy the time you read this, it’s possible that the pytz module will be\ndeprecated in favor of improved time zone support, as described in PEP\n431. Many of the same issues will still apply, however (e.g., advice using\nUTC dates, etc.).\n112 \n| \nChapter 3: Numbers, Dates, and Times",
      "content_length": 1184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": 3,
      "content": "CHAPTER 4\nIterators and Generators\nIteration is one of Python’s strongest features. At a high level, you might simply view\niteration as a way to process items in a sequence. However, there is so much more that\nis possible, such as creating your own iterator objects, applying useful iteration patterns\nin the itertools module, making generator functions, and so forth. This chapter aims\nto address common problems involving iteration.\n4.1. Manually Consuming an Iterator\nProblem\nYou need to process items in an iterable, but for whatever reason, you can’t or don’t want\nto use a for loop.\nSolution\nTo manually consume an iterable, use the next() function and write your code to catch\nthe StopIteration exception. For example, this example manually reads lines from a\nfile:\nwith open('/etc/passwd') as f:\n    try:\n        while True:\n            line = next(f)\n            print(line, end='')\n    except StopIteration:\n        pass\nNormally, StopIteration is used to signal the end of iteration. However, if you’re using\nnext() manually (as shown), you can also instruct it to return a terminating value, such\nas None, instead. For example:\n113",
      "content_length": 1143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": 3,
      "content": "with open('/etc/passwd') as f:\n     while True:\n         line = next(f, None)\n         if line is None:\n             break\n         print(line, end='')\nDiscussion\nIn most cases, the for statement is used to consume an iterable. However, every now\nand then, a problem calls for more precise control over the underlying iteration mech‐\nanism. Thus, it is useful to know what actually happens.\nThe following interactive example illustrates the basic mechanics of what happens dur‐\ning iteration:\n>>> items = [1, 2, 3]\n>>> # Get the iterator\n>>> it = iter(items)     # Invokes items.__iter__()\n>>> # Run the iterator\n>>> next(it)             # Invokes it.__next__()\n1\n>>> next(it)\n2\n>>> next(it)\n3\n>>> next(it)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n>>>\nSubsequent recipes in this chapter expand on iteration techniques, and knowledge of\nthe basic iterator protocol is assumed. Be sure to tuck this first recipe away in your\nmemory.\n4.2. Delegating Iteration\nProblem\nYou have built a custom container object that internally holds a list, tuple, or some other\niterable. You would like to make iteration work with your new container.\nSolution\nTypically, all you need to do is define an __iter__() method that delegates iteration to\nthe internally held container. For example:\n114 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": 3,
      "content": "class Node:\n    def __init__(self, value):\n        self._value = value\n        self._children = []\n    def __repr__(self):\n        return 'Node({!r})'.format(self._value)\n    def add_child(self, node):\n        self._children.append(node)\n    def __iter__(self):\n        return iter(self._children)\n# Example\nif __name__ == '__main__':\n    root = Node(0)\n    child1 = Node(1)\n    child2 = Node(2)\n    root.add_child(child1)\n    root.add_child(child2)\n    for ch in root:\n        print(ch)\n    # Outputs Node(1), Node(2)\nIn this code, the __iter__() method simply forwards the iteration request to the in‐\nternally held _children attribute.\nDiscussion\nPython’s iterator protocol requires __iter__() to return a special iterator object that\nimplements a __next__() method to carry out the actual iteration. If all you are doing\nis iterating over the contents of another container, you don’t really need to worry about\nthe underlying details of how it works. All you need to do is to forward the iteration\nrequest along.\nThe use of the iter() function here is a bit of a shortcut that cleans up the code. iter(s)\nsimply returns the underlying iterator by calling s.__iter__(), much in the same way\nthat len(s) invokes s.__len__().\n4.3. Creating New Iteration Patterns with Generators\nProblem\nYou want to implement a custom iteration pattern that’s different than the usual built-\nin functions (e.g., range(), reversed(), etc.).\n4.3. Creating New Iteration Patterns with Generators \n| \n115",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": 3,
      "content": "Solution\nIf you want to implement a new kind of iteration pattern, define it using a generator\nfunction. Here’s a generator that produces a range of floating-point numbers:\ndef frange(start, stop, increment):\n    x = start\n    while x < stop:\n        yield x\n        x += increment\nTo use such a function, you iterate over it using a for loop or use it with some other\nfunction that consumes an iterable (e.g., sum(), list(), etc.). For example:\n>>> for n in frange(0, 4, 0.5):\n...     print(n)\n...\n0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n>>> list(frange(0, 1, 0.125))\n[0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875]\n>>>\nDiscussion\nThe mere presence of the yield statement in a function turns it into a generator. Unlike\na normal function, a generator only runs in response to iteration. Here’s an experiment\nyou can try to see the underlying mechanics of how such a function works:\n>>> def countdown(n):\n...     print('Starting to count from', n)\n...     while n > 0:\n...             yield n\n...             n -= 1\n...     print('Done!')\n...\n>>> # Create the generator, notice no output appears\n>>> c = countdown(3)\n>>> c\n<generator object countdown at 0x1006a0af0>\n>>> # Run to first yield and emit a value\n>>> next(c)\nStarting to count from 3\n3\n116 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": 3,
      "content": ">>> # Run to the next yield\n>>> next(c)\n2\n>>> # Run to next yield\n>>> next(c)\n1\n>>> # Run to next yield (iteration stops)\n>>> next(c)\nDone!\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n>>>\nThe key feature is that a generator function only runs in response to “next” operations\ncarried out in iteration. Once a generator function returns, iteration stops. However,\nthe for statement that’s usually used to iterate takes care of these details, so you don’t\nnormally need to worry about them.\n4.4. Implementing the Iterator Protocol\nProblem\nYou are building custom objects on which you would like to support iteration, but would\nlike an easy way to implement the iterator protocol.\nSolution\nBy far, the easiest way to implement iteration on an object is to use a generator function.\nIn Recipe 4.2, a Node class was presented for representing tree structures. Perhaps you\nwant to implement an iterator that traverses nodes in a depth-first pattern. Here is how\nyou could do it:\nclass Node:\n    def __init__(self, value):\n        self._value = value\n        self._children = []\n    def __repr__(self):\n        return 'Node({!r})'.format(self._value)\n    def add_child(self, node):\n        self._children.append(node)\n    def __iter__(self):\n        return iter(self._children)\n4.4. Implementing the Iterator Protocol \n| \n117",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": 3,
      "content": "def depth_first(self):\n        yield self\n        for c in self:\n            yield from c.depth_first()\n# Example\nif __name__ == '__main__':\n    root = Node(0)\n    child1 = Node(1)\n    child2 = Node(2)\n    root.add_child(child1)\n    root.add_child(child2)\n    child1.add_child(Node(3))\n    child1.add_child(Node(4))\n    child2.add_child(Node(5))\n    for ch in root.depth_first():\n        print(ch)\n    # Outputs Node(0), Node(1), Node(3), Node(4), Node(2), Node(5)\nIn this code, the depth_first() method is simple to read and describe. It first yields\nitself and then iterates over each child yielding the items produced by the child’s\ndepth_first() method (using yield from).\nDiscussion\nPython’s iterator protocol requires __iter__() to return a special iterator object that\nimplements a __next__() operation and uses a StopIteration exception to signal\ncompletion. However, implementing such objects can often be a messy affair. For ex‐\nample, the following code shows an alternative implementation of the depth_first()\nmethod using an associated iterator class:\nclass Node:\n    def __init__(self, value):\n        self._value = value\n        self._children = []\n    def __repr__(self):\n        return 'Node({!r})'.format(self._value)\n    def add_child(self, other_node):\n        self._children.append(other_node)\n    def __iter__(self):\n        return iter(self._children)\n    def depth_first(self):\n        return DepthFirstIterator(self)\nclass DepthFirstIterator(object):\n118 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1519,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": 3,
      "content": "'''\n    Depth-first traversal\n    '''\n    def __init__(self, start_node):\n        self._node = start_node\n        self._children_iter = None\n        self._child_iter = None\n    def __iter__(self):\n        return self\n    def __next__(self):\n        # Return myself if just started; create an iterator for children\n        if self._children_iter is None:\n            self._children_iter = iter(self._node)\n            return self._node\n        # If processing a child, return its next item\n        elif self._child_iter:\n            try:\n                nextchild = next(self._child_iter)\n                return nextchild\n            except StopIteration:\n                self._child_iter = None\n                return next(self)\n        # Advance to the next child and start its iteration\n        else:\n            self._child_iter = next(self._children_iter).depth_first()\n            return next(self)\nThe DepthFirstIterator class works in the same way as the generator version, but it’s\na mess because the iterator has to maintain a lot of complex state about where it is in\nthe iteration process. Frankly, nobody likes to write mind-bending code like that. Define\nyour iterator as a generator and be done with it.\n4.5. Iterating in Reverse\nProblem\nYou want to iterate in reverse over a sequence.\nSolution\nUse the built-in reversed() function. For example:\n>>> a = [1, 2, 3, 4]\n>>> for x in reversed(a):\n...     print(x)\n...\n4.5. Iterating in Reverse \n| \n119",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": 3,
      "content": "4\n3\n2\n1\nReversed iteration only works if the object in question has a size that can be determined\nor if the object implements a __reversed__() special method. If neither of these can\nbe satisfied, you’ll have to convert the object into a list first. For example:\n# Print a file backwards\nf = open('somefile')\nfor line in reversed(list(f)):\n    print(line, end='')\nBe aware that turning an iterable into a list as shown could consume a lot of memory\nif it’s large.\nDiscussion\nMany programmers don’t realize that reversed iteration can be customized on user-\ndefined classes if they implement the __reversed__() method. For example:\nclass Countdown:\n    def __init__(self, start):\n        self.start = start\n    # Forward iterator\n    def __iter__(self):\n        n = self.start\n        while n > 0:\n            yield n\n            n -= 1\n    # Reverse iterator\n    def __reversed__(self):\n        n = 1\n        while n <= self.start:\n            yield n\n            n += 1\nDefining a reversed iterator makes the code much more efficient, as it’s no longer nec‐\nessary to pull the data into a list and iterate in reverse on the list.\n4.6. Defining Generator Functions with Extra State\nProblem\nYou would like to define a generator function, but it involves extra state that you would\nlike to expose to the user somehow.\n120 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": 3,
      "content": "Solution\nIf you want a generator to expose extra state to the user, don’t forget that you can easily\nimplement it as a class, putting the generator function code in the __iter__() method.\nFor example:\nfrom collections import deque\nclass linehistory:\n    def __init__(self, lines, histlen=3):\n        self.lines = lines\n        self.history = deque(maxlen=histlen)\n    def __iter__(self):\n        for lineno, line in enumerate(self.lines,1):\n            self.history.append((lineno, line))\n            yield line\n    def clear(self):\n        self.history.clear()\nTo use this class, you would treat it like a normal generator function. However, since it\ncreates an instance, you can access internal attributes, such as the history attribute or\nthe clear() method. For example:\nwith open('somefile.txt') as f:\n     lines = linehistory(f)\n     for line in lines:\n         if 'python' in line:\n             for lineno, hline in lines.history:\n                 print('{}:{}'.format(lineno, hline), end='')\nDiscussion\nWith generators, it is easy to fall into a trap of trying to do everything with functions\nalone. This can lead to rather complicated code if the generator function needs to in‐\nteract with other parts of your program in unusual ways (exposing attributes, allowing\ncontrol via method calls, etc.). If this is the case, just use a class definition, as shown.\nDefining your generator in the __iter__() method doesn’t change anything about how\nyou write your algorithm. The fact that it’s part of a class makes it easy for you to provide\nattributes and methods for users to interact with.\nOne potential subtlety with the method shown is that it might require an extra step of\ncalling iter() if you are going to drive iteration using a technique other than a for\nloop. For example:\n>>> f = open('somefile.txt')\n>>> lines = linehistory(f)\n>>> next(lines)\nTraceback (most recent call last):\n4.6. Defining Generator Functions with Extra State \n| \n121",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": 3,
      "content": "File \"<stdin>\", line 1, in <module>\nTypeError: 'linehistory' object is not an iterator\n>>> # Call iter() first, then start iterating\n>>> it = iter(lines)\n>>> next(it)\n'hello world\\n'\n>>> next(it)\n'this is a test\\n'\n>>>\n4.7. Taking a Slice of an Iterator\nProblem\nYou want to take a slice of data produced by an iterator, but the normal slicing operator\ndoesn’t work.\nSolution\nThe itertools.islice() function is perfectly suited for taking slices of iterators and\ngenerators. For example:\n>>> def count(n):\n...     while True:\n...             yield n\n...             n += 1\n...\n>>> c = count(0)\n>>> c[10:20]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'generator' object is not subscriptable\n>>> # Now using islice()\n>>> import itertools\n>>> for x in itertools.islice(c, 10, 20):\n...     print(x)\n...\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n>>>\n122 \n| \nChapter 4: Iterators and Generators",
      "content_length": 916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": 3,
      "content": "Discussion\nIterators and generators can’t normally be sliced, because no information is known about\ntheir length (and they don’t implement indexing). The result of islice() is an iterator\nthat produces the desired slice items, but it does this by consuming and discarding all\nof the items up to the starting slice index. Further items are then produced by the islice\nobject until the ending index has been reached.\nIt’s important to emphasize that islice() will consume data on the supplied iterator.\nSince iterators can’t be rewound, that is something to consider. If it’s important to go\nback, you should probably just turn the data into a list first.\n4.8. Skipping the First Part of an Iterable\nProblem\nYou want to iterate over items in an iterable, but the first few items aren’t of interest and\nyou just want to discard them.\nSolution\nThe itertools module has a few functions that can be used to address this task. The\nfirst is the itertools.dropwhile() function. To use it, you supply a function and an\niterable. The returned iterator discards the first items in the sequence as long as the\nsupplied function returns True. Afterward, the entirety of the sequence is produced.\nTo illustrate, suppose you are reading a file that starts with a series of comment lines.\nFor example:\n>>> with open('/etc/passwd') as f:\n...     for line in f:\n...         print(line, end='')\n...\n##\n# User Database\n#\n# Note that this file is consulted directly only when the system is running\n# in single-user mode.  At other times, this information is provided by\n# Open Directory.\n...\n##\nnobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false\nroot:*:0:0:System Administrator:/var/root:/bin/sh\n...\n>>>\nIf you want to skip all of the initial comment lines, here’s one way to do it:\n4.8. Skipping the First Part of an Iterable \n| \n123",
      "content_length": 1820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": 3,
      "content": ">>> from itertools import dropwhile\n>>> with open('/etc/passwd') as f:\n...     for line in dropwhile(lambda line: line.startswith('#'), f):\n...          print(line, end='')\n...\nnobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false\nroot:*:0:0:System Administrator:/var/root:/bin/sh\n...\n>>>\nThis example is based on skipping the first items according to a test function. If you\nhappen to know the exact number of items you want to skip, then you can use iter\ntools.islice() instead. For example:\n>>> from itertools import islice\n>>> items = ['a', 'b', 'c', 1, 4, 10, 15]\n>>> for x in islice(items, 3, None):\n...     print(x)\n...\n1\n4\n10\n15\n>>>\nIn this example, the last None argument to islice() is required to indicate that you\nwant everything beyond the first three items as opposed to only the first three items\n(e.g., a slice of [3:] as opposed to a slice of [:3]).\nDiscussion\nThe dropwhile() and islice() functions are mainly convenience functions that you\ncan use to avoid writing rather messy code such as this:\nwith open('/etc/passwd') as f:\n    # Skip over initial comments\n    while True:\n        line = next(f, '')\n        if not line.startswith('#'):\n            break\n    # Process remaining lines\n    while line:\n        # Replace with useful processing\n        print(line, end='')\n        line = next(f, None)\nDiscarding the first part of an iterable is also slightly different than simply filtering all\nof it. For example, the first part of this recipe might be rewritten as follows:\nwith open('/etc/passwd') as f:\n    lines = (line for line in f if not line.startswith('#'))\n124 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": 3,
      "content": "for line in lines:\n        print(line, end='')\nThis will obviously discard the comment lines at the start, but will also discard all such\nlines throughout the entire file. On the other hand, the solution only discards items\nuntil an item no longer satisfies the supplied test. After that, all subsequent items are\nreturned with no filtering.\nLast, but not least, it should be emphasized that this recipe works with all iterables,\nincluding those whose size can’t be determined in advance. This includes generators,\nfiles, and similar kinds of objects.\n4.9. Iterating Over All Possible Combinations or\nPermutations\nProblem\nYou want to iterate over all of the possible combinations or permutations of a collection\nof items.\nSolution\nThe itertools module provides three functions for this task. The first of these—iter\ntools.permutations()—takes a collection of items and produces a sequence of tuples\nthat rearranges all of the items into all possible permutations (i.e., it shuffles them into\nall possible configurations). For example:\n>>> items = ['a', 'b', 'c']\n>>> from itertools import permutations\n>>> for p in permutations(items):\n...     print(p)\n...\n('a', 'b', 'c')\n('a', 'c', 'b')\n('b', 'a', 'c')\n('b', 'c', 'a')\n('c', 'a', 'b')\n('c', 'b', 'a')\n>>>\nIf you want all permutations of a smaller length, you can give an optional length argu‐\nment. For example:\n>>> for p in permutations(items, 2):\n...     print(p)\n...\n('a', 'b')\n('a', 'c')\n4.9. Iterating Over All Possible Combinations or Permutations \n| \n125",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": 3,
      "content": "('b', 'a')\n('b', 'c')\n('c', 'a')\n('c', 'b')\n>>>\nUse itertools.combinations() to produce a sequence of combinations of items taken\nfrom the input. For example:\n>>> from itertools import combinations\n>>> for c in combinations(items, 3):\n...     print(c)\n...\n('a', 'b', 'c')\n>>> for c in combinations(items, 2):\n...     print(c)\n...\n('a', 'b')\n('a', 'c')\n('b', 'c')\n>>> for c in combinations(items, 1):\n...     print(c)\n...\n('a',)\n('b',)\n('c',)\n>>>\nFor combinations(), the actual order of the elements is not considered. That is, the\ncombination ('a', 'b') is considered to be the same as ('b', 'a') (which is not\nproduced).\nWhen producing combinations, chosen items are removed from the collection of pos‐\nsible candidates (i.e., if 'a' has already been chosen, then it is removed from consider‐\nation). The itertools.combinations_with_replacement() function relaxes this, and\nallows the same item to be chosen more than once. For example:\n>>> for c in combinations_with_replacement(items, 3):\n...     print(c)\n...\n('a', 'a', 'a')\n('a', 'a', 'b')\n('a', 'a', 'c')\n('a', 'b', 'b')\n('a', 'b', 'c')\n('a', 'c', 'c')\n('b', 'b', 'b')\n('b', 'b', 'c')\n('b', 'c', 'c')\n('c', 'c', 'c')\n>>>\n126 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": 3,
      "content": "Discussion\nThis recipe demonstrates only some of the power found in the itertools module.\nAlthough you could certainly write code to produce permutations and combinations\nyourself, doing so would probably require more than a fair bit of thought. When faced\nwith seemingly complicated iteration problems, it always pays to look at itertools first.\nIf the problem is common, chances are a solution is already available.\n4.10. Iterating Over the Index-Value Pairs of a Sequence\nProblem\nYou want to iterate over a sequence, but would like to keep track of which element of\nthe sequence is currently being processed.\nSolution\nThe built-in enumerate() function handles this quite nicely:\n>>> my_list = ['a', 'b', 'c']\n>>> for idx, val in enumerate(my_list):\n...     print(idx, val)\n...\n0 a\n1 b\n2 c\nFor printing output with canonical line numbers (where you typically start the num‐\nbering at 1 instead of 0), you can pass in a start argument:\n>>> my_list = ['a', 'b', 'c']\n>>> for idx, val in enumerate(my_list, 1):\n...     print(idx, val)\n...\n1 a\n2 b\n3 c\nThis case is especially useful for tracking line numbers in files should you want to use\na line number in an error message:\ndef parse_data(filename):\n    with open(filename, 'rt') as f:\n         for lineno, line in enumerate(f, 1):\n             fields = line.split()\n             try:\n                 count = int(fields[1])\n                 ...\n             except ValueError as e:\n                 print('Line {}: Parse error: {}'.format(lineno, e))\n4.10. Iterating Over the Index-Value Pairs of a Sequence \n| \n127",
      "content_length": 1566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": 3,
      "content": "enumerate() can be handy for keeping track of the offset into a list for occurrences of\ncertain values, for example. So, if you want to map words in a file to the lines in which\nthey occur, it can easily be accomplished using enumerate() to map each word to the\nline offset in the file where it was found:\nword_summary = defaultdict(list)\nwith open('myfile.txt', 'r') as f:\n    lines = f.readlines()\nfor idx, line in enumerate(lines):\n    # Create a list of words in current line\n    words = [w.strip().lower() for w in line.split()]\n    for word in words:\n        word_summary[word].append(idx)\nIf you print word_summary after processing the file, it’ll be a dictionary (a default\ndict to be precise), and it’ll have a key for each word. The value for each word-key will\nbe a list of line numbers that word occurred on. If the word occurred twice on a single\nline, that line number will be listed twice, making it possible to identify various simple\nmetrics about the text.\nDiscussion\nenumerate() is a nice shortcut for situations where you might be inclined to keep your\nown counter variable. You could write code like this:\nlineno = 1\nfor line in f:\n    # Process line\n    ...\n    lineno += 1\nBut it’s usually much more elegant (and less error prone) to use enumerate() instead:\nfor lineno, line in enumerate(f):\n    # Process line\n    ...\nThe value returned by enumerate() is an instance of an enumerate object, which is an\niterator that returns successive tuples consisting of a counter and the value returned by\ncalling next() on the sequence you’ve passed in.\nAlthough a minor point, it’s worth mentioning that sometimes it is easy to get tripped\nup when applying enumerate() to a sequence of tuples that are also being unpacked.\nTo do it, you have to write code like this:\ndata = [ (1, 2), (3, 4), (5, 6), (7, 8) ]\n# Correct!\nfor n, (x, y) in enumerate(data):\n128 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": 3,
      "content": "...\n# Error!\nfor n, x, y in enumerate(data):\n    ...\n4.11. Iterating Over Multiple Sequences Simultaneously\nProblem\nYou want to iterate over the items contained in more than one sequence at a time.\nSolution\nTo iterate over more than one sequence simultaneously, use the zip() function. For\nexample:\n>>> xpts = [1, 5, 4, 2, 10, 7]\n>>> ypts = [101, 78, 37, 15, 62, 99]\n>>> for x, y in zip(xpts, ypts):\n...     print(x,y)\n...\n1 101\n5 78\n4 37\n2 15\n10 62\n7 99\n>>>\nzip(a, b) works by creating an iterator that produces tuples (x, y) where x is taken\nfrom a and y is taken from b. Iteration stops whenever one of the input sequences is\nexhausted. Thus, the length of the iteration is the same as the length of the shortest\ninput. For example:\n>>> a = [1, 2, 3]\n>>> b = ['w', 'x', 'y', 'z']\n>>> for i in zip(a,b):\n...     print(i)\n...\n(1, 'w')\n(2, 'x')\n(3, 'y')\n>>>\nIf this behavior is not desired, use itertools.zip_longest() instead. For example:\n>>> from itertools import zip_longest\n>>> for i in zip_longest(a,b):\n...     print(i)\n...\n4.11. Iterating Over Multiple Sequences Simultaneously \n| \n129",
      "content_length": 1093,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": 3,
      "content": "(1, 'w')\n(2, 'x')\n(3, 'y')\n(None, 'z')\n>>> for i in zip_longest(a, b, fillvalue=0):\n...     print(i)\n...\n(1, 'w')\n(2, 'x')\n(3, 'y')\n(0, 'z')\n>>>\nDiscussion\nzip() is commonly used whenever you need to pair data together. For example, suppose\nyou have a list of column headers and column values like this:\nheaders = ['name', 'shares', 'price']\nvalues = ['ACME', 100, 490.1]\nUsing zip(), you can pair the values together to make a dictionary like this:\ns = dict(zip(headers,values))\nAlternatively, if you are trying to produce output, you can write code like this:\nfor name, val in zip(headers, values):\n    print(name, '=', val)\nIt’s less common, but zip() can be passed more than two sequences as input. For this\ncase, the resulting tuples have the same number of items in them as the number of input\nsequences. For example:\n>>> a = [1, 2, 3]\n>>> b = [10, 11, 12]\n>>> c = ['x','y','z']\n>>> for i in zip(a, b, c):\n...     print(i)\n...\n(1, 10, 'x')\n(2, 11, 'y')\n(3, 12, 'z')\n>>>\nLast, but not least, it’s important to emphasize that zip() creates an iterator as a result.\nIf you need the paired values stored in a list, use the list() function. For example:\n>>> zip(a, b)\n<zip object at 0x1007001b8>\n>>> list(zip(a, b))\n[(1, 10), (2, 11), (3, 12)]\n>>>\n130 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": 3,
      "content": "4.12. Iterating on Items in Separate Containers\nProblem\nYou need to perform the same operation on many objects, but the objects are contained\nin different containers, and you’d like to avoid nested loops without losing the readability\nof your code.\nSolution\nThe itertools.chain() method can be used to simplify this task. It takes a list of\niterables as input, and returns an iterator that effectively masks the fact that you’re really\nacting on multiple containers. To illustrate, consider this example:\n>>> from itertools import chain\n>>> a = [1, 2, 3, 4]\n>>> b = ['x', 'y', 'z']\n>>> for x in chain(a, b):\n...     print(x)\n...\n1\n2\n3\n4\nx\ny\nz\n>>>\nA common use of chain() is in programs where you would like to perform certain\noperations on all of the items at once but the items are pooled into different working\nsets. For example:\n# Various working sets of items\nactive_items = set()\ninactive_items = set()\n# Iterate over all items\nfor item in chain(active_items, inactive_items):\n    # Process item\n    ...\nThis solution is much more elegant than using two separate loops, as in the following:\nfor item in active_items:\n    # Process item\n    ...\nfor item in inactive_items:\n    # Process item\n    ...\n4.12. Iterating on Items in Separate Containers \n| \n131",
      "content_length": 1259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": 3,
      "content": "Discussion\nitertools.chain() accepts one or more iterables as arguments. It then works by cre‐\nating an iterator that successively consumes and returns the items produced by each of\nthe supplied iterables you provided. It’s a subtle distinction, but chain() is more efficient\nthan first combining the sequences and iterating. For example:\n# Inefficent\nfor x in a + b:\n    ...\n# Better\nfor x in chain(a, b):\n    ...\nIn the first case, the operation a + b creates an entirely new sequence and additionally\nrequires a and b to be of the same type. chain() performs no such operation, so it’s far\nmore efficient with memory if the input sequences are large and it can be easily applied\nwhen the iterables in question are of different types.\n4.13. Creating Data Processing Pipelines\nProblem\nYou want to process data iteratively in the style of a data processing pipeline (similar to\nUnix pipes). For instance, you have a huge amount of data that needs to be processed,\nbut it can’t fit entirely into memory.\nSolution\nGenerator functions are a good way to implement processing pipelines. To illustrate,\nsuppose you have a huge directory of log files that you want to process:\n    foo/\n       access-log-012007.gz\n       access-log-022007.gz\n       access-log-032007.gz\n       ...\n       access-log-012008\n    bar/\n       access-log-092007.bz2\n       ...\n       access-log-022008\nSuppose each file contains lines of data like this:\n    124.115.6.12 - - [10/Jul/2012:00:18:50 -0500] \"GET /robots.txt ...\" 200 71\n    210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] \"GET /ply/ ...\" 200 11875\n    210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] \"GET /favicon.ico ...\" 404 369\n132 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1708,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": 3,
      "content": "61.135.216.105 - - [10/Jul/2012:00:20:04 -0500] \"GET /blog/atom.xml ...\" 304 -\n    ...\nTo process these files, you could define a collection of small generator functions that\nperform specific self-contained tasks. For example:\nimport os\nimport fnmatch\nimport gzip\nimport bz2\nimport re\ndef gen_find(filepat, top):\n    '''\n    Find all filenames in a directory tree that match a shell wildcard pattern\n    '''\n    for path, dirlist, filelist in os.walk(top):\n        for name in fnmatch.filter(filelist, filepat):\n            yield os.path.join(path,name)\ndef gen_opener(filenames):\n    '''\n    Open a sequence of filenames one at a time producing a file object.\n    The file is closed immediately when proceeding to the next iteration.\n    '''\n    for filename in filenames:\n        if filename.endswith('.gz'):\n            f = gzip.open(filename, 'rt')\n        elif filename.endswith('.bz2'):\n            f = bz2.open(filename, 'rt')\n        else:\n            f = open(filename, 'rt')\n        yield f\n        f.close()\ndef gen_concatenate(iterators):\n    '''\n    Chain a sequence of iterators together into a single sequence.\n    '''\n    for it in iterators:\n        yield from it\ndef gen_grep(pattern, lines):\n    '''\n    Look for a regex pattern in a sequence of lines\n    '''\n    pat = re.compile(pattern)\n    for line in lines:\n        if pat.search(line):\n            yield line\nYou can now easily stack these functions together to make a processing pipeline. For\nexample, to find all log lines that contain the word python, you would just do this:\n4.13. Creating Data Processing Pipelines \n| \n133",
      "content_length": 1602,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": 3,
      "content": "lognames = gen_find('access-log*', 'www')\nfiles = gen_opener(lognames)\nlines = gen_concatenate(files)\npylines = gen_grep('(?i)python', lines)\nfor line in pylines:\n    print(line)\nIf you want to extend the pipeline further, you can even feed the data in generator\nexpressions. For example, this version finds the number of bytes transferred and sums\nthe total:\nlognames = gen_find('access-log*', 'www')\nfiles = gen_opener(lognames)\nlines = gen_concatenate(files)\npylines = gen_grep('(?i)python', lines)\nbytecolumn = (line.rsplit(None,1)[1] for line in pylines)\nbytes = (int(x) for x in bytecolumn if x != '-')\nprint('Total', sum(bytes))\nDiscussion\nProcessing data in a pipelined manner works well for a wide variety of other problems,\nincluding parsing, reading from real-time data sources, periodic polling, and so on.\nIn understanding the code, it is important to grasp that the yield statement acts as a\nkind of data producer whereas a for loop acts as a data consumer. When the generators\nare stacked together, each yield feeds a single item of data to the next stage of the\npipeline that is consuming it with iteration. In the last example, the sum() function is\nactually driving the entire program, pulling one item at a time out of the pipeline of\ngenerators.\nOne nice feature of this approach is that each generator function tends to be small and\nself-contained. As such, they are easy to write and maintain. In many cases, they are so\ngeneral purpose that they can be reused in other contexts. The resulting code that glues\nthe components together also tends to read like a simple recipe that is easily understood.\nThe memory efficiency of this approach can also not be overstated. The code shown\nwould still work even if used on a massive directory of files. In fact, due to the iterative\nnature of the processing, very little memory would be used at all.\nThere is a bit of extreme subtlety involving the gen_concatenate() function. The\npurpose of this function is to concatenate input sequences together into one long se‐\nquence of lines. The itertools.chain() function performs a similar function, but re‐\nquires that all of the chained iterables be specified as arguments. In the case of this\nparticular recipe, doing that would involve a statement such as lines = iter\ntools.chain(*files), which would cause the gen_opener() generator to be fully con‐\nsumed. Since that generator is producing a sequence of open files that are immediately\n134 \n| \nChapter 4: Iterators and Generators",
      "content_length": 2495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": 3,
      "content": "closed in the next iteration step, chain() can’t be used. The solution shown avoids this\nissue.\nAlso appearing in the gen_concatenate() function is the use of yield from to delegate\nto a subgenerator. The statement yield from it simply makes gen_concatenate()\nemit all of the values produced by the generator it. This is described further in\nRecipe 4.14.\nLast, but not least, it should be noted that a pipelined approach doesn’t always work for\nevery data handling problem. Sometimes you just need to work with all of the data at\nonce. However, even in that case, using generator pipelines can be a way to logically\nbreak a problem down into a kind of workflow.\nDavid Beazley has written extensively about these techniques in his “Generator Tricks\nfor Systems Programmers” tutorial presentation. Consult that for even more examples.\n4.14. Flattening a Nested Sequence\nProblem\nYou have a nested sequence that you want to flatten into a single list of values.\nSolution\nThis is easily solved by writing a recursive generator function involving a yield from\nstatement. For example:\nfrom collections import Iterable\ndef flatten(items, ignore_types=(str, bytes)):\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, ignore_types):\n            yield from flatten(x)\n        else:\n            yield x\nitems = [1, 2, [3, 4, [5, 6], 7], 8]\n# Produces 1 2 3 4 5 6 7 8\nfor x in flatten(items):\n    print(x)\nIn the code, the isinstance(x, Iterable) simply checks to see if an item is iterable.\nIf so, yield from is used to emit all of its values as a kind of subroutine. The end result\nis a single sequence of output with no nesting.\nThe extra argument ignore_types and the check for not isinstance(x, ig\nnore_types) is there to prevent strings and bytes from being interpreted as iterables\n4.14. Flattening a Nested Sequence \n| \n135",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": 3,
      "content": "and expanded as individual characters. This allows nested lists of strings to work in the\nway that most people would expect. For example:\n>>> items = ['Dave', 'Paula', ['Thomas', 'Lewis']]\n>>> for x in flatten(items):\n...     print(x)\n...\nDave\nPaula\nThomas\nLewis\n>>>\nDiscussion\nThe yield from statement is a nice shortcut to use if you ever want to write generators\nthat call other generators as subroutines. If you don’t use it, you need to write code that\nuses an extra for loop. For example:\ndef flatten(items, ignore_types=(str, bytes)):\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, ignore_types):\n            for i in flatten(x):\n                yield i\n        else:\n            yield x\nAlthough it’s only a minor change, the yield from statement just feels better and leads\nto cleaner code.\nAs noted, the extra check for strings and bytes is there to prevent the expansion of those\ntypes into individual characters. If there are other types that you don’t want expanded,\nyou can supply a different value for the ignore_types argument.\nFinally, it should be noted that yield from has a more important role in advanced\nprograms involving coroutines and generator-based concurrency. See Recipe 12.12 for\nanother example.\n4.15. Iterating in Sorted Order Over Merged Sorted\nIterables\nProblem\nYou have a collection of sorted sequences and you want to iterate over a sorted sequence\nof them all merged together.\n136 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1487,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": 3,
      "content": "Solution\nThe heapq.merge() function does exactly what you want. For example:\n>>> import heapq\n>>> a = [1, 4, 7, 10]\n>>> b = [2, 5, 6, 11]\n>>> for c in heapq.merge(a, b):\n...     print(c)\n...\n1\n2\n4\n5\n6\n7\n10\n11\nDiscussion\nThe iterative nature of heapq.merge means that it never reads any of the supplied se‐\nquences all at once. This means that you can use it on very long sequences with very\nlittle overhead. For instance, here is an example of how you would merge two sorted\nfiles:\nimport heapq\nwith open('sorted_file_1', 'rt') as file1, \\\n     open('sorted_file_2') 'rt' as file2, \\\n     open('merged_file', 'wt') as outf:\n    for line in heapq.merge(file1, file2):\n        outf.write(line)\nIt’s important to emphasize that heapq.merge() requires that all of the input sequences\nalready be sorted. In particular, it does not first read all of the data into a heap or do any\npreliminary sorting. Nor does it perform any kind of validation of the inputs to check\nif they meet the ordering requirements. Instead, it simply examines the set of items from\nthe front of each input sequence and emits the smallest one found. A new item from\nthe chosen sequence is then read, and the process repeats itself until all input sequences\nhave been fully consumed.\n4.15. Iterating in Sorted Order Over Merged Sorted Iterables \n| \n137",
      "content_length": 1320,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": 3,
      "content": "4.16. Replacing Infinite while Loops with an Iterator\nProblem\nYou have code that uses a while loop to iteratively process data because it involves a\nfunction or some kind of unusual test condition that doesn’t fall into the usual iteration\npattern.\nSolution\nA somewhat common scenario in programs involving I/O is to write code like this:\nCHUNKSIZE = 8192\ndef reader(s):\n    while True:\n        data = s.recv(CHUNKSIZE)\n        if data == b'':\n            break\n        process_data(data)\nSuch code can often be replaced using iter(), as follows:\ndef reader(s):\n    for chunk in iter(lambda: s.recv(CHUNKSIZE), b''):\n        process_data(data)\nIf you’re a bit skeptical that it might work, you can try a similar example involving files.\nFor example:\n>>> import sys\n>>> f = open('/etc/passwd')\n>>> for chunk in iter(lambda: f.read(10), ''):\n...     n = sys.stdout.write(chunk)\n...\nnobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false\nroot:*:0:0:System Administrator:/var/root:/bin/sh\ndaemon:*:1:1:System Services:/var/root:/usr/bin/false\n_uucp:*:4:4:Unix to Unix Copy Protocol:/var/spool/uucp:/usr/sbin/uucico\n...\n>>>\nDiscussion\nA little-known feature of the built-in iter() function is that it optionally accepts a zero-\nargument callable and sentinel (terminating) value as inputs. When used in this way, it\ncreates an iterator that repeatedly calls the supplied callable over and over again until it\nreturns the value given as a sentinel.\n138 \n| \nChapter 4: Iterators and Generators",
      "content_length": 1490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": 3,
      "content": "This particular approach works well with certain kinds of repeatedly called functions,\nsuch as those involving I/O. For example, if you want to read data in chunks from sockets\nor files, you usually have to repeatedly execute read() or recv() calls followed by an\nend-of-file test. This recipe simply takes these two features and combines them together\ninto a single iter() call. The use of lambda in the solution is needed to create a callable\nthat takes no arguments, yet still supplies the desired size argument to recv() or read().\n4.16. Replacing Infinite while Loops with an Iterator \n| \n139",
      "content_length": 597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": 3,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 159,
      "chapter": 3,
      "content": "CHAPTER 5\nFiles and I/O\nAll programs need to perform input and output. This chapter covers common idioms\nfor working with different kinds of files, including text and binary files, file encodings,\nand other related matters. Techniques for manipulating filenames and directories are\nalso covered.\n5.1. Reading and Writing Text Data\nProblem\nYou need to read or write text data, possibly in different text encodings such as ASCII,\nUTF-8, or UTF-16.\nSolution\nUse the open() function with mode rt to read a text file. For example:\n# Read the entire file as a single string\nwith open('somefile.txt', 'rt') as f:\n    data = f.read()\n# Iterate over the lines of the file\nwith open('somefile.txt', 'rt') as f:\n    for line in f:\n        # process line\n        ...\nSimilarly, to write a text file, use open() with mode wt to write a file, clearing and\noverwriting the previous contents (if any). For example:\n# Write chunks of text data\nwith open('somefile.txt', 'wt') as f:\n    f.write(text1)\n141",
      "content_length": 987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": 3,
      "content": "f.write(text2)\n    ...\n# Redirected print statement\nwith open('somefile.txt', 'wt') as f:\n    print(line1, file=f)\n    print(line2, file=f)\n    ...\nTo append to the end of an existing file, use open() with mode at.\nBy default, files are read/written using the system default text encoding, as can be found\nin sys.getdefaultencoding(). On most machines, this is set to utf-8. If you know\nthat the text you are reading or writing is in a different encoding, supply the optional\nencoding parameter to open(). For example:\nwith open('somefile.txt', 'rt', encoding='latin-1') as f:\n     ...\nPython understands several hundred possible text encodings. However, some of the\nmore common encodings are ascii, latin-1, utf-8, and utf-16. UTF-8 is usually a\nsafe bet if working with web applications. ascii corresponds to the 7-bit characters in\nthe range U+0000 to U+007F. latin-1 is a direct mapping of bytes 0-255 to Unicode\ncharacters U+0000 to U+00FF. latin-1 encoding is notable in that it will never produce\na decoding error when reading text of a possibly unknown encoding. Reading a file as\nlatin-1 might not produce a completely correct text decoding, but it still might be\nenough to extract useful data out of it. Also, if you later write the data back out, the\noriginal input data will be preserved.\nDiscussion\nReading and writing text files is typically very straightforward. However, there are a\nnumber of subtle aspects to keep in mind. First, the use of the with statement in the\nexamples establishes a context in which the file will be used. When control leaves the\nwith block, the file will be closed automatically. You don’t need to use the with statement,\nbut if you don’t use it, make sure you remember to close the file:\nf = open('somefile.txt', 'rt')\ndata = f.read()\nf.close()\nAnother minor complication concerns the recognition of newlines, which are different\non Unix and Windows (i.e., \\n versus \\r\\n). By default, Python operates in what’s known\nas “universal newline” mode. In this mode, all common newline conventions are rec‐\nognized, and newline characters are converted to a single \\n character while reading.\nSimilarly, the newline character \\n is converted to the system default newline character\n142 \n| \nChapter 5: Files and I/O",
      "content_length": 2252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": 4,
      "content": "on output. If you don’t want this translation, supply the newline='' argument to\nopen(), like this:\n# Read with disabled newline translation\nwith open('somefile.txt', 'rt', newline='') as f:\n     ...\nTo illustrate the difference, here’s what you will see on a Unix machine if you read the\ncontents of a Windows-encoded text file containing the raw data hello world!\\r\\n:\n>>> # Newline translation enabled (the default)\n>>> f = open('hello.txt', 'rt')\n>>> f.read()\n'hello world!\\n'\n>>> # Newline translation disabled\n>>> g = open('hello.txt', 'rt', newline='')\n>>> g.read()\n'hello world!\\r\\n'\n>>>\nA final issue concerns possible encoding errors in text files. When reading or writing a\ntext file, you might encounter an encoding or decoding error. For instance:\n>>> f = open('sample.txt', 'rt', encoding='ascii')\n>>> f.read()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.3/encodings/ascii.py\", line 26, in decode\n    return codecs.ascii_decode(input, self.errors)[0]\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position\n12: ordinal not in range(128)\n>>>\nIf you get this error, it usually means that you’re not reading the file in the correct\nencoding. You should carefully read the specification of whatever it is that you’re reading\nand check that you’re doing it right (e.g., reading data as UTF-8 instead of Latin-1 or\nwhatever it needs to be). If encoding errors are still a possibility, you can supply an\noptional errors argument to open() to deal with the errors. Here are a few samples of\ncommon error handling schemes:\n>>> # Replace bad chars with Unicode U+fffd replacement char\n>>> f = open('sample.txt', 'rt', encoding='ascii', errors='replace')\n>>> f.read()\n'Spicy Jalape?o!'\n>>> # Ignore bad chars entirely\n>>> g = open('sample.txt', 'rt', encoding='ascii', errors='ignore')\n>>> g.read()\n'Spicy Jalapeo!'\n>>>\n5.1. Reading and Writing Text Data \n| \n143",
      "content_length": 1938,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": 4,
      "content": "If you’re constantly fiddling with the encoding and errors arguments to open() and\ndoing lots of hacks, you’re probably making life more difficult than it needs to be. The\nnumber one rule with text is that you simply need to make sure you’re always using the\nproper text encoding. When in doubt, use the default setting (typically UTF-8).\n5.2. Printing to a File\nProblem\nYou want to redirect the output of the print() function to a file.\nSolution\nUse the file keyword argument to print(), like this:\nwith open('somefile.txt', 'rt') as f:\n    print('Hello World!', file=f)\nDiscussion\nThere’s not much more to printing to a file other than this. However, make sure that the\nfile is opened in text mode. Printing will fail if the underlying file is in binary mode.\n5.3. Printing with a Different Separator or Line Ending\nProblem\nYou want to output data using print(), but you also want to change the separator\ncharacter or line ending.\nSolution\nUse the sep and end keyword arguments to print() to change the output as you wish.\nFor example:\n>>> print('ACME', 50, 91.5)\nACME 50 91.5\n>>> print('ACME', 50, 91.5, sep=',')\nACME,50,91.5\n>>> print('ACME', 50, 91.5, sep=',', end='!!\\n')\nACME,50,91.5!!\n>>>\nUse of the end argument is also how you suppress the output of newlines in output. For\nexample:\n144 \n| \nChapter 5: Files and I/O",
      "content_length": 1325,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": 4,
      "content": ">>> for i in range(5):\n...     print(i)\n...\n0\n1\n2\n3\n4\n>>> for i in range(5):\n...     print(i, end=' ')\n...\n0 1 2 3 4 >>>\nDiscussion\nUsing print() with a different item separator is often the easiest way to output data\nwhen you need something other than a space separating the items. Sometimes you’ll\nsee programmers using str.join() to accomplish the same thing. For example:\n>>> print(','.join('ACME','50','91.5'))\nACME,50,91.5\n>>>\nThe problem with str.join() is that it only works with strings. This means that it’s\noften necessary to perform various acrobatics to get it to work. For example:\n>>> row = ('ACME', 50, 91.5)\n>>> print(','.join(row))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: sequence item 1: expected str instance, int found\n>>> print(','.join(str(x) for x in row))\nACME,50,91.5\n>>>\nInstead of doing that, you could just write the following:\n>>> print(*row, sep=',')\nACME,50,91.5\n>>>\n5.4. Reading and Writing Binary Data\nProblem\nYou need to read or write binary data, such as that found in images, sound files, and so\non.\n5.4. Reading and Writing Binary Data \n| \n145",
      "content_length": 1125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": 4,
      "content": "Solution\nUse the open() function with mode rb or wb to read or write binary data. For example:\n# Read the entire file as a single byte string\nwith open('somefile.bin', 'rb') as f:\n    data = f.read()\n# Write binary data to a file\nwith open('somefile.bin', 'wb') as f:\n    f.write(b'Hello World')\nWhen reading binary, it is important to stress that all data returned will be in the form\nof byte strings, not text strings. Similarly, when writing, you must supply data in the\nform of objects that expose data as bytes (e.g., byte strings, bytearray objects, etc.).\nDiscussion\nWhen reading binary data, the subtle semantic differences between byte strings and text\nstrings pose a potential gotcha. In particular, be aware that indexing and iteration return\ninteger byte values instead of byte strings. For example:\n>>> # Text string\n>>> t = 'Hello World'\n>>> t[0]\n'H'\n>>> for c in t:\n...     print(c)\n...\nH\ne\nl\nl\no\n...\n>>> # Byte string\n>>> b = b'Hello World'\n>>> b[0]\n72\n>>> for c in b:\n...     print(c)\n...\n72\n101\n108\n108\n111\n...\n>>>\n146 \n| \nChapter 5: Files and I/O",
      "content_length": 1065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": 4,
      "content": "If you ever need to read or write text from a binary-mode file, make sure you remember\nto decode or encode it. For example:\nwith open('somefile.bin', 'rb') as f:\n    data = f.read(16)\n    text = data.decode('utf-8')\nwith open('somefile.bin', 'wb') as f:\n    text = 'Hello World'\n    f.write(text.encode('utf-8'))\nA lesser-known aspect of binary I/O is that objects such as arrays and C structures can\nbe used for writing without any kind of intermediate conversion to a bytes object. For\nexample:\nimport array\nnums = array.array('i', [1, 2, 3, 4])\nwith open('data.bin','wb') as f:\n    f.write(nums)\nThis applies to any object that implements the so-called “buffer interface,” which directly\nexposes an underlying memory buffer to operations that can work with it. Writing\nbinary data is one such operation.\nMany objects also allow binary data to be directly read into their underlying memory\nusing the readinto() method of files. For example:\n>>> import array\n>>> a = array.array('i', [0, 0, 0, 0, 0, 0, 0, 0])\n>>> with open('data.bin', 'rb') as f:\n...     f.readinto(a)\n...\n16\n>>> a\narray('i', [1, 2, 3, 4, 0, 0, 0, 0])\n>>>\nHowever, great care should be taken when using this technique, as it is often platform\nspecific and may depend on such things as the word size and byte ordering (i.e., big\nendian versus little endian). See Recipe 5.9 for another example of reading binary data\ninto a mutable buffer.\n5.5. Writing to a File That Doesn’t Already Exist\nProblem\nYou want to write data to a file, but only if it doesn’t already exist on the filesystem.\n5.5. Writing to a File That Doesn’t Already Exist \n| \n147",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": 4,
      "content": "Solution\nThis problem is easily solved by using the little-known x mode to open() instead of the\nusual w mode. For example:\n>>> with open('somefile', 'wt') as f:\n...     f.write('Hello\\n')\n...\n>>> with open('somefile', 'xt') as f:\n...     f.write('Hello\\n')\n...\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nFileExistsError: [Errno 17] File exists: 'somefile'\n>>>\nIf the file is binary mode, use mode xb instead of xt.\nDiscussion\nThis recipe illustrates an extremely elegant solution to a problem that sometimes arises\nwhen writing files (i.e., accidentally overwriting an existing file). An alternative solution\nis to first test for the file like this:\n>>> import os\n>>> if not os.path.exists('somefile'):\n...     with open('somefile', 'wt') as f:\n...         f.write('Hello\\n')\n... else:\n...     print('File already exists!')\n...\nFile already exists!\n>>>\nClearly, using the x file mode is a lot more straightforward. It is important to note that\nthe x mode is a Python 3 specific extension to the open() function. In particular, no\nsuch mode exists in earlier Python versions or the underlying C libraries used in Python’s\nimplementation.\n5.6. Performing I/O Operations on a String\nProblem\nYou want to feed a text or binary string to code that’s been written to operate on file-\nlike objects instead.\n148 \n| \nChapter 5: Files and I/O",
      "content_length": 1362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": 4,
      "content": "Solution\nUse the io.StringIO() and io.BytesIO() classes to create file-like objects that operate\non string data. For example:\n>>> s = io.StringIO()\n>>> s.write('Hello World\\n')\n12\n>>> print('This is a test', file=s)\n15\n>>> # Get all of the data written so far\n>>> s.getvalue()\n'Hello World\\nThis is a test\\n'\n>>>\n>>> # Wrap a file interface around an existing string\n>>> s = io.StringIO('Hello\\nWorld\\n')\n>>> s.read(4)\n'Hell'\n>>> s.read()\n'o\\nWorld\\n'\n>>>\nThe io.StringIO class should only be used for text. If you are operating with binary\ndata, use the io.BytesIO class instead. For example:\n>>> s = io.BytesIO()\n>>> s.write(b'binary data')\n>>> s.getvalue()\nb'binary data'\n>>>\nDiscussion\nThe StringIO and BytesIO classes are most useful in scenarios where you need to mimic\na normal file for some reason. For example, in unit tests, you might use StringIO to\ncreate a file-like object containing test data that’s fed into a function that would otherwise\nwork with a normal file.\nBe aware that StringIO and BytesIO instances don’t have a proper integer file-\ndescriptor. Thus, they do not work with code that requires the use of a real system-level\nfile such as a file, pipe, or socket.\n5.7. Reading and Writing Compressed Datafiles\nProblem\nYou need to read or write data in a file with gzip or bz2 compression.\n5.7. Reading and Writing Compressed Datafiles \n| \n149",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": 4,
      "content": "Solution\nThe gzip and bz2 modules make it easy to work with such files. Both modules provide\nan alternative implementation of open() that can be used for this purpose. For example,\nto read compressed files as text, do this:\n# gzip compression\nimport gzip\nwith gzip.open('somefile.gz', 'rt') as f:\n    text = f.read()\n# bz2 compression\nimport bz2\nwith bz2.open('somefile.bz2', 'rt') as f:\n    text = f.read()\nSimilarly, to write compressed data, do this:\n# gzip compression\nimport gzip\nwith gzip.open('somefile.gz', 'wt') as f:\n    f.write(text)\n# bz2 compression\nimport bz2\nwith bz2.open('somefile.bz2', 'wt') as f:\n    f.write(text)\nAs shown, all I/O will use text and perform Unicode encoding/decoding. If you want\nto work with binary data instead, use a file mode of rb or wb.\nDiscussion\nFor the most part, reading or writing compressed data is straightforward. However, be\naware that choosing the correct file mode is critically important. If you don’t specify a\nmode, the default mode is binary, which will break programs that expect to receive text.\nBoth gzip.open() and bz2.open() accept the same parameters as the built-in open()\nfunction, including encoding, errors, newline, and so forth.\nWhen writing compressed data, the compression level can be optionally specified using\nthe compresslevel keyword argument. For example:\nwith gzip.open('somefile.gz', 'wt', compresslevel=5) as f:\n     f.write(text)\nThe default level is 9, which provides the highest level of compression. Lower levels offer\nbetter performance, but not as much compression.\nFinally, a little-known feature of gzip.open() and bz2.open() is that they can be layered\non top of an existing file opened in binary mode. For example, this works:\n150 \n| \nChapter 5: Files and I/O",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": 4,
      "content": "import gzip\nf = open('somefile.gz', 'rb')\nwith gzip.open(f, 'rt') as g:\n     text = g.read()\nThis allows the gzip and bz2 modules to work with various file-like objects such as\nsockets, pipes, and in-memory files.\n5.8. Iterating Over Fixed-Sized Records\nProblem\nInstead of iterating over a file by lines, you want to iterate over a collection of fixed-\nsized records or chunks.\nSolution\nUse the iter() function and functools.partial() using this neat trick:\nfrom functools import partial\nRECORD_SIZE = 32\nwith open('somefile.data', 'rb') as f:\n    records = iter(partial(f.read, RECORD_SIZE), b'')\n    for r in records:\n        ...\nThe records object in this example is an iterable that will produce fixed-sized chunks\nuntil the end of the file is reached. However, be aware that the last item may have fewer\nbytes than expected if the file size is not an exact multiple of the record size.\nDiscussion\nA little-known feature of the iter() function is that it can create an iterator if you pass\nit a callable and a sentinel value. The resulting iterator simply calls the supplied callable\nover and over again until it returns the sentinel, at which point iteration stops.\nIn the solution, the functools.partial is used to create a callable that reads a fixed\nnumber of bytes from a file each time it’s called. The sentinel of b'' is what gets returned\nwhen a file is read but the end of file has been reached.\nLast, but not least, the solution shows the file being opened in binary mode. For reading\nfixed-sized records, this would probably be the most common case. For text files, reading\nline by line (the default iteration behavior) is more common.\n5.8. Iterating Over Fixed-Sized Records \n| \n151",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": 4,
      "content": "5.9. Reading Binary Data into a Mutable Buffer\nProblem\nYou want to read binary data directly into a mutable buffer without any intermediate\ncopying. Perhaps you want to mutate the data in-place and write it back out to a file.\nSolution\nTo read data into a mutable array, use the readinto() method of files. For example:\nimport os.path\ndef read_into_buffer(filename):\n    buf = bytearray(os.path.getsize(filename))\n    with open(filename, 'rb') as f:\n         f.readinto(buf)\n    return buf\nHere is an example that illustrates the usage:\n>>> # Write a sample file\n>>> with open('sample.bin', 'wb') as f:\n...      f.write(b'Hello World')\n...\n>>> buf = read_into_buffer('sample.bin')\n>>> buf\nbytearray(b'Hello World')\n>>> buf[0:5] = b'Hallo'\n>>> buf\nbytearray(b'Hallo World')\n>>> with open('newsample.bin', 'wb') as f:\n...     f.write(buf)\n...\n11\n>>>\nDiscussion\nThe readinto() method of files can be used to fill any preallocated array with data. This\neven includes arrays created from the array module or libraries such as numpy. Unlike\nthe normal read() method, readinto() fills the contents of an existing buffer rather\nthan allocating new objects and returning them. Thus, you might be able to use it to\navoid making extra memory allocations. For example, if you are reading a binary file\nconsisting of equally sized records, you can write code like this:\nrecord_size = 32           # Size of each record (adjust value)\nbuf = bytearray(record_size)\nwith open('somefile', 'rb') as f:\n152 \n| \nChapter 5: Files and I/O",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": 4,
      "content": "while True:\n        n = f.readinto(buf)\n        if n < record_size:\n            break\n        # Use the contents of buf\n        ...\nAnother interesting feature to use here might be a memoryview, which lets you make\nzero-copy slices of an existing buffer and even change its contents. For example:\n>>> buf\nbytearray(b'Hello World')\n>>> m1 = memoryview(buf)\n>>> m2 = m1[-5:]\n>>> m2\n<memory at 0x100681390>\n>>> m2[:] = b'WORLD'\n>>> buf\nbytearray(b'Hello WORLD')\n>>>\nOne caution with using f.readinto() is that you must always make sure to check its\nreturn code, which is the number of bytes actually read.\nIf the number of bytes is smaller than the size of the supplied buffer, it might indicate\ntruncated or corrupted data (e.g., if you were expecting an exact number of bytes to be\nread).\nFinally, be on the lookout for other “into” related functions in various library modules\n(e.g., recv_into(), pack_into(), etc.). Many other parts of Python have support for\ndirect I/O or data access that can be used to fill or alter the contents of arrays and buffers.\nSee Recipe 6.12 for a significantly more advanced example of interpreting binary struc‐\ntures and usage of memoryviews.\n5.10. Memory Mapping Binary Files\nProblem\nYou want to memory map a binary file into a mutable byte array, possibly for random\naccess to its contents or to make in-place modifications.\nSolution\nUse the mmap module to memory map files. Here is a utility function that shows how to\nopen a file and memory map it in a portable manner:\nimport os\nimport mmap\n5.10. Memory Mapping Binary Files \n| \n153",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": 4,
      "content": "def memory_map(filename, access=mmap.ACCESS_WRITE):\n    size = os.path.getsize(filename)\n    fd = os.open(filename, os.O_RDWR)\n    return mmap.mmap(fd, size, access=access)\nTo use this function, you would need to have a file already created and filled with data.\nHere is an example of how you could initially create a file and expand it to a desired\nsize:\n>>> size = 1000000\n>>> with open('data', 'wb') as f:\n...      f.seek(size-1)\n...      f.write(b'\\x00')\n...\n>>>\nNow here is an example of memory mapping the contents using the memory_map()\nfunction:\n>>> m = memory_map('data')\n>>> len(m)\n1000000\n>>> m[0:10]\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n>>> m[0]\n0\n>>> # Reassign a slice\n>>> m[0:11] = b'Hello World'\n>>> m.close()\n>>> # Verify that changes were made\n>>> with open('data', 'rb') as f:\n...      print(f.read(11))\n...\nb'Hello World'\n>>>\nThe mmap object returned by mmap() can also be used as a context manager, in which\ncase the underlying file is closed automatically. For example:\n>>> with memory_map('data') as m:\n...      print(len(m))\n...      print(m[0:10])\n...\n1000000\nb'Hello World'\n>>> m.closed\nTrue\n>>>\nBy default, the memory_map() function shown opens a file for both reading and writing.\nAny modifications made to the data are copied back to the original file. If read-only\n154 \n| \nChapter 5: Files and I/O",
      "content_length": 1334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": 4,
      "content": "access is needed instead, supply mmap.ACCESS_READ for the access argument. For\nexample:\nm = memory_map(filename, mmap.ACCESS_READ)\nIf you intend to modify the data locally, but don’t want those changes written back to\nthe original file, use mmap.ACCESS_COPY:\nm = memory_map(filename, mmap.ACCESS_COPY)\nDiscussion\nUsing mmap to map files into memory can be an efficient and elegant means for randomly\naccessing the contents of a file. For example, instead of opening a file and performing\nvarious combinations of seek(), read(), and write() calls, you can simply map the\nfile and access the data using slicing operations.\nNormally, the memory exposed by mmap() looks like a bytearray object. However, you\ncan interpret the data differently using a memoryview. For example:\n>>> m = memory_map('data')\n>>> # Memoryview of unsigned integers\n>>> v = memoryview(m).cast('I')\n>>> v[0] = 7\n>>> m[0:4]\nb'\\x07\\x00\\x00\\x00'\n>>> m[0:4] = b'\\x07\\x01\\x00\\x00'\n>>> v[0]\n263\n>>>\nIt should be emphasized that memory mapping a file does not cause the entire file to be\nread into memory. That is, it’s not copied into some kind of memory buffer or array.\nInstead, the operating system merely reserves a section of virtual memory for the file\ncontents. As you access different regions, those portions of the file will be read and\nmapped into the memory region as needed. However, parts of the file that are never\naccessed simply stay on disk. This all happens transparently, behind the scenes.\nIf more than one Python interpreter memory maps the same file, the resulting mmap\nobject can be used to exchange data between interpreters. That is, all interpreters can\nread/write data simultaneously, and changes made to the data in one interpreter will\nautomatically appear in the others. Obviously, some extra care is required to synchronize\nthings, but this kind of approach is sometimes used as an alternative to transmitting\ndata in messages over pipes or sockets.\nAs shown, this recipe has been written to be as general purpose as possible, working on\nboth Unix and Windows. Be aware that there are some platform differences concerning\nthe use of the mmap() call hidden behind the scenes. In addition, there are options to\n5.10. Memory Mapping Binary Files \n| \n155",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": 4,
      "content": "create anonymously mapped memory regions. If this is of interest to you, make sure\nyou carefully read the Python documentation on the subject. \n5.11. Manipulating Pathnames\nProblem\nYou need to manipulate pathnames in order to find the base filename, directory name,\nabsolute path, and so on.\nSolution\nTo manipulate pathnames, use the functions in the os.path module. Here is an inter‐\nactive example that illustrates a few key features:\n>>> import os\n>>> path = '/Users/beazley/Data/data.csv'\n>>> # Get the last component of the path\n>>> os.path.basename(path)\n'data.csv'\n>>> # Get the directory name\n>>> os.path.dirname(path)\n'/Users/beazley/Data'\n>>> # Join path components together\n>>> os.path.join('tmp', 'data', os.path.basename(path))\n'tmp/data/data.csv'\n>>> # Expand the user's home directory\n>>> path = '~/Data/data.csv'\n>>> os.path.expanduser(path)\n'/Users/beazley/Data/data.csv'\n>>> # Split the file extension\n>>> os.path.splitext(path)\n('~/Data/data', '.csv')\n>>>\nDiscussion\nFor any manipulation of filenames, you should use the os.path module instead of trying\nto cook up your own code using the standard string operations. In part, this is for\nportability. The os.path module knows about differences between Unix and Windows\nand can reliably deal with filenames such as Data/data.csv and Data\\data.csv. Second,\nyou really shouldn’t spend your time reinventing the wheel. It’s usually best to use the\nfunctionality that’s already provided for you.\n156 \n| \nChapter 5: Files and I/O",
      "content_length": 1492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": 4,
      "content": "It should be noted that the os.path module has many more features not shown in this\nrecipe. Consult the documentation for more functions related to file testing, symbolic\nlinks, and so forth.\n5.12. Testing for the Existence of a File\nProblem\nYou need to test whether or not a file or directory exists.\nSolution\nUse the os.path module to test for the existence of a file or directory. For example:\n>>> import os\n>>> os.path.exists('/etc/passwd')\nTrue\n>>> os.path.exists('/tmp/spam')\nFalse\n>>>\nYou can perform further tests to see what kind of file it might be. These tests return\nFalse if the file in question doesn’t exist:\n>>> # Is a regular file\n>>> os.path.isfile('/etc/passwd')\nTrue\n>>> # Is a directory\n>>> os.path.isdir('/etc/passwd')\nFalse\n>>> # Is a symbolic link\n>>> os.path.islink('/usr/local/bin/python3')\nTrue\n>>> # Get the file linked to\n>>> os.path.realpath('/usr/local/bin/python3')\n'/usr/local/bin/python3.3'\n>>>\nIf you need to get metadata (e.g., the file size or modification date), that is also available\nin the os.path module.\n>>> os.path.getsize('/etc/passwd')\n3669\n>>> os.path.getmtime('/etc/passwd')\n1272478234.0\n>>> import time\n>>> time.ctime(os.path.getmtime('/etc/passwd'))\n5.12. Testing for the Existence of a File \n| \n157",
      "content_length": 1249,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": 4,
      "content": "'Wed Apr 28 13:10:34 2010'\n>>>\nDiscussion\nFile testing is a straightforward operation using os.path. Probably the only thing to be\naware of when writing scripts is that you might need to worry about permissions—\nespecially for operations that get metadata. For example:\n>>> os.path.getsize('/Users/guido/Desktop/foo.txt')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.3/genericpath.py\", line 49, in getsize\n    return os.stat(filename).st_size\nPermissionError: [Errno 13] Permission denied: '/Users/guido/Desktop/foo.txt'\n>>>\n5.13. Getting a Directory Listing\nProblem\nYou want to get a list of the files contained in a directory on the filesystem.\nSolution\nUse the os.listdir() function to obtain a list of files in a directory:\nimport os\nnames = os.listdir('somedir')\nThis will give you the raw directory listing, including all files, subdirectories, symbolic\nlinks, and so forth. If you need to filter the data in some way, consider using a list\ncomprehension combined with various functions in the os.path library. For example:\nimport os.path\n# Get all regular files\nnames = [name for name in os.listdir('somedir')\n         if os.path.isfile(os.path.join('somedir', name))]\n# Get all dirs\ndirnames = [name for name in os.listdir('somedir')\n            if os.path.isdir(os.path.join('somedir', name))]\nThe startswith() and endswith() methods of strings can be useful for filtering the\ncontents of a directory as well. For example:\npyfiles = [name for name in os.listdir('somedir')\n           if name.endswith('.py')]\n158 \n| \nChapter 5: Files and I/O",
      "content_length": 1609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": 4,
      "content": "For filename matching, you may want to use the glob or fnmatch modules instead. For\nexample:\nimport glob\npyfiles = glob.glob('somedir/*.py')\nfrom fnmatch import fnmatch\npyfiles = [name for name in os.listdir('somedir')\n           if fnmatch(name, '*.py')]\nDiscussion\nGetting a directory listing is easy, but it only gives you the names of entries in the\ndirectory. If you want to get additional metadata, such as file sizes, modification dates,\nand so forth, you either need to use additional functions in the os.path module or use\nthe os.stat() function. To collect the data. For example:\n# Example of getting a directory listing\nimport os\nimport os.path\nimport glob\npyfiles = glob.glob('*.py')\n# Get file sizes and modification dates\nname_sz_date = [(name, os.path.getsize(name), os.path.getmtime(name))\n                for name in pyfiles]\nfor name, size, mtime in name_sz_date:\n    print(name, size, mtime)\n# Alternative: Get file metadata\nfile_metadata = [(name, os.stat(name)) for name in pyfiles]\nfor name, meta in file_metadata:\n    print(name, meta.st_size, meta.st_mtime)\nLast, but not least, be aware that there are subtle issues that can arise in filename handling\nrelated to encodings. Normally, the entries returned by a function such as os.list\ndir() are decoded according to the system default filename encoding. However, it’s\npossible under certain circumstances to encounter un-decodable filenames. Recipes\n5.14 and 5.15 have more details about handling such names.\n5.13. Getting a Directory Listing \n| \n159",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": 4,
      "content": "5.14. Bypassing Filename Encoding\nProblem\nYou want to perform file I/O operations using raw filenames that have not been decoded\nor encoded according to the default filename encoding.\nSolution\nBy default, all filenames are encoded and decoded according to the text encoding re‐\nturned by sys.getfilesystemencoding(). For example:\n>>> sys.getfilesystemencoding()\n'utf-8'\n>>>\nIf you want to bypass this encoding for some reason, specify a filename using a raw byte\nstring instead. For example:\n>>> # Wrte a file using a unicode filename\n>>> with open('jalape\\xf1o.txt', 'w') as f:\n...     f.write('Spicy!')\n...\n6\n>>> # Directory listing (decoded)\n>>> import os\n>>> os.listdir('.')\n['jalapeño.txt']\n>>> # Directory listing (raw)\n>>> os.listdir(b'.')        # Note: byte string\n[b'jalapen\\xcc\\x83o.txt']\n>>> # Open file with raw filename\n>>> with open(b'jalapen\\xcc\\x83o.txt') as f:\n...     print(f.read())\n...\nSpicy!\n>>>\nAs you can see in the last two operations, the filename handling changes ever so slightly\nwhen byte strings are supplied to file-related functions, such as open() and os.list\ndir().\nDiscussion\nUnder normal circumstances, you shouldn’t need to worry about filename encoding\nand decoding—normal filename operations should just work. However, many operating\nsystems may allow a user through accident or malice to create files with names that don’t\n160 \n| \nChapter 5: Files and I/O",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": 4,
      "content": "conform to the expected encoding rules. Such filenames may mysteriously break Python\nprograms that work with a lot of files.\nReading directories and working with filenames as raw undecoded bytes has the po‐\ntential to avoid such problems, albeit at the cost of programming convenience.\nSee Recipe 5.15 for a recipe on printing undecodable filenames.\n5.15. Printing Bad Filenames\nProblem\nYour program received a directory listing, but when it tried to print the filenames, it\ncrashed with a UnicodeEncodeError exception and a cryptic message about “surrogates\nnot allowed.”\nSolution\nWhen printing filenames of unknown origin, use this convention to avoid errors:\ndef bad_filename(filename):\n    return repr(filename)[1:-1]\ntry:\n    print(filename)\nexcept UnicodeEncodeError:\n    print(bad_filename(filename))\nDiscussion\nThis recipe is about a potentially rare but very annoying problem regarding programs\nthat must manipulate the filesystem. By default, Python assumes that all filenames are\nencoded according to the setting reported by sys.getfilesystemencoding(). How‐\never, certain filesystems don’t necessarily enforce this encoding restriction, thereby al‐\nlowing files to be created without proper filename encoding. It’s not common, but there\nis always the danger that some user will do something silly and create such a file by\naccident (e.g., maybe passing a bad filename to open() in some buggy code).\nWhen executing a command such as os.listdir(), bad filenames leave Python in a\nbind. On the one hand, it can’t just discard bad names. On the other hand, it still can’t\nturn the filename into a proper text string. Python’s solution to this problem is to take\nan undecodable byte value \\xhh in a filename and map it into a so-called “surrogate\nencoding” represented by the Unicode character \\udchh. Here is an example of how a\nbad directory listing might look if it contained a filename bäd.txt, encoded as Latin-1\ninstead of UTF-8:\n5.15. Printing Bad Filenames \n| \n161",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": 4,
      "content": ">>> import os\n>>> files = os.listdir('.')\n>>> files\n['spam.py', 'b\\udce4d.txt', 'foo.txt']\n>>>\nIf you have code that manipulates filenames or even passes them to functions such as\nopen(), everything works normally. It’s only in situations where you want to output the\nfilename that you run into trouble (e.g., printing it to the screen, logging it, etc.). Specif‐\nically, if you tried to print the preceding listing, your program will crash:\n>>> for name in files:\n...     print(name)\n...\nspam.py\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\nUnicodeEncodeError: 'utf-8' codec can't encode character '\\udce4' in\nposition 1: surrogates not allowed\n>>>\nThe reason it crashes is that the character \\udce4 is technically invalid Unicode. It’s\nactually the second half of a two-character combination known as a surrogate pair.\nHowever, since the first half is missing, it’s invalid Unicode. Thus, the only way to pro‐\nduce successful output is to take corrective action when a bad filename is encountered.\nFor example, changing the code to the recipe produces the following:\n>>> for name in files:\n...     try:\n...             print(name)\n...     except UnicodeEncodeError:\n...             print(bad_filename(name))\n...\nspam.py\nb\\udce4d.txt\nfoo.txt\n>>>\nThe choice of what to do for the bad_filename() function is largely up to you. Another\noption is to re-encode the value in some way, like this:\ndef bad_filename(filename):\n    temp = filename.encode(sys.getfilesystemencoding(), errors='surrogateescape')\n    return temp.decode('latin-1')\nUsing this version produces the following output:\n>>> for name in files:\n...     try:\n...             print(name)\n...     except UnicodeEncodeError:\n...             print(bad_filename(name))\n...\n162 \n| \nChapter 5: Files and I/O",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": 4,
      "content": "spam.py\nbäd.txt\nfoo.txt\n>>>\nThis recipe will likely be ignored by most readers. However, if you’re writing mission-\ncritical scripts that need to work reliably with filenames and the filesystem, it’s something\nto think about. Otherwise, you might find yourself called back into the office over the\nweekend to debug a seemingly inscrutable error.\n5.16. Adding or Changing the Encoding of an Already\nOpen File\nProblem\nYou want to add or change the Unicode encoding of an already open file without closing\nit first.\nSolution\nIf you want to add Unicode encoding/decoding to an already existing file object that’s\nopened in binary mode, wrap it with an io.TextIOWrapper() object. For example:\nimport urllib.request\nimport io\nu = urllib.request.urlopen('http://www.python.org')\nf = io.TextIOWrapper(u,encoding='utf-8')\ntext = f.read()\nIf you want to change the encoding of an already open text-mode file, use its detach()\nmethod to remove the existing text encoding layer before replacing it with a new one.\nHere is an example of changing the encoding on sys.stdout:\n>>> import sys\n>>> sys.stdout.encoding\n'UTF-8'\n>>> sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='latin-1')\n>>> sys.stdout.encoding\n'latin-1'\n>>>\nDoing this might break the output of your terminal. It’s only meant to illustrate.\nDiscussion\nThe I/O system is built as a series of layers. You can see the layers yourself by trying this\nsimple example involving a text file:\n5.16. Adding or Changing the Encoding of an Already Open File \n| \n163",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": 4,
      "content": ">>> f = open('sample.txt','w')\n>>> f\n<_io.TextIOWrapper name='sample.txt' mode='w' encoding='UTF-8'>\n>>> f.buffer\n<_io.BufferedWriter name='sample.txt'>\n>>> f.buffer.raw\n<_io.FileIO name='sample.txt' mode='wb'>\n>>>\nIn this example, io.TextIOWrapper is a text-handling layer that encodes and decodes\nUnicode, io.BufferedWriter is a buffered I/O layer that handles binary data, and \nio.FileIO is a raw file representing the low-level file descriptor in the operating system.\nAdding or changing the text encoding involves adding or changing the topmost\nio.TextIOWrapper layer.\nAs a general rule, it’s not safe to directly manipulate the different layers by accessing the\nattributes shown. For example, see what happens if you try to change the encoding using\nthis technique:\n>>> f\n<_io.TextIOWrapper name='sample.txt' mode='w' encoding='UTF-8'>\n>>> f = io.TextIOWrapper(f.buffer, encoding='latin-1')\n>>> f\n<_io.TextIOWrapper name='sample.txt' encoding='latin-1'>\n>>> f.write('Hello')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: I/O operation on closed file.\n>>>\nIt doesn’t work because the original value of f got destroyed and closed the underlying\nfile in the process.\nThe detach() method disconnects the topmost layer of a file and returns the next lower\nlayer. Afterward, the top layer will no longer be usable. For example:\n>>> f = open('sample.txt', 'w')\n>>> f\n<_io.TextIOWrapper name='sample.txt' mode='w' encoding='UTF-8'>\n>>> b = f.detach()\n>>> b\n<_io.BufferedWriter name='sample.txt'>\n>>> f.write('hello')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: underlying buffer has been detached\n>>>\nOnce detached, however, you can add a new top layer to the returned result. For example:\n>>> f = io.TextIOWrapper(b, encoding='latin-1')\n>>> f\n164 \n| \nChapter 5: Files and I/O",
      "content_length": 1854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": 4,
      "content": "<_io.TextIOWrapper name='sample.txt' encoding='latin-1'>\n>>>\nAlthough changing the encoding has been shown, it is also possible to use this technique\nto change the line handling, error policy, and other aspects of file handling. For example:\n>>> sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='ascii',\n...                               errors='xmlcharrefreplace')\n>>> print('Jalape\\u00f1o')\nJalape&#241;o\n>>>\nNotice how the non-ASCII character ñ has been replaced by &#241; in the output.\n5.17. Writing Bytes to a Text File\nProblem\nYou want to write raw bytes to a file opened in text mode.\nSolution\nSimply write the byte data to the files underlying buffer. For example:\n>>> import sys\n>>> sys.stdout.write(b'Hello\\n')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: must be str, not bytes\n>>> sys.stdout.buffer.write(b'Hello\\n')\nHello\n5\n>>>\nSimilarly, binary data can be read from a text file by reading from its buffer attribute\ninstead.\nDiscussion\nThe I/O system is built from layers. Text files are constructed by adding a Unicode\nencoding/decoding layer on top of a buffered binary-mode file. The buffer attribute\nsimply points at this underlying file. If you access it, you’ll bypass the text encoding/\ndecoding layer.\nThe example involving sys.stdout might be viewed as a special case. By default,\nsys.stdout is always opened in text mode. However, if you are writing a script that\nactually needs to dump binary data to standard output, you can use the technique shown\nto bypass the text encoding.)\n5.17. Writing Bytes to a Text File \n| \n165",
      "content_length": 1598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": 4,
      "content": "5.18. Wrapping an Existing File Descriptor As a File Object\nProblem\nYou have an integer file descriptor correponding to an already open I/O channel on the\noperating system (e.g., file, pipe, socket, etc.), and you want to wrap a higher-level\nPython file object around it.\nSolution\nA file descriptor is different than a normal open file in that it is simply an integer handle\nassigned by the operating system to refer to some kind of system I/O channel. If you\nhappen to have such a file descriptor, you can wrap a Python file object around it using\nthe open() function. However, you simply supply the integer file descriptor as the first\nargument instead of the filename. For example:\n# Open a low-level file descriptor\nimport os\nfd = os.open('somefile.txt', os.O_WRONLY | os.O_CREAT)\n# Turn into a proper file\nf = open(fd, 'wt')\nf.write('hello world\\n')\nf.close()\nWhen the high-level file object is closed or destroyed, the underlying file descriptor will\nalso be closed. If this is not desired, supply the optional closefd=False argument to\nopen(). For example:\n# Create a file object, but don't close underlying fd when done\nf = open(fd, 'wt', closefd=False)\n...\nDiscussion\nOn Unix systems, this technique of wrapping a file descriptor can be a convenient means\nfor putting a file-like interface on an existing I/O channel that was opened in a different\nway (e.g., pipes, sockets, etc.). For instance, here is an example involving sockets:\nfrom socket import socket, AF_INET, SOCK_STREAM\ndef echo_client(client_sock, addr):\n    print('Got connection from', addr)\n    # Make text-mode file wrappers for socket reading/writing\n    client_in = open(client_sock.fileno(), 'rt', encoding='latin-1',\n                         closefd=False)\n    client_out = open(client_sock.fileno(), 'wt', encoding='latin-1',\n                          closefd=False)\n166 \n| \nChapter 5: Files and I/O",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": 4,
      "content": "# Echo lines back to the client using file I/O\n    for line in client_in:\n        client_out.write(line)\n        client_out.flush()\n    client_sock.close()\ndef echo_server(address):\n    sock = socket(AF_INET, SOCK_STREAM)\n    sock.bind(address)\n    sock.listen(1)\n    while True:\n        client, addr = sock.accept()\n        echo_client(client, addr)\nIt’s important to emphasize that the above example is only meant to illustrate a feature\nof the built-in open() function and that it only works on Unix-based systems. If you are\ntrying to put a file-like interface on a socket and need your code to be cross platform,\nuse the makefile() method of sockets instead. However, if portability is not a concern,\nyou’ll find that the above solution provides much better performance than using make\nfile().\nYou can also use this to make a kind of alias that allows an already open file to be used\nin a slightly different way than how it was first opened. For example, here’s how you\ncould create a file object that allows you to emit binary data on stdout (which is normally\nopened in text mode):\nimport sys\n# Create a binary-mode file for stdout\nbstdout = open(sys.stdout.fileno(), 'wb', closefd=False)\nbstdout.write(b'Hello World\\n')\nbstdout.flush()\nAlthough it’s possible to wrap an existing file descriptor as a proper file, be aware that\nnot all file modes may be supported and that certain kinds of file descriptors may have\nfunny side effects (especially with respect to error handling, end-of-file conditions, etc.).\nThe behavior can also vary according to operating system. In particular, none of the\nexamples are likely to work on non-Unix systems. The bottom line is that you’ll need\nto thoroughly test your implementation to make sure it works as expected.\n5.19. Making Temporary Files and Directories\nProblem\nYou need to create a temporary file or directory for use when your program executes.\nAfterward, you possibly want the file or directory to be destroyed.\n5.19. Making Temporary Files and Directories \n| \n167",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": 4,
      "content": "Solution\nThe tempfile module has a variety of functions for performing this task. To make an\nunnamed temporary file, use tempfile.TemporaryFile:\nfrom tempfile import TemporaryFile\nwith TemporaryFile('w+t') as f:\n     # Read/write to the file\n     f.write('Hello World\\n')\n     f.write('Testing\\n')\n     # Seek back to beginning and read the data\n     f.seek(0)\n     data = f.read()\n# Temporary file is destroyed\nOr, if you prefer, you can also use the file like this:\nf = TemporaryFile('w+t')\n# Use the temporary file\n...\nf.close()\n# File is destroyed\nThe first argument to TemporaryFile() is the file mode, which is usually w+t for text\nand w+b for binary. This mode simultaneously supports reading and writing, which is\nuseful here since closing the file to change modes would actually destroy it. Temporary\nFile() additionally accepts the same arguments as the built-in open() function. For\nexample:\nwith TemporaryFile('w+t', encoding='utf-8', errors='ignore') as f:\n     ...\nOn most Unix systems, the file created by TemporaryFile() is unnamed and won’t even\nhave a directory entry. If you want to relax this constraint, use NamedTemporary\nFile() instead. For example:\nfrom tempfile import NamedTemporaryFile\nwith NamedTemporaryFile('w+t') as f:\n    print('filename is:', f.name)\n    ...\n# File automatically destroyed\nHere, the f.name attribute of the opened file contains the filename of the temporary file.\nThis can be useful if it needs to be given to some other code that needs to open the file.\nAs with TemporaryFile(), the resulting file is automatically deleted when it’s closed. If\nyou don’t want this, supply a delete=False keyword argument. For example:\n168 \n| \nChapter 5: Files and I/O",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": 4,
      "content": "with NamedTemporaryFile('w+t', delete=False) as f:\n    print('filename is:', f.name)\n    ...\nTo make a temporary directory, use tempfile.TemporaryDirectory(). For example:\nfrom tempfile import TemporaryDirectory\nwith TemporaryDirectory() as dirname:\n     print('dirname is:', dirname)\n     # Use the directory\n     ...\n# Directory and all contents destroyed\nDiscussion\nThe TemporaryFile(), NamedTemporaryFile(), and TemporaryDirectory() functions\nare probably the most convenient way to work with temporary files and directories,\nbecause they automatically handle all of the steps of creation and subsequent cleanup.\nAt a lower level, you can also use the mkstemp() and mkdtemp() to create temporary\nfiles and directories. For example:\n>>> import tempfile\n>>> tempfile.mkstemp()\n(3, '/var/folders/7W/7WZl5sfZEF0pljrEB1UMWE+++TI/-Tmp-/tmp7fefhv')\n>>> tempfile.mkdtemp()\n'/var/folders/7W/7WZl5sfZEF0pljrEB1UMWE+++TI/-Tmp-/tmp5wvcv6'\n>>>\nHowever, these functions don’t really take care of further management. For example,\nthe mkstemp() function simply returns a raw OS file descriptor and leaves it up to you\nto turn it into a proper file. Similarly, it’s up to you to clean up the files if you want.\nNormally, temporary files are created in the system’s default location, such\nas /var/tmp or similar. To find out the actual location, use the tempfile.gettemp\ndir() function. For example:\n>>> tempfile.gettempdir()\n'/var/folders/7W/7WZl5sfZEF0pljrEB1UMWE+++TI/-Tmp-'\n>>>\nAll of the temporary-file-related functions allow you to override this directory as well\nas the naming conventions using the prefix, suffix, and dir keyword arguments. For\nexample:\n>>> f = NamedTemporaryFile(prefix='mytemp', suffix='.txt', dir='/tmp')\n>>> f.name\n'/tmp/mytemp8ee899.txt'\n>>>\nLast, but not least, to the extent possible, the tempfile() module creates temporary\nfiles in the most secure manner possible. This includes only giving access permission\n5.19. Making Temporary Files and Directories \n| \n169",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": 4,
      "content": "to the current user and taking steps to avoid race conditions in file creation. Be aware\nthat there can be differences between platforms. Thus, you should make sure to check\nthe official documentation for the finer points.\n5.20. Communicating with Serial Ports\nProblem\nYou want to read and write data over a serial port, typically to interact with some kind\nof hardware device (e.g., a robot or sensor).\nSolution\nAlthough you can probably do this directly using Python’s built-in I/O primitives, your\nbest bet for serial communication is to use the pySerial package. Getting started with\nthe package is very easy. You simply open up a serial port using code like this:\nimport serial\nser = serial.Serial('/dev/tty.usbmodem641',  # Device name varies\n                     baudrate=9600,\n                     bytesize=8,\n                     parity='N',\n                     stopbits=1)\nThe device name will vary according to the kind of device and operating system. For\ninstance, on Windows, you can use a device of 0, 1, and so on, to open up the commu‐\nnication ports such as “COM0” and “COM1.” Once open, you can read and write data\nusing read(), readline(), and write() calls. For example:\nser.write(b'G1 X50 Y50\\r\\n')\nresp = ser.readline()\nFor the most part, simple serial communication should be pretty simple from this point\nforward.\nDiscussion\nAlthough simple on the surface, serial communication can sometimes get rather messy.\nOne reason you should use a package such as pySerial is that it provides support for\nadvanced features (e.g., timeouts, control flow, buffer flushing, handshaking, etc.). For\ninstance, if you want to enable RTS-CTS handshaking, you simply provide a\nrtscts=True argument to Serial(). The provided documentation is excellent, so\nthere’s little benefit to paraphrasing it here.\nKeep in mind that all I/O involving serial ports is binary. Thus, make sure you write\nyour code to use bytes instead of text (or perform proper text encoding/decoding as\n170 \n| \nChapter 5: Files and I/O",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": 4,
      "content": "needed). The struct module may also be useful should you need to create binary-coded\ncommands or packets.\n5.21. Serializing Python Objects\nProblem\nYou need to serialize a Python object into a byte stream so that you can do things such\nas save it to a file, store it in a database, or transmit it over a network connection.\nSolution\nThe most common approach for serializing data is to use the pickle module. To dump\nan object to a file, you do this:\nimport pickle\ndata = ...   # Some Python object\nf = open('somefile', 'wb')\npickle.dump(data, f)\nTo dump an object to a string, use pickle.dumps():\ns = pickle.dumps(data)\nTo re-create an object from a byte stream, use either the pickle.load() or pick\nle.loads() functions. For example:\n# Restore from a file\nf = open('somefile', 'rb')\ndata = pickle.load(f)\n# Restore from a string\ndata = pickle.loads(s)\nDiscussion\nFor most programs, usage of the dump() and load() function is all you need to effectively\nuse pickle. It simply works with most Python data types and instances of user-defined\nclasses. If you’re working with any kind of library that lets you do things such as save/\nrestore Python objects in databases or transmit objects over the network, there’s a pretty\ngood chance that pickle is being used.\npickle is a Python-specific self-describing data encoding. By self-describing, the seri‐\nalized data contains information related to the start and end of each object as well as\ninformation about its type. Thus, you don’t need to worry about defining records—it\nsimply works. For example, if working with multiple objects, you can do this:\n5.21. Serializing Python Objects \n| \n171",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": 4,
      "content": ">>> import pickle\n>>> f = open('somedata', 'wb')\n>>> pickle.dump([1, 2, 3, 4], f)\n>>> pickle.dump('hello', f)\n>>> pickle.dump({'Apple', 'Pear', 'Banana'}, f)\n>>> f.close()\n>>> f = open('somedata', 'rb')\n>>> pickle.load(f)\n[1, 2, 3, 4]\n>>> pickle.load(f)\n'hello'\n>>> pickle.load(f)\n{'Apple', 'Pear', 'Banana'}\n>>>\nYou can pickle functions, classes, and instances, but the resulting data only encodes\nname references to the associated code objects. For example:\n>>> import math\n>>> import pickle.\n>>> pickle.dumps(math.cos)\nb'\\x80\\x03cmath\\ncos\\nq\\x00.'\n>>>\nWhen the data is unpickled, it is assumed that all of the required source is available.\nModules, classes, and functions will automatically be imported as needed. For applica‐\ntions where Python data is being shared between interpreters on different machines,\nthis is a potential maintenance issue, as all machines must have access to the same source\ncode.\npickle.load() should never be used on untrusted data. As a side effect\nof loading, pickle will automatically load modules and make instances.\nHowever, an evildoer who knows how pickle works can create “mal‐\nformed” data that causes Python to execute arbitrary system com‐\nmands. Thus, it’s essential that pickle only be used internally with in‐\nterpreters that have some ability to authenticate one another.\nCertain kinds of objects can’t be pickled. These are typically objects that involve some\nsort of external system state, such as open files, open network connections, threads,\nprocesses, stack frames, and so forth. User-defined classes can sometimes work around\nthese limitations by providing __getstate__() and __setstate__() methods. If de‐\nfined, pickle.dump() will call __getstate__() to get an object that can be pickled.\nSimilarly, __setstate__() will be invoked on unpickling. To illustrate what’s possible,\nhere is a class that internally defines a thread but can still be pickled/unpickled:\n# countdown.py\nimport time\nimport threading\n172 \n| \nChapter 5: Files and I/O",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": 4,
      "content": "class Countdown:\n    def __init__(self, n):\n        self.n = n\n        self.thr = threading.Thread(target=self.run)\n        self.thr.daemon = True\n        self.thr.start()\n    def run(self):\n        while self.n > 0:\n            print('T-minus', self.n)\n            self.n -= 1\n            time.sleep(5)\n    def __getstate__(self):\n        return self.n\n    def __setstate__(self, n):\n        self.__init__(n)\nTry the following experiment involving pickling:\n>>> import countdown\n>>> c = countdown.Countdown(30)\n>>> T-minus 30\nT-minus 29\nT-minus 28\n...\n>>> # After a few moments\n>>> f = open('cstate.p', 'wb')\n>>> import pickle\n>>> pickle.dump(c, f)\n>>> f.close()\nNow quit Python and try this after restart:\n>>> f = open('cstate.p', 'rb')\n>>> pickle.load(f)\ncountdown.Countdown object at 0x10069e2d0>\nT-minus 19\nT-minus 18\n...\nYou should see the thread magically spring to life again, picking up where it left off when\nyou first pickled it.\npickle is not a particularly efficient encoding for large data structures such as binary\narrays created by libraries like the array module or numpy. If you’re moving large\namounts of array data around, you may be better off simply saving bulk array data in a\nfile or using a more standardized encoding, such as HDF5 (supported by third-party\nlibraries).\n5.21. Serializing Python Objects \n| \n173",
      "content_length": 1335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": 4,
      "content": "Because of its Python-specific nature and attachment to source code, you probably\nshouldn’t use pickle as a format for long-term storage. For example, if the source code\nchanges, all of your stored data might break and become unreadable. Frankly, for storing\ndata in databases and archival storage, you’re probably better off using a more standard\ndata encoding, such as XML, CSV, or JSON. These encodings are more standardized,\nsupported by many different languages, and more likely to be better adapted to changes\nin your source code.\nLast, but not least, be aware that pickle has a huge variety of options and tricky corner\ncases. For the most common uses, you don’t need to worry about them, but a look at\nthe official documentation should be required if you’re going to build a signficant ap‐\nplication that uses pickle for serialization.\n174 \n| \nChapter 5: Files and I/O",
      "content_length": 876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": 4,
      "content": "CHAPTER 6\nData Encoding and Processing\nThe main focus of this chapter is using Python to process data presented in different\nkinds of common encodings, such as CSV files, JSON, XML, and binary packed records.\nUnlike the chapter on data structures, this chapter is not focused on specific algorithms,\nbut instead on the problem of getting data in and out of a program.\n6.1. Reading and Writing CSV Data\nProblem\nYou want to read or write data encoded as a CSV file.\nSolution\nFor most kinds of CSV data, use the csv library. For example, suppose you have some\nstock market data in a file named stocks.csv like this:\n    Symbol,Price,Date,Time,Change,Volume\n    \"AA\",39.48,\"6/11/2007\",\"9:36am\",-0.18,181800\n    \"AIG\",71.38,\"6/11/2007\",\"9:36am\",-0.15,195500\n    \"AXP\",62.58,\"6/11/2007\",\"9:36am\",-0.46,935000\n    \"BA\",98.31,\"6/11/2007\",\"9:36am\",+0.12,104800\n    \"C\",53.08,\"6/11/2007\",\"9:36am\",-0.25,360900\n    \"CAT\",78.29,\"6/11/2007\",\"9:36am\",-0.23,225400\nHere’s how you would read the data as a sequence of tuples:\nimport csv\nwith open('stocks.csv') as f:\n    f_csv = csv.reader(f)\n    headers = next(f_csv)\n    for row in f_csv:\n175",
      "content_length": 1128,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": 4,
      "content": "# Process row\n        ...\nIn the preceding code, row will be a tuple. Thus, to access certain fields, you will need\nto use indexing, such as row[0] (Symbol) and row[4] (Change).\nSince such indexing can often be confusing, this is one place where you might want to\nconsider the use of named tuples. For example:\nfrom collections import namedtuple\nwith open('stock.csv') as f:\n    f_csv = csv.reader(f)\n    headings = next(f_csv)\n    Row = namedtuple('Row', headings)\n    for r in f_csv:\n        row = Row(*r)\n        # Process row\n        ...\nThis would allow you to use the column headers such as row.Symbol and row.Change\ninstead of indices. It should be noted that this only works if the column headers are valid\nPython identifiers. If not, you might have to massage the initial headings (e.g., replacing\nnonidentifier characters with underscores or similar).\nAnother alternative is to read the data as a sequence of dictionaries instead. To do that,\nuse this code:\nimport csv\nwith open('stocks.csv') as f:\n    f_csv = csv.DictReader(f)\n    for row in f_csv:\n        # process row\n        ...\nIn this version, you would access the elements of each row using the row headers. For\nexample, row['Symbol'] or row['Change'].\nTo write CSV data, you also use the csv module but create a writer object. For example:\nheaders = ['Symbol','Price','Date','Time','Change','Volume']\nrows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800),\n        ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500),\n        ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000),\n       ]\nwith open('stocks.csv','w') as f:\n    f_csv = csv.writer(f)\n    f_csv.writerow(headers)\n    f_csv.writerows(rows)\nIf you have the data as a sequence of dictionaries, do this:\n176 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": 4,
      "content": "headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']\nrows = [{'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007',\n          'Time':'9:36am', 'Change':-0.18, 'Volume':181800},\n        {'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007',\n          'Time':'9:36am', 'Change':-0.15, 'Volume': 195500},\n        {'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007',\n          'Time':'9:36am', 'Change':-0.46, 'Volume': 935000},\n        ]\nwith open('stocks.csv','w') as f:\n    f_csv = csv.DictWriter(f, headers)\n    f_csv.writeheader()\n    f_csv.writerows(rows)\nDiscussion\nYou should almost always prefer the use of the csv module over manually trying to split\nand parse CSV data yourself. For instance, you might be inclined to just write some\ncode like this:\nwith open('stocks.csv') as f:\n    for line in f:\n        row = line.split(',')\n        # process row\n        ...\nThe problem with this approach is that you’ll still need to deal with some nasty details.\nFor example, if any of the fields are surrounded by quotes, you’ll have to strip the quotes.\nIn addition, if a quoted field happens to contain a comma, the code will break by pro‐\nducing a row with the wrong size.\nBy default, the csv library is programmed to understand CSV encoding rules used by \nMicrosoft Excel. This is probably the most common variant, and will likely give you the\nbest compatibility. However, if you consult the documentation for csv, you’ll see a few\nways to tweak the encoding to different formats (e.g., changing the separator character,\netc.). For example, if you want to read tab-delimited data instead, use this:\n# Example of reading tab-separated values\nwith open('stock.tsv') as f:\n    f_tsv = csv.reader(f, delimiter='\\t')\n    for row in f_tsv:\n        # Process row\n        ...\nIf you’re reading CSV data and converting it into named tuples, you need to be a little\ncareful with validating column headers. For example, a CSV file could have a header\nline containing nonvalid identifier characters like this:\nStreet Address,Num-Premises,Latitude,Longitude\n5412 N CLARK,10,41.980262,-87.668452\n6.1. Reading and Writing CSV Data \n| \n177",
      "content_length": 2132,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": 4,
      "content": "This will actually cause the creation of a namedtuple to fail with a ValueError exception.\nTo work around this, you might have to scrub the headers first. For instance, carrying\na regex substitution on nonvalid identifier characters like this:\nimport re\nwith open('stock.csv') as f:\n    f_csv = csv.reader(f)\n    headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ]\n    Row = namedtuple('Row', headers)\n    for r in f_csv:\n        row = Row(*r)\n        # Process row\n        ...\nIt’s also important to emphasize that csv does not try to interpret the data or convert it\nto a type other than a string. If such conversions are important, that is something you’ll\nneed to do yourself. Here is one example of performing extra type conversions on CSV\ndata:\ncol_types = [str, float, str, str, float, int]\nwith open('stocks.csv') as f:\n    f_csv = csv.reader(f)\n    headers = next(f_csv)\n    for row in f_csv:\n        # Apply conversions to the row items\n        row = tuple(convert(value) for convert, value in zip(col_types, row))\n        ...\nAlternatively, here is an example of converting selected fields of dictionaries:\nprint('Reading as dicts with type conversion')\nfield_types = [ ('Price', float),\n                ('Change', float),\n                ('Volume', int) ]\nwith open('stocks.csv') as f:\n    for row in csv.DictReader(f):\n        row.update((key, conversion(row[key]))\n                   for key, conversion in field_types)\n        print(row)\nIn general, you’ll probably want to be a bit careful with such conversions, though. In\nthe real world, it’s common for CSV files to have missing values, corrupted data, and\nother issues that would break type conversions. So, unless your data is guaranteed to be\nerror free, that’s something you’ll need to consider (you might need to add suitable\nexception handling).\nFinally, if your goal in reading CSV data is to perform data analysis and statistics, you\nmight want to look at the Pandas package. Pandas includes a convenient pan\ndas.read_csv() function that will load CSV data into a DataFrame object. From there,\n178 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 2130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": 4,
      "content": "you can generate various summary statistics, filter the data, and perform other kinds of\nhigh-level operations. An example is given in Recipe 6.13.\n6.2. Reading and Writing JSON Data\nProblem\nYou want to read or write data encoded as JSON (JavaScript Object Notation).\nSolution\nThe json module provides an easy way to encode and decode data in JSON. The two\nmain functions are json.dumps() and json.loads(), mirroring the interface used in\nother serialization libraries, such as pickle. Here is how you turn a Python data struc‐\nture into JSON:\nimport json\ndata = {\n   'name' : 'ACME',\n   'shares' : 100,\n   'price' : 542.23\n}\njson_str = json.dumps(data)\nHere is how you turn a JSON-encoded string back into a Python data structure:\ndata = json.loads(json_str)\nIf you are working with files instead of strings, you can alternatively use json.dump()\nand json.load() to encode and decode JSON data. For example:\n# Writing JSON data\nwith open('data.json', 'w') as f:\n     json.dump(data, f)\n# Reading data back\nwith open('data.json', 'r') as f:\n     data = json.load(f)\nDiscussion\nJSON encoding supports the basic types of None, bool, int, float, and str, as well as\nlists, tuples, and dictionaries containing those types. For dictionaries, keys are assumed\nto be strings (any nonstring keys in a dictionary are converted to strings when encoding).\nTo be compliant with the JSON specification, you should only encode Python lists and\n6.2. Reading and Writing JSON Data \n| \n179",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": 4,
      "content": "dictionaries. Moreover, in web applications, it is standard practice for the top-level ob‐\nject to be a dictionary.\nThe format of JSON encoding is almost identical to Python syntax except for a few\nminor changes. For instance, True is mapped to true, False is mapped to false, and\nNone is mapped to null. Here is an example that shows what the encoding looks like:\n>>> json.dumps(False)\n'false'\n>>> d = {'a': True,\n...      'b': 'Hello',\n...      'c': None}\n>>> json.dumps(d)\n'{\"b\": \"Hello\", \"c\": null, \"a\": true}'\n>>>\nIf you are trying to examine data you have decoded from JSON, it can often be hard to\nascertain its structure simply by printing it out—especially if the data contains a deep\nlevel of nested structures or a lot of fields. To assist with this, consider using the pprint()\nfunction in the pprint module. This will alphabetize the keys and output a dictionary\nin a more sane way. Here is an example that illustrates how you would pretty print the\nresults of a search on Twitter:\n>>> from urllib.request import urlopen\n>>> import json\n>>> u = urlopen('http://search.twitter.com/search.json?q=python&rpp=5')\n>>> resp = json.loads(u.read().decode('utf-8'))\n>>> from pprint import pprint\n>>> pprint(resp)\n{'completed_in': 0.074,\n 'max_id': 264043230692245504,\n 'max_id_str': '264043230692245504',\n 'next_page': '?page=2&max_id=264043230692245504&q=python&rpp=5',\n 'page': 1,\n 'query': 'python',\n 'refresh_url': '?since_id=264043230692245504&q=python',\n 'results': [{'created_at': 'Thu, 01 Nov 2012 16:36:26 +0000',\n              'from_user': ...\n             },\n             {'created_at': 'Thu, 01 Nov 2012 16:36:14 +0000',\n              'from_user': ...\n             },\n             {'created_at': 'Thu, 01 Nov 2012 16:36:13 +0000',\n              'from_user': ...\n             },\n             {'created_at': 'Thu, 01 Nov 2012 16:36:07 +0000',\n              'from_user': ...\n             }\n             {'created_at': 'Thu, 01 Nov 2012 16:36:04 +0000',\n              'from_user': ...\n             }],\n180 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": 4,
      "content": "'results_per_page': 5,\n 'since_id': 0,\n 'since_id_str': '0'}\n>>>\nNormally, JSON decoding will create dicts or lists from the supplied data. If you want\nto create different kinds of objects, supply the object_pairs_hook or object_hook to\njson.loads(). For example, here is how you would decode JSON data, preserving its\norder in an OrderedDict:\n>>> s = '{\"name\": \"ACME\", \"shares\": 50, \"price\": 490.1}'\n>>> from collections import OrderedDict\n>>> data = json.loads(s, object_pairs_hook=OrderedDict)\n>>> data\nOrderedDict([('name', 'ACME'), ('shares', 50), ('price', 490.1)])\n>>>\nHere is how you could turn a JSON dictionary into a Python object:\n>>> class JSONObject:\n...     def __init__(self, d):\n...             self.__dict__ = d\n...\n>>>\n>>> data = json.loads(s, object_hook=JSONObject)\n>>> data.name\n'ACME'\n>>> data.shares\n50\n>>> data.price\n490.1\n>>>\nIn this last example, the dictionary created by decoding the JSON data is passed as a\nsingle argument to __init__(). From there, you are free to use it as you will, such as\nusing it directly as the instance dictionary of the object.\nThere are a few options that can be useful for encoding JSON. If you would like the\noutput to be nicely formatted, you can use the indent argument to json.dumps(). This\ncauses the output to be pretty printed in a format similar to that with the pprint()\nfunction. For example:\n>>> print(json.dumps(data))\n{\"price\": 542.23, \"name\": \"ACME\", \"shares\": 100}\n>>> print(json.dumps(data, indent=4))\n{\n    \"price\": 542.23,\n    \"name\": \"ACME\",\n    \"shares\": 100\n}\n>>>\n6.2. Reading and Writing JSON Data \n| \n181",
      "content_length": 1586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": 4,
      "content": "If you want the keys to be sorted on output, used the sort_keys argument:\n>>> print(json.dumps(data, sort_keys=True))\n{\"name\": \"ACME\", \"price\": 542.23, \"shares\": 100}\n>>>\nInstances are not normally serializable as JSON. For example:\n>>> class Point:\n...     def __init__(self, x, y):\n...             self.x = x\n...             self.y = y\n...\n>>> p = Point(2, 3)\n>>> json.dumps(p)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.3/json/__init__.py\", line 226, in dumps\n    return _default_encoder.encode(obj)\n  File \"/usr/local/lib/python3.3/json/encoder.py\", line 187, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File \"/usr/local/lib/python3.3/json/encoder.py\", line 245, in iterencode\n    return _iterencode(o, 0)\n  File \"/usr/local/lib/python3.3/json/encoder.py\", line 169, in default\n    raise TypeError(repr(o) + \" is not JSON serializable\")\nTypeError: <__main__.Point object at 0x1006f2650> is not JSON serializable\n>>>\nIf you want to serialize instances, you can supply a function that takes an instance as\ninput and returns a dictionary that can be serialized. For example:\ndef serialize_instance(obj):\n    d = { '__classname__' : type(obj).__name__ }\n    d.update(vars(obj))\n    return d\nIf you want to get an instance back, you could write code like this:\n# Dictionary mapping names to known classes\nclasses = {\n    'Point' : Point\n}\ndef unserialize_object(d):\n    clsname = d.pop('__classname__', None)\n    if clsname:\n        cls = classes[clsname]\n        obj = cls.__new__(cls)   # Make instance without calling __init__\n        for key, value in d.items():\n            setattr(obj, key, value)\n            return obj\n    else:\n        return d\n182 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": 5,
      "content": "Here is an example of how these functions are used:\n>>> p = Point(2,3)\n>>> s = json.dumps(p, default=serialize_instance)\n>>> s\n'{\"__classname__\": \"Point\", \"y\": 3, \"x\": 2}'\n>>> a = json.loads(s, object_hook=unserialize_object)\n>>> a\n<__main__.Point object at 0x1017577d0>\n>>> a.x\n2\n>>> a.y\n3\n>>>\nThe json module has a variety of other options for controlling the low-level interpre‐\ntation of numbers, special values such as NaN, and more. Consult the documentation\nfor further details.\n6.3. Parsing Simple XML Data\nProblem\nYou would like to extract data from a simple XML document.\nSolution\nThe xml.etree.ElementTree module can be used to extract data from simple XML\ndocuments. To illustrate, suppose you want to parse and make a summary of the RSS\nfeed on Planet Python. Here is a script that will do it:\nfrom urllib.request import urlopen\nfrom xml.etree.ElementTree import parse\n# Download the RSS feed and parse it\nu = urlopen('http://planet.python.org/rss20.xml')\ndoc = parse(u)\n# Extract and output tags of interest\nfor item in doc.iterfind('channel/item'):\n    title = item.findtext('title')\n    date = item.findtext('pubDate')\n    link = item.findtext('link')\n    print(title)\n    print(date)\n    print(link)\n    print()\n6.3. Parsing Simple XML Data \n| \n183",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": 5,
      "content": "If you run the preceding script, the output looks similar to the following:\n    Steve Holden: Python for Data Analysis\n    Mon, 19 Nov 2012 02:13:51 +0000\n    http://holdenweb.blogspot.com/2012/11/python-for-data-analysis.html\n    Vasudev Ram: The Python Data model (for v2 and v3)\n    Sun, 18 Nov 2012 22:06:47 +0000\n    http://jugad2.blogspot.com/2012/11/the-python-data-model.html\n    Python Diary: Been playing around with Object Databases\n    Sun, 18 Nov 2012 20:40:29 +0000\n    http://www.pythondiary.com/blog/Nov.18,2012/been-...-object-databases.html\n    Vasudev Ram: Wakari, Scientific Python in the cloud\n    Sun, 18 Nov 2012 20:19:41 +0000\n    http://jugad2.blogspot.com/2012/11/wakari-scientific-python-in-cloud.html\n    Jesse Jiryu Davis: Toro: synchronization primitives for Tornado coroutines\n    Sun, 18 Nov 2012 20:17:49 +0000\n    http://feedproxy.google.com/~r/EmptysquarePython/~3/_DOZT2Kd0hQ/\nObviously, if you want to do more processing, you need to replace the print() state‐\nments with something more interesting.\nDiscussion\nWorking with data encoded as XML is commonplace in many applications. Not only is\nXML widely used as a format for exchanging data on the Internet, it is a common format\nfor storing application data (e.g., word processing, music libraries, etc.). The discussion\nthat follows already assumes the reader is familiar with XML basics.\nIn many cases, when XML is simply being used to store data, the document structure\nis compact and straightforward. For example, the RSS feed from the example looks\nsimilar to the following:\n    <?xml version=\"1.0\"?>\n    <rss version=\"2.0\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\">\n    <channel>\n      <title>Planet Python</title>\n      <link>http://planet.python.org/</link>\n      <language>en</language>\n      <description>Planet Python - http://planet.python.org/</description>\n      <item>\n        <title>Steve Holden: Python for Data Analysis</title>\n          <guid>http://holdenweb.blogspot.com/...-data-analysis.html</guid>\n          <link>http://holdenweb.blogspot.com/...-data-analysis.html</link>\n          <description>...</description>\n          <pubDate>Mon, 19 Nov 2012 02:13:51 +0000</pubDate>\n      </item>\n      <item>\n184 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 2264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": 5,
      "content": "<title>Vasudev Ram: The Python Data model (for v2 and v3)</title>\n        <guid>http://jugad2.blogspot.com/...-data-model.html</guid>\n        <link>http://jugad2.blogspot.com/...-data-model.html</link>\n        <description>...</description>\n        <pubDate>Sun, 18 Nov 2012 22:06:47 +0000</pubDate>\n        </item>\n      <item>\n        <title>Python Diary: Been playing around with Object Databases</title>\n        <guid>http://www.pythondiary.com/...-object-databases.html</guid>\n        <link>http://www.pythondiary.com/...-object-databases.html</link>\n        <description>...</description>\n        <pubDate>Sun, 18 Nov 2012 20:40:29 +0000</pubDate>\n      </item>\n        ...\n    </channel>\n    </rss>\nThe xml.etree.ElementTree.parse() function parses the entire XML document into\na document object. From there, you use methods such as find(), iterfind(), and\nfindtext() to search for specific XML elements. The arguments to these functions are\nthe names of a specific tag, such as channel/item or title.\nWhen specifying tags, you need to take the overall document structure into account.\nEach find operation takes place relative to a starting element. Likewise, the tagname that\nyou supply to each operation is also relative to the start. In the example, the call to\ndoc.iterfind('channel/item') looks for all “item” elements under a “channel” ele‐\nment. doc represents the top of the document (the top-level “rss” element). The later\ncalls to item.findtext() take place relative to the found “item” elements.\nEach element represented by the ElementTree module has a few essential attributes and\nmethods that are useful when parsing. The tag attribute contains the name of the tag,\nthe text attribute contains enclosed text, and the get() method can be used to extract\nattributes (if any). For example:\n>>> doc\n<xml.etree.ElementTree.ElementTree object at 0x101339510>\n>>> e = doc.find('channel/title')\n>>> e\n<Element 'title' at 0x10135b310>\n>>> e.tag\n'title'\n>>> e.text\n'Planet Python'\n>>> e.get('some_attribute')\n>>>\nIt should be noted that xml.etree.ElementTree is not the only option for XML parsing.\nFor more advanced applications, you might consider lxml. It uses the same program‐\nming interface as ElementTree, so the example shown in this recipe works in the same\n6.3. Parsing Simple XML Data \n| \n185",
      "content_length": 2314,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": 5,
      "content": "manner. You simply need to change the first import to from lxml.etree import\nparse. lxml provides the benefit of being fully compliant with XML standards. It is also\nextremely fast, and provides support for features such as validation, XSLT, and XPath.\n6.4. Parsing Huge XML Files Incrementally\nProblem\nYou need to extract data from a huge XML document using as little memory as possible.\nSolution\nAny time you are faced with the problem of incremental data processing, you should\nthink of iterators and generators. Here is a simple function that can be used to incre‐\nmentally process huge XML files using a very small memory footprint:\nfrom xml.etree.ElementTree import iterparse\ndef parse_and_remove(filename, path):\n    path_parts = path.split('/')\n    doc = iterparse(filename, ('start', 'end'))\n    # Skip the root element\n    next(doc)\n    tag_stack = []\n    elem_stack = []\n    for event, elem in doc:\n        if event == 'start':\n            tag_stack.append(elem.tag)\n            elem_stack.append(elem)\n        elif event == 'end':\n            if tag_stack == path_parts:\n                yield elem\n                elem_stack[-2].remove(elem)\n            try:\n                tag_stack.pop()\n                elem_stack.pop()\n            except IndexError:\n                pass\nTo test the function, you now need to find a large XML file to work with. You can often\nfind such files on government and open data websites. For example, you can download\nChicago’s pothole database as XML. At the time of this writing, the downloaded file\nconsists of more than 100,000 rows of data, which are encoded like this:\n    <response>\n      <row>\n        <row ...>\n          <creation_date>2012-11-18T00:00:00</creation_date>\n186 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": 5,
      "content": "<status>Completed</status>\n          <completion_date>2012-11-18T00:00:00</completion_date>\n          <service_request_number>12-01906549</service_request_number>\n          <type_of_service_request>Pot Hole in Street</type_of_service_request>\n          <current_activity>Final Outcome</current_activity>\n          <most_recent_action>CDOT Street Cut ... Outcome</most_recent_action>\n          <street_address>4714 S TALMAN AVE</street_address>\n          <zip>60632</zip>\n          <x_coordinate>1159494.68618856</x_coordinate>\n          <y_coordinate>1873313.83503384</y_coordinate>\n          <ward>14</ward>\n          <police_district>9</police_district>\n          <community_area>58</community_area>\n          <latitude>41.808090232127896</latitude>\n          <longitude>-87.69053684711305</longitude>\n          <location latitude=\"41.808090232127896\"\n                           longitude=\"-87.69053684711305\" />\n        </row>\n        <row ...>\n          <creation_date>2012-11-18T00:00:00</creation_date>\n          <status>Completed</status>\n          <completion_date>2012-11-18T00:00:00</completion_date>\n          <service_request_number>12-01906695</service_request_number>\n          <type_of_service_request>Pot Hole in Street</type_of_service_request>\n          <current_activity>Final Outcome</current_activity>\n          <most_recent_action>CDOT Street Cut ... Outcome</most_recent_action>\n          <street_address>3510 W NORTH AVE</street_address>\n          <zip>60647</zip>\n          <x_coordinate>1152732.14127696</x_coordinate>\n          <y_coordinate>1910409.38979075</y_coordinate>\n          <ward>26</ward>\n          <police_district>14</police_district>\n          <community_area>23</community_area>\n          <latitude>41.91002084292946</latitude>\n          <longitude>-87.71435952353961</longitude>\n          <location latitude=\"41.91002084292946\"\n                           longitude=\"-87.71435952353961\" />\n        </row>\n      </row>\n    </response>\nSuppose you want to write a script that ranks ZIP codes by the number of pothole\nreports. To do it, you could write code like this:\nfrom xml.etree.ElementTree import parse\nfrom collections import Counter\npotholes_by_zip = Counter()\ndoc = parse('potholes.xml')\nfor pothole in doc.iterfind('row/row'):\n    potholes_by_zip[pothole.findtext('zip')] += 1\n6.4. Parsing Huge XML Files Incrementally \n| \n187",
      "content_length": 2375,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": 5,
      "content": "for zipcode, num in potholes_by_zip.most_common():\n    print(zipcode, num)\nThe only problem with this script is that it reads and parses the entire XML file into\nmemory. On our machine, it takes about 450 MB of memory to run. Using this recipe’s\ncode, the program changes only slightly:\nfrom collections import Counter\npotholes_by_zip = Counter()\ndata = parse_and_remove('potholes.xml', 'row/row')\nfor pothole in data:\n    potholes_by_zip[pothole.findtext('zip')] += 1\nfor zipcode, num in potholes_by_zip.most_common():\n    print(zipcode, num)\nThis version of code runs with a memory footprint of only 7 MB—a huge savings!\nDiscussion\nThis recipe relies on two core features of the ElementTree module. First, the iter\nparse() method allows incremental processing of XML documents. To use it, you sup‐\nply the filename along with an event list consisting of one or more of the following:\nstart, end, start-ns, and end-ns. The iterator created by iterparse() produces tuples\nof the form (event, elem), where event is one of the listed events and elem is the\nresulting XML element. For example:\n>>> data = iterparse('potholes.xml',('start','end'))\n>>> next(data)\n('start', <Element 'response' at 0x100771d60>)\n>>> next(data)\n('start', <Element 'row' at 0x100771e68>)\n>>> next(data)\n('start', <Element 'row' at 0x100771fc8>)\n>>> next(data)\n('start', <Element 'creation_date' at 0x100771f18>)\n>>> next(data)\n('end', <Element 'creation_date' at 0x100771f18>)\n>>> next(data)\n('start', <Element 'status' at 0x1006a7f18>)\n>>> next(data)\n('end', <Element 'status' at 0x1006a7f18>)\n>>>\nstart events are created when an element is first created but not yet populated with any\nother data (e.g., child elements). end events are created when an element is completed.\nAlthough not shown in this recipe, start-ns and end-ns events are used to handle XML\nnamespace declarations.\n188 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": 5,
      "content": "In this recipe, the start and end events are used to manage stacks of elements and tags.\nThe stacks represent the current hierarchical structure of the document as it’s being\nparsed, and are also used to determine if an element matches the requested path given\nto the parse_and_remove() function. If a match is made, yield is used to emit it back\nto the caller.\nThe following statement after the yield is the core feature of ElementTree that makes\nthis recipe save memory:\nelem_stack[-2].remove(elem)\nThis statement causes the previously yielded element to be removed from its parent.\nAssuming that no references are left to it anywhere else, the element is destroyed and\nmemory reclaimed.\nThe end effect of the iterative parse and the removal of nodes is a highly efficient in‐\ncremental sweep over the document. At no point is a complete document tree ever\nconstructed. Yet, it is still possible to write code that processes the XML data in a\nstraightforward manner.\nThe primary downside to this recipe is its runtime performance. When tested, the ver‐\nsion of code that reads the entire document into memory first runs approximately twice\nas fast as the version that processes it incrementally. However, it requires more than 60\ntimes as much memory. So, if memory use is a greater concern, the incremental version\nis a big win.\n6.5. Turning a Dictionary into XML\nProblem\nYou want to take the data in a Python dictionary and turn it into XML.\nSolution\nAlthough the xml.etree.ElementTree library is commonly used for parsing, it can also\nbe used to create XML documents. For example, consider this function:\nfrom xml.etree.ElementTree import Element\ndef dict_to_xml(tag, d):\n    '''\n    Turn a simple dict of key/value pairs into XML\n    '''\n    elem = Element(tag)\n    for key, val in d.items():\n        child = Element(key)\n        child.text = str(val)\n6.5. Turning a Dictionary into XML \n| \n189",
      "content_length": 1900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": 5,
      "content": "elem.append(child)\n    return elem\nHere is an example:\n>>> s = { 'name': 'GOOG', 'shares': 100, 'price':490.1 }\n>>> e = dict_to_xml('stock', s)\n>>> e\n<Element 'stock' at 0x1004b64c8>\n>>>\nThe result of this conversion is an Element instance. For I/O, it is easy to convert this\nto a byte string using the tostring() function in xml.etree.ElementTree. For\nexample:\n>>> from xml.etree.ElementTree import tostring\n>>> tostring(e)\nb'<stock><price>490.1</price><shares>100</shares><name>GOOG</name></stock>'\n>>>\nIf you want to attach attributes to an element, use its set() method:\n>>> e.set('_id','1234')\n>>> tostring(e)\nb'<stock _id=\"1234\"><price>490.1</price><shares>100</shares><name>GOOG</name>\n</stock>'\n>>>\nIf the order of the elements matters, consider making an OrderedDict instead of a\nnormal dictionary. See Recipe 1.7.\nDiscussion\nWhen creating XML, you might be inclined to just make strings instead. For example:\ndef dict_to_xml_str(tag, d):\n    '''\n    Turn a simple dict of key/value pairs into XML\n    '''\n    parts = ['<{}>'.format(tag)]\n    for key, val in d.items():\n        parts.append('<{0}>{1}</{0}>'.format(key,val))\n    parts.append('</{}>'.format(tag))\n    return ''.join(parts)\nThe problem is that you’re going to make a real mess for yourself if you try to do things\nmanually. For example, what happens if the dictionary values contain special characters\nlike this?\n>>> d = { 'name' : '<spam>' }\n>>> # String creation\n>>> dict_to_xml_str('item',d)\n190 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": 5,
      "content": "'<item><name><spam></name></item>'\n>>> # Proper XML creation\n>>> e = dict_to_xml('item',d)\n>>> tostring(e)\nb'<item><name>&lt;spam&gt;</name></item>'\n>>>\nNotice how in the latter example, the characters < and > got replaced with &lt; and &gt;.\nJust for reference, if you ever need to manually escape or unescape such characters, you\ncan use the escape() and unescape() functions in xml.sax.saxutils. For example:\n>>> from xml.sax.saxutils import escape, unescape\n>>> escape('<spam>')\n'&lt;spam&gt;'\n>>> unescape(_)\n'<spam>'\n>>>\nAside from creating correct output, the other reason why it’s a good idea to create\nElement instances instead of strings is that they can be more easily combined together\nto make a larger document. The resulting Element instances can also be processed in\nvarious ways without ever having to worry about parsing the XML text. Essentially, you\ncan do all of the processing of the data in a more high-level form and then output it as\na string at the very end.\n6.6. Parsing, Modifying, and Rewriting XML\nProblem\nYou want to read an XML document, make changes to it, and then write it back out as\nXML.\nSolution\nThe xml.etree.ElementTree module makes it easy to perform such tasks. Essentially,\nyou start out by parsing the document in the usual way. For example, suppose you have\na document named pred.xml that looks like this:\n<?xml version=\"1.0\"?>\n<stop>\n    <id>14791</id>\n    <nm>Clark &amp; Balmoral</nm>\n    <sri>\n        <rt>22</rt>\n        <d>North Bound</d>\n        <dd>North Bound</dd>\n    </sri>\n6.6. Parsing, Modifying, and Rewriting XML \n| \n191",
      "content_length": 1579,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": 5,
      "content": "<cr>22</cr>\n    <pre>\n       <pt>5 MIN</pt>\n       <fd>Howard</fd>\n       <v>1378</v>\n       <rn>22</rn>\n   </pre>\n   <pre>\n       <pt>15 MIN</pt>\n       <fd>Howard</fd>\n       <v>1867</v>\n       <rn>22</rn>\n   </pre>\n</stop>\nHere is an example of using ElementTree to read it and make changes to the structure:\n>>> from xml.etree.ElementTree import parse, Element\n>>> doc = parse('pred.xml')\n>>> root = doc.getroot()\n>>> root\n<Element 'stop' at 0x100770cb0>\n>>> # Remove a few elements\n>>> root.remove(root.find('sri'))\n>>> root.remove(root.find('cr'))\n>>> # Insert a new element after <nm>...</nm>\n>>> root.getchildren().index(root.find('nm'))\n1\n>>> e = Element('spam')\n>>> e.text = 'This is a test'\n>>> root.insert(2, e)\n>>> # Write back to a file\n>>> doc.write('newpred.xml', xml_declaration=True)\n>>>\nThe result of these operations is a new XML file that looks like this:\n<?xml version='1.0' encoding='us-ascii'?>\n<stop>\n    <id>14791</id>\n    <nm>Clark &amp; Balmoral</nm>\n    <spam>This is a test</spam><pre>\n       <pt>5 MIN</pt>\n       <fd>Howard</fd>\n       <v>1378</v>\n       <rn>22</rn>\n   </pre>\n   <pre>\n       <pt>15 MIN</pt>\n       <fd>Howard</fd>\n192 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1211,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": 5,
      "content": "<v>1867</v>\n       <rn>22</rn>\n   </pre>\n</stop>\nDiscussion\nModifying the structure of an XML document is straightforward, but you must re‐\nmember that all modifications are generally made to the parent element, treating it as\nif it were a list. For example, if you remove an element, it is removed from its immediate\nparent using the parent’s remove() method. If you insert or append new elements, you\nalso use insert() and append() methods on the parent. Elements can also be manip‐\nulated using indexing and slicing operations, such as element[i] or element[i:j].\nIf you need to make new elements, use the Element class, as shown in this recipe’s\nsolution. This is described further in Recipe 6.5.\n6.7. Parsing XML Documents with Namespaces\nProblem\nYou need to parse an XML document, but it’s using XML namespaces.\nSolution\nConsider a document that uses namespaces like this:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<top>\n  <author>David Beazley</author>\n  <content>\n      <html xmlns=\"http://www.w3.org/1999/xhtml\">\n          <head>\n              <title>Hello World</title>\n          </head>\n          <body>\n              <h1>Hello World!</h1>\n          </body>\n      </html>\n  </content>\n</top>\nIf you parse this document and try to perform the usual queries, you’ll find that it doesn’t\nwork so easily because everything becomes incredibly verbose:\n>>> # Some queries that work\n>>> doc.findtext('author')\n'David Beazley'\n>>> doc.find('content')\n6.7. Parsing XML Documents with Namespaces \n| \n193",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": 5,
      "content": "<Element 'content' at 0x100776ec0>\n>>> # A query involving a namespace (doesn't work)\n>>> doc.find('content/html')\n>>> # Works if fully qualified\n>>> doc.find('content/{http://www.w3.org/1999/xhtml}html')\n<Element '{http://www.w3.org/1999/xhtml}html' at 0x1007767e0>\n>>> # Doesn't work\n>>> doc.findtext('content/{http://www.w3.org/1999/xhtml}html/head/title')\n>>> # Fully qualified\n>>> doc.findtext('content/{http://www.w3.org/1999/xhtml}html/'\n...  '{http://www.w3.org/1999/xhtml}head/{http://www.w3.org/1999/xhtml}title')\n'Hello World'\n>>>\nYou can often simplify matters for yourself by wrapping namespace handling up into a\nutility class.\nclass XMLNamespaces:\n    def __init__(self, **kwargs):\n        self.namespaces = {}\n        for name, uri in kwargs.items():\n            self.register(name, uri)\n    def register(self, name, uri):\n        self.namespaces[name] = '{'+uri+'}'\n    def __call__(self, path):\n        return path.format_map(self.namespaces)\nTo use this class, you do the following:\n>>> ns = XMLNamespaces(html='http://www.w3.org/1999/xhtml')\n>>> doc.find(ns('content/{html}html'))\n<Element '{http://www.w3.org/1999/xhtml}html' at 0x1007767e0>\n>>> doc.findtext(ns('content/{html}html/{html}head/{html}title'))\n'Hello World'\n>>>\nDiscussion\nParsing XML documents that contain namespaces can be messy. The XMLNamespaces\nclass is really just meant to clean it up slightly by allowing you to use the shortened\nnamespace names in subsequent operations as opposed to fully qualified URIs.\nUnfortunately, there is no mechanism in the basic ElementTree parser to get further\ninformation about namespaces. However, you can get a bit more information about the\nscope of namespace processing if you’re willing to use the iterparse() function instead.\nFor example:\n194 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": 5,
      "content": ">>> from xml.etree.ElementTree import iterparse\n>>> for evt, elem in iterparse('ns2.xml', ('end', 'start-ns', 'end-ns')):\n...     print(evt, elem)\n...\nend <Element 'author' at 0x10110de10>\nstart-ns ('', 'http://www.w3.org/1999/xhtml')\nend <Element '{http://www.w3.org/1999/xhtml}title' at 0x1011131b0>\nend <Element '{http://www.w3.org/1999/xhtml}head' at 0x1011130a8>\nend <Element '{http://www.w3.org/1999/xhtml}h1' at 0x101113310>\nend <Element '{http://www.w3.org/1999/xhtml}body' at 0x101113260>\nend <Element '{http://www.w3.org/1999/xhtml}html' at 0x10110df70>\nend-ns None\nend <Element 'content' at 0x10110de68>\nend <Element 'top' at 0x10110dd60>\n>>> elem      # This is the topmost element\n<Element 'top' at 0x10110dd60>\n>>>\nAs a final note, if the text you are parsing makes use of namespaces in addition to other\nadvanced XML features, you’re really better off using the lxml library instead of Ele\nmentTree. For instance, lxml provides better support for validating documents against\na DTD, more complete XPath support, and other advanced XML features. This recipe\nis really just a simple fix to make parsing a little easier.\n6.8. Interacting with a Relational Database\nProblem\nYou need to select, insert, or delete rows in a relational database.\nSolution\nA standard way to represent rows of data in Python is as a sequence of tuples. For\nexample:\nstocks = [\n    ('GOOG', 100, 490.1),\n    ('AAPL', 50, 545.75),\n    ('FB', 150, 7.45),\n    ('HPQ', 75, 33.2),\n]\nGiven data in this form, it is relatively straightforward to interact with a relational\ndatabase using Python’s standard database API, as described in PEP 249. The gist of the\nAPI is that all operations on the database are carried out by SQL queries. Each row of\ninput or output data is represented by a tuple.\nTo illustrate, you can use the sqlite3 module that comes with Python. If you are using\na different database (e.g., MySql, Postgres, or ODBC), you’ll have to install a third-party\n6.8. Interacting with a Relational Database \n| \n195",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": 5,
      "content": "module to support it. However, the underlying programming interface will be virtually\nthe same, if not identical.\nThe first step is to connect to the database. Typically, you execute a connect() function,\nsupplying parameters such as the name of the database, hostname, username, password,\nand other details as needed. For example:\n>>> import sqlite3\n>>> db = sqlite3.connect('database.db')\n>>>\nTo do anything with the data, you next create a cursor. Once you have a cursor, you can\nstart executing SQL queries. For example:\n>>> c = db.cursor()\n>>> c.execute('create table portfolio (symbol text, shares integer, price real)')\n<sqlite3.Cursor object at 0x10067a730>\n>>> db.commit()\n>>>\nTo insert a sequence of rows into the data, use a statement like this:\n>>> c.executemany('insert into portfolio values (?,?,?)', stocks)\n<sqlite3.Cursor object at 0x10067a730>\n>>> db.commit()\n>>>\nTo perform a query, use a statement such as this:\n>>> for row in db.execute('select * from portfolio'):\n...     print(row)\n...\n('GOOG', 100, 490.1)\n('AAPL', 50, 545.75)\n('FB', 150, 7.45)\n('HPQ', 75, 33.2)\n>>>\nIf you want to perform queries that accept user-supplied input parameters, make sure\nyou escape the parameters using ? like this:\n>>> min_price = 100\n>>> for row in db.execute('select * from portfolio where price >= ?',\n                         (min_price,)):\n...     print(row)\n...\n('GOOG', 100, 490.1)\n('AAPL', 50, 545.75)\n>>>\n196 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": 5,
      "content": "Discussion\nAt a low level, interacting with a database is an extremely straightforward thing to do.\nYou simply form SQL statements and feed them to the underlying module to either\nupdate the database or retrieve data. That said, there are still some tricky details you’ll\nneed to sort out on a case-by-case basis.\nOne complication is the mapping of data from the database into Python types. For\nentries such as dates, it is most common to use datetime instances from the date\ntime module, or possibly system timestamps, as used in the time module. For numerical\ndata, especially financial data involving decimals, numbers may be represented as Dec\nimal instances from the decimal module. Unfortunately, the exact mapping varies by\ndatabase backend so you’ll have to read the associated documentation.\nAnother extremely critical complication concerns the formation of SQL statement\nstrings. You should never use Python string formatting operators (e.g., %) or the .for\nmat() method to create such strings. If the values provided to such formatting operators\nare derived from user input, this opens up your program to an SQL-injection attack (see\nhttp://xkcd.com/327). The special ? wildcard in queries instructs the database backend\nto use its own string substitution mechanism, which (hopefully) will do it safely.\nSadly, there is some inconsistency across database backends with respect to the wildcard.\nMany modules use ? or %s, while others may use a different symbol, such as :0 or :1,\nto refer to parameters. Again, you’ll have to consult the documentation for the database\nmodule you’re using. The paramstyle attribute of a database module also contains in‐\nformation about the quoting style.\nFor simply pulling data in and out of a database table, using the database API is usually\nsimple enough. If you’re doing something more complicated, it may make sense to use\na higher-level interface, such as that provided by an object-relational mapper. Libraries\nsuch as SQLAlchemy allow database tables to be described as Python classes and for\ndatabase operations to be carried out while hiding most of the underlying SQL.\n6.9. Decoding and Encoding Hexadecimal Digits\nProblem\nYou need to decode a string of hexadecimal digits into a byte string or encode a byte\nstring as hex.\nSolution\nIf you simply need to decode or encode a raw string of hex digits, use the binascii\nmodule. For example:\n6.9. Decoding and Encoding Hexadecimal Digits \n| \n197",
      "content_length": 2447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": 5,
      "content": ">>> # Initial byte string\n>>> s = b'hello'\n>>> # Encode as hex\n>>> import binascii\n>>> h = binascii.b2a_hex(s)\n>>> h\nb'68656c6c6f'\n>>> # Decode back to bytes\n>>> binascii.a2b_hex(h)\nb'hello'\n>>>\nSimilar functionality can also be found in the base64 module. For example:\n>>> import base64\n>>> h = base64.b16encode(s)\n>>> h\nb'68656C6C6F'\n>>> base64.b16decode(h)\nb'hello'\n>>>\nDiscussion\nFor the most part, converting to and from hex is straightforward using the functions\nshown. The main difference between the two techniques is in case folding. The \nbase64.b16decode() and base64.b16encode() functions only operate with uppercase\nhexadecimal letters, whereas the functions in binascii work with either case.\nIt’s also important to note that the output produced by the encoding functions is always\na byte string. To coerce it to Unicode for output, you may need to add an extra decoding\nstep. For example:\n>>> h = base64.b16encode(s)\n>>> print(h)\nb'68656C6C6F'\n>>> print(h.decode('ascii'))\n68656C6C6F\n>>>\nWhen decoding hex digits, the b16decode() and a2b_hex() functions accept either\nbytes or unicode strings. However, those strings must only contain ASCII-encoded\nhexadecimal digits.\n198 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": 5,
      "content": "6.10. Decoding and Encoding Base64\nProblem\nYou need to decode or encode binary data using Base64 encoding.\nSolution\nThe base64 module has two functions—b64encode() and b64decode()—that do ex‐\nactly what you want. For example:\n>>> # Some byte data\n>>> s = b'hello'\n>>> import base64\n>>> # Encode as Base64\n>>> a = base64.b64encode(s)\n>>> a\nb'aGVsbG8='\n>>> # Decode from Base64\n>>> base64.b64decode(a)\nb'hello'\n>>>\nDiscussion\nBase64 encoding is only meant to be used on byte-oriented data such as byte strings and\nbyte arrays. Moreover, the output of the encoding process is always a byte string. If you\nare mixing Base64-encoded data with Unicode text, you may have to perform an extra\ndecoding step. For example:\n>>> a = base64.b64encode(s).decode('ascii')\n>>> a\n'aGVsbG8='\n>>>\nWhen decoding Base64, both byte strings and Unicode text strings can be supplied.\nHowever, Unicode strings can only contain ASCII characters.\n6.11. Reading and Writing Binary Arrays of Structures\nProblem\nYou want to read or write data encoded as a binary array of uniform structures into\nPython tuples.\n6.10. Decoding and Encoding Base64 \n| \n199",
      "content_length": 1123,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": 5,
      "content": "Solution\nTo work with binary data, use the struct module. Here is an example of code that writes\na list of Python tuples out to a binary file, encoding each tuple as a structure using\nstruct:\nfrom struct import Struct\ndef write_records(records, format, f):\n    '''\n    Write a sequence of tuples to a binary file of structures.\n    '''\n    record_struct = Struct(format)\n    for r in records:\n        f.write(record_struct.pack(*r))\n# Example\nif __name__ == '__main__':\n    records = [ (1, 2.3, 4.5),\n                (6, 7.8, 9.0),\n                (12, 13.4, 56.7) ]\n    with open('data.b', 'wb') as f:\n         write_records(records, '<idd', f)\nThere are several approaches for reading this file back into a list of tuples. First, if you’re\ngoing to read the file incrementally in chunks, you can write code such as this:\nfrom struct import Struct\ndef read_records(format, f):\n    record_struct = Struct(format)\n    chunks = iter(lambda: f.read(record_struct.size), b'')\n    return (record_struct.unpack(chunk) for chunk in chunks)\n# Example\nif __name__ == '__main__':\n    with open('data.b','rb') as f:\n        for rec in read_records('<idd', f):\n            # Process rec\n            ...\nIf you want to read the file entirely into a byte string with a single read and convert it\npiece by piece, you can write the following:\nfrom struct import Struct\ndef unpack_records(format, data):\n    record_struct = Struct(format)\n    return (record_struct.unpack_from(data, offset)\n            for offset in range(0, len(data), record_struct.size))\n200 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": 5,
      "content": "# Example\nif __name__ == '__main__':\n    with open('data.b', 'rb') as f:\n        data = f.read()\n    for rec in unpack_records('<idd', data):\n        # Process rec\n        ...\nIn both cases, the result is an iterable that produces the tuples originally stored when\nthe file was created.\nDiscussion\nFor programs that must encode and decode binary data, it is common to use the struct\nmodule. To declare a new structure, simply create an instance of Struct such as:\n# Little endian 32-bit integer, two double precision floats\nrecord_struct = Struct('<idd')\nStructures are always defined using a set of structure codes such as i, d, f, and so forth\n[see the Python documentation]. These codes correspond to specific binary data types\nsuch as 32-bit integers, 64-bit floats, 32-bit floats, and so forth. The < in the first character\nspecifies the byte ordering. In this example, it is indicating “little endian.” Change the\ncharacter to > for big endian or ! for network byte order.\nThe resulting Struct instance has various attributes and methods for manipulating\nstructures of that type. The size attribute contains the size of the structure in bytes,\nwhich is useful to have in I/O operations. pack() and unpack() methods are used to\npack and unpack data. For example:\n>>> from struct import Struct\n>>> record_struct = Struct('<idd')\n>>> record_struct.size\n20\n>>> record_struct.pack(1, 2.0, 3.0)\nb'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@'\n>>> record_struct.unpack(_)\n(1, 2.0, 3.0)\n>>>\nSometimes you’ll see the pack() and unpack() operations called as module-level func‐\ntions, as in the following:\n>>> import struct\n>>> struct.pack('<idd', 1, 2.0, 3.0)\nb'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@'\n>>> struct.unpack('<idd', _)\n(1, 2.0, 3.0)\n>>>\n6.11. Reading and Writing Binary Arrays of Structures \n| \n201",
      "content_length": 1873,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": 5,
      "content": "This works, but feels less elegant than creating a single Struct instance—especially if\nthe same structure appears in multiple places in your code. By creating a Struct in‐\nstance, the format code is only specified once and all of the useful operations are grouped\ntogether nicely. This certainly makes it easier to maintain your code if you need to fiddle\nwith the structure code (as you only have to change it in one place).\nThe code for reading binary structures involves a number of interesting, yet elegant\nprogramming idioms. In the read_records() function, iter() is being used to make\nan iterator that returns fixed-sized chunks. See Recipe 5.8. This iterator repeatedly calls\na user-supplied callable (e.g., lambda: f.read(record_struct.size)) until it returns\na specified value (e.g., b), at which point iteration stops. For example:\n>>> f = open('data.b', 'rb')\n>>> chunks = iter(lambda: f.read(20), b'')\n>>> chunks\n<callable_iterator object at 0x10069e6d0>\n>>> for chk in chunks:\n...     print(chk)\n...\nb'\\x01\\x00\\x00\\x00ffffff\\x02@\\x00\\x00\\x00\\x00\\x00\\x00\\x12@'\nb'\\x06\\x00\\x00\\x00333333\\x1f@\\x00\\x00\\x00\\x00\\x00\\x00\"@'\nb'\\x0c\\x00\\x00\\x00\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc*@\\x9a\\x99\\x99\\x99\\x99YL@'\n>>>\nOne reason for creating an iterable is that it nicely allows records to be created using a\ngenerator comprehension, as shown in the solution. If you didn’t use this approach, the\ncode might look like this:\ndef read_records(format, f):\n    record_struct = Struct(format)\n    while True:\n        chk = f.read(record_struct.size)\n        if chk == b'':\n            break\n        yield record_struct.unpack(chk)\n    return records\nIn the unpack_records() function, a different approach using the unpack_from()\nmethod is used. unpack_from() is a useful method for extracting binary data from a\nlarger binary array, because it does so without making any temporary objects or memory\ncopies. You just give it a byte string (or any array) along with a byte offset, and it will\nunpack fields directly from that location.\nIf you used unpack() instead of unpack_from(), you would need to modify the code to\nmake a lot of small slices and offset calculations. For example:\ndef unpack_records(format, data):\n    record_struct = Struct(format)\n    return (record_struct.unpack(data[offset:offset + record_struct.size])\n            for offset in range(0, len(data), record_struct.size))\n202 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 2423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": 5,
      "content": "In addition to being more complicated to read, this version also requires a lot more\nwork, as it performs various offset calculations, copies data, and makes small slice ob‐\njects. If you’re going to be unpacking a lot of structures from a large byte string you’ve\nalready read, unpack_from() is a more elegant approach.\nUnpacking records is one place where you might want to use namedtuple objects from\nthe collections module. This allows you to set attribute names on the returned tuples.\nFor example:\nfrom collections import namedtuple\nRecord = namedtuple('Record', ['kind','x','y'])\nwith open('data.p', 'rb') as f:\n    records = (Record(*r) for r in read_records('<idd', f))\nfor r in records:\n    print(r.kind, r.x, r.y)\nIf you’re writing a program that needs to work with a large amount of binary data, you\nmay be better off using a library such as numpy. For example, instead of reading a binary\ninto a list of tuples, you could read it into a structured array, like this:\n>>> import numpy as np\n>>> f = open('data.b', 'rb')\n>>> records = np.fromfile(f, dtype='<i,<d,<d')\n>>> records\narray([(1, 2.3, 4.5), (6, 7.8, 9.0), (12, 13.4, 56.7)],\n      dtype=[('f0', '<i4'), ('f1', '<f8'), ('f2', '<f8')])\n>>> records[0]\n(1, 2.3, 4.5)\n>>> records[1]\n(6, 7.8, 9.0)\n>>>\nLast, but not least, if you’re faced with the task of reading binary data in some known\nfile format (i.e., image formats, shape files, HDF5, etc.), check to see if a Python module\nalready exists for it. There’s no reason to reinvent the wheel if you don’t have to.\n6.12. Reading Nested and Variable-Sized Binary\nStructures\nProblem\nYou need to read complicated binary-encoded data that contains a collection of nested\nand/or variable-sized records. Such data might include images, video, shapefiles, and\nso on.\n6.12. Reading Nested and Variable-Sized Binary Structures \n| \n203",
      "content_length": 1842,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": 5,
      "content": "Solution\nThe struct module can be used to decode and encode almost any kind of binary data\nstructure. To illustrate the kind of data in question here, suppose you have this Python\ndata structure representing a collection of points that make up a series of polygons:\npolys = [\n          [ (1.0, 2.5), (3.5, 4.0), (2.5, 1.5) ],\n          [ (7.0, 1.2), (5.1, 3.0), (0.5, 7.5), (0.8, 9.0) ],\n          [ (3.4, 6.3), (1.2, 0.5), (4.6, 9.2) ],\n        ]\nNow suppose this data was to be encoded into a binary file where the file started with\nthe following header:\nByte\nType\nDescription\n0\nint\nFile code (0x1234, little endian)\n4\ndouble\nMinimum x (little endian)\n12\ndouble\nMinimum y (little endian)\n20\ndouble\nMaximum x (little endian)\n28\ndouble\nMaximum y (little endian)\n36\nint\nNumber of polygons (little endian)\nFollowing the header, a series of polygon records follow, each encoded as follows:\nByte\nType\nDescription\n0\nint\nRecord length including length (N bytes)\n4-N\nPoints\nPairs of (X,Y) coords as doubles\nTo write this file, you can use Python code like this:\nimport struct\nimport itertools\ndef write_polys(filename, polys):\n    # Determine bounding box\n    flattened = list(itertools.chain(*polys))\n    min_x = min(x for x, y in flattened)\n    max_x = max(x for x, y in flattened)\n    min_y = min(y for x, y in flattened)\n    max_y = max(y for x, y in flattened)\n    with open(filename, 'wb') as f:\n        f.write(struct.pack('<iddddi',\n                            0x1234,\n                            min_x, min_y,\n                            max_x, max_y,\n                            len(polys)))\n204 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": 5,
      "content": "for poly in polys:\n            size = len(poly) * struct.calcsize('<dd')\n            f.write(struct.pack('<i', size+4))\n            for pt in poly:\n                f.write(struct.pack('<dd', *pt))\n# Call it with our polygon data\nwrite_polys('polys.bin', polys)\nTo read the resulting data back, you can write very similar looking code using the \nstruct.unpack() function, reversing the operations performed during writing. For\nexample:\nimport struct\ndef read_polys(filename):\n    with open(filename, 'rb') as f:\n        # Read the header\n        header = f.read(40)\n        file_code, min_x, min_y, max_x, max_y, num_polys = \\\n            struct.unpack('<iddddi', header)\n        polys = []\n        for n in range(num_polys):\n            pbytes, = struct.unpack('<i', f.read(4))\n            poly = []\n            for m in range(pbytes // 16):\n                pt = struct.unpack('<dd', f.read(16))\n                poly.append(pt)\n            polys.append(poly)\n    return polys\nAlthough this code works, it’s also a rather messy mix of small reads, struct unpacking,\nand other details. If code like this is used to process a real datafile, it can quickly become\neven messier. Thus, it’s an obvious candidate for an alternative solution that might sim‐\nplify some of the steps and free the programmer to focus on more important matters.\nIn the remainder of this recipe, a rather advanced solution for interpreting binary data\nwill be built up in pieces. The goal will be to allow a programmer to provide a high-level\nspecification of the file format, and to simply have the details of reading and unpacking\nall of the data worked out under the covers. As a forewarning, the code that follows may\nbe the most advanced example in this entire book, utilizing various object-oriented\nprogramming and metaprogramming techniques. Be sure to carefully read the discus‐\nsion section as well as cross-references to other recipes.\nFirst, when reading binary data, it is common for the file to contain headers and other\ndata structures. Although the struct module can unpack this data into a tuple, another\nway to represent such information is through the use of a class. Here’s some code that\nallows just that:\n6.12. Reading Nested and Variable-Sized Binary Structures \n| \n205",
      "content_length": 2263,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": 5,
      "content": "import struct\nclass StructField:\n    '''\n    Descriptor representing a simple structure field\n    '''\n    def __init__(self, format, offset):\n        self.format = format\n        self.offset = offset\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            r =  struct.unpack_from(self.format,\n                                    instance._buffer, self.offset)\n            return r[0] if len(r) == 1 else r\nclass Structure:\n    def __init__(self, bytedata):\n        self._buffer = memoryview(bytedata)\nThis code uses a descriptor to represent each structure field. Each descriptor contains\na struct-compatible format code along with a byte offset into an underlying memory\nbuffer. In the __get__() method, the struct.unpack_from() function is used to unpack\na value from the buffer without having to make extra slices or copies.\nThe Structure class just serves as a base class that accepts some byte data and stores it\nas the underlying memory buffer used by the StructField descriptor. The use of a \nmemoryview() in this class serves a purpose that will become clear later.\nUsing this code, you can now define a structure as a high-level class that mirrors the\ninformation found in the tables that described the expected file format. For example:\nclass PolyHeader(Structure):\n    file_code = StructField('<i', 0)\n    min_x = StructField('<d', 4)\n    min_y = StructField('<d', 12)\n    max_x = StructField('<d', 20)\n    max_y = StructField('<d', 28)\n    num_polys = StructField('<i', 36)\nHere is an example of using this class to read the header from the polygon data written\nearlier:\n>>> f = open('polys.bin', 'rb')\n>>> phead = PolyHeader(f.read(40))\n>>> phead.file_code == 0x1234\nTrue\n>>> phead.min_x\n0.5\n>>> phead.min_y\n0.5\n206 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1833,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": 5,
      "content": ">>> phead.max_x\n7.0\n>>> phead.max_y\n9.2\n>>> phead.num_polys\n3\n>>>\nThis is interesting, but there are a number of annoyances with this approach. For one,\neven though you get the convenience of a class-like interface, the code is rather verbose\nand requires the user to specify a lot of low-level detail (e.g., repeated uses of Struct\nField, specification of offsets, etc.). The resulting class is also missing common con‐\nveniences such as providing a way to compute the total size of the structure.\nAny time you are faced with class definitions that are overly verbose like this, you might\nconsider the use of a class decorator or metaclass. One of the features of a metaclass is\nthat it can be used to fill in a lot of low-level implementation details, taking that burden\noff of the user. As an example, consider this metaclass and slight reformulation of the\nStructure class:\nclass StructureMeta(type):\n    '''\n    Metaclass that automatically creates StructField descriptors\n    '''\n    def __init__(self, clsname, bases, clsdict):\n        fields = getattr(self, '_fields_', [])\n        byte_order = ''\n        offset = 0\n        for format, fieldname in fields:\n            if format.startswith(('<','>','!','@')):\n                byte_order = format[0]\n                format = format[1:]\n            format = byte_order + format\n            setattr(self, fieldname, StructField(format, offset))\n            offset += struct.calcsize(format)\n        setattr(self, 'struct_size', offset)\nclass Structure(metaclass=StructureMeta):\n    def __init__(self, bytedata):\n        self._buffer = bytedata\n    @classmethod\n    def from_file(cls, f):\n        return cls(f.read(cls.struct_size))\nUsing this new Structure class, you can now write a structure definition like this:\nclass PolyHeader(Structure):\n    _fields_ = [\n       ('<i', 'file_code'),\n       ('d', 'min_x'),\n       ('d', 'min_y'),\n6.12. Reading Nested and Variable-Sized Binary Structures \n| \n207",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": 5,
      "content": "('d', 'max_x'),\n       ('d', 'max_y'),\n       ('i', 'num_polys')\n    ]\nAs you can see, the specification is a lot less verbose. The added from_file() class\nmethod also makes it easier to read the data from a file without knowing any details\nabout the size or structure of the data. For example:\n>>> f = open('polys.bin', 'rb')\n>>> phead = PolyHeader.from_file(f)\n>>> phead.file_code == 0x1234\nTrue\n>>> phead.min_x\n0.5\n>>> phead.min_y\n0.5\n>>> phead.max_x\n7.0\n>>> phead.max_y\n9.2\n>>> phead.num_polys\n3\n>>>\nOnce you introduce a metaclass into the mix, you can build more intelligence into it.\nFor example, suppose you want to support nested binary structures. Here’s a reformu‐\nlation of the metaclass along with a new supporting descriptor that allows it:\nclass NestedStruct:\n    '''\n    Descriptor representing a nested structure\n    '''\n    def __init__(self, name, struct_type, offset):\n        self.name = name\n        self.struct_type = struct_type\n        self.offset = offset\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            data = instance._buffer[self.offset:\n                               self.offset+self.struct_type.struct_size]\n            result = self.struct_type(data)\n            # Save resulting structure back on instance to avoid\n            # further recomputation of this step\n            setattr(instance, self.name, result)\n            return result\nclass StructureMeta(type):\n    '''\n    Metaclass that automatically creates StructField descriptors\n208 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": 5,
      "content": "'''\n    def __init__(self, clsname, bases, clsdict):\n        fields = getattr(self, '_fields_', [])\n        byte_order = ''\n        offset = 0\n        for format, fieldname in fields:\n            if isinstance(format, StructureMeta):\n                setattr(self, fieldname,\n                        NestedStruct(fieldname, format, offset))\n                offset += format.struct_size\n            else:\n                if format.startswith(('<','>','!','@')):\n                    byte_order = format[0]\n                    format = format[1:]\n                format = byte_order + format\n                setattr(self, fieldname, StructField(format, offset))\n                offset += struct.calcsize(format)\n        setattr(self, 'struct_size', offset)\nIn this code, the NestedStruct descriptor is used to overlay another structure definition\nover a region of memory. It does this by taking a slice of the original memory buffer\nand using it to instantiate the given structure type. Since the underlying memory buffer\nwas initialized as a memoryview, this slicing does not incur any extra memory copies.\nInstead, it’s just an overlay on the original memory. Moreover, to avoid repeated in‐\nstantiations, the descriptor then stores the resulting inner structure object on the in‐\nstance using the same technique described in Recipe 8.10.\nUsing this new formulation, you can start to write code like this:\nclass Point(Structure):\n    _fields_ = [\n          ('<d', 'x'),\n          ('d', 'y')\n    ]\nclass PolyHeader(Structure):\n    _fields_ = [\n          ('<i', 'file_code'),\n          (Point, 'min'),         # nested struct\n          (Point, 'max'),         # nested struct\n          ('i', 'num_polys')\n    ]\nAmazingly, it will all still work as you expect. For example:\n>>> f = open('polys.bin', 'rb')\n>>> phead = PolyHeader.from_file(f)\n>>> phead.file_code == 0x1234\nTrue\n>>> phead.min       # Nested structure\n<__main__.Point object at 0x1006a48d0>\n>>> phead.min.x\n6.12. Reading Nested and Variable-Sized Binary Structures \n| \n209",
      "content_length": 2031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": 5,
      "content": "0.5\n>>> phead.min.y\n0.5\n>>> phead.max.x\n7.0\n>>> phead.max.y\n9.2\n>>> phead.num_polys\n3\n>>>\nAt this point, a framework for dealing with fixed-sized records has been developed, but\nwhat about the variable-sized components? For example, the remainder of the polygon\nfiles contain sections of variable size.\nOne way to handle this is to write a class that simply represents a chunk of binary data\nalong with a utility function for interpreting the contents in different ways. This is closely\nrelated to the code in Recipe 6.11:\nclass SizedRecord:\n    def __init__(self, bytedata):\n        self._buffer = memoryview(bytedata)\n    @classmethod\n    def from_file(cls, f, size_fmt, includes_size=True):\n        sz_nbytes = struct.calcsize(size_fmt)\n        sz_bytes = f.read(sz_nbytes)\n        sz, = struct.unpack(size_fmt, sz_bytes)\n        buf = f.read(sz - includes_size * sz_nbytes)\n        return cls(buf)\n    def iter_as(self, code):\n        if isinstance(code, str):\n            s = struct.Struct(code)\n            for off in range(0, len(self._buffer), s.size):\n                yield s.unpack_from(self._buffer, off)\n        elif isinstance(code, StructureMeta):\n            size = code.struct_size\n            for off in range(0, len(self._buffer), size):\n                data = self._buffer[off:off+size]\n                yield code(data)\nThe SizedRecord.from_file() class method is a utility for reading a size-prefixed\nchunk of data from a file, which is common in many file formats. As input, it accepts a\nstructure format code containing the encoding of the size, which is expected to be in\nbytes. The optional includes_size argument specifies whether the number of bytes\nincludes the size header or not. Here’s an example of how you would use this code to\nread the individual polygons in the polygon file:\n210 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": 5,
      "content": ">>> f = open('polys.bin', 'rb')\n>>> phead = PolyHeader.from_file(f)\n>>> phead.num_polys\n3\n>>> polydata = [ SizedRecord.from_file(f, '<i')\n...              for n in range(phead.num_polys) ]\n>>> polydata\n[<__main__.SizedRecord object at 0x1006a4d50>,\n <__main__.SizedRecord object at 0x1006a4f50>,\n <__main__.SizedRecord object at 0x10070da90>]\n>>>\nAs shown, the contents of the SizedRecord instances have not yet been interpreted. To\ndo that, use the iter_as() method, which accepts a structure format code or Struc\nture class as input. This gives you a lot of flexibility in how to interpret the data. For\nexample:\n>>> for n, poly in enumerate(polydata):\n...     print('Polygon', n)\n...     for p in poly.iter_as('<dd'):\n...             print(p)\n...\nPolygon 0\n(1.0, 2.5)\n(3.5, 4.0)\n(2.5, 1.5)\nPolygon 1\n(7.0, 1.2)\n(5.1, 3.0)\n(0.5, 7.5)\n(0.8, 9.0)\nPolygon 2\n(3.4, 6.3)\n(1.2, 0.5)\n(4.6, 9.2)\n>>>\n>>> for n, poly in enumerate(polydata):\n...     print('Polygon', n)\n...     for p in poly.iter_as(Point):\n...             print(p.x, p.y)\n...\nPolygon 0\n1.0 2.5\n3.5 4.0\n2.5 1.5\nPolygon 1\n7.0 1.2\n5.1 3.0\n0.5 7.5\n0.8 9.0\nPolygon 2\n6.12. Reading Nested and Variable-Sized Binary Structures \n| \n211",
      "content_length": 1187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": 5,
      "content": "3.4 6.3\n1.2 0.5\n4.6 9.2\n>>>\nPutting all of this together, here’s an alternative formulation of the read_polys() func‐\ntion:\nclass Point(Structure):\n    _fields_ = [\n        ('<d', 'x'),\n        ('d', 'y')\n        ]\nclass PolyHeader(Structure):\n    _fields_ = [\n        ('<i', 'file_code'),\n        (Point, 'min'),\n        (Point, 'max'),\n        ('i', 'num_polys')\n    ]\ndef read_polys(filename):\n    polys = []\n    with open(filename, 'rb') as f:\n        phead = PolyHeader.from_file(f)\n        for n in range(phead.num_polys):\n            rec = SizedRecord.from_file(f, '<i')\n            poly = [ (p.x, p.y)\n                      for p in rec.iter_as(Point) ]\n            polys.append(poly)\n    return polys\nDiscussion\nThis recipe provides a practical application of various advanced programming techni‐\nques, including descriptors, lazy evaluation, metaclasses, class variables, and memory‐\nviews. However, they all serve a very specific purpose.\nA major feature of the implementation is that it is strongly based on the idea of lazy-\nunpacking. When an instance of Structure is created, the __init__() merely creates\na memoryview of the supplied byte data and does nothing else. Specifically, no unpack‐\ning or other structure-related operations take place at this time. One motivation for\ntaking this approach is that you might only be interested in a few specific parts of a\nbinary record. Rather than unpacking the whole file, only the parts that are actually\naccessed will be unpacked.\nTo implement the lazy unpacking and packing of values, the StructField descriptor\nclass is used. Each attribute the user lists in _fields_ gets converted to a Struct\nField descriptor that stores the associated structure format code and byte offset into\n212 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 1794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": 5,
      "content": "the stored buffer. The StructureMeta metaclass is what creates these descriptors auto‐\nmatically when various structure classes are defined. The main reason for using a\nmetaclass is to make it extremely easy for a user to specify a structure format with a\nhigh-level description without worrying about low-level details.\nOne subtle aspect of the StructureMeta metaclass is that it makes byte order sticky.\nThat is, if any attribute specifies a byte order (< for little endian or > for big endian),\nthat ordering is applied to all fields that follow. This helps avoid extra typing, but also\nmakes it possible to switch in the middle of a definition. For example, you might have\nsomething more complicated, such as this:\nclass ShapeFile(Structure):\n    _fields_ = [ ('>i', 'file_code'),    # Big endian\n                 ('20s', 'unused'),\n                 ('i', 'file_length'),\n                 ('<i', 'version'),      # Little endian\n                 ('i', 'shape_type'),\n                 ('d', 'min_x'),\n                 ('d', 'min_y'),\n                 ('d', 'max_x'),\n                 ('d', 'max_y'),\n                 ('d', 'min_z'),\n                 ('d', 'max_z'),\n                 ('d', 'min_m'),\n                 ('d', 'max_m') ]\nAs noted, the use of a memoryview() in the solution serves a useful role in avoiding\nmemory copies. When structures start to nest, memoryviews can be used to overlay\ndifferent parts of the structure definition on the same region of memory. This aspect of\nthe solution is subtle, but it concerns the slicing behavior of a memoryview versus a\nnormal byte array. If you slice a byte string or byte array, you usually get a copy of the\ndata. Not so with a memoryview—slices simply overlay the existing memory. Thus, this\napproach is more efficient.\nA number of related recipes will help expand upon the topics used in the solution. See\nRecipe 8.13 for a closely related recipe that uses descriptors to build a type system.\nRecipe 8.10 has information about lazily computed properties and is related to the\nimplementation of the NestedStruct descriptor. Recipe 9.19 has an example of using a\nmetaclass to initialize class members, much in the same manner as the StructureMe\nta class. The source code for Python’s ctypes library may also be of interest, due to its\nsimilar support for defining data structures, nesting of data structures, and similar\nfunctionality.\n6.12. Reading Nested and Variable-Sized Binary Structures \n| \n213",
      "content_length": 2461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": 5,
      "content": "6.13. Summarizing Data and Performing Statistics\nProblem\nYou need to crunch through large datasets and generate summaries or other kinds of\nstatistics.\nSolution\nFor any kind of data analysis involving statistics, time series, and other related techni‐\nques, you should look at the Pandas library.\nTo give you a taste, here’s an example of using Pandas to analyze the City of Chicago \nrat and rodent database. At the time of this writing, it’s a CSV file with about 74,000\nentries:\n>>> import pandas\n>>> # Read a CSV file, skipping last line\n>>> rats = pandas.read_csv('rats.csv', skip_footer=1)\n>>> rats\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 74055 entries, 0 to 74054\nData columns:\nCreation Date                      74055  non-null values\nStatus                             74055  non-null values\nCompletion Date                    72154  non-null values\nService Request Number             74055  non-null values\nType of Service Request            74055  non-null values\nNumber of Premises Baited          65804  non-null values\nNumber of Premises with Garbage    65600  non-null values\nNumber of Premises with Rats       65752  non-null values\nCurrent Activity                   66041  non-null values\nMost Recent Action                 66023  non-null values\nStreet Address                     74055  non-null values\nZIP Code                           73584  non-null values\nX Coordinate                       74043  non-null values\nY Coordinate                       74043  non-null values\nWard                               74044  non-null values\nPolice District                    74044  non-null values\nCommunity Area                     74044  non-null values\nLatitude                           74043  non-null values\nLongitude                          74043  non-null values\nLocation                           74043  non-null values\ndtypes: float64(11), object(9)\n>>> # Investigate range of values for a certain field\n>>> rats['Current Activity'].unique()\narray([nan, Dispatch Crew, Request Sanitation Inspector], dtype=object)\n214 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": 5,
      "content": ">>> # Filter the data\n>>> crew_dispatched = rats[rats['Current Activity'] == 'Dispatch Crew']\n>>> len(crew_dispatched)\n65676\n>>>\n>>> # Find 10 most rat-infested ZIP codes in Chicago\n>>> crew_dispatched['ZIP Code'].value_counts()[:10]\n60647    3837\n60618    3530\n60614    3284\n60629    3251\n60636    2801\n60657    2465\n60641    2238\n60609    2206\n60651    2152\n60632    2071\n>>>\n>>> # Group by completion date\n>>> dates = crew_dispatched.groupby('Completion Date')\n<pandas.core.groupby.DataFrameGroupBy object at 0x10d0a2a10>\n>>> len(dates)\n472\n>>>\n>>> # Determine counts on each day\n>>> date_counts = dates.size()\n>>> date_counts[0:10]\nCompletion Date\n01/03/2011           4\n01/03/2012         125\n01/04/2011          54\n01/04/2012          38\n01/05/2011          78\n01/05/2012         100\n01/06/2011         100\n01/06/2012          58\n01/07/2011           1\n01/09/2012          12\n>>>\n>>> # Sort the counts\n>>> date_counts.sort()\n>>> date_counts[-10:]\nCompletion Date\n10/12/2012         313\n10/21/2011         314\n09/20/2011         316\n10/26/2011         319\n02/22/2011         325\n6.13. Summarizing Data and Performing Statistics \n| \n215",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": 5,
      "content": "10/26/2012         333\n03/17/2011         336\n10/13/2011         378\n10/14/2011         391\n10/07/2011         457\n>>>\nYes, October 7, 2011, was indeed a very busy day for rats.\nDiscussion\nPandas is a large library that has more features than can be described here. However, if\nyou need to analyze large datasets, group data, perform statistics, or other similar tasks,\nit’s definitely worth a look.\nPython for Data Analysis by Wes McKinney (O’Reilly) also contains much more\ninformation.\n216 \n| \nChapter 6: Data Encoding and Processing",
      "content_length": 536,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": 5,
      "content": "CHAPTER 7\nFunctions\nDefining functions using the def statement is a cornerstone of all programs. The goal\nof this chapter is to present some more advanced and unusual function definition and\nusage patterns. Topics include default arguments, functions that take any number of\narguments, keyword-only arguments, annotations, and closures. In addition, some\ntricky control flow and data passing problems involving callback functions are\naddressed.\n7.1. Writing Functions That Accept Any Number of\nArguments\nProblem\nYou want to write a function that accepts any number of input arguments.\nSolution\nTo write a function that accepts any number of positional arguments, use a * argument.\nFor example:\ndef avg(first, *rest):\n    return (first + sum(rest)) / (1 + len(rest))\n# Sample use\navg(1, 2)          # 1.5\navg(1, 2, 3, 4)    # 2.5\nIn this example, rest is a tuple of all the extra positional arguments passed. The code\ntreats it as a sequence in performing subsequent calculations.\n217",
      "content_length": 983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": 5,
      "content": "To accept any number of keyword arguments, use an argument that starts with **. For\nexample:\nimport html\ndef make_element(name, value, **attrs):\n    keyvals = [' %s=\"%s\"' % item for item in attrs.items()]\n    attr_str = ''.join(keyvals)\n    element = '<{name}{attrs}>{value}</{name}>'.format(\n                  name=name,\n                  attrs=attr_str,\n                  value=html.escape(value))\n    return element\n# Example\n# Creates '<item size=\"large\" quantity=\"6\">Albatross</item>'\nmake_element('item', 'Albatross', size='large', quantity=6)\n# Creates '<p>&lt;spam&gt;</p>'\nmake_element('p', '<spam>')\nHere, attrs is a dictionary that holds the passed keyword arguments (if any).\nIf you want a function that can accept both any number of positional and keyword-only\narguments, use * and ** together. For example:\ndef anyargs(*args, **kwargs):\n    print(args)      # A tuple\n    print(kwargs)    # A dict\nWith this function, all of the positional arguments are placed into a tuple args, and all\nof the keyword arguments are placed into a dictionary kwargs.\nDiscussion\nA * argument can only appear as the last positional argument in a function definition.\nA ** argument can only appear as the last argument. A subtle aspect of function defi‐\nnitions is that arguments can still appear after a * argument.\ndef a(x, *args, y):\n    pass\ndef b(x, *args, y, **kwargs):\n    pass\nSuch arguments are known as keyword-only arguments, and are discussed further in\nRecipe 7.2.\n218 \n| \nChapter 7: Functions",
      "content_length": 1500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": 5,
      "content": "7.2. Writing Functions That Only Accept Keyword\nArguments\nProblem\nYou want a function to only accept certain arguments by keyword.\nSolution\nThis feature is easy to implement if you place the keyword arguments after a * argument\nor a single unnamed *. For example:\ndef recv(maxsize, *, block):\n    'Receives a message'\n    pass\nrecv(1024, True)        #  TypeError\nrecv(1024, block=True)  # Ok\nThis technique can also be used to specify keyword arguments for functions that accept\na varying number of positional arguments. For example:\ndef mininum(*values, clip=None):\n    m = min(values)\n    if clip is not None:\n        m = clip if clip > m else m\n    return m\nminimum(1, 5, 2, -5, 10)          # Returns -5\nminimum(1, 5, 2, -5, 10, clip=0)  # Returns 0\nDiscussion\nKeyword-only arguments are often a good way to enforce greater code clarity when\nspecifying optional function arguments. For example, consider a call like this:\nmsg = recv(1024, False)\nIf someone is not intimately familiar with the workings of the recv(), they may have\nno idea what the False argument means. On the other hand, it is much clearer if the\ncall is written like this:\nmsg = recv(1024, block=False)\nThe use of keyword-only arguments is also often preferrable to tricks involving\n**kwargs, since they show up properly when the user asks for help:\n>>> help(recv)\nHelp on function recv in module __main__:\n7.2. Writing Functions That Only Accept Keyword Arguments \n| \n219",
      "content_length": 1446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": 5,
      "content": "recv(maxsize, *, block)\n    Receives a message\nKeyword-only arguments also have utility in more advanced contexts. For example,\nthey can be used to inject arguments into functions that make use of the *args and\n**kwargs convention for accepting all inputs. See Recipe 9.11 for an example.\n7.3. Attaching Informational Metadata to Function\nArguments\nProblem\nYou’ve written a function, but would like to attach some additional information to the\narguments so that others know more about how a function is supposed to be used.\nSolution\nFunction argument annotations can be a useful way to give programmers hints about\nhow a function is supposed to be used. For example, consider the following annotated\nfunction:\ndef add(x:int, y:int) -> int:\n    return x + y\nThe Python interpreter does not attach any semantic meaning to the attached annota‐\ntions. They are not type checks, nor do they make Python behave any differently than\nit did before. However, they might give useful hints to others reading the source code\nabout what you had in mind. Third-party tools and frameworks might also attach se‐\nmantic meaning to the annotations. They also appear in documentation:\n>>> help(add)\nHelp on function add in module __main__:\nadd(x: int, y: int) -> int\n>>>\nAlthough you can attach any kind of object to a function as an annotation (e.g., numbers,\nstrings, instances, etc.), classes or strings often seem to make the most sense.\nDiscussion\nFunction annotations are merely stored in a function’s __annotations__ attribute. For\nexample:\n>>> add.__annotations__\n{'y': <class 'int'>, 'return': <class 'int'>, 'x': <class 'int'>}\n220 \n| \nChapter 7: Functions",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": 5,
      "content": "Although there are many potential uses of annotations, their primary utility is probably\njust documentation. Because Python doesn’t have type declarations, it can often be dif‐\nficult to know what you’re supposed to pass into a function if you’re simply reading its\nsource code in isolation. An annotation gives someone more of a hint.\nSee Recipe 9.20 for an advanced example showing how to use annotations to implement\nmultiple dispatch (i.e., overloaded functions).\n7.4. Returning Multiple Values from a Function\nProblem\nYou want to return multiple values from a function.\nSolution\nTo return multiple values from a function, simply return a tuple. For example:\n>>> def myfun():\n...     return 1, 2, 3\n...\n>>> a, b, c = myfun()\n>>> a\n1\n>>> b\n2\n>>> c\n3\nDiscussion\nAlthough it looks like myfun() returns multiple values, a tuple is actually being created.\nIt looks a bit peculiar, but it’s actually the comma that forms a tuple, not the parentheses.\nFor example:\n>>> a = (1, 2)     # With parentheses\n>>> a\n(1, 2)\n>>> b = 1, 2       # Without parentheses\n>>> b\n(1, 2)\n>>>\nWhen calling functions that return a tuple, it is common to assign the result to multiple\nvariables, as shown. This is simply tuple unpacking, as described in Recipe 1.1. The\nreturn value could also have been assigned to a single variable:\n>>> x = myfun()\n>>> x\n7.4. Returning Multiple Values from a Function \n| \n221",
      "content_length": 1387,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": 5,
      "content": "(1, 2, 3)\n>>>\n7.5. Defining Functions with Default Arguments\nProblem\nYou want to define a function or method where one or more of the arguments are\noptional and have a default value.\nSolution\nOn the surface, defining a function with optional arguments is easy—simply assign\nvalues in the definition and make sure that default arguments appear last. For example:\ndef spam(a, b=42):\n    print(a, b)\nspam(1)      # Ok. a=1, b=42\nspam(1, 2)   # Ok. a=1, b=2\nIf the default value is supposed to be a mutable container, such as a list, set, or dictionary,\nuse None as the default and write code like this:\n# Using a list as a default value\ndef spam(a, b=None):\n    if b is None:\n        b = []\n    ...\nIf, instead of providing a default value, you want to write code that merely tests whether\nan optional argument was given an interesting value or not, use this idiom:\n_no_value = object()\ndef spam(a, b=_no_value):\n    if b is _no_value:\n        print('No b value supplied')\n    ...\nHere’s how this function behaves:\n>>> spam(1)\nNo b value supplied\n>>> spam(1, 2)     # b = 2\n>>> spam(1, None)  # b = None\n>>>\nCarefully observe that there is a distinction between passing no value at all and passing\na value of None.\n222 \n| \nChapter 7: Functions",
      "content_length": 1240,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": 6,
      "content": "Discussion\nDefining functions with default arguments is easy, but there is a bit more to it than meets\nthe eye.\nFirst, the values assigned as a default are bound only once at the time of function defi‐\nnition. Try this example to see it:\n>>> x = 42\n>>> def spam(a, b=x):\n...     print(a, b)\n...\n>>> spam(1)\n1 42\n>>> x = 23     # Has no effect\n>>> spam(1)\n1 42\n>>>\nNotice how changing the variable x (which was used as a default value) has no effect\nwhatsoever. This is because the default value was fixed at function definition time.\nSecond, the values assigned as defaults should always be immutable objects, such as\nNone, True, False, numbers, or strings. Specifically, never write code like this:\ndef spam(a, b=[]):     # NO!\n    ...\nIf you do this, you can run into all sorts of trouble if the default value ever escapes the\nfunction and gets modified. Such changes will permanently alter the default value across\nfuture function calls. For example:\n>>> def spam(a, b=[]):\n...     print(b)\n...     return b\n...\n>>> x = spam(1)\n>>> x\n[]\n>>> x.append(99)\n>>> x.append('Yow!')\n>>> x\n[99, 'Yow!']\n>>> spam(1)       # Modified list gets returned!\n[99, 'Yow!']\n>>>\nThat’s probably not what you want. To avoid this, it’s better to assign None as a default\nand add a check inside the function for it, as shown in the solution.\nThe use of the is operator when testing for None is a critical part of this recipe. Some‐\ntimes people make this mistake:\n7.5. Defining Functions with Default Arguments \n| \n223",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": 6,
      "content": "def spam(a, b=None):\n    if not b:      # NO! Use 'b is None' instead\n        b = []\n    ...\nThe problem here is that although None evaluates to False, many other objects (e.g.,\nzero-length strings, lists, tuples, dicts, etc.) do as well. Thus, the test just shown would\nfalsely treat certain inputs as missing. For example:\n>>> spam(1)        # OK\n>>> x = []\n>>> spam(1, x)     # Silent error. x value overwritten by default\n>>> spam(1, 0)     # Silent error. 0 ignored\n>>> spam(1, '')    # Silent error. '' ignored\n>>>\nThe last part of this recipe is something that’s rather subtle—a function that tests to see\nwhether a value (any value) has been supplied to an optional argument or not. The tricky\npart here is that you can’t use a default value of None, 0, or False to test for the presence\nof a user-supplied argument (since all of these are perfectly valid values that a user might\nsupply). Thus, you need something else to test against.\nTo solve this problem, you can create a unique private instance of object, as shown in\nthe solution (the _no_value variable). In the function, you then check the identity of\nthe supplied argument against this special value to see if an argument was supplied or\nnot. The thinking here is that it would be extremely unlikely for a user to pass the\n_no_value instance in as an input value. Therefore, it becomes a safe value to check\nagainst if you’re trying to determine whether an argument was supplied or not.\nThe use of object() might look rather unusual here. object is a class that serves as the\ncommon base class for almost all objects in Python. You can create instances of ob\nject, but they are wholly uninteresting, as they have no notable methods nor any in‐\nstance data (because there is no underlying instance dictionary, you can’t even set any\nattributes). About the only thing you can do is perform tests for identity. This makes\nthem useful as special values, as shown in the solution.\n7.6. Defining Anonymous or Inline Functions\nProblem\nYou need to supply a short callback function for use with an operation such as sort(),\nbut you don’t want to write a separate one-line function using the def statement. Instead,\nyou’d like a shortcut that allows you to specify the function “in line.”\n224 \n| \nChapter 7: Functions",
      "content_length": 2275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": 6,
      "content": "Solution\nSimple functions that do nothing more than evaluate an expression can be replaced by\na lambda expression. For example:\n>>> add = lambda x, y: x + y\n>>> add(2,3)\n5\n>>> add('hello', 'world')\n'helloworld'\n>>>\nThe use of lambda here is the same as having typed this:\n>>> def add(x, y):\n...     return x + y\n...\n>>> add(2,3)\n5\n>>>\nTypically, lambda is used in the context of some other operation, such as sorting or a\ndata reduction:\n>>> names = ['David Beazley', 'Brian Jones',\n...           'Raymond Hettinger', 'Ned Batchelder']\n>>> sorted(names, key=lambda name: name.split()[-1].lower())\n['Ned Batchelder', 'David Beazley', 'Raymond Hettinger', 'Brian Jones']\n>>>\nDiscussion\nAlthough lambda allows you to define a simple function, its use is highly restricted. In\nparticular, only a single expression can be specified, the result of which is the return\nvalue. This means that no other language features, including multiple statements, con‐\nditionals, iteration, and exception handling, can be included.\nYou can quite happily write a lot of Python code without ever using lambda. However,\nyou’ll occasionally encounter it in programs where someone is writing a lot of tiny\nfunctions that evaluate various expressions, or in programs that require users to supply\ncallback functions.\n7.7. Capturing Variables in Anonymous Functions\nProblem\nYou’ve defined an anonymous function using lambda, but you also need to capture the\nvalues of certain variables at the time of definition.\n7.7. Capturing Variables in Anonymous Functions \n| \n225",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": 6,
      "content": "Solution\nConsider the behavior of the following code:\n>>> x = 10\n>>> a = lambda y: x + y\n>>> x = 20\n>>> b = lambda y: x + y\n>>>\nNow ask yourself a question. What are the values of a(10) and b(10)? If you think the\nresults might be 20 and 30, you would be wrong:\n>>> a(10)\n30\n>>> b(10)\n30\n>>>\nThe problem here is that the value of x used in the lambda expression is a free variable\nthat gets bound at runtime, not definition time. Thus, the value of x in the lambda\nexpressions is whatever the value of the x variable happens to be at the time of execution.\nFor example:\n>>> x = 15\n>>> a(10)\n25\n>>> x = 3\n>>> a(10)\n13\n>>>\nIf you want an anonymous function to capture a value at the point of definition and\nkeep it, include the value as a default value, like this:\n>>> x = 10\n>>> a = lambda y, x=x: x + y\n>>> x = 20\n>>> b = lambda y, x=x: x + y\n>>> a(10)\n20\n>>> b(10)\n30\n>>>\nDiscussion\nThe problem addressed in this recipe is something that tends to come up in code that\ntries to be just a little bit too clever with the use of lambda functions. For example,\ncreating a list of lambda expressions using a list comprehension or in a loop of some\n226 \n| \nChapter 7: Functions",
      "content_length": 1171,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": 6,
      "content": "kind and expecting the lambda functions to remember the iteration variable at the time\nof definition. For example:\n>>> funcs = [lambda x: x+n for n in range(5)]\n>>> for f in funcs:\n...     print(f(0))\n...\n4\n4\n4\n4\n4\n>>>\nNotice how all functions think that n has the last value during iteration. Now compare\nto the following:\n>>> funcs = [lambda x, n=n: x+n for n in range(5)]\n>>> for f in funcs:\n...     print(f(0))\n...\n0\n1\n2\n3\n4\n>>>\nAs you can see, the functions now capture the value of n at the time of definition.\n7.8. Making an N-Argument Callable Work As a Callable\nwith Fewer Arguments\nProblem\nYou have a callable that you would like to use with some other Python code, possibly as\na callback function or handler, but it takes too many arguments and causes an exception\nwhen called.\nSolution\nIf you need to reduce the number of arguments to a function, you should use func\ntools.partial(). The partial() function allows you to assign fixed values to one or\nmore of the arguments, thus reducing the number of arguments that need to be supplied\nto subsequent calls. To illustrate, suppose you have this function:\ndef spam(a, b, c, d):\n    print(a, b, c, d)\n7.8. Making an N-Argument Callable Work As a Callable with Fewer Arguments \n| \n227",
      "content_length": 1243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": 6,
      "content": "Now consider the use of partial() to fix certain argument values:\n>>> from functools import partial\n>>> s1 = partial(spam, 1)       # a = 1\n>>> s1(2, 3, 4)\n1 2 3 4\n>>> s1(4, 5, 6)\n1 4 5 6\n>>> s2 = partial(spam, d=42)    # d = 42\n>>> s2(1, 2, 3)\n1 2 3 42\n>>> s2(4, 5, 5)\n4 5 5 42\n>>> s3 = partial(spam, 1, 2, d=42) # a = 1, b = 2, d = 42\n>>> s3(3)\n1 2 3 42\n>>> s3(4)\n1 2 4 42\n>>> s3(5)\n1 2 5 42\n>>>\nObserve that partial() fixes the values for certain arguments and returns a new callable\nas a result. This new callable accepts the still unassigned arguments, combines them\nwith the arguments given to partial(), and passes everything to the original function.\nDiscussion\nThis recipe is really related to the problem of making seemingly incompatible bits of\ncode work together. A series of examples will help illustrate.\nAs a first example, suppose you have a list of points represented as tuples of (x,y) co‐\nordinates. You could use the following function to compute the distance between two\npoints:\npoints = [ (1, 2), (3, 4), (5, 6), (7, 8) ]\nimport math\ndef distance(p1, p2):\n    x1, y1 = p1\n    x2, y2 = p2\n    return math.hypot(x2 - x1, y2 - y1)\nNow suppose you want to sort all of the points according to their distance from some\nother point. The sort() method of lists accepts a key argument that can be used to\ncustomize sorting, but it only works with functions that take a single argument (thus,\ndistance() is not suitable). Here’s how you might use partial() to fix it:\n228 \n| \nChapter 7: Functions",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": 6,
      "content": ">>> pt = (4, 3)\n>>> points.sort(key=partial(distance,pt))\n>>> points\n[(3, 4), (1, 2), (5, 6), (7, 8)]\n>>>\nAs an extension of this idea, partial() can often be used to tweak the argument sig‐\nnatures of callback functions used in other libraries. For example, here’s a bit of code\nthat uses multiprocessing to asynchronously compute a result which is handed to a\ncallback function that accepts both the result and an optional logging argument:\ndef output_result(result, log=None):\n    if log is not None:\n        log.debug('Got: %r', result)\n# A sample function\ndef add(x, y):\n    return x + y\nif __name__ == '__main__':\n    import logging\n    from multiprocessing import Pool\n    from functools import partial\n    logging.basicConfig(level=logging.DEBUG)\n    log = logging.getLogger('test')\n    p = Pool()\n    p.apply_async(add, (3, 4), callback=partial(output_result, log=log))\n    p.close()\n    p.join()\nWhen supplying the callback function using apply_async(), the extra logging argu‐\nment is given using partial(). multiprocessing is none the wiser about all of this—\nit simply invokes the callback function with a single value.\nAs a similar example, consider the problem of writing network servers. The socket\nserver module makes it relatively easy. For example, here is a simple echo server:\nfrom socketserver import StreamRequestHandler, TCPServer\nclass EchoHandler(StreamRequestHandler):\n    def handle(self):\n        for line in self.rfile:\n            self.wfile.write(b'GOT:' + line)\nserv = TCPServer(('', 15000), EchoHandler)\nserv.serve_forever()\nHowever, suppose you want to give the EchoHandler class an __init__() method that\naccepts an additional configuration argument. For example:\n7.8. Making an N-Argument Callable Work As a Callable with Fewer Arguments \n| \n229",
      "content_length": 1782,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": 6,
      "content": "class EchoHandler(StreamRequestHandler):\n    # ack is added keyword-only argument. *args, **kwargs are\n    # any normal parameters supplied (which are passed on)\n    def __init__(self, *args, ack, **kwargs):\n        self.ack = ack\n        super().__init__(*args, **kwargs)\n    def handle(self):\n        for line in self.rfile:\n            self.wfile.write(self.ack + line)\nIf you make this change, you’ll find there is no longer an obvious way to plug it into the\nTCPServer class. In fact, you’ll find that the code now starts generating exceptions like\nthis:\nException happened during processing of request from ('127.0.0.1', 59834)\nTraceback (most recent call last):\n ...\nTypeError: __init__() missing 1 required keyword-only argument: 'ack'\nAt first glance, it seems impossible to fix this code, short of modifying the source code\nto socketserver or coming up with some kind of weird workaround. However, it’s easy\nto resolve using partial()—just use it to supply the value of the ack argument, like\nthis:\nfrom functools import partial\nserv = TCPServer(('', 15000), partial(EchoHandler, ack=b'RECEIVED:'))\nserv.serve_forever()\nIn this example, the specification of the ack argument in the __init__() method might\nlook a little funny, but it’s being specified as a keyword-only argument. This is discussed\nfurther in Recipe 7.2.\nThe functionality of partial() is sometimes replaced with a lambda expression. For\nexample, the previous examples might use statements such as this:\npoints.sort(key=lambda p: distance(pt, p))\np.apply_async(add, (3, 4), callback=lambda result: output_result(result,log))\nserv = TCPServer(('', 15000),\n                 lambda *args, **kwargs: EchoHandler(*args,\n                                                     ack=b'RECEIVED:',\n                                                     **kwargs))\nThis code works, but it’s more verbose and potentially a lot more confusing to someone\nreading it. Using partial() is a bit more explicit about your intentions (supplying\nvalues for some of the arguments).)\n230 \n| \nChapter 7: Functions",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": 6,
      "content": "7.9. Replacing Single Method Classes with Functions\nProblem\nYou have a class that only defines a single method besides __init__(). However, to\nsimplify your code, you would much rather just have a simple function.\nSolution\nIn many cases, single-method classes can be turned into functions using closures. Con‐\nsider, as an example, the following class, which allows a user to fetch URLs using a kind\nof templating scheme.\nfrom urllib.request import urlopen\nclass UrlTemplate:\n    def __init__(self, template):\n        self.template = template\n    def open(self, **kwargs):\n        return urlopen(self.template.format_map(kwargs))\n# Example use. Download stock data from yahoo\nyahoo = UrlTemplate('http://finance.yahoo.com/d/quotes.csv?s={names}&f={fields}')\nfor line in yahoo.open(names='IBM,AAPL,FB', fields='sl1c1v'):\n    print(line.decode('utf-8'))\nThe class could be replaced with a much simpler function:\ndef urltemplate(template):\n    def opener(**kwargs):\n        return urlopen(template.format_map(kwargs))\n    return opener\n# Example use\nyahoo = urltemplate('http://finance.yahoo.com/d/quotes.csv?s={names}&f={fields}')\nfor line in yahoo(names='IBM,AAPL,FB', fields='sl1c1v'):\n    print(line.decode('utf-8'))\nDiscussion\nIn many cases, the only reason you might have a single-method class is to store addi‐\ntional state for use in the method. For example, the only purpose of the UrlTemplate\nclass is to hold the template value someplace so that it can be used in the open() method.\nUsing an inner function or closure, as shown in the solution, is often more elegant.\nSimply stated, a closure is just a function, but with an extra environment of the variables\nthat are used inside the function. A key feature of a closure is that it remembers the\nenvironment in which it was defined. Thus, in the solution, the opener() function\nremembers the value of the template argument, and uses it in subsequent calls.\n7.9. Replacing Single Method Classes with Functions \n| \n231",
      "content_length": 1975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": 6,
      "content": "Whenever you’re writing code and you encounter the problem of attaching additional\nstate to a function, think closures. They are often a more minimal and elegant solution\nthan the alternative of turning your function into a full-fledged class.\n7.10. Carrying Extra State with Callback Functions\nProblem\nYou’re writing code that relies on the use of callback functions (e.g., event handlers,\ncompletion callbacks, etc.), but you want to have the callback function carry extra state\nfor use inside the callback function.\nSolution\nThis recipe pertains to the use of callback functions that are found in many libraries\nand frameworks—especially those related to asynchronous processing. To illustrate and\nfor the purposes of testing, define the following function, which invokes a callback:\ndef apply_async(func, args, *, callback):\n    # Compute the result\n    result = func(*args)\n    # Invoke the callback with the result\n    callback(result)\nIn reality, such code might do all sorts of advanced processing involving threads, pro‐\ncesses, and timers, but that’s not the main focus here. Instead, we’re simply focused on\nthe invocation of the callback. Here’s an example that shows how the preceding code\ngets used:\n>>> def print_result(result):\n...     print('Got:', result)\n...\n>>> def add(x, y):\n...     return x + y\n...\n>>> apply_async(add, (2, 3), callback=print_result)\nGot: 5\n>>> apply_async(add, ('hello', 'world'), callback=print_result)\nGot: helloworld\n>>>\nAs you will notice, the print_result() function only accepts a single argument, which\nis the result. No other information is passed in. This lack of information can sometimes\npresent problems when you want the callback to interact with other variables or parts\nof the environment.\n232 \n| \nChapter 7: Functions",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": 6,
      "content": "One way to carry extra information in a callback is to use a bound-method instead of\na simple function. For example, this class keeps an internal sequence number that is\nincremented every time a result is received:\nclass ResultHandler:\n    def __init__(self):\n        self.sequence = 0\n    def handler(self, result):\n        self.sequence += 1\n        print('[{}] Got: {}'.format(self.sequence, result))\nTo use this class, you would create an instance and use the bound method handler as\nthe callback:\n>>> r = ResultHandler()\n>>> apply_async(add, (2, 3), callback=r.handler)\n[1] Got: 5\n>>> apply_async(add, ('hello', 'world'), callback=r.handler)\n[2] Got: helloworld\n>>>\nAs an alternative to a class, you can also use a closure to capture state. For example:\ndef make_handler():\n    sequence = 0\n    def handler(result):\n        nonlocal sequence\n        sequence += 1\n        print('[{}] Got: {}'.format(sequence, result))\n    return handler\nHere is an example of this variant:\n>>> handler = make_handler()\n>>> apply_async(add, (2, 3), callback=handler)\n[1] Got: 5\n>>> apply_async(add, ('hello', 'world'), callback=handler)\n[2] Got: helloworld\n>>>\nAs yet another variation on this theme, you can sometimes use a coroutine to accomplish\nthe same thing:\ndef make_handler():\n    sequence = 0\n    while True:\n        result = yield\n        sequence += 1\n        print('[{}] Got: {}'.format(sequence, result))\nFor a coroutine, you would use its send() method as the callback, like this:\n>>> handler = make_handler()\n>>> next(handler)        # Advance to the yield\n7.10. Carrying Extra State with Callback Functions \n| \n233",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": 6,
      "content": ">>> apply_async(add, (2, 3), callback=handler.send)\n[1] Got: 5\n>>> apply_async(add, ('hello', 'world'), callback=handler.send)\n[2] Got: helloworld\n>>>\nLast, but not least, you can also carry state into a callback using an extra argument and\npartial function application. For example:\n>>> class SequenceNo:\n...     def __init__(self):\n...         self.sequence = 0\n...\n>>> def handler(result, seq):\n...     seq.sequence += 1\n...     print('[{}] Got: {}'.format(seq.sequence, result))\n...\n>>> seq = SequenceNo()\n>>> from functools import partial\n>>> apply_async(add, (2, 3), callback=partial(handler, seq=seq))\n[1] Got: 5\n>>> apply_async(add, ('hello', 'world'), callback=partial(handler, seq=seq))\n[2] Got: helloworld\n>>>\nDiscussion\nSoftware based on callback functions often runs the risk of turning into a huge tangled\nmess. Part of the issue is that the callback function is often disconnected from the code\nthat made the initial request leading to callback execution. Thus, the execution envi‐\nronment between making the request and handling the result is effectively lost. If you\nwant the callback function to continue with a procedure involving multiple steps, you\nhave to figure out how to save and restore the associated state.\nThere are really two main approaches that are useful for capturing and carrying state.\nYou can carry it around on an instance (attached to a bound method perhaps) or you\ncan carry it around in a closure (an inner function). Of the two techniques, closures are\nperhaps a bit more lightweight and natural in that they are simply built from functions.\nThey also automatically capture all of the variables being used. Thus, it frees you from\nhaving to worry about the exact state needs to be stored (it’s determined automatically\nfrom your code).\nIf using closures, you need to pay careful attention to mutable variables. In the solution,\nthe nonlocal declaration is used to indicate that the sequence variable is being modified\nfrom within the callback. Without this declaration, you’ll get an error. \nThe use of a coroutine as a callback handler is interesting in that it is closely related to\nthe closure approach. In some sense, it’s even cleaner, since there is just a single function.\nMoreover, variables can be freely modified without worrying about nonlocal declara‐\n234 \n| \nChapter 7: Functions",
      "content_length": 2334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": 6,
      "content": "tions. The potential downside is that coroutines don’t tend to be as well understood as\nother parts of Python. There are also a few tricky bits such as the need to call next() on\na coroutine prior to using it. That’s something that could be easy to forget in practice.\nNevertheless, coroutines have other potential uses here, such as the definition of an\ninlined callback (covered in the next recipe).\nThe last technique involving partial() is useful if all you need to do is pass extra values\ninto a callback. Instead of using partial(), you’ll sometimes see the same thing ac‐\ncomplished with the use of a lambda:\n>>> apply_async(add, (2, 3), callback=lambda r: handler(r, seq))\n[1] Got: 5\n>>>\nFor more examples, see Recipe 7.8, which shows how to use partial() to change ar‐\ngument signatures.\n7.11. Inlining Callback Functions\nProblem\nYou’re writing code that uses callback functions, but you’re concerned about the pro‐\nliferation of small functions and mind boggling control flow. You would like some way\nto make the code look more like a normal sequence of procedural steps.\nSolution\nCallback functions can be inlined into a function using generators and coroutines. To\nillustrate, suppose you have a function that performs work and invokes a callback as\nfollows (see Recipe 7.10):\ndef apply_async(func, args, *, callback):\n    # Compute the result\n    result = func(*args)\n    # Invoke the callback with the result\n    callback(result)\nNow take a look at the following supporting code, which involves an Async class and\nan inlined_async decorator:\nfrom queue import Queue\nfrom functools import wraps\nclass Async:\n    def __init__(self, func, args):\n        self.func = func\n        self.args = args\n7.11. Inlining Callback Functions \n| \n235",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": 6,
      "content": "def inlined_async(func):\n    @wraps(func)\n    def wrapper(*args):\n        f = func(*args)\n        result_queue = Queue()\n        result_queue.put(None)\n        while True:\n            result = result_queue.get()\n            try:\n                a = f.send(result)\n                apply_async(a.func, a.args, callback=result_queue.put)\n            except StopIteration:\n                break\n    return wrapper\nThese two fragments of code will allow you to inline the callback steps using yield\nstatements. For example:\ndef add(x, y):\n    return x + y\n@inlined_async\ndef test():\n    r = yield Async(add, (2, 3))\n    print(r)\n    r = yield Async(add, ('hello', 'world'))\n    print(r)\n    for n in range(10):\n        r = yield Async(add, (n, n))\n        print(r)\n    print('Goodbye')\nIf you call test(), you’ll get output like this:\n5\nhelloworld\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\nGoodbye\nAside from the special decorator and use of yield, you will notice that no callback\nfunctions appear anywhere (except behind the scenes).\n236 \n| \nChapter 7: Functions",
      "content_length": 1042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": 6,
      "content": "Discussion\nThis recipe will really test your knowledge of callback functions, generators, and control\nflow.\nFirst, in code involving callbacks, the whole point is that the current calculation will\nsuspend and resume at some later point in time (e.g., asynchronously). When the cal‐\nculation resumes, the callback will get executed to continue the processing. The ap\nply_async() function illustrates the essential parts of executing the callback, although\nin reality it might be much more complicated (involving threads, processes, event han‐\ndlers, etc.).\nThe idea that a calculation will suspend and resume naturally maps to the execution\nmodel of a generator function. Specifically, the yield operation makes a generator\nfunction emit a value and suspend. Subsequent calls to the __next__() or send()\nmethod of a generator will make it start again.\nWith this in mind, the core of this recipe is found in the inline_async() decorator\nfunction. The key idea is that the decorator will step the generator function through all\nof its yield statements, one at a time. To do this, a result queue is created and initially\npopulated with a value of None. A loop is then initiated in which a result is popped off\nthe queue and sent into the generator. This advances to the next yield, at which point\nan instance of Async is received. The loop then looks at the function and arguments,\nand initiates the asynchronous calculation apply_async(). However, the sneakiest part\nof this calculation is that instead of using a normal callback function, the callback is set\nto the queue put() method.\nAt this point, it is left somewhat open as to precisely what happens. The main loop\nimmediately goes back to the top and simply executes a get() operation on the queue.\nIf data is present, it must be the result placed there by the put() callback. If nothing is\nthere, the operation blocks, waiting for a result to arrive at some future time. How that\nmight happen depends on the precise implementation of the apply_async() function.\nIf you’re doubtful that anything this crazy would work, you can try it with the multi‐\nprocessing library and have async operations executed in separate processes:\nif __name__ == '__main__':\n    import multiprocessing\n    pool = multiprocessing.Pool()\n    apply_async = pool.apply_async\n    # Run the test function\n    test()\nIndeed, you’ll find that it works, but unraveling the control flow might require more\ncoffee.\n7.11. Inlining Callback Functions \n| \n237",
      "content_length": 2478,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": 6,
      "content": "Hiding tricky control flow behind generator functions is found elsewhere in the stan‐\ndard library and third-party packages. For example, the @contextmanager decorator in\nthe contextlib performs a similar mind-bending trick that glues the entry and exit\nfrom a context manager together across a yield statement. The popular Twisted pack‐\nage has inlined callbacks that are also similar.\n7.12. Accessing Variables Defined Inside a Closure\nProblem\nYou would like to extend a closure with functions that allow the inner variables to be\naccessed and modified.\nSolution\nNormally, the inner variables of a closure are completely hidden to the outside world.\nHowever, you can provide access by writing accessor functions and attaching them to\nthe closure as function attributes. For example:\ndef sample():\n    n = 0\n    # Closure function\n    def func():\n        print('n=', n)\n    # Accessor methods for n\n    def get_n():\n        return n\n    def set_n(value):\n        nonlocal n\n        n = value\n    # Attach as function attributes\n    func.get_n = get_n\n    func.set_n = set_n\n    return func\nHere is an example of using this code:\n>>> f = sample()\n>>> f()\nn= 0\n>>> f.set_n(10)\n>>> f()\nn= 10\n>>> f.get_n()\n10\n>>>\n238 \n| \nChapter 7: Functions",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": 6,
      "content": "Discussion\nThere are two main features that make this recipe work. First, nonlocal declarations\nmake it possible to write functions that change inner variables. Second, function at‐\ntributes allow the accessor methods to be attached to the closure function in a straight‐\nforward manner where they work a lot like instance methods (even though no class is\ninvolved).\nA slight extension to this recipe can be made to have closures emulate instances of a\nclass. All you need to do is copy the inner functions over to the dictionary of an instance\nand return it. For example:\nimport sys\nclass ClosureInstance:\n    def __init__(self, locals=None):\n        if locals is None:\n            locals = sys._getframe(1).f_locals\n        # Update instance dictionary with callables\n        self.__dict__.update((key,value) for key, value in locals.items()\n                             if callable(value) )\n    # Redirect special methods\n    def __len__(self):\n        return self.__dict__['__len__']()\n# Example use\ndef Stack():\n    items = []\n    def push(item):\n        items.append(item)\n    def pop():\n        return items.pop()\n    def __len__():\n        return len(items)\n    return ClosureInstance()\nHere’s an interactive session to show that it actually works:\n>>> s = Stack()\n>>> s\n<__main__.ClosureInstance object at 0x10069ed10>\n>>> s.push(10)\n>>> s.push(20)\n>>> s.push('Hello')\n>>> len(s)\n3\n>>> s.pop()\n'Hello'\n7.12. Accessing Variables Defined Inside a Closure \n| \n239",
      "content_length": 1469,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": 6,
      "content": ">>> s.pop()\n20\n>>> s.pop()\n10\n>>>\nInterestingly, this code runs a bit faster than using a normal class definition. For example,\nyou might be inclined to test the performance against a class like this:\nclass Stack2:\n    def __init__(self):\n        self.items = []\n    def push(self, item):\n        self.items.append(item)\n    def pop(self):\n        return self.items.pop()\n    def __len__(self):\n        return len(self.items)\nIf you do, you’ll get results similar to the following:\n>>> from timeit import timeit\n>>> # Test involving closures\n>>> s = Stack()\n>>> timeit('s.push(1);s.pop()', 'from __main__ import s')\n0.9874754269840196\n>>> # Test involving a class\n>>> s = Stack2()\n>>> timeit('s.push(1);s.pop()', 'from __main__ import s')\n1.0707052160287276\n>>>\nAs shown, the closure version runs about 8% faster. Most of that is coming from\nstreamlined access to the instance variables. Closures are faster because there’s no extra\nself variable involved.\nRaymond Hettinger has devised an even more diabolical variant of this idea. However,\nshould you be inclined to do something like this in your code, be aware that it’s still a\nrather weird substitute for a real class. For example, major features such as inheritance,\nproperties, descriptors, or class methods don’t work. You also have to play some tricks\nto get special methods to work (e.g., see the implementation of __len__() in Closur\neInstance).\n240 \n| \nChapter 7: Functions",
      "content_length": 1435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": 6,
      "content": "Lastly, you’ll run the risk of confusing people who read your code and wonder why it\ndoesn’t look anything like a normal class definition (of course, they’ll also wonder why\nit’s faster). Nevertheless, it’s an interesting example of what can be done by providing\naccess to the internals of a closure.\nIn the big picture, adding methods to closures might have more utility in settings where\nyou want to do things like reset the internal state, flush buffers, clear caches, or have\nsome kind of feedback mechanism.\n7.12. Accessing Variables Defined Inside a Closure \n| \n241",
      "content_length": 571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": 6,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 261,
      "chapter": 6,
      "content": "CHAPTER 8\nClasses and Objects\nThe primary focus of this chapter is to present recipes to common programming pat‐\nterns related to class definitions. Topics include making objects support common\nPython features, usage of special methods, encapsulation techniques, inheritance, mem‐\nory management, and useful design patterns.\n8.1. Changing the String Representation of Instances\nProblem\nYou want to change the output produced by printing or viewing instances to something\nmore sensible.\nSolution\nTo change the string representation of an instance, define the __str__() and\n__repr__() methods. For example:\nclass Pair:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return 'Pair({0.x!r}, {0.y!r})'.format(self)\n    def __str__(self):\n        return '({0.x!s}, {0.y!s})'.format(self)\nThe __repr__() method returns the code representation of an instance, and is usually\nthe text you would type to re-create the instance. The built-in repr() function returns\nthis text, as does the interactive interpreter when inspecting values. The __str__()\nmethod converts the instance to a string, and is the output produced by the str() and\nprint() functions. For example:\n243",
      "content_length": 1213,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": 6,
      "content": ">>> p = Pair(3, 4)\n>>> p\nPair(3, 4)         # __repr__() output\n>>> print(p)\n(3, 4)             # __str__() output\n>>>\nThe implementation of this recipe also shows how different string representations may\nbe used during formatting. Specifically, the special !r formatting code indicates that the\noutput of __repr__() should be used instead of __str__(), the default. You can try this\nexperiment with the preceding class to see this:\n>>> p = Pair(3, 4)\n>>> print('p is {0!r}'.format(p))\np is Pair(3, 4)\n>>> print('p is {0}'.format(p))\np is (3, 4)\n>>>\nDiscussion\nDefining __repr__() and __str__() is often good practice, as it can simplify debugging\nand instance output. For example, by merely printing or logging an instance, a pro‐\ngrammer will be shown more useful information about the instance contents.\nIt is standard practice for the output of __repr__() to produce text such that\neval(repr(x)) == x. If this is not possible or desired, then it is common to create a\nuseful textual representation enclosed in < and > instead. For example:\n>>> f = open('file.dat')\n>>> f\n<_io.TextIOWrapper name='file.dat' mode='r' encoding='UTF-8'>\n>>>\nIf no __str__() is defined, the output of __repr__() is used as a fallback.\nThe use of format() in the solution might look a little funny, but the format code {0.x}\nspecifies the x-attribute of argument 0. So, in the following function, the 0 is actually\nthe instance self:\ndef __repr__(self):\n    return 'Pair({0.x!r}, {0.y!r})'.format(self)\nAs an alternative to this implementation, you could also use the % operator and the\nfollowing code:\ndef __repr__(self):\n    return 'Pair(%r, %r)' % (self.x, self.y)\n244 \n| \nChapter 8: Classes and Objects",
      "content_length": 1687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": 6,
      "content": "8.2. Customizing String Formatting\nProblem\nYou want an object to support customized formatting through the format() function\nand string method.\nSolution\nTo customize string formatting, define the __format__() method on a class. For\nexample:\n_formats = {\n    'ymd' : '{d.year}-{d.month}-{d.day}',\n    'mdy' : '{d.month}/{d.day}/{d.year}',\n    'dmy' : '{d.day}/{d.month}/{d.year}'\n    }\nclass Date:\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n    def __format__(self, code):\n        if code == '':\n            code = 'ymd'\n        fmt = _formats[code]\n        return fmt.format(d=self)\nInstances of the Date class now support formatting operations such as the following:\n>>> d = Date(2012, 12, 21)\n>>> format(d)\n'2012-12-21'\n>>> format(d, 'mdy')\n'12/21/2012'\n>>> 'The date is {:ymd}'.format(d)\n'The date is 2012-12-21'\n>>> 'The date is {:mdy}'.format(d)\n'The date is 12/21/2012'\n>>>\nDiscussion\nThe __format__() method provides a hook into Python’s string formatting function‐\nality. It’s important to emphasize that the interpretation of format codes is entirely up\nto the class itself. Thus, the codes can be almost anything at all. For example, consider\nthe following from the datetime module:\n8.2. Customizing String Formatting \n| \n245",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": 6,
      "content": ">>> from datetime import date\n>>> d = date(2012, 12, 21)\n>>> format(d)\n'2012-12-21'\n>>> format(d,'%A, %B %d, %Y')\n'Friday, December 21, 2012'\n>>> 'The end is {:%d %b %Y}. Goodbye'.format(d)\n'The end is 21 Dec 2012. Goodbye'\n>>>\nThere are some standard conventions for the formatting of the built-in types. See the\ndocumentation for the string module for a formal specification.\n8.3. Making Objects Support the Context-Management\nProtocol\nProblem\nYou want to make your objects support the context-management protocol (the with\nstatement).\nSolution\nIn order to make an object compatible with the with statement, you need to implement\n__enter__() and __exit__() methods. For example, consider the following class,\nwhich provides a network connection:\nfrom socket import socket, AF_INET, SOCK_STREAM\nclass LazyConnection:\n    def __init__(self, address, family=AF_INET, type=SOCK_STREAM):\n        self.address = address\n        self.family = AF_INET\n        self.type = SOCK_STREAM\n        self.sock = None\n    def __enter__(self):\n        if self.sock is not None:\n            raise RuntimeError('Already connected')\n        self.sock = socket(self.family, self.type)\n        self.sock.connect(self.address)\n        return self.sock\n    def __exit__(self, exc_ty, exc_val, tb):\n        self.sock.close()\n        self.sock = None\n246 \n| \nChapter 8: Classes and Objects",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": 6,
      "content": "The key feature of this class is that it represents a network connection, but it doesn’t\nactually do anything initially (e.g., it doesn’t establish a connection). Instead, the con‐\nnection is established and closed using the with statement (essentially on demand). For\nexample:\nfrom functools import partial\nconn = LazyConnection(('www.python.org', 80))\n# Connection closed\nwith conn as s:\n    # conn.__enter__() executes: connection open\n    s.send(b'GET /index.html HTTP/1.0\\r\\n')\n    s.send(b'Host: www.python.org\\r\\n')\n    s.send(b'\\r\\n')\n    resp = b''.join(iter(partial(s.recv, 8192), b''))\n    # conn.__exit__() executes: connection closed\nDiscussion\nThe main principle behind writing a context manager is that you’re writing code that’s\nmeant to surround a block of statements as defined by the use of the with statement.\nWhen the with statement is first encountered, the __enter__() method is triggered.\nThe return value of __enter__() (if any) is placed into the variable indicated with the\nas qualifier. Afterward, the statements in the body of the with statement execute. Finally,\nthe __exit__() method is triggered to clean up.\nThis control flow happens regardless of what happens in the body of the with statement,\nincluding if there are exceptions. In fact, the three arguments to the __exit__() method\ncontain the exception type, value, and traceback for pending exceptions (if any). The\n__exit__() method can choose to use the exception information in some way or to\nignore it by doing nothing and returning None as a result. If __exit__() returns True,\nthe exception is cleared as if nothing happened and the program continues executing\nstatements immediately after the with block.\nOne subtle aspect of this recipe is whether or not the LazyConnection class allows nested\nuse of the connection with multiple with statements. As shown, only a single socket\nconnection at a time is allowed, and an exception is raised if a repeated with statement\nis attempted when a socket is already in use. You can work around this limitation with\na slightly different implementation, as shown here:\nfrom socket import socket, AF_INET, SOCK_STREAM\nclass LazyConnection:\n    def __init__(self, address, family=AF_INET, type=SOCK_STREAM):\n        self.address = address\n        self.family = AF_INET\n        self.type = SOCK_STREAM\n        self.connections = []\n8.3. Making Objects Support the Context-Management Protocol \n| \n247",
      "content_length": 2429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": 6,
      "content": "def __enter__(self):\n        sock = socket(self.family, self.type)\n        sock.connect(self.address)\n        self.connections.append(sock)\n        return sock\n    def __exit__(self, exc_ty, exc_val, tb):\n        self.connections.pop().close()\n# Example use\nfrom functools import partial\nconn = LazyConnection(('www.python.org', 80))\nwith conn as s1:\n     ...\n     with conn as s2:\n          ...\n          # s1 and s2 are independent sockets\nIn this second version, the LazyConnection class serves as a kind of factory for con‐\nnections. Internally, a list is used to keep a stack. Whenever __enter__() executes, it\nmakes a new connection and adds it to the stack. The __exit__() method simply pops\nthe last connection off the stack and closes it. It’s subtle, but this allows multiple con‐\nnections to be created at once with nested with statements, as shown.\nContext managers are most commonly used in programs that need to manage resources\nsuch as files, network connections, and locks. A key part of such resources is they have\nto be explicitly closed or released to operate correctly. For instance, if you acquire a lock,\nthen you have to make sure you release it, or else you risk deadlock. By implementing\n__enter__(), __exit__(), and using the with statement, it is much easier to avoid such\nproblems, since the cleanup code in the __exit__() method is guaranteed to run no\nmatter what.\nAn alternative formulation of context managers is found in the contextmanager mod‐\nule. See Recipe 9.22. A thread-safe version of this recipe can be found in Recipe 12.6.\n8.4. Saving Memory When Creating a Large Number of\nInstances\nProblem\nYour program creates a large number (e.g., millions) of instances and uses a large\namount of memory.\n248 \n| \nChapter 8: Classes and Objects",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": 6,
      "content": "Solution\nFor classes that primarily serve as simple data structures, you can often greatly reduce\nthe memory footprint of instances by adding the __slots__ attribute to the class defi‐\nnition. For example:\nclass Date:\n    __slots__ = ['year', 'month', 'day']\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\nWhen you define __slots__, Python uses a much more compact internal representation\nfor instances. Instead of each instance consisting of a dictionary, instances are built\naround a small fixed-sized array, much like a tuple or list. Attribute names listed in the\n__slots__ specifier are internally mapped to specific indices within this array. A side\neffect of using slots is that it is no longer possible to add new attributes to instances—\nyou are restricted to only those attribute names listed in the __slots__ specifier.\nDiscussion\nThe memory saved by using slots varies according to the number and type of attributes\nstored. However, in general, the resulting memory use is comparable to that of storing\ndata in a tuple. To give you an idea, storing a single Date instance without slots requires\n428 bytes of memory on a 64-bit version of Python. If slots is defined, it drops to 156\nbytes. In a program that manipulated a large number of dates all at once, this would\nmake a significant reduction in overall memory use.\nAlthough slots may seem like a feature that could be generally useful, you should resist\nthe urge to use it in most code. There are many parts of Python that rely on the normal\ndictionary-based implementation. In addition, classes that define slots don’t support\ncertain features such as multiple inheritance. For the most part, you should only use\nslots on classes that are going to serve as frequently used data structures in your program\n(e.g., if your program created millions of instances of a particular class).\nA common misperception of __slots__ is that it is an encapsulation tool that prevents\nusers from adding new attributes to instances. Although this is a side effect of using\nslots, this was never the original purpose. Instead, __slots__ was always intended to\nbe an optimization tool.\n8.4. Saving Memory When Creating a Large Number of Instances \n| \n249",
      "content_length": 2272,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": 6,
      "content": "8.5. Encapsulating Names in a Class\nProblem\nYou want to encapsulate “private” data on instances of a class, but are concerned about\nPython’s lack of access control.\nSolution\nRather than relying on language features to encapsulate data, Python programmers are\nexpected to observe certain naming conventions concerning the intended usage of data\nand methods. The first convention is that any name that starts with a single leading\nunderscore (_) should always be assumed to be internal implementation. For example:\nclass A:\n    def __init__(self):\n        self._internal = 0    # An internal attribute\n        self.public = 1       # A public attribute\n    def public_method(self):\n        '''\n        A public method\n        '''\n        ...\n    def _internal_method(self):\n        ...\nPython doesn’t actually prevent someone from accessing internal names. However, do‐\ning so is considered impolite, and may result in fragile code. It should be noted, too,\nthat the use of the leading underscore is also used for module names and module-level\nfunctions. For example, if you ever see a module name that starts with a leading un‐\nderscore (e.g., _socket), it’s internal implementation. Likewise, module-level functions\nsuch as sys._getframe() should only be used with great caution.\nYou may also encounter the use of two leading underscores (__) on names within class\ndefinitions. For example:\nclass B:\n    def __init__(self):\n        self.__private = 0\n    def __private_method(self):\n        ...\n    def public_method(self):\n        ...\n        self.__private_method()\n        ...\n250 \n| \nChapter 8: Classes and Objects",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": 6,
      "content": "The use of double leading underscores causes the name to be mangled to something\nelse. Specifically, the private attributes in the preceding class get renamed to _B__pri\nvate and _B__private_method, respectively. At this point, you might ask what purpose\nsuch name mangling serves. The answer is inheritance—such attributes cannot be\noverridden via inheritance. For example:\nclass C(B):\n    def __init__(self):\n        super().__init__()\n        self.__private = 1      # Does not override B.__private\n    # Does not override B.__private_method()\n    def __private_method(self):\n        ...\nHere, the private names __private and __private_method get renamed to _C__pri\nvate and _C__private_method, which are different than the mangled names in the base\nclass B.\nDiscussion\nThe fact that there are two different conventions (single underscore versus double un‐\nderscore) for “private” attributes leads to the obvious question of which style you should\nuse. For most code, you should probably just make your nonpublic names start with a\nsingle underscore. If, however, you know that your code will involve subclassing, and\nthere are internal attributes that should be hidden from subclasses, use the double un‐\nderscore instead.\nIt should also be noted that sometimes you may want to define a variable that clashes\nwith the name of a reserved word. For this, you should use a single trailing underscore.\nFor example:\nlambda_ = 2.0     # Trailing _ to avoid clash with lambda keyword\nThe reason for not using a leading underscore here is that it avoids confusion about the\nintended usage (i.e., the use of a leading underscore could be interpreted as a way to\navoid a name collision rather than as an indication that the value is private). Using a\nsingle trailing underscore solves this problem.\n8.6. Creating Managed Attributes\nProblem\nYou want to add extra processing (e.g., type checking or validation) to the getting or\nsetting of an instance attribute.\n8.6. Creating Managed Attributes \n| \n251",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": 6,
      "content": "Solution\nA simple way to customize access to an attribute is to define it as a “property.” For\nexample, this code defines a property that adds simple type checking to an attribute:\nclass Person:\n    def __init__(self, first_name):\n        self.first_name = first_name\n    # Getter function\n    @property\n    def first_name(self):\n        return self._first_name\n    # Setter function\n    @first_name.setter\n    def first_name(self, value):\n        if not isinstance(value, str):\n            raise TypeError('Expected a string')\n        self._first_name = value\n    # Deleter function (optional)\n    @first_name.deleter\n    def first_name(self):\n        raise AttributeError(\"Can't delete attribute\")\nIn the preceding code, there are three related methods, all of which must have the same\nname. The first method is a getter function, and establishes first_name as being a\nproperty. The other two methods attach optional setter and deleter functions to the\nfirst_name property. It’s important to stress that the @first_name.setter and\n@first_name.deleter decorators won’t be defined unless first_name was already es‐\ntablished as a property using @property.\nA critical feature of a property is that it looks like a normal attribute, but access auto‐\nmatically triggers the getter, setter, and deleter methods. For example:\n>>> a = Person('Guido')\n>>> a.first_name       # Calls the getter\n'Guido'\n>>> a.first_name = 42  # Calls the setter\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"prop.py\", line 14, in first_name\n    raise TypeError('Expected a string')\nTypeError: Expected a string\n>>> del a.first_name\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: can't delete attribute\n>>>\n252 \n| \nChapter 8: Classes and Objects",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": 6,
      "content": "When implementing a property, the underlying data (if any) still needs to be stored\nsomewhere. Thus, in the get and set methods, you see direct manipulation of a\n_first_name attribute, which is where the actual data lives. In addition, you may ask\nwhy the __init__() method sets self.first_name instead of self._first_name. In\nthis example, the entire point of the property is to apply type checking when setting an\nattribute. Thus, chances are you would also want such checking to take place during\ninitialization. By setting self.first_name, the set operation uses the setter method (as\nopposed to bypassing it by accessing self._first_name).\nProperties can also be defined for existing get and set methods. For example:\nclass Person:\n    def __init__(self, first_name):\n        self.set_first_name(first_name)\n    # Getter function\n    def get_first_name(self):\n        return self._first_name\n    # Setter function\n    def set_first_name(self, value):\n        if not isinstance(value, str):\n            raise TypeError('Expected a string')\n        self._first_name = value\n    # Deleter function (optional)\n    def del_first_name(self):\n        raise AttributeError(\"Can't delete attribute\")\n    # Make a property from existing get/set methods\n    name = property(get_first_name, set_first_name, del_first_name)\nDiscussion\nA property attribute is actually a collection of methods bundled together. If you inspect\na class with a property, you can find the raw methods in the fget, fset, and fdel\nattributes of the property itself. For example:\n>>> Person.first_name.fget\n<function Person.first_name at 0x1006a60e0>\n>>> Person.first_name.fset\n<function Person.first_name at 0x1006a6170>\n>>> Person.first_name.fdel\n<function Person.first_name at 0x1006a62e0>\n>>>\nNormally, you wouldn’t call fget or fset directly, but they are triggered automatically\nwhen the property is accessed.\n8.6. Creating Managed Attributes \n| \n253",
      "content_length": 1923,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": 6,
      "content": "Properties should only be used in cases where you actually need to perform extra pro‐\ncessing on attribute access. Sometimes programmers coming from languages such as\nJava feel that all access should be handled by getters and setters, and that they should\nwrite code like this:\nclass Person:\n    def __init__(self, first_name):\n        self.first_name = name\n    @property\n    def first_name(self):\n        return self._first_name\n    @first_name.setter\n    def first_name(self, value):\n        self._first_name = value\nDon’t write properties that don’t actually add anything extra like this. For one, it makes\nyour code more verbose and confusing to others. Second, it will make your program\nrun a lot slower. Lastly, it offers no real design benefit. Specifically, if you later decide\nthat extra processing needs to be added to the handling of an ordinary attribute, you\ncould promote it to a property without changing existing code. This is because the syntax\nof code that accessed the attribute would remain unchanged.\nProperties can also be a way to define computed attributes. These are attributes that are\nnot actually stored, but computed on demand. For example:\nimport math\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n    @property\n    def area(self):\n        return math.pi * self.radius ** 2\n    @property\n    def perimeter(self):\n        return 2 * math.pi * self.radius\nHere, the use of properties results in a very uniform instance interface in that radius,\narea, and perimeter are all accessed as simple attributes, as opposed to a mix of simple\nattributes and method calls. For example:\n>>> c = Circle(4.0)\n>>> c.radius\n4.0\n>>> c.area           # Notice lack of ()\n50.26548245743669\n>>> c.perimeter      # Notice lack of ()\n25.132741228718345\n>>>\n254 \n| \nChapter 8: Classes and Objects",
      "content_length": 1832,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": 6,
      "content": "Although properties give you an elegant programming interface, sometimes you ac‐\ntually may want to directly use getter and setter functions. For example:\n>>> p = Person('Guido')\n>>> p.get_first_name()\n'Guido'\n>>> p.set_first_name('Larry')\n>>>\nThis often arises in situations where Python code is being integrated into a larger in‐\nfrastructure of systems or programs. For example, perhaps a Python class is going to be\nplugged into a large distributed system based on remote procedure calls or distributed\nobjects. In such a setting, it may be much easier to work with an explicit get/set method\n(as a normal method call) rather than a property that implicitly makes such calls.\nLast, but not least, don’t write Python code that features a lot of repetitive property\ndefinitions. For example:\nclass Person:\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\n    @property\n    def first_name(self):\n        return self._first_name\n    @first_name.setter\n    def first_name(self, value):\n        if not isinstance(value, str):\n            raise TypeError('Expected a string')\n        self._first_name = value\n    # Repeated property code, but for a different name (bad!)\n    @property\n    def last_name(self):\n        return self._last_name\n    @last_name.setter\n    def last_name(self, value):\n        if not isinstance(value, str):\n            raise TypeError('Expected a string')\n        self._last_name = value\nCode repetition leads to bloated, error prone, and ugly code. As it turns out, there are\nmuch better ways to achieve the same thing using descriptors or closures. See Recipes\n8.9 and 9.21.\n8.6. Creating Managed Attributes \n| \n255",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": 6,
      "content": "8.7. Calling a Method on a Parent Class\nProblem\nYou want to invoke a method in a parent class in place of a method that has been\noverridden in a subclass.\nSolution\nTo call a method in a parent (or superclass), use the super() function. For example:\nclass A:\n    def spam(self):\n        print('A.spam')\nclass B(A):\n    def spam(self):\n        print('B.spam')\n        super().spam()      # Call parent spam()\nA very common use of super() is in the handling of the __init__() method to make\nsure that parents are properly initialized:\nclass A:\n    def __init__(self):\n        self.x = 0\nclass B(A):\n    def __init__(self):\n        super().__init__()\n        self.y = 1\nAnother common use of super() is in code that overrides any of Python’s special meth‐\nods. For example:\nclass Proxy:\n    def __init__(self, obj):\n        self._obj = obj\n    # Delegate attribute lookup to internal obj\n    def __getattr__(self, name):\n        return getattr(self._obj, name)\n    # Delegate attribute assignment\n    def __setattr__(self, name, value):\n        if name.startswith('_'):\n            super().__setattr__(name, value)    # Call original __setattr__\n        else:\n            setattr(self._obj, name, value)\n256 \n| \nChapter 8: Classes and Objects",
      "content_length": 1238,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": 6,
      "content": "In this code, the implementation of __setattr__() includes a name check. If the\nname starts with an underscore (_), it invokes the original implementation of\n__setattr__() using super(). Otherwise, it delegates to the internally held object\nself._obj. It looks a little funny, but super() works even though there is no explicit\nbase class listed.\nDiscussion\nCorrect use of the super() function is actually one of the most poorly understood\naspects of Python. Occasionally, you will see code written that directly calls a method\nin a parent like this:\nclass Base:\n    def __init__(self):\n        print('Base.__init__')\nclass A(Base):\n    def __init__(self):\n        Base.__init__(self)\n        print('A.__init__')\nAlthough this “works” for most code, it can lead to bizarre trouble in advanced code\ninvolving multiple inheritance. For example, consider the following:\nclass Base:\n    def __init__(self):\n        print('Base.__init__')\nclass A(Base):\n    def __init__(self):\n        Base.__init__(self)\n        print('A.__init__')\nclass B(Base):\n    def __init__(self):\n        Base.__init__(self)\n        print('B.__init__')\nclass C(A,B):\n    def __init__(self):\n        A.__init__(self)\n        B.__init__(self)\n        print('C.__init__')\nIf you run this code, you’ll see that the Base.__init__() method gets invoked twice, as\nshown here:\n>>> c = C()\nBase.__init__\nA.__init__\nBase.__init__\n8.7. Calling a Method on a Parent Class \n| \n257",
      "content_length": 1438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": 6,
      "content": "B.__init__\nC.__init__\n>>>\nPerhaps double-invocation of Base.__init__() is harmless, but perhaps not. If, on the\nother hand, you change the code to use super(), it all works:\nclass Base:\n    def __init__(self):\n        print('Base.__init__')\nclass A(Base):\n    def __init__(self):\n        super().__init__()\n        print('A.__init__')\nclass B(Base):\n    def __init__(self):\n        super().__init__()\n        print('B.__init__')\nclass C(A,B):\n    def __init__(self):\n        super().__init__()     # Only one call to super() here\n        print('C.__init__')\nWhen you use this new version, you’ll find that each __init__() method only gets\ncalled once:\n>>> c = C()\nBase.__init__\nB.__init__\nA.__init__\nC.__init__\n>>>\nTo understand why it works, we need to step back for a minute and discuss how Python\nimplements inheritance. For every class that you define, Python computes what’s known\nas a method resolution order (MRO) list. The MRO list is simply a linear ordering of\nall the base classes. For example:\n>>> C.__mro__\n(<class '__main__.C'>, <class '__main__.A'>, <class '__main__.B'>,\n<class '__main__.Base'>, <class 'object'>)\n>>>\nTo implement inheritance, Python starts with the leftmost class and works its way left-\nto-right through classes on the MRO list until it finds the first attribute match.\nThe actual determination of the MRO list itself is made using a technique known as C3\nLinearization. Without getting too bogged down in the mathematics of it, it is actually\na merge sort of the MROs from the parent classes subject to three constraints:\n258 \n| \nChapter 8: Classes and Objects",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": 6,
      "content": "• Child classes get checked before parents\n• Multiple parents get checked in the order listed.\n• If there are two valid choices for the next class, pick the one from the first parent.\nHonestly, all you really need to know is that the order of classes in the MRO list “makes\nsense” for almost any class hierarchy you are going to define.\nWhen you use the super() function, Python continues its search starting with the next\nclass on the MRO. As long as every redefined method consistently uses super() and\nonly calls it once, control will ultimately work its way through the entire MRO list and\neach method will only be called once. This is why you don’t get double calls to\nBase.__init__() in the second example.\nA somewhat surprising aspect of super() is that it doesn’t necessarily go to the direct\nparent of a class next in the MRO and that you can even use it in a class with no direct\nparent at all. For example, consider this class:\nclass A:\n    def spam(self):\n        print('A.spam')\n        super().spam()\nIf you try to use this class, you’ll find that it’s completely broken:\n>>> a = A()\n>>> a.spam()\nA.spam\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 4, in spam\nAttributeError: 'super' object has no attribute 'spam'\n>>>\nYet, watch what happens if you start using the class with multiple inheritance:\n>>> class B:\n...     def spam(self):\n...         print('B.spam')\n...\n>>> class C(A,B):\n...     pass\n...\n>>> c = C()\n>>> c.spam()\nA.spam\nB.spam\n>>>\n8.7. Calling a Method on a Parent Class \n| \n259",
      "content_length": 1558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": 6,
      "content": "Here you see that the use of super().spam() in class A has, in fact, called the spam()\nmethod in class B—a class that is completely unrelated to A! This is all explained by the\nMRO of class C:\n>>> C.__mro__\n(<class '__main__.C'>, <class '__main__.A'>, <class '__main__.B'>,\n<class 'object'>)\n>>>\nUsing super() in this manner is most common when defining mixin classes. See Recipes\n8.13 and 8.18.\nHowever, because super() might invoke a method that you’re not expecting, there are\na few general rules of thumb you should try to follow. First, make sure that all methods\nwith the same name in an inheritance hierarchy have a compatible calling signature (i.e.,\nsame number of arguments, argument names). This ensures that super() won’t get\ntripped up if it tries to invoke a method on a class that’s not a direct parent. Second, it’s\nusually a good idea to make sure that the topmost class provides an implementation of\nthe method so that the chain of lookups that occur along the MRO get terminated by\nan actual method of some sort.\nUse of super() is sometimes a source of debate in the Python community. However, all\nthings being equal, you should probably use it in modern code. Raymond Hettinger has\nwritten an excellent blog post “Python’s super() Considered Super!” that has even more\nexamples and reasons why super() might be super-awesome.\n8.8. Extending a Property in a Subclass\nProblem\nWithin a subclass, you want to extend the functionality of a property defined in a parent\nclass.\nSolution\nConsider the following code, which defines a property:\nclass Person:\n    def __init__(self, name):\n        self.name = name\n    # Getter function\n    @property\n    def name(self):\n        return self._name\n    # Setter function\n    @name.setter\n260 \n| \nChapter 8: Classes and Objects",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": 6,
      "content": "def name(self, value):\n        if not isinstance(value, str):\n            raise TypeError('Expected a string')\n        self._name = value\n    # Deleter function\n    @name.deleter\n    def name(self):\n        raise AttributeError(\"Can't delete attribute\")\nHere is an example of a class that inherits from Person and extends the name property\nwith new functionality:\nclass SubPerson(Person):\n    @property\n    def name(self):\n        print('Getting name')\n        return super().name\n    @name.setter\n    def name(self, value):\n        print('Setting name to', value)\n        super(SubPerson, SubPerson).name.__set__(self, value)\n    @name.deleter\n    def name(self):\n        print('Deleting name')\n        super(SubPerson, SubPerson).name.__delete__(self)\nHere is an example of the new class in use:\n>>> s = SubPerson('Guido')\nSetting name to Guido\n>>> s.name\nGetting name\n'Guido'\n>>> s.name = 'Larry'\nSetting name to Larry\n>>> s.name = 42\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example.py\", line 16, in name\n       raise TypeError('Expected a string')\nTypeError: Expected a string\n>>>\nIf you only want to extend one of the methods of a property, use code such as the\nfollowing:\nclass SubPerson(Person):\n    @Person.name.getter\n    def name(self):\n        print('Getting name')\n        return super().name\n8.8. Extending a Property in a Subclass \n| \n261",
      "content_length": 1393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": 6,
      "content": "Or, alternatively, for just the setter, use this code:\nclass SubPerson(Person):\n    @Person.name.setter\n    def name(self, value):\n        print('Setting name to', value)\n        super(SubPerson, SubPerson).name.__set__(self, value)\nDiscussion\nExtending a property in a subclass introduces a number of very subtle problems related\nto the fact that a property is defined as a collection of getter, setter, and deleter methods,\nas opposed to just a single method. Thus, when extending a property, you need to figure\nout if you will redefine all of the methods together or just one of the methods.\nIn the first example, all of the property methods are redefined together. Within each\nmethod, super() is used to call the previous implementation. The use of super(Sub\nPerson, SubPerson).name.__set__(self, value) in the setter function is no mis‐\ntake. To delegate to the previous implementation of the setter, control needs to pass\nthrough the __set__() method of the previously defined name property. However, the\nonly way to get to this method is to access it as a class variable instead of an instance\nvariable. This is what happens with the super(SubPerson, SubPerson) operation.\nIf you only want to redefine one of the methods, it’s not enough to use @property by\nitself. For example, code like this doesn’t work:\nclass SubPerson(Person):\n    @property              # Doesn't work\n    def name(self):\n        print('Getting name')\n        return super().name\nIf you try the resulting code, you’ll find that the setter function disappears entirely:\n>>> s = SubPerson('Guido')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example.py\", line 5, in __init__\n    self.name = name\nAttributeError: can't set attribute\n>>>\nInstead, you should change the code to that shown in the solution:\nclass SubPerson(Person):\n    @Person.getter\n    def name(self):\n        print('Getting name')\n        return super().name\n262 \n| \nChapter 8: Classes and Objects",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": 7,
      "content": "When you do this, all of the previously defined methods of the property are copied, and\nthe getter function is replaced. It now works as expected:\n>>> s = SubPerson('Guido')\n>>> s.name\nGetting name\n'Guido'\n>>> s.name = 'Larry'\n>>> s.name\nGetting name\n'Larry'\n>>> s.name = 42\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example.py\", line 16, in name\n    raise TypeError('Expected a string')\nTypeError: Expected a string\n>>>\nIn this particular solution, there is no way to replace the hardcoded class name Per\nson with something more generic. If you don’t know which base class defined a property,\nyou should use the solution where all of the property methods are redefined and su\nper() is used to pass control to the previous implementation.\nIt’s worth noting that the first technique shown in this recipe can also be used to extend\na descriptor, as described in Recipe 8.9. For example:\n# A descriptor\nclass String:\n    def __init__(self, name):\n        self.name = name\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        return instance.__dict__[self.name]\n    def __set__(self, instance, value):\n        if not isinstance(value, str):\n            raise TypeError('Expected a string')\n        instance.__dict__[self.name] = value\n# A class with a descriptor\nclass Person:\n    name = String('name')\n    def __init__(self, name):\n        self.name = name\n# Extending a descriptor with a property\nclass SubPerson(Person):\n    @property\n    def name(self):\n8.8. Extending a Property in a Subclass \n| \n263",
      "content_length": 1586,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": 7,
      "content": "print('Getting name')\n        return super().name\n    @name.setter\n    def name(self, value):\n        print('Setting name to', value)\n        super(SubPerson, SubPerson).name.__set__(self, value)\n    @name.deleter\n    def name(self):\n        print('Deleting name')\n        super(SubPerson, SubPerson).name.__delete__(self)\nFinally, it’s worth noting that by the time you read this, subclassing of setter and deleter\nmethods might be somewhat simplified. The solution shown will still work, but the bug\nreported at Python’s issues page might resolve into a cleaner approach in a future Python\nversion.\n8.9. Creating a New Kind of Class or Instance Attribute\nProblem\nYou want to create a new kind of instance attribute type with some extra functionality,\nsuch as type checking.\nSolution\nIf you want to create an entirely new kind of instance attribute, define its functionality\nin the form of a descriptor class. Here is an example:\n# Descriptor attribute for an integer type-checked attribute\nclass Integer:\n    def __init__(self, name):\n        self.name = name\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            return instance.__dict__[self.name]\n    def __set__(self, instance, value):\n        if not isinstance(value, int):\n            raise TypeError('Expected an int')\n        instance.__dict__[self.name] = value\n    def __delete__(self, instance):\n        del instance.__dict__[self.name]\n264 \n| \nChapter 8: Classes and Objects",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": 7,
      "content": "A descriptor is a class that implements the three core attribute access operations (get,\nset, and delete) in the form of __get__(), __set__(), and __delete__() special meth‐\nods. These methods work by receiving an instance as input. The underlying dictionary\nof the instance is then manipulated as appropriate.\nTo use a descriptor, instances of the descriptor are placed into a class definition as class\nvariables. For example:\nclass Point:\n    x = Integer('x')\n    y = Integer('y')\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\nWhen you do this, all access to the descriptor attributes (e.g., x or y) is captured by the\n__get__(), __set__(), and __delete__() methods. For example:\n>>> p = Point(2, 3)\n>>> p.x           # Calls Point.x.__get__(p,Point)\n2\n>>> p.y = 5       # Calls Point.y.__set__(p, 5)\n>>> p.x = 2.3     # Calls Point.x.__set__(p, 2.3)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"descrip.py\", line 12, in __set__\n    raise TypeError('Expected an int')\nTypeError: Expected an int\n>>>\nAs input, each method of a descriptor receives the instance being manipulated. To carry\nout the requested operation, the underlying instance dictionary (the __dict__ attribute)\nis manipulated as appropriate. The self.name attribute of the descriptor holds the dic‐\ntionary key being used to store the actual data in the instance dictionary.\nDiscussion\nDescriptors provide the underlying magic for most of Python’s class features, including\n@classmethod, @staticmethod, @property, and even the __slots__ specification.\nBy defining a descriptor, you can capture the core instance operations (get, set, delete)\nat a very low level and completely customize what they do. This gives you great power,\nand is one of the most important tools employed by the writers of advanced libraries\nand frameworks.\nOne confusion with descriptors is that they can only be defined at the class level, not\non a per-instance basis. Thus, code like this will not work:\n# Does NOT work\nclass Point:\n8.9. Creating a New Kind of Class or Instance Attribute \n| \n265",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": 7,
      "content": "def __init__(self, x, y):\n        self.x = Integer('x')    # No! Must be a class variable\n        self.y = Integer('y')\n        self.x = x\n        self.y = y\nAlso, the implementation of the __get__() method is trickier than it seems:\n# Descriptor attribute for an integer type-checked attribute\nclass Integer:\n    ...\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            return instance.__dict__[self.name]\n    ...\nThe reason __get__() looks somewhat complicated is to account for the distinction\nbetween instance variables and class variables. If a descriptor is accessed as a class vari‐\nable, the instance argument is set to None. In this case, it is standard practice to simply\nreturn the descriptor instance itself (although any kind of custom processing is also\nallowed). For example:\n>>> p = Point(2,3)\n>>> p.x      # Calls Point.x.__get__(p, Point)\n2\n>>> Point.x  # Calls Point.x.__get__(None, Point)\n<__main__.Integer object at 0x100671890>\n>>>\nDescriptors are often just one component of a larger programming framework involving\ndecorators or metaclasses. As such, their use may be hidden just barely out of sight. As\nan example, here is some more advanced descriptor-based code involving a class\ndecorator:\n# Descriptor for a type-checked attribute\nclass Typed:\n    def __init__(self, name, expected_type):\n        self.name = name\n        self.expected_type = expected_type\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            return instance.__dict__[self.name]\n    def __set__(self, instance, value):\n        if not isinstance(value, self.expected_type):\n            raise TypeError('Expected ' + str(self.expected_type))\n        instance.__dict__[self.name] = value\n266 \n| \nChapter 8: Classes and Objects",
      "content_length": 1847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": 7,
      "content": "def __delete__(self, instance):\n        del instance.__dict__[self.name]\n# Class decorator that applies it to selected attributes\ndef typeassert(**kwargs):\n    def decorate(cls):\n        for name, expected_type in kwargs.items():\n            # Attach a Typed descriptor to the class\n            setattr(cls, name, Typed(name, expected_type))\n        return cls\n    return decorate\n# Example use\n@typeassert(name=str, shares=int, price=float)\nclass Stock:\n    def __init__(self, name, shares, price):\n        self.name = name\n        self.shares = shares\n        self.price = price\nFinally, it should be stressed that you would probably not write a descriptor if you simply\nwant to customize the access of a single attribute of a specific class. For that, it’s easier\nto use a property instead, as described in Recipe 8.6. Descriptors are more useful in\nsituations where there will be a lot of code reuse (i.e., you want to use the functionality\nprovided by the descriptor in hundreds of places in your code or provide it as a library\nfeature).\n8.10. Using Lazily Computed Properties\nProblem\nYou’d like to define a read-only attribute as a property that only gets computed on access.\nHowever, once accessed, you’d like the value to be cached and not recomputed on each\naccess.\nSolution\nAn efficient way to define a lazy attribute is through the use of a descriptor class, such\nas the following:\nclass lazyproperty:\n    def __init__(self, func):\n        self.func = func\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n8.10. Using Lazily Computed Properties \n| \n267",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": 7,
      "content": "value = self.func(instance)\n            setattr(instance, self.func.__name__, value)\n            return value\nTo utilize this code, you would use it in a class such as the following:\nimport math\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n    @lazyproperty\n    def area(self):\n        print('Computing area')\n        return math.pi * self.radius ** 2\n    @lazyproperty\n    def perimeter(self):\n        print('Computing perimeter')\n        return 2 * math.pi * self.radius\nHere is an interactive session that illustrates how it works:\n>>> c = Circle(4.0)\n>>> c.radius\n4.0\n>>> c.area\nComputing area\n50.26548245743669\n>>> c.area\n50.26548245743669\n>>> c.perimeter\nComputing perimeter\n25.132741228718345\n>>> c.perimeter\n25.132741228718345\n>>>\nCarefully observe that the messages “Computing area” and “Computing perimeter” only\nappear once.\nDiscussion\nIn many cases, the whole point of having a lazily computed attribute is to improve\nperformance. For example, you avoid computing values unless you actually need them\nsomewhere. The solution shown does just this, but it exploits a subtle feature of de‐\nscriptors to do it in a highly efficient way.\nAs shown in other recipes (e.g., Recipe 8.9), when a descriptor is placed into a class\ndefinition, its __get__(), __set__(), and __delete__() methods get triggered on at‐\ntribute access. However, if a descriptor only defines a __get__() method, it has a much\n268 \n| \nChapter 8: Classes and Objects",
      "content_length": 1472,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": 7,
      "content": "weaker binding than usual. In particular, the __get__() method only fires if the attribute\nbeing accessed is not in the underlying instance dictionary.\nThe lazyproperty class exploits this by having the __get__() method store the com‐\nputed value on the instance using the same name as the property itself. By doing this,\nthe value gets stored in the instance dictionary and disables further computation of the\nproperty. You can observe this by digging a little deeper into the example:\n>>> c = Circle(4.0)\n>>> # Get instance variables\n>>> vars(c)\n{'radius': 4.0}\n>>> # Compute area and observe variables afterward\n>>> c.area\nComputing area\n50.26548245743669\n>>> vars(c)\n{'area': 50.26548245743669, 'radius': 4.0}\n>>> # Notice access doesn't invoke property anymore\n>>> c.area\n50.26548245743669\n>>> # Delete the variable and see property trigger again\n>>> del c.area\n>>> vars(c)\n{'radius': 4.0}\n>>> c.area\nComputing area\n50.26548245743669\n>>>\nOne possible downside to this recipe is that the computed value becomes mutable after\nit’s created. For example:\n>>> c.area\nComputing area\n50.26548245743669\n>>> c.area = 25\n>>> c.area\n25\n>>>\nIf that’s a concern, you can use a slightly less efficient implementation, like this:\ndef lazyproperty(func):\n    name = '_lazy_' + func.__name__\n    @property\n    def lazy(self):\n        if hasattr(self, name):\n            return getattr(self, name)\n        else:\n8.10. Using Lazily Computed Properties \n| \n269",
      "content_length": 1445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": 7,
      "content": "value = func(self)\n            setattr(self, name, value)\n            return value\n    return lazy\nIf you use this version, you’ll find that set operations are not allowed. For example:\n>>> c = Circle(4.0)\n>>> c.area\nComputing area\n50.26548245743669\n>>> c.area\n50.26548245743669\n>>> c.area = 25\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: can't set attribute\n>>>\nHowever, a disadvantage is that all get operations have to be routed through the prop‐\nerty’s getter function. This is less efficient than simply looking up the value in the in‐\nstance dictionary, as was done in the original solution.\nFor more information on properties and managed attributes, see Recipe 8.6. Descriptors\nare described in Recipe 8.9.\n8.11. Simplifying the Initialization of Data Structures\nProblem\nYou are writing a lot of classes that serve as data structures, but you are getting tired of\nwriting highly repetitive and boilerplate __init__() functions.\nSolution\nYou can often generalize the initialization of data structures into a single __init__()\nfunction defined in a common base class. For example:\nclass Structure:\n    # Class variable that specifies expected fields\n    _fields= []\n    def __init__(self, *args):\n        if len(args) != len(self._fields):\n            raise TypeError('Expected {} arguments'.format(len(self._fields)))\n        # Set the arguments\n        for name, value in zip(self._fields, args):\n            setattr(self, name, value)\n270 \n| \nChapter 8: Classes and Objects",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": 7,
      "content": "# Example class definitions\nif __name__ == '__main__':\n    class Stock(Structure):\n        _fields = ['name', 'shares', 'price']\n    class Point(Structure):\n        _fields = ['x','y']\n    class Circle(Structure):\n        _fields = ['radius']\n        def area(self):\n            return math.pi * self.radius ** 2\nIf you use the resulting classes, you’ll find that they are easy to construct. For example:\n>>> s = Stock('ACME', 50, 91.1)\n>>> p = Point(2, 3)\n>>> c = Circle(4.5)\n>>> s2 = Stock('ACME', 50)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"structure.py\", line 6, in __init__\n    raise TypeError('Expected {} arguments'.format(len(self._fields)))\nTypeError: Expected 3 arguments\nShould you decide to support keyword arguments, there are several design options. One\nchoice is to map the keyword arguments so that they only correspond to the attribute\nnames specified in _fields. For example:\nclass Structure:\n    _fields= []\n    def __init__(self, *args, **kwargs):\n        if len(args) > len(self._fields):\n            raise TypeError('Expected {} arguments'.format(len(self._fields)))\n        # Set all of the positional arguments\n        for name, value in zip(self._fields, args):\n            setattr(self, name, value)\n        # Set the remaining keyword arguments\n        for name in self._fields[len(args):]:\n            setattr(self, name, kwargs.pop(name))\n        # Check for any remaining unknown arguments\n        if kwargs:\n            raise TypeError('Invalid argument(s): {}'.format(','.join(kwargs)))\n# Example use\nif __name__ == '__main__':\n    class Stock(Structure):\n        _fields = ['name', 'shares', 'price']\n    s1 = Stock('ACME', 50, 91.1)\n8.11. Simplifying the Initialization of Data Structures \n| \n271",
      "content_length": 1771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": 7,
      "content": "s2 = Stock('ACME', 50, price=91.1)\n    s3 = Stock('ACME', shares=50, price=91.1)\nAnother possible choice is to use keyword arguments as a means for adding additional\nattributes to the structure not specified in _fields. For example:\nclass Structure:\n    # Class variable that specifies expected fields\n    _fields= []\n    def __init__(self, *args, **kwargs):\n        if len(args) != len(self._fields):\n            raise TypeError('Expected {} arguments'.format(len(self._fields)))\n        # Set the arguments\n        for name, value in zip(self._fields, args):\n            setattr(self, name, value)\n        # Set the additional arguments (if any)\n        extra_args = kwargs.keys() - self._fields\n        for name in extra_args:\n            setattr(self, name, kwargs.pop(name))\n        if kwargs:\n            raise TypeError('Duplicate values for {}'.format(','.join(kwargs)))\n# Example use\nif __name__ == '__main__':\n    class Stock(Structure):\n        _fields = ['name', 'shares', 'price']\n    s1 = Stock('ACME', 50, 91.1)\n    s2 = Stock('ACME', 50, 91.1, date='8/2/2012')\nDiscussion\nThis technique of defining a general purpose __init__() method can be extremely\nuseful if you’re ever writing a program built around a large number of small data struc‐\ntures. It leads to much less code than manually writing __init__() methods like this:\nclass Stock:\n    def __init__(self, name, shares, price):\n        self.name = name\n        self.shares = shares\n        self.price = price\nclass Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n272 \n| \nChapter 8: Classes and Objects",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": 7,
      "content": "def area(self):\n        return math.pi * self.radius ** 2\nOne subtle aspect of the implementation concerns the mechanism used to set value\nusing the setattr() function. Instead of doing that, you might be inclined to directly\naccess the instance dictionary. For example:\nclass Structure:\n    # Class variable that specifies expected fields\n    _fields= []\n    def __init__(self, *args):\n        if len(args) != len(self._fields):\n            raise TypeError('Expected {} arguments'.format(len(self._fields)))\n        # Set the arguments (alternate)\n        self.__dict__.update(zip(self._fields,args))\nAlthough this works, it’s often not safe to make assumptions about the implementation\nof a subclass. If a subclass decided to use __slots__ or wrap a specific attribute with a\nproperty (or descriptor), directly acccessing the instance dictionary would break. The\nsolution has been written to be as general purpose as possible and not to make any\nassumptions about subclasses.\nA potential downside of this technique is that it impacts documentation and help fea‐\ntures of IDEs. If a user asks for help on a specific class, the required arguments aren’t\ndescribed in the usual way. For example:\n>>> help(Stock)\nHelp on class Stock in module __main__:\nclass Stock(Structure)\n...\n |  Methods inherited from Structure:\n |\n |  __init__(self, *args, **kwargs)\n |\n...\n>>>\nMany of these problems can be fixed by either attaching or enforcing a type signature\nin the __init__() function. See Recipe 9.16.\nIt should be noted that it is also possible to automatically initialize instance variables\nusing a utility function and a so-called “frame hack.” For example:\ndef init_fromlocals(self):\n    import sys\n    locs = sys._getframe(1).f_locals\n    for k, v in locs.items():\n        if k != 'self':\n            setattr(self, k, v)\n8.11. Simplifying the Initialization of Data Structures \n| \n273",
      "content_length": 1884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": 7,
      "content": "class Stock:\n    def __init__(self, name, shares, price):\n        init_fromlocals(self)\nIn this variation, the init_fromlocals() function uses sys._getframe() to peek at the\nlocal variables of the calling method. If used as the first step of an __init__() method,\nthe local variables will be the same as the passed arguments and can be easily used to\nset attributes with the same names. Although this approach avoids the problem of get‐\nting the right calling signature in IDEs, it runs more than 50% slower than the solution\nprovided in the recipe, requires more typing, and involves more sophisticated magic\nbehind the scenes. If your code doesn’t need this extra power, often times the simpler\nsolution will work just fine. \n8.12. Defining an Interface or Abstract Base Class\nProblem\nYou want to define a class that serves as an interface or abstract base class from which\nyou can perform type checking and ensure that certain methods are implemented in\nsubclasses.\nSolution\nTo define an abstract base class, use the abc module. For example:\nfrom abc import ABCMeta, abstractmethod\nclass IStream(metaclass=ABCMeta):\n    @abstractmethod\n    def read(self, maxbytes=-1):\n        pass\n    @abstractmethod\n    def write(self, data):\n        pass\nA central feature of an abstract base class is that it cannot be instantiated directly. For\nexample, if you try to do it, you’ll get an error:\na = IStream()   # TypeError: Can't instantiate abstract class\n                # IStream with abstract methods read, write\nInstead, an abstract base class is meant to be used as a base class for other classes that\nare expected to implement the required methods. For example:\nclass SocketStream(IStream):\n    def read(self, maxbytes=-1):\n        ...\n    def write(self, data):\n        ...\n274 \n| \nChapter 8: Classes and Objects",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": 7,
      "content": "A major use of abstract base classes is in code that wants to enforce an expected pro‐\ngramming interface. For example, one way to view the IStream base class is as a high-\nlevel specification for an interface that allows reading and writing of data. Code that\nexplicitly checks for this interface could be written as follows:\ndef serialize(obj, stream):\n    if not isinstance(stream, IStream):\n        raise TypeError('Expected an IStream')\n    ...\nYou might think that this kind of type checking only works by subclassing the abstract\nbase class (ABC), but ABCs allow other classes to be registered as implementing the\nrequired interface. For example, you can do this:\nimport io\n# Register the built-in I/O classes as supporting our interface\nIStream.register(io.IOBase)\n# Open a normal file and type check\nf = open('foo.txt')\nisinstance(f, IStream)      # Returns True\nIt should be noted that @abstractmethod can also be applied to static methods, class\nmethods, and properties. You just need to make sure you apply it in the proper sequence\nwhere @abstractmethod appears immediately before the function definition, as shown\nhere:\nfrom abc import ABCMeta, abstractmethod\nclass A(metaclass=ABCMeta):\n    @property\n    @abstractmethod\n    def name(self):\n        pass\n    @name.setter\n    @abstractmethod\n    def name(self, value):\n        pass\n    @classmethod\n    @abstractmethod\n    def method1(cls):\n        pass\n    @staticmethod\n    @abstractmethod\n    def method2():\n        pass\n8.12. Defining an Interface or Abstract Base Class \n| \n275",
      "content_length": 1546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": 7,
      "content": "Discussion\nPredefined abstract base classes are found in various places in the standard library. The\ncollections module defines a variety of ABCs related to containers and iterators (se‐\nquences, mappings, sets, etc.), the numbers library defines ABCs related to numeric\nobjects (integers, floats, rationals, etc.), and the io library defines ABCs related to I/O\nhandling.\nYou can use the predefined ABCs to perform more generalized kinds of type checking.\nHere are some examples:\nimport collections\n# Check if x is a sequence\nif isinstance(x, collections.Sequence):\n    ...\n# Check if x is iterable\nif isinstance(x, collections.Iterable):\n    ...\n# Check if x has a size\nif isinstance(x, collections.Sized):\n    ...\n# Check if x is a mapping\nif isinstance(x, collections.Mapping):\n    ...\nIt should be noted that, as of this writing, certain library modules don’t make use of\nthese predefined ABCs as you might expect. For example:\nfrom decimal import Decimal\nimport numbers\nx = Decimal('3.4')\nisinstance(x, numbers.Real)   # Returns False\nEven though the value 3.4 is technically a real number, it doesn’t type check that way\nto help avoid inadvertent mixing of floating-point numbers and decimals. Thus, if you\nuse the ABC functionality, it is wise to carefully write tests that verify that the behavior\nis as you intended.\nAlthough ABCs facilitate type checking, it’s not something that you should overuse in\na program. At its heart, Python is a dynamic language that gives you great flexibility.\nTrying to enforce type constraints everywhere tends to result in code that is more com‐\nplicated than it needs to be. You should embrace Python’s flexibility.\n276 \n| \nChapter 8: Classes and Objects",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": 7,
      "content": "8.13. Implementing a Data Model or Type System\nProblem\nYou want to define various kinds of data structures, but want to enforce constraints on\nthe values that are allowed to be assigned to certain attributes.\nSolution\nIn this problem, you are basically faced with the task of placing checks or assertions on\nthe setting of certain instance attributes. To do this, you need to customize the setting\nof attributes on a per-attribute basis. To do this, you should use descriptors.\nThe following code illustrates the use of descriptors to implement a system type and\nvalue checking framework:\n# Base class. Uses a descriptor to set a value\nclass Descriptor:\n    def __init__(self, name=None, **opts):\n        self.name = name\n        for key, value in opts.items():\n            setattr(self, key, value)\n    def __set__(self, instance, value):\n        instance.__dict__[self.name] = value\n# Descriptor for enforcing types\nclass Typed(Descriptor):\n    expected_type = type(None)\n    def __set__(self, instance, value):\n        if not isinstance(value, self.expected_type):\n            raise TypeError('expected ' + str(self.expected_type))\n        super().__set__(instance, value)\n# Descriptor for enforcing values\nclass Unsigned(Descriptor):\n    def __set__(self, instance, value):\n        if value < 0:\n            raise ValueError('Expected >= 0')\n        super().__set__(instance, value)\nclass MaxSized(Descriptor):\n    def __init__(self, name=None, **opts):\n        if 'size' not in opts:\n            raise TypeError('missing size option')\n        super().__init__(name, **opts)\n    def __set__(self, instance, value):\n        if len(value) >= self.size:\n8.13. Implementing a Data Model or Type System \n| \n277",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": 7,
      "content": "raise ValueError('size must be < ' + str(self.size))\n        super().__set__(instance, value)\nThese classes should be viewed as basic building blocks from which you construct a data\nmodel or type system. Continuing, here is some code that implements some different\nkinds of data:\nclass Integer(Typed):\n    expected_type = int\nclass UnsignedInteger(Integer, Unsigned):\n    pass\nclass Float(Typed):\n    expected_type = float\nclass UnsignedFloat(Float, Unsigned):\n    pass\nclass String(Typed):\n    expected_type = str\nclass SizedString(String, MaxSized):\n    pass\nUsing these type objects, it is now possible to define a class such as this:\nclass Stock:\n    # Specify constraints\n    name = SizedString('name',size=8)\n    shares = UnsignedInteger('shares')\n    price = UnsignedFloat('price')\n    def __init__(self, name, shares, price):\n        self.name = name\n        self.shares = shares\n        self.price = price\nWith the constraints in place, you’ll find that assigning of attributes is now validated.\nFor example:\n>>> s = Stock('ACME', 50, 91.1)\n>>> s.name\n'ACME'\n>>> s.shares = 75\n>>> s.shares = -10\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example.py\", line 17, in __set__\n    super().__set__(instance, value)\n  File \"example.py\", line 23, in __set__\n    raise ValueError('Expected >= 0')\nValueError: Expected >= 0\n>>> s.price = 'a lot'\n278 \n| \nChapter 8: Classes and Objects",
      "content_length": 1421,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": 7,
      "content": "Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example.py\", line 16, in __set__\n    raise TypeError('expected ' + str(self.expected_type))\nTypeError: expected <class 'float'>\n>>> s.name = 'ABRACADABRA'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example.py\", line 17, in __set__\n    super().__set__(instance, value)\n  File \"example.py\", line 35, in __set__\n    raise ValueError('size must be < ' + str(self.size))\nValueError: size must be < 8\n>>>\nThere are some techniques that can be used to simplify the specification of constraints\nin classes. One approach is to use a class decorator, like this:\n# Class decorator to apply constraints\ndef check_attributes(**kwargs):\n    def decorate(cls):\n        for key, value in kwargs.items():\n            if isinstance(value, Descriptor):\n                value.name = key\n                setattr(cls, key, value)\n            else:\n                setattr(cls, key, value(key))\n        return cls\n    return decorate\n# Example\n@check_attributes(name=SizedString(size=8),\n                  shares=UnsignedInteger,\n                  price=UnsignedFloat)\nclass Stock:\n    def __init__(self, name, shares, price):\n        self.name = name\n        self.shares = shares\n        self.price = price\nAnother approach to simplify the specification of constraints is to use a metaclass. For\nexample:\n# A metaclass that applies checking\nclass checkedmeta(type):\n    def __new__(cls, clsname, bases, methods):\n        # Attach attribute names to the descriptors\n        for key, value in methods.items():\n            if isinstance(value, Descriptor):\n                value.name = key\n        return type.__new__(cls, clsname, bases, methods)\n8.13. Implementing a Data Model or Type System \n| \n279",
      "content_length": 1796,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": 7,
      "content": "# Example\nclass Stock(metaclass=checkedmeta):\n    name   = SizedString(size=8)\n    shares = UnsignedInteger()\n    price  = UnsignedFloat()\n    def __init__(self, name, shares, price):\n        self.name = name\n        self.shares = shares\n        self.price = price\nDiscussion\nThis recipe involves a number of advanced techniques, including descriptors, mixin\nclasses, the use of super(), class decorators, and metaclasses. Covering the basics of all\nthose topics is beyond what can be covered here, but examples can be found in other\nrecipes (see Recipes 8.9, 8.18, 9.12, and 9.19). However, there are a number of subtle\npoints worth noting.\nFirst, in the Descriptor base class, you will notice that there is a __set__() method,\nbut no corresponding __get__(). If a descriptor will do nothing more than extract an\nidentically named value from the underlying instance dictionary, defining __get__()\nis unnecessary. In fact, defining __get__() will just make it run slower. Thus, this recipe\nonly focuses on the implementation of __set__().\nThe overall design of the various descriptor classes is based on mixin classes. For ex‐\nample, the Unsigned and MaxSized classes are meant to be mixed with the other de‐\nscriptor classes derived from Typed. To handle a specific kind of data type, multiple\ninheritance is used to combine the desired functionality.\nYou will also notice that all __init__() methods of the various descriptors have been\nprogrammed to have an identical signature involving keyword arguments **opts. The\nclass for MaxSized looks for its required attribute in opts, but simply passes it along to\nthe Descriptor base class, which actually sets it. One tricky part about composing\nclasses like this (especially mixins), is that you don’t always know how the classes are\ngoing to be chained together or what super() will invoke. For this reason, you need to\nmake it work with any possible combination of classes.\nThe definitions of the various type classes such as Integer, Float, and String illustrate\na useful technique of using class variables to customize an implementation. The Ty\nped descriptor merely looks for an expected_type attribute that is provided by each of\nthose subclasses.\nThe use of a class decorator or metaclass is often useful for simplifying the specification\nby the user. You will notice that in those examples, the user no longer has to type the\nname of the attribute more than once. For example:\n280 \n| \nChapter 8: Classes and Objects",
      "content_length": 2473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": 7,
      "content": "# Normal\nclass Point:\n    x = Integer('x')\n    y = Integer('y')\n# Metaclass\nclass Point(metaclass=checkedmeta):\n    x = Integer()\n    y = Integer()\nThe code for the class decorator and metaclass simply scan the class dictionary looking\nfor descriptors. When found, they simply fill in the descriptor name based on the key\nvalue.\nOf all the approaches, the class decorator solution may provide the most flexibility and\nsanity. For one, it does not rely on any advanced machinery, such as metaclasses. Second,\ndecoration is something that can easily be added or removed from a class definition as\ndesired. For example, within the decorator, there could be an option to simply omit the\nadded checking altogether. These might allow the checking to be something that could\nbe turned on or off depending on demand (maybe for debugging versus production).\nAs a final twist, a class decorator approach can also be used as a replacement for mixin\nclasses, multiple inheritance, and tricky use of the super() function. Here is an alter‐\nnative formulation of this recipe that uses class decorators:\n# Base class. Uses a descriptor to set a value\nclass Descriptor:\n    def __init__(self, name=None, **opts):\n        self.name = name\n        for key, value in opts.items():\n            setattr(self, key, value)\n    def __set__(self, instance, value):\n        instance.__dict__[self.name] = value\n# Decorator for applying type checking\ndef Typed(expected_type, cls=None):\n    if cls is None:\n        return lambda cls: Typed(expected_type, cls)\n    super_set = cls.__set__\n    def __set__(self, instance, value):\n        if not isinstance(value, expected_type):\n            raise TypeError('expected ' + str(expected_type))\n        super_set(self, instance, value)\n    cls.__set__ = __set__\n    return cls\n# Decorator for unsigned values\ndef Unsigned(cls):\n    super_set = cls.__set__\n8.13. Implementing a Data Model or Type System \n| \n281",
      "content_length": 1927,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": 7,
      "content": "def __set__(self, instance, value):\n        if value < 0:\n            raise ValueError('Expected >= 0')\n        super_set(self, instance, value)\n    cls.__set__ = __set__\n    return cls\n# Decorator for allowing sized values\ndef MaxSized(cls):\n    super_init = cls.__init__\n    def __init__(self, name=None, **opts):\n        if 'size' not in opts:\n            raise TypeError('missing size option')\n        super_init(self, name, **opts)\n    cls.__init__ = __init__\n    super_set = cls.__set__\n    def __set__(self, instance, value):\n        if len(value) >= self.size:\n            raise ValueError('size must be < ' + str(self.size))\n        super_set(self, instance, value)\n    cls.__set__ = __set__\n    return cls\n# Specialized descriptors\n@Typed(int)\nclass Integer(Descriptor):\n    pass\n@Unsigned\nclass UnsignedInteger(Integer):\n    pass\n@Typed(float)\nclass Float(Descriptor):\n    pass\n@Unsigned\nclass UnsignedFloat(Float):\n    pass\n@Typed(str)\nclass String(Descriptor):\n    pass\n@MaxSized\nclass SizedString(String):\n    pass\nThe classes defined in this alternative formulation work in exactly the same manner as\nbefore (none of the earlier example code changes) except that everything runs much\nfaster. For example, a simple timing test of setting a typed attribute reveals that the class\n282 \n| \nChapter 8: Classes and Objects",
      "content_length": 1331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": 7,
      "content": "decorator approach runs almost 100% faster than the approach using mixins. Now aren’t\nyou glad you read all the way to the end?\n8.14. Implementing Custom Containers\nProblem\nYou want to implement a custom class that mimics the behavior of a common built-in\ncontainer type, such as a list or dictionary. However, you’re not entirely sure what\nmethods need to be implemented to do it.\nSolution\nThe collections library defines a variety of abstract base classes that are extremely\nuseful when implementing custom container classes. To illustrate, suppose you want\nyour class to support iteration. To do that, simply start by having it inherit from col\nlections.Iterable, as follows:\nimport collections\nclass A(collections.Iterable):\n    pass\nThe special feature about inheriting from collections.Iterable is that it ensures you\nimplement all of the required special methods. If you don’t, you’ll get an error upon\ninstantiation:\n>>> a = A()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: Can't instantiate abstract class A with abstract methods __iter__\n>>>\nTo fix this error, simply give the class the required __iter__() method and implement\nit as desired (see Recipes 4.2 and 4.7).\nOther notable classes defined in collections include Sequence, MutableSequence,\nMapping, MutableMapping, Set, and MutableSet. Many of these classes form hierarchies\nwith increasing levels of functionality (e.g., one such hierarchy is Container, Itera\nble, Sized, Sequence, and MutableSequence). Again, simply instantiate any of these\nclasses to see what methods need to be implemented to make a custom container with\nthat behavior:\n>>> import collections\n>>> collections.Sequence()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: Can't instantiate abstract class Sequence with abstract methods \\\n8.14. Implementing Custom Containers \n| \n283",
      "content_length": 1894,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": 7,
      "content": "__getitem__, __len__\n>>>\nHere is a simple example of a class that implements the preceding methods to create a\nsequence where items are always stored in sorted order (it’s not a particularly efficient\nimplementation, but it illustrates the general idea):\nimport collections\nimport bisect\nclass SortedItems(collections.Sequence):\n    def __init__(self, initial=None):\n        self._items = sorted(initial) if initial is None else []\n    # Required sequence methods\n    def __getitem__(self, index):\n        return self._items[index]\n    def __len__(self):\n        return len(self._items)\n    # Method for adding an item in the right location\n    def add(self, item):\n        bisect.insort(self._items, item)\nHere’s an example of using this class:\n>>> items = SortedItems([5, 1, 3])\n>>> list(items)\n[1, 3, 5]\n>>> items[0]\n1\n>>> items[-1]\n5\n>>> items.add(2)\n>>> list(items)\n[1, 2, 3, 5]\n>>> items.add(-10)\n>>> list(items)\n[-10, 1, 2, 3, 5]\n>>> items[1:4]\n[1, 2, 3]\n>>> 3 in items\nTrue\n>>> len(items)\n5\n>>> for n in items:\n...     print(n)\n...\n-10\n1\n2\n3\n284 \n| \nChapter 8: Classes and Objects",
      "content_length": 1088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": 7,
      "content": "5\n>>>\nAs you can see, instances of SortedItems behave exactly like a normal sequence and\nsupport all of the usual operations, including indexing, iteration, len(), containment\n(the in operator), and even slicing.\nAs an aside, the bisect module used in this recipe is a convenient way to keep items in\na list sorted. The bisect.insort() inserts an item into a list so that the list remains in\norder.\nDiscussion\nInheriting from one of the abstract base classes in collections ensures that your cus‐\ntom container implements all of the required methods expected of the container. How‐\never, this inheritance also facilitates type checking.\nFor example, your custom container will satisfy various type checks like this:\n>>> items = SortedItems()\n>>> import collections\n>>> isinstance(items, collections.Iterable)\nTrue\n>>> isinstance(items, collections.Sequence)\nTrue\n>>> isinstance(items, collections.Container)\nTrue\n>>> isinstance(items, collections.Sized)\nTrue\n>>> isinstance(items, collections.Mapping)\nFalse\n>>>\nMany of the abstract base classes in collections also provide default implementations\nof common container methods. To illustrate, suppose you have a class that inherits from\ncollections.MutableSequence, like this:\nclass Items(collections.MutableSequence):\n    def __init__(self, initial=None):\n        self._items = list(initial) if initial is None else []\n    # Required sequence methods\n    def __getitem__(self, index):\n        print('Getting:', index)\n        return self._items[index]\n    def __setitem__(self, index, value):\n        print('Setting:', index, value)\n        self._items[index] = value\n    def __delitem__(self, index):\n8.14. Implementing Custom Containers \n| \n285",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": 7,
      "content": "print('Deleting:', index)\n        del self._items[index]\n    def insert(self, index, value):\n        print('Inserting:', index, value)\n        self._items.insert(index, value)\n    def __len__(self):\n        print('Len')\n        return len(self._items)\nIf you create an instance of Items, you’ll find that it supports almost all of the core list\nmethods (e.g., append(), remove(), count(), etc.). These methods are implemented in\nsuch a way that they only use the required ones. Here’s an interactive session that illus‐\ntrates this:\n>>> a = Items([1, 2, 3])\n>>> len(a)\nLen\n3\n>>> a.append(4)\nLen\nInserting: 3 4\n>>> a.append(2)\nLen\nInserting: 4 2\n>>> a.count(2)\nGetting: 0\nGetting: 1\nGetting: 2\nGetting: 3\nGetting: 4\nGetting: 5\n2\n>>> a.remove(3)\nGetting: 0\nGetting: 1\nGetting: 2\nDeleting: 2\n>>>\nThis recipe only provides a brief glimpse into Python’s abstract class functionality. The\nnumbers module provides a similar collection of abstract classes related to numeric data\ntypes. See Recipe 8.12 for more information about making your own abstract base\nclasses.\n286 \n| \nChapter 8: Classes and Objects",
      "content_length": 1099,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": 7,
      "content": "8.15. Delegating Attribute Access\nProblem\nYou want an instance to delegate attribute access to an internally held instance possibly\nas an alternative to inheritance or in order to implement a proxy.\nSolution\nSimply stated, delegation is a programming pattern where the responsibility for imple‐\nmenting a particular operation is handed off (i.e., delegated) to a different object. In its\nsimplest form, it often looks something like this:\nclass A:\n    def spam(self, x):\n        pass\n    def foo(self):\n        pass\nclass B:\n    def __init__(self):\n        self._a = A()\n    def spam(self, x):\n        # Delegate to the internal self._a instance\n        return self._a.spam(x)\n    def foo(self):\n        # Delegate to the internal self._a instance\n        return self._a.foo()\n    def bar(self):\n        pass\nIf there are only a couple of methods to delegate, writing code such as that just given is\neasy enough. However, if there are many methods to delegate, an alternative approach\nis to define the __getattr__() method, like this:\nclass A:\n    def spam(self, x):\n        pass\n    def foo(self):\n        pass\nclass B:\n    def __init__(self):\n        self._a = A()\n8.15. Delegating Attribute Access \n| \n287",
      "content_length": 1208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": 7,
      "content": "def bar(self):\n        pass\n    # Expose all of the methods defined on class A\n    def __getattr__(self, name):\n        return getattr(self._a, name)\nThe __getattr__() method is kind of like a catch-all for attribute lookup. It’s a method\nthat gets called if code tries to access an attribute that doesn’t exist. In the preceding\ncode, it would catch access to undefined methods on B and simply delegate them to A.\nFor example:\nb = B()\nb.bar()    # Calls B.bar() (exists on B)\nb.spam(42) # Calls B.__getattr__('spam') and delegates to A.spam\nAnother example of delegation is in the implementation of proxies. For example:\n# A proxy class that wraps around another object, but\n# exposes its public attributes\nclass Proxy:\n    def __init__(self, obj):\n        self._obj = obj\n    # Delegate attribute lookup to internal obj\n    def __getattr__(self, name):\n        print('getattr:', name)\n        return getattr(self._obj, name)\n    # Delegate attribute assignment\n    def __setattr__(self, name, value):\n        if name.startswith('_'):\n            super().__setattr__(name, value)\n        else:\n            print('setattr:', name, value)\n            setattr(self._obj, name, value)\n    # Delegate attribute deletion\n    def __delattr__(self, name):\n        if name.startswith('_'):\n            super().__delattr__(name)\n        else:\n            print('delattr:', name)\n            delattr(self._obj, name)\nTo use this proxy class, you simply wrap it around another instance. For example:\nclass Spam:\n    def __init__(self, x):\n        self.x = x\n    def bar(self, y):\n        print('Spam.bar:', self.x, y)\n288 \n| \nChapter 8: Classes and Objects",
      "content_length": 1645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": 7,
      "content": "# Create an instance\ns = Spam(2)\n# Create a proxy around it\np = Proxy(s)\n# Access the proxy\nprint(p.x)     # Outputs 2\np.bar(3)       # Outputs \"Spam.bar: 2 3\"\np.x = 37       # Changes s.x to 37\nBy customizing the implementation of the attribute access methods, you could cus‐\ntomize the proxy to behave in different ways (e.g., logging access, only allowing read-\nonly access, etc.).\nDiscussion\nDelegation is sometimes used as an alternative to inheritance. For example, instead of\nwriting code like this:\nclass A:\n    def spam(self, x):\n        print('A.spam', x)\n    def foo(self):\n        print('A.foo')\nclass B(A):\n    def spam(self, x):\n        print('B.spam')\n        super().spam(x)\n    def bar(self):\n        print('B.bar')\nA solution involving delegation would be written as follows:\nclass A:\n    def spam(self, x):\n        print('A.spam', x)\n    def foo(self):\n        print('A.foo')\nclass B:\n    def __init__(self):\n        self._a = A()\n    def spam(self, x):\n        print('B.spam', x)\n        self._a.spam(x)\n8.15. Delegating Attribute Access \n| \n289",
      "content_length": 1065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": 7,
      "content": "def bar(self):\n        print('B.bar')\n    def __getattr__(self, name):\n        return getattr(self._a, name)\nThis use of delegation is often useful in situations where direct inheritance might not\nmake much sense or where you want to have more control of the relationship between\nobjects (e.g., only exposing certain methods, implementing interfaces, etc.).\nWhen using delegation to implement proxies, there are a few additional details to note.\nFirst, the __getattr__() method is actually a fallback method that only gets called when\nan attribute is not found. Thus, when attributes of the proxy instance itself are accessed\n(e.g., the _obj attribute), this method would not be triggered. Second, the __se\ntattr__() and __delattr__() methods need a bit of extra logic added to separate\nattributes from the proxy instance inself and attributes on the internal object _obj. A\ncommon convention is for proxies to only delegate to attributes that don’t start with a\nleading underscore (i.e., proxies only expose the “public” attributes of the held instance).\nIt is also important to emphasize that the __getattr__() method usually does not apply\nto most special methods that start and end with double underscores. For example, con‐\nsider this class:\nclass ListLike:\n    def __init__(self):\n        self._items = []\n    def __getattr__(self, name):\n        return getattr(self._items, name)\nIf you try to make a ListLike object, you’ll find that it supports the common list meth‐\nods, such as append() and insert(). However, it does not support any of the operators\nlike len(), item lookup, and so forth. For example:\n>>> a = ListLike()\n>>> a.append(2)\n>>> a.insert(0, 1)\n>>> a.sort()\n>>> len(a)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: object of type 'ListLike' has no len()\n>>> a[0]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'ListLike' object does not support indexing\n>>>\nTo support the different operators, you have to manually delegate the associated special\nmethods yourself. For example:\n290 \n| \nChapter 8: Classes and Objects",
      "content_length": 2114,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": 7,
      "content": "class ListLike:\n    def __init__(self):\n        self._items = []\n    def __getattr__(self, name):\n        return getattr(self._items, name)\n    # Added special methods to support certain list operations\n    def __len__(self):\n        return len(self._items)\n    def __getitem__(self, index):\n        return self._items[index]\n    def __setitem__(self, index, value):\n        self._items[index] = value\n    def __delitem__(self, index):\n        del self._items[index]\nSee Recipe 11.8 for another example of using delegation in the context of creating proxy\nclasses for remote procedure call.\n8.16. Defining More Than One Constructor in a Class\nProblem\nYou’re writing a class, but you want users to be able to create instances in more than the\none way provided by __init__().\nSolution\nTo define a class with more than one constructor, you should use a class method. Here\nis a simple example:\nimport time\nclass Date:\n    # Primary constructor\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n    # Alternate constructor\n    @classmethod\n    def today(cls):\n        t = time.localtime()\n        return cls(t.tm_year, t.tm_mon, t.tm_mday)\nTo use the alternate constructor, you simply call it as a function, such as Date.to\nday(). Here is an example:\n8.16. Defining More Than One Constructor in a Class \n| \n291",
      "content_length": 1372,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": 7,
      "content": "a = Date(2012, 12, 21)      # Primary\nb = Date.today()            # Alternate\nDiscussion\nOne of the primary uses of class methods is to define alternate constructors, as shown\nin this recipe. A critical feature of a class method is that it receives the class as the first\nargument (cls). You will notice that this class is used within the method to create and\nreturn the final instance. It is extremely subtle, but this aspect of class methods makes\nthem work correctly with features such as inheritance. For example:\nclass NewDate(Date):\n    pass\nc = Date.today()      # Creates an instance of Date (cls=Date)\nd = NewDate.today()   # Creates an instance of NewDate (cls=NewDate)\nWhen defining a class with multiple constructors, you should make the __init__()\nfunction as simple as possible—doing nothing more than assigning attributes from\ngiven values. Alternate constructors can then choose to perform advanced operations\nif needed.\nInstead of defining a separate class method, you might be inclined to implement the\n__init__() method in a way that allows for different calling conventions. For example:\nclass Date:\n    def __init__(self, *args):\n        if len(args) == 0:\n            t = time.localtime()\n            args = (t.tm_year, t.tm_mon, t.tm_mday)\n        self.year, self.month, self.day = args\nAlthough this technique works in certain cases, it often leads to code that is hard to\nunderstand and difficult to maintain. For example, this implementation won’t show\nuseful help strings (with argument names). In addition, code that creates Date instances\nwill be less clear. Compare and contrast the following:\na = Date(2012, 12, 21)   # Clear. A specific date.\nb = Date()               # ??? What does this do?\n# Class method version\nc = Date.today()         # Clear. Today's date.\nAs shown, the Date.today() invokes the regular Date.__init__() method by instan‐\ntiating a Date() with suitable year, month, and day arguments. If necessary, instances\ncan be created without ever invoking the __init__() method. This is described in the\nnext recipe.\n292 \n| \nChapter 8: Classes and Objects",
      "content_length": 2100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": 7,
      "content": "8.17. Creating an Instance Without Invoking init\nProblem\nYou need to create an instance, but want to bypass the execution of the __init__()\nmethod for some reason.\nSolution\nA bare uninitialized instance can be created by directly calling the __new__() method\nof a class. For example, consider this class:\nclass Date:\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\nHere’s how you can create a Date instance without invoking __init__():\n>>> d = Date.__new__(Date)\n>>> d\n<__main__.Date object at 0x1006716d0>\n>>> d.year\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'Date' object has no attribute 'year'\n>>>\nAs you can see, the resulting instance is uninitialized. Thus, it is now your responsibility\nto set the appropriate instance variables. For example:\n>>> data = {'year':2012, 'month':8, 'day':29}\n>>> for key, value in data.items():\n...     setattr(d, key, value)\n...\n>>> d.year\n2012\n>>> d.month\n8\n>>>\nDiscussion\nThe problem of bypassing __init__() sometimes arises when instances are being cre‐\nated in a nonstandard way such as when deserializing data or in the implementation of\na class method that’s been defined as an alternate constructor. For example, on the Date\nclass shown, someone might define an alternate constructor today() as follows:\n8.17. Creating an Instance Without Invoking init \n| \n293",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": 7,
      "content": "from time import localtime\nclass Date:\n    def __init__(self, year, month, day):\n        self.year = year\n        self.month = month\n        self.day = day\n    @classmethod\n    def today(cls):\n        d = cls.__new__(cls)\n        t = localtime()\n        d.year = t.tm_year\n        d.month = t.tm_mon\n        d.day = t.tm_mday\n        return d\nSimilarly, suppose you are deserializing JSON data and, as a result, produce a dictionary\nlike this:\ndata = { 'year': 2012, 'month': 8, 'day': 29 }\nIf you want to turn this into a Date instance, simply use the technique shown in the\nsolution.\nWhen creating instances in a nonstandard way, it’s usually best to not make too many\nassumptions about their implementation. As such, you generally don’t want to write\ncode that directly manipulates the underlying instance dictionary __dict__ unless you\nknow it’s guaranteed to be defined. Otherwise, the code will break if the class uses\n__slots__, properties, descriptors, or other advanced techniques. By using se\ntattr() to set the values, your code will be as general purpose as possible.\n8.18. Extending Classes with Mixins\nProblem\nYou have a collection of generally useful methods that you would like to make available\nfor extending the functionality of other class definitions. However, the classes where\nthe methods might be added aren’t necessarily related to one another via inheritance.\nThus, you can’t just attach the methods to a common base class.\nSolution\nThe problem addressed by this recipe often arises in code where one is interested in the\nissue of class customization. For example, maybe a library provides a basic set of classes\nalong with a set of optional customizations that can be applied if desired by the user.\n294 \n| \nChapter 8: Classes and Objects",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": 7,
      "content": "To illustrate, suppose you have an interest in adding various customizations (e.g., log‐\nging, set-once, type checking, etc.) to mapping objects. Here are a set of mixin classes\nthat do that:\nclass LoggedMappingMixin:\n    '''\n    Add logging to get/set/delete operations for debugging.\n    '''\n    __slots__ = ()\n    def __getitem__(self, key):\n        print('Getting ' + str(key))\n        return super().__getitem__(key)\n    def __setitem__(self, key, value):\n        print('Setting {} = {!r}'.format(key, value))\n        return super().__setitem__(key, value)\n    def __delitem__(self, key):\n        print('Deleting ' + str(key))\n        return super().__delitem__(key)\nclass SetOnceMappingMixin:\n    '''\n    Only allow a key to be set once.\n    '''\n    __slots__ = ()\n    def __setitem__(self, key, value):\n        if key in self:\n            raise KeyError(str(key) + ' already set')\n        return super().__setitem__(key, value)\nclass StringKeysMappingMixin:\n    '''\n    Restrict keys to strings only\n    '''\n    __slots__ = ()\n    def __setitem__(self, key, value):\n        if not isinstance(key, str):\n            raise TypeError('keys must be strings')\n        return super().__setitem__(key, value)\nThese classes, by themselves, are useless. In fact, if you instantiate any one of them, it\ndoes nothing useful at all (other than generate exceptions). Instead, they are supposed\nto be mixed with other mapping classes through multiple inheritance. For example:\n>>> class LoggedDict(LoggedMappingMixin, dict):\n...     pass\n...\n>>> d = LoggedDict()\n>>> d['x'] = 23\nSetting x = 23\n8.18. Extending Classes with Mixins \n| \n295",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": 7,
      "content": ">>> d['x']\nGetting x\n23\n>>> del d['x']\nDeleting x\n>>> from collections import defaultdict\n>>> class SetOnceDefaultDict(SetOnceMappingMixin, defaultdict):\n...     pass\n...\n>>> d = SetOnceDefaultDict(list)\n>>> d['x'].append(2)\n>>> d['y'].append(3)\n>>> d['x'].append(10)\n>>> d['x'] = 23\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"mixin.py\", line 24, in __setitem__\n    raise KeyError(str(key) + ' already set')\nKeyError: 'x already set'\n>>> from collections import OrderedDict\n>>> class StringOrderedDict(StringKeysMappingMixin,\n...                         SetOnceMappingMixin,\n...                         OrderedDict):\n...     pass\n...\n>>> d = StringOrderedDict()\n>>> d['x'] = 23\n>>> d[42] = 10\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"mixin.py\", line 45, in __setitem__\n    '''\nTypeError: keys must be strings\n>>> d['x'] = 42\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"mixin.py\", line 46, in __setitem__\n    __slots__ = ()\n  File \"mixin.py\", line 24, in __setitem__\n    if key in self:\nKeyError: 'x already set'\n>>>\nIn the example, you will notice that the mixins are combined with other existing classes\n(e.g., dict, defaultdict, OrderedDict), and even one another. When combined, the\nclasses all work together to provide the desired functionality.\n296 \n| \nChapter 8: Classes and Objects",
      "content_length": 1403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": 7,
      "content": "Discussion\nMixin classes appear in various places in the standard library, mostly as a means for\nextending the functionality of other classes similar to as shown. They are also one of\nthe main uses of multiple inheritance. For instance, if you are writing network code,\nyou can often use the ThreadingMixIn from the socketserver module to add thread\nsupport to other network-related classes. For example, here is a multithreaded XML-\nRPC server:\nfrom xmlrpc.server import SimpleXMLRPCServer\nfrom socketserver import ThreadingMixIn\nclass ThreadedXMLRPCServer(ThreadingMixIn, SimpleXMLRPCServer):\n    pass\nIt is also common to find mixins defined in large libraries and frameworks—again,\ntypically to enhance the functionality of existing classes with optional features in some\nway.\nThere is a rich history surrounding the theory of mixin classes. However, rather than\ngetting into all of the details, there are a few important implementation details to keep\nin mind.\nFirst, mixin classes are never meant to be instantiated directly. For example, none of the\nclasses in this recipe work by themselves. They have to be mixed with another class that\nimplements the required mapping functionality. Similarly, the ThreadingMixIn from\nthe socketserver library has to be mixed with an appropriate server class—it can’t be\nused all by itself.\nSecond, mixin classes typically have no state of their own. This means there is no\n__init__() method and no instance variables. In this recipe, the specification of\n__slots__ = () is meant to serve as a strong hint that the mixin classes do not have\ntheir own instance data.\nIf you are thinking about defining a mixin class that has an __init__() method and\ninstance variables, be aware that there is significant peril associated with the fact that\nthe class doesn’t know anything about the other classes it’s going to be mixed with. Thus,\nany instance variables created would have to be named in a way that avoids name clashes.\nIn addition, the __init__() method would have to be programmed in a way that prop‐\nerly invokes the __init__() method of other classes that are mixed in. In general, this\nis difficult to implement since you know nothing about the argument signatures of the\nother classes. At the very least, you would have to implement something very general\nusing *arg, **kwargs. If the __init__() of the mixin class took any arguments of its\nown, those arguments should be specified by keyword only and named in such a way\nto avoid name collisions with other arguments. Here is one possible implementation of\na mixin defining an __init__() and accepting a keyword argument:\n8.18. Extending Classes with Mixins \n| \n297",
      "content_length": 2665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": 7,
      "content": "class RestrictKeysMixin:\n    def __init__(self, *args, _restrict_key_type, **kwargs):\n        self.__restrict_key_type = _restrict_key_type\n        super().__init__(*args, **kwargs)\n    def __setitem__(self, key, value):\n        if not isinstance(key, self.__restrict_key_type):\n            raise TypeError('Keys must be ' + str(self.__restrict_key_type))\n        super().__setitem__(key, value)\nHere is an example that shows how this class might be used:\n>>> class RDict(RestrictKeysMixin, dict):\n...     pass\n...\n>>> d = RDict(_restrict_key_type=str)\n>>> e = RDict([('name','Dave'), ('n',37)], _restrict_key_type=str)\n>>> f = RDict(name='Dave', n=37, _restrict_key_type=str)\n>>> f\n{'n': 37, 'name': 'Dave'}\n>>> f[42] = 10\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"mixin.py\", line 83, in __setitem__\n    raise TypeError('Keys must be ' + str(self.__restrict_key_type))\nTypeError: Keys must be <class 'str'>\n>>>\nIn this example, you’ll notice that initializing an RDict() still takes the arguments\nunderstood by dict(). However, there is an extra keyword argument\nrestrict_key_type that is provided to the mixin class.\nFinally, use of the super() function is an essential and critical part of writing mixin\nclasses. In the solution, the classes redefine certain critical methods, such as\n__getitem__() and __setitem__(). However, they also need to call the original im‐\nplementation of those methods. Using super() delegates to the next class on the method\nresolution order (MRO). This aspect of the recipe, however, is not obvious to novices,\nbecause super() is being used in classes that have no parent (at first glance, it might\nlook like an error). However, in a class definition such as this:\nclass LoggedDict(LoggedMappingMixin, dict):\n    pass\nthe use of super() in LoggedMappingMixin delegates to the next class over in the mul‐\ntiple inheritance list. That is, a call such as super().__getitem__() in LoggedMapping\nMixin actually steps over and invokes dict.__getitem__(). Without this behavior, the\nmixin class wouldn’t work at all.\nAn alternative implementation of mixins involves the use of class decorators. For ex‐\nample, consider this code:\n298 \n| \nChapter 8: Classes and Objects",
      "content_length": 2233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": 7,
      "content": "def LoggedMapping(cls):\n    cls_getitem = cls.__getitem__\n    cls_setitem = cls.__setitem__\n    cls_delitem = cls.__delitem__\n    def __getitem__(self, key):\n        print('Getting ' + str(key))\n        return cls_getitem(self, key)\n    def __setitem__(self, key, value):\n        print('Setting {} = {!r}'.format(key, value))\n        return cls_setitem(self, key, value)\n    def __delitem__(self, key):\n        print('Deleting ' + str(key))\n        return cls_delitem(self, key)\n    cls.__getitem__ = __getitem__\n    cls.__setitem__ = __setitem__\n    cls.__delitem__ = __delitem__\n    return cls\nThis function is applied as a decorator to a class definition. For example:\n@LoggedMapping\nclass LoggedDict(dict):\n    pass\nIf you try it, you’ll find that you get the same behavior, but multiple inheritance is no\nlonger involved. Instead, the decorator has simply performed a bit of surgery on the\nclass definition to replace certain methods. Further details about class decorators can\nbe found in Recipe 9.12.\nSee Recipe 8.13 for an advanced recipe involving both mixins and class decorators.\n8.19. Implementing Stateful Objects or State Machines\nProblem\nYou want to implement a state machine or an object that operates in a number of dif‐\nferent states, but don’t want to litter your code with a lot of conditionals.\nSolution\nIn certain applications, you might have objects that operate differently according to\nsome kind of internal state. For example, consider a simple class representing a\nconnection:\nclass Connection:\n    def __init__(self):\n8.19. Implementing Stateful Objects or State Machines \n| \n299",
      "content_length": 1607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": 7,
      "content": "self.state = 'CLOSED'\n    def read(self):\n        if self.state != 'OPEN':\n            raise RuntimeError('Not open')\n        print('reading')\n    def write(self, data):\n        if self.state != 'OPEN':\n           raise RuntimeError('Not open')\n        print('writing')\n    def open(self):\n        if self.state == 'OPEN':\n           raise RuntimeError('Already open')\n        self.state = 'OPEN'\n    def close(self):\n        if self.state == 'CLOSED':\n           raise RuntimeError('Already closed')\n        self.state = 'CLOSED'\nThis implementation presents a couple of difficulties. First, the code is complicated by\nthe introduction of many conditional checks for the state. Second, the performance is\ndegraded because common operations (e.g., read() and write()) always check the state\nbefore proceeding.\nA more elegant approach is to encode each operational state as a separate class and\narrange for the Connection class to delegate to the state class. For example:\nclass Connection:\n    def __init__(self):\n        self.new_state(ClosedConnectionState)\n    def new_state(self, newstate):\n        self._state = newstate\n    # Delegate to the state class\n    def read(self):\n        return self._state.read(self)\n    def write(self, data):\n        return self._state.write(self, data)\n    def open(self):\n        return self._state.open(self)\n    def close(self):\n        return self._state.close(self)\n# Connection state base class\nclass ConnectionState:\n300 \n| \nChapter 8: Classes and Objects",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": 7,
      "content": "@staticmethod\n    def read(conn):\n        raise NotImplementedError()\n    @staticmethod\n    def write(conn, data):\n        raise NotImplementedError()\n    @staticmethod\n    def open(conn):\n        raise NotImplementedError()\n    @staticmethod\n    def close(conn):\n        raise NotImplementedError()\n# Implementation of different states\nclass ClosedConnectionState(ConnectionState):\n    @staticmethod\n    def read(conn):\n        raise RuntimeError('Not open')\n    @staticmethod\n    def write(conn, data):\n        raise RuntimeError('Not open')\n    @staticmethod\n    def open(conn):\n        conn.new_state(OpenConnectionState)\n    @staticmethod\n    def close(conn):\n        raise RuntimeError('Already closed')\nclass OpenConnectionState(ConnectionState):\n    @staticmethod\n    def read(conn):\n        print('reading')\n    @staticmethod\n    def write(conn, data):\n        print('writing')\n    @staticmethod\n    def open(conn):\n        raise RuntimeError('Already open')\n    @staticmethod\n    def close(conn):\n        conn.new_state(ClosedConnectionState)\nHere is an interactive session that illustrates the use of these classes:\n8.19. Implementing Stateful Objects or State Machines \n| \n301",
      "content_length": 1188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": 7,
      "content": ">>> c = Connection()\n>>> c._state\n<class '__main__.ClosedConnectionState'>\n>>> c.read()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example.py\", line 10, in read\n    return self._state.read(self)\n  File \"example.py\", line 43, in read\n    raise RuntimeError('Not open')\nRuntimeError: Not open\n>>> c.open()\n>>> c._state\n<class '__main__.OpenConnectionState'>\n>>> c.read()\nreading\n>>> c.write('hello')\nwriting\n>>> c.close()\n>>> c._state\n<class '__main__.ClosedConnectionState'>\n>>>\nDiscussion\nWriting code that features a large set of complicated conditionals and intertwined states\nis hard to maintain and explain. The solution presented here avoids that by splitting the\nindividual states into their own classes.\nIt might look a little weird, but each state is implemented by a class with static methods,\neach of which take an instance of Connection as the first argument. This design is based\non a decision to not store any instance data in the different state classes themselves.\nInstead, all instance data should be stored on the Connection instance. The grouping\nof states under a common base class is mostly there to help organize the code and to\nensure that the proper methods get implemented. The NotImplementedError exception\nraised in base class methods is just there to make sure that subclasses provide an im‐\nplementation of the required methods. As an alternative, you might consider the use of\nan abstract base class, as described in Recipe 8.12.\nAn alternative implementation technique concerns direct manipulation of the\n__class__ attribute of instances. Consider this code:\nclass Connection:\n    def __init__(self):\n        self.new_state(ClosedConnection)\n    def new_state(self, newstate):\n        self.__class__ = newstate\n    def read(self):\n302 \n| \nChapter 8: Classes and Objects",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": 8,
      "content": "raise NotImplementedError()\n    def write(self, data):\n        raise NotImplementedError()\n    def open(self):\n        raise NotImplementedError()\n    def close(self):\n        raise NotImplementedError()\nclass ClosedConnection(Connection):\n    def read(self):\n        raise RuntimeError('Not open')\n    def write(self, data):\n        raise RuntimeError('Not open')\n    def open(self):\n        self.new_state(OpenConnection)\n    def close(self):\n        raise RuntimeError('Already closed')\nclass OpenConnection(Connection):\n    def read(self):\n        print('reading')\n    def write(self, data):\n        print('writing')\n    def open(self):\n        raise RuntimeError('Already open')\n    def close(self):\n        self.new_state(ClosedConnection)\nThe main feature of this implementation is that it eliminates an extra level of indirection.\nInstead of having separate Connection and ConnectionState classes, the two classes\nare merged together into one. As the state changes, the instance will change its type, as\nshown here:\n>>> c = Connection()\n>>> c\n<__main__.ClosedConnection object at 0x1006718d0>\n>>> c.read()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"state.py\", line 15, in read\n    raise RuntimeError('Not open')\nRuntimeError: Not open\n>>> c.open()\n8.19. Implementing Stateful Objects or State Machines \n| \n303",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": 8,
      "content": ">>> c\n<__main__.OpenConnection object at 0x1006718d0>\n>>> c.read()\nreading\n>>> c.close()\n>>> c\n<__main__.ClosedConnection object at 0x1006718d0>\n>>>\nObject-oriented purists might be offended by the idea of simply changing the instance\n__class__ attribute. However, it’s technically allowed. Also, it might result in slightly\nfaster code since all of the methods on the connection no longer involve an extra dele‐\ngation step.)\nFinally, either technique is useful in implementing more complicated state machines—\nespecially in code that might otherwise feature large if-elif-else blocks. For example:\n# Original implementation\nclass State:\n    def __init__(self):\n        self.state = 'A'\n    def action(self, x):\n        if state == 'A':\n            # Action for A\n            ...\n            state = 'B'\n        elif state == 'B':\n            # Action for B\n            ...\n            state = 'C'\n        elif state == 'C':\n            # Action for C\n            ...\n            state = 'A'\n# Alternative implementation\nclass State:\n    def __init__(self):\n        self.new_state(State_A)\n    def new_state(self, state):\n        self.__class__ = state\n    def action(self, x):\n        raise NotImplementedError()\nclass State_A(State):\n    def action(self, x):\n         # Action for A\n         ...\n         self.new_state(State_B)\n304 \n| \nChapter 8: Classes and Objects",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": 8,
      "content": "class State_B(State):\n    def action(self, x):\n         # Action for B\n         ...\n         self.new_state(State_C)\nclass State_C(State):\n    def action(self, x):\n         # Action for C\n         ...\n         self.new_state(State_A)\nThis recipe is loosely based on the state design pattern found in Design Patterns: Ele‐\nments of Reusable Object-Oriented Software by Erich Gamma, Richard Helm, Ralph\nJohnson, and John Vlissides (Addison-Wesley, 1995).\n8.20. Calling a Method on an Object Given the Name As a\nString\nProblem\nYou have the name of a method that you want to call on an object stored in a string and\nyou want to execute the method.\nSolution\nFor simple cases, you might use getattr(), like this:\nimport math\nclass Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __repr__(self):\n        return 'Point({!r:},{!r:})'.format(self.x, self.y)\n    def distance(self, x, y):\n        return math.hypot(self.x - x, self.y - y)\np = Point(2, 3)\nd = getattr(p, 'distance')(0, 0)     # Calls p.distance(0, 0)\nAn alternative approach is to use operator.methodcaller(). For example:\nimport operator\noperator.methodcaller('distance', 0, 0)(p)\n8.20. Calling a Method on an Object Given the Name As a String \n| \n305",
      "content_length": 1242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": 8,
      "content": "operator.methodcaller() may be useful if you want to look up a method by name and\nsupply the same arguments over and over again. For instance, if you need to sort an\nentire list of points:\npoints = [\n    Point(1, 2),\n    Point(3, 0),\n    Point(10, -3),\n    Point(-5, -7),\n    Point(-1, 8),\n    Point(3, 2)\n]\n# Sort by distance from origin (0, 0)\npoints.sort(key=operator.methodcaller('distance', 0, 0))\nDiscussion\nCalling a method is actually two separate steps involving an attribute lookup and a\nfunction call. Therefore, to call a method, you simply look up the attribute using get\nattr(), as for any other attribute. To invoke the result as a method, simply treat the\nresult of the lookup as a function.\noperator.methodcaller() creates a callable object, but also fixes any arguments that\nare going to be supplied to the method. All that you need to do is provide the appropriate\nself argument. For example:\n>>> p = Point(3, 4)\n>>> d = operator.methodcaller('distance', 0, 0)\n>>> d(p)\n5.0\n>>>\nInvoking methods using names contained in strings is somewhat common in code that\nemulates case statements or variants of the visitor pattern. See the next recipe for a more\nadvanced example.\n8.21. Implementing the Visitor Pattern\nProblem\nYou need to write code that processes or navigates through a complicated data structure\nconsisting of many different kinds of objects, each of which needs to be handled in a\ndifferent way. For example, walking through a tree structure and performing different\nactions depending on what kind of tree nodes are encountered.\n306 \n| \nChapter 8: Classes and Objects",
      "content_length": 1596,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": 8,
      "content": "Solution\nThe problem addressed by this recipe is one that often arises in programs that build\ndata structures consisting of a large number of different kinds of objects. To illustrate,\nsuppose you are trying to write a program that represents mathematical expressions.\nTo do that, the program might employ a number of classes, like this:\nclass Node:\n    pass\nclass UnaryOperator(Node):\n    def __init__(self, operand):\n        self.operand = operand\nclass BinaryOperator(Node):\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\nclass Add(BinaryOperator):\n    pass\nclass Sub(BinaryOperator):\n    pass\nclass Mul(BinaryOperator):\n    pass\nclass Div(BinaryOperator):\n    pass\nclass Negate(UnaryOperator):\n    pass\nclass Number(Node):\n    def __init__(self, value):\n        self.value = value\nThese classes would then be used to build up nested data structures, like this:\n# Representation of 1 + 2 * (3 - 4) / 5\nt1 = Sub(Number(3), Number(4))\nt2 = Mul(Number(2), t1)\nt3 = Div(t2, Number(5))\nt4 = Add(Number(1), t3)\nThe problem is not the creation of such structures, but in writing code that processes\nthem later. For example, given such an expression, a program might want to do any\nnumber of things (e.g., produce output, generate instructions, perform translation, etc.).\nTo enable general-purpose processing, a common solution is to implement the so-called\n“visitor pattern” using a class similar to this:\n8.21. Implementing the Visitor Pattern \n| \n307",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": 8,
      "content": "class NodeVisitor:\n    def visit(self, node):\n        methname = 'visit_' + type(node).__name__\n        meth = getattr(self, methname, None)\n        if meth is None:\n            meth = self.generic_visit\n        return meth(node)\n    def generic_visit(self, node):\n        raise RuntimeError('No {} method'.format('visit_' + type(node).__name__))\nTo use this class, a programmer inherits from it and implements various methods of the\nform visit_Name(), where Name is substituted with the node type. For example, if you\nwant to evaluate the expression, you could write this:\nclass Evaluator(NodeVisitor):\n    def visit_Number(self, node):\n        return node.value\n    def visit_Add(self, node):\n        return self.visit(node.left) + self.visit(node.right)\n    def visit_Sub(self, node):\n        return self.visit(node.left) - self.visit(node.right)\n    def visit_Mul(self, node):\n        return self.visit(node.left) * self.visit(node.right)\n    def visit_Div(self, node):\n        return self.visit(node.left) / self.visit(node.right)\n    def visit_Negate(self, node):\n        return -node.operand\nHere is an example of how you would use this class using the previously generated\nexpression:\n>>> e = Evaluator()\n>>> e.visit(t4)\n0.6\n>>>\nAs a completely different example, here is a class that translates an expression into\noperations on a simple stack machine:\nclass StackCode(NodeVisitor):\n    def generate_code(self, node):\n        self.instructions = []\n        self.visit(node)\n        return self.instructions\n    def visit_Number(self, node):\n        self.instructions.append(('PUSH', node.value))\n308 \n| \nChapter 8: Classes and Objects",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": 8,
      "content": "def binop(self, node, instruction):\n        self.visit(node.left)\n        self.visit(node.right)\n        self.instructions.append((instruction,))\n    def visit_Add(self, node):\n        self.binop(node, 'ADD')\n    def visit_Sub(self, node):\n        self.binop(node, 'SUB')\n    def visit_Mul(self, node):\n        self.binop(node, 'MUL')\n    def visit_Div(self, node):\n        self.binop(node, 'DIV')\n    def unaryop(self, node, instruction):\n        self.visit(node.operand)\n        self.instructions.append((instruction,))\n    def visit_Negate(self, node):\n        self.unaryop(node, 'NEG')\nHere is an example of this class in action:\n>>> s = StackCode()\n>>> s.generate_code(t4)\n[('PUSH', 1), ('PUSH', 2), ('PUSH', 3), ('PUSH', 4), ('SUB',),\n ('MUL',), ('PUSH', 5), ('DIV',), ('ADD',)]\n>>>\nDiscussion\nThere are really two key ideas in this recipe. The first is a design strategy where code\nthat manipulates a complicated data structure is decoupled from the data structure itself.\nThat is, in this recipe, none of the various Node classes provide any implementation that\ndoes anything with the data. Instead, all of the data manipulation is carried out by\nspecific implementations of the separate NodeVisitor class. This separation makes the\ncode extremely general purpose.\nThe second major idea of this recipe is in the implementation of the visitor class itself.\nIn the visitor, you want to dispatch to a different handling method based on some value\nsuch as the node type. In a naive implementation, you might be inclined to write a huge\nif statement, like this:\nclass NodeVisitor:\n    def visit(self, node):\n        nodetype = type(node).__name__\n        if nodetype == 'Number':\n8.21. Implementing the Visitor Pattern \n| \n309",
      "content_length": 1729,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": 8,
      "content": "return self.visit_Number(node)\n        elif nodetype == 'Add':\n            return self.visit_Add(node)\n        elif nodetype == 'Sub':\n            return self.visit_Sub(node)\n        ...\nHowever, it quickly becomes apparent that you don’t really want to take that approach.\nAside from being incredibly verbose, it runs slowly, and it’s hard to maintain if you ever\nadd or change the kind of nodes being handled. Instead, it’s much better to play a little\ntrick where you form the name of a method and go fetch it with the getattr() function,\nas shown. The generic_visit() method in the solution is a fallback should no matching\nhandler method be found. In this recipe, it raises an exception to alert the programmer\nthat an unexpected node type was encountered.\nWithin each visitor class, it is common for calculations to be driven by recursive calls\nto the visit() method. For example:\nclass Evaluator(NodeVisitor):\n    ...\n    def visit_Add(self, node):\n        return self.visit(node.left) + self.visit(node.right)\nThis recursion is what makes the visitor class traverse the entire data structure. Essen‐\ntially, you keep calling visit() until you reach some sort of terminal node, such as\nNumber in the example. The exact order of the recursion and other operations depend\nentirely on the application.\nIt should be noted that this particular technique of dispatching to a method is also a\ncommon way to emulate the behavior of switch or case statements from other languages.\nFor example, if you are writing an HTTP framework, you might have classes that do a\nsimilar kind of dispatch:\nclass HTTPHandler:\n    def handle(self, request):\n        methname = 'do_' + request.request_method\n        getattr(self, methname)(request)\n    def do_GET(self, request):\n        ...\n    def do_POST(self, request):\n        ...\n    def do_HEAD(self, request):\n        ...\nOne weakness of the visitor pattern is its heavy reliance on recursion. If you try to apply\nit to a deeply nested structure, it’s possible that you will hit Python’s recursion depth\nlimit (see sys.getrecursionlimit()). To avoid this problem, you can make certain\nchoices in your data structures. For example, you can use normal Python lists instead\nof linked lists or try to aggregate more data in each node to make the data more shallow.\n310 \n| \nChapter 8: Classes and Objects",
      "content_length": 2338,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": 8,
      "content": "You can also try to employ nonrecursive traversal algorithms using generators or iter‐\nators as discussed in Recipe 8.22.\nUse of the visitor pattern is extremely common in programs related to parsing and\ncompiling. One notable implementation can be found in Python’s own ast module. In\naddition to allowing traversal of tree structures, it provides a variation that allows a data\nstructure to be rewritten or transformed as it is traversed (e.g., nodes added or removed).\nLook at the source for ast for more details. Recipe 9.24 shows an example of using the\nast module to process Python source code.\n8.22. Implementing the Visitor Pattern Without Recursion\nProblem\nYou’re writing code that navigates through a deeply nested tree structure using the visitor\npattern, but it blows up due to exceeding the recursion limit. You’d like to eliminate the\nrecursion, but keep the programming style of the visitor pattern.\nSolution\nClever use of generators can sometimes be used to eliminate recursion from algorithms\ninvolving tree traversal or searching. In Recipe 8.21, a visitor class was presented. Here\nis an alternative implementation of that class that drives the computation in an entirely\ndifferent way using a stack and generators:\nimport types\nclass Node:\n    pass\nimport types\nclass NodeVisitor:\n    def visit(self, node):\n        stack = [ node ]\n        last_result = None\n        while stack:\n            try:\n                last = stack[-1]\n                if isinstance(last, types.GeneratorType):\n                    stack.append(last.send(last_result))\n                    last_result = None\n                elif isinstance(last, Node):\n                    stack.append(self._visit(stack.pop()))\n                else:\n                    last_result = stack.pop()\n            except StopIteration:\n                stack.pop()\n        return last_result\n8.22. Implementing the Visitor Pattern Without Recursion \n| \n311",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": 8,
      "content": "def _visit(self, node):\n        methname = 'visit_' + type(node).__name__\n        meth = getattr(self, methname, None)\n        if meth is None:\n            meth = self.generic_visit\n        return meth(node)\n    def generic_visit(self, node):\n        raise RuntimeError('No {} method'.format('visit_' + type(node).__name__))\nIf you use this class, you’ll find that it still works with existing code that might have used\nrecursion. In fact, you can use it as a drop-in replacement for the visitor implementation\nin the prior recipe. For example, consider the following code, which involves expression\ntrees:\nclass UnaryOperator(Node):\n    def __init__(self, operand):\n        self.operand = operand\nclass BinaryOperator(Node):\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\nclass Add(BinaryOperator):\n    pass\nclass Sub(BinaryOperator):\n    pass\nclass Mul(BinaryOperator):\n    pass\nclass Div(BinaryOperator):\n    pass\nclass Negate(UnaryOperator):\n    pass\nclass Number(Node):\n    def __init__(self, value):\n        self.value = value\n# A sample visitor class that evaluates expressions\nclass Evaluator(NodeVisitor):\n    def visit_Number(self, node):\n        return node.value\n    def visit_Add(self, node):\n        return self.visit(node.left) + self.visit(node.right)\n312 \n| \nChapter 8: Classes and Objects",
      "content_length": 1348,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": 8,
      "content": "def visit_Sub(self, node):\n        return self.visit(node.left) - self.visit(node.right)\n    def visit_Mul(self, node):\n        return self.visit(node.left) * self.visit(node.right)\n    def visit_Div(self, node):\n        return self.visit(node.left) / self.visit(node.right)\n    def visit_Negate(self, node):\n        return -self.visit(node.operand)\nif __name__ == '__main__':\n    # 1 + 2*(3-4) / 5\n    t1 = Sub(Number(3), Number(4))\n    t2 = Mul(Number(2), t1)\n    t3 = Div(t2, Number(5))\n    t4 = Add(Number(1), t3)\n    # Evaluate it\n    e = Evaluator()\n    print(e.visit(t4))     # Outputs 0.6\nThe preceding code works for simple expressions. However, the implementation of\nEvaluator uses recursion and crashes if things get too nested. For example:\n>>> a = Number(0)\n>>> for n in range(1, 100000):\n...     a = Add(a, Number(n))\n...\n>>> e = Evaluator()\n>>> e.visit(a)\nTraceback (most recent call last):\n...\n  File \"visitor.py\", line 29, in _visit\n    return meth(node)\n  File \"visitor.py\", line 67, in visit_Add\n    return self.visit(node.left) + self.visit(node.right)\nRuntimeError: maximum recursion depth exceeded\n>>>\nNow let’s change the Evaluator class ever so slightly to the following:\nclass Evaluator(NodeVisitor):\n    def visit_Number(self, node):\n        return node.value\n    def visit_Add(self, node):\n        yield (yield node.left) + (yield node.right)\n    def visit_Sub(self, node):\n        yield (yield node.left) - (yield node.right)\n8.22. Implementing the Visitor Pattern Without Recursion \n| \n313",
      "content_length": 1518,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": 8,
      "content": "def visit_Mul(self, node):\n        yield (yield node.left) * (yield node.right)\n    def visit_Div(self, node):\n        yield (yield node.left) / (yield node.right)\n    def visit_Negate(self, node):\n        yield -(yield node.operand)\nIf you try the same recursive experiment, you’ll find that it suddenly works. It’s magic!\n>>> a = Number(0)\n>>> for n in range(1,100000):\n...     a = Add(a, Number(n))\n...\n>>> e = Evaluator()\n>>> e.visit(a)\n4999950000\n>>>\nIf you want to add custom processing into any of the methods, it still works. For example:\nclass Evaluator(NodeVisitor):\n    ...\n    def visit_Add(self, node):\n        print('Add:', node)\n        lhs = yield node.left\n        print('left=', lhs)\n        rhs = yield node.right\n        print('right=', rhs)\n        yield lhs + rhs\n    ...\nHere is some sample output:\n>>> e = Evaluator()\n>>> e.visit(t4)\nAdd: <__main__.Add object at 0x1006a8d90>\nleft= 1\nright= -0.4\n0.6\n>>>\nDiscussion\nThis recipe nicely illustrates how generators and coroutines can perform mind-bending\ntricks involving program control flow, often to great advantage. To understand this\nrecipe, a few key insights are required.\nFirst, in problems related to tree traversal, a common implementation strategy for\navoiding recursion is to write algorithms involving a stack or queue. For example, depth-\nfirst traversal can be implemented entirely by pushing nodes onto a stack when first\nencountered and then popping them off once processing has finished. The central core\n314 \n| \nChapter 8: Classes and Objects",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": 8,
      "content": "of the visit() method in the solution is built around this idea. The algorithm starts by\npushing the initial node onto the stack list and runs until the stack is empty. During\nexecution, the stack will grow according to the depth of the underlying tree structure.\nThe second insight concerns the behavior of the yield statement in generators. When\nyield is encountered, the behavior of a generator is to emit a value and to suspend. This\nrecipe uses this as a replacement for recursion. For example, instead of writing a recur‐\nsive expression like this:\nvalue = self.visit(node.left)\nyou replace it with the following:\nvalue = yield node.left\nBehind the scenes, this sends the node in question (node.left) back to the visit()\nmethod. The visit() method then carries out the execution of the appropriate vis\nit_Name() method for that node. In some sense, this is almost the opposite of recursion.\nThat is, instead of calling visit() recursively to move the algorithm forward, the yield\nstatement is being used to temporarily back out of the computation in progress. Thus,\nthe yield is essentially a signal that tells the algorithm that the yielded node needs to\nbe processed first before further progress can be made.\nThe final part of this recipe concerns propagation of results. When generator functions\nare used, you can no longer use return statements to emit values (doing so will cause\na SyntaxError exception). Thus, the yield statement has to do double duty to cover\nthe case. In this recipe, if the value produced by a yield statement is a non-Node type,\nit is assumed to be a value that will be propagated to the next step of the calculation.\nThis is the purpose of the last_return variable in the code. Typically, this would hold\nthe last value yielded by a visit method. That value would then be sent into the previously\nexecuting method, where it would show up as the return value from a yield statement.\nFor example, in this code:\nvalue = yield node.left\nThe value variable gets the value of last_return, which is the result returned by the\nvisitor method invoked for node.left.\nAll of these aspects of the recipe are found in this fragment of code:\ntry:\n    last = stack[-1]\n    if isinstance(last, types.GeneratorType):\n        stack.append(last.send(last_result))\n        last_result = None\n    elif isinstance(last, Node):\n        stack.append(self._visit(stack.pop()))\n    else:\n        last_result = stack.pop()\n8.22. Implementing the Visitor Pattern Without Recursion \n| \n315",
      "content_length": 2496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": 8,
      "content": "except StopIteration:\n    stack.pop()\nThe code works by simply looking at the top of the stack and deciding what to do next.\nIf it’s a generator, then its send() method is invoked with the last result (if any) and the\nresult appended onto the stack for further processing. The value returned by send() is\nthe same value that was given to the yield statement. Thus, in a statement such as yield\nnode.left, the Node instance node.left is returned by send() and placed on the top\nof the stack.\nIf the top of the stack is a Node instance, then it is replaced by the result of calling the\nappropriate visit method for that node. This is where the underlying recursion is being\neliminated. Instead of the various visit methods directly calling visit() recursively, it\ntakes place here. As long as the methods use yield, it all works out.\nFinally, if the top of the stack is anything else, it’s assumed to be a return value of some\nkind. It just gets popped off the stack and placed into last_result. If the next item on\nthe stack is a generator, then it gets sent in as a return value for the yield. It should be\nnoted that the final return value of visit() is also set to last_result. This is what\nmakes this recipe work with a traditional recursive implementation. If no generators\nare being used, this value simply holds the value given to any return statements used\nin the code.\nOne potential danger of this recipe concerns the distinction between yielding Node and\nnon-Node values. In the implementation, all Node instances are automatically traversed.\nThis means that you can’t use a Node as a return value to be propagated. In practice, this\nmay not matter. However, if it does, you might need to adapt the algorithm slightly. For\nexample, possibly by introducing another class into the mix, like this:\nclass Visit:\n    def __init__(self, node):\n        self.node = node\nclass NodeVisitor:\n    def visit(self, node):\n        stack = [ Visit(node) ]\n        last_result = None\n        while stack:\n            try:\n                last = stack[-1]\n                if isinstance(last, types.GeneratorType):\n                    stack.append(last.send(last_result))\n                    last_result = None\n                elif isinstance(last, Visit):\n                    stack.append(self._visit(stack.pop().node))\n                else:\n                    last_result = stack.pop()\n            except StopIteration:\n316 \n| \nChapter 8: Classes and Objects",
      "content_length": 2452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": 8,
      "content": "stack.pop()\n        return last_result\n    def _visit(self, node):\n        methname = 'visit_' + type(node).__name__\n        meth = getattr(self, methname, None)\n        if meth is None:\n            meth = self.generic_visit\n        return meth(node)\n    def generic_visit(self, node):\n        raise RuntimeError('No {} method'.format('visit_' + type(node).__name__))\nWith this implementation, the various visitor methods would now look like this:\nclass Evaluator(NodeVisitor):\n    ...\n    def visit_Add(self, node):\n        yield (yield Visit(node.left)) + (yield Visit(node.right))\n    def visit_Sub(self, node):\n        yield (yield Visit(node.left)) - (yield Visit(node.right))\n    ...\nHaving seen this recipe, you might be inclined to investigate a solution that doesn’t\ninvolve yield. However, doing so will lead to code that has to deal with many of the\nsame issues presented here. For example, to eliminate recursion, you’ll need to maintain\na stack. You’ll also need to come up with some scheme for managing the traversal and\ninvoking various visitor-related logic. Without generators, this code ends up being a\nvery messy mix of stack manipulation, callback functions, and other constructs. Frankly,\nthe main benefit of using yield is that you can write nonrecursive code in an elegant\nstyle that looks almost exactly like the recursive implementation.\n8.23. Managing Memory in Cyclic Data Structures\nProblem\nYour program creates data structures with cycles (e.g., trees, graphs, observer patterns,\netc.), but you are experiencing problems with memory management.\nSolution\nA simple example of a cyclic data structure is a tree structure where a parent points to\nits children and the children point back to their parent. For code like this, you should\nconsider making one of the links a weak reference using the weakref library. For\nexample:\n8.23. Managing Memory in Cyclic Data Structures \n| \n317",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": 8,
      "content": "import weakref\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self._parent = None\n        self.children = []\n    def __repr__(self):\n        return 'Node({!r:})'.format(self.value)\n    # property that manages the parent as a weak-reference\n    @property\n    def parent(self):\n        return self._parent if self._parent is None else self._parent()\n    @parent.setter\n    def parent(self, node):\n        self._parent = weakref.ref(node)\n    def add_child(self, child):\n        self.children.append(child)\n        child.parent = self\nThis implementation allows the parent to quietly die. For example:\n>>> root = Node('parent')\n>>> c1 = Node('child')\n>>> root.add_child(c1)\n>>> print(c1.parent)\nNode('parent')\n>>> del root\n>>> print(c1.parent)\nNone\n>>>\nDiscussion\nCyclic data structures are a somewhat tricky aspect of Python that require careful study\nbecause the usual rules of garbage collection often don’t apply. For example, consider\nthis code:\n# Class just to illustrate when deletion occurs\nclass Data:\n    def __del__(self):\n        print('Data.__del__')\n# Node class involving a cycle\nclass Node:\n    def __init__(self):\n        self.data = Data()\n        self.parent = None\n318 \n| \nChapter 8: Classes and Objects",
      "content_length": 1251,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": 8,
      "content": "self.children = []\n    def add_child(self, child):\n        self.children.append(child)\n        child.parent = self\nNow, using this code, try some experiments to see some subtle issues with garbage\ncollection:\n>>> a = Data()\n>>> del a               # Immediately deleted\nData.__del__\n>>> a = Node()\n>>> del a               # Immediately deleted\nData.__del__\n>>> a = Node()\n>>> a.add_child(Node())\n>>> del a               # Not deleted (no message)\n>>>\nAs you can see, objects are deleted immediately all except for the last case involving a\ncycle. The reason is that Python’s garbage collection is based on simple reference count‐\ning. When the reference count of an object reaches 0, it is immediately deleted. For\ncyclic data structures, however, this never happens. Thus, in the last part of the example,\nthe parent and child nodes refer to each other, keeping the reference count nonzero.\nTo deal with cycles, there is a separate garbage collector that runs periodically. However,\nas a general rule, you never know when it might run. Consequently, you never really\nknow when cyclic data structures might get collected. If necessary, you can force garbage\ncollection, but doing so is a bit clunky:\n>>> import gc\n>>> gc.collect()     # Force collection\nData.__del__\nData.__del__\n>>>\nAn even worse problem occurs if the objects involved in a cycle define their own\n__del__() method. For example, suppose the code looked like this:\n# Class just to illustrate when deletion occurs\nclass Data:\n    def __del__(self):\n        print('Data.__del__')\n# Node class involving a cycle\nclass Node:\n    def __init__(self):\n        self.data = Data()\n        self.parent = None\n        self.children = []\n    # NEVER DEFINE LIKE THIS.\n8.23. Managing Memory in Cyclic Data Structures \n| \n319",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": 8,
      "content": "# Only here to illustrate pathological behavior\n    def __del__(self):\n        del self.data\n        del.parent\n        del.children\n    def add_child(self, child):\n        self.children.append(child)\n        child.parent = self\nIn this case, the data structures will never be garbage collected at all and your program\nwill leak memory! If you try it, you’ll see that the Data.__del__ message never appears\nat all—even after a forced garbage collection:\n>>> a = Node()\n>>> a.add_child(Node()\n>>> del a             # No message (not collected)\n>>> import gc\n>>> gc.collect()      # No message (not collected)\n>>>\nWeak references solve this problem by eliminating reference cycles. Essentially, a weak\nreference is a pointer to an object that does not increase its reference count. You create\nweak references using the weakref library. For example:\n>>> import weakref\n>>> a = Node()\n>>> a_ref = weakref.ref(a)\n>>> a_ref\n<weakref at 0x100581f70; to 'Node' at 0x1005c5410>\n>>>\nTo dereference a weak reference, you call it like a function. If the referenced object still\nexists, it is returned. Otherwise, None is returned. Since the reference count of the orig‐\ninal object wasn’t increased, it can be deleted normally. For example:\n>>> print(a_ref())\n<__main__.Node object at 0x1005c5410>\n>>> del a\nData.__del__\n>>> print(a_ref())\nNone\n>>>\nBy using weak references, as shown in the solution, you’ll find that there are no longer\nany reference cycles and that garbage collection occurs immediately once a node is no\nlonger being used. See Recipe 8.25 for another example involving weak references.\n320 \n| \nChapter 8: Classes and Objects",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": 8,
      "content": "8.24. Making Classes Support Comparison Operations\nProblem\nYou’d like to be able to compare instances of your class using the standard comparison\noperators (e.g., >=, !=, <=, etc.), but without having to write a lot of special methods.\nSolution\nPython classes can support comparison by implementing a special method for each\ncomparison operator. For example, to support the >= operator, you define a __ge__()\nmethod in the classes. Although defining a single method is usually no problem, it\nquickly gets tedious to create implementations of every possible comparison operator.\nThe functools.total_ordering decorator can be used to simplify this process. To use\nit, you decorate a class with it, and define __eq__() and one other comparison method\n(__lt__, __le__, __gt__, or __ge__). The decorator then fills in the other comparison\nmethods for you.\nAs an example, let’s build some houses and add some rooms to them, and then perform\ncomparisons based on the size of the houses:\nfrom functools import total_ordering\nclass Room:\n    def __init__(self, name, length, width):\n        self.name = name\n        self.length = length\n        self.width = width\n        self.square_feet = self.length * self.width\n@total_ordering\nclass House:\n    def __init__(self, name, style):\n        self.name = name\n        self.style = style\n        self.rooms = list()\n    @property\n    def living_space_footage(self):\n        return sum(r.square_feet for r in self.rooms)\n    def add_room(self, room):\n        self.rooms.append(room)\n    def __str__(self):\n        return '{}: {} square foot {}'.format(self.name,\n                                              self.living_space_footage,\n                                              self.style)\n8.24. Making Classes Support Comparison Operations \n| \n321",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": 8,
      "content": "def __eq__(self, other):\n        return self.living_space_footage == other.living_space_footage\n    def __lt__(self, other):\n        return self.living_space_footage < other.living_space_footage\nHere, the House class has been decorated with @total_ordering. Definitions of\n__eq__() and __lt__() are provided to compare houses based on the total square\nfootage of their rooms. This minimum definition is all that is required to make all of\nthe other comparison operations work. For example:\n# Build a few houses, and add rooms to them\nh1 = House('h1', 'Cape')\nh1.add_room(Room('Master Bedroom', 14, 21))\nh1.add_room(Room('Living Room', 18, 20))\nh1.add_room(Room('Kitchen', 12, 16))\nh1.add_room(Room('Office', 12, 12))\nh2 = House('h2', 'Ranch')\nh2.add_room(Room('Master Bedroom', 14, 21))\nh2.add_room(Room('Living Room', 18, 20))\nh2.add_room(Room('Kitchen', 12, 16))\nh3 = House('h3', 'Split')\nh3.add_room(Room('Master Bedroom', 14, 21))\nh3.add_room(Room('Living Room', 18, 20))\nh3.add_room(Room('Office', 12, 16))\nh3.add_room(Room('Kitchen', 15, 17))\nhouses = [h1, h2, h3]\nprint('Is h1 bigger than h2?', h1 > h2) # prints True\nprint('Is h2 smaller than h3?', h2 < h3) # prints True\nprint('Is h2 greater than or equal to h1?', h2 >= h1) # Prints False\nprint('Which one is biggest?', max(houses)) # Prints 'h3: 1101-square-foot Split'\nprint('Which is smallest?', min(houses)) # Prints 'h2: 846-square-foot Ranch'\nDiscussion\nIf you’ve written the code to make a class support all of the basic comparison operators,\nthen total_ordering probably doesn’t seem all that magical: it literally defines a map‐\nping from each of the comparison-supporting methods to all of the other ones that\nwould be required. So, if you defined __lt__() in your class as in the solution, it is used\nto build all of the other comparison operators. It’s really just filling in the class with\nmethods like this:\nclass House:\n    def __eq__(self, other):\n        ...\n    def __lt__(self, other):\n        ...\n322 \n| \nChapter 8: Classes and Objects",
      "content_length": 2015,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": 8,
      "content": "# Methods created by @total_ordering\n    __le__ = lambda self, other: self < other or self == other\n    __gt__ = lambda self, other: not (self < other or self == other)\n    __ge__ = lambda self, other: not (self < other)\n    __ne__ = lambda self, other: not self == other\nSure, it’s not hard to write these methods yourself, but @total_ordering simply takes\nthe guesswork out of it.\n8.25. Creating Cached Instances\nProblem\nWhen creating instances of a class, you want to return a cached reference to a previous\ninstance created with the same arguments (if any).\nSolution\nThe problem being addressed in this recipe sometimes arises when you want to ensure\nthat there is only one instance of a class created for a set of input arguments. Practical\nexamples include the behavior of libraries, such as the logging module, that only want\nto associate a single logger instance with a given name. For example:\n>>> import logging\n>>> a = logging.getLogger('foo')\n>>> b = logging.getLogger('bar')\n>>> a is b\nFalse\n>>> c = logging.getLogger('foo')\n>>> a is c\nTrue\n>>>\nTo implement this behavior, you should make use of a factory function that’s separate\nfrom the class itself. For example:\n# The class in question\nclass Spam:\n    def __init__(self, name):\n        self.name = name\n# Caching support\nimport weakref\n_spam_cache = weakref.WeakValueDictionary()\ndef get_spam(name):\n    if name not in _spam_cache:\n        s = Spam(name)\n        _spam_cache[name] = s\n    else:\n8.25. Creating Cached Instances \n| \n323",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": 8,
      "content": "s = _spam_cache[name]\n    return s\nIf you use this implementation, you’ll find that it behaves in the manner shown earlier:\n>>> a = get_spam('foo')\n>>> b = get_spam('bar')\n>>> a is b\nFalse\n>>> c = get_spam('foo')\n>>> a is c\nTrue\n>>>\nDiscussion\nWriting a special factory function is often a simple approach for altering the normal\nrules of instance creation. One question that often arises at this point is whether or not\na more elegant approach could be taken.\nFor example, you might consider a solution that redefines the __new__() method of a\nclass as follows:\n# Note: This code doesn't quite work\nimport weakref\nclass Spam:\n    _spam_cache = weakref.WeakValueDictionary()\n    def __new__(cls, name):\n        if name in cls._spam_cache:\n            return cls._spam_cache[name]\n        else:\n            self = super().__new__(cls)\n            cls._spam_cache[name] = self\n            return self\n    def __init__(self, name):\n        print('Initializing Spam')\n        self.name = name\nAt first glance, it seems like this code might do the job. However, a major problem is\nthat the __init__() method always gets called, regardless of whether the instance was\ncached or not. For example:\n>>> s = Spam('Dave')\nInitializing Spam\n>>> t = Spam('Dave')\nInitializing Spam\n>>> s is t\nTrue\n>>>\n324 \n| \nChapter 8: Classes and Objects",
      "content_length": 1326,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": 8,
      "content": "That behavior is probably not what you want. So, to solve the problem of caching without\nreinitialization, you need to take a slightly different approach.\nThe use of weak references in this recipe serves an important purpose related to garbage\ncollection, as described in Recipe 8.23. When maintaining a cache of instances, you\noften only want to keep items in the cache as long as they’re actually being used some‐\nwhere in the program. A WeakValueDictionary instance only holds onto the referenced\nitems as long as they exist somewhere else. Otherwise, the dictionary keys disappear\nwhen instances are no longer being used. Observe:\n>>> a = get_spam('foo')\n>>> b = get_spam('bar')\n>>> c = get_spam('foo')\n>>> list(_spam_cache)\n['foo', 'bar']\n>>> del a\n>>> del c\n>>> list(_spam_cache)\n['bar']\n>>> del b\n>>> list(_spam_cache)\n[]\n>>>\nFor many programs, the bare-bones code shown in this recipe will often suffice. How‐\never, there are a number of more advanced implementation techniques that can be\nconsidered.\nOne immediate concern with this recipe might be its reliance on global variables and a\nfactory function that’s decoupled from the original class definition. One way to clean\nthis up is to put the caching code into a separate manager class and glue things together\nlike this:\nimport weakref\nclass CachedSpamManager:\n    def __init__(self):\n        self._cache = weakref.WeakValueDictionary()\n    def get_spam(self, name):\n        if name not in self._cache:\n            s = Spam(name)\n            self._cache[name] = s\n        else:\n            s = self._cache[name]\n        return s\n    def clear(self):\n        self._cache.clear()\nclass Spam:\n    manager = CachedSpamManager()\n8.25. Creating Cached Instances \n| \n325",
      "content_length": 1727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": 8,
      "content": "def __init__(self, name):\n        self.name = name\ndef get_spam(name):\n    return Spam.manager.get_spam(name)\nOne feature of this approach is that it affords a greater degree of potential flexibility. For\nexample, different kinds of management schemes could be be implemented (as separate\nclasses) and attached to the Spam class as a replacement for the default caching imple‐\nmentation. None of the other code (e.g., get_spam) would need to be changed to make\nit work.\nAnother design consideration is whether or not you want to leave the class definition\nexposed to the user. If you do nothing, a user can easily make instances, bypassing the\ncaching mechanism:\n>>> a = Spam('foo')\n>>> b = Spam('foo')\n>>> a is b\nFalse\n>>>\nIf preventing this is important, you can take certain steps to avoid it. For example, you\nmight give the class a name starting with an underscore, such as _Spam, which at least\ngives the user a clue that they shouldn’t access it directly.\nAlternatively, if you want to give users a stronger hint that they shouldn’t instantiate\nSpam instances directly, you can make __init__() raise an exception and use a class\nmethod to make an alternate constructor like this:\nclass Spam:\n    def __init__(self, *args, **kwargs):\n        raise RuntimeError(\"Can't instantiate directly\")\n    # Alternate constructor\n    @classmethod\n    def _new(cls, name):\n        self = cls.__new__(cls)\n        self.name = name\nTo use this, you modify the caching code to use Spam._new() to create instances instead\nof the usual call to Spam(). For example:\nimport weakref\nclass CachedSpamManager:\n    def __init__(self):\n        self._cache = weakref.WeakValueDictionary()\n    def get_spam(self, name):\n        if name not in self._cache:\n            s = Spam._new(name)          # Modified creation\n326 \n| \nChapter 8: Classes and Objects",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": 8,
      "content": "self._cache[name] = s\n        else:\n            s = self._cache[name]\n        return s\nAlthough there are more extreme measures that can be taken to hide the visibility of\nthe Spam class, it’s probably best to not overthink the problem. Using an underscore on\nthe name or defining a class method constructor is usually enough for programmers to\nget a hint.\nCaching and other creational patterns can often be solved in a more elegant (albeit\nadvanced) manner through the use of metaclasses. See Recipe 9.13.\n8.25. Creating Cached Instances \n| \n327",
      "content_length": 546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": 8,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 347,
      "chapter": 8,
      "content": "CHAPTER 9\nMetaprogramming\nOne of the most important mantras of software development is “don’t repeat yourself.”\nThat is, any time you are faced with a problem of creating highly repetitive code (or\ncutting or pasting source code), it often pays to look for a more elegant solution. In\nPython, such problems are often solved under the category of “metaprogramming.” In\na nutshell, metaprogramming is about creating functions and classes whose main goal\nis to manipulate code (e.g., modifying, generating, or wrapping existing code). The main\nfeatures for this include decorators, class decorators, and metaclasses. However, a variety\nof other useful topics—including signature objects, execution of code with exec(), and\ninspecting the internals of functions and classes—enter the picture. The main purpose\nof this chapter is to explore various metaprogramming techniques and to give examples\nof how they can be used to customize the behavior of Python to your own whims.\n9.1. Putting a Wrapper Around a Function\nProblem\nYou want to put a wrapper layer around a function that adds extra processing (e.g.,\nlogging, timing, etc.).\nSolution\nIf you ever need to wrap a function with extra code, define a decorator function. For\nexample:\nimport time\nfrom functools import wraps\ndef timethis(func):\n    '''\n    Decorator that reports the execution time.\n329",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": 8,
      "content": "'''\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(func.__name__, end-start)\n        return result\n    return wrapper\nHere is an example of using the decorator:\n>>> @timethis\n... def countdown(n):\n...     '''\n...     Counts down\n...     '''\n...     while n > 0:\n...             n -= 1\n...\n>>> countdown(100000)\ncountdown 0.008917808532714844\n>>> countdown(10000000)\ncountdown 0.87188299392912\n>>>\nDiscussion\nA decorator is a function that accepts a function as input and returns a new function as\noutput. Whenever you write code like this:\n@timethis\ndef countdown(n):\n    ...\nit’s the same as if you had performed these separate steps:\ndef countdown(n):\n    ...\ncountdown = timethis(countdown)\nAs an aside, built-in decorators such as @staticmethod, @classmethod, and @proper\nty work in the same way. For example, these two code fragments are equivalent:\nclass A:\n    @classmethod\n    def method(cls):\n        pass\nclass B:\n    # Equivalent definition of a class method\n    def method(cls):\n330 \n| \nChapter 9: Metaprogramming",
      "content_length": 1136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": 8,
      "content": "pass\n    method = classmethod(method)\nThe code inside a decorator typically involves creating a new function that accepts any\narguments using *args and **kwargs, as shown with the wrapper() function in this\nrecipe. Inside this function, you place a call to the original input function and return its\nresult. However, you also place whatever extra code you want to add (e.g., timing). The\nnewly created function wrapper is returned as a result and takes the place of the original\nfunction.\nIt’s critical to emphasize that decorators generally do not alter the calling signature or\nreturn value of the function being wrapped. The use of *args and **kwargs is there to\nmake sure that any input arguments can be accepted. The return value of a decorator is\nalmost always the result of calling func(*args, **kwargs), where func is the original\nunwrapped function.\nWhen first learning about decorators, it is usually very easy to get started with some\nsimple examples, such as the one shown. However, if you are going to write decorators\nfor real, there are some subtle details to consider. For example, the use of the decorator\n@wraps(func) in the solution is an easy to forget but important technicality related to\npreserving function metadata, which is described in the next recipe. The next few recipes\nthat follow fill in some details that will be important if you wish to write decorator\nfunctions of your own.\n9.2. Preserving Function Metadata When Writing\nDecorators\nProblem\nYou’ve written a decorator, but when you apply it to a function, important metadata\nsuch as the name, doc string, annotations, and calling signature are lost.\nSolution\nWhenever you define a decorator, you should always remember to apply the @wraps\ndecorator from the functools library to the underlying wrapper function. For example:\nimport time\nfrom functools import wraps\ndef timethis(func):\n    '''\n    Decorator that reports the execution time.\n    '''\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n9.2. Preserving Function Metadata When Writing Decorators \n| \n331",
      "content_length": 2050,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": 8,
      "content": "start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(func.__name__, end-start)\n        return result\n    return wrapper\nHere is an example of using the decorator and examining the resulting function meta‐\ndata:\n>>> @timethis\n... def countdown(n:int):\n...     '''\n...     Counts down\n...     '''\n...     while n > 0:\n...             n -= 1\n...\n>>> countdown(100000)\ncountdown 0.008917808532714844\n>>> countdown.__name__\n'countdown'\n>>> countdown.__doc__\n'\\n\\tCounts down\\n\\t'\n>>> countdown.__annotations__\n{'n': <class 'int'>}\n>>>\nDiscussion\nCopying decorator metadata is an important part of writing decorators. If you forget to\nuse @wraps, you’ll find that the decorated function loses all sorts of useful information.\nFor instance, if omitted, the metadata in the last example would look like this:\n>>> countdown.__name__\n'wrapper'\n>>> countdown.__doc__\n>>> countdown.__annotations__\n{}\n>>>\nAn important feature of the @wraps decorator is that it makes the wrapped function\navailable to you in the __wrapped__ attribute. For example, if you want to access the\nwrapped function directly, you could do this:\n>>> countdown.__wrapped__(100000)\n>>>\nThe presence of the __wrapped__ attribute also makes decorated functions properly\nexpose the underlying signature of the wrapped function. For example:\n332 \n| \nChapter 9: Metaprogramming",
      "content_length": 1378,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": 8,
      "content": ">>> from inspect import signature\n>>> print(signature(countdown))\n(n:int)\n>>>\nOne common question that sometimes arises is how to make a decorator that directly\ncopies the calling signature of the original function being wrapped (as opposed to using\n*args and **kwargs). In general, this is difficult to implement without resorting to some\ntrick involving the generator of code strings and exec(). Frankly, you’re usually best off\nusing @wraps and relying on the fact that the underlying function signature can be\npropagated by access to the underlying __wrapped__ attribute. See Recipe 9.16 for more\ninformation about signatures.\n9.3. Unwrapping a Decorator\nProblem\nA decorator has been applied to a function, but you want to “undo” it, gaining access to\nthe original unwrapped function.\nSolution\nAssuming that the decorator has been implemented properly using @wraps (see\nRecipe 9.2), you can usually gain access to the original function by accessing the __wrap\nped__ attribute. For example:\n>>> @somedecorator\n>>> def add(x, y):\n...     return x + y\n...\n>>> orig_add = add.__wrapped__\n>>> orig_add(3, 4)\n7\n>>>\nDiscussion\nGaining direct access to the unwrapped function behind a decorator can be useful for\ndebugging, introspection, and other operations involving functions. However, this\nrecipe only works if the implementation of a decorator properly copies metadata using\n@wraps from the functools module or sets the __wrapped__ attribute directly.\nIf multiple decorators have been applied to a function, the behavior of accessing __wrap\nped__ is currently undefined and should probably be avoided. In Python 3.3, it bypasses\nall of the layers. For example, suppose you have code like this:\n9.3. Unwrapping a Decorator \n| \n333",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": 8,
      "content": "from functools import wraps\ndef decorator1(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print('Decorator 1')\n        return func(*args, **kwargs)\n    return wrapper\ndef decorator2(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print('Decorator 2')\n        return func(*args, **kwargs)\n    return wrapper\n@decorator1\n@decorator2\ndef add(x, y):\n    return x + y\nHere is what happens when you call the decorated function and the original function\nthrough __wrapped__:\n>>> add(2, 3)\nDecorator 1\nDecorator 2\n5\n>>> add.__wrapped__(2, 3)\n5\n>>>\nHowever, this behavior has been reported as a bug (see http://bugs.python.org/\nissue17482) and may be changed to explose the proper decorator chain in a future re‐\nlease.\nLast, but not least, be aware that not all decorators utilize @wraps, and thus, they may\nnot work as described. In particular, the built-in decorators @staticmethod and @class\nmethod create descriptor objects that don’t follow this convention (instead, they store\nthe original function in a __func__ attribute). Your mileage may vary.\n9.4. Defining a Decorator That Takes Arguments\nProblem\nYou want to write a decorator function that takes arguments.\n334 \n| \nChapter 9: Metaprogramming",
      "content_length": 1233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": 8,
      "content": "Solution\nLet’s illustrate the process of accepting arguments with an example. Suppose you want\nto write a decorator that adds logging to a function, but allows the user to specify the\nlogging level and other details as arguments. Here is how you might define the decorator:\nfrom functools import wraps\nimport logging\ndef logged(level, name=None, message=None):\n    '''\n    Add logging to a function.  level is the logging\n    level, name is the logger name, and message is the\n    log message.  If name and message aren't specified,\n    they default to the function's module and name.\n    '''\n    def decorate(func):\n        logname = name if name else func.__module__\n        log = logging.getLogger(logname)\n        logmsg = message if message else func.__name__\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            log.log(level, logmsg)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorate\n# Example use\n@logged(logging.DEBUG)\ndef add(x, y):\n    return x + y\n@logged(logging.CRITICAL, 'example')\ndef spam():\n    print('Spam!')\nOn first glance, the implementation looks tricky, but the idea is relatively simple. The\noutermost function logged() accepts the desired arguments and simply makes them\navailable to the inner functions of the decorator. The inner function decorate() accepts\na function and puts a wrapper around it as normal. The key part is that the wrapper is\nallowed to use the arguments passed to logged().\nDiscussion\nWriting a decorator that takes arguments is tricky because of the underlying calling\nsequence involved. Specifically, if you have code like this:\n@decorator(x, y, z)\ndef func(a, b):\n    pass\n9.4. Defining a Decorator That Takes Arguments \n| \n335",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": 8,
      "content": "The decoration process evaluates as follows:\ndef func(a, b):\n    pass\nfunc = decorator(x, y, z)(func)\nCarefully observe that the result of decorator(x, y, z) must be a callable which, in\nturn, takes a function as input and wraps it. See Recipe 9.7 for another example of a\ndecorator taking arguments.\n9.5. Defining a Decorator with User Adjustable Attributes\nProblem\nYou want to write a decorator function that wraps a function, but has user adjustable\nattributes that can be used to control the behavior of the decorator at runtime.\nSolution\nHere is a solution that expands on the last recipe by introducing accessor functions that\nchange internal variables through the use of nonlocal variable declarations. The ac‐\ncessor functions are then attached to the wrapper function as function attributes.\nfrom functools import wraps, partial\nimport logging\n# Utility decorator to attach a function as an attribute of obj\ndef attach_wrapper(obj, func=None):\n    if func is None:\n        return partial(attach_wrapper, obj)\n    setattr(obj, func.__name__, func)\n    return func\ndef logged(level, name=None, message=None):\n    '''\n    Add logging to a function.  level is the logging\n    level, name is the logger name, and message is the\n    log message.  If name and message aren't specified,\n    they default to the function's module and name.\n    '''\n    def decorate(func):\n        logname = name if name else func.__module__\n        log = logging.getLogger(logname)\n        logmsg = message if message else func.__name__\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            log.log(level, logmsg)\n            return func(*args, **kwargs)\n336 \n| \nChapter 9: Metaprogramming",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": 8,
      "content": "# Attach setter functions\n        @attach_wrapper(wrapper)\n        def set_level(newlevel):\n            nonlocal level\n            level = newlevel\n        @attach_wrapper(wrapper)\n        def set_message(newmsg):\n            nonlocal logmsg\n            logmsg = newmsg\n        return wrapper\n    return decorate\n# Example use\n@logged(logging.DEBUG)\ndef add(x, y):\n    return x + y\n@logged(logging.CRITICAL, 'example')\ndef spam():\n    print('Spam!')\nHere is an interactive session that shows the various attributes being changed after\ndefinition:\n>>> import logging\n>>> logging.basicConfig(level=logging.DEBUG)\n>>> add(2, 3)\nDEBUG:__main__:add\n5\n>>> # Change the log message\n>>> add.set_message('Add called')\n>>> add(2, 3)\nDEBUG:__main__:Add called\n5\n>>> # Change the log level\n>>> add.set_level(logging.WARNING)\n>>> add(2, 3)\nWARNING:__main__:Add called\n5\n>>>\n9.5. Defining a Decorator with User Adjustable Attributes \n| \n337",
      "content_length": 926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": 8,
      "content": "Discussion\nThe key to this recipe lies in the accessor functions [e.g., set_message() and set_lev\nel()] that get attached to the wrapper as attributes. Each of these accessors allows in‐\nternal parameters to be adjusted through the use of nonlocal assignments.\nAn amazing feature of this recipe is that the accessor functions will propagate through\nmultiple levels of decoration (if all of your decorators utilize @functools.wraps). For\nexample, suppose you introduced an additional decorator, such as the @timethis dec‐\norator from Recipe 9.2, and wrote code like this:\n@timethis\n@logged(logging.DEBUG)\ndef countdown(n):\n    while n > 0:\n        n -= 1\nYou’ll find that the accessor methods still work:\n>>> countdown(10000000)\nDEBUG:__main__:countdown\ncountdown 0.8198461532592773\n>>> countdown.set_level(logging.WARNING)\n>>> countdown.set_message(\"Counting down to zero\")\n>>> countdown(10000000)\nWARNING:__main__:Counting down to zero\ncountdown 0.8225970268249512\n>>>\nYou’ll also find that it all still works exactly the same way if the decorators are composed\nin the opposite order, like this:\n@logged(logging.DEBUG)\n@timethis\ndef countdown(n):\n    while n > 0:\n        n -= 1\nAlthough it’s not shown, accessor functions to return the value of various settings could\nalso be written just as easily by adding extra code such as this:\n...\n@attach_wrapper(wrapper)\ndef get_level():\n    return level\n# Alternative\nwrapper.get_level = lambda: level\n...\n338 \n| \nChapter 9: Metaprogramming",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": 8,
      "content": "One extremely subtle facet of this recipe is the choice to use accessor functions in the\nfirst place. For example, you might consider an alternative formulation solely based on\ndirect access to function attributes like this:\n...\n@wraps(func)\ndef wrapper(*args, **kwargs):\n    wrapper.log.log(wrapper.level, wrapper.logmsg)\n    return func(*args, **kwargs)\n# Attach adjustable attributes\nwrapper.level = level\nwrapper.logmsg = logmsg\nwrapper.log = log\n...\nThis approach would work to a point, but only if it was the topmost decorator. If you\nhad another decorator applied on top (such as the @timethis example), it would shadow\nthe underlying attributes and make them unavailable for modification. The use of ac‐\ncessor functions avoids this limitation.\nLast, but not least, the solution shown in this recipe might be a possible alternative for\ndecorators defined as classes, as shown in Recipe 9.9.\n9.6. Defining a Decorator That Takes an Optional\nArgument\nProblem\nYou would like to write a single decorator that can be used without arguments, such as\n@decorator, or with optional arguments, such as @decorator(x,y,z). However, there\nseems to be no straightforward way to do it due to differences in calling conventions\nbetween simple decorators and decorators taking arguments.\nSolution\nHere is a variant of the logging code shown in Recipe 9.5 that defines such a decorator:\nfrom functools import wraps, partial\nimport logging\ndef logged(func=None, *, level=logging.DEBUG, name=None, message=None):\n    if func is None:\n        return partial(logged, level=level, name=name, message=message)\n    logname = name if name else func.__module__\n    log = logging.getLogger(logname)\n    logmsg = message if message else func.__name__\n9.6. Defining a Decorator That Takes an Optional Argument \n| \n339",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": 8,
      "content": "@wraps(func)\n    def wrapper(*args, **kwargs):\n        log.log(level, logmsg)\n        return func(*args, **kwargs)\n    return wrapper\n# Example use\n@logged\ndef add(x, y):\n    return x + y\n@logged(level=logging.CRITICAL, name='example')\ndef spam():\n    print('Spam!')\nAs you can see from the example, the decorator can be used in both a simple form (i.e.,\n@logged) or with optional arguments supplied (i.e., @logged(level=logging.CRITI\nCAL, name='example')).\nDiscussion\nThe problem addressed by this recipe is really one of programming consistency. When\nusing decorators, most programmers are used to applying them without any arguments\nat all or with arguments, as shown in the example. Technically speaking, a decorator\nwhere all arguments are optional could be applied, like this:\n@logged()\ndef add(x, y):\n    return x+y\nHowever, this is not a form that’s especially common, and might lead to common usage\nerrors if programmers forget to add the extra parentheses. The recipe simply makes the\ndecorator work with or without parentheses in a consistent way.\nTo understand how the code works, you need to have a firm understanding of how\ndecorators get applied to functions and their calling conventions. For a simple decorator\nsuch as this:\n# Example use\n@logged\ndef add(x, y):\n    return x + y\nThe calling sequence is as follows:\ndef add(x, y):\n    return x + y\nadd = logged(add)\n340 \n| \nChapter 9: Metaprogramming",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": 8,
      "content": "In this case, the function to be wrapped is simply passed to logged as the first argument.\nThus, in the solution, the first argument of logged() is the function being wrapped. All\nof the other arguments must have default values.\nFor a decorator taking arguments such as this:\n@logged(level=logging.CRITICAL, name='example')\ndef spam():\n    print('Spam!')\nThe calling sequence is as follows:\ndef spam():\n    print('Spam!')\nspam = logged(level=logging.CRITICAL, name='example')(spam)\nOn the initial invocation of logged(), the function to be wrapped is not passed. Thus,\nin the decorator, it has to be optional. This, in turn, forces the other arguments to be\nspecified by keyword. Furthermore, when arguments are passed, a decorator is supposed\nto return a function that accepts the function and wraps it (see Recipe 9.5). To do this,\nthe solution uses a clever trick involving functools.partial. Specifically, it simply\nreturns a partially applied version of itself where all arguments are fixed except for the\nfunction to be wrapped. See Recipe 7.8 for more details about using partial().\n9.7. Enforcing Type Checking on a Function Using a\nDecorator\nProblem\nYou want to optionally enforce type checking of function arguments as a kind of asser‐\ntion or contract.\nSolution\nBefore showing the solution code, the aim of this recipe is to have a means of enforcing\ntype contracts on the input arguments to a function. Here is a short example that illus‐\ntrates the idea:\n>>> @typeassert(int, int)\n... def add(x, y):\n...     return x + y\n...\n>>>\n>>> add(2, 3)\n5\n>>> add(2, 'hello')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n9.7. Enforcing Type Checking on a Function Using a Decorator \n| \n341",
      "content_length": 1719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": 8,
      "content": "File \"contract.py\", line 33, in wrapper\nTypeError: Argument y must be <class 'int'>\n>>>\nNow, here is an implementation of the @typeassert decorator:\nfrom inspect import signature\nfrom functools import wraps\ndef typeassert(*ty_args, **ty_kwargs):\n    def decorate(func):\n        # If in optimized mode, disable type checking\n        if not __debug__:\n            return func\n        # Map function argument names to supplied types\n        sig = signature(func)\n        bound_types = sig.bind_partial(*ty_args, **ty_kwargs).arguments\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            bound_values = sig.bind(*args, **kwargs)\n            # Enforce type assertions across supplied arguments\n            for name, value in bound_values.arguments.items():\n                if name in bound_types:\n                    if not isinstance(value, bound_types[name]):\n                      raise TypeError(\n                        'Argument {} must be {}'.format(name, bound_types[name])\n                        )\n            return func(*args, **kwargs)\n        return wrapper\n    return decorate\nYou will find that this decorator is rather flexible, allowing types to be specified for all\nor a subset of a function’s arguments. Moreover, types can be specified by position or\nby keyword. Here is an example:\n>>> @typeassert(int, z=int)\n... def spam(x, y, z=42):\n...     print(x, y, z)\n...\n>>> spam(1, 2, 3)\n1 2 3\n>>> spam(1, 'hello', 3)\n1 hello 3\n>>> spam(1, 'hello', 'world')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"contract.py\", line 33, in wrapper\nTypeError: Argument z must be <class 'int'>\n>>>\n342 \n| \nChapter 9: Metaprogramming",
      "content_length": 1684,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": 8,
      "content": "Discussion\nThis recipe is an advanced decorator example that introduces a number of important\nand useful concepts.\nFirst, one aspect of decorators is that they only get applied once, at the time of function\ndefinition. In certain cases, you may want to disable the functionality added by a dec‐\norator. To do this, simply have your decorator function return the function unwrapped.\nIn the solution, the following code fragment returns the function unmodified if the\nvalue of the global __debug__ variable is set to False (as is the case when Python executes\nin optimized mode with the -O or -OO options to the interpreter):\n...\ndef decorate(func):\n    # If in optimized mode, disable type checking\n    if not __debug__:\n        return func\n    ...\nNext, a tricky part of writing this decorator is that it involves examining and working\nwith the argument signature of the function being wrapped. Your tool of choice here\nshould be the inspect.signature() function. Simply stated, it allows you to extract\nsignature information from a callable. For example:\n>>> from inspect import signature\n>>> def spam(x, y, z=42):\n...     pass\n...\n>>> sig = signature(spam)\n>>> print(sig)\n(x, y, z=42)\n>>> sig.parameters\nmappingproxy(OrderedDict([('x', <Parameter at 0x10077a050 'x'>),\n('y', <Parameter at 0x10077a158 'y'>), ('z', <Parameter at 0x10077a1b0 'z'>)]))\n>>> sig.parameters['z'].name\n'z'\n>>> sig.parameters['z'].default\n42\n>>> sig.parameters['z'].kind\n<_ParameterKind: 'POSITIONAL_OR_KEYWORD'>\n>>>\nIn the first part of our decorator, we use the bind_partial() method of signatures to\nperform a partial binding of the supplied types to argument names. Here is an example\nof what happens:\n>>> bound_types = sig.bind_partial(int,z=int)\n>>> bound_types\n<inspect.BoundArguments object at 0x10069bb50>\n>>> bound_types.arguments\n9.7. Enforcing Type Checking on a Function Using a Decorator \n| \n343",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": 8,
      "content": "OrderedDict([('x', <class 'int'>), ('z', <class 'int'>)])\n>>>\nIn this partial binding, you will notice that missing arguments are simply ignored (i.e.,\nthere is no binding for argument y). However, the most important part of the binding\nis the creation of the ordered dictionary bound_types.arguments. This dictionary maps\nthe argument names to the supplied values in the same order as the function signature.\nIn the case of our decorator, this mapping contains the type assertions that we’re going\nto enforce.\nIn the actual wrapper function made by the decorator, the sig.bind() method is used.\nbind() is like bind_partial() except that it does not allow for missing arguments. So,\nhere is what happens:\n>>> bound_values = sig.bind(1, 2, 3)\n>>> bound_values.arguments\nOrderedDict([('x', 1), ('y', 2), ('z', 3)])\n>>>\nUsing this mapping, it is relatively easy to enforce the required assertions.\n>>> for name, value in bound_values.arguments.items():\n...     if name in bound_types.arguments:\n...         if not isinstance(value, bound_types.arguments[name]):\n...              raise TypeError()\n...\n>>>\nA somewhat subtle aspect of the solution is that the assertions do not get applied to\nunsupplied arguments with default values. For example, this code works, even though\nthe default value of items is of the “wrong” type:\n>>> @typeassert(int, list)\n... def bar(x, items=None):\n...     if items is None:\n...         items = []\n...     items.append(x)\n...     return items\n>>> bar(2)\n[2]\n>>> bar(2,3)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"contract.py\", line 33, in wrapper\nTypeError: Argument items must be <class 'list'>\n>>> bar(4, [1, 2, 3])\n[1, 2, 3, 4]\n>>>\nA final point of design discussion might be the use of decorator arguments versus func‐\ntion annotations. For example, why not write the decorator to look at annotations like\nthis?\n344 \n| \nChapter 9: Metaprogramming",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": 8,
      "content": "@typeassert\ndef spam(x:int, y, z:int = 42):\n    print(x,y,z)\nOne possible reason for not using annotations is that each argument to a function can\nonly have a single annotation assigned. Thus, if the annotations are used for type as‐\nsertions, they can’t really be used for anything else. Likewise, the @typeassert decorator\nwon’t work with functions that use annotations for a different purpose. By using deco‐\nrator arguments, as shown in the solution, the decorator becomes a lot more general\npurpose and can be used with any function whatsoever—even functions that use\nannotations.\nMore information about function signature objects can be found in PEP 362, as well as\nthe documentation for the inspect module. Recipe 9.16 also has an additional example.\n9.8. Defining Decorators As Part of a Class\nProblem\nYou want to define a decorator inside a class definition and apply it to other functions\nor methods.\nSolution\nDefining a decorator inside a class is straightforward, but you first need to sort out the\nmanner in which the decorator will be applied. Specifically, whether it is applied as an\ninstance or a class method. Here is an example that illustrates the difference:\nfrom functools import wraps\nclass A:\n    # Decorator as an instance method\n    def decorator1(self, func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            print('Decorator 1')\n            return func(*args, **kwargs)\n        return wrapper\n    # Decorator as a class method\n    @classmethod\n    def decorator2(cls, func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            print('Decorator 2')\n            return func(*args, **kwargs)\n        return wrapper\nHere is an example of how the two decorators would be applied:\n9.8. Defining Decorators As Part of a Class \n| \n345",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": 8,
      "content": "# As an instance method\na = A()\n@a.decorator1\ndef spam():\n    pass\n# As a class method\n@A.decorator2\ndef grok():\n    pass\nIf you look carefully, you’ll notice that one is applied from an instance a and the other\nis applied from the class A.\nDiscussion\nDefining decorators in a class might look odd at first glance, but there are examples of\nthis in the standard library. In particular, the built-in @property decorator is actually a\nclass with getter(), setter(), and deleter() methods that each act as a decorator.\nFor example:\nclass Person:\n    # Create a property instance\n    first_name = property()\n    # Apply decorator methods\n    @first_name.getter\n    def first_name(self):\n        return self._first_name\n    @first_name.setter\n    def first_name(self, value):\n        if not isinstance(value, str):\n            raise TypeError('Expected a string')\n        self._first_name = value\nThe key reason why it’s defined in this way is that the various decorator methods are\nmanipulating state on the associated property instance. So, if you ever had a problem\nwhere decorators needed to record or combine information behind the scenes, it’s a\nsensible approach.\nA common confusion when writing decorators in classes is getting tripped up by the\nproper use of the extra self or cls arguments in the decorator code itself. Although\nthe outermost decorator function, such as decorator1() or decorator2(), needs to\nprovide a self or cls argument (since they’re part of a class), the wrapper function\ncreated inside doesn’t generally need to include an extra argument. This is why the\nwrapper() function created in both decorators doesn’t include a self argument. The\nonly time you would ever need this argument is in situations where you actually needed\n346 \n| \nChapter 9: Metaprogramming",
      "content_length": 1788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": 8,
      "content": "to access parts of an instance in the wrapper. Otherwise, you just don’t have to worry\nabout it.\nA final subtle facet of having decorators defined in a class concerns their potential use\nwith inheritance. For example, suppose you want to apply one of the decorators defined\nin class A to methods defined in a subclass B. To do that, you would need to write code\nlike this:\nclass B(A):\n    @A.decorator2\n    def bar(self):\n        pass\nIn particular, the decorator in question has to be defined as a class method and you have\nto explicitly use the name of the superclass A when applying it. You can’t use a name\nsuch as @B.decorator2, because at the time of method definition, class B has not yet\nbeen created.\n9.9. Defining Decorators As Classes\nProblem\nYou want to wrap functions with a decorator, but the result is going to be a callable\ninstance. You need your decorator to work both inside and outside class definitions.\nSolution\nTo define a decorator as an instance, you need to make sure it implements the\n__call__() and __get__() methods. For example, this code defines a class that puts a\nsimple profiling layer around another function:\nimport types\nfrom functools import wraps\nclass Profiled:\n    def __init__(self, func):\n        wraps(func)(self)\n        self.ncalls = 0\n    def __call__(self, *args, **kwargs):\n        self.ncalls += 1\n        return self.__wrapped__(*args, **kwargs)\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            return types.MethodType(self, instance)\n9.9. Defining Decorators As Classes \n| \n347",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": 8,
      "content": "To use this class, you use it like a normal decorator, either inside or outside of a class:\n@Profiled\ndef add(x, y):\n    return x + y\nclass Spam:\n    @Profiled\n    def bar(self, x):\n        print(self, x)\nHere is an interactive session that shows how these functions work:\n>>> add(2, 3)\n5\n>>> add(4, 5)\n9\n>>> add.ncalls\n2\n>>> s = Spam()\n>>> s.bar(1)\n<__main__.Spam object at 0x10069e9d0> 1\n>>> s.bar(2)\n<__main__.Spam object at 0x10069e9d0> 2\n>>> s.bar(3)\n<__main__.Spam object at 0x10069e9d0> 3\n>>> Spam.bar.ncalls\n3\nDiscussion\nDefining a decorator as a class is usually straightforward. However, there are some rather\nsubtle details that deserve more explanation, especially if you plan to apply the decorator\nto instance methods.\nFirst, the use of the functools.wraps() function serves the same purpose here as it\ndoes in normal decorators—namely to copy important metadata from the wrapped\nfunction to the callable instance.\nSecond, it is common to overlook the __get__() method shown in the solution. If you\nomit the __get__() and keep all of the other code the same, you’ll find that bizarre\nthings happen when you try to invoke decorated instance methods. For example:\n>>> s = Spam()\n>>> s.bar(3)\nTraceback (most recent call last):\n...\nTypeError: spam() missing 1 required positional argument: 'x'\nThe reason it breaks is that whenever functions implementing methods are looked up\nin a class, their __get__() method is invoked as part of the descriptor protocol, which\n348 \n| \nChapter 9: Metaprogramming",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": 8,
      "content": "is described in Recipe 8.9. In this case, the purpose of __get__() is to create a bound\nmethod object (which ultimately supplies the self argument to the method). Here is\nan example that illustrates the underlying mechanics:\n>>> s = Spam()\n>>> def grok(self, x):\n...     pass\n...\n>>> grok.__get__(s, Spam)\n<bound method Spam.grok of <__main__.Spam object at 0x100671e90>>\n>>>\nIn this recipe, the __get__() method is there to make sure bound method objects get\ncreated properly. type.MethodType() creates a bound method manually for use here.\nBound methods only get created if an instance is being used. If the method is accessed\non a class, the instance argument to __get__() is set to None and the Profiled instance\nitself is just returned. This makes it possible for someone to extract its ncalls attribute,\nas shown.\nIf you want to avoid some of this of this mess, you might consider an alternative for‐\nmulation of the decorator using closures and nonlocal variables, as described in\nRecipe 9.5. For example:\nimport types\nfrom functools import wraps\ndef profiled(func):\n    ncalls = 0\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal ncalls\n        ncalls += 1\n        return func(*args, **kwargs)\n    wrapper.ncalls = lambda: ncalls\n    return wrapper\n# Example\n@profiled\ndef add(x, y):\n    return x + y\nThis example almost works in exactly the same way except that access to ncalls is now\nprovided through a function attached as a function attribute. For example:\n>>> add(2, 3)\n5\n>>> add(4, 5)\n9\n>>> add.ncalls()\n2\n>>>\n9.9. Defining Decorators As Classes \n| \n349",
      "content_length": 1587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": 8,
      "content": "9.10. Applying Decorators to Class and Static Methods\nProblem\nYou want to apply a decorator to a class or static method.\nSolution\nApplying decorators to class and static methods is straightforward, but make sure that\nyour decorators are applied before @classmethod or @staticmethod. For example:\nimport time\nfrom functools import wraps\n# A simple decorator\ndef timethis(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        r = func(*args, **kwargs)\n        end = time.time()\n        print(end-start)\n        return r\n    return wrapper\n# Class illustrating application of the decorator to different kinds of methods\nclass Spam:\n    @timethis\n    def instance_method(self, n):\n        print(self, n)\n        while n > 0:\n            n -= 1\n    @classmethod\n    @timethis\n    def class_method(cls, n):\n        print(cls, n)\n        while n > 0:\n            n -= 1\n    @staticmethod\n    @timethis\n    def static_method(n):\n        print(n)\n        while n > 0:\n            n -= 1\nThe resulting class and static methods should operate normally, but have the extra\ntiming:\n350 \n| \nChapter 9: Metaprogramming",
      "content_length": 1143,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": 8,
      "content": ">>> s = Spam()\n>>> s.instance_method(1000000)\n<__main__.Spam object at 0x1006a6050> 1000000\n0.11817407608032227\n>>> Spam.class_method(1000000)\n<class '__main__.Spam'> 1000000\n0.11334395408630371\n>>> Spam.static_method(1000000)\n1000000\n0.11740279197692871\n>>>\nDiscussion\nIf you get the order of decorators wrong, you’ll get an error. For example, if you use the\nfollowing:\nclass Spam:\n    ...\n    @timethis\n    @staticmethod\n    def static_method(n):\n        print(n)\n        while n > 0:\n            n -= 1\nThen the static method will crash:\n>>> Spam.static_method(1000000)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"timethis.py\", line 6, in wrapper\n    start = time.time()\nTypeError: 'staticmethod' object is not callable\n>>>\nThe problem here is that @classmethod and @staticmethod don’t actually create objects\nthat are directly callable. Instead, they create special descriptor objects, as described in\nRecipe 8.9. Thus, if you try to use them like functions in another decorator, the decorator\nwill crash. Making sure that these decorators appear first in the decorator list fixes the\nproblem.\nOne situation where this recipe is of critical importance is in defining class and static\nmethods in abstract base classes, as described in Recipe 8.12. For example, if you want\nto define an abstract class method, you can use this code:\nfrom abc import ABCMeta, abstractmethod\nclass A(metaclass=ABCMeta):\n    @classmethod\n    @abstractmethod\n9.10. Applying Decorators to Class and Static Methods \n| \n351",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": 8,
      "content": "def method(cls):\n        pass\nIn this code, the order of @classmethod and @abstractmethod matters. If you flip the\ntwo decorators around, everything breaks.\n9.11. Writing Decorators That Add Arguments to Wrapped\nFunctions\nProblem\nYou want to write a decorator that adds an extra argument to the calling signature of\nthe wrapped function. However, the added argument can’t interfere with the existing\ncalling conventions of the function.\nSolution\nExtra arguments can be injected into the calling signature using keyword-only argu‐\nments. Consider the following decorator:\nfrom functools import wraps\ndef optional_debug(func):\n    @wraps(func)\n    def wrapper(*args, debug=False, **kwargs):\n        if debug:\n            print('Calling', func.__name__)\n        return func(*args, **kwargs)\n    return wrapper\nHere is an example of how the decorator works:\n>>> @optional_debug\n... def spam(a,b,c):\n...     print(a,b,c)\n...\n>>> spam(1,2,3)\n1 2 3\n>>> spam(1,2,3, debug=True)\nCalling spam\n1 2 3\n>>>\nDiscussion\nAdding arguments to the signature of wrapped functions is not the most common ex‐\nample of using decorators. However, it might be a useful technique in avoiding certain\nkinds of code replication patterns. For example, if you have code like this:\n352 \n| \nChapter 9: Metaprogramming",
      "content_length": 1284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": 8,
      "content": "def a(x, debug=False):\n    if debug:\n        print('Calling a')\n    ...\ndef b(x, y, z, debug=False):\n    if debug:\n        print('Calling b')\n    ...\ndef c(x, y, debug=False):\n    if debug:\n        print('Calling c')\n    ...\nYou can refactor it into the following:\n@optional_debug\ndef a(x):\n    ...\n@optional_debug\ndef b(x, y, z):\n    ...\n@optional_debug\ndef c(x, y):\n    ...\nThe implementation of this recipe relies on the fact that keyword-only arguments are\neasy to add to functions that also accept *args and **kwargs parameters. By using a\nkeyword-only argument, it gets singled out as a special case and removed from subse‐\nquent calls that only use the remaining positional and keyword arguments.\nOne tricky part here concerns a potential name clash between the added argument and\nthe arguments of the function being wrapped. For example, if the @optional_debug\ndecorator was applied to a function that already had a debug argument, then it would\nbreak. If that’s a concern, an extra check could be added:\nfrom functools import wraps\nimport inspect\ndef optional_debug(func):\n    if 'debug' in inspect.getargspec(func).args:\n        raise TypeError('debug argument already defined')\n    @wraps(func)\n    def wrapper(*args, debug=False, **kwargs):\n        if debug:\n            print('Calling', func.__name__)\n        return func(*args, **kwargs)\n    return wrapper\n9.11. Writing Decorators That Add Arguments to Wrapped Functions \n| \n353",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": 8,
      "content": "A final refinement to this recipe concerns the proper management of function signa‐\ntures. An astute programmer will realize that the signature of wrapped functions is\nwrong. For example:\n>>> @optional_debug\n... def add(x,y):\n...     return x+y\n...\n>>> import inspect\n>>> print(inspect.signature(add))\n(x, y)\n>>>\nThis can be fixed by making the following modification:\nfrom functools import wraps\nimport inspect\ndef optional_debug(func):\n    if 'debug' in inspect.getargspec(func).args:\n        raise TypeError('debug argument already defined')\n    @wraps(func)\n    def wrapper(*args, debug=False, **kwargs):\n        if debug:\n            print('Calling', func.__name__)\n        return func(*args, **kwargs)\n    sig = inspect.signature(func)\n    parms = list(sig.parameters.values())\n    parms.append(inspect.Parameter('debug',\n                                   inspect.Parameter.KEYWORD_ONLY,\n                                   default=False))\n    wrapper.__signature__ = sig.replace(parameters=parms)\n    return wrapper\nWith this change, the signature of the wrapper will now correctly reflect the presence\nof the debug argument. For example:\n>>> @optional_debug\n... def add(x,y):\n...     return x+y\n...\n>>> print(inspect.signature(add))\n(x, y, *, debug=False)\n>>> add(2,3)\n5\n>>>\nSee Recipe 9.16 for more information about function signatures.\n354 \n| \nChapter 9: Metaprogramming",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": 8,
      "content": "9.12. Using Decorators to Patch Class Definitions\nProblem\nYou want to inspect or rewrite portions of a class definition to alter its behavior, but\nwithout using inheritance or metaclasses.\nSolution\nThis might be a perfect use for a class decorator. For example, here is a class decorator\nthat rewrites the __getattribute__ special method to perform logging.\ndef log_getattribute(cls):\n    # Get the original implementation\n    orig_getattribute = cls.__getattribute__\n    # Make a new definition\n    def new_getattribute(self, name):\n        print('getting:', name)\n        return orig_getattribute(self, name)\n    # Attach to the class and return\n    cls.__getattribute__ = new_getattribute\n    return cls\n# Example use\n@log_getattribute\nclass A:\n    def __init__(self,x):\n        self.x = x\n    def spam(self):\n        pass\nHere is what happens if you try to use the class in the solution:\n>>> a = A(42)\n>>> a.x\ngetting: x\n42\n>>> a.spam()\ngetting: spam\n>>>\nDiscussion\nClass decorators can often be used as a straightforward alternative to other more ad‐\nvanced techniques involving mixins or metaclasses. For example, an alternative imple‐\nmentation of the solution might involve inheritance, as in the following:\n9.12. Using Decorators to Patch Class Definitions \n| \n355",
      "content_length": 1273,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": 8,
      "content": "class LoggedGetattribute:\n    def __getattribute__(self, name):\n        print('getting:', name)\n        return super().__getattribute__(name)\n# Example:\nclass A(LoggedGetattribute):\n    def __init__(self,x):\n        self.x = x\n    def spam(self):\n        pass\nThis works, but to understand it, you have to have some awareness of the method res‐\nolution order, super(), and other aspects of inheritance, as described in Recipe 8.7. In\nsome sense, the class decorator solution is much more direct in how it operates, and it\ndoesn’t introduce new dependencies into the inheritance hierarchy. As it turns out, it’s\nalso just a bit faster, due to not relying on the super() function.\nIf you are applying multiple class decorators to a class, the application order might\nmatter. For example, a decorator that replaces a method with an entirely new imple‐\nmentation would probably need to be applied before a decorator that simply wraps an\nexisting method with some extra logic.\nSee Recipe 8.13 for another example of class decorators in action.\n9.13. Using a Metaclass to Control Instance Creation\nProblem\nYou want to change the way in which instances are created in order to implement sin‐\ngletons, caching, or other similar features.\nSolution\nAs Python programmers know, if you define a class, you call it like a function to create\ninstances. For example:\nclass Spam:\n    def __init__(self, name):\n        self.name = name\na = Spam('Guido')\nb = Spam('Diana')\nIf you want to customize this step, you can do it by defining a metaclass and reimple‐\nmenting its __call__() method in some way. To illustrate, suppose that you didn’t want\nanyone creating instances at all:\n356 \n| \nChapter 9: Metaprogramming",
      "content_length": 1697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": 8,
      "content": "class NoInstances(type):\n    def __call__(self, *args, **kwargs):\n        raise TypeError(\"Can't instantiate directly\")\n# Example\nclass Spam(metaclass=NoInstances):\n    @staticmethod\n    def grok(x):\n        print('Spam.grok')\nIn this case, users can call the defined static method, but it’s impossible to create an\ninstance in the normal way. For example:\n>>> Spam.grok(42)\nSpam.grok\n>>> s = Spam()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"example1.py\", line 7, in __call__\n    raise TypeError(\"Can't instantiate directly\")\nTypeError: Can't instantiate directly\n>>>\nNow, suppose you want to implement the singleton pattern (i.e., a class where only one\ninstance is ever created). That is also relatively straightforward, as shown here:\nclass Singleton(type):\n    def __init__(self, *args, **kwargs):\n        self.__instance = None\n        super().__init__(*args, **kwargs)\n    def __call__(self, *args, **kwargs):\n        if self.__instance is None:\n            self.__instance = super().__call__(*args, **kwargs)\n            return self.__instance\n        else:\n            return self.__instance\n# Example\nclass Spam(metaclass=Singleton):\n    def __init__(self):\n        print('Creating Spam')\nIn this case, only one instance ever gets created. For example:\n>>> a = Spam()\nCreating Spam\n>>> b = Spam()\n>>> a is b\nTrue\n>>> c = Spam()\n>>> a is c\nTrue\n>>>\n9.13. Using a Metaclass to Control Instance Creation \n| \n357",
      "content_length": 1456,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": 8,
      "content": "Finally, suppose you want to create cached instances, as described in Recipe 8.25. Here’s\na metaclass that implements it:\nimport weakref\nclass Cached(type):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__cache = weakref.WeakValueDictionary()\n    def __call__(self, *args):\n        if args in self.__cache:\n            return self.__cache[args]\n        else:\n            obj = super().__call__(*args)\n            self.__cache[args] = obj\n            return obj\n# Example\nclass Spam(metaclass=Cached):\n    def __init__(self, name):\n        print('Creating Spam({!r})'.format(name))\n        self.name = name\nHere’s an example showing the behavior of this class:\n>>> a = Spam('Guido')\nCreating Spam('Guido')\n>>> b = Spam('Diana')\nCreating Spam('Diana')\n>>> c = Spam('Guido')       # Cached\n>>> a is b\nFalse\n>>> a is c                  # Cached value returned\nTrue\n>>>\nDiscussion\nUsing a metaclass to implement various instance creation patterns can often be a much\nmore elegant approach than other solutions not involving metaclasses. For example, if\nyou didn’t use a metaclass, you might have to hide the classes behind some kind of extra\nfactory function. For example, to get a singleton, you might use a hack such as the\nfollowing:\nclass _Spam:\n    def __init__(self):\n        print('Creating Spam')\n_spam_instance = None\ndef Spam():\n    global _spam_instance\n358 \n| \nChapter 9: Metaprogramming",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": 8,
      "content": "if _spam_instance is not None:\n        return _spam_instance\n    else:\n        _spam_instance = _Spam()\n        return _spam_instance\nAlthough the solution involving metaclasses involves a much more advanced concept,\nthe resulting code feels cleaner and less hacked together.\nSee Recipe 8.25 for more information on creating cached instances, weak references,\nand other details.\n9.14. Capturing Class Attribute Definition Order\nProblem\nYou want to automatically record the order in which attributes and methods are defined\ninside a class body so that you can use it in various operations (e.g., serializing, mapping\nto databases, etc.).\nSolution\nCapturing information about the body of class definition is easily accomplished through\nthe use of a metaclass. Here is an example of a metaclass that uses an OrderedDict to\ncapture definition order of descriptors:\nfrom collections import OrderedDict\n# A set of descriptors for various types\nclass Typed:\n    _expected_type = type(None)\n    def __init__(self, name=None):\n        self._name = name\n    def __set__(self, instance, value):\n        if not isinstance(value, self._expected_type):\n            raise TypeError('Expected ' + str(self._expected_type))\n        instance.__dict__[self._name] = value\nclass Integer(Typed):\n    _expected_type = int\nclass Float(Typed):\n    _expected_type = float\nclass String(Typed):\n    _expected_type = str\n# Metaclass that uses an OrderedDict for class body\n9.14. Capturing Class Attribute Definition Order \n| \n359",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": 8,
      "content": "class OrderedMeta(type):\n    def __new__(cls, clsname, bases, clsdict):\n        d = dict(clsdict)\n        order = []\n        for name, value in clsdict.items():\n            if isinstance(value, Typed):\n                value._name = name\n                order.append(name)\n        d['_order'] = order\n        return type.__new__(cls, clsname, bases, d)\n    @classmethod\n    def __prepare__(cls, clsname, bases):\n        return OrderedDict()\nIn this metaclass, the definition order of descriptors is captured by using an Ordered\nDict during the execution of the class body. The resulting order of names is then ex‐\ntracted from the dictionary and stored into a class attribute _order. This can then be\nused by methods of the class in various ways. For example, here is a simple class that\nuses the ordering to implement a method for serializing the instance data as a line of\nCSV data:\nclass Structure(metaclass=OrderedMeta):\n    def as_csv(self):\n        return ','.join(str(getattr(self,name)) for name in self._order)\n# Example use\nclass Stock(Structure):\n    name = String()\n    shares = Integer()\n    price = Float()\n    def __init__(self, name, shares, price):\n        self.name = name\n        self.shares = shares\n        self.price = price\nHere is an interactive session illustrating the use of the Stock class in the example:\n>>> s = Stock('GOOG',100,490.1)\n>>> s.name\n'GOOG'\n>>> s.as_csv()\n'GOOG,100,490.1'\n>>> t = Stock('AAPL','a lot', 610.23)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"dupmethod.py\", line 34, in __init__\nTypeError: shares expects <class 'int'>\n>>>\n360 \n| \nChapter 9: Metaprogramming",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": 8,
      "content": "Discussion\nThe entire key to this recipe is the __prepare__() method, which is defined in the\nOrderedMeta metaclass. This method is invoked immediately at the start of a class def‐\ninition with the class name and base classes. It must then return a mapping object to\nuse when processing the class body. By returning an OrderedDict instead of a normal\ndictionary, the resulting definition order is easily captured.\nIt is possible to extend this functionality even further if you are willing to make your\nown dictionary-like objects. For example, consider this variant of the solution that re‐\njects duplicate definitions:\nfrom collections import OrderedDict\nclass NoDupOrderedDict(OrderedDict):\n    def __init__(self, clsname):\n        self.clsname = clsname\n        super().__init__()\n    def __setitem__(self, name, value):\n        if name in self:\n            raise TypeError('{} already defined in {}'.format(name, self.clsname))\n        super().__setitem__(name, value)\nclass OrderedMeta(type):\n    def __new__(cls, clsname, bases, clsdict):\n        d = dict(clsdict)\n        d['_order'] = [name for name in clsdict if name[0] != '_']\n        return type.__new__(cls, clsname, bases, d)\n    @classmethod\n    def __prepare__(cls, clsname, bases):\n        return NoDupOrderedDict(clsname)\nHere’s what happens if you use this metaclass and make a class with duplicate entries:\n>>> class A(metaclass=OrderedMeta):\n...     def spam(self):\n...             pass\n...     def spam(self):\n...             pass\n...\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 4, in A\n  File \"dupmethod2.py\", line 25, in __setitem__\n    (name, self.clsname))\nTypeError: spam already defined in A\n>>>\nA final important part of this recipe concerns the treatment of the modified dictionary\nin the metaclass __new__() method. Even though the class was defined using an alter‐\n9.14. Capturing Class Attribute Definition Order \n| \n361",
      "content_length": 1956,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": 8,
      "content": "native dictionary, you still have to convert this dictionary to a proper dict instance\nwhen making the final class object. This is the purpose of the d = dict(clsdict)\nstatement.\nBeing able to capture definition order is a subtle but important feature for certain kinds\nof applications. For instance, in an object relational mapper, classes might be written in\na manner similar to that shown in the example:\nclass Stock(Model):\n    name = String()\n    shares = Integer()\n    price = Float()\nUnderneath the covers, the code might want to capture the definition order to map\nobjects to tuples or rows in a database table (e.g., similar to the functionality of the\nas_csv() method in the example). The solution shown is very straightforward and often\nsimpler than alternative approaches (which typically involve maintaining hidden coun‐\nters within the descriptor classes).\n9.15. Defining a Metaclass That Takes Optional Arguments\nProblem\nYou want to define a metaclass that allows class definitions to supply optional argu‐\nments, possibly to control or configure aspects of processing during type creation.\nSolution\nWhen defining classes, Python allows a metaclass to be specified using the metaclass\nkeyword argument in the class statement. For example, with abstract base classes:\nfrom abc import ABCMeta, abstractmethod\nclass IStream(metaclass=ABCMeta):\n    @abstractmethod\n    def read(self, maxsize=None):\n        pass\n    @abstractmethod\n    def write(self, data):\n        pass\nHowever, in custom metaclasses, additional keyword arguments can be supplied, like\nthis:\nclass Spam(metaclass=MyMeta, debug=True, synchronize=True):\n    ...\n362 \n| \nChapter 9: Metaprogramming",
      "content_length": 1674,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": 9,
      "content": "To support such keyword arguments in a metaclass, make sure you define them on the\n__prepare__(), __new__(), and __init__() methods using keyword-only arguments,\nlike this:\nclass MyMeta(type):\n    # Optional\n    @classmethod\n    def __prepare__(cls, name, bases, *, debug=False, synchronize=False):\n        # Custom processing\n        ...\n        return super().__prepare__(name, bases)\n    # Required\n    def __new__(cls, name, bases, ns, *, debug=False, synchronize=False):\n        # Custom processing\n        ...\n        return super().__new__(cls, name, bases, ns)\n    # Required\n    def __init__(self, name, bases, ns, *, debug=False, synchronize=False):\n        # Custom processing\n        ...\n        super().__init__(name, bases, ns)\nDiscussion\nAdding optional keyword arguments to a metaclass requires that you understand all of\nthe steps involved in class creation, because the extra arguments are passed to every\nmethod involved. The __prepare__() method is called first and used to create the class\nnamespace prior to the body of any class definition being processed. Normally, this\nmethod simply returns a dictionary or other mapping object. The __new__() method\nis used to instantiate the resulting type object. It is called after the class body has been\nfully executed. The __init__() method is called last and used to perform any additional\ninitialization steps.\nWhen writing metaclasses, it is somewhat common to only define a __new__() or\n__init__() method, but not both. However, if extra keyword arguments are going to\nbe accepted, then both methods must be provided and given compatible signatures. The\ndefault __prepare__() method accepts any set of keyword arguments, but ignores them.\nYou only need to define it yourself if the extra arguments would somehow affect man‐\nagement of the class namespace creation.\nThe use of keyword-only arguments in this recipe reflects the fact that such arguments\nwill only be supplied by keyword during class creation.\nThe specification of keyword arguments to configure a metaclass might be viewed as\nan alternative to using class variables for a similar purpose. For example:\n9.15. Defining a Metaclass That Takes Optional Arguments \n| \n363",
      "content_length": 2201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": 9,
      "content": "class Spam(metaclass=MyMeta):\n    debug = True\n    synchronize = True\n    ...\nThe advantage to supplying such parameters as an argument is that they don’t pollute\nthe class namespace with extra names that only pertain to class creation and not the\nsubsequent execution of statements in the class. In addition, they are available to the\n__prepare__() method, which runs prior to processing any statements in the class body.\nClass variables, on the other hand, would only be accessible in the __new__() and\n__init__() methods of a metaclass.\n9.16. Enforcing an Argument Signature on *args and\n**kwargs\nProblem\nYou’ve written a function or method that uses *args and **kwargs, so that it can be\ngeneral purpose, but you would also like to check the passed arguments to see if they\nmatch a specific function calling signature.\nSolution\nFor any problem where you want to manipulate function calling signatures, you should\nuse the signature features found in the inspect module. Two classes, Signature and\nParameter, are of particular interest here. Here is an interactive example of creating a\nfunction signature:\n>>> from inspect import Signature, Parameter\n>>> # Make a signature for a func(x, y=42, *, z=None)\n>>> parms = [ Parameter('x', Parameter.POSITIONAL_OR_KEYWORD),\n...           Parameter('y', Parameter.POSITIONAL_OR_KEYWORD, default=42),\n...           Parameter('z', Parameter.KEYWORD_ONLY, default=None) ]\n>>> sig = Signature(parms)\n>>> print(sig)\n(x, y=42, *, z=None)\n>>>\nOnce you have a signature object, you can easily bind it to *args and **kwargs using\nthe signature’s bind() method, as shown in this simple example:\n>>> def func(*args, **kwargs):\n...     bound_values = sig.bind(*args, **kwargs)\n...     for name, value in bound_values.arguments.items():\n...         print(name,value)\n...\n>>> # Try various examples\n>>> func(1, 2, z=3)\n364 \n| \nChapter 9: Metaprogramming",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": 9,
      "content": "x 1\ny 2\nz 3\n>>> func(1)\nx 1\n>>> func(1, z=3)\nx 1\nz 3\n>>> func(y=2, x=1)\nx 1\ny 2\n>>> func(1, 2, 3, 4)\nTraceback (most recent call last):\n...\n  File \"/usr/local/lib/python3.3/inspect.py\", line 1972, in _bind\n    raise TypeError('too many positional arguments')\nTypeError: too many positional arguments\n>>> func(y=2)\nTraceback (most recent call last):\n...\n  File \"/usr/local/lib/python3.3/inspect.py\", line 1961, in _bind\n    raise TypeError(msg) from None\nTypeError: 'x' parameter lacking default value\n>>> func(1, y=2, x=3)\nTraceback (most recent call last):\n...\n  File \"/usr/local/lib/python3.3/inspect.py\", line 1985, in _bind\n    '{arg!r}'.format(arg=param.name))\nTypeError: multiple values for argument 'x'\n>>>\nAs you can see, the binding of a signature to the passed arguments enforces all of the\nusual function calling rules concerning required arguments, defaults, duplicates, and\nso forth.\nHere is a more concrete example of enforcing function signatures. In this code, a base\nclass has defined an extremely general-purpose version of __init__(), but subclasses\nare expected to supply an expected signature.\nfrom inspect import Signature, Parameter\ndef make_sig(*names):\n    parms = [Parameter(name, Parameter.POSITIONAL_OR_KEYWORD)\n             for name in names]\n    return Signature(parms)\nclass Structure:\n    __signature__ = make_sig()\n    def __init__(self, *args, **kwargs):\n        bound_values = self.__signature__.bind(*args, **kwargs)\n        for name, value in bound_values.arguments.items():\n            setattr(self, name, value)\n9.16. Enforcing an Argument Signature on *args and **kwargs \n| \n365",
      "content_length": 1618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": 9,
      "content": "# Example use\nclass Stock(Structure):\n    __signature__ = make_sig('name', 'shares', 'price')\nclass Point(Structure):\n    __signature__ = make_sig('x', 'y')\nHere is an example of how the Stock class works:\n>>> import inspect\n>>> print(inspect.signature(Stock))\n(name, shares, price)\n>>> s1 = Stock('ACME', 100, 490.1)\n>>> s2 = Stock('ACME', 100)\nTraceback (most recent call last):\n...\nTypeError: 'price' parameter lacking default value\n>>> s3 = Stock('ACME', 100, 490.1, shares=50)\nTraceback (most recent call last):\n...\nTypeError: multiple values for argument 'shares'\n>>>\nDiscussion\nThe use of functions involving *args and **kwargs is very common when trying to\nmake general-purpose libraries, write decorators or implement proxies. However, one\ndownside of such functions is that if you want to implement your own argument check‐\ning, it can quickly become an unwieldy mess. As an example, see Recipe 8.11. The use\nof a signature object simplifies this.\nIn the last example of the solution, it might make sense to create signature objects\nthrough the use of a custom metaclass. Here is an alternative implementation that shows\nhow to do this:\nfrom inspect import Signature, Parameter\ndef make_sig(*names):\n    parms = [Parameter(name, Parameter.POSITIONAL_OR_KEYWORD)\n             for name in names]\n    return Signature(parms)\nclass StructureMeta(type):\n    def __new__(cls, clsname, bases, clsdict):\n        clsdict['__signature__'] = make_sig(*clsdict.get('_fields',[]))\n        return super().__new__(cls, clsname, bases, clsdict)\nclass Structure(metaclass=StructureMeta):\n    _fields = []\n    def __init__(self, *args, **kwargs):\n366 \n| \nChapter 9: Metaprogramming",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": 9,
      "content": "bound_values = self.__signature__.bind(*args, **kwargs)\n        for name, value in bound_values.arguments.items():\n            setattr(self, name, value)\n# Example\nclass Stock(Structure):\n    _fields = ['name', 'shares', 'price']\nclass Point(Structure):\n    _fields = ['x', 'y']\nWhen defining custom signatures, it is often useful to store the signature in a special\nattribute __signature__, as shown. If you do this, code that uses the inspect module\nto perform introspection will see the signature and report it as the calling convention.\nFor example:\n>>> import inspect\n>>> print(inspect.signature(Stock))\n(name, shares, price)\n>>> print(inspect.signature(Point))\n(x, y)\n>>>\n9.17. Enforcing Coding Conventions in Classes\nProblem\nYour program consists of a large class hierarchy and you would like to enforce certain\nkinds of coding conventions (or perform diagnostics) to help maintain programmer\nsanity.\nSolution\nIf you want to monitor the definition of classes, you can often do it by defining a\nmetaclass. A basic metaclass is usually defined by inheriting from type and redefining\nits __new__() method or __init__() method. For example:\nclass MyMeta(type):\n    def __new__(self, clsname, bases, clsdict):\n        # clsname is name of class being defined\n        # bases is tuple of base classes\n        # clsdict is class dictionary\n        return super().__new__(cls, clsname, bases, clsdict)\nAlternatively, if __init__() is defined:\nclass MyMeta(type):\n    def __init__(self, clsname, bases, clsdict):\n        super().__init__(clsname, bases, clsdict)\n        # clsname is name of class being defined\n9.17. Enforcing Coding Conventions in Classes \n| \n367",
      "content_length": 1663,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": 9,
      "content": "# bases is tuple of base classes\n        # clsdict is class dictionary\nTo use a metaclass, you would generally incorporate it into a top-level base class from\nwhich other objects inherit. For example:\nclass Root(metaclass=MyMeta):\n    pass\nclass A(Root):\n    pass\nclass B(Root):\n    pass\nA key feature of a metaclass is that it allows you to examine the contents of a class at the\ntime of definition. Inside the redefined __init__() method, you are free to inspect the\nclass dictionary, base classes, and more. Moreover, once a metaclass has been specified\nfor a class, it gets inherited by all of the subclasses. Thus, a sneaky framework builder\ncan specify a metaclass for one of the top-level classes in a large hierarchy and capture\nthe definition of all classes under it.\nAs a concrete albeit whimsical example, here is a metaclass that rejects any class defi‐\nnition containing methods with mixed-case names (perhaps as a means for annoying\nJava programmers):\nclass NoMixedCaseMeta(type):\n    def __new__(cls, clsname, bases, clsdict):\n        for name in clsdict:\n            if name.lower() != name:\n                raise TypeError('Bad attribute name: ' + name)\n        return super().__new__(cls, clsname, bases, clsdict)\nclass Root(metaclass=NoMixedCaseMeta):\n    pass\nclass A(Root):\n    def foo_bar(self):      # Ok\n        pass\nclass B(Root):\n    def fooBar(self):       # TypeError\n        pass\nAs a more advanced and useful example, here is a metaclass that checks the definition\nof redefined methods to make sure they have the same calling signature as the original\nmethod in the superclass.\nfrom inspect import signature\nimport logging\nclass MatchSignaturesMeta(type):\n368 \n| \nChapter 9: Metaprogramming",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": 9,
      "content": "def __init__(self, clsname, bases, clsdict):\n        super().__init__(clsname, bases, clsdict)\n        sup = super(self, self)\n        for name, value in clsdict.items():\n            if name.startswith('_') or not callable(value):\n                continue\n            # Get the previous definition (if any) and compare the signatures\n            prev_dfn = getattr(sup,name,None)\n            if prev_dfn:\n                prev_sig = signature(prev_dfn)\n                val_sig = signature(value)\n                if prev_sig != val_sig:\n                    logging.warning('Signature mismatch in %s. %s != %s',\n                                value.__qualname__, prev_sig, val_sig)\n# Example\nclass Root(metaclass=MatchSignaturesMeta):\n    pass\nclass A(Root):\n    def foo(self, x, y):\n        pass\n    def spam(self, x, *, z):\n        pass\n# Class with redefined methods, but slightly different signatures\nclass B(A):\n    def foo(self, a, b):\n        pass\n    def spam(self,x,z):\n        pass\nIf you run this code, you will get output such as the following:\n    WARNING:root:Signature mismatch in B.spam. (self, x, *, z) != (self, x, z)\n    WARNING:root:Signature mismatch in B.foo. (self, x, y) != (self, a, b)\nSuch warnings might be useful in catching subtle program bugs. For example, code that\nrelies on keyword argument passing to a method will break if a subclass changes the\nargument names.\nDiscussion\nIn large object-oriented programs, it can sometimes be useful to put class definitions\nunder the control of a metaclass. The metaclass can observe class definitions and be\nused to alert programmers to potential problems that might go unnoticed (e.g., using\nslightly incompatible method signatures).\n9.17. Enforcing Coding Conventions in Classes \n| \n369",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": 9,
      "content": "One might argue that such errors would be better caught by program analysis tools or\nIDEs. To be sure, such tools are useful. However, if you’re creating a framework or library\nthat’s going to be used by others, you often don’t have any control over the rigor of their\ndevelopment practices. Thus, for certain kinds of applications, it might make sense to\nput a bit of extra checking in a metaclass if such checking would result in a better user\nexperience.\nThe choice of redefining __new__() or __init__() in a metaclass depends on how you\nwant to work with the resulting class. __new__() is invoked prior to class creation and\nis typically used when a metaclass wants to alter the class definition in some way (by\nchanging the contents of the class dictionary). The __init__() method is invoked after\na class has been created, and is useful if you want to write code that works with the fully\nformed class object. In the last example, this is essential since it is using the super()\nfunction to search for prior definitions. This only works once the class instance has been\ncreated and the underlying method resolution order has been set.\nThe last example also illustrates the use of Python’s function signature objects. Essen‐\ntially, the metaclass takes each callable definition in a class, searches for a prior definition\n(if any), and then simply compares their calling signatures using inspect.signature().\nLast, but not least, the line of code that uses super(self, self) is not a typo. When\nworking with a metaclass, it’s important to realize that the self is actually a class object.\nSo, that statement is actually being used to find definitions located further up the class\nhierarchy that make up the parents of self.\n9.18. Defining Classes Programmatically\nProblem\nYou’re writing code that ultimately needs to create a new class object. You’ve thought\nabout emitting emit class source code to a string and using a function such as exec()\nto evaluate it, but you’d prefer a more elegant solution.\nSolution\nYou can use the function types.new_class() to instantiate new class objects. All you\nneed to do is provide the name of the class, tuple of parent classes, keyword arguments,\nand a callback that populates the class dictionary with members. For example:\n# stock.py\n# Example of making a class manually from parts\n# Methods\ndef __init__(self, name, shares, price):\n    self.name = name\n370 \n| \nChapter 9: Metaprogramming",
      "content_length": 2434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": 9,
      "content": "self.shares = shares\n    self.price = price\ndef cost(self):\n    return self.shares * self.price\ncls_dict = {\n    '__init__' : __init__,\n    'cost' : cost,\n}\n# Make a class\nimport types\nStock = types.new_class('Stock', (), {}, lambda ns: ns.update(cls_dict))\nStock.__module__ = __name__\nThis makes a normal class object that works just like you expect:\n>>> s = Stock('ACME', 50, 91.1)\n>>> s\n<stock.Stock object at 0x1006a9b10>\n>>> s.cost()\n4555.0\n>>>\nA subtle facet of the solution is the assignment to Stock.__module__ after the call to\ntypes.new_class(). Whenever a class is defined, its __module__ attribute contains the\nname of the module in which it was defined. This name is used to produce the output\nmade by methods such as __repr__(). It’s also used by various libraries, such as pick\nle. Thus, in order for the class you make to be “proper,” you need to make sure this\nattribute is set accordingly.\nIf the class you want to create involves a different metaclass, it would be specified in the\nthird argument to types.new_class(). For example:\n>>> import abc\n>>> Stock = types.new_class('Stock', (), {'metaclass': abc.ABCMeta},\n...                         lambda ns: ns.update(cls_dict))\n...\n>>> Stock.__module__ = __name__\n>>> Stock\n<class '__main__.Stock'>\n>>> type(Stock)\n<class 'abc.ABCMeta'>\n>>>\nThe third argument may also contain other keyword arguments. For example, a class\ndefinition like this\nclass Spam(Base, debug=True, typecheck=False):\n    ...\n9.18. Defining Classes Programmatically \n| \n371",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": 9,
      "content": "would translate to a new_class() call similar to this:\nSpam = types.new_class('Spam', (Base,),\n                       {'debug': True, 'typecheck': False},\n                       lambda ns: ns.update(cls_dict))\nThe fourth argument to new_class() is the most mysterious, but it is a function that\nreceives the mapping object being used for the class namespace as input. This is normally\na dictionary, but it’s actually whatever object gets returned by the __prepare__() meth‐\nod, as described in Recipe 9.14. This function should add new entries to the namespace\nusing the update() method (as shown) or other mapping operations.\nDiscussion\nBeing able to manufacture new class objects can be useful in certain contexts. One of\nthe more familiar examples involves the collections.namedtuple() function. For\nexample:\n>>> Stock = collections.namedtuple('Stock', ['name', 'shares', 'price'])\n>>> Stock\n<class '__main__.Stock'>\n>>>\nnamedtuple() uses exec() instead of the technique shown here. However, here is a\nsimple variant that creates a class directly:\nimport operator\nimport types\nimport sys\ndef named_tuple(classname, fieldnames):\n    # Populate a dictionary of field property accessors\n    cls_dict = { name: property(operator.itemgetter(n))\n                 for n, name in enumerate(fieldnames) }\n    # Make a __new__ function and add to the class dict\n    def __new__(cls, *args):\n        if len(args) != len(fieldnames):\n            raise TypeError('Expected {} arguments'.format(len(fieldnames)))\n        return tuple.__new__(cls, args)\n    cls_dict['__new__'] = __new__\n    # Make the class\n    cls = types.new_class(classname, (tuple,), {},\n                          lambda ns: ns.update(cls_dict))\n    # Set the module to that of the caller\n    cls.__module__ = sys._getframe(1).f_globals['__name__']\n    return cls\n372 \n| \nChapter 9: Metaprogramming",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": 9,
      "content": "The last part of this code uses a so-called “frame hack” involving sys._getframe() to\nobtain the module name of the caller. Another example of frame hacking appears in\nRecipe 2.15.\nThe following example shows how the preceding code works:\n>>> Point = named_tuple('Point', ['x', 'y'])\n>>> Point\n<class '__main__.Point'>\n>>> p = Point(4, 5)\n>>> len(p)\n2\n>>> p.x\n4\n>>> p.y\n5\n>>> p.x = 2\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: can't set attribute\n>>> print('%s %s' % p)\n4 5\n>>>\nOne important aspect of the technique used in this recipe is its proper support for\nmetaclasses. You might be inclined to create a class directly by instantiating a metaclass\ndirectly. For example:\nStock = type('Stock', (), cls_dict)\nThe problem is that this approach skips certain critical steps, such as invocation of the\nmetaclass __prepare__() method. By using types.new_class() instead, you ensure\nthat all of the necessary initialization steps get carried out. For instance, the callback\nfunction that’s given as the fourth argument to types.new_class() receives the map‐\nping object that’s returned by the __prepare__() method.\nIf you only want to carry out the preparation step, use types.prepare_class(). For\nexample:\nimport types\nmetaclass, kwargs, ns = types.prepare_class('Stock', (), {'metaclass': type})\nThis finds the appropriate metaclass and invokes its __prepare__() method. The\nmetaclass, remaining keyword arguments, and prepared namespace are then returned.\nFor more information, see PEP 3115, as well as the Python documentation. \n9.18. Defining Classes Programmatically \n| \n373",
      "content_length": 1624,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": 9,
      "content": "9.19. Initializing Class Members at Definition Time\nProblem\nYou want to initialize parts of a class definition once at the time a class is defined, not\nwhen instances are created.\nSolution\nPerforming initialization or setup actions at the time of class definition is a classic use\nof metaclasses. Essentially, a metaclass is triggered at the point of a definition, at which\npoint you can perform additional steps.\nHere is an example that uses this idea to create classes similar to named tuples from the\ncollections module:\nimport operator\nclass StructTupleMeta(type):\n    def __init__(cls, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        for n, name in enumerate(cls._fields):\n            setattr(cls, name, property(operator.itemgetter(n)))\nclass StructTuple(tuple, metaclass=StructTupleMeta):\n    _fields = []\n    def __new__(cls, *args):\n        if len(args) != len(cls._fields):\n            raise ValueError('{} arguments required'.format(len(cls._fields)))\n        return super().__new__(cls,args)\nThis code allows simple tuple-based data structures to be defined, like this:\nclass Stock(StructTuple):\n    _fields = ['name', 'shares', 'price']\nclass Point(StructTuple):\n    _fields = ['x', 'y']\nHere’s how they work:\n>>> s = Stock('ACME', 50, 91.1)\n>>> s\n('ACME', 50, 91.1)\n>>> s[0]\n'ACME'\n>>> s.name\n'ACME'\n>>> s.shares * s.price\n4555.0\n>>> s.shares = 23\n374 \n| \nChapter 9: Metaprogramming",
      "content_length": 1416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": 9,
      "content": "Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: can't set attribute\n>>>\nDiscussion\nIn this recipe, the StructTupleMeta class takes the listing of attribute names in the\n_fields class attribute and turns them into property methods that access a particular\ntuple slot. The operator.itemgetter() function creates an accessor function and the\nproperty() function turns it into a property.\nThe trickiest part of this recipe is knowing when the different initialization steps occur.\nThe __init__() method in StructTupleMeta is only called once for each class that is\ndefined. The cls argument is the class that has just been defined. Essentially, the code\nis using the _fields class variable to take the newly defined class and add some new\nparts to it.\nThe StructTuple class serves as a common base class for users to inherit from. The\n__new__() method in that class is responsible for making new instances. The use of\n__new__() here is a bit unusual, but is partly related to the fact that we’re modifying the\ncalling signature of tuples so that we can create instances with code that uses a normal-\nlooking calling convention like this:\ns = Stock('ACME', 50, 91.1)        # OK\ns = Stock(('ACME', 50, 91.1))      # Error\nUnlike __init__(), the __new__() method gets triggered before an instance is created.\nSince tuples are immutable, it’s not possible to make any changes to them once they\nhave been created. An __init__() function gets triggered too late in the instance cre‐\nation process to do what we want. That’s why __new__() has been defined.\nAlthough this is a short recipe, careful study will reward the reader with a deep insight\nabout how Python classes are defined, how instances are created, and the points at which\ndifferent methods of metaclasses and classes are invoked.\nPEP 422 may provide an alternative means for performing the task described in this\nrecipe. However, as of this writing, it has not been adopted or accepted. Nevertheless,\nit might be worth a look in case you’re working with a version of Python newer than\nPython 3.3.\n9.19. Initializing Class Members at Definition Time \n| \n375",
      "content_length": 2151,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": 9,
      "content": "9.20. Implementing Multiple Dispatch with Function\nAnnotations\nProblem\nYou’ve learned about function argument annotations and you have a thought that you\nmight be able to use them to implement multiple-dispatch (method overloading) based\non types. However, you’re not quite sure what’s involved (or if it’s even a good idea).\nSolution\nThis recipe is based on a simple observation—namely, that since Python allows argu‐\nments to be annotated, perhaps it might be possible to write code like this:\nclass Spam:\n    def bar(self, x:int, y:int):\n        print('Bar 1:', x, y)\n    def bar(self, s:str, n:int = 0):\n        print('Bar 2:', s, n)\ns = Spam()\ns.bar(2, 3)     # Prints Bar 1: 2 3\ns.bar('hello')  # Prints Bar 2: hello 0\nHere is the start of a solution that does just that, using a combination of metaclasses and\ndescriptors:\n# multiple.py\nimport inspect\nimport types\nclass MultiMethod:\n    '''\n    Represents a single multimethod.\n    '''\n    def __init__(self, name):\n        self._methods = {}\n        self.__name__ = name\n    def register(self, meth):\n        '''\n        Register a new method as a multimethod\n        '''\n        sig = inspect.signature(meth)\n        # Build a type signature from the method's annotations\n        types = []\n        for name, parm in sig.parameters.items():\n            if name == 'self':\n376 \n| \nChapter 9: Metaprogramming",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": 9,
      "content": "continue\n            if parm.annotation is inspect.Parameter.empty:\n                raise TypeError(\n                    'Argument {} must be annotated with a type'.format(name)\n                )\n            if not isinstance(parm.annotation, type):\n                raise TypeError(\n                    'Argument {} annotation must be a type'.format(name)\n                )\n            if parm.default is not inspect.Parameter.empty:\n                self._methods[tuple(types)] = meth\n            types.append(parm.annotation)\n        self._methods[tuple(types)] = meth\n    def __call__(self, *args):\n        '''\n        Call a method based on type signature of the arguments\n        '''\n        types = tuple(type(arg) for arg in args[1:])\n        meth = self._methods.get(types, None)\n        if meth:\n            return meth(*args)\n        else:\n            raise TypeError('No matching method for types {}'.format(types))\n    def __get__(self, instance, cls):\n        '''\n        Descriptor method needed to make calls work in a class\n        '''\n        if instance is not None:\n            return types.MethodType(self, instance)\n        else:\n            return self\nclass MultiDict(dict):\n    '''\n    Special dictionary to build multimethods in a metaclass\n    '''\n    def __setitem__(self, key, value):\n        if key in self:\n            # If key already exists, it must be a multimethod or callable\n            current_value = self[key]\n            if isinstance(current_value, MultiMethod):\n                current_value.register(value)\n            else:\n                mvalue = MultiMethod(key)\n                mvalue.register(current_value)\n                mvalue.register(value)\n                super().__setitem__(key, mvalue)\n        else:\n            super().__setitem__(key, value)\n9.20. Implementing Multiple Dispatch with Function Annotations \n| \n377",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": 9,
      "content": "class MultipleMeta(type):\n    '''\n    Metaclass that allows multiple dispatch of methods\n    '''\n    def __new__(cls, clsname, bases, clsdict):\n        return type.__new__(cls, clsname, bases, dict(clsdict))\n    @classmethod\n    def __prepare__(cls, clsname, bases):\n        return MultiDict()\nTo use this class, you write code like this:\nclass Spam(metaclass=MultipleMeta):\n    def bar(self, x:int, y:int):\n        print('Bar 1:', x, y)\n    def bar(self, s:str, n:int = 0):\n        print('Bar 2:', s, n)\n# Example: overloaded __init__\nimport time\nclass Date(metaclass=MultipleMeta):\n    def __init__(self, year: int, month:int, day:int):\n        self.year = year\n        self.month = month\n        self.day = day\n    def __init__(self):\n        t = time.localtime()\n        self.__init__(t.tm_year, t.tm_mon, t.tm_mday)\nHere is an interactive session that verifies that it works:\n>>> s = Spam()\n>>> s.bar(2, 3)\nBar 1: 2 3\n>>> s.bar('hello')\nBar 2: hello 0\n>>> s.bar('hello', 5)\nBar 2: hello 5\n>>> s.bar(2, 'hello')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"multiple.py\", line 42, in __call__\n    raise TypeError('No matching method for types {}'.format(types))\nTypeError: No matching method for types (<class 'int'>, <class 'str'>)\n>>> # Overloaded __init__\n>>> d = Date(2012, 12, 21)\n>>> # Get today's date\n>>> e = Date()\n>>> e.year\n2012\n378 \n| \nChapter 9: Metaprogramming",
      "content_length": 1413,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": 9,
      "content": ">>> e.month\n12\n>>> e.day\n3\n>>>\nDiscussion\nHonestly, there might be too much magic going on in this recipe to make it applicable\nto real-world code. However, it does dive into some of the inner workings of metaclasses\nand descriptors, and reinforces some of their concepts. Thus, even though you might\nnot apply this recipe directly, some of its underlying ideas might influence other pro‐\ngramming techniques involving metaclasses, descriptors, and function annotations.\nThe main idea in the implementation is relatively simple. The MutipleMeta metaclass\nuses its __prepare__() method to supply a custom class dictionary as an instance of\nMultiDict. Unlike a normal dictionary, MultiDict checks to see whether entries already\nexist when items are set. If so, the duplicate entries get merged together inside an in‐\nstance of MultiMethod.\nInstances of MultiMethod collect methods by building a mapping from type signatures\nto functions. During construction, function annotations are used to collect these sig‐\nnatures and build the mapping. This takes place in the MultiMethod.register()\nmethod. One critical part of this mapping is that for multimethods, types must be\nspecified on all of the arguments or else an error occurs.\nTo make MultiMethod instances emulate a callable, the __call__() method is imple‐\nmented. This method builds a type tuple from all of the arguments except self, looks\nup the method in the internal map, and invokes the appropriate method. The __get__()\nis required to make MultiMethod instances operate correctly inside class definitions. In\nthe implementation, it’s being used to create proper bound methods. For example:\n>>> b = s.bar\n>>> b\n<bound method Spam.bar of <__main__.Spam object at 0x1006a46d0>>\n>>> b.__self__\n<__main__.Spam object at 0x1006a46d0>\n>>> b.__func__\n<__main__.MultiMethod object at 0x1006a4d50>\n>>> b(2, 3)\nBar 1: 2 3\n>>> b('hello')\nBar 2: hello 0\n>>>\nTo be sure, there are a lot of moving parts to this recipe. However, it’s all a little unfor‐\ntunate considering how many limitations there are. For one, the solution doesn’t work\nwith keyword arguments: For example:\n9.20. Implementing Multiple Dispatch with Function Annotations \n| \n379",
      "content_length": 2192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": 9,
      "content": ">>> s.bar(x=2, y=3)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: __call__() got an unexpected keyword argument 'y'\n>>> s.bar(s='hello')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: __call__() got an unexpected keyword argument 's'\n>>>\nThere might be some way to add such support, but it would require a completely dif‐\nferent approach to method mapping. The problem is that the keyword arguments don’t\narrive in any kind of particular order. When mixed up with positional arguments, you\nsimply get a jumbled mess of arguments that you have to somehow sort out in the\n__call__() method.\nThis recipe is also severely limited in its support for inheritance. For example, something\nlike this doesn’t work:\nclass A:\n    pass\nclass B(A):\n    pass\nclass C:\n    pass\nclass Spam(metaclass=MultipleMeta):\n    def foo(self, x:A):\n        print('Foo 1:', x)\n    def foo(self, x:C):\n        print('Foo 2:', x)\nThe reason it fails is that the x:A annotation fails to match instances that are subclasses\n(such as instances of B). For example:\n>>> s = Spam()\n>>> a = A()\n>>> s.foo(a)\nFoo 1: <__main__.A object at 0x1006a5310>\n>>> c = C()\n>>> s.foo(c)\nFoo 2: <__main__.C object at 0x1007a1910>\n>>> b = B()\n>>> s.foo(b)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"multiple.py\", line 44, in __call__\n    raise TypeError('No matching method for types {}'.format(types))\n380 \n| \nChapter 9: Metaprogramming",
      "content_length": 1499,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": 9,
      "content": "TypeError: No matching method for types (<class '__main__.B'>,)\n>>>\nAs an alternative to using metaclasses and annotations, it is possible to implement a\nsimilar recipe using decorators. For example:\nimport types\nclass multimethod:\n    def __init__(self, func):\n        self._methods = {}\n        self.__name__ = func.__name__\n        self._default = func\n    def match(self, *types):\n        def register(func):\n            ndefaults = len(func.__defaults__) if func.__defaults__ else 0\n            for n in range(ndefaults+1):\n                self._methods[types[:len(types) - n]] = func\n            return self\n        return register\n    def __call__(self, *args):\n        types = tuple(type(arg) for arg in args[1:])\n        meth = self._methods.get(types, None)\n        if meth:\n            return meth(*args)\n        else:\n            return self._default(*args)\n    def __get__(self, instance, cls):\n        if instance is not None:\n            return types.MethodType(self, instance)\n        else:\n            return self\nTo use the decorator version, you would write code like this:\nclass Spam:\n    @multimethod\n    def bar(self, *args):\n        # Default method called if no match\n        raise TypeError('No matching method for bar')\n    @bar.match(int, int)\n    def bar(self, x, y):\n        print('Bar 1:', x, y)\n    @bar.match(str, int)\n    def bar(self, s, n = 0):\n        print('Bar 2:', s, n)\n9.20. Implementing Multiple Dispatch with Function Annotations \n| \n381",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": 9,
      "content": "The decorator solution also suffers the same limitations as the previous implementation\n(namely, no support for keyword arguments and broken inheritance).\nAll things being equal, it’s probably best to stay away from multiple dispatch in general-\npurpose code. There are special situations where it might make sense, such as in pro‐\ngrams that are dispatching methods based on some kind of pattern matching. For ex‐\nample, perhaps the visitor pattern described in Recipe 8.21 could be recast into a class\nthat used multiple dispatch in some way. However, other than that, it’s usually never a\nbad idea to stick with a more simple approach (simply use methods with different\nnames).\nIdeas concerning different ways to implement multiple dispatch have floated around\nthe Python community for years. As a decent starting point for that discussion, see\nGuido van Rossum’s blog post “Five-Minute Multimethods in Python”. \n9.21. Avoiding Repetitive Property Methods\nProblem\nYou are writing classes where you are repeatedly having to define property methods that\nperform common tasks, such as type checking. You would like to simplify the code so\nthere is not so much code repetition.\nSolution\nConsider a simple class where attributes are being wrapped by property methods:\nclass Person:\n    def __init__(self, name ,age):\n        self.name = name\n        self.age = age\n    @property\n    def name(self):\n        return self._name\n    @name.setter\n    def name(self, value):\n        if not isinstance(value, str):\n            raise TypeError('name must be a string')\n        self._name = value\n    @property\n    def age(self):\n        return self._age\n    @age.setter\n    def age(self, value):\n382 \n| \nChapter 9: Metaprogramming",
      "content_length": 1720,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": 9,
      "content": "if not isinstance(value, int):\n            raise TypeError('age must be an int')\n        self._age = value\nAs you can see, a lot of code is being written simply to enforce some type assertions on\nattribute values. Whenever you see code like this, you should explore different ways of\nsimplifying it. One possible approach is to make a function that simply defines the\nproperty for you and returns it. For example:\ndef typed_property(name, expected_type):\n    storage_name = '_' + name\n    @property\n    def prop(self):\n        return getattr(self, storage_name)\n    @prop.setter\n    def prop(self, value):\n        if not isinstance(value, expected_type):\n            raise TypeError('{} must be a {}'.format(name, expected_type))\n        setattr(self, storage_name, value)\n    return prop\n# Example use\nclass Person:\n    name = typed_property('name', str)\n    age = typed_property('age', int)\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\nDiscussion\nThis recipe illustrates an important feature of inner function or closures—namely, their\nuse in writing code that works a lot like a macro. The typed_property() function in\nthis example may look a little weird, but it’s really just generating the property code for\nyou and returning the resulting property object. Thus, when it’s used in a class, it op‐\nerates exactly as if the code appearing inside typed_property() was placed into the\nclass definition itself. Even though the property getter and setter methods are accessing\nlocal variables such as name, expected_type, and storage_name, that is fine—those\nvalues are held behind the scenes in a closure.\nThis recipe can be tweaked in an interesting manner using the functools.partial()\nfunction. For example, you can do this:\nfrom functools import partial\nString = partial(typed_property, expected_type=str)\nInteger = partial(typed_property, expected_type=int)\n9.21. Avoiding Repetitive Property Methods \n| \n383",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": 9,
      "content": "# Example:\nclass Person:\n    name = String('name')\n    age = Integer('age')\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\nHere the code is starting to look a lot like some of the type system descriptor code shown\nin Recipe 8.13.\n9.22. Defining Context Managers the Easy Way\nProblem\nYou want to implement new kinds of context managers for use with the with statement.\nSolution\nOne of the most straightforward ways to write a new context manager is to use the\n@contextmanager decorator in the contextlib module. Here is an example of a context\nmanager that times the execution of a code block:\nimport time\nfrom contextlib import contextmanager\n@contextmanager\ndef timethis(label):\n    start = time.time()\n    try:\n        yield\n    finally:\n        end = time.time()\n        print('{}: {}'.format(label, end - start))\n# Example use\nwith timethis('counting'):\n    n = 10000000\n    while n > 0:\n        n -= 1\nIn the timethis() function, all of the code prior to the yield executes as the __en\nter__() method of a context manager. All of the code after the yield executes as the\n__exit__() method. If there was an exception, it is raised at the yield statement.\nHere is a slightly more advanced context manager that implements a kind of transaction\non a list object:\n@contextmanager\ndef list_transaction(orig_list):\n384 \n| \nChapter 9: Metaprogramming",
      "content_length": 1383,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": 9,
      "content": "working = list(orig_list)\n    yield working\n    orig_list[:] = working\nThe idea here is that changes made to a list only take effect if an entire code block runs\nto completion with no exceptions. Here is an example that illustrates:\n>>> items = [1, 2, 3]\n>>> with list_transaction(items) as working:\n...     working.append(4)\n...     working.append(5)\n...\n>>> items\n[1, 2, 3, 4, 5]\n>>> with list_transaction(items) as working:\n...     working.append(6)\n...     working.append(7)\n...     raise RuntimeError('oops')\n...\nTraceback (most recent call last):\n  File \"<stdin>\", line 4, in <module>\nRuntimeError: oops\n>>> items\n[1, 2, 3, 4, 5]\n>>>\nDiscussion\nNormally, to write a context manager, you define a class with an __enter__() and\n__exit__() method, like this:\nimport time\nclass timethis:\n    def __init__(self, label):\n        self.label = label\n    def __enter__(self):\n        self.start = time.time()\n    def __exit__(self, exc_ty, exc_val, exc_tb):\n        end = time.time()\n        print('{}: {}'.format(self.label, end - self.start))\nAlthough this isn’t hard, it’s a lot more tedious than writing a simple function using\n@contextmanager.\n@contextmanager is really only used for writing self-contained context-management\nfunctions. If you have some object (e.g., a file, network connection, or lock) that needs\nto support the with statement, you still need to implement the __enter__() and\n__exit__() methods separately.\n9.22. Defining Context Managers the Easy Way \n| \n385",
      "content_length": 1480,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": 9,
      "content": "9.23. Executing Code with Local Side Effects\nProblem\nYou are using exec() to execute a fragment of code in the scope of the caller, but after\nexecution, none of its results seem to be visible.\nSolution\nTo better understand the problem, try a little experiment. First, execute a fragment of\ncode in the global namespace:\n>>> a = 13\n>>> exec('b = a + 1')\n>>> print(b)\n14\n>>>\nNow, try the same experiment inside a function:\n>>> def test():\n...     a = 13\n...     exec('b = a + 1')\n...     print(b)\n...\n>>> test()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 4, in test\nNameError: global name 'b' is not defined\n>>>\nAs you can see, it fails with a NameError almost as if the exec() statement never actually\nexecuted. This can be a problem if you ever want to use the result of the exec() in a\nlater calculation.\nTo fix this kind of problem, you need to use the locals() function to obtain a dictionary\nof the local variables prior to the call to exec(). Immediately afterward, you can extract\nmodified values from the locals dictionary. For example:\n>>> def test():\n...     a = 13\n...     loc = locals()\n...     exec('b = a + 1')\n...     b = loc['b']\n...     print(b)\n...\n>>> test()\n14\n>>>\n386 \n| \nChapter 9: Metaprogramming",
      "content_length": 1271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": 9,
      "content": "Discussion\nCorrect use of exec() is actually quite tricky in practice. In fact, in most situations where\nyou might be considering the use of exec(), a more elegant solution probably exists\n(e.g., decorators, closures, metaclasses, etc.).\nHowever, if you still must use exec(), this recipe outlines some subtle aspects of using\nit correctly. By default, exec() executes code in the local and global scope of the caller.\nHowever, inside functions, the local scope passed to exec() is a dictionary that is a copy\nof the actual local variables. Thus, if the code in exec() makes any kind of modification,\nthat modification is never reflected in the actual local variables. Here is another example\nthat shows this effect:\n>>> def test1():\n...     x = 0\n...     exec('x += 1')\n...     print(x)\n...\n>>> test1()\n0\n>>>\nWhen you call locals() to obtain the local variables, as shown in the solution, you get\nthe copy of the locals that is passed to exec(). By inspecting the value of the dictionary\nafter execution, you can obtain the modified values. Here is an experiment that shows\nthis:\n>>> def test2():\n...     x = 0\n...     loc = locals()\n...     print('before:', loc)\n...     exec('x += 1')\n...     print('after:', loc)\n...     print('x =', x)\n...\n>>> test2()\nbefore: {'x': 0}\nafter: {'loc': {...}, 'x': 1}\nx = 0\n>>>\nCarefully observe the output of the last step. Unless you copy the modified value from\nloc back to x, the variable remains unchanged.\nWith any use of locals(), you need to be careful about the order of operations. Each\ntime it is invoked, locals() will take the current value of local variables and overwrite\nthe corresponding entries in the dictionary. Observe the outcome of this experiment:\n>>> def test3():\n...     x = 0\n9.23. Executing Code with Local Side Effects \n| \n387",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": 9,
      "content": "...     loc = locals()\n...     print(loc)\n...     exec('x += 1')\n...     print(loc)\n...     locals()\n...     print(loc)\n...\n>>> test3()\n{'x': 0}\n{'loc': {...}, 'x': 1}\n{'loc': {...}, 'x': 0}\n>>>\nNotice how the last call to locals() caused x to be overwritten.\nAs an alternative to using locals(), you might make your own dictionary and pass it\nto exec(). For example:\n>>> def test4():\n...     a = 13\n...     loc = { 'a' : a }\n...     glb = { }\n...     exec('b = a + 1', glb, loc)\n...     b = loc['b']\n...     print(b)\n...\n>>> test4()\n14\n>>>\nFor most uses of exec(), this is probably good practice. You just need to make sure that\nthe global and local dictionaries are properly initialized with names that the executed\ncode will access.\nLast, but not least, before using exec(), you might ask yourself if other alternatives are\navailable. Many problems where you might consider the use of exec() can be replaced\nby closures, decorators, metaclasses, or other metaprogramming features.\n9.24. Parsing and Analyzing Python Source\nProblem\nYou want to write programs that parse and analyze Python source code.\nSolution\nMost programmers know that Python can evaluate or execute source code provided in\nthe form of a string. For example:\n388 \n| \nChapter 9: Metaprogramming",
      "content_length": 1264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": 9,
      "content": ">>> x = 42\n>>> eval('2 + 3*4 + x')\n56\n>>> exec('for i in range(10): print(i)')\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n>>>\nHowever, the ast module can be used to compile Python source code into an abstract\nsyntax tree (AST) that can be analyzed. For example:\n>>> import ast\n>>> ex = ast.parse('2 + 3*4 + x', mode='eval')\n>>> ex\n<_ast.Expression object at 0x1007473d0>\n>>> ast.dump(ex)\n\"Expression(body=BinOp(left=BinOp(left=Num(n=2), op=Add(),\nright=BinOp(left=Num(n=3), op=Mult(), right=Num(n=4))), op=Add(),\nright=Name(id='x', ctx=Load())))\"\n>>> top = ast.parse('for i in range(10): print(i)', mode='exec')\n>>> top\n<_ast.Module object at 0x100747390>\n>>> ast.dump(top)\n\"Module(body=[For(target=Name(id='i', ctx=Store()),\niter=Call(func=Name(id='range', ctx=Load()), args=[Num(n=10)],\nkeywords=[], starargs=None, kwargs=None),\nbody=[Expr(value=Call(func=Name(id='print', ctx=Load()),\nargs=[Name(id='i', ctx=Load())], keywords=[], starargs=None,\nkwargs=None))], orelse=[])])\"\n>>>\nAnalyzing the source tree requires a bit of study on your part, but it consists of a col‐\nlection of AST nodes. The easiest way to work with these nodes is to define a visitor\nclass that implements various visit_NodeName() methods where NodeName() matches\nthe node of interest. Here is an example of such a class that records information about\nwhich names are loaded, stored, and deleted.\nimport ast\nclass CodeAnalyzer(ast.NodeVisitor):\n    def __init__(self):\n        self.loaded = set()\n        self.stored = set()\n9.24. Parsing and Analyzing Python Source \n| \n389",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": 9,
      "content": "self.deleted = set()\n    def visit_Name(self, node):\n        if isinstance(node.ctx, ast.Load):\n            self.loaded.add(node.id)\n        elif isinstance(node.ctx, ast.Store):\n            self.stored.add(node.id)\n        elif isinstance(node.ctx, ast.Del):\n            self.deleted.add(node.id)\n# Sample usage\nif __name__ == '__main__':\n    # Some Python code\n    code = '''\nfor i in range(10):\n    print(i)\ndel i\n'''\n    # Parse into an AST\n    top = ast.parse(code, mode='exec')\n    # Feed the AST to analyze name usage\n    c = CodeAnalyzer()\n    c.visit(top)\n    print('Loaded:', c.loaded)\n    print('Stored:', c.stored)\n    print('Deleted:', c.deleted)\nIf you run this program, you’ll get output like this:\nLoaded: {'i', 'range', 'print'}\nStored: {'i'}\nDeleted: {'i'}\nFinally, ASTs can be compiled and executed using the compile() function. For example:\n>>> exec(compile(top,'<stdin>', 'exec'))\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n>>>\nDiscussion\nThe fact that you can analyze source code and get information from it could be the start\nof writing various code analysis, optimization, or verification tools. For instance, instead\n390 \n| \nChapter 9: Metaprogramming",
      "content_length": 1153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": 9,
      "content": "of just blindly passing some fragment of code into a function like exec(), you could\nturn it into an AST first and look at it in some detail to see what it’s doing. You could\nalso write tools that look at the entire source code for a module and perform some sort\nof static analysis over it.\nIt should be noted that it is also possible to rewrite the AST to represent new code if you\nreally know what you’re doing. Here is an example of a decorator that lowers globally\naccessed names into the body of a function by reparsing the function body’s source code,\nrewriting the AST, and recreating the function’s code object:\n# namelower.py\nimport ast\nimport inspect\n# Node visitor that lowers globally accessed names into\n# the function body as local variables.\nclass NameLower(ast.NodeVisitor):\n    def __init__(self, lowered_names):\n        self.lowered_names = lowered_names\n    def visit_FunctionDef(self, node):\n        # Compile some assignments to lower the constants\n        code = '__globals = globals()\\n'\n        code += '\\n'.join(\"{0} = __globals['{0}']\".format(name)\n                          for name in self.lowered_names)\n        code_ast = ast.parse(code, mode='exec')\n        # Inject new statements into the function body\n        node.body[:0] = code_ast.body\n        # Save the function object\n        self.func = node\n# Decorator that turns global names into locals\ndef lower_names(*namelist):\n    def lower(func):\n        srclines = inspect.getsource(func).splitlines()\n        # Skip source lines prior to the @lower_names decorator\n        for n, line in enumerate(srclines):\n            if '@lower_names' in line:\n                break\n        src = '\\n'.join(srclines[n+1:])\n        # Hack to deal with indented code\n        if src.startswith((' ','\\t')):\n            src = 'if 1:\\n' + src\n        top = ast.parse(src, mode='exec')\n        # Transform the AST\n        cl = NameLower(namelist)\n9.24. Parsing and Analyzing Python Source \n| \n391",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": 9,
      "content": "cl.visit(top)\n        # Execute the modified AST\n        temp = {}\n        exec(compile(top,'','exec'), temp, temp)\n        # Pull out the modified code object\n        func.__code__ = temp[func.__name__].__code__\n        return func\n    return lower\nTo use this code, you would write code such as the following:\nINCR = 1\n@lower_names('INCR')\ndef countdown(n):\n    while n > 0:\n        n -= INCR\nThe decorator rewrites the source code of the countdown() function to look like this:\ndef countdown(n):\n    __globals = globals()\n    INCR = __globals['INCR']\n    while n > 0:\n        n -= INCR\nIn a performance test, it makes the function run about 20% faster.\nNow, should you go applying this decorator to all of your functions? Probably not.\nHowever, it’s a good illustration of some very advanced things that might be possible\nthrough AST manipulation, source code manipulation, and other techniques.\nThis recipe was inspired by a similar recipe at ActiveState that worked by manipulating\nPython’s byte code. Working with the AST is a higher-level approach that might be a bit\nmore straightforward. See the next recipe for more information about byte code.\n9.25. Disassembling Python Byte Code\nProblem\nYou want to know in detail what your code is doing under the covers by disassembling\nit into lower-level byte code used by the interpreter.\nSolution\nThe dis module can be used to output a disassembly of any Python function. For\nexample:\n392 \n| \nChapter 9: Metaprogramming",
      "content_length": 1471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": 9,
      "content": ">>> def countdown(n):\n...     while n > 0:\n...         print('T-minus', n)\n...         n -= 1\n...     print('Blastoff!')\n...\n>>> import dis\n>>> dis.dis(countdown)\n  2           0 SETUP_LOOP              39 (to 42)\n        >>    3 LOAD_FAST                0 (n)\n              6 LOAD_CONST               1 (0)\n              9 COMPARE_OP               4 (>)\n             12 POP_JUMP_IF_FALSE       41\n  3          15 LOAD_GLOBAL              0 (print)\n             18 LOAD_CONST               2 ('T-minus')\n             21 LOAD_FAST                0 (n)\n             24 CALL_FUNCTION            2 (2 positional, 0 keyword pair)\n             27 POP_TOP\n  4          28 LOAD_FAST                0 (n)\n             31 LOAD_CONST               3 (1)\n             34 INPLACE_SUBTRACT\n             35 STORE_FAST               0 (n)\n             38 JUMP_ABSOLUTE            3\n        >>   41 POP_BLOCK\n  5     >>   42 LOAD_GLOBAL              0 (print)\n             45 LOAD_CONST               4 ('Blastoff!')\n             48 CALL_FUNCTION            1 (1 positional, 0 keyword pair)\n             51 POP_TOP\n             52 LOAD_CONST               0 (None)\n             55 RETURN_VALUE\n>>>\nDiscussion\nThe dis module can be useful if you ever need to study what’s happening in your pro‐\ngram at a very low level (e.g., if you’re trying to understand performance characteristics).\nThe raw byte code interpreted by the dis() function is available on functions as follows:\n>>> countdown.__code__.co_code\nb\"x'\\x00|\\x00\\x00d\\x01\\x00k\\x04\\x00r)\\x00t\\x00\\x00d\\x02\\x00|\\x00\\x00\\x83\n\\x02\\x00\\x01|\\x00\\x00d\\x03\\x008}\\x00\\x00q\\x03\\x00Wt\\x00\\x00d\\x04\\x00\\x83\n\\x01\\x00\\x01d\\x00\\x00S\"\n>>>\nIf you ever want to interpret this code yourself, you would need to use some of the\nconstants defined in the opcode module. For example:\n9.25. Disassembling Python Byte Code \n| \n393",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": 9,
      "content": ">>> c = countdown.__code__.co_code\n>>> import opcode\n>>> opcode.opname[c[0]]\n>>> opcode.opname[c[0]]\n'SETUP_LOOP'\n>>> opcode.opname[c[3]]\n'LOAD_FAST'\n>>>\nIronically, there is no function in the dis module that makes it easy for you to process\nthe byte code in a programmatic way. However, this generator function will take the raw\nbyte code sequence and turn it into opcodes and arguments.\nimport opcode\ndef generate_opcodes(codebytes):\n    extended_arg = 0\n    i = 0\n    n = len(codebytes)\n    while i < n:\n        op = codebytes[i]\n        i += 1\n        if op >= opcode.HAVE_ARGUMENT:\n            oparg = codebytes[i] + codebytes[i+1]*256 + extended_arg\n            extended_arg = 0\n            i += 2\n            if op == opcode.EXTENDED_ARG:\n                extended_arg = oparg * 65536\n                continue\n        else:\n            oparg = None\n        yield (op, oparg)\nTo use this function, you would use code like this:\n>>> for op, oparg in generate_opcodes(countdown.__code__.co_code):\n...     print(op, opcode.opname[op], oparg)\n...\n120 SETUP_LOOP 39\n124 LOAD_FAST 0\n100 LOAD_CONST 1\n107 COMPARE_OP 4\n114 POP_JUMP_IF_FALSE 41\n116 LOAD_GLOBAL 0\n100 LOAD_CONST 2\n124 LOAD_FAST 0\n131 CALL_FUNCTION 2\n1 POP_TOP None\n124 LOAD_FAST 0\n100 LOAD_CONST 3\n56 INPLACE_SUBTRACT None\n125 STORE_FAST 0\n113 JUMP_ABSOLUTE 3\n394 \n| \nChapter 9: Metaprogramming",
      "content_length": 1357,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": 9,
      "content": "87 POP_BLOCK None\n116 LOAD_GLOBAL 0\n100 LOAD_CONST 4\n131 CALL_FUNCTION 1\n1 POP_TOP None\n100 LOAD_CONST 0\n83 RETURN_VALUE None\n>>>\nIt’s a little-known fact, but you can replace the raw byte code of any function that you\nwant. It takes a bit of work to do it, but here’s an example of what’s involved:\n>>> def add(x, y):\n...     return x + y\n...\n>>> c = add.__code__\n>>> c\n<code object add at 0x1007beed0, file \"<stdin>\", line 1>\n>>> c.co_code\nb'|\\x00\\x00|\\x01\\x00\\x17S'\n>>>\n>>> # Make a completely new code object with bogus byte code\n>>> import types\n>>> newbytecode = b'xxxxxxx'\n>>> nc = types.CodeType(c.co_argcount, c.co_kwonlyargcount,\n...    c.co_nlocals, c.co_stacksize, c.co_flags, newbytecode, c.co_consts,\n...    c.co_names, c.co_varnames, c.co_filename, c.co_name,\n...    c.co_firstlineno, c.co_lnotab)\n>>> nc\n<code object add at 0x10069fe40, file \"<stdin>\", line 1>\n>>> add.__code__ = nc\n>>> add(2,3)\nSegmentation fault\nHaving the interpreter crash is a pretty likely outcome of pulling a crazy stunt like this.\nHowever, developers working on advanced optimization and metaprogramming tools\nmight be inclined to rewrite byte code for real. This last part illustrates how to do it. See\nthis code on ActiveState for another example of such code in action.\n9.25. Disassembling Python Byte Code \n| \n395",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": 9,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 415,
      "chapter": 9,
      "content": "CHAPTER 10\nModules and Packages\nModules and packages are the core of any large project, and the Python installation\nitself. This chapter focuses on common programming techniques involving modules\nand packages, such as how to organize packages, splitting large modules into multiple\nfiles, and creating namespace packages. Recipes that allow you to customize the oper‐\nation of the import statement itself are also given.\n10.1. Making a Hierarchical Package of Modules\nProblem\nYou want to organize your code into a package consisting of a hierarchical collection of\nmodules.\nSolution\nMaking a package structure is simple. Just organize your code as you wish on the file-\nsystem and make sure that every directory defines an __init__.py file. For example:\n    graphics/\n        __init__.py\n        primitive/\n             __init__.py\n             line.py\n             fill.py\n             text.py\n        formats/\n             __init__.py\n             png.py\n             jpg.py\n397",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": 9,
      "content": "Once you have done this, you should be able to perform various import statements,\nsuch as the following:\nimport graphics.primitive.line\nfrom graphics.primitive import line\nimport graphics.formats.jpg as jpg\nDiscussion\nDefining a hierarchy of modules is as easy as making a directory structure on the file‐\nsystem. The purpose of the __init__.py files is to include optional initialization code\nthat runs as different levels of a package are encountered. For example, if you have the\nstatement import graphics, the file graphics/__init__.py will be imported and form\nthe contents of the graphics namespace. For an import such as import graphics.for\nmats.jpg, the files graphics/__init__.py and graphics/formats/__init__.py will both be\nimported prior to the final import of the graphics/formats/jpg.py file.\nMore often that not, it’s fine to just leave the __init__.py files empty. However, there are\ncertain situations where they might include code. For example, an __init__.py file can\nbe used to automatically load submodules like this:\n# graphics/formats/__init__.py\nfrom . import jpg\nfrom . import png\nFor such a file, a user merely has to use a single import graphics.formats instead of\na separate import for graphics.formats.jpg and graphics.formats.png.\nOther common uses of __init__.py include consolidating definitions from multiple files\ninto a single logical namespace, as is sometimes done when splitting modules. This is\ndiscussed in Recipe 10.4.\nAstute programmers will notice that Python 3.3 still seems to perform package imports\neven if no __init__.py files are present. If you don’t define __init__.py, you actually\ncreate what’s known as a “namespace package,” which is described in Recipe 10.5. All\nthings being equal, include the __init__.py files if you’re just starting out with the cre‐\nation of a new package.\n10.2. Controlling the Import of Everything\nProblem\nYou want precise control over the symbols that are exported from a module or package\nwhen a user uses the from module import * statement.\n398 \n| \nChapter 10: Modules and Packages",
      "content_length": 2064,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": 9,
      "content": "Solution\nDefine a variable __all__ in your module that explicitly lists the exported names. For\nexample:\n# somemodule.py\ndef spam():\n    pass\ndef grok():\n    pass\nblah = 42\n# Only export 'spam' and 'grok'\n__all__ = ['spam', 'grok']\nDiscussion\nAlthough the use of from module import * is strongly discouraged, it still sees frequent\nuse in modules that define a large number of names. If you don’t do anything, this form\nof import will export all names that don’t start with an underscore. On the other hand,\nif __all__ is defined, then only the names explicitly listed will be exported.\nIf you define __all__ as an empty list, then nothing will be exported. An AttributeEr\nror is raised on import if __all__ contains undefined names.\n10.3. Importing Package Submodules Using Relative\nNames\nProblem\nYou have code organized as a package and want to import a submodule from one of the\nother package submodules without hardcoding the package name into the import\nstatement.\nSolution\nTo import modules of a package from other modules in the same package, use a package-\nrelative import. For example, suppose you have a package mypackage organized as fol‐\nlows on the filesystem:\n    mypackage/\n        __init__.py\n        A/\n            __init__.py\n10.3. Importing Package Submodules Using Relative Names \n| \n399",
      "content_length": 1307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": 9,
      "content": "spam.py\n            grok.py\n        B/\n            __init__.py\n            bar.py\nIf the module mypackage.A.spam wants to import the module grok located in the same\ndirectory, it should include an import statement like this:\n# mypackage/A/spam.py\nfrom . import grok\nIf the same module wants to import the module B.bar located in a different directory,\nit can use an import statement like this:\n# mypackage/A/spam.py\nfrom ..B import bar\nBoth of the import statements shown operate relative to the location of the spam.py file\nand do not include the top-level package name.\nDiscussion\nInside packages, imports involving modules in the same package can either use fully\nspecified absolute names or a relative imports using the syntax shown. For example:\n# mypackage/A/spam.py\nfrom mypackage.A import grok      # OK\nfrom . import grok                # OK\nimport grok                       # Error (not found)\nThe downside of using an absolute name, such as mypackage.A, is that it hardcodes the\ntop-level package name into your source code. This, in turn, makes your code more\nbrittle and hard to work with if you ever want to reorganize it. For example, if you ever\nchanged the name of the package, you would have to go through all of your files and fix\nthe source code. Similarly, hardcoded names make it difficult for someone else to move\nthe code around. For example, perhaps someone wants to install two different versions\nof a package, differentiating them only by name. If relative imports are used, it would\nall work fine, whereas everything would break with absolute names.\nThe . and .. syntax on the import statement might look funny, but think of it as spec‐\nifying a directory name. . means look in the current directory and ..B means look in\nthe ../B directory. This syntax only works with the from form of import. For example:\nfrom . import grok         # OK\nimport .grok               # ERROR\n400 \n| \nChapter 10: Modules and Packages",
      "content_length": 1944,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": 9,
      "content": "Although it looks like you could navigate the filesystem using a relative import, they are\nnot allowed to escape the directory in which a package is defined. That is, combinations\nof dotted name patterns that would cause an import to occur from a non-package di‐\nrectory cause an error.\nFinally, it should be noted that relative imports only work for modules that are located\ninside a proper package. In particular, they do not work inside simple modules located\nat the top level of scripts. They also won’t work if parts of a package are executed directly\nas a script. For example:\n% python3 mypackage/A/spam.py      # Relative imports fail\nOn the other hand, if you execute the preceding script using the -m option to Python,\nthe relative imports will work properly. For example:\n% python3 -m mypackage.A.spam      # Relative imports work\nFor more background on relative package imports, see PEP 328. \n10.4. Splitting a Module into Multiple Files\nProblem\nYou have a module that you would like to split into multiple files. However, you would\nlike to do it without breaking existing code by keeping the separate files unified as a\nsingle logical module.\nSolution\nA program module can be split into separate files by turning it into a package. Consider\nthe following simple module:\n# mymodule.py\nclass A:\n    def spam(self):\n        print('A.spam')\nclass B(A):\n    def bar(self):\n        print('B.bar')\nSuppose you want to split mymodule.py into two files, one for each class definition. To\ndo that, start by replacing the mymodule.py file with a directory called mymodule. In\nthat directory, create the following files:\n10.4. Splitting a Module into Multiple Files \n| \n401",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": 9,
      "content": "mymodule/\n        __init__.py\n        a.py\n        b.py\nIn the a.py file, put this code:\n# a.py\nclass A:\n    def spam(self):\n        print('A.spam')\nIn the b.py file, put this code:\n# b.py\nfrom .a import A\nclass B(A):\n    def bar(self):\n        print('B.bar')\nFinally, in the __init__.py file, glue the two files together:\n# __init__.py\nfrom .a import A\nfrom .b import B\nIf you follow these steps, the resulting mymodule package will appear to be a single logical\nmodule:\n>>> import mymodule\n>>> a = mymodule.A()\n>>> a.spam()\nA.spam\n>>> b = mymodule.B()\n>>> b.bar()\nB.bar\n>>>\nDiscussion\nThe primary concern in this recipe is a design question of whether or not you want\nusers to work with a lot of small modules or just a single module. For example, in a large\ncode base, you could just break everything up into separate files and make users use a\nlot of import statements like this:\nfrom mymodule.a import A\nfrom mymodule.b import B\n...\n402 \n| \nChapter 10: Modules and Packages",
      "content_length": 978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": 9,
      "content": "This works, but it places more of a burden on the user to know where the different parts\nare located. Often, it’s just easier to unify things and allow a single import like this:\nfrom mymodule import A, B\nFor this latter case, it’s most common to think of mymodule as being one large source\nfile. However, this recipe shows how to stitch multiple files together into a single logical\nnamespace. The key to doing this is to create a package directory and to use the\n__init__.py file to glue the parts together.\nWhen a module gets split, you’ll need to pay careful attention to cross-filename refer‐\nences. For instance, in this recipe, class B needs to access class A as a base class. A package-\nrelative import from .a import A is used to get it.\nPackage-relative imports are used throughout the recipe to avoid hardcoding the top-\nlevel module name into the source code. This makes it easier to rename the module or\nmove it around elsewhere later (see Recipe 10.3).\nOne extension of this recipe involves the introduction of “lazy” imports. As shown, the\n__init__.py file imports all of the required subcomponents all at once. However, for a\nvery large module, perhaps you only want to load components as they are needed. To\ndo that, here is a slight variation of __init__.py:\n# __init__.py\ndef A():\n    from .a import A\n    return A()\ndef B():\n    from .b import B\n    return B()\nIn this version, classes A and B have been replaced by functions that load the desired\nclasses when they are first accessed. To a user, it won’t look much different. For example:\n>>> import mymodule\n>>> a = mymodule.A()\n>>> a.spam()\nA.spam\n>>>\nThe main downside of lazy loading is that inheritance and type checking might break.\nFor example, you might have to change your code slightly:\nif isinstance(x, mymodule.A):       # Error\n   ...\nif isinstance(x, mymodule.a.A):    # Ok\n   ...\n10.4. Splitting a Module into Multiple Files \n| \n403",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": 9,
      "content": "For a real-world example of lazy loading, look at the source code for multiprocessing/\n__init__.py in the standard library.\n10.5. Making Separate Directories of Code Import Under a\nCommon Namespace\nProblem\nYou have a large base of code with parts possibly maintained and distributed by different\npeople. Each part is organized as a directory of files, like a package. However, instead\nof having each part installed as a separated named package, you would like all of the\nparts to join together under a common package prefix.\nSolution\nEssentially, the problem here is that you would like to define a top-level Python package\nthat serves as a namespace for a large collection of separately maintained subpackages.\nThis problem often arises in large application frameworks where the framework devel‐\nopers want to encourage users to distribute plug-ins or add-on packages.\nTo unify separate directories under a common namespace, you organize the code just\nlike a normal Python package, but you omit __init__.py files in the directories where\nthe components are going to join together. To illustrate, suppose you have two different\ndirectories of Python code like this:\n    foo-package/\n        spam/\n             blah.py\n    bar-package/\n        spam/\n             grok.py\nIn these directories, the name spam is being used as a common namespace. Observe that\nthere is no __init__.py file in either directory.\nNow watch what happens if you add both foo-package and bar-package to the Python\nmodule path and try some imports:\n>>> import sys\n>>> sys.path.extend(['foo-package', 'bar-package'])\n>>> import spam.blah\n>>> import spam.grok\n>>>\nYou’ll observe that, by magic, the two different package directories merge together and\nyou can import either spam.blah or spam.grok. It just works.\n404 \n| \nChapter 10: Modules and Packages",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": 9,
      "content": "Discussion\nThe mechanism at work here is a feature known as a “namespace package.” Essentially,\na namespace package is a special kind of package designed for merging different direc‐\ntories of code together under a common namespace, as shown. For large frameworks,\nthis can be useful, since it allows parts of a framework to be broken up into separately\ninstalled downloads. It also enables people to easily make third-party add-ons and other\nextensions to such frameworks.\nThe key to making a namespace package is to make sure there are no __init__.py files\nin the top-level directory that is to serve as the common namespace. The missing\n__init__.py file causes an interesting thing to happen on package import. Instead of\ncausing an error, the interpreter instead starts creating a list of all directories that happen\nto contain a matching package name. A special namespace package module is then\ncreated and a read-only copy of the list of directories is stored in its __path__ variable.\nFor example:\n>>> import spam\n>>> spam.__path__\n_NamespacePath(['foo-package/spam', 'bar-package/spam'])\n>>>\nThe directories on __path__ are used when locating further package subcomponents\n(e.g., when importing spam.grok or spam.blah).\nAn important feature of namespace packages is that anyone can extend the namespace\nwith their own code. For example, suppose you made your own directory of code like\nthis:\n    my-package/\n         spam/\n             custom.py\nIf you added your directory of code to sys.path along with the other packages, it would\njust seamlessly merge together with the other spam package directories:\n>>> import spam.custom\n>>> import spam.grok\n>>> import spam.blah\n>>>\nAs a debugging tool, the main way that you can tell if a package is serving as a namespace\npackage is to check its __file__ attribute. If it’s missing altogether, the package is a\nnamespace. This will also be indicated in the representation string by the word “name‐\nspace”:\n>>> spam.__file__\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'module' object has no attribute '__file__'\n10.5. Making Separate Directories of Code Import Under a Common Namespace \n| \n405",
      "content_length": 2190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": 9,
      "content": ">>> spam\n<module 'spam' (namespace)>\n>>>\nFurther information about namespace packages can be found in PEP 420. \n10.6. Reloading Modules\nProblem\nYou want to reload an already loaded module because you’ve made changes to its source.\nSolution\nTo reload a previously loaded module, use imp.reload(). For example:\n>>> import spam\n>>> import imp\n>>> imp.reload(spam)\n<module 'spam' from './spam.py'>\n>>>\nDiscussion\nReloading a module is something that is often useful during debugging and develop‐\nment, but which is generally never safe in production code due to the fact that it doesn’t\nalways work as you expect.\nUnder the covers, the reload() operation wipes out the contents of a module’s under‐\nlying dictionary and refreshes it by re-executing the module’s source code. The identity\nof the module object itself remains unchanged. Thus, this operation updates the module\neverywhere that it has been imported in a program.\nHowever, reload() does not update definitions that have been imported using state‐\nments such as from module import name. To illustrate, consider the following code:\n# spam.py\ndef bar():\n    print('bar')\ndef grok():\n    print('grok')\nNow start an interactive session:\n>>> import spam\n>>> from spam import grok\n>>> spam.bar()\nbar\n406 \n| \nChapter 10: Modules and Packages",
      "content_length": 1291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": 9,
      "content": ">>> grok()\ngrok\n>>>\nWithout quitting Python, go edit the source code to spam.py so that the function grok()\nlooks like this:\ndef grok():\n    print('New grok')\nNow go back to the interactive session, perform a reload, and try this experiment:\n>>> import imp\n>>> imp.reload(spam)\n<module 'spam' from './spam.py'>\n>>> spam.bar()\nbar\n>>> grok()             # Notice old output\ngrok\n>>> spam.grok()        # Notice new output\nNew grok\n>>>\nIn this example, you’ll observe that there are two versions of the grok() function loaded.\nGenerally, this is not what you want, and is just the sort of thing that eventually leads\nto massive headaches.\nFor this reason, reloading of modules is probably something to be avoided in production\ncode. Save it for debugging or for interactive sessions where you’re experimenting with\nthe interpreter and trying things out.\n10.7. Making a Directory or Zip File Runnable As a Main\nScript\nProblem\nYou have a program that has grown beyond a simple script into an application involving\nmultiple files. You’d like to have some easy way for users to run the program.\nSolution\nIf your application program has grown into multiple files, you can put it into its own\ndirectory and add a __main__.py file. For example, you can create a directory like this:\n    myapplication/\n         spam.py\n         bar.py\n         grok.py\n         __main__.py\n10.7. Making a Directory or Zip File Runnable As a Main Script \n| \n407",
      "content_length": 1434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": 9,
      "content": "If __main__.py is present, you can simply run the Python interpreter on the top-level\ndirectory like this:\nbash % python3 myapplication\nThe interpreter will execute the __main__.py file as the main program.\nThis technique also works if you package all of your code up into a zip file. For example:\n    bash % ls\n    spam.py    bar.py   grok.py   __main__.py\n    bash % zip -r myapp.zip *.py\n    bash % python3 myapp.zip\n    ... output from __main__.py ...\nDiscussion\nCreating a directory or zip file and adding a __main__.py file is one possible way to\npackage a larger Python application. It’s a little bit different than a package in that the\ncode isn’t meant to be used as a standard library module that’s installed into the Python\nlibrary. Instead, it’s just this bundle of code that you want to hand someone to execute.\nSince directories and zip files are a little different than normal files, you may also want\nto add a supporting shell script to make execution easier. For example, if the code was\nin a file named myapp.zip, you could make a top-level script like this:\n    #!/usr/bin/env python3 /usr/local/bin/myapp.zip\n10.8. Reading Datafiles Within a Package\nProblem\nYour package includes a datafile that your code needs to read. You need to do this in the\nmost portable way possible.\nSolution\nSuppose you have a package with files organized as follows:\nmypackage/\n    __init__.py\n    somedata.dat\n    spam.py\nNow suppose the file spam.py wants to read the contents of the file somedata.dat. To do\nit, use the following code:\n408 \n| \nChapter 10: Modules and Packages",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": 9,
      "content": "# spam.py\nimport pkgutil\ndata = pkgutil.get_data(__package__, 'somedata.dat')\nThe resulting variable data will be a byte string containing the raw contents of the file.\nDiscussion\nTo read a datafile, you might be inclined to write code that uses built-in I/O functions,\nsuch as open(). However, there are several problems with this approach.\nFirst, a package has very little control over the current working directory of the inter‐\npreter. Thus, any I/O operations would have to be programmed to use absolute file‐\nnames. Since each module includes a __file__ variable with the full path, it’s not im‐\npossible to figure out the location, but it’s messy.\nSecond, packages are often installed as .zip or .egg files, which don’t preserve the files in\nthe same way as a normal directory on the filesystem. Thus, if you tried to use open()\non a datafile contained in an archive, it wouldn’t work at all.\nThe pkgutil.get_data() function is meant to be a high-level tool for getting a datafile\nregardless of where or how a package has been installed. It will simply “work” and return\nthe file contents back to you as a byte string.\nThe first argument to get_data() is a string containing the package name. You can\neither supply it directly or use a special variable, such as __package__. The second\nargument is the relative name of the file within the package. If necessary, you can nav‐\nigate into different directories using standard Unix filename conventions as long as the\nfinal directory is still located within the package.\n10.9. Adding Directories to sys.path\nProblem\nYou have Python code that can’t be imported because it’s not located in a directory listed\nin sys.path. You would like to add new directories to Python’s path, but don’t want to\nhardwire it into your code.\nSolution\nThere are two common ways to get new directories added to sys.path. First, you can\nadd them through the use of the PYTHONPATH environment variable. For example:\n    bash % env PYTHONPATH=/some/dir:/other/dir python3\n    Python 3.3.0 (default, Oct  4 2012, 10:17:33)\n    [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\n10.9. Adding Directories to sys.path \n| \n409",
      "content_length": 2152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": 9,
      "content": "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> import sys\n    >>> sys.path\n    ['', '/some/dir', '/other/dir', ...]\n    >>>\nIn a custom application, this environment variable could be set at program startup or\nthrough a shell script of some kind.\nThe second approach is to create a .pth file that lists the directories like this:\n    # myapplication.pth\n    /some/dir\n    /other/dir\nThis .pth file needs to be placed into one of Python’s site-packages directories, which are\ntypically located at /usr/local/lib/python3.3/site-packages or ~/.local/lib/python3.3/site-\npackages. On interpreter startup, the directories listed in the .pth file will be added to\nsys.path as long as they exist on the filesystem. Installation of a .pth file might require\nadministrator access if it’s being added to the system-wide Python interpreter.\nDiscussion\nFaced with trouble locating files, you might be inclined to write code that manually\nadjusts the value of sys.path. For example:\nimport sys\nsys.path.insert(0, '/some/dir')\nsys.path.insert(0, '/other/dir')\nAlthough this “works,” it is extremely fragile in practice and should be avoided if pos‐\nsible. Part of the problem with this approach is that it adds hardcoded directory names\nto your source. This can cause maintenance problems if your code ever gets moved\naround to a new location. It’s usually much better to configure the path elsewhere in a\nmanner that can be adjusted without making source code edits.\nYou can sometimes work around the problem of hardcoded directories if you carefully\nconstruct an appropriate absolute path using module-level variables, such as\n__file__. For example:\nimport sys\nfrom os.path import abspath, join, dirname\nsys.path.insert(0, abspath(dirname('__file__'), 'src'))\nThis adds an src directory to the path where that directory is located in the same direc‐\ntory as the code that’s executing the insertion step.\nThe site-packages directories are the locations where third-party modules and packages\nnormally get installed. If your code was installed in that manner, that’s where it would\nbe placed. Although .pth files for configuring the path must appear in site-packages, they\n410 \n| \nChapter 10: Modules and Packages",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": 9,
      "content": "can refer to any directories on the system that you wish. Thus, you can elect to have\nyour code in a completely different set of directories as long as those directories are\nincluded in a .pth file.\n10.10. Importing Modules Using a Name Given in a String\nProblem\nYou have the name of a module that you would like to import, but it’s being held in a\nstring. You would like to invoke the import command on the string.\nSolution\nUse the importlib.import_module() function to manually import a module or part of\na package where the name is given as a string. For example:\n>>> import importlib\n>>> math = importlib.import_module('math')\n>>> math.sin(2)\n0.9092974268256817\n>>> mod = importlib.import_module('urllib.request')\n>>> u = mod.urlopen('http://www.python.org')\n>>>\nimport_module simply performs the same steps as import, but returns the resulting\nmodule object back to you as a result. You just need to store it in a variable and use it\nlike a normal module afterward.\nIf you are working with packages, import_module() can also be used to perform relative\nimports. However, you need to give it an extra argument. For example:\nimport importlib\n# Same as 'from . import b'\nb = importlib.import_module('.b', __package__)\nDiscussion\nThe problem of manually importing modules with import_module() most commonly\narises when writing code that manipulates or wraps around modules in some way. For\nexample, perhaps you’re implementing a customized importing mechanism of some\nkind where you need to load a module by name and perform patches to the loaded code.\nIn older code, you will sometimes see the built-in __import__() function used to per‐\nform imports. Although this works, importlib.import_module() is usually easier to\nuse.\nSee Recipe 10.11 for an advanced example of customizing the import process.\n10.10. Importing Modules Using a Name Given in a String \n| \n411",
      "content_length": 1866,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": 9,
      "content": "10.11. Loading Modules from a Remote Machine Using\nImport Hooks\nProblem\nYou would like to customize Python’s import statement so that it can transparently load\nmodules from a remote machine.\nSolution\nFirst, a serious disclaimer about security. The idea discussed in this recipe would be\nwholly bad without some kind of extra security and authentication layer. That said, the\nmain goal is actually to take a deep dive into the inner workings of Python’s import\nstatement. If you get this recipe to work and understand the inner workings, you’ll have\na solid foundation of customizing import for almost any other purpose. With that out\nof the way, let’s carry on.\nAt the core of this recipe is a desire to extend the functionality of the import statement.\nThere are several approaches for doing this, but for the purposes of illustration, start by\nmaking the following directory of Python code:\ntestcode/\n    spam.py\n    fib.py\n    grok/\n        __init__.py\n        blah.py\nThe content of these files doesn’t matter, but put a few simple statements and functions\nin each file so you can test them and see output when they’re imported. For example:\n# spam.py\nprint(\"I'm spam\")\ndef hello(name):\n    print('Hello %s' % name)\n# fib.py\nprint(\"I'm fib\")\ndef fib(n):\n    if n < 2:\n        return 1\n    else:\n        return fib(n-1) + fib(n-2)\n# grok/__init__.py\nprint(\"I'm grok.__init__\")\n412 \n| \nChapter 10: Modules and Packages",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": 9,
      "content": "# grok/blah.py\nprint(\"I'm grok.blah\")\nThe goal here is to allow remote access to these files as modules. Perhaps the easiest way\nto do this is to publish them on a web server. Simply go to the testcode directory and\nrun Python like this:\nbash % cd testcode\nbash % python3 -m http.server 15000\nServing HTTP on 0.0.0.0 port 15000 ...\nLeave that server running and start up a separate Python interpreter. Make sure you can\naccess the remote files using urllib. For example:\n>>> from urllib.request import urlopen\n>>> u = urlopen('http://localhost:15000/fib.py')\n>>> data = u.read().decode('utf-8')\n>>> print(data)\n# fib.py\nprint(\"I'm fib\")\ndef fib(n):\n    if n < 2:\n        return 1\n    else:\n        return fib(n-1) + fib(n-2)\n>>>\nLoading source code from this server is going to form the basis for the remainder of\nthis recipe. Specifically, instead of manually grabbing a file of source code using urlop\nen(), the import statement will be customized to do it transparently behind the scenes.\nThe first approach to loading a remote module is to create an explicit loading function\nfor doing it. For example:\nimport imp\nimport urllib.request\nimport sys\ndef load_module(url):\n    u = urllib.request.urlopen(url)\n    source = u.read().decode('utf-8')\n    mod = sys.modules.setdefault(url, imp.new_module(url))\n    code = compile(source, url, 'exec')\n    mod.__file__ = url\n    mod.__package__ = ''\n    exec(code, mod.__dict__)\n    return mod\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n413",
      "content_length": 1509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": 9,
      "content": "This function merely downloads the source code, compiles it into a code object using\ncompile(), and executes it in the dictionary of a newly created module object. Here’s\nhow you would use the function:\n>>> fib = load_module('http://localhost:15000/fib.py')\nI'm fib\n>>> fib.fib(10)\n89\n>>> spam = load_module('http://localhost:15000/spam.py')\nI'm spam\n>>> spam.hello('Guido')\nHello Guido\n>>> fib\n<module 'http://localhost:15000/fib.py' from 'http://localhost:15000/fib.py'>\n>>> spam\n<module 'http://localhost:15000/spam.py' from 'http://localhost:15000/spam.py'>\n>>>\nAs you can see, it “works” for simple modules. However, it’s not plugged into the usual\nimport statement, and extending the code to support more advanced constructs, such\nas packages, would require additional work.\nA much slicker approach is to create a custom importer. The first way to do this is to\ncreate what’s known as a meta path importer. Here is an example:\n# urlimport.py\nimport sys\nimport importlib.abc\nimport imp\nfrom urllib.request import urlopen\nfrom urllib.error import HTTPError, URLError\nfrom html.parser import HTMLParser\n# Debugging\nimport logging\nlog = logging.getLogger(__name__)\n# Get links from a given URL\ndef _get_links(url):\n    class LinkParser(HTMLParser):\n        def handle_starttag(self, tag, attrs):\n            if tag == 'a':\n                attrs = dict(attrs)\n                links.add(attrs.get('href').rstrip('/'))\n    links = set()\n    try:\n        log.debug('Getting links from %s' % url)\n        u = urlopen(url)\n        parser = LinkParser()\n        parser.feed(u.read().decode('utf-8'))\n414 \n| \nChapter 10: Modules and Packages",
      "content_length": 1635,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": 9,
      "content": "except Exception as e:\n        log.debug('Could not get links. %s', e)\n    log.debug('links: %r', links)\n    return links\nclass UrlMetaFinder(importlib.abc.MetaPathFinder):\n    def __init__(self, baseurl):\n        self._baseurl = baseurl\n        self._links   = { }\n        self._loaders = { baseurl : UrlModuleLoader(baseurl) }\n    def find_module(self, fullname, path=None):\n        log.debug('find_module: fullname=%r, path=%r', fullname, path)\n        if path is None:\n            baseurl = self._baseurl\n        else:\n            if not path[0].startswith(self._baseurl):\n                return None\n            baseurl = path[0]\n        parts = fullname.split('.')\n        basename = parts[-1]\n        log.debug('find_module: baseurl=%r, basename=%r', baseurl, basename)\n        # Check link cache\n        if basename not in self._links:\n            self._links[baseurl] = _get_links(baseurl)\n        # Check if it's a package\n        if basename in self._links[baseurl]:\n            log.debug('find_module: trying package %r', fullname)\n            fullurl = self._baseurl + '/' + basename\n            # Attempt to load the package (which accesses __init__.py)\n            loader = UrlPackageLoader(fullurl)\n            try:\n                loader.load_module(fullname)\n                self._links[fullurl] = _get_links(fullurl)\n                self._loaders[fullurl] = UrlModuleLoader(fullurl)\n                log.debug('find_module: package %r loaded', fullname)\n            except ImportError as e:\n                log.debug('find_module: package failed. %s', e)\n                loader = None\n            return loader\n        # A normal module\n        filename = basename + '.py'\n        if filename in self._links[baseurl]:\n            log.debug('find_module: module %r found', fullname)\n            return self._loaders[baseurl]\n        else:\n            log.debug('find_module: module %r not found', fullname)\n            return None\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n415",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": 9,
      "content": "def invalidate_caches(self):\n        log.debug('invalidating link cache')\n        self._links.clear()\n# Module Loader for a URL\nclass UrlModuleLoader(importlib.abc.SourceLoader):\n    def __init__(self, baseurl):\n        self._baseurl = baseurl\n        self._source_cache = {}\n    def module_repr(self, module):\n        return '<urlmodule %r from %r>' % (module.__name__, module.__file__)\n    # Required method\n    def load_module(self, fullname):\n        code = self.get_code(fullname)\n        mod = sys.modules.setdefault(fullname, imp.new_module(fullname))\n        mod.__file__ = self.get_filename(fullname)\n        mod.__loader__ = self\n        mod.__package__ = fullname.rpartition('.')[0]\n        exec(code, mod.__dict__)\n        return mod\n    # Optional extensions\n    def get_code(self, fullname):\n        src = self.get_source(fullname)\n        return compile(src, self.get_filename(fullname), 'exec')\n    def get_data(self, path):\n        pass\n    def get_filename(self, fullname):\n        return self._baseurl + '/' + fullname.split('.')[-1] + '.py'\n    def get_source(self, fullname):\n        filename = self.get_filename(fullname)\n        log.debug('loader: reading %r', filename)\n        if filename in self._source_cache:\n            log.debug('loader: cached %r', filename)\n            return self._source_cache[filename]\n        try:\n            u = urlopen(filename)\n            source = u.read().decode('utf-8')\n            log.debug('loader: %r loaded', filename)\n            self._source_cache[filename] = source\n            return source\n        except (HTTPError, URLError) as e:\n            log.debug('loader: %r failed.  %s', filename, e)\n            raise ImportError(\"Can't load %s\" % filename)\n416 \n| \nChapter 10: Modules and Packages",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": 9,
      "content": "def is_package(self, fullname):\n        return False\n# Package loader for a URL\nclass UrlPackageLoader(UrlModuleLoader):\n    def load_module(self, fullname):\n        mod = super().load_module(fullname)\n        mod.__path__ = [ self._baseurl ]\n        mod.__package__ = fullname\n    def get_filename(self, fullname):\n        return self._baseurl + '/' + '__init__.py'\n    def is_package(self, fullname):\n        return True\n# Utility functions for installing/uninstalling the loader\n_installed_meta_cache = { }\ndef install_meta(address):\n    if address not in _installed_meta_cache:\n        finder = UrlMetaFinder(address)\n        _installed_meta_cache[address] = finder\n        sys.meta_path.append(finder)\n        log.debug('%r installed on sys.meta_path', finder)\ndef remove_meta(address):\n    if address in _installed_meta_cache:\n        finder = _installed_meta_cache.pop(address)\n        sys.meta_path.remove(finder)\n        log.debug('%r removed from sys.meta_path', finder)\nHere is an interactive session showing how to use the preceding code:\n>>> # importing currently fails\n>>> import fib\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'fib'\n>>> # Load the importer and retry (it works)\n>>> import urlimport\n>>> urlimport.install_meta('http://localhost:15000')\n>>> import fib\nI'm fib\n>>> import spam\nI'm spam\n>>> import grok.blah\nI'm grok.__init__\nI'm grok.blah\n>>> grok.blah.__file__\n'http://localhost:15000/grok/blah.py'\n>>>\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n417",
      "content_length": 1562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": 9,
      "content": "This particular solution involves installing an instance of a special finder object UrlMe\ntaFinder as the last entry in sys.meta_path. Whenever modules are imported, the\nfinders in sys.meta_path are consulted in order to locate the module. In this example,\nthe UrlMetaFinder instance becomes a finder of last resort that’s triggered when a\nmodule can’t be found in any of the normal locations.\nAs for the general implementation approach, the UrlMetaFinder class wraps around a\nuser-specified URL. Internally, the finder builds sets of valid links by scraping them\nfrom the given URL. When imports are made, the module name is compared against\nthis set of known links. If a match can be found, a separate UrlModuleLoader class is\nused to load source code from the remote machine and create the resulting module\nobject. One reason for caching the links is to avoid unnecessary HTTP requests on\nrepeated imports.\nThe second approach to customizing import is to write a hook that plugs directly into\nthe sys.path variable, recognizing certain directory naming patterns. Add the following\nclass and support functions to urlimport.py:\n# urlimport.py\n# ... include previous code above ...\n# Path finder class for a URL\nclass UrlPathFinder(importlib.abc.PathEntryFinder):\n    def __init__(self, baseurl):\n        self._links = None\n        self._loader = UrlModuleLoader(baseurl)\n        self._baseurl = baseurl\n    def find_loader(self, fullname):\n        log.debug('find_loader: %r', fullname)\n        parts = fullname.split('.')\n        basename = parts[-1]\n        # Check link cache\n        if self._links is None:\n            self._links = []     # See discussion\n            self._links = _get_links(self._baseurl)\n        # Check if it's a package\n        if basename in self._links:\n            log.debug('find_loader: trying package %r', fullname)\n            fullurl = self._baseurl + '/' + basename\n            # Attempt to load the package (which accesses __init__.py)\n            loader = UrlPackageLoader(fullurl)\n            try:\n                loader.load_module(fullname)\n                log.debug('find_loader: package %r loaded', fullname)\n            except ImportError as e:\n                log.debug('find_loader: %r is a namespace package', fullname)\n418 \n| \nChapter 10: Modules and Packages",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": 9,
      "content": "loader = None\n            return (loader, [fullurl])\n        # A normal module\n        filename = basename + '.py'\n        if filename in self._links:\n            log.debug('find_loader: module %r found', fullname)\n            return (self._loader, [])\n        else:\n            log.debug('find_loader: module %r not found', fullname)\n            return (None, [])\n    def invalidate_caches(self):\n        log.debug('invalidating link cache')\n        self._links = None\n# Check path to see if it looks like a URL\n_url_path_cache = {}\ndef handle_url(path):\n    if path.startswith(('http://', 'https://')):\n        log.debug('Handle path? %s. [Yes]', path)\n        if path in _url_path_cache:\n            finder = _url_path_cache[path]\n        else:\n            finder = UrlPathFinder(path)\n            _url_path_cache[path] = finder\n        return finder\n    else:\n        log.debug('Handle path? %s. [No]', path)\ndef install_path_hook():\n    sys.path_hooks.append(handle_url)\n    sys.path_importer_cache.clear()\n    log.debug('Installing handle_url')\ndef remove_path_hook():\n    sys.path_hooks.remove(handle_url)\n    sys.path_importer_cache.clear()\n    log.debug('Removing handle_url')\nTo use this path-based finder, you simply add URLs to sys.path. For example:\n>>> # Initial import fails\n>>> import fib\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'fib'\n>>> # Install the path hook\n>>> import urlimport\n>>> urlimport.install_path_hook()\n>>> # Imports still fail (not on path)\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n419",
      "content_length": 1606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": 9,
      "content": ">>> import fib\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'fib'\n>>> # Add an entry to sys.path and watch it work\n>>> import sys\n>>> sys.path.append('http://localhost:15000')\n>>> import fib\nI'm fib\n>>> import grok.blah\nI'm grok.__init__\nI'm grok.blah\n>>> grok.blah.__file__\n'http://localhost:15000/grok/blah.py'\n>>>\nThe key to this last example is the handle_url() function, which is added to the \nsys.path_hooks variable. When the entries on sys.path are being processed, the func‐\ntions in sys.path_hooks are invoked. If any of those functions return a finder object,\nthat finder is used to try to load modules for that entry on sys.path.\nIt should be noted that the remotely imported modules work exactly like any other\nmodule. For instance:\n>>> fib\n<urlmodule 'fib' from 'http://localhost:15000/fib.py'>\n>>> fib.__name__\n'fib'\n>>> fib.__file__\n'http://localhost:15000/fib.py'\n>>> import inspect\n>>> print(inspect.getsource(fib))\n# fib.py\nprint(\"I'm fib\")\ndef fib(n):\n    if n < 2:\n        return 1\n    else:\n        return fib(n-1) + fib(n-2)\n>>>\nDiscussion\nBefore discussing this recipe in further detail, it should be emphasized that Python’s\nmodule, package, and import mechanism is one of the most complicated parts of the\nentire language—often poorly understood by even the most seasoned Python pro‐\ngrammers unless they’ve devoted effort to peeling back the covers. There are several\n420 \n| \nChapter 10: Modules and Packages",
      "content_length": 1492,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": 9,
      "content": "critical documents that are worth reading, including the documentation for the \nimportlib module and PEP 302. That documentation won’t be repeated here, but some\nessential highlights will be discussed.\nFirst, if you want to create a new module object, you use the imp.new_module() function.\nFor example:\n>>> import imp\n>>> m = imp.new_module('spam')\n>>> m\n<module 'spam'>\n>>> m.__name__\n'spam'\n>>>\nModule objects usually have a few expected attributes, including __file__ (the name\nof the file that the module was loaded from) and __package__ (the name of the enclosing\npackage, if any).\nSecond, modules are cached by the interpreter. The module cache can be found in the\ndictionary sys.modules. Because of this caching, it’s common to combine caching and\nmodule creation together into a single step. For example:\n>>> import sys\n>>> import imp\n>>> m = sys.modules.setdefault('spam', imp.new_module('spam'))\n>>> m\n<module 'spam'>\n>>>\nThe main reason for doing this is that if a module with the given name already exists,\nyou’ll get the already created module instead. For example:\n>>> import math\n>>> m = sys.modules.setdefault('math', imp.new_module('math'))\n>>> m\n<module 'math' from '/usr/local/lib/python3.3/lib-dynload/math.so'>\n>>> m.sin(2)\n0.9092974268256817\n>>> m.cos(2)\n-0.4161468365471424\n>>>\nSince creating modules is easy, it is straightforward to write simple functions, such as\nthe load_module() function in the first part of this recipe. A downside of this approach\nis that it is actually rather tricky to handle more complicated cases, such as package\nimports. In order to handle a package, you would have to reimplement much of the\nunderlying logic that’s already part of the normal import statement (e.g., checking for\ndirectories, looking for __init__.py files, executing those files, setting up paths, etc.).\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n421",
      "content_length": 1899,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": 9,
      "content": "This complexity is one of the reasons why it’s often better to extend the import statement\ndirectly rather than defining a custom function.\nExtending the import statement is straightforward, but involves a number of moving\nparts. At the highest level, import operations are processed by a list of “meta-path”\nfinders that you can find in the list sys.meta_path. If you output its value, you’ll see\nthe following:\n>>> from pprint import pprint\n>>> pprint(sys.meta_path)\n[<class '_frozen_importlib.BuiltinImporter'>,\n <class '_frozen_importlib.FrozenImporter'>,\n <class '_frozen_importlib.PathFinder'>]\n>>>\nWhen executing a statement such as import fib, the interpreter walks through the\nfinder objects on sys.meta_path and invokes their find_module() method in order to\nlocate an appropriate module loader. It helps to see this by experimentation, so define\nthe following class and try the following:\n>>> class Finder:\n...     def find_module(self, fullname, path):\n...             print('Looking for', fullname, path)\n...             return None\n...\n>>> import sys\n>>> sys.meta_path.insert(0, Finder())   # Insert as first entry\n>>> import math\nLooking for math None\n>>> import types\nLooking for types None\n>>> import threading\nLooking for threading None\nLooking for time None\nLooking for traceback None\nLooking for linecache None\nLooking for tokenize None\nLooking for token None\n>>>\nNotice how the find_module() method is being triggered on every import. The role of\nthe path argument in this method is to handle packages. When packages are imported,\nit is a list of the directories that are found in the package’s __path__ attribute. These are\nthe paths that need to be checked to find package subcomponents. For example, notice\nthe path setting for xml.etree and xml.etree.ElementTree:\n>>> import xml.etree.ElementTree\nLooking for xml None\nLooking for xml.etree ['/usr/local/lib/python3.3/xml']\nLooking for xml.etree.ElementTree ['/usr/local/lib/python3.3/xml/etree']\nLooking for warnings None\n422 \n| \nChapter 10: Modules and Packages",
      "content_length": 2037,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": 10,
      "content": "Looking for contextlib None\nLooking for xml.etree.ElementPath ['/usr/local/lib/python3.3/xml/etree']\nLooking for _elementtree None\nLooking for copy None\nLooking for org None\nLooking for pyexpat None\nLooking for ElementC14N None\n>>>\nThe placement of the finder on sys.meta_path is critical. Remove it from the front of\nthe list to the end of the list and try more imports:\n>>> del sys.meta_path[0]\n>>> sys.meta_path.append(Finder())\n>>> import urllib.request\n>>> import datetime\nNow you don’t see any output because the imports are being handled by other entries\nin sys.meta_path. In this case, you would only see it trigger when nonexistent modules\nare imported:\n>>> import fib\nLooking for fib None\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'fib'\n>>> import xml.superfast\nLooking for xml.superfast ['/usr/local/lib/python3.3/xml']\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'xml.superfast'\n>>>\nThe fact that you can install a finder to catch unknown modules is the key to the\nUrlMetaFinder class in this recipe. An instance of UrlMetaFinder is added to the end\nof sys.meta_path, where it serves as a kind of importer of last resort. If the requested\nmodule name can’t be located by any of the other import mechanisms, it gets handled\nby this finder. Some care needs to be taken when handling packages. Specifically, the\nvalue presented in the path argument needs to be checked to see if it starts with the URL\nregistered in the finder. If not, the submodule must belong to some other finder and\nshould be ignored.\nAdditional handling of packages is found in the UrlPackageLoader class. This class,\nrather than importing the package name, tries to load the underlying __init__.py file.\nIt also sets the module __path__ attribute. This last part is critical, as the value set will\nbe passed to subsequent find_module() calls when loading package submodules.\nThe path-based import hook is an extension of these ideas, but based on a somewhat\ndifferent mechanism. As you know, sys.path is a list of directories where Python looks\nfor modules. For example:\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n423",
      "content_length": 2242,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": 10,
      "content": ">>> from pprint import pprint\n>>> import sys\n>>> pprint(sys.path)\n['',\n '/usr/local/lib/python33.zip',\n '/usr/local/lib/python3.3',\n '/usr/local/lib/python3.3/plat-darwin',\n '/usr/local/lib/python3.3/lib-dynload',\n '/usr/local/lib/...3.3/site-packages']\n>>>\nEach entry in sys.path is additionally attached to a finder object. You can view these\nfinders by looking at sys.path_importer_cache:\n>>> pprint(sys.path_importer_cache)\n{'.': FileFinder('.'),\n '/usr/local/lib/python3.3': FileFinder('/usr/local/lib/python3.3'),\n '/usr/local/lib/python3.3/': FileFinder('/usr/local/lib/python3.3/'),\n '/usr/local/lib/python3.3/collections': FileFinder('...python3.3/collections'),\n '/usr/local/lib/python3.3/encodings': FileFinder('...python3.3/encodings'),\n '/usr/local/lib/python3.3/lib-dynload': FileFinder('...python3.3/lib-dynload'),\n '/usr/local/lib/python3.3/plat-darwin': FileFinder('...python3.3/plat-darwin'),\n '/usr/local/lib/python3.3/site-packages': FileFinder('...python3.3/site-packages'),\n '/usr/local/lib/python33.zip': None}\n>>>\nsys.path_importer_cache tends to be much larger than sys.path because it records\nfinders for all known directories where code is being loaded. This includes subdirecto‐\nries of packages which usually aren’t included on sys.path.\nTo execute import fib, the directories on sys.path are checked in order. For each\ndirectory, the name fib is presented to the associated finder found in sys.path_im\nporter_cache. This is also something that you can investigate by making your own\nfinder and putting an entry in the cache. Try this experiment:\n>>> class Finder:\n...     def find_loader(self, name):\n...             print('Looking for', name)\n...             return (None, [])\n...\n>>> import sys\n>>> # Add a \"debug\" entry to the importer cache\n>>> sys.path_importer_cache['debug'] = Finder()\n>>> # Add a \"debug\" directory to sys.path\n>>> sys.path.insert(0, 'debug')\n>>> import threading\nLooking for threading\nLooking for time\nLooking for traceback\nLooking for linecache\nLooking for tokenize\n424 \n| \nChapter 10: Modules and Packages",
      "content_length": 2062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": 10,
      "content": "Looking for token\n>>>\nHere, you’ve installed a new cache entry for the name debug and installed the name\ndebug as the first entry on sys.path. On all subsequent imports, you see your finder\nbeing triggered. However, since it returns (None, []), processing simply continues to the\nnext entry.\nThe population of sys.path_importer_cache is controlled by a list of functions stored\nin sys.path_hooks. Try this experiment, which clears the cache and adds a new path\nchecking function to sys.path_hooks:\n>>> sys.path_importer_cache.clear()\n>>> def check_path(path):\n...     print('Checking', path)\n...     raise ImportError()\n...\n>>> sys.path_hooks.insert(0, check_path)\n>>> import fib\nChecked debug\nChecking .\nChecking /usr/local/lib/python33.zip\nChecking /usr/local/lib/python3.3\nChecking /usr/local/lib/python3.3/plat-darwin\nChecking /usr/local/lib/python3.3/lib-dynload\nChecking /Users/beazley/.local/lib/python3.3/site-packages\nChecking /usr/local/lib/python3.3/site-packages\nLooking for fib\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'fib'\n>>>\nAs you can see, the check_path() function is being invoked for every entry on\nsys.path. However, since an ImportError exception is raised, nothing else happens\n(checking just moves to the next function on sys.path_hooks).\nUsing this knowledge of how sys.path is processed, you can install a custom path\nchecking function that looks for filename patterns, such as URLs. For instance:\n>>> def check_url(path):\n...     if path.startswith('http://'):\n...             return Finder()\n...     else:\n...             raise ImportError()\n...\n>>> sys.path.append('http://localhost:15000')\n>>> sys.path_hooks[0] = check_url\n>>> import fib\nLooking for fib             # Finder output!\nTraceback (most recent call last):\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n425",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": 10,
      "content": "File \"<stdin>\", line 1, in <module>\nImportError: No module named 'fib'\n>>> # Notice installation of Finder in sys.path_importer_cache\n>>> sys.path_importer_cache['http://localhost:15000']\n<__main__.Finder object at 0x10064c850>\n>>>\nThis is the key mechanism at work in the last part of this recipe. Essentially, a custom\npath checking function has been installed that looks for URLs in sys.path. When they\nare encountered, a new UrlPathFinder instance is created and installed into\nsys.path_importer_cache. From that point forward, all import statements that pass\nthrough that part of sys.path will try to use your custom finder.\nPackage handling with a path-based importer is somewhat tricky, and relates to the\nreturn value of the find_loader() method. For simple modules, find_loader() re‐\nturns a tuple (loader, None) where loader is an instance of a loader that will import\nthe module.\nFor a normal package, find_loader() returns a tuple (loader, path) where loader\nis the loader instance that will import the package (and execute __init__.py) and path\nis a list of the directories that will make up the initial setting of the package’s __path__\nattribute. For example, if the base URL was http://localhost:15000 and a user exe‐\ncuted import grok, the path returned by find_loader() would be [ 'http://local\nhost:15000/grok' ].\nThe find_loader() must additionally account for the possibility of a namespace pack‐\nage. A namespace package is a package where a valid package directory name exists,\nbut no __init__.py file can be found. For this case, find_loader() must return a tuple\n(None, path) where path is a list of directories that would have made up the package’s\n__path__ attribute had it defined an __init__.py file. For this case, the import mecha‐\nnism moves on to check further directories on sys.path. If more namespace packages\nare found, all of the resulting paths are joined together to make a final namespace pack‐\nage. See Recipe 10.5 for more information on namespace packages.\nThere is a recursive element to package handling that is not immediately obvious in the\nsolution, but also at work. All packages contain an internal path setting, which can be\nfound in __path__ attribute. For example:\n>>> import xml.etree.ElementTree\n>>> xml.__path__\n['/usr/local/lib/python3.3/xml']\n>>> xml.etree.__path__\n['/usr/local/lib/python3.3/xml/etree']\n>>>\n426 \n| \nChapter 10: Modules and Packages",
      "content_length": 2408,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": 10,
      "content": "As mentioned, the setting of __path__ is controlled by the return value of the find_load\ner() method. However, the subsequent processing of __path__ is also handled by the\nfunctions in sys.path_hooks. Thus, when package subcomponents are loaded, the en‐\ntries in __path__ are checked by the handle_url() function. This causes new instances\nof UrlPathFinder to be created and added to sys.path_importer_cache.\nOne remaining tricky part of the implementation concerns the behavior of the han\ndle_url() function and its interaction with the _get_links() function used internally.\nIf your implementation of a finder involves the use of other modules (e.g., urllib.re\nquest), there is a possibility that those modules will attempt to make further imports\nin the middle of the finder’s operation. This can actually cause handle_url() and other\nparts of the finder to get executed in a kind of recursive loop. To account for this pos‐\nsibility, the implementation maintains a cache of created finders (one per URL). This\navoids the problem of creating duplicate finders. In addition, the following fragment of\ncode ensures that the finder doesn’t respond to any import requests while it’s in the\nprocesss of getting the initial set of links:\n        # Check link cache\n        if self._links is None:\n            self._links = []     # See discussion\n            self._links = _get_links(self._baseurl)\nYou may not need this checking in other implementations, but for this example involv‐\ning URLs, it was required.\nFinally, the invalidate_caches() method of both finders is a utility method that is\nsupposed to clear internal caches should the source code change. This method is trig‐\ngered when a user invokes importlib.invalidate_caches(). You might use it if you\nwant the URL importers to reread the list of links, possibly for the purpose of being able\nto access newly added files.\nIn comparing the two approaches (modifying sys.meta_path or using a path hook), it\nhelps to take a high-level view. Importers installed using sys.meta_path are free to\nhandle modules in any manner that they wish. For instance, they could load modules\nout of a database or import them in a manner that is radically different than normal\nmodule/package handling. This freedom also means that such importers need to do\nmore bookkeeping and internal management. This explains, for instance, why the im‐\nplementation of UrlMetaFinder needs to do its own caching of links, loaders, and other\ndetails. On the other hand, path-based hooks are more narrowly tied to the processing\nof sys.path. Because of the connection to sys.path, modules loaded with such exten‐\nsions will tend to have the same features as normal modules and packages that pro‐\ngrammers are used to.\n10.11. Loading Modules from a Remote Machine Using Import Hooks \n| \n427",
      "content_length": 2812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": 10,
      "content": "Assuming that your head hasn’t completely exploded at this point, a key to understand‐\ning and experimenting with this recipe may be the added logging calls. You can enable\nlogging and try experiments such as this:\n>>> import logging\n>>> logging.basicConfig(level=logging.DEBUG)\n>>> import urlimport\n>>> urlimport.install_path_hook()\nDEBUG:urlimport:Installing handle_url\n>>> import fib\nDEBUG:urlimport:Handle path? /usr/local/lib/python33.zip. [No]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named 'fib'\n>>> import sys\n>>> sys.path.append('http://localhost:15000')\n>>> import fib\nDEBUG:urlimport:Handle path? http://localhost:15000. [Yes]\nDEBUG:urlimport:Getting links from http://localhost:15000\nDEBUG:urlimport:links: {'spam.py', 'fib.py', 'grok'}\nDEBUG:urlimport:find_loader: 'fib'\nDEBUG:urlimport:find_loader: module 'fib' found\nDEBUG:urlimport:loader: reading 'http://localhost:15000/fib.py'\nDEBUG:urlimport:loader: 'http://localhost:15000/fib.py' loaded\nI'm fib\n>>>\nLast, but not least, spending some time sleeping with PEP 302 and the documentation\nfor importlib under your pillow may be advisable.\n10.12. Patching Modules on Import\nProblem\nYou want to patch or apply decorators to functions in an existing module. However, you\nonly want to do it if the module actually gets imported and used elsewhere.\nSolution\nThe essential problem here is that you would like to carry out actions in response to a\nmodule being loaded. Perhaps you want to trigger some kind of callback function that\nwould notify you when a module was loaded.\nThis problem can be solved using the same import hook machinery discussed in\nRecipe 10.11. Here is a possible solution:\n428 \n| \nChapter 10: Modules and Packages",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": 10,
      "content": "# postimport.py\nimport importlib\nimport sys\nfrom collections import defaultdict\n_post_import_hooks = defaultdict(list)\nclass PostImportFinder:\n    def __init__(self):\n        self._skip = set()\n    def find_module(self, fullname, path=None):\n        if fullname in self._skip:\n            return None\n        self._skip.add(fullname)\n        return PostImportLoader(self)\nclass PostImportLoader:\n    def __init__(self, finder):\n        self._finder = finder\n    def load_module(self, fullname):\n        importlib.import_module(fullname)\n        module = sys.modules[fullname]\n        for func in _post_import_hooks[fullname]:\n            func(module)\n        self._finder._skip.remove(fullname)\n        return module\ndef when_imported(fullname):\n    def decorate(func):\n        if fullname in sys.modules:\n            func(sys.modules[fullname])\n        else:\n            _post_import_hooks[fullname].append(func)\n        return func\n    return decorate\nsys.meta_path.insert(0, PostImportFinder())\nTo use this code, you use the when_imported() decorator. For example:\n>>> from postimport import when_imported\n>>> @when_imported('threading')\n... def warn_threads(mod):\n...     print('Threads?  Are you crazy?')\n...\n>>>\n>>> import threading\nThreads?  Are you crazy?\n>>>\n10.12. Patching Modules on Import \n| \n429",
      "content_length": 1309,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": 10,
      "content": "As a more practical example, maybe you want to apply decorators to existing definitions,\nsuch as shown here:\nfrom functools import wraps\nfrom postimport import when_imported\ndef logged(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print('Calling', func.__name__, args, kwargs)\n        return func(*args, **kwargs)\n    return wrapper\n# Example\n@when_imported('math')\ndef add_logging(mod):\n    mod.cos = logged(mod.cos)\n    mod.sin = logged(mod.sin)\nDiscussion\nThis recipe relies on the import hooks that were discussed in Recipe 10.11, with a slight\ntwist.\nFirst, the role of the @when_imported decorator is to register handler functions that get\ntriggered on import. The decorator checks sys.modules to see if a module was already\nloaded. If so, the handler is invoked immediately. Otherwise, the handler is added to a\nlist in the _post_import_hooks dictionary. The purpose of _post_import_hooks is\nsimply to collect all handler objects that have been registered for each module. In prin‐\nciple, more than one handler could be registered for a given module.\nTo trigger the pending actions in _post_import_hooks after module import, the Post\nImportFinder class is installed as the first item in sys.meta_path. If you recall from\nRecipe 10.11, sys.meta_path contains a list of finder objects that are consulted in order\nto locate modules. By installing PostImportFinder as the first item, it captures all mod‐\nule imports.\nIn this recipe, however, the role of PostImportFinder is not to load modules, but to\ntrigger actions upon the completion of an import. To do this, the actual import is dele‐\ngated to the other finders on sys.meta_path. Rather than trying to do this directly, the\nfunction imp.import_module() is called recursively in the PostImportLoader class. To\navoid getting stuck in an infinite loop, PostImportFinder keeps a set of all the modules\nthat are currently in the process of being loaded. If a module name is part of this set, it\nis simply ignored by PostImportFinder. This is what causes the import request to pass\nto the other finders on sys.meta_path.\n430 \n| \nChapter 10: Modules and Packages",
      "content_length": 2135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": 10,
      "content": "After a module has been loaded with imp.import_module(), all handlers currently reg‐\nistered in _post_import_hooks are called with the newly loaded module as an argument.\nFrom this point forward, the handlers are free to do what they want with the module.\nA major feature of the approach shown in this recipe is that the patching of a module\noccurs in a seamless fashion, regardless of where or how a module of interest is actually\nloaded. You simply write a handler function that’s decorated with @when_imported()\nand it all just magically works from that point forward.\nOne caution about this recipe is that it does not work for modules that have been ex‐\nplicitly reloaded using imp.reload(). That is, if you reload a previously loaded module,\nthe post import handler function doesn’t get triggered again (all the more reason to not\nuse reload() in production code). On the other hand, if you delete the module from\nsys.modules and redo the import, you’ll see the handler trigger again.\nMore information about post-import hooks can be found in PEP 369 . As of this writing,\nthe PEP has been withdrawn by the author due to it being out of date with the current\nimplementation of the importlib module. However, it is easy enough to implement\nyour own solution using this recipe.\n10.13. Installing Packages Just for Yourself\nProblem\nYou want to install a third-party package, but you don’t have permission to install pack‐\nages into the system Python. Alternatively, perhaps you just want to install a package\nfor your own use, not all users on the system.\nSolution\nPython has a per-user installation directory that’s typically located in a directory such\nas ~/.local/lib/python3.3/site-packages. To force packages to install in this directory, give\nthe --user option to the installation command. For example:\n     python3 setup.py install --user\nor\n     pip install --user packagename\nThe user site-packages directory normally appears before the system site-packages di‐\nrectory on sys.path. Thus, packages you install using this technique take priority over\nthe packages already installed on the system (although this is not always the case de‐\npending on the behavior of third-party package managers, such as distribute or pip).\n10.13. Installing Packages Just for Yourself \n| \n431",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": 10,
      "content": "Discussion\nNormally, packages get installed into the system-wide site-packages directory, which is\nfound in a location such as /usr/local/lib/python3.3/site-packages. However, doing so\ntypically requires administrator permissions and use of the sudo command. Even if you\nhave permission to execute such a command, using sudo to install a new, possibly un‐\nproven, package might give you some pause.\nInstalling packages into the per-user directory is often an effective workaround that\nallows you to create a custom installation.\nAs an alternative, you can also create a virtual environment, which is discussed in the\nnext recipe.\n10.14. Creating a New Python Environment\nProblem\nYou want to create a new Python environment in which you can install modules and\npackages. However, you want to do this without installing a new copy of Python or\nmaking changes that might affect the system Python installation.\nSolution\nYou can make a new “virtual” environment using the pyvenv command. This command\nis installed in the same directory as the Python interpreter or possibly in the Scripts\ndirectory on Windows. Here is an example:\n    bash % pyvenv Spam\n    bash %\nThe name supplied to pyvenv is the name of a directory that will be created. Upon\ncreation, the Spam directory will look something like this:\n    bash % cd Spam\n    bash % ls\n    bin               include             lib           pyvenv.cfg\n    bash %\nIn the bin directory, you’ll find a Python interpreter that you can use. For example:\n    bash % Spam/bin/python3\n    Python 3.3.0 (default, Oct  6 2012, 15:45:22)\n    [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> from pprint import pprint\n    >>> import sys\n    >>> pprint(sys.path)\n    ['',\n432 \n| \nChapter 10: Modules and Packages",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": 10,
      "content": "'/usr/local/lib/python33.zip',\n     '/usr/local/lib/python3.3',\n     '/usr/local/lib/python3.3/plat-darwin',\n     '/usr/local/lib/python3.3/lib-dynload',\n     '/Users/beazley/Spam/lib/python3.3/site-packages']\n    >>>\nA key feature of this interpreter is that its site-packages directory has been set to the\nnewly created environment. Should you decide to install third-party packages, they will\nbe installed here, not in the normal system site-packages directory.\nDiscussion\nThe creation of a virtual environment mostly pertains to the installation and manage‐\nment of third-party packages. As you can see in the example, the sys.path variable\ncontains directories from the normal system Python, but the site-packages directory has\nbeen relocated to a new directory.\nWith a new virtual environment, the next step is often to install a package manager,\nsuch as distribute or pip. When installing such tools and subsequent packages, you\njust need to make sure you use the interpreter that’s part of the virtual environment.\nThis should install the packages into the newly created site-packages directory.\nAlthough a virtual environment might look like a copy of the Python installation, it\nreally only consists of a few files and symbolic links. All of the standard library files and\ninterpreter executables come from the original Python installation. Thus, creating such\nenvironments is easy, and takes almost no machine resources.\nBy default, virtual environments are completely clean and contain no third-party add-\nons. If you would like to include already installed packages as part of a virtual environ‐\nment, create the environment using the --system-site-packages option. For example:\n    bash % pyvenv --system-site-packages Spam\n    bash %\nMore information about pyvenv and virtual environments can be found in PEP 405. \n10.15. Distributing Packages\nProblem\nYou’ve written a useful library, and you want to be able to give it away to others.\n10.15. Distributing Packages \n| \n433",
      "content_length": 1987,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": 10,
      "content": "Solution\nIf you’re going to start giving code away, the first thing to do is to give it a unique name\nand clean up its directory structure. For example, a typical library package might look\nsomething like this:\n    projectname/\n         README.txt\n         Doc/\n             documentation.txt\n         projectname/\n            __init__.py\n            foo.py\n            bar.py\n            utils/\n                 __init__.py\n                 spam.py\n                 grok.py\n         examples/\n            helloworld.py\n            ...\nTo make the package something that you can distribute, first write a setup.py file that\nlooks like this:\n# setup.py\nfrom distutils.core import setup\nsetup(name='projectname',\n      version='1.0',\n      author='Your Name',\n      author_email='you@youraddress.com',\n      url='http://www.you.com/projectname',\n      packages=['projectname', 'projectname.utils'],\n)\nNext, make a file MANIFEST.in that lists various nonsource files that you want to in‐\nclude in your package:\n    # MANIFEST.in\n    include *.txt\n    recursive-include examples *\n    recursive-include Doc *\nMake sure the setup.py and MANIFEST.in files appear in the top-level directory of your\npackage. Once you have done this, you should be able to make a source distribution by\ntyping a command such as this:\n% bash python3 setup.py sdist\nThis will create a file such as projectname-1.0.zip or projectname-1.0.tar.gz, depending\non the platform. If it all works, this file is suitable for giving to others or uploading to\nthe Python Package Index.\n434 \n| \nChapter 10: Modules and Packages",
      "content_length": 1587,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": 10,
      "content": "Discussion\nFor pure Python code, writing a plain setup.py file is usually straightforward. One po‐\ntential gotcha is that you have to manually list every subdirectory that makes up the\npackages source code. A common mistake is to only list the top-level directory of a\npackage and to forget to include package subcomponents. This is why the specification\nfor packages in setup.py includes the list packages=['projectname', 'project\nname.utils'].\nAs most Python programmers know, there are many third-party packaging options,\nincluding setuptools, distribute, and so forth. Some of these are replacements for the\ndistutils library found in the standard library. Be aware that if you rely on these\npackages, users may not be able to install your software unless they also install the\nrequired package manager first. Because of this, you can almost never go wrong by\nkeeping things as simple as possible. At a bare minimum, make sure your code can be\ninstalled using a standard Python 3 installation. Additional features can be supported\nas an option if additional packages are available.\nPackaging and distribution of code involving C extensions can get considerably more\ncomplicated. Chapter 15 on C extensions has a few details on this. In particular, see\nRecipe 15.2.\n10.15. Distributing Packages \n| \n435",
      "content_length": 1305,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": 10,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 455,
      "chapter": 10,
      "content": "CHAPTER 11\nNetwork and Web Programming\nThis chapter is about various topics related to using Python in networked and dis‐\ntributed applications. Topics are split between using Python as a client to access existing\nservices and using Python to implement networked services as a server. Common tech‐\nniques for writing code involving cooperating or communicating with interpreters are\nalso given.\n11.1. Interacting with HTTP Services As a Client\nProblem\nYou need to access various services via HTTP as a client. For example, downloading\ndata or interacting with a REST-based API.\nSolution\nFor simple things, it’s usually easy enough to use the urllib.request module. For\nexample, to send a simple HTTP GET request to a remote service, do something like this:\nfrom urllib import request, parse\n# Base URL being accessed\nurl = 'http://httpbin.org/get'\n# Dictionary of query parameters (if any)\nparms = {\n   'name1' : 'value1',\n   'name2' : 'value2'\n}\n# Encode the query string\nquerystring = parse.urlencode(parms)\n437",
      "content_length": 1013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": 10,
      "content": "# Make a GET request and read the response\nu = request.urlopen(url+'?' + querystring)\nresp = u.read()\nIf you need to send the query parameters in the request body using a POST method,\nencode them and supply them as an optional argument to urlopen() like this:\nfrom urllib import request, parse\n# Base URL being accessed\nurl = 'http://httpbin.org/post'\n# Dictionary of query parameters (if any)\nparms = {\n   'name1' : 'value1',\n   'name2' : 'value2'\n}\n# Encode the query string\nquerystring = parse.urlencode(parms)\n# Make a POST request and read the response\nu = request.urlopen(url, querystring.encode('ascii'))\nresp = u.read()\nIf you need to supply some custom HTTP headers in the outgoing request such as a\nchange to the user-agent field, make a dictionary containing their value and create a\nRequest instance and pass it to urlopen() like this:\nfrom urllib import request, parse\n...\n# Extra headers\nheaders = {\n    'User-agent' : 'none/ofyourbusiness',\n    'Spam' : 'Eggs'\n}\nreq = request.Request(url, querystring.encode('ascii'), headers=headers)\n# Make a request and read the response\nu = request.urlopen(req)\nresp = u.read()\nIf your interaction with a service is more complicated than this, you should probably\nlook at the requests library. For example, here is equivalent requests code for the\npreceding operations:\nimport requests\n# Base URL being accessed\nurl = 'http://httpbin.org/post'\n438 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": 10,
      "content": "# Dictionary of query parameters (if any)\nparms = {\n   'name1' : 'value1',\n   'name2' : 'value2'\n}\n# Extra headers\nheaders = {\n    'User-agent' : 'none/ofyourbusiness',\n    'Spam' : 'Eggs'\n}\nresp = requests.post(url, data=parms, headers=headers)\n# Decoded text returned by the request\ntext = resp.text\nA notable feature of requests is how it returns the resulting response content from a\nrequest. As shown, the resp.text attribute gives you the Unicode decoded text of a\nrequest. However, if you access resp.content, you get the raw binary content instead.\nOn the other hand, if you access resp.json, then you get the response content inter‐\npreted as JSON.\nHere is an example of using requests to make a HEAD request and extract a few fields\nof header data from the response:\nimport requests\nresp = requests.head('http://www.python.org/index.html')\nstatus = resp.status_code\nlast_modified = resp.headers['last-modified']\ncontent_type = resp.headers['content-type']\ncontent_length = resp.headers['content-length']\nHere is a requests example that executes a login into the Python Package index using\nbasic authentication:\nimport requests\nresp = requests.get('http://pypi.python.org/pypi?:action=login',\n                    auth=('user','password'))\nHere is an example of using requests to pass HTTP cookies from one request to the\nnext:\nimport requests\n# First request\nresp1 = requests.get(url)\n...\n11.1. Interacting with HTTP Services As a Client \n| \n439",
      "content_length": 1454,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": 10,
      "content": "# Second requests with cookies received on first requests\nresp2 = requests.get(url, cookies=resp1.cookies)\nLast, but not least, here is an example of using requests to upload content:\nimport requests\nurl = 'http://httpbin.org/post'\nfiles = { 'file': ('data.csv', open('data.csv', 'rb')) }\nr = requests.post(url, files=files)\nDiscussion\nFor really simple HTTP client code, using the built-in urllib module is usually fine.\nHowever, if you have to do anything other than simple GET or POST requests, you really\ncan’t rely on its functionality. This is where a third-party module, such as requests,\ncomes in handy.\nFor example, if you decided to stick entirely with the standard library instead of a library\nlike requests, you might have to implement your code using the low-level http.cli\nent module instead. For example, this code shows how to execute a HEAD request:\nfrom http.client import HTTPConnection\nfrom urllib import parse\nc = HTTPConnection('www.python.org', 80)\nc.request('HEAD', '/index.html')\nresp = c.getresponse()\nprint('Status', resp.status)\nfor name, value in resp.getheaders():\n    print(name, value)\nSimilarly, if you have to write code involving proxies, authentication, cookies, and other\ndetails, using urllib is awkward and verbose. For example, here is a sample of code that\nauthenticates to the Python package index:\nimport urllib.request\nauth = urllib.request.HTTPBasicAuthHandler()\nauth.add_password('pypi','http://pypi.python.org','username','password')\nopener = urllib.request.build_opener(auth)\nr = urllib.request.Request('http://pypi.python.org/pypi?:action=login')\nu = opener.open(r)\nresp = u.read()\n# From here. You can access more pages using opener\n...\n440 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": 10,
      "content": "Frankly, all of this is much easier in requests.\nTesting HTTP client code during development can often be frustrating because of all\nthe tricky details you need to worry about (e.g., cookies, authentication, headers, en‐\ncodings, etc.). To do this, consider using the httpbin service. This site receives requests\nand then echoes information back to you in the form a JSON response. Here is an\ninteractive example:\n>>> import requests\n>>> r = requests.get('http://httpbin.org/get?name=Dave&n=37',\n...     headers = { 'User-agent': 'goaway/1.0' })\n>>> resp = r.json\n>>> resp['headers']\n{'User-Agent': 'goaway/1.0', 'Content-Length': '', 'Content-Type': '',\n'Accept-Encoding': 'gzip, deflate, compress', 'Connection':\n'keep-alive', 'Host': 'httpbin.org', 'Accept': '*/*'}\n>>> resp['args']\n{'name': 'Dave', 'n': '37'}\n>>>\nWorking with a site such as httpbin.org is often preferable to experimenting with a real\nsite—especially if there’s a risk it might shut down your account after three failed login\nattempts (i.e., don’t try to learn how to write an HTTP authentication client by logging\ninto your bank).\nAlthough it’s not discussed here, requests provides support for many more advanced\nHTTP-client protocols, such as OAuth. The requests documentation is excellent (and\nfrankly better than anything that could be provided in this short space). Go there for\nmore information.\n11.2. Creating a TCP Server\nProblem\nYou want to implement a server that communicates with clients using the TCP Internet\nprotocol.\nSolution\nAn easy way to create a TCP server is to use the socketserver library. For example,\nhere is a simple echo server:\nfrom socketserver import BaseRequestHandler, TCPServer\nclass EchoHandler(BaseRequestHandler):\n    def handle(self):\n        print('Got connection from', self.client_address)\n        while True:\n11.2. Creating a TCP Server \n| \n441",
      "content_length": 1858,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": 10,
      "content": "msg = self.request.recv(8192)\n            if not msg:\n                break\n            self.request.send(msg)\nif __name__ == '__main__':\n    serv = TCPServer(('', 20000), EchoHandler)\n    serv.serve_forever()\nIn this code, you define a special handler class that implements a handle() method for\nservicing client connections. The request attribute is the underlying client socket and\nclient_address has client address.\nTo test the server, run it and then open a separate Python process that connects to it:\n>>> from socket import socket, AF_INET, SOCK_STREAM\n>>> s = socket(AF_INET, SOCK_STREAM)\n>>> s.connect(('localhost', 20000))\n>>> s.send(b'Hello')\n5\n>>> s.recv(8192)\nb'Hello'\n>>>\nIn many cases, it may be easier to define a slightly different kind of handler. Here is an\nexample that uses the StreamRequestHandler base class to put a file-like interface on\nthe underlying socket:\nfrom socketserver import StreamRequestHandler, TCPServer\nclass EchoHandler(StreamRequestHandler):\n    def handle(self):\n        print('Got connection from', self.client_address)\n        # self.rfile is a file-like object for reading\n        for line in self.rfile:\n            # self.wfile is a file-like object for writing\n            self.wfile.write(line)\nif __name__ == '__main__':\n    serv = TCPServer(('', 20000), EchoHandler)\n    serv.serve_forever()\nDiscussion\nsocketserver makes it relatively easy to create simple TCP servers. However, you\nshould be aware that, by default, the servers are single threaded and can only serve one\nclient at a time. If you want to handle multiple clients, either instantiate a ForkingTCP\nServer or ThreadingTCPServer object instead. For example:\nfrom socketserver import ThreadingTCPServer\n...\n442 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1768,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": 10,
      "content": "if __name__ == '__main__':\n    serv = ThreadingTCPServer(('', 20000), EchoHandler)\n    serv.serve_forever()\nOne issue with forking and threaded servers is that they spawn a new process or thread\non each client connection. There is no upper bound on the number of allowed clients,\nso a malicious hacker could potentially launch a large number of simultaneous con‐\nnections in an effort to make your server explode.\nIf this is a concern, you can create a pre-allocated pool of worker threads or processes.\nTo do this, you create an instance of a normal nonthreaded server, but then launch the\nserve_forever() method in a pool of multiple threads. For example:\n...\nif __name__ == '__main__':\n    from threading import Thread\n    NWORKERS = 16\n    serv = TCPServer(('', 20000), EchoHandler)\n    for n in range(NWORKERS):\n        t = Thread(target=serv.serve_forever)\n        t.daemon = True\n        t.start()\n    serv.serve_forever()\nNormally, a TCPServer binds and activates the underlying socket upon instantiation.\nHowever, sometimes you might want to adjust the underlying socket by setting options.\nTo do this, supply the bind_and_activate=False argument, like this:\nif __name__ == '__main__':\n    serv = TCPServer(('', 20000), EchoHandler, bind_and_activate=False)\n    # Set up various socket options\n    serv.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n    # Bind and activate\n    serv.server_bind()\n    serv.server_activate()\n    serv.serve_forever()\nThe socket option shown is actually a very common setting that allows the server to\nrebind to a previously used port number. It’s actually so common that it’s a class variable\nthat can be set on TCPServer. Set it before instantiating the server, as shown in this\nexample:\n...\nif __name__ == '__main__':\n    TCPServer.allow_reuse_address = True\n    serv = TCPServer(('', 20000), EchoHandler)\n    serv.serve_forever()\nIn the solution, two different handler base classes were shown (BaseRequestHandler\nand StreamRequestHandler). The StreamRequestHandler class is actually a bit more\n11.2. Creating a TCP Server \n| \n443",
      "content_length": 2090,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": 10,
      "content": "flexible, and supports some features that can be enabled through the specification of\nadditional class variables. For example:\nimport socket\nclass EchoHandler(StreamRequestHandler):\n    # Optional settings (defaults shown)\n    timeout = 5                      # Timeout on all socket operations\n    rbufsize = -1                    # Read buffer size\n    wbufsize = 0                     # Write buffer size\n    disable_nagle_algorithm = False  # Sets TCP_NODELAY socket option\n    def handle(self):\n        print('Got connection from', self.client_address)\n        try:\n            for line in self.rfile:\n                # self.wfile is a file-like object for writing\n                self.wfile.write(line)\n        except socket.timeout:\n            print('Timed out!')\nFinally, it should be noted that most of Python’s higher-level networking modules (e.g.,\nHTTP, XML-RPC, etc.) are built on top of the socketserver functionality. That said,\nit is also not difficult to implement servers directly using the socket library as well. Here\nis a simple example of directly programming a server with Sockets:\nfrom socket import socket, AF_INET, SOCK_STREAM\ndef echo_handler(address, client_sock):\n    print('Got connection from {}'.format(address))\n    while True:\n        msg = client_sock.recv(8192)\n        if not msg:\n            break\n        client_sock.sendall(msg)\n    client_sock.close()\ndef echo_server(address, backlog=5):\n    sock = socket(AF_INET, SOCK_STREAM)\n    sock.bind(address)\n    sock.listen(backlog)\n    while True:\n        client_sock, client_addr = sock.accept()\n        echo_handler(client_addr, client_sock)\nif __name__ == '__main__':\n    echo_server(('', 20000))\n444 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": 10,
      "content": "11.3. Creating a UDP Server\nProblem\nYou want to implement a server that communicates with clients using the UDP Internet\nprotocol.\nSolution\nAs with TCP, UDP servers are also easy to create using the socketserver library. For\nexample, here is a simple time server:\nfrom socketserver import BaseRequestHandler, UDPServer\nimport time\nclass TimeHandler(BaseRequestHandler):\n    def handle(self):\n        print('Got connection from', self.client_address)\n        # Get message and client socket\n        msg, sock = self.request\n        resp = time.ctime()\n        sock.sendto(resp.encode('ascii'), self.client_address)\nif __name__ == '__main__':\n    serv = UDPServer(('', 20000), TimeHandler)\n    serv.serve_forever()\nAs before, you define a special handler class that implements a handle() method for\nservicing client connections. The request attribute is a tuple that contains the incoming\ndatagram and underlying socket object for the server. The client_address contains\nthe client address.\nTo test the server, run it and then open a separate Python process that sends messages\nto it:\n>>> from socket import socket, AF_INET, SOCK_DGRAM\n>>> s = socket(AF_INET, SOCK_DGRAM)\n>>> s.sendto(b'', ('localhost', 20000))\n0\n>>> s.recvfrom(8192)\n(b'Wed Aug 15 20:35:08 2012', ('127.0.0.1', 20000))\n>>>\nDiscussion\nA typical UDP server receives an incoming datagram (message) along with a client\naddress. If the server is to respond, it sends a datagram back to the client. For trans‐\nmission of datagrams, you should use the sendto() and recvfrom() methods of a\n11.3. Creating a UDP Server \n| \n445",
      "content_length": 1583,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": 10,
      "content": "socket. Although the traditional send() and recv() methods also might work, the for‐\nmer two methods are more commonly used with UDP communication.\nGiven that there is no underlying connection, UDP servers are often much easier to\nwrite than a TCP server. However, UDP is also inherently unreliable (e.g., no “connec‐\ntion” is established and messages might be lost). Thus, it would be up to you to figure\nout how to deal with lost messages. That’s a topic beyond the scope of this book, but\ntypically you might need to introduce sequence numbers, retries, timeouts, and other\nmechanisms to ensure reliability if it matters for your application. UDP is often used in\ncases where the requirement of reliable delivery can be relaxed. For instance, in real-\ntime applications such as multimedia streaming and games where there is simply no\noption to go back in time and recover a lost packet (the program simply skips it and\nkeeps moving forward).\nThe UDPServer class is single threaded, which means that only one request can be serv‐\niced at a time. In practice, this is less of an issue with UDP than with TCP connections.\nHowever, should you want concurrent operation, instantiate a ForkingUDPServer or\nThreadingUDPServer object instead:\nfrom socketserver import ThreadingUDPServer\n...\nif __name__ == '__main__':\n    serv = ThreadingUDPServer(('',20000), TimeHandler)\n    serv.serve_forever()\nImplementing a UDP server directly using sockets is also not difficult. Here is an\nexample:\nfrom socket import socket, AF_INET, SOCK_DGRAM\nimport time\ndef time_server(address):\n    sock = socket(AF_INET, SOCK_DGRAM)\n    sock.bind(address)\n    while True:\n        msg, addr = sock.recvfrom(8192)\n        print('Got message from', addr)\n        resp = time.ctime()\n        sock.sendto(resp.encode('ascii'), addr)\nif __name__ == '__main__':\n    time_server(('', 20000))\n446 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": 10,
      "content": "11.4. Generating a Range of IP Addresses from a CIDR\nAddress\nProblem\nYou have a CIDR network address such as “123.45.67.89/27,” and you want to generate\na range of all the IP addresses that it represents (e.g., “123.45.67.64,” “123.45.67.65,” …,\n“123.45.67.95”).\nSolution\nThe ipaddress module can be easily used to perform such calculations. For example:\n>>> import ipaddress\n>>> net = ipaddress.ip_network('123.45.67.64/27')\n>>> net\nIPv4Network('123.45.67.64/27')\n>>> for a in net:\n...     print(a)\n...\n123.45.67.64\n123.45.67.65\n123.45.67.66\n123.45.67.67\n123.45.67.68\n...\n123.45.67.95\n>>>\n>>> net6 = ipaddress.ip_network('12:3456:78:90ab:cd:ef01:23:30/125')\n>>> net6\nIPv6Network('12:3456:78:90ab:cd:ef01:23:30/125')\n>>> for a in net6:\n...     print(a)\n...\n12:3456:78:90ab:cd:ef01:23:30\n12:3456:78:90ab:cd:ef01:23:31\n12:3456:78:90ab:cd:ef01:23:32\n12:3456:78:90ab:cd:ef01:23:33\n12:3456:78:90ab:cd:ef01:23:34\n12:3456:78:90ab:cd:ef01:23:35\n12:3456:78:90ab:cd:ef01:23:36\n12:3456:78:90ab:cd:ef01:23:37\n>>>\nNetwork objects also allow indexing like arrays. For example:\n>>> net.num_addresses\n32\n>>> net[0]\n11.4. Generating a Range of IP Addresses from a CIDR Address \n| \n447",
      "content_length": 1167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": 10,
      "content": "IPv4Address('123.45.67.64')\n>>> net[1]\nIPv4Address('123.45.67.65')\n>>> net[-1]\nIPv4Address('123.45.67.95')\n>>> net[-2]\nIPv4Address('123.45.67.94')\n>>>\nIn addition, you can perform operations such as a check for network membership:\n>>> a = ipaddress.ip_address('123.45.67.69')\n>>> a in net\nTrue\n>>> b = ipaddress.ip_address('123.45.67.123')\n>>> b in net\nFalse\n>>>\nAn IP address and network address can be specified together as an IP interface. For\nexample:\n>>> inet = ipaddress.ip_interface('123.45.67.73/27')\n>>> inet.network\nIPv4Network('123.45.67.64/27')\n>>> inet.ip\nIPv4Address('123.45.67.73')\n>>>\nDiscussion\nThe ipaddress module has classes for representing IP addresses, networks, and inter‐\nfaces. This can be especially useful if you want to write code that needs to manipulate\nnetwork addresses in some way (e.g., parsing, printing, validating, etc.).\nBe aware that there is only limited interaction between the ipaddress module and other\nnetwork-related modules, such as the socket library. In particular, it is usually not\npossible to use an instance of IPv4Address as a substitute for address string. Instead,\nyou have to explicitly convert it using str() first. For example:\n>>> a = ipaddress.ip_address('127.0.0.1')\n>>> from socket import socket, AF_INET, SOCK_STREAM\n>>> s = socket(AF_INET, SOCK_STREAM)\n>>> s.connect((a, 8080))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: Can't convert 'IPv4Address' object to str implicitly\n>>> s.connect((str(a), 8080))\n>>>\nSee “An Introduction to the ipaddress Module” for more information and advanced\nusage.\n448 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": 10,
      "content": "11.5. Creating a Simple REST-Based Interface\nProblem\nYou want to be able to control or interact with your program remotely over the network\nusing a simple REST-based interface. However, you don’t want to do it by installing a\nfull-fledged web programming framework.\nSolution\nOne of the easiest ways to build REST-based interfaces is to create a tiny library based\non the WSGI standard, as described in PEP 3333. Here is an example:\n# resty.py\nimport cgi\ndef notfound_404(environ, start_response):\n    start_response('404 Not Found', [ ('Content-type', 'text/plain') ])\n    return [b'Not Found']\nclass PathDispatcher:\n    def __init__(self):\n        self.pathmap = { }\n    def __call__(self, environ, start_response):\n        path = environ['PATH_INFO']\n        params = cgi.FieldStorage(environ['wsgi.input'],\n                                  environ=environ)\n        method = environ['REQUEST_METHOD'].lower()\n        environ['params'] = { key: params.getvalue(key) for key in params }\n        handler = self.pathmap.get((method,path), notfound_404)\n        return handler(environ, start_response)\n    def register(self, method, path, function):\n        self.pathmap[method.lower(), path] = function\n        return function\nTo use this dispatcher, you simply write different handlers, such as the following:\nimport time\n_hello_resp = '''\\\n<html>\n  <head>\n     <title>Hello {name}</title>\n   </head>\n   <body>\n     <h1>Hello {name}!</h1>\n   </body>\n</html>'''\n11.5. Creating a Simple REST-Based Interface \n| \n449",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": 10,
      "content": "def hello_world(environ, start_response):\n    start_response('200 OK', [ ('Content-type','text/html')])\n    params = environ['params']\n    resp = _hello_resp.format(name=params.get('name'))\n    yield resp.encode('utf-8')\n_localtime_resp = '''\\\n<?xml version=\"1.0\"?>\n<time>\n  <year>{t.tm_year}</year>\n  <month>{t.tm_mon}</month>\n  <day>{t.tm_mday}</day>\n  <hour>{t.tm_hour}</hour>\n  <minute>{t.tm_min}</minute>\n  <second>{t.tm_sec}</second>\n</time>'''\ndef localtime(environ, start_response):\n    start_response('200 OK', [ ('Content-type', 'application/xml') ])\n    resp = _localtime_resp.format(t=time.localtime())\n    yield resp.encode('utf-8')\nif __name__ == '__main__':\n    from resty import PathDispatcher\n    from wsgiref.simple_server import make_server\n    # Create the dispatcher and register functions\n    dispatcher = PathDispatcher()\n    dispatcher.register('GET', '/hello', hello_world)\n    dispatcher.register('GET', '/localtime', localtime)\n    # Launch a basic server\n    httpd = make_server('', 8080, dispatcher)\n    print('Serving on port 8080...')\n    httpd.serve_forever()\nTo test your server, you can interact with it using a browser or urllib. For example:\n>>> u = urlopen('http://localhost:8080/hello?name=Guido')\n>>> print(u.read().decode('utf-8'))\n<html>\n  <head>\n     <title>Hello Guido</title>\n   </head>\n   <body>\n     <h1>Hello Guido!</h1>\n   </body>\n</html>\n>>> u = urlopen('http://localhost:8080/localtime')\n>>> print(u.read().decode('utf-8'))\n<?xml version=\"1.0\"?>\n<time>\n450 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1550,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": 10,
      "content": "<year>2012</year>\n  <month>11</month>\n  <day>24</day>\n  <hour>14</hour>\n  <minute>49</minute>\n  <second>17</second>\n</time>\n>>>\nDiscussion\nIn REST-based interfaces, you are typically writing programs that respond to common\nHTTP requests. However, unlike a full-fledged website, you’re often just pushing data\naround. This data might be encoded in a variety of standard formats such as XML, JSON,\nor CSV. Although it seems minimal, providing an API in this manner can be a very\nuseful thing for a wide variety of applications.\nFor example, long-running programs might use a REST API to implement monitoring\nor diagnostics. Big data applications can use REST to build a query/data extraction\nsystem. REST can even be used to control hardware devices, such as robots, sensors,\nmills, or lightbulbs. What’s more, REST APIs are well supported by various client-side\nprogramming environments, such as Javascript, Android, iOS, and so forth. Thus, hav‐\ning such an interface can be a way to encourage the development of more complex\napplications that interface with your code.\nFor implementing a simple REST interface, it is often easy enough to base your code on\nthe Python WSGI standard. WSGI is supported by the standard library, but also by most\nthird-party web frameworks. Thus, if you use it, there is a lot of flexibility in how your\ncode might be used later.\nIn WSGI, you simply implement applications in the form of a callable that accepts this\ncalling convention:\nimport cgi\ndef wsgi_app(environ, start_response):\n    ...\nThe environ argument is a dictionary that contains values inspired by the CGI interface\nprovided by various web servers such as Apache [see Internet RFC 3875]. To extract\ndifferent fields, you would write code like this:\ndef wsgi_app(environ, start_response):\n    method = environ['REQUEST_METHOD']\n    path = environ['PATH_INFO']\n    # Parse the query parameters\n    params = cgi.FieldStorage(environ['wsgi.input'], environ=environ)\n    ...\n11.5. Creating a Simple REST-Based Interface \n| \n451",
      "content_length": 2019,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": 10,
      "content": "A few common values are shown here. environ['REQUEST_METHOD'] is the type of re‐\nquest (e.g., GET, POST, HEAD, etc.). environ['PATH_INFO'] is the path or the resource\nbeing requested. The call to cgi.FieldStorage() extracts supplied query parameters\nfrom the request and puts them into a dictionary-like object for later use.\nThe start_response argument is a function that must be called to initiate a response.\nThe first argument is the resulting HTTP status. The second argument is a list of (name,\nvalue) tuples that make up the HTTP headers of the response. For example:\ndef wsgi_app(environ, start_response):\n    ...\n    start_response('200 OK', [('Content-type', 'text/plain')])\nTo return data, an WSGI application must return a sequence of byte strings. This can\nbe done using a list like this:\ndef wsgi_app(environ, start_response):\n    ...\n    start_response('200 OK', [('Content-type', 'text/plain')])\n    resp = []\n    resp.append(b'Hello World\\n')\n    resp.append(b'Goodbye!\\n')\n    return resp\nAlternatively, you can use yield:\ndef wsgi_app(environ, start_response):\n    ...\n    start_response('200 OK', [('Content-type', 'text/plain')])\n    yield b'Hello World\\n'\n    yield b'Goodbye!\\n'\nIt’s important to emphasize that byte strings must be used in the result. If the response\nconsists of text, it will need to be encoded into bytes first. Of course, there is no re‐\nquirement that the returned value be text—you could easily write an application func‐\ntion that creates images.\nAlthough WSGI applications are commonly defined as a function, as shown, an instance\nmay also be used as long as it implements a suitable __call__() method. For example:\nclass WSGIApplication:\n    def __init__(self):\n        ...\n    def __call__(self, environ, start_response)\n       ...\nThis technique has been used to create the PathDispatcher class in the recipe. The\ndispatcher does nothing more than manage a dictionary mapping (method, path) pairs\nto handler functions. When a request arrives, the method and path are extracted and\nused to dispatch to a handler. In addition, any query variables are parsed and put into\n452 \n| \nChapter 11: Network and Web Programming",
      "content_length": 2167,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": 10,
      "content": "a dictionary that is stored as environ['params'] (this latter step is so common, it makes\na lot of sense to simply do it in the dispatcher in order to avoid a lot of replicated code).\nTo use the dispatcher, you simply create an instance and register various WSGI-style\napplication functions with it, as shown in the recipe. Writing these functions should be\nextremely straightforward, as you follow the rules concerning the start_response()\nfunction and produce output as byte strings.\nOne thing to consider when writing such functions is the careful use of string templates.\nNobody likes to work with code that is a tangled mess of print() functions, XML, and\nvarious formatting operations. In the solution, triple-quoted string templates are being\ndefined and used internally. This particular approach makes it easier to change the\nformat of the output later (just change the template as opposed to any of the code that\nuses it).\nFinally, an important part of using WSGI is that nothing in the implementation is spe‐\ncific to a particular web server. That is actually the whole idea—since the standard is\nserver and framework neutral, you should be able to plug your application into a wide\nvariety of servers. In the recipe, the following code is used for testing:\nif __name__ == '__main__':\n    from wsgiref.simple_server import make_server\n    # Create the dispatcher and register functions\n    dispatcher = PathDispatcher()\n    ...\n    # Launch a basic server\n    httpd = make_server('', 8080, dispatcher)\n    print('Serving on port 8080...')\n    httpd.serve_forever()\nThis will create a simple server that you can use to see if your implementation works.\nLater on, when you’re ready to scale things up to a larger level, you will change this code\nto work with a particular server.\nWSGI is an intentionally minimal specification. As such, it doesn’t provide any support\nfor more advanced concepts such as authentication, cookies, redirection, and so forth.\nThese are not hard to implement yourself. However, if you want just a bit more support,\nyou might consider third-party libraries, such as WebOb or Paste. \n11.5. Creating a Simple REST-Based Interface \n| \n453",
      "content_length": 2170,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": 10,
      "content": "11.6. Implementing a Simple Remote Procedure Call with\nXML-RPC\nProblem\nYou want an easy way to execute functions or methods in Python programs running on\nremote machines.\nSolution\nPerhaps the easiest way to implement a simple remote procedure call mechanism is to\nuse XML-RPC. Here is an example of a simple server that implements a simple key-\nvalue store:\nfrom xmlrpc.server import SimpleXMLRPCServer\nclass KeyValueServer:\n    _rpc_methods_ = ['get', 'set', 'delete', 'exists', 'keys']\n    def __init__(self, address):\n        self._data = {}\n        self._serv = SimpleXMLRPCServer(address, allow_none=True)\n        for name in self._rpc_methods_:\n            self._serv.register_function(getattr(self, name))\n    def get(self, name):\n        return self._data[name]\n    def set(self, name, value):\n        self._data[name] = value\n    def delete(self, name):\n        del self._data[name]\n    def exists(self, name):\n        return name in self._data\n    def keys(self):\n        return list(self._data)\n    def serve_forever(self):\n        self._serv.serve_forever()\n# Example\nif __name__ == '__main__':\n    kvserv = KeyValueServer(('', 15000))\n    kvserv.serve_forever()\nHere is how you would access the server remotely from a client:\n454 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": 10,
      "content": ">>> from xmlrpc.client import ServerProxy\n>>> s = ServerProxy('http://localhost:15000', allow_none=True)\n>>> s.set('foo', 'bar')\n>>> s.set('spam', [1, 2, 3])\n>>> s.keys()\n['spam', 'foo']\n>>> s.get('foo')\n'bar'\n>>> s.get('spam')\n[1, 2, 3]\n>>> s.delete('spam')\n>>> s.exists('spam')\nFalse\n>>>\nDiscussion\nXML-RPC can be an extremely easy way to set up a simple remote procedure call service.\nAll you need to do is create a server instance, register functions with it using the regis\nter_function() method, and then launch it using the serve_forever() method. This\nrecipe packages it up into a class to put all of the code together, but there is no such\nrequirement. For example, you could create a server by trying something like this:\nfrom xmlrpc.server import SimpleXMLRPCServer\ndef add(x,y):\n    return x+y\nserv = SimpleXMLRPCServer(('', 15000))\nserv.register_function(add)\nserv.serve_forever()\nFunctions exposed via XML-RPC only work with certain kinds of data such as strings,\nnumbers, lists, and dictionaries. For everything else, some study is required. For in‐\nstance, if you pass an instance through XML-RPC, only its instance dictionary is\nhandled:\n>>> class Point:\n...     def __init__(self, x, y):\n...             self.x = x\n...             self.y = y\n...\n>>> p = Point(2, 3)\n>>> s.set('foo', p)\n>>> s.get('foo')\n{'x': 2, 'y': 3}\n>>>\nSimilarly, handling of binary data is a bit different than you expect:\n>>> s.set('foo', b'Hello World')\n>>> s.get('foo')\n<xmlrpc.client.Binary object at 0x10131d410>\n11.6. Implementing a Simple Remote Procedure Call with XML-RPC \n| \n455",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": 10,
      "content": ">>> _.data\nb'Hello World'\n>>>\nAs a general rule, you probably shouldn’t expose an XML-RPC service to the rest of the\nworld as a public API. It often works best on internal networks where you might want\nto write simple distributed programs involving a few different machines.\nA downside to XML-RPC is its performance. The SimpleXMLRPCServer implementa‐\ntion is only single threaded, and wouldn’t be appropriate for scaling a large application,\nalthough it can be made to run multithreaded, as shown in Recipe 11.2. Also, since\nXML-RPC serializes all data as XML, it’s inherently slower than other approaches.\nHowever, one benefit of this encoding is that it’s understood by a variety of other pro‐\ngramming languages. By using it, clients written in languages other than Python will be\nable to access your service.\nDespite its limitations, XML-RPC is worth knowing about if you ever have the need to\nmake a quick and dirty remote procedure call system. Oftentimes, the simple solution\nis good enough.\n11.7. Communicating Simply Between Interpreters\nProblem\nYou are running multiple instances of the Python interpreter, possibly on different ma‐\nchines, and you would like to exchange data between interpreters using messages.\nSolution\nIt is easy to communicate between interpreters if you use the multiprocessing.con\nnection module. Here is a simple example of writing an echo server:\nfrom multiprocessing.connection import Listener\nimport traceback\ndef echo_client(conn):\n    try:\n        while True:\n            msg = conn.recv()\n            conn.send(msg)\n    except EOFError:\n        print('Connection closed')\ndef echo_server(address, authkey):\n    serv = Listener(address, authkey=authkey)\n    while True:\n        try:\n            client = serv.accept()\n456 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": 10,
      "content": "echo_client(client)\n        except Exception:\n            traceback.print_exc()\necho_server(('', 25000), authkey=b'peekaboo')\nHere is a simple example of a client connecting to the server and sending various\nmessages:\n>>> from multiprocessing.connection import Client\n>>> c = Client(('localhost', 25000), authkey=b'peekaboo')\n>>> c.send('hello')\n>>> c.recv()\n'hello'\n>>> c.send(42)\n>>> c.recv()\n42\n>>> c.send([1, 2, 3, 4, 5])\n>>> c.recv()\n[1, 2, 3, 4, 5]\n>>>\nUnlike a low-level socket, messages are kept intact (each object sent using send() is\nreceived in its entirety with recv()). In addition, objects are serialized using pickle.\nSo, any object compatible with pickle can be sent or received over the connection.\nDiscussion\nThere are many packages and libraries related to implementing various forms of mes‐\nsage passing, such as ZeroMQ, Celery, and so forth. As an alternative, you might also\nbe inclined to implement a message layer on top of low-level sockets. However, some‐\ntimes you just want a simple solution. The multiprocessing.connection library is just\nthat—using a few simple primitives, you can easily connect interpreters together and\nhave them exchange messages.\nIf you know that the interpreters are going to be running on the same machine, you can\nuse alternative forms of networking, such as UNIX domain sockets or Windows named\npipes. To create a connection using a UNIX domain socket, simply change the address\nto a filename such as this:\ns = Listener('/tmp/myconn', authkey=b'peekaboo')\nTo create a connection using a Windows named pipe, use a filename such as this:\ns = Listener(r'\\\\.\\pipe\\myconn', authkey=b'peekaboo')\nAs a general rule, you would not be using multiprocessing to implement public-facing\nservices. The authkey parameter to Client() and Listener() is there to help authen‐\nticate the end points of the connection. Connection attempts with a bad key raise an\nexception. In addition, the module is probably best suited for long-running connections\n11.7. Communicating Simply Between Interpreters \n| \n457",
      "content_length": 2044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": 10,
      "content": "(not a large number of short connections). For example, two interpreters might establish\na connection at startup and keep the connection active for the entire duration of a\nproblem.\nDon’t use multiprocessing if you need more low-level control over aspects of the con‐\nnection. For example, if you needed to support timeouts, nonblocking I/O, or anything\nsimilar, you’re probably better off using a different library or implementing such features\non top of sockets instead.\n11.8. Implementing Remote Procedure Calls\nProblem\nYou want to implement simple remote procedure call (RPC) on top of a message passing\nlayer, such as sockets, multiprocessing connections, or ZeroMQ.\nSolution\nRPC is easy to implement by encoding function requests, arguments, and return values\nusing pickle, and passing the pickled byte strings between interpreters. Here is an\nexample of a simple RPC handler that could be incorporated into a server:\n# rpcserver.py\nimport pickle\nclass RPCHandler:\n    def __init__(self):\n        self._functions = { }\n    def register_function(self, func):\n        self._functions[func.__name__] = func\n    def handle_connection(self, connection):\n        try:\n            while True:\n                # Receive a message\n                func_name, args, kwargs = pickle.loads(connection.recv())\n                # Run the RPC and send a response\n                try:\n                    r = self._functions[func_name](*args,**kwargs)\n                    connection.send(pickle.dumps(r))\n                except Exception as e:\n                    connection.send(pickle.dumps(e))\n        except EOFError:\n             pass\n458 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1675,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": 10,
      "content": "To use this handler, you need to add it into a messaging server. There are many possible\nchoices, but the multiprocessing library provides a simple option. Here is an example\nRPC server:\nfrom multiprocessing.connection import Listener\nfrom threading import Thread\ndef rpc_server(handler, address, authkey):\n    sock = Listener(address, authkey=authkey)\n    while True:\n        client = sock.accept()\n        t = Thread(target=handler.handle_connection, args=(client,))\n        t.daemon = True\n        t.start()\n# Some remote functions\ndef add(x, y):\n    return x + y\ndef sub(x, y):\n    return x - y\n# Register with a handler\nhandler = RPCHandler()\nhandler.register_function(add)\nhandler.register_function(sub)\n# Run the server\nrpc_server(handler, ('localhost', 17000), authkey=b'peekaboo')\nTo access the server from a remote client, you need to create a corresponding RPC proxy\nclass that forwards requests. For example:\nimport pickle\nclass RPCProxy:\n    def __init__(self, connection):\n        self._connection = connection\n    def __getattr__(self, name):\n        def do_rpc(*args, **kwargs):\n            self._connection.send(pickle.dumps((name, args, kwargs)))\n            result = pickle.loads(self._connection.recv())\n            if isinstance(result, Exception):\n                raise result\n            return result\n        return do_rpc\nTo use the proxy, you wrap it around a connection to the server. For example:\n>>> from multiprocessing.connection import Client\n>>> c = Client(('localhost', 17000), authkey=b'peekaboo')\n>>> proxy = RPCProxy(c)\n>>> proxy.add(2, 3)\n11.8. Implementing Remote Procedure Calls \n| \n459",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": 10,
      "content": "5\n>>> proxy.sub(2, 3)\n-1\n>>> proxy.sub([1, 2], 4)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"rpcserver.py\", line 37, in do_rpc\n    raise result\nTypeError: unsupported operand type(s) for -: 'list' and 'int'\n>>>\nIt should be noted that many messaging layers (such as multiprocessing) already se‐\nrialize data using pickle. If this is the case, the pickle.dumps() and pickle.loads()\ncalls can be eliminated.\nDiscussion\nThe general idea of the RPCHandler and RPCProxy classes is relatively simple. If a client\nwants to call a remote function, such as foo(1, 2, z=3), the proxy class creates a tuple\n('foo', (1, 2), {'z': 3}) that contains the function name and arguments. This\ntuple is pickled and sent over the connection. This is performed in the do_rpc() closure\nthat’s returned by the __getattr__() method of RPCProxy. The server receives and\nunpickles the message, looks up the function name to see if it’s registered, and executes\nit with the given arguments. The result (or exception) is then pickled and sent back.\nAs shown, the example relies on multiprocessing for communication. However, this\napproach could be made to work with just about any other messaging system. For ex‐\nample, if you want to implement RPC over ZeroMQ, just replace the connection objects\nwith an appropriate ZeroMQ socket object.\nGiven the reliance on pickle, security is a major concern (because a clever hacker can\ncreate messages that make arbitrary functions execute during unpickling). In particular,\nyou should never allow RPC from untrusted or unauthenticated clients. In particular,\nyou definitely don’t want to allow access from just any machine on the Internet—this\nshould really only be used internally, behind a firewall, and not exposed to the rest of\nthe world.\nAs an alternative to pickle, you might consider the use of JSON, XML, or some other\ndata encoding for serialization. For example, this recipe is fairly easy to adapt to JSON\nencoding if you simply replace pickle.loads() and pickle.dumps() with\njson.loads() and json.dumps(). For example:\n# jsonrpcserver.py\nimport json\nclass RPCHandler:\n    def __init__(self):\n        self._functions = { }\n460 \n| \nChapter 11: Network and Web Programming",
      "content_length": 2233,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": 10,
      "content": "def register_function(self, func):\n        self._functions[func.__name__] = func\n    def handle_connection(self, connection):\n        try:\n            while True:\n                # Receive a message\n                func_name, args, kwargs = json.loads(connection.recv())\n                # Run the RPC and send a response\n                try:\n                    r = self._functions[func_name](*args,**kwargs)\n                    connection.send(json.dumps(r))\n                except Exception as e:\n                    connection.send(json.dumps(str(e)))\n        except EOFError:\n             pass\n# jsonrpcclient.py\nimport json\nclass RPCProxy:\n    def __init__(self, connection):\n        self._connection = connection\n    def __getattr__(self, name):\n        def do_rpc(*args, **kwargs):\n            self._connection.send(json.dumps((name, args, kwargs)))\n            result = json.loads(self._connection.recv())\n            return result\n        return do_rpc\nOne complicated factor in implementing RPC is how to handle exceptions. At the very\nleast, the server shouldn’t crash if an exception is raised by a method. However, the\nmeans by which the exception gets reported back to the client requires some study. If\nyou’re using pickle, exception instances can often be serialized and reraised in the\nclient. If you’re using some other protocol, you might have to think of an alternative\napproach. At the very least, you would probably want to return the exception string in\nthe response. This is the approach taken in the JSON example.\nFor another example of an RPC implementation, it can be useful to look at the imple‐\nmentation of the SimpleXMLRPCServer and ServerProxy classes used in XML-RPC, as\ndescribed in Recipe 11.6.\n11.9. Authenticating Clients Simply\nProblem\nYou want a simple way to authenticate the clients connecting to servers in a distributed\nsystem, but don’t need the complexity of something like SSL.\n11.9. Authenticating Clients Simply \n| \n461",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": 10,
      "content": "Solution\nSimple but effective authentication can be performed by implementing a connection\nhandshake using the hmac module. Here is sample code:\nimport hmac\nimport os\ndef client_authenticate(connection, secret_key):\n    '''\n    Authenticate client to a remote service.\n    connection represents a network connection.\n    secret_key is a key known only to both client/server.\n    '''\n    message = connection.recv(32)\n    hash = hmac.new(secret_key, message)\n    digest = hash.digest()\n    connection.send(digest)\ndef server_authenticate(connection, secret_key):\n    '''\n    Request client authentication.\n    '''\n    message = os.urandom(32)\n    connection.send(message)\n    hash = hmac.new(secret_key, message)\n    digest = hash.digest()\n    response = connection.recv(len(digest))\n    return hmac.compare_digest(digest,response)\nThe general idea is that upon connection, the server presents the client with a message\nof random bytes (returned by os.urandom(), in this case). The client and server both\ncompute a cryptographic hash of the random data using hmac and a secret key known\nonly to both ends. The client sends its computed digest back to the server, where it is\ncompared and used to decide whether or not to accept or reject the connection.\nComparison of resulting digests should be performed using the hmac.compare_di\ngest() function. This function has been written in a way that avoids timing-analysis-\nbased attacks and should be used instead of a normal comparison operator (==).\nTo use these functions, you would incorporate them into existing networking or mes‐\nsaging code. For example, with sockets, the server code might look something like this:\nfrom socket import socket, AF_INET, SOCK_STREAM\nsecret_key = b'peekaboo'\ndef echo_handler(client_sock):\n    if not server_authenticate(client_sock, secret_key):\n        client_sock.close()\n        return\n    while True:\n462 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": 11,
      "content": "msg = client_sock.recv(8192)\n        if not msg:\n            break\n        client_sock.sendall(msg)\ndef echo_server(address):\n    s = socket(AF_INET, SOCK_STREAM)\n    s.bind(address)\n    s.listen(5)\n    while True:\n        c,a = s.accept()\n        echo_handler(c)\necho_server(('', 18000))\nWithin a client, you would do this:\nfrom socket import socket, AF_INET, SOCK_STREAM\nsecret_key = b'peekaboo'\ns = socket(AF_INET, SOCK_STREAM)\ns.connect(('localhost', 18000))\nclient_authenticate(s, secret_key)\ns.send(b'Hello World')\nresp = s.recv(1024)\n...\nDiscussion\nA common use of hmac authentication is in internal messaging systems and interprocess\ncommunication. For example, if you are writing a system that involves multiple pro‐\ncesses communicating across a cluster of machines, you can use this approach to make\nsure that only allowed processes are allowed to connect to one another. In fact, HMAC-\nbased authentication is used internally by the multiprocessing library when it sets up\ncommunication with subprocesses.\nIt’s important to stress that authenticating a connection is not the same as encryption.\nSubsequent communication on an authenticated connection is sent in the clear, and\nwould be visible to anyone inclined to sniff the traffic (although the secret key known\nto both sides is never transmitted).\nThe authentication algorithm used by hmac is based on cryptographic hashing functions,\nsuch as MD5 and SHA-1, and is described in detail in IETF RFC 2104. \n11.9. Authenticating Clients Simply \n| \n463",
      "content_length": 1513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": 11,
      "content": "11.10. Adding SSL to Network Services\nProblem\nYou want to implement a network service involving sockets where servers and clients\nauthenticate themselves and encrypt the transmitted data using SSL.\nSolution\nThe ssl module provides support for adding SSL to low-level socket connections. In\nparticular, the ssl.wrap_socket() function takes an existing socket and wraps an SSL\nlayer around it. For example, here’s an example of a simple echo server that presents a\nserver certificate to connecting clients:\nfrom socket import socket, AF_INET, SOCK_STREAM\nimport ssl\nKEYFILE = 'server_key.pem'   # Private key of the server\nCERTFILE = 'server_cert.pem' # Server certificate (given to client)\ndef echo_client(s):\n    while True:\n        data = s.recv(8192)\n        if data == b'':\n            break\n        s.send(data)\n    s.close()\n    print('Connection closed')\ndef echo_server(address):\n    s = socket(AF_INET, SOCK_STREAM)\n    s.bind(address)\n    s.listen(1)\n    # Wrap with an SSL layer requiring client certs\n    s_ssl = ssl.wrap_socket(s,\n                            keyfile=KEYFILE,\n                            certfile=CERTFILE,\n                            server_side=True\n                            )\n    # Wait for connections\n    while True:\n        try:\n            c,a = s_ssl.accept()\n            print('Got connection', c, a)\n            echo_client(c)\n        except Exception as e:\n            print('{}: {}'.format(e.__class__.__name__, e))\necho_server(('', 20000))\n464 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": 11,
      "content": "Here’s an interactive session that shows how to connect to the server as a client. The\nclient requires the server to present its certificate and verifies it:\n>>> from socket import socket, AF_INET, SOCK_STREAM\n>>> import ssl\n>>> s = socket(AF_INET, SOCK_STREAM)\n>>> s_ssl = ssl.wrap_socket(s,\n...                         cert_reqs=ssl.CERT_REQUIRED,\n...                         ca_certs = 'server_cert.pem')\n>>> s_ssl.connect(('localhost', 20000))\n>>> s_ssl.send(b'Hello World?')\n12\n>>> s_ssl.recv(8192)\nb'Hello World?'\n>>>\nThe problem with all of this low-level socket hacking is that it doesn’t play well with\nexisting network services already implemented in the standard library. For example,\nmost server code (HTTP, XML-RPC, etc.) is actually based on the socketserver library.\nClient code is also implemented at a higher level. It is possible to add SSL to existing\nservices, but a slightly different approach is needed.\nFirst, for servers, SSL can be added through the use of a mixin class like this:\nimport ssl\nclass SSLMixin:\n    '''\n    Mixin class that adds support for SSL to existing servers based\n    on the socketserver module.\n    '''\n    def __init__(self, *args,\n                 keyfile=None, certfile=None, ca_certs=None,\n                 cert_reqs=ssl.NONE,\n                 **kwargs):\n        self._keyfile = keyfile\n        self._certfile = certfile\n        self._ca_certs = ca_certs\n        self._cert_reqs = cert_reqs\n        super().__init__(*args, **kwargs)\n    def get_request(self):\n        client, addr = super().get_request()\n        client_ssl = ssl.wrap_socket(client,\n                                     keyfile = self._keyfile,\n                                     certfile = self._certfile,\n                                     ca_certs = self._ca_certs,\n                                     cert_reqs = self._cert_reqs,\n                                     server_side = True)\n        return client_ssl, addr\n11.10. Adding SSL to Network Services \n| \n465",
      "content_length": 1991,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": 11,
      "content": "To use this mixin class, you can mix it with other server classes. For example, here’s an\nexample of defining an XML-RPC server that operates over SSL:\n# XML-RPC server with SSL\nfrom xmlrpc.server import SimpleXMLRPCServer\nclass SSLSimpleXMLRPCServer(SSLMixin, SimpleXMLRPCServer):\n    pass\nHere’s the XML-RPC server from Recipe 11.6 modified only slightly to use SSL:\nimport ssl\nfrom xmlrpc.server import SimpleXMLRPCServer\nfrom sslmixin import SSLMixin\nclass SSLSimpleXMLRPCServer(SSLMixin, SimpleXMLRPCServer):\n    pass\nclass KeyValueServer:\n    _rpc_methods_ = ['get', 'set', 'delete', 'exists', 'keys']\n    def __init__(self, *args, **kwargs):\n        self._data = {}\n        self._serv = SSLSimpleXMLRPCServer(*args, allow_none=True, **kwargs)\n        for name in self._rpc_methods_:\n            self._serv.register_function(getattr(self, name))\n    def get(self, name):\n        return self._data[name]\n    def set(self, name, value):\n        self._data[name] = value\n    def delete(self, name):\n        del self._data[name]\n    def exists(self, name):\n        return name in self._data\n    def keys(self):\n        return list(self._data)\n    def serve_forever(self):\n        self._serv.serve_forever()\nif __name__ == '__main__':\n    KEYFILE='server_key.pem'    # Private key of the server\n    CERTFILE='server_cert.pem'  # Server certificate\n    kvserv = KeyValueServer(('', 15000),\n                            keyfile=KEYFILE,\n                            certfile=CERTFILE),\n    kvserv.serve_forever()\n466 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1557,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": 11,
      "content": "To use this server, you can connect using the normal xmlrpc.client module. Just spec‐\nify a https: in the URL. For example:\n>>> from xmlrpc.client import ServerProxy\n>>> s = ServerProxy('https://localhost:15000', allow_none=True)\n>>> s.set('foo','bar')\n>>> s.set('spam', [1, 2, 3])\n>>> s.keys()\n['spam', 'foo']\n>>> s.get('foo')\n'bar'\n>>> s.get('spam')\n[1, 2, 3]\n>>> s.delete('spam')\n>>> s.exists('spam')\nFalse\n>>>\nOne complicated issue with SSL clients is performing extra steps to verify the server\ncertificate or to present a server with client credentials (such as a client certificate).\nUnfortunately, there seems to be no standardized way to accomplish this, so research is\noften required. However, here is an example of how to set up a secure XML-RPC con‐\nnection that verifies the server’s certificate:\nfrom xmlrpc.client import SafeTransport, ServerProxy\nimport ssl\nclass VerifyCertSafeTransport(SafeTransport):\n    def __init__(self, cafile, certfile=None, keyfile=None):\n        SafeTransport.__init__(self)\n        self._ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)\n        self._ssl_context.load_verify_locations(cafile)\n        if cert:\n            self._ssl_context.load_cert_chain(certfile, keyfile)\n        self._ssl_context.verify_mode = ssl.CERT_REQUIRED\n    def make_connection(self, host):\n        # Items in the passed dictionary are passed as keyword\n        # arguments to the http.client.HTTPSConnection() constructor.\n        # The context argument allows an ssl.SSLContext instance to\n        # be passed with information about the SSL configuration\n        s = super().make_connection((host, {'context': self._ssl_context}))\n        return s\n# Create the client proxy\ns = ServerProxy('https://localhost:15000',\n                transport=VerifyCertSafeTransport('server_cert.pem'),\n                allow_none=True)\n11.10. Adding SSL to Network Services \n| \n467",
      "content_length": 1890,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": 11,
      "content": "As shown, the server presents a certificate to the client and the client verifies it. This\nverification can go both directions. If the server wants to verify the client, change the\nserver startup to the following:\nif __name__ == '__main__':\n    KEYFILE='server_key.pem'   # Private key of the server\n    CERTFILE='server_cert.pem' # Server certificate\n    CA_CERTS='client_cert.pem' # Certificates of accepted clients\n    kvserv = KeyValueServer(('', 15000),\n                            keyfile=KEYFILE,\n                            certfile=CERTFILE,\n                            ca_certs=CA_CERTS,\n                            cert_reqs=ssl.CERT_REQUIRED,\n                            )\n    kvserv.serve_forever()\nTo make the XML-RPC client present its certificates, change the ServerProxy initiali‐\nzation to this:\n# Create the client proxy\ns = ServerProxy('https://localhost:15000',\n                transport=VerifyCertSafeTransport('server_cert.pem',\n                                                  'client_cert.pem',\n                                                  'client_key.pem'),\n                allow_none=True)\nDiscussion\nGetting this recipe to work will test your system configuration skills and understanding\nof SSL. Perhaps the biggest challenge is simply getting the initial configuration of keys,\ncertificates, and other matters in order.\nTo clarify what’s required, each endpoint of an SSL connection typically has a private\nkey and a signed certificate file. The certificate file contains the public key and is pre‐\nsented to the remote peer on each connection. For public servers, certificates are nor‐\nmally signed by a certificate authority such as Verisign, Equifax, or similar organization\n(something that costs money). To verify server certificates, clients maintain a file con‐\ntaining the certificates of trusted certificate authorities. For example, web browsers\nmaintain certificates corresponding to the major certificate authorities and use them to\nverify the integrity of certificates presented by web servers during HTTPS connections.\nFor the purposes of this recipe, you can create what’s known as a self-signed certificate.\nHere’s how you do it:\nbash % openssl req -new -x509 -days 365 -nodes -out server_cert.pem \\\n           -keyout server_key.pem\nGenerating a 1024 bit RSA private key\n..........................................++++++\n...++++++\n468 \n| \nChapter 11: Network and Web Programming",
      "content_length": 2429,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": 11,
      "content": "writing new private key to 'server_key.pem'\n -----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n -----\nCountry Name (2 letter code) [AU]:US\nState or Province Name (full name) [Some-State]:Illinois\nLocality Name (eg, city) []:Chicago\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Dabeaz, LLC\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, YOUR name) []:localhost\nEmail Address []:\nbash %\nWhen creating the certificate, the values for the various fields are often arbitrary. How‐\never, the “Common Name” field often contains the DNS hostname of servers. If you’re\njust testing things out on your own machine, use “localhost.” Otherwise, use the domain\nname of the machine that’s going to run the server.\nAs a result of this configuration, you will have a server_key.pem file that contains the\nprivate key. It looks like this:\n    -----BEGIN RSA PRIVATE KEY-----\n    MIICXQIBAAKBgQCZrCNLoEyAKF+f9UNcFaz5Osa6jf7qkbUl8si5xQrY3ZYC7juu\n    nL1dZLn/VbEFIITaUOgvBtPv1qUWTJGwga62VSG1oFE0ODIx3g2Nh4sRf+rySsx2\n    L4442nx0z4O5vJQ7k6eRNHAZUUnCL50+YvjyLyt7ryLSjSuKhCcJsbZgPwIDAQAB\n    AoGAB5evrr7eyL4160tM5rHTeATlaLY3UBOe5Z8XN8Z6gLiB/ucSX9AysviVD/6F\n    3oD6z2aL8jbeJc1vHqjt0dC2dwwm32vVl8mRdyoAsQpWmiqXrkvP4Bsl04VpBeHw\n    Qt8xNSW9SFhceL3LEvw9M8i9MV39viih1ILyH8OuHdvJyFECQQDLEjl2d2ppxND9\n    PoLqVFAirDfX2JnLTdWbc+M11a9Jdn3hKF8TcxfEnFVs5Gav1MusicY5KB0ylYPb\n    YbTvqKc7AkEAwbnRBO2VYEZsJZp2X0IZqP9ovWokkpYx+PE4+c6MySDgaMcigL7v\n    WDIHJG1CHudD09GbqENasDzyb2HAIW4CzQJBAKDdkv+xoW6gJx42Auc2WzTcUHCA\n    eXR/+BLpPrhKykzbvOQ8YvS5W764SUO1u1LWs3G+wnRMvrRvlMCZKgggBjkCQQCG\n    Jewto2+a+WkOKQXrNNScCDE5aPTmZQc5waCYq4UmCZQcOjkUOiN3ST1U5iuxRqfb\n    V/yX6fw0qh+fLWtkOs/JAkA+okMSxZwqRtfgOFGBfwQ8/iKrnizeanTQ3L6scFXI\n    CHZXdJ3XQ6qUmNxNn7iJ7S/LDawo1QfWkCfD9FYoxBlg\n    -----END RSA PRIVATE KEY-----\nThe server certificate in server_cert.pem looks similar:\n    -----BEGIN CERTIFICATE-----\n    MIIC+DCCAmGgAwIBAgIJAPMd+vi45js3MA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNV\n    BAYTAlVTMREwDwYDVQQIEwhJbGxpbm9pczEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIG\n    A1UEChMLRGFiZWF6LCBMTEMxEjAQBgNVBAMTCWxvY2FsaG9zdDAeFw0xMzAxMTEx\n    ODQyMjdaFw0xNDAxMTExODQyMjdaMFwxCzAJBgNVBAYTAlVTMREwDwYDVQQIEwhJ\n    bGxpbm9pczEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIGA1UEChMLRGFiZWF6LCBMTEMx\n    EjAQBgNVBAMTCWxvY2FsaG9zdDCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA\n    mawjS6BMgChfn/VDXBWs+TrGuo3+6pG1JfLIucUK2N2WAu47rpy9XWS5/1WxBSCE\n    2lDoLwbT79alFkyRsIGutlUhtaBRNDgyMd4NjYeLEX/q8krMdi+OONp8dM+DubyU\n11.10. Adding SSL to Network Services \n| \n469",
      "content_length": 2771,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": 11,
      "content": "O5OnkTRwGVFJwi+dPmL48i8re68i0o0rioQnCbG2YD8CAwEAAaOBwTCBvjAdBgNV\n    HQ4EFgQUrtoLHHgXiDZTr26NMmgKJLJLFtIwgY4GA1UdIwSBhjCBg4AUrtoLHHgX\n    iDZTr26NMmgKJLJLFtKhYKReMFwxCzAJBgNVBAYTAlVTMREwDwYDVQQIEwhJbGxp\n    bm9pczEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIGA1UEChMLRGFiZWF6LCBMTEMxEjAQ\n    BgNVBAMTCWxvY2FsaG9zdIIJAPMd+vi45js3MAwGA1UdEwQFMAMBAf8wDQYJKoZI\n    hvcNAQEFBQADgYEAFci+dqvMG4xF8UTnbGVvZJPIzJDRee6Nbt6AHQo9pOdAIMAu\n    WsGCplSOaDNdKKzl+b2UT2Zp3AIW4Qd51bouSNnR4M/gnr9ZD1ZctFd3jS+C5XRp\n    D3vvcW5lAnCCC80P6rXy7d7hTeFu5EYKtRGXNvVNd/06NALGDflrrOwxF3Y=\n    -----END CERTIFICATE-----\nIn server-related code, both the private key and certificate file will be presented to the\nvarious SSL-related wrapping functions. The certificate is what gets presented to clients.\nThe private key should be protected and remains on the server.\nIn client-related code, a special file of valid certificate authorities needs to be maintained\nto verify the server’s certificate. If you have no such file, then at the very least, you can\nput a copy of the server’s certificate on the client machine and use that as a means for\nverification. During connection, the server will present its certificate, and then you’ll\nuse the stored certificate you already have to verify that it’s correct.\nServers can also elect to verify the identity of clients. To do that, clients need to have\ntheir own private key and certificate key. The server would also need to maintain a file\nof trusted certificate authorities for verifying the client certificates.\nIf you intend to add SSL support to a network service for real, this recipe really only\ngives a small taste of how to set it up. You will definitely want to consult the documen‐\ntation for more of the finer points. Be prepared to spend a significant amount of time\nexperimenting with it to get things to work.\n11.11. Passing a Socket File Descriptor Between Processes\nProblem\nYou have multiple Python interpreter processes running and you want to pass an open\nfile descriptor from one interpreter to the other. For instance, perhaps there is a server\nprocess that is responsible for receiving connections, but the actual servicing of clients\nis to be handled by a different interpreter.\nSolution\nTo pass a file descriptor between processes, you first need to connect the processes\ntogether. On Unix machines, you might use a Unix domain socket, whereas on Win‐\ndows, you could use a named pipe. However, rather than deal with such low-level\nmechanics, it is often easier to use the multiprocessing module to set up such a\nconnection.\n470 \n| \nChapter 11: Network and Web Programming",
      "content_length": 2595,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": 11,
      "content": "Once a connection is established, you can use the send_handle() and recv_handle()\nfunctions in multiprocessing.reduction to send file descriptors between processes.\nThe following example illustrates the basics:\nimport multiprocessing\nfrom multiprocessing.reduction import recv_handle, send_handle\nimport socket\ndef worker(in_p, out_p):\n    out_p.close()\n    while True:\n        fd = recv_handle(in_p)\n        print('CHILD: GOT FD', fd)\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM, fileno=fd) as s:\n            while True:\n                msg = s.recv(1024)\n                if not msg:\n                    break\n                print('CHILD: RECV {!r}'.format(msg))\n                s.send(msg)\ndef server(address, in_p, out_p, worker_pid):\n    in_p.close()\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n    s.bind(address)\n    s.listen(1)\n    while True:\n        client, addr = s.accept()\n        print('SERVER: Got connection from', addr)\n        send_handle(out_p, client.fileno(), worker_pid)\n        client.close()\nif __name__ == '__main__':\n    c1, c2 = multiprocessing.Pipe()\n    worker_p = multiprocessing.Process(target=worker, args=(c1,c2))\n    worker_p.start()\n    server_p = multiprocessing.Process(target=server,\n                  args=(('', 15000), c1, c2, worker_p.pid))\n    server_p.start()\n    c1.close()\n    c2.close()\nIn this example, two processes are spawned and connected by a multiprocessing Pipe\nobject. The server process opens a socket and waits for client connections. The worker\nprocess merely waits to receive a file descriptor on the pipe using recv_handle(). When\nthe server receives a connection, it sends the resulting socket file descriptor to the worker\n11.11. Passing a Socket File Descriptor Between Processes \n| \n471",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": 11,
      "content": "using send_handle(). The worker takes over the socket and echoes data back to the\nclient until the connection is closed.\nIf you connect to the running server using Telnet or a similar tool, here is an example\nof what you might see:\n    bash % python3 passfd.py\n    SERVER: Got connection from ('127.0.0.1', 55543)\n    CHILD: GOT FD 7\n    CHILD: RECV b'Hello\\r\\n'\n    CHILD: RECV b'World\\r\\n'\nThe most important part of this example is the fact that the client socket accepted in the\nserver is actually serviced by a completely different process. The server merely hands it\noff, closes it, and waits for the next connection.\nDiscussion\nPassing file descriptors between processes is something that many programmers don’t\neven realize is possible. However, it can sometimes be a useful tool in building scalable\nsystems. For example, on a multicore machine, you could have multiple instances of the\nPython interpreter and use file descriptor passing to more evenly balance the number\nof clients being handled by each interpreter.\nThe send_handle() and recv_handle() functions shown in the solution really only\nwork with multiprocessing connections. Instead of using a pipe, you can connect in‐\nterpreters as shown in Recipe 11.7, and it will work as long as you use UNIX domain\nsockets or Windows pipes. For example, you could implement the server and worker\nas completely separate programs to be started separately. Here is the implementation of\nthe server:\n# servermp.py\nfrom multiprocessing.connection import Listener\nfrom multiprocessing.reduction import send_handle\nimport socket\ndef server(work_address, port):\n    # Wait for the worker to connect\n    work_serv = Listener(work_address, authkey=b'peekaboo')\n    worker = work_serv.accept()\n    worker_pid = worker.recv()\n    # Now run a TCP/IP server and send clients to worker\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n    s.bind(('', port))\n    s.listen(1)\n    while True:\n        client, addr = s.accept()\n        print('SERVER: Got connection from', addr)\n472 \n| \nChapter 11: Network and Web Programming",
      "content_length": 2139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": 11,
      "content": "send_handle(worker, client.fileno(), worker_pid)\n        client.close()\nif __name__ == '__main__':\n    import sys\n    if len(sys.argv) != 3:\n        print('Usage: server.py server_address port', file=sys.stderr)\n        raise SystemExit(1)\n    server(sys.argv[1], int(sys.argv[2]))\nTo run this server, you would run a command such as python3 servermp.py /tmp/\nservconn 15000. Here is the corresponding client code:\n# workermp.py\nfrom multiprocessing.connection import Client\nfrom multiprocessing.reduction import recv_handle\nimport os\nfrom socket import socket, AF_INET, SOCK_STREAM\ndef worker(server_address):\n    serv = Client(server_address, authkey=b'peekaboo')\n    serv.send(os.getpid())\n    while True:\n        fd = recv_handle(serv)\n        print('WORKER: GOT FD', fd)\n        with socket(AF_INET, SOCK_STREAM, fileno=fd) as client:\n            while True:\n                msg = client.recv(1024)\n                if not msg:\n                    break\n                print('WORKER: RECV {!r}'.format(msg))\n                client.send(msg)\nif __name__ == '__main__':\n    import sys\n    if len(sys.argv) != 2:\n        print('Usage: worker.py server_address', file=sys.stderr)\n        raise SystemExit(1)\n    worker(sys.argv[1])\nTo run the worker, you would type python3 workermp.py /tmp/servconn. The result‐\ning operation should be exactly the same as the example that used Pipe().\nUnder the covers, file descriptor passing involves creating a UNIX domain socket and\nthe sendmsg() method of sockets. Since this technique is not widely known, here is a\ndifferent implementation of the server that shows how to pass descriptors using sockets:\n# server.py\nimport socket\n11.11. Passing a Socket File Descriptor Between Processes \n| \n473",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": 11,
      "content": "import struct\ndef send_fd(sock, fd):\n    '''\n    Send a single file descriptor.\n    '''\n    sock.sendmsg([b'x'],\n                 [(socket.SOL_SOCKET, socket.SCM_RIGHTS, struct.pack('i', fd))])\n    ack = sock.recv(2)\n    assert ack == b'OK'\ndef server(work_address, port):\n    # Wait for the worker to connect\n    work_serv = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n    work_serv.bind(work_address)\n    work_serv.listen(1)\n    worker, addr = work_serv.accept()\n    # Now run a TCP/IP server and send clients to worker\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n    s.bind(('',port))\n    s.listen(1)\n    while True:\n        client, addr = s.accept()\n        print('SERVER: Got connection from', addr)\n        send_fd(worker, client.fileno())\n        client.close()\nif __name__ == '__main__':\n    import sys\n    if len(sys.argv) != 3:\n        print('Usage: server.py server_address port', file=sys.stderr)\n        raise SystemExit(1)\n    server(sys.argv[1], int(sys.argv[2]))\nHere is an implementation of the worker using sockets:\n# worker.py\nimport socket\nimport struct\ndef recv_fd(sock):\n    '''\n    Receive a single file descriptor\n    '''\n    msg, ancdata, flags, addr = sock.recvmsg(1,\n                                     socket.CMSG_LEN(struct.calcsize('i')))\n    cmsg_level, cmsg_type, cmsg_data = ancdata[0]\n    assert cmsg_level == socket.SOL_SOCKET and cmsg_type == socket.SCM_RIGHTS\n    sock.sendall(b'OK')\n474 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": 11,
      "content": "return struct.unpack('i', cmsg_data)[0]\ndef worker(server_address):\n    serv = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n    serv.connect(server_address)\n    while True:\n        fd = recv_fd(serv)\n        print('WORKER: GOT FD', fd)\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM, fileno=fd) as client:\n            while True:\n                msg = client.recv(1024)\n                if not msg:\n                    break\n                print('WORKER: RECV {!r}'.format(msg))\n                client.send(msg)\nif __name__ == '__main__':\n    import sys\n    if len(sys.argv) != 2:\n        print('Usage: worker.py server_address', file=sys.stderr)\n        raise SystemExit(1)\n    worker(sys.argv[1])\nIf you are going to use file-descriptor passing in your program, it is advisable to read\nmore about it in an advanced text, such as Unix Network Programming by W. Richard\nStevens (Prentice Hall, 1990). Passing file descriptors on Windows uses a different\ntechnique than Unix (not shown). For that platform, it is advisable to study the source\ncode to multiprocessing.reduction in close detail to see how it works.\n11.12. Understanding Event-Driven I/O\nProblem\nYou have heard about packages based on “event-driven” or “asynchronous” I/O, but\nyou’re not entirely sure what it means, how it actually works under the covers, or how\nit might impact your program if you use it.\nSolution\nAt a fundamental level, event-driven I/O is a technique that takes basic I/O operations\n(e.g., reads and writes) and converts them into events that must be handled by your\nprogram. For example, whenever data was received on a socket, it turns into a “receive”\nevent that is handled by some sort of callback method or function that you supply to\nrespond to it. As a possible starting point, an event-driven framework might start with\na base class that implements a series of basic event handler methods like this:\n11.12. Understanding Event-Driven I/O \n| \n475",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": 11,
      "content": "class EventHandler:\n    def fileno(self):\n        'Return the associated file descriptor'\n        raise NotImplemented('must implement')\n    def wants_to_receive(self):\n        'Return True if receiving is allowed'\n        return False\n    def handle_receive(self):\n        'Perform the receive operation'\n        pass\n    def wants_to_send(self):\n        'Return True if sending is requested'\n        return False\n    def handle_send(self):\n        'Send outgoing data'\n        pass\nInstances of this class then get plugged into an event loop that looks like this:\nimport select\ndef event_loop(handlers):\n    while True:\n        wants_recv = [h for h in handlers if h.wants_to_receive()]\n        wants_send = [h for h in handlers if h.wants_to_send()]\n        can_recv, can_send, _ = select.select(wants_recv, wants_send, [])\n        for h in can_recv:\n            h.handle_receive()\n        for h in can_send:\n            h.handle_send()\nThat’s it! The key to the event loop is the select() call, which polls file descriptors for\nactivity. Prior to calling select(), the event loop simply queries all of the handlers to\nsee which ones want to receive or send. It then supplies the resulting lists to select().\nAs a result, select() returns the list of objects that are ready to receive or send. The\ncorresponding handle_receive() or handle_send() methods are triggered.\nTo write applications, specific instances of EventHandler classes are created. For ex‐\nample, here are two simple handlers that illustrate two UDP-based network services:\nimport socket\nimport time\nclass UDPServer(EventHandler):\n    def __init__(self, address):\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self.sock.bind(address)\n476 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1781,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": 11,
      "content": "def fileno(self):\n        return self.sock.fileno()\n    def wants_to_receive(self):\n        return True\nclass UDPTimeServer(UDPServer):\n    def handle_receive(self):\n        msg, addr = self.sock.recvfrom(1)\n        self.sock.sendto(time.ctime().encode('ascii'), addr)\nclass UDPEchoServer(UDPServer):\n    def handle_receive(self):\n        msg, addr = self.sock.recvfrom(8192)\n        self.sock.sendto(msg, addr)\nif __name__ == '__main__':\n    handlers = [ UDPTimeServer(('',14000)), UDPEchoServer(('',15000))  ]\n    event_loop(handlers)\nTo test this code, you can try connecting to it from another Python interpreter:\n>>> from socket import *\n>>> s = socket(AF_INET, SOCK_DGRAM)\n>>> s.sendto(b'',('localhost',14000))\n0\n>>> s.recvfrom(128)\n(b'Tue Sep 18 14:29:23 2012', ('127.0.0.1', 14000))\n>>> s.sendto(b'Hello',('localhost',15000))\n5\n>>> s.recvfrom(128)\n(b'Hello', ('127.0.0.1', 15000))\n>>>\nImplementing a TCP server is somewhat more complex, since each client involves the\ninstantiation of a new handler object. Here is an example of a TCP echo client.\nclass TCPServer(EventHandler):\n    def __init__(self, address, client_handler, handler_list):\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n        self.sock.bind(address)\n        self.sock.listen(1)\n        self.client_handler = client_handler\n        self.handler_list = handler_list\n    def fileno(self):\n        return self.sock.fileno()\n    def wants_to_receive(self):\n        return True\n11.12. Understanding Event-Driven I/O \n| \n477",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": 11,
      "content": "def handle_receive(self):\n        client, addr = self.sock.accept()\n        # Add the client to the event loop's handler list\n        self.handler_list.append(self.client_handler(client, self.handler_list))\nclass TCPClient(EventHandler):\n    def __init__(self, sock, handler_list):\n        self.sock = sock\n        self.handler_list = handler_list\n        self.outgoing = bytearray()\n    def fileno(self):\n        return self.sock.fileno()\n    def close(self):\n        self.sock.close()\n        # Remove myself from the event loop's handler list\n        self.handler_list.remove(self)\n    def wants_to_send(self):\n        return True if self.outgoing else False\n    def handle_send(self):\n        nsent = self.sock.send(self.outgoing)\n        self.outgoing = self.outgoing[nsent:]\nclass TCPEchoClient(TCPClient):\n    def wants_to_receive(self):\n        return True\n    def handle_receive(self):\n        data = self.sock.recv(8192)\n        if not data:\n            self.close()\n        else:\n            self.outgoing.extend(data)\nif __name__ == '__main__':\n   handlers = []\n   handlers.append(TCPServer(('',16000), TCPEchoClient, handlers))\n   event_loop(handlers)\nThe key to the TCP example is the addition and removal of clients from the handler list.\nOn each connection, a new handler is created for the client and added to the list. When\nthe connection is closed, each client must take care to remove themselves from the list.\nIf you run this program and try connecting with Telnet or some similar tool, you’ll see\nit echoing received data back to you. It should easily handle multiple clients.\n478 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": 11,
      "content": "Discussion\nVirtually all event-driven frameworks operate in a manner that is similar to that shown\nin the solution. The actual implementation details and overall software architecture\nmight vary greatly, but at the core, there is a polling loop that checks sockets for activity\nand which performs operations in response.\nOne potential benefit of event-driven I/O is that it can handle a very large number of\nsimultaneous connections without ever using threads or processes. That is, the se\nlect() call (or equivalent) can be used to monitor hundreds or thousands of sockets\nand respond to events occuring on any of them. Events are handled one at a time by the\nevent loop, without the need for any other concurrency primitives.\nThe downside to event-driven I/O is that there is no true concurrency involved. If any\nof the event handler methods blocks or performs a long-running calculation, it blocks\nthe progress of everything. There is also the problem of calling out to library functions\nthat aren’t written in an event-driven style. There is always the risk that some library\ncall will block, causing the event loop to stall.\nProblems with blocking or long-running calculations can be solved by sending the work\nout to a separate thread or process. However, coordinating threads and processes with\nan event loop is tricky. Here is an example of code that will do it using the concur\nrent.futures module:\nfrom concurrent.futures import ThreadPoolExecutor\nimport os\nclass ThreadPoolHandler(EventHandler):\n    def __init__(self, nworkers):\n        if os.name == 'posix':\n            self.signal_done_sock, self.done_sock = socket.socketpair()\n        else:\n            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            server.bind(('127.0.0.1', 0))\n            server.listen(1)\n            self.signal_done_sock = socket.socket(socket.AF_INET,\n                                                  socket.SOCK_STREAM)\n            self.signal_done_sock.connect(server.getsockname())\n            self.done_sock, _ = server.accept()\n            server.close()\n        self.pending = []\n        self.pool = ThreadPoolExecutor(nworkers)\n    def fileno(self):\n        return self.done_sock.fileno()\n    # Callback that executes when the thread is done\n    def _complete(self, callback, r):\n11.12. Understanding Event-Driven I/O \n| \n479",
      "content_length": 2347,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": 11,
      "content": "self.pending.append((callback, r.result()))\n        self.signal_done_sock.send(b'x')\n    # Run a function in a thread pool\n    def run(self, func, args=(), kwargs={},*,callback):\n        r = self.pool.submit(func, *args, **kwargs)\n        r.add_done_callback(lambda r: self._complete(callback, r))\n    def wants_to_receive(self):\n        return True\n    # Run callback functions of completed work\n    def handle_receive(self):\n        # Invoke all pending callback functions\n        for callback, result in self.pending:\n            callback(result)\n            self.done_sock.recv(1)\n        self.pending = []\nIn this code, the run() method is used to submit work to the pool along with a callback\nfunction that should be triggered upon completion. The actual work is then submitted\nto a ThreadPoolExecutor instance. However, a really tricky problem concerns the co‐\nordination of the computed result and the event loop. To do this, a pair of sockets are\ncreated under the covers and used as a kind of signaling mechanism. When work is\ncompleted by the thread pool, it executes the _complete() method in the class. This\nmethod queues up the pending callback and result before writing a byte of data on one\nof these sockets. The fileno() method is programmed to return the other socket. Thus,\nwhen this byte is written, it will signal to the event loop that something has happened.\nThe handle_receive() method, when triggered, will then execute all of the callback\nfunctions for previously submitted work. Frankly, it’s enough to make one’s head spin.\nHere is a simple server that shows how to use the thread pool to carry out a long-running\ncalculation:\n# A really bad Fibonacci implementation\ndef fib(n):\n    if n < 2:\n        return 1\n    else:\n        return fib(n - 1) + fib(n - 2)\nclass UDPFibServer(UDPServer):\n    def handle_receive(self):\n        msg, addr = self.sock.recvfrom(128)\n        n = int(msg)\n        pool.run(fib, (n,), callback=lambda r: self.respond(r, addr))\n    def respond(self, result, addr):\n        self.sock.sendto(str(result).encode('ascii'), addr)\n480 \n| \nChapter 11: Network and Web Programming",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": 11,
      "content": "if __name__ == '__main__':\n    pool = ThreadPoolHandler(16)\n    handlers = [ pool, UDPFibServer(('',16000))]\n    event_loop(handlers)\nTo try this server, simply run it and try some experiments with another Python program:\nfrom socket import *\nsock = socket(AF_INET, SOCK_DGRAM)\nfor x in range(40):\n    sock.sendto(str(x).encode('ascii'), ('localhost', 16000))\n    resp = sock.recvfrom(8192)\n    print(resp[0])\nYou should be able to run this program repeatedly from many different windows and\nhave it operate without stalling other programs, even though it gets slower and slower\nas the numbers get larger.\nHaving gone through this recipe, should you use its code? Probably not. Instead, you\nshould look for a more fully developed framework that accomplishes the same task.\nHowever, if you understand the basic concepts presented here, you’ll understand the\ncore techniques used to make such frameworks operate. As an alternative to callback-\nbased programming, event-driven code will sometimes use coroutines. See Recipe 12.12\nfor an example.\n11.13. Sending and Receiving Large Arrays\nProblem\nYou want to send and receive large arrays of contiguous data across a network connec‐\ntion, making as few copies of the data as possible.\nSolution\nThe following functions utilize memoryviews to send and receive large arrays:\n# zerocopy.py\ndef send_from(arr, dest):\n    view = memoryview(arr).cast('B')\n    while len(view):\n        nsent = dest.send(view)\n        view = view[nsent:]\ndef recv_into(arr, source):\n    view = memoryview(arr).cast('B')\n    while len(view):\n        nrecv = source.recv_into(view)\n        view = view[nrecv:]\n11.13. Sending and Receiving Large Arrays \n| \n481",
      "content_length": 1678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": 11,
      "content": "To test the program, first create a server and client program connected over a socket.\nIn the server:\n>>> from socket import *\n>>> s = socket(AF_INET, SOCK_STREAM)\n>>> s.bind(('', 25000))\n>>> s.listen(1)\n>>> c,a = s.accept()\n>>>\nIn the client (in a separate interpreter):\n>>> from socket import *\n>>> c = socket(AF_INET, SOCK_STREAM)\n>>> c.connect(('localhost', 25000))\n>>>\nNow, the whole idea of this recipe is that you can blast a huge array through the con‐\nnection. In this case, arrays might be created by the array module or perhaps numpy.\nFor example:\n# Server\n>>> import numpy\n>>> a = numpy.arange(0.0, 50000000.0)\n>>> send_from(a, c)\n>>>\n# Client\n>>> import numpy\n>>> a = numpy.zeros(shape=50000000, dtype=float)\n>>> a[0:10]\narray([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n>>> recv_into(a, c)\n>>> a[0:10]\narray([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\n>>>\nDiscussion\nIn data-intensive distributed computing and parallel programming applications, it’s not\nuncommon to write programs that need to send/receive large chunks of data. However,\nto do this, you somehow need to reduce the data down to raw bytes for use with low-\nlevel network functions. You may also need to slice the data into chunks, since most\nnetwork-related functions aren’t able to send or receive huge blocks of data entirely all\nat once.\nOne approach is to serialize the data in some way—possibly by converting into a byte\nstring. However, this usually ends up making a copy of the data. Even if you do this\npiecemeal, your code still ends up making a lot of little copies.\n482 \n| \nChapter 11: Network and Web Programming",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": 11,
      "content": "This recipe gets around this by playing a sneaky trick with memoryviews. Essentially, a\nmemoryview is an overlay of an existing array. Not only that, memoryviews can be cast\nto different types to allow interpretation of the data in a different manner. This is the\npurpose of the following statement:\nview = memoryview(arr).cast('B')\nIt takes an array arr and casts into a memoryview of unsigned bytes.\nIn this form, the view can be passed to socket-related functions, such as sock.send()\nor send.recv_into(). Under the covers, those methods are able to work directly with\nthe memory region. For example, sock.send() sends data directly from memory\nwithout a copy. send.recv_into() uses the memoryview as the input buffer for the\nreceive operation.\nThe remaining complication is the fact that the socket functions may only work with\npartial data. In general, it will take many different send() and recv_into() calls to\ntransmit the entire array. Not to worry. After each operation, the view is sliced by the\nnumber of sent or received bytes to produce a new view. The new view is also a memory\noverlay. Thus, no copies are made.\nOne issue here is that the receiver has to know in advance how much data will be sent\nso that it can either preallocate an array or verify that it can receive the data into an\nexisting array. If this is a problem, the sender could always arrange to send the size first,\nfollowed by the array data.\n11.13. Sending and Receiving Large Arrays \n| \n483",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": 11,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 503,
      "chapter": 11,
      "content": "CHAPTER 12\nConcurrency\nPython has long supported different approaches to concurrent programming, including\nprogramming with threads, launching subprocesses, and various tricks involving gen‐\nerator functions. In this chapter, recipes related to various aspects of concurrent pro‐\ngramming are presented, including common thread programming techniques and ap‐\nproaches for parallel processing.\nAs experienced programmers know, concurrent programming is fraught with potential\nperil. Thus, a major focus of this chapter is on recipes that tend to lead to more reliable\nand debuggable code.\n12.1. Starting and Stopping Threads\nProblem\nYou want to create and destroy threads for concurrent execution of code.\nSolution\nThe threading library can be used to execute any Python callable in its own thread. To\ndo this, you create a Thread instance and supply the callable that you wish to execute\nas a target. Here is a simple example:\n# Code to execute in an independent thread\nimport time\ndef countdown(n):\n    while n > 0:\n        print('T-minus', n)\n        n -= 1\n        time.sleep(5)\n485",
      "content_length": 1085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": 11,
      "content": "# Create and launch a thread\nfrom threading import Thread\nt = Thread(target=countdown, args=(10,))\nt.start()\nWhen you create a thread instance, it doesn’t start executing until you invoke its start()\nmethod (which invokes the target function with the arguments you supplied).\nThreads are executed in their own system-level thread (e.g., a POSIX thread or Windows\nthreads) that is fully managed by the host operating system. Once started, threads run\nindependently until the target function returns. You can query a thread instance to see\nif it’s still running:\nif t.is_alive():\n    print('Still running')\nelse:\n    print('Completed')\nYou can also request to join with a thread, which waits for it to terminate:\n    t.join()\nThe interpreter remains running until all threads terminate. For long-running threads\nor background tasks that run forever, you should consider making the thread daemonic.\nFor example:\nt = Thread(target=countdown, args=(10,), daemon=True)\nt.start()\nDaemonic threads can’t be joined. However, they are destroyed automatically when the\nmain thread terminates.\nBeyond the two operations shown, there aren’t many other things you can do with\nthreads. For example, there are no operations to terminate a thread, signal a thread,\nadjust its scheduling, or perform any other high-level operations. If you want these\nfeatures, you need to build them yourself.\nIf you want to be able to terminate threads, the thread must be programmed to poll for\nexit at selected points. For example, you might put your thread in a class such as this:\nclass CountdownTask:\n    def __init__(self):\n        self._running = True\n    def terminate(self):\n        self._running = False\n    def run(self, n):\n        while self._running and n > 0:\n            print('T-minus', n)\n            n -= 1\n            time.sleep(5)\n486 \n| \nChapter 12: Concurrency",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": 11,
      "content": "c = CountdownTask()\nt = Thread(target=c.run, args=(10,))\nt.start()\n...\nc.terminate() # Signal termination\nt.join()      # Wait for actual termination (if needed)\nPolling for thread termination can be tricky to coordinate if threads perform blocking\noperations such as I/O. For example, a thread blocked indefinitely on an I/O operation\nmay never return to check if it’s been killed. To correctly deal with this case, you’ll need\nto carefully program thread to utilize timeout loops. For example:\nclass IOTask:\n    def terminate(self):\n        self._running = False\n    def run(self, sock):\n        # sock is a socket\n        sock.settimeout(5)        # Set timeout period\n        while self._running:\n            # Perform a blocking I/O operation w/ timeout\n            try:\n                data = sock.recv(8192)\n                break\n            except socket.timeout:\n                continue\n            # Continued processing\n            ...\n        # Terminated\n        return\nDiscussion\nDue to a global interpreter lock (GIL), Python threads are restricted to an execution\nmodel that only allows one thread to execute in the interpreter at any given time. For\nthis reason, Python threads should generally not be used for computationally intensive\ntasks where you are trying to achieve parallelism on multiple CPUs. They are much\nbetter suited for I/O handling and handling concurrent execution in code that performs\nblocking operations (e.g., waiting for I/O, waiting for results from a database, etc.).\nSometimes you will see threads defined via inheritance from the Thread class. For\nexample:\nfrom threading import Thread\nclass CountdownThread(Thread):\n    def __init__(self, n):\n        super().__init__()\n        self.n = 0\n    def run(self):\n        while self.n > 0:\n12.1. Starting and Stopping Threads \n| \n487",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": 11,
      "content": "print('T-minus', self.n)\n            self.n -= 1\n            time.sleep(5)\nc = CountdownThread(5)\nc.start()\nAlthough this works, it introduces an extra dependency between the code and the \nthreading library. That is, you can only use the resulting code in the context of threads,\nwhereas the technique shown earlier involves writing code with no explicit dependency\non threading. By freeing your code of such dependencies, it becomes usable in other\ncontexts that may or may not involve threads. For instance, you might be able to execute\nyour code in a separate process using the multiprocessing module using code like this:\nimport multiprocessing\nc = CountdownTask(5)\np = multiprocessing.Process(target=c.run)\np.start()\n...\nAgain, this only works if the CountdownTask class has been written in a manner that is\nneutral to the actual means of concurrency (threads, processes, etc.).\n12.2. Determining If a Thread Has Started\nProblem\nYou’ve launched a thread, but want to know when it actually starts running.\nSolution\nA key feature of threads is that they execute independently and nondeterministically.\nThis can present a tricky synchronization problem if other threads in the program need\nto know if a thread has reached a certain point in its execution before carrying out\nfurther operations. To solve such problems, use the Event object from the threading\nlibrary.\nEvent instances are similar to a “sticky” flag that allows threads to wait for something\nto happen. Initially, an event is set to 0. If the event is unset and a thread waits on the\nevent, it will block (i.e., go to sleep) until the event gets set. A thread that sets the event\nwill wake up all of the threads that happen to be waiting (if any). If a thread waits on an\nevent that has already been set, it merely moves on, continuing to execute.\nHere is some sample code that uses an Event to coordinate the startup of a thread:\nfrom threading import Thread, Event\nimport time\n488 \n| \nChapter 12: Concurrency",
      "content_length": 1977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": 11,
      "content": "# Code to execute in an independent thread\ndef countdown(n, started_evt):\n    print('countdown starting')\n    started_evt.set()\n    while n > 0:\n        print('T-minus', n)\n        n -= 1\n        time.sleep(5)\n# Create the event object that will be used to signal startup\nstarted_evt = Event()\n# Launch the thread and pass the startup event\nprint('Launching countdown')\nt = Thread(target=countdown, args=(10,started_evt))\nt.start()\n# Wait for the thread to start\nstarted_evt.wait()\nprint('countdown is running')\nWhen you run this code, the “countdown is running” message will always appear after\nthe “countdown starting” message. This is coordinated by the event that makes the main\nthread wait until the countdown() function has first printed the startup message.\nDiscussion\nEvent objects are best used for one-time events. That is, you create an event, threads\nwait for the event to be set, and once set, the Event is discarded. Although it is possible\nto clear an event using its clear() method, safely clearing an event and waiting for it\nto be set again is tricky to coordinate, and can lead to missed events, deadlock, or other\nproblems (in particular, you can’t guarantee that a request to clear an event after setting\nit will execute before a released thread cycles back to wait on the event again).\nIf a thread is going to repeatedly signal an event over and over, you’re probably better\noff using a Condition object instead. For example, this code implements a periodic timer\nthat other threads can monitor to see whenever the timer expires:\nimport threading\nimport time\nclass PeriodicTimer:\n    def __init__(self, interval):\n        self._interval = interval\n        self._flag = 0\n        self._cv = threading.Condition()\n    def start(self):\n        t = threading.Thread(target=self.run)\n        t.daemon = True\n12.2. Determining If a Thread Has Started \n| \n489",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": 11,
      "content": "t.start()\n    def run(self):\n        '''\n        Run the timer and notify waiting threads after each interval\n        '''\n        while True:\n            time.sleep(self._interval)\n            with self._cv:\n                 self._flag ^= 1\n                 self._cv.notify_all()\n    def wait_for_tick(self):\n        '''\n        Wait for the next tick of the timer\n        '''\n        with self._cv:\n            last_flag = self._flag\n            while last_flag == self._flag:\n                self._cv.wait()\n# Example use of the timer\nptimer = PeriodicTimer(5)\nptimer.start()\n# Two threads that synchronize on the timer\ndef countdown(nticks):\n    while nticks > 0:\n        ptimer.wait_for_tick()\n        print('T-minus', nticks)\n        nticks -= 1\ndef countup(last):\n    n = 0\n    while n < last:\n        ptimer.wait_for_tick()\n        print('Counting', n)\n        n += 1\nthreading.Thread(target=countdown, args=(10,)).start()\nthreading.Thread(target=countup, args=(5,)).start()\nA critical feature of Event objects is that they wake all waiting threads. If you are writing\na program where you only want to wake up a single waiting thread, it is probably better\nto use a Semaphore or Condition object instead.\nFor example, consider this code involving semaphores:\n# Worker thread\ndef worker(n, sema):\n    # Wait to be signaled\n    sema.acquire()\n490 \n| \nChapter 12: Concurrency",
      "content_length": 1379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": 11,
      "content": "# Do some work\n    print('Working', n)\n# Create some threads\nsema = threading.Semaphore(0)\nnworkers = 10\nfor n in range(nworkers):\n    t = threading.Thread(target=worker, args=(n, sema,))\n    t.start()\nIf you run this, a pool of threads will start, but nothing happens because they’re all\nblocked waiting to acquire the semaphore. Each time the semaphore is released, only\none worker will wake up and run. For example:\n>>> sema.release()\nWorking 0\n>>> sema.release()\nWorking 1\n>>>\nWriting code that involves a lot of tricky synchronization between threads is likely to\nmake your head explode. A more sane approach is to thread threads as communicating\ntasks using queues or as actors. Queues are described in the next recipe. Actors are\ndescribed in Recipe 12.10.\n12.3. Communicating Between Threads\nProblem\nYou have multiple threads in your program and you want to safely communicate or\nexchange data between them.\nSolution\nPerhaps the safest way to send data from one thread to another is to use a Queue from\nthe queue library. To do this, you create a Queue instance that is shared by the threads.\nThreads then use put() or get() operations to add or remove items from the queue.\nFor example:\nfrom queue import Queue\nfrom threading import Thread\n# A thread that produces data\ndef producer(out_q):\n    while True:\n        # Produce some data\n        ...\n        out_q.put(data)\n12.3. Communicating Between Threads \n| \n491",
      "content_length": 1423,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": 11,
      "content": "# A thread that consumes data\ndef consumer(in_q):\n    while True:\n        # Get some data\n        data = in_q.get()\n        # Process the data\n        ...\n# Create the shared queue and launch both threads\nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(target=producer, args=(q,))\nt1.start()\nt2.start()\nQueue instances already have all of the required locking, so they can be safely shared by\nas many threads as you wish.\nWhen using queues, it can be somewhat tricky to coordinate the shutdown of the pro‐\nducer and consumer. A common solution to this problem is to rely on a special sentinel\nvalue, which when placed in the queue, causes consumers to terminate. For example:\nfrom queue import Queue\nfrom threading import Thread\n# Object that signals shutdown\n_sentinel = object()\n# A thread that produces data\ndef producer(out_q):\n    while running:\n        # Produce some data\n        ...\n        out_q.put(data)\n    # Put the sentinel on the queue to indicate completion\n    out_q.put(_sentinel)\n# A thread that consumes data\ndef consumer(in_q):\n    while True:\n        # Get some data\n        data = in_q.get()\n        # Check for termination\n        if data is _sentinel:\n            in_q.put(_sentinel)\n            break\n        # Process the data\n        ...\n492 \n| \nChapter 12: Concurrency",
      "content_length": 1313,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": 11,
      "content": "A subtle feature of this example is that the consumer, upon receiving the special sentinel\nvalue, immediately places it back onto the queue. This propagates the sentinel to other\nconsumers threads that might be listening on the same queue—thus shutting them all\ndown one after the other.\nAlthough queues are the most common thread communication mechanism, you can\nbuild your own data structures as long as you add the required locking and synchroni‐\nzation. The most common way to do this is to wrap your data structures with a condition\nvariable. For example, here is how you might build a thread-safe priority queue, as\ndiscussed in Recipe 1.5.\nimport heapq\nimport threading\nclass PriorityQueue:\n    def __init__(self):\n        self._queue = []\n        self._count = 0\n        self._cv = threading.Condition()\n    def put(self, item, priority):\n        with self._cv:\n            heapq.heappush(self._queue, (-priority, self._count, item))\n            self._count += 1\n            self._cv.notify()\n    def get(self):\n        with self._cv:\n            while len(self._queue) == 0:\n                self._cv.wait()\n            return heapq.heappop(self._queue)[-1]\nThread communication with a queue is a one-way and nondeterministic process. In\ngeneral, there is no way to know when the receiving thread has actually received a\nmessage and worked on it. However, Queue objects do provide some basic completion\nfeatures, as illustrated by the task_done() and join() methods in this example:\nfrom queue import Queue\nfrom threading import Thread\n# A thread that produces data\ndef producer(out_q):\n    while running:\n        # Produce some data\n        ...\n        out_q.put(data)\n# A thread that consumes data\ndef consumer(in_q):\n    while True:\n        # Get some data\n        data = in_q.get()\n12.3. Communicating Between Threads \n| \n493",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": 11,
      "content": "# Process the data\n        ...\n        # Indicate completion\n        in_q.task_done()\n# Create the shared queue and launch both threads\nq = Queue()\nt1 = Thread(target=consumer, args=(q,))\nt2 = Thread(target=producer, args=(q,))\nt1.start()\nt2.start()\n# Wait for all produced items to be consumed\nq.join()\nIf a thread needs to know immediately when a consumer thread has processed a par‐\nticular item of data, you should pair the sent data with an Event object that allows the\nproducer to monitor its progress. For example:\nfrom queue import Queue\nfrom threading import Thread, Event\n# A thread that produces data\ndef producer(out_q):\n    while running:\n        # Produce some data\n        ...\n        # Make an (data, event) pair and hand it to the consumer\n        evt = Event()\n        out_q.put((data, evt))\n        ...\n        # Wait for the consumer to process the item\n        evt.wait()\n# A thread that consumes data\ndef consumer(in_q):\n    while True:\n        # Get some data\n        data, evt = in_q.get()\n        # Process the data\n        ...\n        # Indicate completion\n        evt.set()\nDiscussion\nWriting threaded programs based on simple queuing is often a good way to maintain\nsanity. If you can break everything down to simple thread-safe queuing, you’ll find that\nyou don’t need to litter your program with locks and other low-level synchronization.\nAlso, communicating with queues often leads to designs that can be scaled up to other\nkinds of message-based communication patterns later on. For instance, you might be\n494 \n| \nChapter 12: Concurrency",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": 11,
      "content": "able to split your program into multiple processes, or even a distributed system, without\nchanging much of its underlying queuing architecture.\nOne caution with thread queues is that putting an item in a queue doesn’t make a copy\nof the item. Thus, communication actually involves passing an object reference between\nthreads. If you are concerned about shared state, it may make sense to only pass im‐\nmutable data structures (e.g., integers, strings, or tuples) or to make deep copies of the\nqueued items. For example:\nfrom queue import Queue\nfrom threading import Thread\nimport copy\n# A thread that produces data\ndef producer(out_q):\n    while True:\n        # Produce some data\n        ...\n        out_q.put(copy.deepcopy(data))\n# A thread that consumes data\ndef consumer(in_q):\n    while True:\n        # Get some data\n        data = in_q.get()\n        # Process the data\n        ...\nQueue objects provide a few additional features that may prove to be useful in certain\ncontexts. If you create a Queue with an optional size, such as Queue(N), it places a limit\non the number of items that can be enqueued before the put() blocks the producer.\nAdding an upper bound to a queue might make sense if there is mismatch in speed\nbetween a producer and consumer. For instance, if a producer is generating items at a\nmuch faster rate than they can be consumed. On the other hand, making a queue block\nwhen it’s full can also have an unintended cascading effect throughout your program,\npossibly causing it to deadlock or run poorly. In general, the problem of “flow control”\nbetween communicating threads is a much harder problem than it seems. If you ever\nfind yourself trying to fix a problem by fiddling with queue sizes, it could be an indicator\nof a fragile design or some other inherent scaling problem.\nBoth the get() and put() methods support nonblocking and timeouts. For example:\nimport queue\nq = queue.Queue()\ntry:\n    data = q.get(block=False)\nexcept queue.Empty:\n    ...\n12.3. Communicating Between Threads \n| \n495",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": 11,
      "content": "try:\n    q.put(item, block=False)\nexcept queue.Full:\n    ...\ntry:\n    data = q.get(timeout=5.0)\nexcept queue.Empty:\n    ...\nBoth of these options can be used to avoid the problem of just blocking indefinitely on\na particular queuing operation. For example, a nonblocking put() could be used with\na fixed-sized queue to implement different kinds of handling code for when a queue is\nfull. For example, issuing a log message and discarding:\ndef producer(q):\n    ...\n    try:\n        q.put(item, block=False)\n    except queue.Full:\n        log.warning('queued item %r discarded!', item)\nA timeout is useful if you’re trying to make consumer threads periodically give up on\noperations such as q.get() so that they can check things such as a termination flag, as\ndescribed in Recipe 12.1.\n_running = True\ndef consumer(q):\n    while _running:\n        try:\n            item = q.get(timeout=5.0)\n            # Process item\n            ...\n        except queue.Empty:\n            pass\nLastly, there are utility methods q.qsize(), q.full(), q.empty() that can tell you the\ncurrent size and status of the queue. However, be aware that all of these are unreliable\nin a multithreaded environment. For example, a call to q.empty() might tell you that\nthe queue is empty, but in the time that has elapsed since making the call, another thread\ncould have added an item to the queue. Frankly, it’s best to write your code not to rely\non such functions.\n496 \n| \nChapter 12: Concurrency",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": 11,
      "content": "12.4. Locking Critical Sections\nProblem\nYour program uses threads and you want to lock critical sections of code to avoid race\nconditions.\nSolution\nTo make mutable objects safe to use by multiple threads, use Lock objects in the thread\ning library, as shown here:\nimport threading\nclass SharedCounter:\n    '''\n    A counter object that can be shared by multiple threads.\n    '''\n    def __init__(self, initial_value = 0):\n        self._value = initial_value\n        self._value_lock = threading.Lock()\n    def incr(self,delta=1):\n        '''\n        Increment the counter with locking\n        '''\n        with self._value_lock:\n             self._value += delta\n    def decr(self,delta=1):\n        '''\n        Decrement the counter with locking\n        '''\n        with self._value_lock:\n             self._value -= delta\nA Lock guarantees mutual exclusion when used with the with statement—that is, only\none thread is allowed to execute the block of statements under the with statement at a\ntime. The with statement acquires the lock for the duration of the indented statements\nand releases the lock when control flow exits the indented block.\nDiscussion\nThread scheduling is inherently nondeterministic. Because of this, failure to use locks\nin threaded programs can result in randomly corrupted data and bizarre behavior\nknown as a “race condition.” To avoid this, locks should always be used whenever shared\nmutable state is accessed by multiple threads.\n12.4. Locking Critical Sections \n| \n497",
      "content_length": 1498,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": 11,
      "content": "In older Python code, it is common to see locks explicitly acquired and released. For\nexample, in this variant of the last example:\nimport threading\nclass SharedCounter:\n    '''\n    A counter object that can be shared by multiple threads.\n    '''\n    def __init__(self, initial_value = 0):\n        self._value = initial_value\n        self._value_lock = threading.Lock()\n    def incr(self,delta=1):\n        '''\n        Increment the counter with locking\n        '''\n        self._value_lock.acquire()\n        self._value += delta\n        self._value_lock.release()\n    def decr(self,delta=1):\n        '''\n        Decrement the counter with locking\n        '''\n        self._value_lock.acquire()\n        self._value -= delta\n        self._value_lock.release()\nThe with statement is more elegant and less prone to error—especially in situations\nwhere a programmer might forget to call the release() method or if a program happens\nto raise an exception while holding a lock (the with statement guarantees that locks are\nalways released in both cases).\nTo avoid the potential for deadlock, programs that use locks should be written in a way\nsuch that each thread is only allowed to acquire one lock at a time. If this is not possible,\nyou may need to introduce more advanced deadlock avoidance into your program, as\ndescribed in Recipe 12.5.\nIn the threading library, you’ll find other synchronization primitives, such as RLock\nand Semaphore objects. As a general rule of thumb, these are more special purpose and\nshould not be used for simple locking of mutable state. An RLock or re-entrant lock\nobject is a lock that can be acquired multiple times by the same thread. It is primarily\nused to implement code based locking or synchronization based on a construct known\nas a “monitor.” With this kind of locking, only one thread is allowed to use an entire\nfunction or the methods of a class while the lock is held. For example, you could im‐\nplement the SharedCounter class like this:\n498 \n| \nChapter 12: Concurrency",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": 11,
      "content": "import threading\nclass SharedCounter:\n    '''\n    A counter object that can be shared by multiple threads.\n    '''\n    _lock = threading.RLock()\n    def __init__(self, initial_value = 0):\n        self._value = initial_value\n    def incr(self,delta=1):\n        '''\n        Increment the counter with locking\n        '''\n        with SharedCounter._lock:\n            self._value += delta\n    def decr(self,delta=1):\n        '''\n        Decrement the counter with locking\n        '''\n        with SharedCounter._lock:\n             self.incr(-delta)\nIn this variant of the code, there is just a single class-level lock shared by all instances\nof the class. Instead of the lock being tied to the per-instance mutable state, the lock is\nmeant to synchronize the methods of the class. Specifically, this lock ensures that only\none thread is allowed to be using the methods of the class at once. However, unlike a\nstandard lock, it is OK for methods that already have the lock to call other methods that\nalso use the lock (e.g., see the decr() method).\nOne feature of this implementation is that only one lock is created, regardless of how\nmany counter instances are created. Thus, it is much more memory-efficient in situa‐\ntions where there are a large number of counters. However, a possible downside is that\nit may cause more lock contention in programs that use a large number of threads and\nmake frequent counter updates.\nA Semaphore object is a synchronization primitive based on a shared counter. If the\ncounter is nonzero, the with statement decrements the count and a thread is allowed to\nproceed. The counter is incremented upon the conclusion of the with block. If the\ncounter is zero, progress is blocked until the counter is incremented by another thread.\nAlthough a semaphore can be used in the same manner as a standard Lock, the added\ncomplexity in implementation negatively impacts performance. Instead of simple lock‐\ning, Semaphore objects are more useful for applications involving signaling between\nthreads or throttling. For example, if you want to limit the amount of concurrency in a\npart of code, you might use a semaphore, as follows:\nfrom threading import Semaphore\nimport urllib.request\n12.4. Locking Critical Sections \n| \n499",
      "content_length": 2247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": 11,
      "content": "# At most, five threads allowed to run at once\n_fetch_url_sema = Semaphore(5)\ndef fetch_url(url):\n    with _fetch_url_sema:\n        return urllib.request.urlopen(url)\nIf you’re interested in the underlying theory and implementation of thread synchroni‐\nzation primitives, consult almost any textbook on operating systems.\n12.5. Locking with Deadlock Avoidance\nProblem\nYou’re writing a multithreaded program where threads need to acquire more than one\nlock at a time while avoiding deadlock.\nSolution\nIn multithreaded programs, a common source of deadlock is due to threads that attempt\nto acquire multiple locks at once. For instance, if a thread acquires the first lock, but\nthen blocks trying to acquire the second lock, that thread can potentially block the\nprogress of other threads and make the program freeze.\nOne solution to deadlock avoidance is to assign each lock in the program a unique\nnumber, and to enforce an ordering rule that only allows multiple locks to be acquired\nin ascending order. This is surprisingly easy to implement using a context manager as\nfollows:\nimport threading\nfrom contextlib import contextmanager\n# Thread-local state to stored information on locks already acquired\n_local = threading.local()\n@contextmanager\ndef acquire(*locks):\n    # Sort locks by object identifier\n    locks = sorted(locks, key=lambda x: id(x))\n    # Make sure lock order of previously acquired locks is not violated\n    acquired = getattr(_local,'acquired',[])\n    if acquired and max(id(lock) for lock in acquired) >= id(locks[0]):\n        raise RuntimeError('Lock Order Violation')\n    # Acquire all of the locks\n    acquired.extend(locks)\n    _local.acquired = acquired\n500 \n| \nChapter 12: Concurrency",
      "content_length": 1713,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": 11,
      "content": "try:\n        for lock in locks:\n            lock.acquire()\n        yield\n    finally:\n        # Release locks in reverse order of acquisition\n        for lock in reversed(locks):\n            lock.release()\n        del acquired[-len(locks):]\nTo use this context manager, you simply allocate lock objects in the normal way, but use\nthe acquire() function whenever you want to work with one or more locks. For\nexample:\nimport threading\nx_lock = threading.Lock()\ny_lock = threading.Lock()\ndef thread_1():\n    while True:\n        with acquire(x_lock, y_lock):\n            print('Thread-1')\ndef thread_2():\n    while True:\n        with acquire(y_lock, x_lock):\n            print('Thread-2')\nt1 = threading.Thread(target=thread_1)\nt1.daemon = True\nt1.start()\nt2 = threading.Thread(target=thread_2)\nt2.daemon = True\nt2.start()\nIf you run this program, you’ll find that it happily runs forever without deadlock—even\nthough the acquisition of locks is specified in a different order in each function.\nThe key to this recipe lies in the first statement that sorts the locks according to object\nidentifier. By sorting the locks, they always get acquired in a consistent order regardless\nof how the user might have provided them to acquire().\nThe solution uses thread-local storage to solve a subtle problem with detecting potential\ndeadlock if multiple acquire() operations are nested. For example, suppose you wrote\nthe code like this:\nimport threading\nx_lock = threading.Lock()\ny_lock = threading.Lock()\ndef thread_1():\n12.5. Locking with Deadlock Avoidance \n| \n501",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": 11,
      "content": "while True:\n        with acquire(x_lock):\n            with acquire(y_lock):\n                print('Thread-1')\ndef thread_2():\n    while True:\n        with acquire(y_lock):\n            with acquire(x_lock):\n                print('Thread-2')\nt1 = threading.Thread(target=thread_1)\nt1.daemon = True\nt1.start()\nt2 = threading.Thread(target=thread_2)\nt2.daemon = True\nt2.start()\nIf you run this version of the program, one of the threads will crash with an exception\nsuch as this:\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.3/threading.py\", line 639, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.3/threading.py\", line 596, in run\n    self._target(*self._args, **self._kwargs)\n  File \"deadlock.py\", line 49, in thread_1\n    with acquire(y_lock):\n  File \"/usr/local/lib/python3.3/contextlib.py\", line 48, in __enter__\n    return next(self.gen)\n  File \"deadlock.py\", line 15, in acquire\n    raise RuntimeError(\"Lock Order Violation\")\nRuntimeError: Lock Order Violation\n>>>\nThis crash is caused by the fact that each thread remembers the locks it has already\nacquired. The acquire() function checks the list of previously acquired locks and en‐\nforces the ordering constraint that previously acquired locks must have an object ID\nthat is less than the new locks being acquired.\nDiscussion\nThe issue of deadlock is a well-known problem with programs involving threads (as\nwell as a common subject in textbooks on operating systems). As a rule of thumb, as\nlong as you can ensure that threads can hold only one lock at a time, your program will\nbe deadlock free. However, once multiple locks are being acquired at the same time, all\nbets are off.\n502 \n| \nChapter 12: Concurrency",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": 12,
      "content": "Detecting and recovering from deadlock is an extremely tricky problem with few elegant\nsolutions. For example, a common deadlock detection and recovery scheme involves\nthe use of a watchdog timer. As threads run, they periodically reset the timer, and as\nlong as everything is running smoothly, all is well. However, if the program deadlocks,\nthe watchdog timer will eventually expire. At that point, the program “recovers” by\nkilling and then restarting itself.\nDeadlock avoidance is a different strategy where locking operations are carried out in\na manner that simply does not allow the program to enter a deadlocked state. The\nsolution in which locks are always acquired in strict order of ascending object ID can\nbe mathematically proven to avoid deadlock, although the proof is left as an exercise to\nthe reader (the gist of it is that by acquiring locks in a purely increasing order, you can’t\nget cyclic locking dependencies, which are a necessary condition for deadlock to occur).\nAs a final example, a classic thread deadlock problem is the so-called “dining philoso‐\npher’s problem.” In this problem, five philosophers sit around a table on which there\nare five bowls of rice and five chopsticks. Each philosopher represents an independent\nthread and each chopstick represents a lock. In the problem, philosophers either sit and\nthink or they eat rice. However, in order to eat rice, a philosopher needs two chopsticks.\nUnfortunately, if all of the philosophers reach over and grab the chopstick to their left,\nthey’ll all just sit there with one stick and eventually starve to death. It’s a gruesome\nscene.\nUsing the solution, here is a simple deadlock free implementation of the dining philos‐\nopher’s problem:\nimport threading\n# The philosopher thread\ndef philosopher(left, right):\n    while True:\n        with acquire(left,right):\n             print(threading.currentThread(), 'eating')\n# The chopsticks (represented by locks)\nNSTICKS = 5\nchopsticks = [threading.Lock() for n in range(NSTICKS)]\n# Create all of the philosophers\nfor n in range(NSTICKS):\n    t = threading.Thread(target=philosopher,\n                         args=(chopsticks[n],chopsticks[(n+1) % NSTICKS]))\n    t.start()\nLast, but not least, it should be noted that in order to avoid deadlock, all locking oper‐\nations must be carried out using our acquire() function. If some fragment of code\ndecided to acquire a lock directly, then the deadlock avoidance algorithm wouldn’t work.\n12.5. Locking with Deadlock Avoidance \n| \n503",
      "content_length": 2509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": 12,
      "content": "12.6. Storing Thread-Specific State\nProblem\nYou need to store state that’s specific to the currently executing thread and not visible\nto other threads.\nSolution\nSometimes in multithreaded programs, you need to store data that is only specific to\nthe currently executing thread. To do this, create a thread-local storage object using\nthreading.local(). Attributes stored and read on this object are only visible to the\nexecuting thread and no others.\nAs an interesting practical example of using thread-local storage, consider the LazyCon\nnection context-manager class that was first defined in Recipe 8.3. Here is a slightly\nmodified version that safely works with multiple threads:\nfrom socket import socket, AF_INET, SOCK_STREAM\nimport threading\nclass LazyConnection:\n    def __init__(self, address, family=AF_INET, type=SOCK_STREAM):\n        self.address = address\n        self.family = AF_INET\n        self.type = SOCK_STREAM\n        self.local = threading.local()\n    def __enter__(self):\n        if hasattr(self.local, 'sock'):\n            raise RuntimeError('Already connected')\n        self.local.sock = socket(self.family, self.type)\n        self.local.sock.connect(self.address)\n        return self.local.sock\n    def __exit__(self, exc_ty, exc_val, tb):\n        self.local.sock.close()\n        del self.local.sock\nIn this code, carefully observe the use of the self.local attribute. It is initialized as an\ninstance of threading.local(). The other methods then manipulate a socket that’s\nstored as self.local.sock. This is enough to make it possible to safely use an instance\nof LazyConnection in multiple threads. For example:\nfrom functools import partial\ndef test(conn):\n    with conn as s:\n        s.send(b'GET /index.html HTTP/1.0\\r\\n')\n        s.send(b'Host: www.python.org\\r\\n')\n504 \n| \nChapter 12: Concurrency",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": 12,
      "content": "s.send(b'\\r\\n')\n        resp = b''.join(iter(partial(s.recv, 8192), b''))\n    print('Got {} bytes'.format(len(resp)))\nif __name__ == '__main__':\n    conn = LazyConnection(('www.python.org', 80))\n    t1 = threading.Thread(target=test, args=(conn,))\n    t2 = threading.Thread(target=test, args=(conn,))\n    t1.start()\n    t2.start()\n    t1.join()\n    t2.join()\nThe reason it works is that each thread actually creates its own dedicated socket con‐\nnection (stored as self.local.sock). Thus, when the different threads perform socket\noperations, they don’t interfere with one another as they are being performed on dif‐\nferent sockets.\nDiscussion\nCreating and manipulating thread-specific state is not a problem that often arises in\nmost programs. However, when it does, it commonly involves situations where an object\nbeing used by multiple threads needs to manipulate some kind of dedicated system\nresource, such as a socket or file. You can’t just have a single socket object shared by\neveryone because chaos would ensue if multiple threads ever started reading and writing\non it at the same time. Thread-local storage fixes this by making such resources only\nvisible in the thread where they’re being used.\nIn this recipe, the use of threading.local() makes the LazyConnection class support\none connection per thread, as opposed to one connection for the entire process. It’s a\nsubtle but interesting distinction.\nUnder the covers, an instance of threading.local() maintains a separate instance\ndictionary for each thread. All of the usual instance operations of getting, setting, and\ndeleting values just manipulate the per-thread dictionary. The fact that each thread uses\na separate dictionary is what provides the isolation of data.\n12.7. Creating a Thread Pool\nProblem\nYou want to create a pool of worker threads for serving clients or performing other kinds\nof work.\n12.7. Creating a Thread Pool \n| \n505",
      "content_length": 1910,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": 12,
      "content": "Solution\nThe concurrent.futures library has a ThreadPoolExecutor class that can be used for\nthis purpose. Here is an example of a simple TCP server that uses a thread-pool to serve\nclients:\nfrom socket import AF_INET, SOCK_STREAM, socket\nfrom concurrent.futures import ThreadPoolExecutor\ndef echo_client(sock, client_addr):\n    '''\n    Handle a client connection\n    '''\n    print('Got connection from', client_addr)\n    while True:\n        msg = sock.recv(65536)\n        if not msg:\n            break\n        sock.sendall(msg)\n    print('Client closed connection')\n    sock.close()\ndef echo_server(addr):\n    pool = ThreadPoolExecutor(128)\n    sock = socket(AF_INET, SOCK_STREAM)\n    sock.bind(addr)\n    sock.listen(5)\n    while True:\n        client_sock, client_addr = sock.accept()\n        pool.submit(echo_client, client_sock, client_addr)\necho_server(('',15000))\nIf you want to manually create your own thread pool, it’s usually easy enough to do it\nusing a Queue. Here is a slightly different, but manual implementation of the same code:\nfrom socket import socket, AF_INET, SOCK_STREAM\nfrom threading import Thread\nfrom queue import Queue\ndef echo_client(q):\n    '''\n    Handle a client connection\n    '''\n    sock, client_addr = q.get()\n    print('Got connection from', client_addr)\n    while True:\n        msg = sock.recv(65536)\n        if not msg:\n            break\n        sock.sendall(msg)\n    print('Client closed connection')\n506 \n| \nChapter 12: Concurrency",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": 12,
      "content": "sock.close()\ndef echo_server(addr, nworkers):\n    # Launch the client workers\n    q = Queue()\n    for n in range(nworkers):\n        t = Thread(target=echo_client, args=(q,))\n        t.daemon = True\n        t.start()\n    # Run the server\n    sock = socket(AF_INET, SOCK_STREAM)\n    sock.bind(addr)\n    sock.listen(5)\n    while True:\n        client_sock, client_addr = sock.accept()\n        q.put((client_sock, client_addr))\necho_server(('',15000), 128)\nOne advantage of using ThreadPoolExecutor over a manual implementation is that it\nmakes it easier for the submitter to receive results from the called function. For example,\nyou could write code like this:\nfrom concurrent.futures import ThreadPoolExecutor\nimport urllib.request\ndef fetch_url(url):\n    u = urllib.request.urlopen(url)\n    data = u.read()\n    return data\npool = ThreadPoolExecutor(10)\n# Submit work to the pool\na = pool.submit(fetch_url, 'http://www.python.org')\nb = pool.submit(fetch_url, 'http://www.pypy.org')\n# Get the results back\nx = a.result()\ny = b.result()\nThe result objects in the example handle all of the blocking and coordination needed\nto get data back from the worker thread. Specifically, the operation a.result() blocks\nuntil the corresponding function has been executed by the pool and returned a value.\nDiscussion\nGenerally, you should avoid writing programs that allow unlimited growth in the num‐\nber of threads. For example, take a look at the following server:\nfrom threading import Thread\nfrom socket import socket, AF_INET, SOCK_STREAM\n12.7. Creating a Thread Pool \n| \n507",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": 12,
      "content": "def echo_client(sock, client_addr):\n    '''\n    Handle a client connection\n    '''\n    print('Got connection from', client_addr)\n    while True:\n        msg = sock.recv(65536)\n        if not msg:\n            break\n        sock.sendall(msg)\n    print('Client closed connection')\n    sock.close()\ndef echo_server(addr, nworkers):\n    # Run the server\n    sock = socket(AF_INET, SOCK_STREAM)\n    sock.bind(addr)\n    sock.listen(5)\n    while True:\n        client_sock, client_addr = sock.accept()\n        t = Thread(target=echo_client, args=(client_sock, client_addr))\n        t.daemon = True\n        t.start()\necho_server(('',15000))\nAlthough this works, it doesn’t prevent some asynchronous hipster from launching an\nattack on the server that makes it create so many threads that your program runs out\nof resources and crashes (thus further demonstrating the “evils” of using threads). By\nusing a pre-initialized thread pool, you can carefully put an upper limit on the amount\nof supported concurrency.\nYou might be concerned with the effect of creating a large number of threads. However,\nmodern systems should have no trouble creating pools of a few thousand threads.\nMoreover, having a thousand threads just sitting around waiting for work isn’t going to\nhave much, if any, impact on the performance of other code (a sleeping thread does just\nthat—nothing at all). Of course, if all of those threads wake up at the same time and\nstart hammering on the CPU, that’s a different story—especially in light of the Global\nInterpreter Lock (GIL). Generally, you only want to use thread pools for I/O-bound\nprocessing.\nOne possible concern with creating large thread pools might be memory use. For ex‐\nample, if you create 2,000 threads on OS X, the system shows the Python process using\nup more than 9 GB of virtual memory. However, this is actually somewhat misleading.\nWhen creating a thread, the operating system reserves a region of virtual memory to\nhold the thread’s execution stack (often as large as 8 MB). Only a small fragment of this\nmemory is actually mapped to real memory, though. Thus, if you look a bit closer, you\nmight find the Python process is using far less real memory (e.g., for 2,000 threads, only\n508 \n| \nChapter 12: Concurrency",
      "content_length": 2247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": 12,
      "content": "70 MB of real memory is used, not 9 GB). If the size of the virtual memory is a concern,\nyou can dial it down using the threading.stack_size() function. For example:\nimport threading\nthreading.stack_size(65536)\nIf you add this call and repeat the experiment of creating 2,000 threads, you’ll find that\nthe Python process is now only using about 210 MB of virtual memory, although the\namount of real memory in use remains about the same. Note that the thread stack size\nmust be at least 32,768 bytes, and is usually restricted to be a multiple of the system\nmemory page size (4096, 8192, etc.).\n12.8. Performing Simple Parallel Programming\nProblem\nYou have a program that performs a lot of CPU-intensive work, and you want to make\nit run faster by having it take advantage of multiple CPUs.\nSolution\nThe concurrent.futures library provides a ProcessPoolExecutor class that can be\nused to execute computationally intensive functions in a separately running instance of\nthe Python interpreter. However, in order to use it, you first need to have some com‐\nputationally intensive work. Let’s illustrate with a simple yet practical example.\nSuppose you have an entire directory of gzip-compressed Apache web server logs:\nlogs/\n   20120701.log.gz\n   20120702.log.gz\n   20120703.log.gz\n   20120704.log.gz\n   20120705.log.gz\n   20120706.log.gz\n   ...\nFurther suppose each log file contains lines like this:\n124.115.6.12 - - [10/Jul/2012:00:18:50 -0500] \"GET /robots.txt ...\" 200 71\n210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] \"GET /ply/ ...\" 200 11875\n210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] \"GET /favicon.ico ...\" 404 369\n61.135.216.105 - - [10/Jul/2012:00:20:04 -0500] \"GET /blog/atom.xml ...\" 304 -\n...\nHere is a simple script that takes this data and identifies all hosts that have accessed the\nrobots.txt file:\n12.8. Performing Simple Parallel Programming \n| \n509",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": 12,
      "content": "# findrobots.py\nimport gzip\nimport io\nimport glob\ndef find_robots(filename):\n    '''\n    Find all of the hosts that access robots.txt in a single log file\n    '''\n    robots = set()\n    with gzip.open(filename) as f:\n        for line in io.TextIOWrapper(f,encoding='ascii'):\n            fields = line.split()\n            if fields[6] == '/robots.txt':\n                robots.add(fields[0])\n    return robots\ndef find_all_robots(logdir):\n    '''\n    Find all hosts across and entire sequence of files\n    '''\n    files = glob.glob(logdir+'/*.log.gz')\n    all_robots = set()\n    for robots in map(find_robots, files):\n        all_robots.update(robots)\n    return all_robots\nif __name__ == '__main__':\n    robots = find_all_robots('logs')\n    for ipaddr in robots:\n        print(ipaddr)\nThe preceding program is written in the commonly used map-reduce style. The function\nfind_robots() is mapped across a collection of filenames and the results are combined\ninto a single result (the all_robots set in the find_all_robots() function).\nNow, suppose you want to modify this program to use multiple CPUs. It turns out to\nbe easy—simply replace the map() operation with a similar operation carried out on a\nprocess pool from the concurrent.futures library. Here is a slightly modified version\nof the code:\n# findrobots.py\nimport gzip\nimport io\nimport glob\nfrom concurrent import futures\ndef find_robots(filename):\n    '''\n    Find all of the hosts that access robots.txt in a single log file\n510 \n| \nChapter 12: Concurrency",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": 12,
      "content": "'''\n    robots = set()\n    with gzip.open(filename) as f:\n        for line in io.TextIOWrapper(f,encoding='ascii'):\n            fields = line.split()\n            if fields[6] == '/robots.txt':\n                robots.add(fields[0])\n    return robots\ndef find_all_robots(logdir):\n    '''\n    Find all hosts across and entire sequence of files\n    '''\n    files = glob.glob(logdir+'/*.log.gz')\n    all_robots = set()\n    with futures.ProcessPoolExecutor() as pool:\n        for robots in pool.map(find_robots, files):\n            all_robots.update(robots)\n    return all_robots\nif __name__ == '__main__':\n    robots = find_all_robots('logs')\n    for ipaddr in robots:\n        print(ipaddr)\nWith this modification, the script produces the same result but runs about 3.5 times\nfaster on our quad-core machine. The actual performance will vary according to the\nnumber of CPUs available on your machine.\nDiscussion\nTypical usage of a ProcessPoolExecutor is as follows:\nfrom concurrent.futures import ProcessPoolExecutor\nwith ProcessPoolExecutor() as pool:\n    ...\n    do work in parallel using pool\n    ...\nUnder the covers, a ProcessPoolExecutor creates N independent running Python in‐\nterpreters where N is the number of available CPUs detected on the system. You can\nchange the number of processes created by supplying an optional argument to Proces\nsPoolExecutor(N). The pool runs until the last statement in the with block is executed,\nat which point the process pool is shut down. However, the program will wait until all\nsubmitted work has been processed.\nWork to be submitted to a pool must be defined in a function. There are two methods\nfor submission. If you are are trying to parallelize a list comprehension or a map()\noperation, you use pool.map():\n12.8. Performing Simple Parallel Programming \n| \n511",
      "content_length": 1808,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": 12,
      "content": "# A function that performs a lot of work\ndef work(x):\n    ...\n    return result\n# Nonparallel code\nresults = map(work, data)\n# Parallel implementation\nwith ProcessPoolExecutor() as pool:\n    results = pool.map(work, data)\nAlternatively, you can manually submit single tasks using the pool.submit() method:\n# Some function\ndef work(x):\n    ...\n    return result\nwith ProcessPoolExecutor() as pool:\n    ...\n    # Example of submitting work to the pool\n    future_result = pool.submit(work, arg)\n    # Obtaining the result (blocks until done)\n    r = future_result.result()\n    ...\nIf you manually submit a job, the result is an instance of Future. To obtain the actual\nresult, you call its result() method. This blocks until the result is computed and re‐\nturned by the pool.\nInstead of blocking, you can also arrange to have a callback function triggered upon\ncompletion instead. For example:\ndef when_done(r):\n    print('Got:', r.result())\nwith ProcessPoolExecutor() as pool:\n     future_result = pool.submit(work, arg)\n     future_result.add_done_callback(when_done)\nThe user-supplied callback function receives an instance of Future that must be used\nto obtain the actual result (i.e., by calling its result() method).\nAlthough process pools can be easy to use, there are a number of important consider‐\nations to be made in designing larger programs. In no particular order:\n• This technique for parallelization only works well for problems that can be trivially\ndecomposed into independent parts.\n512 \n| \nChapter 12: Concurrency",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": 12,
      "content": "• Work must be submitted in the form of simple functions. Parallel execution of\ninstance methods, closures, or other kinds of constructs are not supported.\n• Function arguments and return values must be compatible with pickle. Work is\ncarried out in a separate interpreter using interprocess communication. Thus, data\nexchanged between interpreters has to be serialized.\n• Functions submitted for work should not maintain persistent state or have side\neffects. With the exception of simple things such as logging, you don’t really have\nany control over the behavior of child processes once started. Thus, to preserve your\nsanity, it is probably best to keep things simple and carry out work in pure-functions\nthat don’t alter their environment.\n• Process pools are created by calling the fork() system call on Unix. This makes a\nclone of the Python interpreter, including all of the program state at the time of the\nfork. On Windows, an independent copy of the interpreter that does not clone state\nis launched. The actual forking process does not occur until the first pool.map()\nor pool.submit() method is called.\n• Great care should be made when combining process pools and programs that use\nthreads. In particular, you should probably create and launch process pools prior\nto the creation of any threads (e.g., create the pool in the main thread at program\nstartup).\n12.9. Dealing with the GIL (and How to Stop Worrying\nAbout It)\nProblem\nYou’ve heard about the Global Interpreter Lock (GIL), and are worried that it might be\naffecting the performance of your multithreaded program.\nSolution\nAlthough Python fully supports thread programming, parts of the C implementation\nof the interpreter are not entirely thread safe to a level of allowing fully concurrent\nexecution. In fact, the interpreter is protected by a so-called Global Interpreter Lock\n(GIL) that only allows one Python thread to execute at any given time. The most no‐\nticeable effect of the GIL is that multithreaded Python programs are not able to fully\ntake advantage of multiple CPU cores (e.g., a computationally intensive application\nusing more than one thread only runs on a single CPU).\n12.9. Dealing with the GIL (and How to Stop Worrying About It) \n| \n513",
      "content_length": 2232,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": 12,
      "content": "Before discussing common GIL workarounds, it is important to emphasize that the GIL\ntends to only affect programs that are heavily CPU bound (i.e., dominated by compu‐\ntation). If your program is mostly doing I/O, such as network communication, threads\nare often a sensible choice because they’re mostly going to spend their time sitting\naround waiting. In fact, you can create thousands of Python threads with barely a con‐\ncern. Modern operating systems have no trouble running with that many threads, so\nit’s simply not something you should worry much about.\nFor CPU-bound programs, you really need to study the nature of the computation being\nperformed. For instance, careful choice of the underlying algorithm may produce a far\ngreater speedup than trying to parallelize an unoptimal algorithm with threads. Simi‐\nlarly, given that Python is interpreted, you might get a far greater speedup simply by\nmoving performance-critical code into a C extension module. Extensions such as \nNumPy are also highly effective at speeding up certain kinds of calculations involving\narray data. Last, but not least, you might investigate alternative implementations, such\nas PyPy, which features optimizations such as a JIT compiler (although, as of this writing,\nit does not yet support Python 3).\nIt’s also worth noting that threads are not necessarily used exclusively for performance.\nA CPU-bound program might be using threads to manage a graphical user interface, a\nnetwork connection, or provide some other kind of service. In this case, the GIL can\nactually present more of a problem, since code that holds it for an excessively long period\nwill cause annoying stalls in the non-CPU-bound threads. In fact, a poorly written C\nextension can actually make this problem worse, even though the computation part of\nthe code might run faster than before.\nHaving said all of this, there are two common strategies for working around the limi‐\ntations of the GIL. First, if you are working entirely in Python, you can use the multi\nprocessing module to create a process pool and use it like a co-processor. For example,\nsuppose you have the following thread code:\n# Performs a large calculation (CPU bound)\ndef some_work(args):\n    ...\n    return result\n# A thread that calls the above function\ndef some_thread():\n    while True:\n        ...\n        r = some_work(args)\n        ...\nHere’s how you would modify the code to use a pool:\n# Processing pool (see below for initiazation)\npool = None\n514 \n| \nChapter 12: Concurrency",
      "content_length": 2513,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": 12,
      "content": "# Performs a large calculation (CPU bound)\ndef some_work(args):\n    ...\n    return result\n# A thread that calls the above function\ndef some_thread():\n    while True:\n        ...\n        r = pool.apply(some_work, (args))\n        ...\n# Initiaze the pool\nif __name__ == '__main__':\n    import multiprocessing\n    pool = multiprocessing.Pool()\nThis example with a pool works around the GIL using a neat trick. Whenever a thread\nwants to perform CPU-intensive work, it hands the work to the pool. The pool, in turn,\nhands the work to a separate Python interpreter running in a different process. While\nthe thread is waiting for the result, it releases the GIL. Moreover, because the calculation\nis being performed in a separate interpreter, it’s no longer bound by the restrictions of\nthe GIL. On a multicore system, you’ll find that this technique easily allows you to take\nadvantage of all the CPUs.\nThe second strategy for working around the GIL is to focus on C extension program‐\nming. The general idea is to move computationally intensive tasks to C, independent of\nPython, and have the C code release the GIL while it’s working. This is done by inserting\nspecial macros into the C code like this:\n#include \"Python.h\"\n...\nPyObject *pyfunc(PyObject *self, PyObject *args) {\n   ...\n   Py_BEGIN_ALLOW_THREADS\n   // Threaded C code\n   ...\n   Py_END_ALLOW_THREADS\n   ...\n}\nIf you are using other tools to access C, such as the ctypes library or Cython, you may\nnot need to do anything. For example, ctypes releases the GIL when calling into C by\ndefault.\nDiscussion\nMany programmers, when faced with thread performance problems, are quick to blame\nthe GIL for all of their ills. However, doing so is shortsighted and naive. Just as a real-\n12.9. Dealing with the GIL (and How to Stop Worrying About It) \n| \n515",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": 12,
      "content": "world example, mysterious “stalls” in a multithreaded network program might be caused\nby something entirely different (e.g., a stalled DNS lookup) rather than anything related\nto the GIL. The bottom line is that you really need to study your code to know if the\nGIL is an issue or not. Again, realize that the GIL is mostly concerned with CPU-bound\nprocessing, not I/O.\nIf you are going to use a process pool as a workaround, be aware that doing so involves\ndata serialization and communication with a different Python interpreter. For this to\nwork, the operation to be performed needs to be contained within a Python function\ndefined by the def statement (i.e., no lambdas, closures, callable instances, etc.), and the\nfunction arguments and return value must be compatible with pickle. Also, the amount\nof work to be performed must be sufficiently large to make up for the extra communi‐\ncation overhead.\nAnother subtle aspect of pools is that mixing threads and process pools together can be\na good way to make your head explode. If you are going to use both of these features\ntogether, it is often best to create the process pool as a singleton at program startup,\nprior to the creation of any threads. Threads will then use the same process pool for all\nof their computationally intensive work.\nFor C extensions, the most important feature is maintaining isolation from the Python\ninterpreter process. That is, if you’re going to offload work from Python to C, you need\nto make sure the C code operates independently of Python. This means using no Python\ndata structures and making no calls to Python’s C API. Another consideration is that\nyou want to make sure your C extension does enough work to make it all worthwhile.\nThat is, it’s much better if the extension can perform millions of calculations as opposed\nto just a few small calculations.\nNeedless to say, these solutions to working around the GIL don’t apply to all possible\nproblems. For instance, certain kinds of applications don’t work well if separated into\nmultiple processes, nor may you want to code parts in C. For these kinds of applications,\nyou may have to come up with your own solution (e.g., multiple processes accessing\nshared memory regions, multiple interpreters running in the same process, etc.). Al‐\nternatively, you might look at some other implementations of the interpreter, such as\nPyPy.\nSee Recipes 15.7 and 15.10 for additional information on releasing the GIL in C\nextensions.\n12.10. Defining an Actor Task\nProblem\nYou’d like to define tasks with behavior similar to “actors” in the so-called “actor model.”\n516 \n| \nChapter 12: Concurrency",
      "content_length": 2632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": 12,
      "content": "Solution\nThe “actor model” is one of the oldest and most simple approaches to concurrency and\ndistributed computing. In fact, its underlying simplicity is part of its appeal. In a nutshell,\nan actor is a concurrently executing task that simply acts upon messages sent to it. In\nresponse to these messages, it may decide to send further messages to other actors.\nCommunication with actors is one way and asynchronous. Thus, the sender of a message\ndoes not know when a message actually gets delivered, nor does it receive a response\nor acknowledgment that the message has been processed.\nActors are straightforward to define using a combination of a thread and a queue. For\nexample:\nfrom queue import Queue\nfrom threading import Thread, Event\n# Sentinel used for shutdown\nclass ActorExit(Exception):\n    pass\nclass Actor:\n    def __init__(self):\n        self._mailbox = Queue()\n    def send(self, msg):\n        '''\n        Send a message to the actor\n        '''\n        self._mailbox.put(msg)\n    def recv(self):\n        '''\n        Receive an incoming message\n        '''\n        msg = self._mailbox.get()\n        if msg is ActorExit:\n            raise ActorExit()\n        return msg\n    def close(self):\n        '''\n        Close the actor, thus shutting it down\n        '''\n        self.send(ActorExit)\n    def start(self):\n        '''\n        Start concurrent execution\n        '''\n        self._terminated = Event()\n        t = Thread(target=self._bootstrap)\n12.10. Defining an Actor Task \n| \n517",
      "content_length": 1501,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": 12,
      "content": "t.daemon = True\n        t.start()\n    def _bootstrap(self):\n        try:\n            self.run()\n        except ActorExit:\n            pass\n        finally:\n            self._terminated.set()\n    def join(self):\n        self._terminated.wait()\n    def run(self):\n        '''\n        Run method to be implemented by the user\n        '''\n        while True:\n            msg = self.recv()\n# Sample ActorTask\nclass PrintActor(Actor):\n    def run(self):\n        while True:\n            msg = self.recv()\n            print('Got:', msg)\n# Sample use\np = PrintActor()\np.start()\np.send('Hello')\np.send('World')\np.close()\np.join()\nIn this example, Actor instances are things that you simply send a message to using\ntheir send() method. Under the covers, this places the message on a queue and hands\nit off to an internal thread that runs to process the received messages. The close()\nmethod is programmed to shut down the actor by placing a special sentinel value\n(ActorExit) on the queue. Users define new actors by inheriting from Actor and re‐\ndefining the run() method to implement their custom processing. The usage of the\nActorExit exception is such that user-defined code can be programmed to catch the\ntermination request and handle it if appropriate (the exception is raised by the get()\nmethod and propagated).\nIf you relax the requirement of concurrent and asynchronous message delivery, actor-\nlike objects can also be minimally defined by generators. For example:\ndef print_actor():\n    while True:\n518 \n| \nChapter 12: Concurrency",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": 12,
      "content": "try:\n            msg = yield      # Get a message\n            print('Got:', msg)\n        except GeneratorExit:\n            print('Actor terminating')\n# Sample use\np = print_actor()\nnext(p)     # Advance to the yield (ready to receive)\np.send('Hello')\np.send('World')\np.close()\nDiscussion\nPart of the appeal of actors is their underlying simplicity. In practice, there is just one\ncore operation, send(). Plus, the general concept of a “message” in actor-based systems\nis something that can be expanded in many different directions. For example, you could\npass tagged messages in the form of tuples and have actors take different courses of\naction like this:\nclass TaggedActor(Actor):\n    def run(self):\n        while True:\n             tag, *payload = self.recv()\n             getattr(self,'do_'+tag)(*payload)\n    # Methods correponding to different message tags\n    def do_A(self, x):\n        print('Running A', x)\n    def do_B(self, x, y):\n        print('Running B', x, y)\n# Example\na = TaggedActor()\na.start()\na.send(('A', 1))      # Invokes do_A(1)\na.send(('B', 2, 3))   # Invokes do_B(2,3)\nAs another example, here is a variation of an actor that allows arbitrary functions to be\nexecuted in a worker and results to be communicated back using a special Result object:\nfrom threading import Event\nclass Result:\n    def __init__(self):\n        self._evt = Event()\n        self._result = None\n    def set_result(self, value):\n        self._result = value\n12.10. Defining an Actor Task \n| \n519",
      "content_length": 1495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": 12,
      "content": "self._evt.set()\n    def result(self):\n        self._evt.wait()\n        return self._result\nclass Worker(Actor):\n    def submit(self, func, *args, **kwargs):\n        r = Result()\n        self.send((func, args, kwargs, r))\n        return r\n    def run(self):\n        while True:\n            func, args, kwargs, r = self.recv()\n            r.set_result(func(*args, **kwargs))\n# Example use\nworker = Worker()\nworker.start()\nr = worker.submit(pow, 2, 3)\nprint(r.result())\nLast, but not least, the concept of “sending” a task a message is something that can be\nscaled up into systems involving multiple processes or even large distributed systems.\nFor example, the send() method of an actor-like object could be programmed to trans‐\nmit data on a socket connection or deliver it via some kind of messaging infrastructure\n(e.g., AMQP, ZMQ, etc.).\n12.11. Implementing Publish/Subscribe Messaging\nProblem\nYou have a program based on communicating threads and want them to implement\npublish/subscribe messaging.\nSolution\nTo implement publish/subscribe messaging, you typically introduce a separate “ex‐\nchange” or “gateway” object that acts as an intermediary for all messages. That is, instead\nof directly sending a message from one task to another, a message is sent to the exchange\nand it delivers it to one or more attached tasks. Here is one example of a very simple\nexchange implementation:\nfrom collections import defaultdict\nclass Exchange:\n    def __init__(self):\n        self._subscribers = set()\n520 \n| \nChapter 12: Concurrency",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": 12,
      "content": "def attach(self, task):\n        self._subscribers.add(task)\n    def detach(self, task):\n        self._subscribers.remove(task)\n    def send(self, msg):\n        for subscriber in self._subscribers:\n            subscriber.send(msg)\n# Dictionary of all created exchanges\n_exchanges = defaultdict(Exchange)\n# Return the Exchange instance associated with a given name\ndef get_exchange(name):\n    return _exchanges[name]\nAn exchange is really nothing more than an object that keeps a set of active subscribers\nand provides methods for attaching, detaching, and sending messages. Each exchange\nis identified by a name, and the get_exchange() function simply returns the Ex\nchange instance associated with a given name.\nHere is a simple example that shows how to use an exchange:\n# Example of a task.  Any object with a send() method\nclass Task:\n    ...\n    def send(self, msg):\n        ...\ntask_a = Task()\ntask_b = Task()\n# Example of getting an exchange\nexc = get_exchange('name')\n# Examples of subscribing tasks to it\nexc.attach(task_a)\nexc.attach(task_b)\n# Example of sending messages\nexc.send('msg1')\nexc.send('msg2')\n# Example of unsubscribing\nexc.detach(task_a)\nexc.detach(task_b)\n12.11. Implementing Publish/Subscribe Messaging \n| \n521",
      "content_length": 1235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": 12,
      "content": "Although there are many different variations on this theme, the overall idea is the same.\nMessages will be delivered to an exchange and the exchange will deliver them to attached\nsubscribers.\nDiscussion\nThe concept of tasks or threads sending messages to one another (often via queues) is\neasy to implement and quite popular. However, the benefits of using a public/subscribe\n(pub/sub) model instead are often overlooked.\nFirst, the use of an exchange can simplify much of the plumbing involved in setting up\ncommunicating threads. Instead of trying to wire threads together across multiple pro‐\ngram modules, you only worry about connecting them to a known exchange. In some\nsense, this is similar to how the logging library works. In practice, it can make it easier\nto decouple various tasks in the program.\nSecond, the ability of the exchange to broadcast messages to multiple subscribers opens\nup new communication patterns. For example, you could implement systems with re‐\ndundant tasks, broadcasting, or fan-out. You could also build debugging and diagnostic\ntools that attach themselves to exchanges as ordinary subscribers. For example, here is\na simple diagnostic class that would display sent messages:\nclass DisplayMessages:\n    def __init__(self):\n        self.count = 0\n    def send(self, msg):\n        self.count += 1\n        print('msg[{}]: {!r}'.format(self.count, msg))\nexc = get_exchange('name')\nd = DisplayMessages()\nexc.attach(d)\nLast, but not least, a notable aspect of the implementation is that it works with a variety\nof task-like objects. For example, the receivers of a message could be actors (as described\nin Recipe 12.10), coroutines, network connections, or just about anything that imple‐\nments a proper send() method.\nOne potentially problematic aspect of an exchange concerns the proper attachment and\ndetachment of subscribers. In order to properly manage resources, every subscriber that\nattaches must eventually detach. This leads to a programming model similar to this:\nexc = get_exchange('name')\nexc.attach(some_task)\ntry:\n    ...\nfinally:\n    exc.detach(some_task)\n522 \n| \nChapter 12: Concurrency",
      "content_length": 2136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": 12,
      "content": "In some sense, this is similar to the usage of files, locks, and similar objects. Experience\nhas shown that it is quite easy to forget the final detach() step. To simplify this, you\nmight consider the use of the context-management protocol. For example, adding a\nsubscribe() method to the exchange like this:\nfrom contextlib import contextmanager\nfrom collections import defaultdict\nclass Exchange:\n    def __init__(self):\n        self._subscribers = set()\n    def attach(self, task):\n        self._subscribers.add(task)\n    def detach(self, task):\n        self._subscribers.remove(task)\n    @contextmanager\n    def subscribe(self, *tasks):\n        for task in tasks:\n            self.attach(task)\n        try:\n            yield\n        finally:\n            for task in tasks:\n                self.detach(task)\n    def send(self, msg):\n        for subscriber in self._subscribers:\n            subscriber.send(msg)\n# Dictionary of all created exchanges\n_exchanges = defaultdict(Exchange)\n# Return the Exchange instance associated with a given name\ndef get_exchange(name):\n    return _exchanges[name]\n# Example of using the subscribe() method\nexc = get_exchange('name')\nwith exc.subscribe(task_a, task_b):\n     ...\n     exc.send('msg1')\n     exc.send('msg2')\n     ...\n# task_a and task_b detached here\nFinally, it should be noted that there are numerous possible extensions to the exchange\nidea. For example, exchanges could implement an entire collection of message channels\n12.11. Implementing Publish/Subscribe Messaging \n| \n523",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": 12,
      "content": "or apply pattern matching rules to exchange names. Exchanges can also be extended\ninto distributed computing applications (e.g., routing messages to tasks on different\nmachines, etc.).\n12.12. Using Generators As an Alternative to Threads\nProblem\nYou want to implement concurrency using generators (coroutines) as an alternative to\nsystem threads. This is sometimes known as user-level threading or green threading.\nSolution\nTo implement your own concurrency using generators, you first need a fundamental\ninsight concerning generator functions and the yield statement. Specifically, the fun‐\ndamental behavior of yield is that it causes a generator to suspend its execution. By\nsuspending execution, it is possible to write a scheduler that treats generators as a kind\nof “task” and alternates their execution using a kind of cooperative task switching.\nTo illustrate this idea, consider the following two generator functions using a simple\nyield:\n# Two simple generator functions\ndef countdown(n):\n    while n > 0:\n        print('T-minus', n)\n        yield\n        n -= 1\n    print('Blastoff!')\ndef countup(n):\n    x = 0\n    while x < n:\n        print('Counting up', x)\n        yield\n        x += 1\nThese functions probably look a bit funny using yield all by itself. However, consider\nthe following code that implements a simple task scheduler:\nfrom collections import deque\nclass TaskScheduler:\n    def __init__(self):\n        self._task_queue = deque()\n    def new_task(self, task):\n        '''\n        Admit a newly started task to the scheduler\n524 \n| \nChapter 12: Concurrency",
      "content_length": 1582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": 12,
      "content": "'''\n        self._task_queue.append(task)\n    def run(self):\n        '''\n        Run until there are no more tasks\n        '''\n        while self._task_queue:\n            task = self._task_queue.popleft()\n            try:\n                # Run until the next yield statement\n                next(task)\n                self._task_queue.append(task)\n            except StopIteration:\n                # Generator is no longer executing\n                pass\n# Example use\nsched = TaskScheduler()\nsched.new_task(countdown(10))\nsched.new_task(countdown(5))\nsched.new_task(countup(15))\nsched.run()\nIn this code, the TaskScheduler class runs a collection of generators in a round-robin\nmanner—each one running until they reach a yield statement. For the sample, the\noutput will be as follows:\nT-minus 10\nT-minus 5\nCounting up 0\nT-minus 9\nT-minus 4\nCounting up 1\nT-minus 8\nT-minus 3\nCounting up 2\nT-minus 7\nT-minus 2\n...\nAt this point, you’ve essentially implemented the tiny core of an “operating system” if\nyou will. Generator functions are the tasks and the yield statement is how tasks signal\nthat they want to suspend. The scheduler simply cycles over the tasks until none are left\nexecuting.\nIn practice, you probably wouldn’t use generators to implement concurrency for some‐\nthing as simple as shown. Instead, you might use generators to replace the use of threads\nwhen implementing actors (see Recipe 12.10) or network servers.\n12.12. Using Generators As an Alternative to Threads \n| \n525",
      "content_length": 1488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": 12,
      "content": "The following code illustrates the use of generators to implement a thread-free version\nof actors:\nfrom collections import deque\nclass ActorScheduler:\n    def __init__(self):\n        self._actors = { }          # Mapping of names to actors\n        self._msg_queue = deque()   # Message queue\n    def new_actor(self, name, actor):\n        '''\n        Admit a newly started actor to the scheduler and give it a name\n        '''\n        self._msg_queue.append((actor,None))\n        self._actors[name] = actor\n    def send(self, name, msg):\n        '''\n        Send a message to a named actor\n        '''\n        actor = self._actors.get(name)\n        if actor:\n            self._msg_queue.append((actor,msg))\n    def run(self):\n        '''\n        Run as long as there are pending messages.\n        '''\n        while self._msg_queue:\n            actor, msg = self._msg_queue.popleft()\n            try:\n                 actor.send(msg)\n            except StopIteration:\n                 pass\n# Example use\nif __name__ == '__main__':\n    def printer():\n        while True:\n            msg = yield\n            print('Got:', msg)\n    def counter(sched):\n        while True:\n            # Receive the current count\n            n = yield\n            if n == 0:\n                break\n            # Send to the printer task\n            sched.send('printer', n)\n            # Send the next count to the counter task (recursive)\n526 \n| \nChapter 12: Concurrency",
      "content_length": 1447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": 12,
      "content": "sched.send('counter', n-1)\n    sched = ActorScheduler()\n    # Create the initial actors\n    sched.new_actor('printer', printer())\n    sched.new_actor('counter', counter(sched))\n    # Send an initial message to the counter to initiate\n    sched.send('counter', 10000)\n    sched.run()\nThe execution of this code might take a bit of study, but the key is the queue of pending\nmessages. Essentially, the scheduler runs as long as there are messages to deliver. A\nremarkable feature is that the counter generator sends messages to itself and ends up\nin a recursive cycle not bound by Python’s recursion limit.\nHere is an advanced example showing the use of generators to implement a concurrent\nnetwork application:\nfrom collections import deque\nfrom select import select\n# This class represents a generic yield event in the scheduler\nclass YieldEvent:\n    def handle_yield(self, sched, task):\n        pass\n    def handle_resume(self, sched, task):\n        pass\n# Task Scheduler\nclass Scheduler:\n    def __init__(self):\n        self._numtasks = 0       # Total num of tasks\n        self._ready = deque()    # Tasks ready to run\n        self._read_waiting = {}  # Tasks waiting to read\n        self._write_waiting = {} # Tasks waiting to write\n    # Poll for I/O events and restart waiting tasks\n    def _iopoll(self):\n        rset,wset,eset = select(self._read_waiting,\n                                self._write_waiting,[])\n        for r in rset:\n            evt, task = self._read_waiting.pop(r)\n            evt.handle_resume(self, task)\n        for w in wset:\n            evt, task = self._write_waiting.pop(w)\n            evt.handle_resume(self, task)\n    def new(self,task):\n        '''\n        Add a newly started task to the scheduler\n        '''\n12.12. Using Generators As an Alternative to Threads \n| \n527",
      "content_length": 1809,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": 12,
      "content": "self._ready.append((task, None))\n        self._numtasks += 1\n    def add_ready(self, task, msg=None):\n        '''\n        Append an already started task to the ready queue.\n        msg is what to send into the task when it resumes.\n        '''\n        self._ready.append((task, msg))\n    # Add a task to the reading set\n    def _read_wait(self, fileno, evt, task):\n        self._read_waiting[fileno] = (evt, task)\n    # Add a task to the write set\n    def _write_wait(self, fileno, evt, task):\n        self._write_waiting[fileno] = (evt, task)\n    def run(self):\n        '''\n        Run the task scheduler until there are no tasks\n        '''\n        while self._numtasks:\n             if not self._ready:\n                  self._iopoll()\n             task, msg = self._ready.popleft()\n             try:\n                 # Run the coroutine to the next yield\n                 r = task.send(msg)\n                 if isinstance(r, YieldEvent):\n                     r.handle_yield(self, task)\n                 else:\n                     raise RuntimeError('unrecognized yield event')\n             except StopIteration:\n                 self._numtasks -= 1\n# Example implementation of coroutine-based socket I/O\nclass ReadSocket(YieldEvent):\n    def __init__(self, sock, nbytes):\n        self.sock = sock\n        self.nbytes = nbytes\n    def handle_yield(self, sched, task):\n        sched._read_wait(self.sock.fileno(), self, task)\n    def handle_resume(self, sched, task):\n        data = self.sock.recv(self.nbytes)\n        sched.add_ready(task, data)\nclass WriteSocket(YieldEvent):\n    def __init__(self, sock, data):\n        self.sock = sock\n        self.data = data\n    def handle_yield(self, sched, task):\n528 \n| \nChapter 12: Concurrency",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": 12,
      "content": "sched._write_wait(self.sock.fileno(), self, task)\n    def handle_resume(self, sched, task):\n        nsent = self.sock.send(self.data)\n        sched.add_ready(task, nsent)\nclass AcceptSocket(YieldEvent):\n    def __init__(self, sock):\n        self.sock = sock\n    def handle_yield(self, sched, task):\n        sched._read_wait(self.sock.fileno(), self, task)\n    def handle_resume(self, sched, task):\n        r = self.sock.accept()\n        sched.add_ready(task, r)\n# Wrapper around a socket object for use with yield\nclass Socket(object):\n    def __init__(self, sock):\n        self._sock = sock\n    def recv(self, maxbytes):\n        return ReadSocket(self._sock, maxbytes)\n    def send(self, data):\n        return WriteSocket(self._sock, data)\n    def accept(self):\n        return AcceptSocket(self._sock)\n    def __getattr__(self, name):\n        return getattr(self._sock, name)\nif __name__ == '__main__':\n    from socket import socket, AF_INET, SOCK_STREAM\n    import time\n    # Example of a function involving generators.  This should\n    # be called using line = yield from readline(sock)\n    def readline(sock):\n        chars = []\n        while True:\n            c = yield sock.recv(1)\n            if not c:\n                break\n            chars.append(c)\n            if c == b'\\n':\n                break\n        return b''.join(chars)\n    # Echo server using generators\n    class EchoServer:\n        def __init__(self,addr,sched):\n            self.sched = sched\n            sched.new(self.server_loop(addr))\n        def server_loop(self,addr):\n            s = Socket(socket(AF_INET,SOCK_STREAM))\n12.12. Using Generators As an Alternative to Threads \n| \n529",
      "content_length": 1661,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": 12,
      "content": "s.bind(addr)\n            s.listen(5)\n            while True:\n                c,a = yield s.accept()\n                print('Got connection from ', a)\n                self.sched.new(self.client_handler(Socket(c)))\n        def client_handler(self,client):\n            while True:\n                line = yield from readline(client)\n                if not line:\n                    break\n                line = b'GOT:' + line\n                while line:\n                    nsent = yield client.send(line)\n                    line = line[nsent:]\n            client.close()\n            print('Client closed')\n    sched = Scheduler()\n    EchoServer(('',16000),sched)\n    sched.run()\nThis code will undoubtedly require a certain amount of careful study. However, it is\nessentially implementing a small operating system. There is a queue of tasks ready to\nrun and there are waiting areas for tasks sleeping for I/O. Much of the scheduler involves\nmoving tasks between the ready queue and the I/O waiting area.\nDiscussion\nWhen building generator-based concurrency frameworks, it is most common to work\nwith the more general form of yield:\ndef some_generator():\n    ...\n    result = yield data\n    ...\nFunctions that use yield in this manner are more generally referred to as “coroutines.”\nWithin a scheduler, the yield statement gets handled in a loop as follows:\nf = some_generator()\n# Initial result. Is None to start since nothing has been computed\nresult = None\nwhile True:\n    try:\n        data = f.send(result)\n        result = ... do some calculation ...\n    except StopIteration:\n        break\n530 \n| \nChapter 12: Concurrency",
      "content_length": 1623,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": 12,
      "content": "The logic concerning the result is a bit convoluted. However, the value passed to send()\ndefines what gets returned when the yield statement wakes back up. So, if a yield is\ngoing to return a result in response to data that was previously yielded, it gets returned\non the next send() operation. If a generator function has just started, sending in a value\nof None simply makes it advance to the first yield statement.\nIn addition to sending in values, it is also possible to execute a close() method on a\ngenerator. This causes a silent GeneratorExit exception to be raised at the yield state‐\nment, which stops execution. If desired, a generator can catch this exception and per‐\nform cleanup actions. It’s also possible to use the throw() method of a generator to raise\nan arbitrary execution at the yield statement. A task scheduler might use this to com‐\nmunicate errors into running generators.\nThe yield from statement used in the last example is used to implement coroutines\nthat serve as subroutines or procedures to be called from other generators. Essentially,\ncontrol transparently transfers to the new function. Unlike normal generators, a func‐\ntion that is called using yield from can return a value that becomes the result of the\nyield from statement. More information about yield from can be found in PEP 380.\nFinally, if programming with generators, it is important to stress that there are some\nmajor limitations. In particular, you get none of the benefits that threads provide. For\ninstance, if you execute any code that is CPU bound or which blocks for I/O, it will\nsuspend the entire task scheduler until the completion of that operation. To work around\nthis, your only real option is to delegate the operation to a separate thread or process\nwhere it can run independently. Another limitation is that most Python libraries have\nnot been written to work well with generator-based threading. If you take this approach,\nyou may find that you need to write replacements for many standard library functions.\nAs basic background on coroutines and the techniques utilized in this recipe, see PEP\n342 and “A Curious Course on Coroutines and Concurrency”.\nPEP 3156 also has a modern take on asynchronous I/O involving coroutines. In practice,\nit is extremelyunlikely that you will write a low-level coroutine scheduler yourself.\nHowever, ideas surrounding coroutines are the basis for many popular libraries, in‐\ncluding gevent, greenlet, Stackless Python, and similar projects.\n12.13. Polling Multiple Thread Queues\nProblem\nYou have a collection of thread queues, and you would like to be able to poll them for\nincoming items, much in the same way as you might poll a collection of network con‐\nnections for incoming data.\n12.13. Polling Multiple Thread Queues \n| \n531",
      "content_length": 2783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": 12,
      "content": "Solution\nA common solution to polling problems involves a little-known trick involving a hidden\nloopback network connection. Essentially, the idea is as follows: for each queue (or any\nobject) that you want to poll, you create a pair of connected sockets. You then write on\none of the sockets to signal the presence of data. The other sockect is then passed to\nselect() or a similar function to poll for the arrival of data. Here is some sample code\nthat illustrates this idea:\nimport queue\nimport socket\nimport os\nclass PollableQueue(queue.Queue):\n    def __init__(self):\n        super().__init__()\n        # Create a pair of connected sockets\n        if os.name == 'posix':\n            self._putsocket, self._getsocket = socket.socketpair()\n        else:\n            # Compatibility on non-POSIX systems\n            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            server.bind(('127.0.0.1', 0))\n            server.listen(1)\n            self._putsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self._putsocket.connect(server.getsockname())\n            self._getsocket, _ = server.accept()\n            server.close()\n    def fileno(self):\n        return self._getsocket.fileno()\n    def put(self, item):\n        super().put(item)\n        self._putsocket.send(b'x')\n    def get(self):\n        self._getsocket.recv(1)\n        return super().get()\nIn this code, a new kind of Queue instance is defined where there is an underlying pair\nof connected sockets. The socketpair() function on Unix machines can establish such\nsockets easily. On Windows, you have to fake it using code similar to that shown (it\nlooks a bit weird, but a server socket is created and a client immediately connects to it\nafterward). The normal get() and put() methods are then redefined slightly to perform\na small bit of I/O on these sockets. The put() method writes a single byte of data to one\nof the sockets after putting data on the queue. The get() method reads a single byte of\ndata from the other socket when removing an item from the queue.\n532 \n| \nChapter 12: Concurrency",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": 12,
      "content": "The fileno() method is what makes the queue pollable using a function such as se\nlect(). Essentially, it just exposes the underlying file descriptor of the socket used by\nthe get() function.\nHere is an example of some code that defines a consumer which monitors multiple\nqueues for incoming items:\nimport select\nimport threading\ndef consumer(queues):\n    '''\n    Consumer that reads data on multiple queues simultaneously\n    '''\n    while True:\n        can_read, _, _ = select.select(queues,[],[])\n        for r in can_read:\n            item = r.get()\n            print('Got:', item)\nq1 = PollableQueue()\nq2 = PollableQueue()\nq3 = PollableQueue()\nt = threading.Thread(target=consumer, args=([q1,q2,q3],))\nt.daemon = True\nt.start()\n# Feed data to the queues\nq1.put(1)\nq2.put(10)\nq3.put('hello')\nq2.put(15)\n...\nIf you try it, you’ll find that the consumer indeed receives all of the put items, regardless\nof which queues they are placed in.\nDiscussion\nThe problem of polling non-file-like objects, such as queues, is often a lot trickier than\nit looks. For instance, if you don’t use the socket technique shown, your only option is\nto write code that cycles through the queues and uses a timer, like this:\nimport time\ndef consumer(queues):\n    while True:\n        for q in queues:\n            if not q.empty():\n                item = q.get()\n                print('Got:', item)\n12.13. Polling Multiple Thread Queues \n| \n533",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": 12,
      "content": "# Sleep briefly to avoid 100% CPU\n        time.sleep(0.01)\nThis might work for certain kinds of problems, but it’s clumsy and introduces other\nweird performance problems. For example, if new data is added to a queue, it won’t be\ndetected for as long as 10 milliseconds (an eternity on a modern processor).\nYou run into even further problems if the preceding polling is mixed with the polling\nof other objects, such as network sockets. For example, if you want to poll both sockets\nand queues at the same time, you might have to use code like this:\nimport select\ndef event_loop(sockets, queues):\n    while True:\n        # polling with a timeout\n        can_read, _, _ = select.select(sockets, [], [], 0.01)\n        for r in can_read:\n            handle_read(r)\n        for q in queues:\n            if not q.empty():\n                item = q.get()\n                print('Got:', item)\nThe solution shown solves a lot of these problems by simply putting queues on equal\nstatus with sockets. A single select() call can be used to poll for activity on both. It is\nnot necessary to use timeouts or other time-based hacks to periodically check. More‐\nover, if data gets added to a queue, the consumer will be notified almost instantaneously.\nAlthough there is a tiny amount of overhead associated with the underlying I/O, it often\nis worth it to have better response time and simplified coding.\n12.14. Launching a Daemon Process on Unix\nProblem\nYou would like to write a program that runs as a proper daemon process on Unix or\nUnix-like systems.\nSolution\nCreating a proper daemon process requires a precise sequence of system calls and careful\nattention to detail. The following code shows how to define a daemon process along\nwith the ability to easily stop it once launched:\n#!/usr/bin/env python3\n# daemon.py\nimport os\nimport sys\n534 \n| \nChapter 12: Concurrency",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": 12,
      "content": "import atexit\nimport signal\ndef daemonize(pidfile, *, stdin='/dev/null',\n                          stdout='/dev/null',\n                          stderr='/dev/null'):\n    if os.path.exists(pidfile):\n        raise RuntimeError('Already running')\n    # First fork (detaches from parent)\n    try:\n        if os.fork() > 0:\n            raise SystemExit(0)   # Parent exit\n    except OSError as e:\n        raise RuntimeError('fork #1 failed.')\n    os.chdir('/')\n    os.umask(0)\n    os.setsid()\n    # Second fork (relinquish session leadership)\n    try:\n        if os.fork() > 0:\n            raise SystemExit(0)\n    except OSError as e:\n        raise RuntimeError('fork #2 failed.')\n    # Flush I/O buffers\n    sys.stdout.flush()\n    sys.stderr.flush()\n    # Replace file descriptors for stdin, stdout, and stderr\n    with open(stdin, 'rb', 0) as f:\n        os.dup2(f.fileno(), sys.stdin.fileno())\n    with open(stdout, 'ab', 0) as f:\n        os.dup2(f.fileno(), sys.stdout.fileno())\n    with open(stderr, 'ab', 0) as f:\n        os.dup2(f.fileno(), sys.stderr.fileno())\n    # Write the PID file\n    with open(pidfile,'w') as f:\n        print(os.getpid(),file=f)\n    # Arrange to have the PID file removed on exit/signal\n    atexit.register(lambda: os.remove(pidfile))\n    # Signal handler for termination (required)\n    def sigterm_handler(signo, frame):\n        raise SystemExit(1)\n    signal.signal(signal.SIGTERM, sigterm_handler)\n12.14. Launching a Daemon Process on Unix \n| \n535",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": 12,
      "content": "def main():\n    import time\n    sys.stdout.write('Daemon started with pid {}\\n'.format(os.getpid()))\n    while True:\n        sys.stdout.write('Daemon Alive! {}\\n'.format(time.ctime()))\n        time.sleep(10)\nif __name__ == '__main__':\n    PIDFILE = '/tmp/daemon.pid'\n    if len(sys.argv) != 2:\n        print('Usage: {} [start|stop]'.format(sys.argv[0]), file=sys.stderr)\n        raise SystemExit(1)\n    if sys.argv[1] == 'start':\n        try:\n            daemonize(PIDFILE,\n                      stdout='/tmp/daemon.log',\n                      stderr='/tmp/dameon.log')\n        except RuntimeError as e:\n            print(e, file=sys.stderr)\n            raise SystemExit(1)\n        main()\n    elif sys.argv[1] == 'stop':\n        if os.path.exists(PIDFILE):\n            with open(PIDFILE) as f:\n                os.kill(int(f.read()), signal.SIGTERM)\n        else:\n            print('Not running', file=sys.stderr)\n            raise SystemExit(1)\n    else:\n        print('Unknown command {!r}'.format(sys.argv[1]), file=sys.stderr)\n        raise SystemExit(1)\nTo launch the daemon, the user would use a command like this:\nbash % daemon.py start\nbash % cat /tmp/daemon.pid\n2882\nbash % tail -f /tmp/daemon.log\nDaemon started with pid 2882\nDaemon Alive! Fri Oct 12 13:45:37 2012\nDaemon Alive! Fri Oct 12 13:45:47 2012\n...\nDaemon processes run entirely in the background, so the command returns immedi‐\nately. However, you can view its associated pid file and log, as just shown. To stop the\ndaemon, use:\n536 \n| \nChapter 12: Concurrency",
      "content_length": 1530,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": 12,
      "content": "bash % daemon.py stop\nbash %\nDiscussion\nThis recipe defines a function daemonize() that should be called at program startup to\nmake the program run as a daemon. The signature to daemonize() is using keyword-\nonly arguments to make the purpose of the optional arguments more clear when used.\nThis forces the user to use a call such as this:\ndaemonize('daemon.pid',\n          stdin='/dev/null,\n          stdout='/tmp/daemon.log',\n          stderr='/tmp/daemon.log')\nAs opposed to a more cryptic call such as:\n# Illegal. Must use keyword arguments\ndaemonize('daemon.pid',\n          '/dev/null', '/tmp/daemon.log','/tmp/daemon.log')\nThe steps involved in creating a daemon are fairly cryptic, but the general idea is as\nfollows. First, a daemon has to detach itself from its parent process. This is the purpose\nof the first os.fork() operation and immediate termination by the parent.\nAfter the child has been orphaned, the call to os.setsid() creates an entirely new\nprocess session and sets the child as the leader. This also sets the child as the leader of\na new process group and makes sure there is no controlling terminal. If this all sounds\na bit too magical, it has to do with getting the daemon to detach properly from the\nterminal and making sure that things like signals don’t interfere with its operation.\nThe calls to os.chdir() and os.umask(0) change the current working directory and\nreset the file mode mask. Changing the directory is usually a good idea so that the\ndaemon is no longer working in the directory from which it was launched.\nThe second call to os.fork() is by far the more mysterious operation here. This step\nmakes the daemon process give up the ability to acquire a new controlling terminal and\nprovides even more isolation (essentially, the daemon gives up its session leadership\nand thus no longer has the permission to open controlling terminals). Although you\ncould probably omit this step, it’s typically recommended.\nOnce the daemon process has been properly detached, it performs steps to reinitialize\nthe standard I/O streams to point at files specified by the user. This part is actually\nsomewhat tricky. References to file objects associated with the standard I/O streams are\nfound in multiple places in the interpreter (sys.stdout, sys.__stdout__, etc.). Simply\nclosing sys.stdout and reassigning it is not likely to work correctly, because there’s no\nway to know if it will fix all uses of sys.stdout. Instead, a separate file object is opened,\nand the os.dup2() call is used to have it replace the file descriptor currently being used\n12.14. Launching a Daemon Process on Unix \n| \n537",
      "content_length": 2626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": 12,
      "content": "by sys.stdout. When this happens, the original file for sys.stdout will be closed and\nthe new one takes its place. It must be emphasized that any file encoding or text handling\nalready applied to the standard I/O streams will remain in place.\nA common practice with daemon processes is to write the process ID of the daemon in\na file for later use by other programs. The last part of the daemonize() function writes\nthis file, but also arranges to have the file removed on program termination. The atex\nit.register() function registers a function to execute when the Python interpreter\nterminates. The definition of a signal handler for SIGTERM is also required for a graceful\ntermination. The signal handler merely raises SystemExit() and nothing more. This\nmight look unnecessary, but without it, termination signals kill the interpreter without\nperforming the cleanup actions registered with atexit.register(). An example of\ncode that kills the daemon can be found in the handling of the stop command at the\nend of the program.\nMore information about writing daemon processes can be found in Advanced Pro‐\ngramming in the UNIX Environment, 2nd Edition, by W. Richard Stevens and Stephen\nA. Rago (Addison-Wesley, 2005). Although focused on C programming, all of the ma‐\nterial is easily adapted to Python, since all of the required POSIX functions are available\nin the standard library.\n538 \n| \nChapter 12: Concurrency",
      "content_length": 1420,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": 12,
      "content": "CHAPTER 13\nUtility Scripting and System Administration\nA lot of people use Python as a replacement for shell scripts, using it to automate com‐\nmon system tasks, such as manipulating files, configuring systems, and so forth. The\nmain goal of this chapter is to describe features related to common tasks encountered\nwhen writing scripts. For example, parsing command-line options, manipulating files\non the filesystem, getting useful system configuration data, and so forth. Chapter 5 also\ncontains general information related to files and directories.\n13.1. Accepting Script Input via Redirection, Pipes, or\nInput Files\nProblem\nYou want a script you’ve written to be able to accept input using whatever mechanism\nis easiest for the user. This should include piping output from a command to the script,\nredirecting a file into the script, or just passing a filename, or list of filenames, to the\nscript on the command line.\nSolution\nPython’s built-in fileinput module makes this very simple and concise. If you have a\nscript that looks like this:\n#!/usr/bin/env python3\nimport fileinput\nwith fileinput.input() as f_input:\n    for line in f_input:\n        print(line, end='')\n539",
      "content_length": 1177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": 12,
      "content": "Then you can already accept input to the script in all of the previously mentioned ways.\nIf you save this script as filein.py and make it executable, you can do all of the following\nand get the expected output:\n$ ls | ./filein.py          # Prints a directory listing to stdout.\n$ ./filein.py /etc/passwd   # Reads /etc/passwd to stdout.\n$ ./filein.py < /etc/passwd # Reads /etc/passwd to stdout.\nDiscussion\nThe fileinput.input() function creates and returns an instance of the FileInput\nclass. In addition to containing a few handy helper methods, the instance can also be\nused as a context manager. So, to put all of this together, if we wrote a script that expected\nto be printing output from several files at once, we might have it include the filename\nand line number in the output, like this:\n>>> import fileinput\n>>> with fileinput.input('/etc/passwd') as f:\n>>>     for line in f:\n...         print(f.filename(), f.lineno(), line, end='')\n...\n/etc/passwd 1 ##\n/etc/passwd 2 # User Database\n/etc/passwd 3 #\n<other output omitted>\nUsing it as a context manager ensures that the file is closed when it’s no longer being\nused, and we leveraged a few handy FileInput helper methods here to get some extra\ninformation in the output.\n13.2. Terminating a Program with an Error Message\nProblem\nYou want your program to terminate by printing a message to standard error and re‐\nturning a nonzero status code.\nSolution\nTo have a program terminate in this manner, raise a SystemExit exception, but supply\nthe error message as an argument. For example:\nraise SystemExit('It failed!')\nThis will cause the supplied message to be printed to sys.stderr and the program to\nexit with a status code of 1.\n540 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1756,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": 12,
      "content": "Discussion\nThis is a small recipe, but it solves a common problem that arises when writing scripts.\nNamely, to terminate a program, you might be inclined to write code like this:\nimport sys\nsys.stderr.write('It failed!\\n')\nraise SystemExit(1)\nNone of the extra steps involving import or writing to sys.stderr are neccessary if you\nsimply supply the message to SystemExit() instead.\n13.3. Parsing Command-Line Options\nProblem\nYou want to write a program that parses options supplied on the command line (found\nin sys.argv).\nSolution\nThe argparse module can be used to parse command-line options. A simple example\nwill help to illustrate the essential features:\n# search.py\n'''\nHypothetical command-line tool for searching a collection of\nfiles for one or more text patterns.\n'''\nimport argparse\nparser = argparse.ArgumentParser(description='Search some files')\nparser.add_argument(dest='filenames',metavar='filename', nargs='*')\nparser.add_argument('-p', '--pat',metavar='pattern', required=True,\n                    dest='patterns', action='append',\n                    help='text pattern to search for')\nparser.add_argument('-v', dest='verbose', action='store_true',\n                    help='verbose mode')\nparser.add_argument('-o', dest='outfile', action='store',\n                    help='output file')\nparser.add_argument('--speed', dest='speed', action='store',\n                    choices={'slow','fast'}, default='slow',\n                    help='search speed')\nargs = parser.parse_args()\n13.3. Parsing Command-Line Options \n| \n541",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": 12,
      "content": "# Output the collected arguments\nprint(args.filenames)\nprint(args.patterns)\nprint(args.verbose)\nprint(args.outfile)\nprint(args.speed)\nThis program defines a command-line parser with the following usage:\nbash % python3 search.py -h\nusage: search.py [-h] [-p pattern] [-v] [-o OUTFILE] [--speed {slow,fast}]\n                 [filename [filename ...]]\nSearch some files\npositional arguments:\n  filename\noptional arguments:\n  -h, --help            show this help message and exit\n  -p pattern, --pat pattern\n                        text pattern to search for\n  -v                    verbose mode\n  -o OUTFILE            output file\n  --speed {slow,fast}   search speed\nThe following session shows how data shows up in the program. Carefully observe the\noutput of the print() statements.\nbash % python3 search.py foo.txt bar.txt\nusage: search.py [-h] -p pattern [-v] [-o OUTFILE] [--speed {fast,slow}]\n                 [filename [filename ...]]\nsearch.py: error: the following arguments are required: -p/--pat\nbash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt\nfilenames = ['foo.txt', 'bar.txt']\npatterns  = ['spam', 'eggs']\nverbose   = True\noutfile   = None\nspeed     = slow\nbash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt -o results\nfilenames = ['foo.txt', 'bar.txt']\npatterns  = ['spam', 'eggs']\nverbose   = True\noutfile   = results\nspeed     = slow\nbash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt -o results \\\n             --speed=fast\nfilenames = ['foo.txt', 'bar.txt']\npatterns  = ['spam', 'eggs']\nverbose   = True\noutfile   = results\nspeed     = fast\n542 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": 13,
      "content": "Further processing of the options is up to the program. Replace the print() functions\nwith something more interesting.\nDiscussion\nThe argparse module is one of the largest modules in the standard library, and has a\nhuge number of configuration options. This recipe shows an essential subset that can\nbe used and extended to get started.\nTo parse options, you first create an ArgumentParser instance and add declarations for\nthe options you want to support it using the add_argument() method. In each add_ar\ngument() call, the dest argument specifies the name of an attribute where the result of\nparsing will be placed. The metavar argument is used when generating help messages.\nThe action argument specifies the processing associated with the argument and is often\nstore for storing a value or append for collecting multiple argument values into a list.\nThe following argument collects all of the extra command-line arguments into a list. It’s\nbeing used to make a list of filenames in the example:\nparser.add_argument(dest='filenames',metavar='filename', nargs='*')\nThe following argument sets a Boolean flag depending on whether or not the argument\nwas provided:\nparser.add_argument('-v', dest='verbose', action='store_true',\n                    help='verbose mode')\nThe following argument takes a single value and stores it as a string:\nparser.add_argument('-o', dest='outfile', action='store',\n                    help='output file')\nThe following argument specification allows an argument to be repeated multiple times\nand all of the values append into a list. The required flag means that the argument must\nbe supplied at least once. The use of -p and --pat mean that either argument name is\nacceptable.\nparser.add_argument('-p', '--pat',metavar='pattern', required=True,\n                    dest='patterns', action='append',\n                    help='text pattern to search for')\nFinally, the following argument specification takes a value, but checks it against a set of\npossible choices.\nparser.add_argument('--speed', dest='speed', action='store',\n                    choices={'slow','fast'}, default='slow',\n                    help='search speed')\nOnce the options have been given, you simply execute the parser.parse() method.\nThis will process the sys.argv value and return an instance with the results. The results\n13.3. Parsing Command-Line Options \n| \n543",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": 13,
      "content": "for each argument are placed into an attribute with the name given in the dest parameter\nto add_argument().\nThere are several other approaches for parsing command-line options. For example,\nyou might be inclined to manually process sys.argv yourself or use the getopt module\n(which is modeled after a similarly named C library). However, if you take this approach,\nyou’ll simply end up replicating much of the code that argparse already provides. You\nmay also encounter code that uses the optparse library to parse options. Although\noptparse is very similar to argparse, the latter is more modern and should be preferred\nin new projects.\n13.4. Prompting for a Password at Runtime\nProblem\nYou’ve written a script that requires a password, but since the script is meant for inter‐\nactive use, you’d like to prompt the user for a password rather than hardcode it into the\nscript.\nSolution\nPython’s getpass module is precisely what you need in this situation. It will allow you\nto very easily prompt for a password without having the keyed-in password displayed\non the user’s terminal. Here’s how it’s done:\nimport getpass\nuser = getpass.getuser()\npasswd = getpass.getpass()\nif svc_login(user, passwd):    # You must write svc_login()\n   print('Yay!')\nelse:\n   print('Boo!')\nIn this code, the svc_login() function is code that you must write to further process\nthe password entry. Obviously, the exact handling is application-specific.\nDiscussion\nNote in the preceding code that getpass.getuser() doesn’t prompt the user for their\nusername. Instead, it uses the current user’s login name, according to the user’s shell\nenvironment, or as a last resort, according to the local system’s password database (on\nplatforms that support the pwd module).\n544 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": 13,
      "content": "If you want to explicitly prompt the user for their username, which can be more reliable,\nuse the built-in input function:\nuser = input('Enter your username: ')\nIt’s also important to remember that some systems may not support the hiding of the\ntyped password input to the getpass() method. In this case, Python does all it can to\nforewarn you of problems (i.e., it alerts you that passwords will be shown in cleartext)\nbefore moving on.\n13.5. Getting the Terminal Size\nProblem\nYou need to get the terminal size in order to properly format the output of your program.\nSolution\nUse the os.get_terminal_size() function to do this:\n>>> import os\n>>> sz = os.get_terminal_size()\n>>> sz\nos.terminal_size(columns=80, lines=24)\n>>> sz.columns\n80\n>>> sz.lines\n24\n>>>\nDiscussion\nThere are many other possible approaches for obtaining the terminal size, ranging from\nreading environment variables to executing low-level system calls involving ioctl()\nand TTYs. Frankly, why would you bother with that when this one simple call will\nsuffice?\n13.6. Executing an External Command and Getting Its\nOutput\nProblem\nYou want to execute an external command and collect its output as a Python string.\n13.5. Getting the Terminal Size \n| \n545",
      "content_length": 1220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": 13,
      "content": "Solution\nUse the subprocess.check_output() function. For example:\nimport subprocess\nout_bytes = subprocess.check_output(['netstat','-a'])\nThis runs the specified command and returns its output as a byte string. If you need to\ninterpret the resulting bytes as text, add a further decoding step. For example:\nout_text = out_bytes.decode('utf-8')\nIf the executed command returns a nonzero exit code, an exception is raised. Here is\nan example of catching errors and getting the output created along with the exit code:\ntry:\n    out_bytes = subprocess.check_output(['cmd','arg1','arg2'])\nexcept subprocess.CalledProcessError as e:\n    out_bytes = e.output       # Output generated before error\n    code      = e.returncode   # Return code\nBy default, check_output() only returns output written to standard output. If you want\nboth standard output and error collected, use the stderr argument:\nout_bytes = subprocess.check_output(['cmd','arg1','arg2'],\n                                    stderr=subprocess.STDOUT)\nIf you need to execute a command with a timeout, use the timeout argument:\ntry:\n    out_bytes = subprocess.check_output(['cmd','arg1','arg2'], timeout=5)\nexcept subprocess.TimeoutExpired as e:\n    ...\nNormally, commands are executed without the assistance of an underlying shell (e.g.,\nsh, bash, etc.). Instead, the list of strings supplied are given to a low-level system com‐\nmand, such as os.execve(). If you want the command to be interpreted by a shell,\nsupply it using a simple string and give the shell=True argument. This is sometimes\nuseful if you’re trying to get Python to execute a complicated shell command involving\npipes, I/O redirection, and other features. For example:\nout_bytes = subprocess.check_output('grep python | wc > out', shell=True)\nBe aware that executing commands under the shell is a potential security risk if argu‐\nments are derived from user input. The shlex.quote() function can be used to properly\nquote arguments for inclusion in shell commands in this case.\nDiscussion\nThe check_output() function is the easiest way to execute an external command and\nget its output. However, if you need to perform more advanced communication with a\n546 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": 13,
      "content": "subprocess, such as sending it input, you’ll need to take a difference approach. For that,\nuse the subprocess.Popen class directly. For example:\nimport subprocess\n# Some text to send\ntext = b'''\nhello world\nthis is a test\ngoodbye\n'''\n# Launch a command with pipes\np = subprocess.Popen(['wc'],\n          stdout = subprocess.PIPE,\n          stdin = subprocess.PIPE)\n# Send the data and get the output\nstdout, stderr = p.communicate(text)\n# To interpret as text, decode\nout = stdout.decode('utf-8')\nerr = stderr.decode('utf-8')\nThe subprocess module is not suitable for communicating with external commands\nthat expect to interact with a proper TTY. For example, you can’t use it to automate tasks\nthat ask the user to enter a password (e.g., a ssh session). For that, you would need to\nturn to a third-party module, such as those based on the popular “expect” family of tools\n(e.g., pexpect or similar).\n13.7. Copying or Moving Files and Directories\nProblem\nYou need to copy or move files and directories around, but you don’t want to do it by\ncalling out to shell commands.\nSolution\nThe shutil module has portable implementations of functions for copying files and\ndirectories. The usage is extremely straightforward. For example:\nimport shutil\n# Copy src to dst. (cp src dst)\nshutil.copy(src, dst)\n# Copy files, but preserve metadata (cp -p src dst)\nshutil.copy2(src, dst)\n13.7. Copying or Moving Files and Directories \n| \n547",
      "content_length": 1426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": 13,
      "content": "# Copy directory tree (cp -R src dst)\nshutil.copytree(src, dst)\n# Move src to dst (mv src dst)\nshutil.move(src, dst)\nThe arguments to these functions are all strings supplying file or directory names. The\nunderlying semantics try to emulate that of similar Unix commands, as shown in the\ncomments.\nBy default, symbolic links are followed by these commands. For example, if the source\nfile is a symbolic link, then the destination file will be a copy of the file the link points\nto. If you want to copy the symbolic link instead, supply the follow_symlinks keyword\nargument like this:\nshutil.copy2(src, dst, follow_symlinks=False)\nIf you want to preserve symbolic links in copied directories, do this:\nshutil.copytree(src, dst, symlinks=True)\nThe copytree() optionally allows you to ignore certain files and directories during the\ncopy process. To do this, you supply an ignore function that takes a directory name\nand filename listing as input, and returns a list of names to ignore as a result. For ex‐\nample:\ndef ignore_pyc_files(dirname, filenames):\n    return [name in filenames if name.endswith('.pyc')]\nshutil.copytree(src, dst, ignore=ignore_pyc_files)\nSince ignoring filename patterns is common, a utility function ignore_patterns() has\nalready been provided to do it. For example:\nshutil.copytree(src, dst, ignore=shutil.ignore_patterns('*~','*.pyc'))\nDiscussion\nUsing shutil to copy files and directories is mostly straightforward. However, one\ncaution concerning file metadata is that functions such as copy2() only make a best\neffort in preserving this data. Basic information, such as access times, creation times,\nand permissions, will always be preserved, but preservation of owners, ACLs, resource\nforks, and other extended file metadata may or may not work depending on the un‐\nderlying operating system and the user’s own access permissions. You probably wouldn’t\nwant to use a function like shutil.copytree() to perform system backups.\nWhen working with filenames, make sure you use the functions in os.path for the\ngreatest portability (especially if working with both Unix and Windows). For example:\n548 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 2184,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": 13,
      "content": ">>> filename = '/Users/guido/programs/spam.py'\n>>> import os.path\n>>> os.path.basename(filename)\n'spam.py'\n>>> os.path.dirname(filename)\n'/Users/guido/programs'\n>>> os.path.split(filename)\n('/Users/guido/programs', 'spam.py')\n>>> os.path.join('/new/dir', os.path.basename(filename))\n'/new/dir/spam.py'\n>>> os.path.expanduser('~/guido/programs/spam.py')\n'/Users/guido/programs/spam.py'\n>>>\nOne tricky bit about copying directories with copytree() is the handling of errors. For\nexample, in the process of copying, the function might encounter broken symbolic links,\nfiles that can’t be accessed due to permission problems, and so on. To deal with this, all\nexceptions encountered are collected into a list and grouped into a single exception that\ngets raised at the end of the operation. Here is how you would handle it:\ntry:\n    shutil.copytree(src, dst)\nexcept shutil.Error as e:\n    for src, dst, msg in e.args[0]:\n         # src is source name\n         # dst is destination name\n         # msg is error message from exception\n         print(dst, src, msg)\nIf you supply the ignore_dangling_symlinks=True keyword argument, then copy\ntree() will ignore dangling symlinks.\nThe functions shown in this recipe are probably the most commonly used. However,\nshutil has many more operations related to copying data. The documentation is def‐\ninitely worth a further look. See the Python documentation.\n13.8. Creating and Unpacking Archives\nProblem\nYou need to create or unpack archives in common formats (e.g., .tar, .tgz, or .zip).\nSolution\nThe shutil module has two functions—make_archive() and unpack_archive()—that\ndo exactly what you want. For example:\n>>> import shutil\n>>> shutil.unpack_archive('Python-3.3.0.tgz')\n13.8. Creating and Unpacking Archives \n| \n549",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": 13,
      "content": ">>> shutil.make_archive('py33','zip','Python-3.3.0')\n'/Users/beazley/Downloads/py33.zip'\n>>>\nThe second argument to make_archive() is the desired output format. To get a list of\nsupported archive formats, use get_archive_formats(). For example:\n>>> shutil.get_archive_formats()\n[('bztar', \"bzip2'ed tar-file\"), ('gztar', \"gzip'ed tar-file\"),\n ('tar', 'uncompressed tar file'), ('zip', 'ZIP file')]\n>>>\nDiscussion\nPython has other library modules for dealing with the low-level details of various archive\nformats (e.g., tarfile, zipfile, gzip, bz2, etc.). However, if all you’re trying to do is\nmake or extract an archive, there’s really no need to go so low level. You can just use\nthese high-level functions in shutil instead.\nThe functions have a variety of additional options for logging, dryruns, file permissions,\nand so forth. Consult the shutil library documentation for further details.\n13.9. Finding Files by Name\nProblem\nYou need to write a script that involves finding files, like a file renaming script or a log\narchiver utility, but you’d rather not have to call shell utilities from within your Python\nscript, or you want to provide specialized behavior not easily available by “shelling out.”\nSolution\nTo search for files, use the os.walk() function, supplying it with the top-level directory.\nHere is an example of a function that finds a specific filename and prints out the full\npath of all matches:\n#!/usr/bin/env python3.3\nimport os\ndef findfile(start, name):\n    for relpath, dirs, files in os.walk(start):\n        if name in files:\n            full_path = os.path.join(start, relpath, name)\n            print(os.path.normpath(os.path.abspath(full_path)))\nif __name__ == '__main__':\n    findfile(sys.argv[1], sys.argv[2])\n550 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": 13,
      "content": "Save this script as findfile.py and run it from the command line, feeding in the starting\npoint and the name as positional arguments, like this:\nbash % ./findfile.py . myfile.txt\nDiscussion\nThe os.walk() method traverses the directory hierarchy for us, and for each directory\nit enters, it returns a 3-tuple, containing the relative path to the directory it’s inspecting,\na list containing all of the directory names in that directory, and a list of filenames in\nthat directory.\nFor each tuple, you simply check if the target filename is in the files list. If it is,\nos.path.join() is used to put together a path. To avoid the possibility of weird looking\npaths like ././foo//bar, two additional functions are used to fix the result. The first is\nos.path.abspath(), which takes a path that might be relative and forms the absolute\npath, and the second is os.path.normpath(), which will normalize the path, thereby\nresolving issues with double slashes, multiple references to the current directory, and \nso on.\nAlthough this script is pretty simple compared to the features of the find utility found\non UNIX platforms, it has the benefit of being cross-platform. Furthermore, a lot of\nadditional functionality can be added in a portable manner without much more work.\nTo illustrate, here is a function that prints out all of the files that have a recent modifi‐\ncation time:\n#!/usr/bin/env python3.3\nimport os\nimport time\ndef modified_within(top, seconds):\n    now = time.time()\n    for path, dirs, files in os.walk(top):\n        for name in files:\n            fullpath = os.path.join(path, name)\n            if os.path.exists(fullpath):\n                mtime = os.path.getmtime(fullpath)\n                if mtime > (now - seconds):\n                    print(fullpath)\nif __name__ == '__main__':\n    import sys\n    if len(sys.argv) != 3:\n        print('Usage: {} dir seconds'.format(sys.argv[0]))\n        raise SystemExit(1)\n    modified_within(sys.argv[1], float(sys.argv[2]))\n13.9. Finding Files by Name \n| \n551",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": 13,
      "content": "It wouldn’t take long for you to build far more complex operations on top of this little\nfunction using various features of the os, os.path, glob, and similar modules. See Rec‐\nipes 5.11 and 5.13 for related recipes.\n13.10. Reading Configuration Files\nProblem\nYou want to read configuration files written in the common .ini configuration file\nformat.\nSolution\nThe configparser module can be used to read configuration files. For example, suppose\nyou have this configuration file:\n; config.ini\n; Sample configuration file\n[installation]\nlibrary=%(prefix)s/lib\ninclude=%(prefix)s/include\nbin=%(prefix)s/bin\nprefix=/usr/local\n# Setting related to debug configuration\n[debug]\nlog_errors=true\nshow_warnings=False\n[server]\nport: 8080\nnworkers: 32\npid-file=/tmp/spam.pid\nroot=/www/root\nsignature:\n    =================================\n    Brought to you by the Python Cookbook\n    =================================\nHere is an example of how to read it and extract values:\n>>> from configparser import ConfigParser\n>>> cfg = ConfigParser()\n>>> cfg.read('config.ini')\n['config.ini']\n>>> cfg.sections()\n['installation', 'debug', 'server']\n>>> cfg.get('installation','library')\n'/usr/local/lib'\n>>> cfg.getboolean('debug','log_errors')\n552 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": 13,
      "content": "True\n>>> cfg.getint('server','port')\n8080\n>>> cfg.getint('server','nworkers')\n32\n>>> print(cfg.get('server','signature'))\n=================================\nBrought to you by the Python Cookbook\n=================================\n>>>\nIf desired, you can also modify the configuration and write it back to a file using the\ncfg.write() method. For example:\n>>> cfg.set('server','port','9000')\n>>> cfg.set('debug','log_errors','False')\n>>> import sys\n>>> cfg.write(sys.stdout)\n[installation]\nlibrary = %(prefix)s/lib\ninclude = %(prefix)s/include\nbin = %(prefix)s/bin\nprefix = /usr/local\n[debug]\nlog_errors = False\nshow_warnings = False\n[server]\nport = 9000\nnworkers = 32\npid-file = /tmp/spam.pid\nroot = /www/root\nsignature =\n          =================================\n          Brought to you by the Python Cookbook\n          =================================\n>>>\nDiscussion\nConfiguration files are well suited as a human-readable format for specifying configu‐\nration data to your program. Within each config file, values are grouped into different\nsections (e.g., “installation,” “debug,” and “server,” in the example). Each section then\nspecifies values for various variables in that section.\nThere are several notable differences between a config file and using a Python source\nfile for the same purpose. First, the syntax is much more permissive and “sloppy.” For\nexample, both of these assignments are equivalent:\n13.10. Reading Configuration Files \n| \n553",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": 13,
      "content": "prefix=/usr/local\nprefix: /usr/local\nThe names used in a config file are also assumed to be case-insensitive. For example:\n>>> cfg.get('installation','PREFIX')\n'/usr/local'\n>>> cfg.get('installation','prefix')\n'/usr/local'\n>>>\nWhen parsing values, methods such as getboolean() look for any reasonable value.\nFor example, these are all equivalent:\n    log_errors = true\n    log_errors = TRUE\n    log_errors = Yes\n    log_errors = 1\nPerhaps the most significant difference between a config file and Python code is that,\nunlike scripts, configuration files are not executed in a top-down manner. Instead, the\nfile is read in its entirety. If variable substitutions are made, they are done after the fact.\nFor example, in this part of the config file, it doesn’t matter that the prefix variable is\nassigned after other variables that happen to use it:\n    [installation]\n    library=%(prefix)s/lib\n    include=%(prefix)s/include\n    bin=%(prefix)s/bin\n    prefix=/usr/local\nAn easily overlooked feature of ConfigParser is that it can read multiple configuration\nfiles together and merge their results into a single configuration. For example, suppose\na user made their own configuration file that looked like this:\n    ; ~/.config.ini\n    [installation]\n    prefix=/Users/beazley/test\n    [debug]\n    log_errors=False\nThis file can be merged with the previous configuration by reading it separately. For\nexample:\n>>> # Previously read configuration\n>>> cfg.get('installation', 'prefix')\n'/usr/local'\n>>> # Merge in user-specific configuration\n>>> import os\n>>> cfg.read(os.path.expanduser('~/.config.ini'))\n['/Users/beazley/.config.ini']\n554 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": 13,
      "content": ">>> cfg.get('installation', 'prefix')\n'/Users/beazley/test'\n>>> cfg.get('installation', 'library')\n'/Users/beazley/test/lib'\n>>> cfg.getboolean('debug', 'log_errors')\nFalse\n>>>\nObserve how the override of the prefix variable affects other related variables, such as\nthe setting of library. This works because variable interpolation is performed as late\nas possible. You can see this by trying the following experiment:\n>>> cfg.get('installation','library')\n'/Users/beazley/test/lib'\n>>> cfg.set('installation','prefix','/tmp/dir')\n>>> cfg.get('installation','library')\n'/tmp/dir/lib'\n>>>\nFinally, it’s important to note that Python does not support the full range of features you\nmight find in an .ini file used by other programs (e.g., applications on Windows). Make\nsure you consult the configparser documentation for the finer details of the syntax\nand supported features. \n13.11. Adding Logging to Simple Scripts\nProblem\nYou want scripts and simple programs to write diagnostic information to log files.\nSolution\nThe easiest way to add logging to simple programs is to use the logging module. For\nexample:\nimport logging\ndef main():\n    # Configure the logging system\n    logging.basicConfig(\n        filename='app.log',\n        level=logging.ERROR\n    )\n    # Variables (to make the calls that follow work)\n    hostname = 'www.python.org'\n    item = 'spam'\n    filename = 'data.csv'\n    mode = 'r'\n13.11. Adding Logging to Simple Scripts \n| \n555",
      "content_length": 1450,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": 13,
      "content": "# Example logging calls (insert into your program)\n    logging.critical('Host %s unknown', hostname)\n    logging.error(\"Couldn't find %r\", item)\n    logging.warning('Feature is deprecated')\n    logging.info('Opening file %r, mode=%r', filename, mode)\n    logging.debug('Got here')\nif __name__ == '__main__':\n    main()\nThe five logging calls (critical(), error(), warning(), info(), debug()) represent\ndifferent severity levels in decreasing order. The level argument to basicConfig() is\na filter. All messages issued at a level lower than this setting will be ignored.\nThe argument to each logging operation is a message string followed by zero or more\narguments. When making the final log message, the % operator is used to format the\nmessage string using the supplied arguments.\nIf you run this program, the contents of the file app.log will be as follows:\n    CRITICAL:root:Host www.python.org unknown\n    ERROR:root:Could not find 'spam'\nIf you want to change the output or level of output, you can change the parameters to\nthe basicConfig() call. For example:\nlogging.basicConfig(\n     filename='app.log',\n     level=logging.WARNING,\n     format='%(levelname)s:%(asctime)s:%(message)s')\nAs a result, the output changes to the following:\n    CRITICAL:2012-11-20 12:27:13,595:Host www.python.org unknown\n    ERROR:2012-11-20 12:27:13,595:Could not find 'spam'\n    WARNING:2012-11-20 12:27:13,595:Feature is deprecated\nAs shown, the logging configuration is hardcoded directly into the program. If you want\nto configure it from a configuration file, change the basicConfig() call to the following:\nimport logging\nimport logging.config\ndef main():\n    # Configure the logging system\n    logging.config.fileConfig('logconfig.ini')\n    ...\n556 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": 13,
      "content": "Now make a configuration file logconfig.ini that looks like this:\n    [loggers]\n    keys=root\n    [handlers]\n    keys=defaultHandler\n    [formatters]\n    keys=defaultFormatter\n    [logger_root]\n    level=INFO\n    handlers=defaultHandler\n    qualname=root\n    [handler_defaultHandler]\n    class=FileHandler\n    formatter=defaultFormatter\n    args=('app.log', 'a')\n    [formatter_defaultFormatter]\n    format=%(levelname)s:%(name)s:%(message)s\nIf you want to make changes to the configuration, you can simply edit the logcon‐\nfig.ini file as appropriate.\nDiscussion\nIgnoring for the moment that there are about a million advanced configuration options\nfor the logging module, this solution is quite sufficient for simple programs and scripts.\nSimply make sure that you execute the basicConfig() call prior to making any logging\ncalls, and your program will generate logging output.\nIf you want the logging messages to route to standard error instead of a file, don’t supply\nany filename information to basicConfig(). For example, simply do this:\nlogging.basicConfig(level=logging.INFO)\nOne subtle aspect of basicConfig() is that it can only be called once in your program.\nIf you later need to change the configuration of the logging module, you need to obtain\nthe root logger and make changes to it directly. For example:\nlogging.getLogger().level = logging.DEBUG\nIt must be emphasized that this recipe only shows a basic use of the logging module.\nThere are significantly more advanced customizations that can be made. An excellent\nresource for such customization is the “Logging Cookbook”.\n13.11. Adding Logging to Simple Scripts \n| \n557",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": 13,
      "content": "13.12. Adding Logging to Libraries\nProblem\nYou would like to add a logging capability to a library, but don’t want it to interfere with\nprograms that don’t use logging.\nSolution\nFor libraries that want to perform logging, you should create a dedicated logger object,\nand initially configure it as follows:\n# somelib.py\nimport logging\nlog = logging.getLogger(__name__)\nlog.addHandler(logging.NullHandler())\n# Example function (for testing)\ndef func():\n    log.critical('A Critical Error!')\n    log.debug('A debug message')\nWith this configuration, no logging will occur by default. For example:\n>>> import somelib\n>>> somelib.func()\n>>>\nHowever, if the logging system gets configured, log messages will start to appear. For\nexample:\n>>> import logging\n>>> logging.basicConfig()\n>>> somelib.func()\nCRITICAL:somelib:A Critical Error!\n>>>\nDiscussion\nLibraries present a special problem for logging, since information about the environ‐\nment in which they are used isn’t known. As a general rule, you should never write\nlibrary code that tries to configure the logging system on its own or which makes as‐\nsumptions about an already existing logging configuration. Thus, you need to take great\ncare to provide isolation.\nThe call to getLogger(__name__) creates a logger module that has the same name as\nthe calling module. Since all modules are unique, this creates a dedicated logger that is\nlikely to be separate from other loggers.\n558 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": 13,
      "content": "The log.addHandler(logging.NullHandler()) operation attaches a null handler to\nthe just created logger object. A null handler ignores all logging messages by default.\nThus, if the library is used and logging is never configured, no messages or warnings\nwill appear.\nOne subtle feature of this recipe is that the logging of individual libraries can be inde‐\npendently configured, regardless of other logging settings. For example, consider the\nfollowing code:\n>>> import logging\n>>> logging.basicConfig(level=logging.ERROR)\n>>> import somelib\n>>> somelib.func()\nCRITICAL:somelib:A Critical Error!\n>>> # Change the logging level for 'somelib' only\n>>> logging.getLogger('somelib').level=logging.DEBUG\n>>> somelib.func()\nCRITICAL:somelib:A Critical Error!\nDEBUG:somelib:A debug message\n>>>\nHere, the root logger has been configured to only output messages at the ERROR level or\nhigher. However, the level of the logger for somelib has been separately configured to\noutput debugging messages. That setting takes precedence over the global setting.\nThe ability to change the logging settings for a single module like this can be a useful\ndebugging tool, since you don’t have to change any of the global logging settings—simply\nchange the level for the one module where you want more output.\nThe “Logging HOWTO” has more information about configuring the logging module\nand other useful tips.\n13.13. Making a Stopwatch Timer\nProblem\nYou want to be able to record the time it takes to perform various tasks.\nSolution\nThe time module contains various functions for performing timing-related functions.\nHowever, it’s often useful to put a higher-level interface on them that mimics a stop\nwatch. For example:\n13.13. Making a Stopwatch Timer \n| \n559",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": 13,
      "content": "import time\nclass Timer:\n    def __init__(self, func=time.perf_counter):\n        self.elapsed = 0.0\n        self._func = func\n        self._start = None\n    def start(self):\n        if self._start is not None:\n            raise RuntimeError('Already started')\n        self._start = self._func()\n    def stop(self):\n        if self._start is None:\n            raise RuntimeError('Not started')\n        end = self._func()\n        self.elapsed += end - self._start\n        self._start = None\n    def reset(self):\n        self.elapsed = 0.0\n    @property\n    def running(self):\n        return self._start is not None\n    def __enter__(self):\n        self.start()\n        return self\n    def __exit__(self, *args):\n        self.stop()\nThis class defines a timer that can be started, stopped, and reset as needed by the user.\nIt keeps track of the total elapsed time in the elapsed attribute. Here is an example that\nshows how it can be used:\ndef countdown(n):\n    while n > 0:\n        n -= 1\n# Use 1: Explicit start/stop\nt = Timer()\nt.start()\ncountdown(1000000)\nt.stop()\nprint(t.elapsed)\n# Use 2: As a context manager\nwith t:\n    countdown(1000000)\n560 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": 13,
      "content": "print(t.elapsed)\nwith Timer() as t2:\n    countdown(1000000)\nprint(t2.elapsed)\nDiscussion\nThis recipe provides a simple yet very useful class for making timing measurements and\ntracking elapsed time. It’s also a nice illustration of how to support the context-\nmanagement protocol and the with statement.\nOne issue in making timing measurements concerns the underlying time function used\nto do it. As a general rule, the accuracy of timing measurements made with functions\nsuch as time.time() or time.clock() varies according to the operating system. In\ncontrast, the time.perf_counter() function always uses the highest-resolution timer\navailable on the system.\nAs shown, the time recorded by the Timer class is made according to wall-clock time,\nand includes all time spent sleeping. If you only want the amount of CPU time used by\nthe process, use time.process_time() instead. For example:\nt = Timer(time.process_time)\nwith t:\n    countdown(1000000)\nprint(t.elapsed)\nBoth the time.perf_counter() and time.process_time() return a “time” in fractional\nseconds. However, the actual value of the time doesn’t have any particular meaning. To\nmake sense of the results, you have to call the functions twice and compute a time\ndifference.\nMore examples of timing and profiling are given in Recipe 14.13.\n13.14. Putting Limits on Memory and CPU Usage\nProblem\nYou want to place some limits on the memory or CPU use of a program running on\nUnix system.\nSolution\nThe resource module can be used to perform both tasks. For example, to restrict CPU\ntime, do the following:\n13.14. Putting Limits on Memory and CPU Usage \n| \n561",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": 13,
      "content": "import signal\nimport resource\nimport os\ndef time_exceeded(signo, frame):\n    print(\"Time's up!\")\n    raise SystemExit(1)\ndef set_max_runtime(seconds):\n    # Install the signal handler and set a resource limit\n    soft, hard = resource.getrlimit(resource.RLIMIT_CPU)\n    resource.setrlimit(resource.RLIMIT_CPU, (seconds, hard))\n    signal.signal(signal.SIGXCPU, time_exceeded)\nif __name__ == '__main__':\n    set_max_runtime(15)\n    while True:\n        pass\nWhen this runs, the SIGXCPU signal is generated when the time expires. The program\ncan then clean up and exit.\nTo restrict memory use, put a limit on the total address space in use. For example:\nimport resource\ndef limit_memory(maxsize):\n    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))\nWith a memory limit in place, programs will start generating MemoryError exceptions\nwhen no more memory is available.\nDiscussion\nIn this recipe, the setrlimit() function is used to set a soft and hard limit on a particular\nresource. The soft limit is a value upon which the operating system will typically restrict\nor notify the process via a signal. The hard limit represents an upper bound on the values\nthat may be used for the soft limit. Typically, this is controlled by a system-wide pa‐\nrameter set by the system administrator. Although the hard limit can be lowered, it can\nnever be raised by user processes (even if the process lowered itself).\nThe setrlimit() function can additionally be used to set limits on things such as the\nnumber of child processes, number of open files, and similar system resources. Consult\nthe documentation for the resource module for further details.\nBe aware that this recipe only works on Unix systems, and that it might not work on all\nof them. For example, when tested, it works on Linux but not on OS X.\n562 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": 13,
      "content": "13.15. Launching a Web Browser\nProblem\nYou want to launch a browser from a script and have it point to some URL that you\nspecify.\nSolution\nThe webbrowser module can be used to launch a browser in a platform-independent\nmanner. For example:\n>>> import webbrowser\n>>> webbrowser.open('http://www.python.org')\nTrue\n>>>\nThis opens the requested page using the default browser. If you want a bit more control\nover how the page gets opened, you can use one of the following functions:\n>>> # Open the page in a new browser window\n>>> webbrowser.open_new('http://www.python.org')\nTrue\n>>>\n>>> # Open the page in a new browser tab\n>>> webbrowser.open_new_tab('http://www.python.org')\nTrue\n>>>\nThese will try to open the page in a new browser window or tab, if possible and supported\nby the browser.\nIf you want to open a page in a specific browser, you can use the webbrowser.get()\nfunction to specify a particular browser. For example:\n>>> c = webbrowser.get('firefox')\n>>> c.open('http://www.python.org')\nTrue\n>>> c.open_new_tab('http://docs.python.org')\nTrue\n>>>\nA full list of supported browser names can be found in the Python documentation.\n13.15. Launching a Web Browser \n| \n563",
      "content_length": 1176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": 13,
      "content": "Discussion\nBeing able to easily launch a browser can be a useful operation in many scripts. For\nexample, maybe a script performs some kind of deployment to a server and you’d like\nto have it quickly launch a browser so you can verify that it’s working. Or maybe a\nprogram writes data out in the form of HTML pages and you’d just like to fire up a\nbrowser to see the result. Either way, the webbrowser module is a simple solution.\n564 \n| \nChapter 13: Utility Scripting and System Administration",
      "content_length": 493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": 13,
      "content": "CHAPTER 14\nTesting, Debugging, and Exceptions\nTesting rocks, but debugging? Not so much. The fact that there’s no compiler to analyze\nyour code before Python executes it makes testing a critical part of development. The\ngoal of this chapter is to discuss some common problems related to testing, debugging,\nand exception handling. It is not meant to be a gentle introduction to test-driven de‐\nvelopment or the unittest module. Thus, some familiarity with testing concepts is \nassumed.\n14.1. Testing Output Sent to stdout\nProblem\nYou have a program that has a method whose output goes to standard Output\n(sys.stdout). This almost always means that it emits text to the screen. You’d like to\nwrite a test for your code to prove that, given the proper input, the proper output is\ndisplayed.\nSolution\nUsing the unittest.mock module’s patch() function, it’s pretty simple to mock out\nsys.stdout for just a single test, and put it back again, without messy temporary vari‐\nables or leaking mocked-out state between test cases.\nConsider, as an example, the following function in a module mymodule:\n# mymodule.py\ndef urlprint(protocol, host, domain):\n    url = '{}://{}.{}'.format(protocol, host, domain)\n    print(url)\n565",
      "content_length": 1216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": 13,
      "content": "The built-in print function, by default, sends output to sys.stdout. In order to test\nthat output is actually getting there, you can mock it out using a stand-in object, and\nthen make assertions about what happened. Using the unittest.mock module’s patch()\nmethod makes it convenient to replace objects only within the context of a running test,\nreturning things to their original state immediately after the test is complete. Here’s the\ntest code for mymodule:\nfrom io import StringIO\nfrom unittest import TestCase\nfrom unittest.mock import patch\nimport mymodule\nclass TestURLPrint(TestCase):\n    def test_url_gets_to_stdout(self):\n        protocol = 'http'\n        host = 'www'\n        domain = 'example.com'\n        expected_url = '{}://{}.{}\\n'.format(protocol, host, domain)\n        with patch('sys.stdout', new=StringIO()) as fake_out:\n            mymodule.urlprint(protocol, host, domain)\n            self.assertEqual(fake_out.getvalue(), expected_url)\nDiscussion\nThe urlprint() function takes three arguments, and the test starts by setting up dummy\narguments for each one. The expected_url variable is set to a string containing the\nexpected output.\nTo run the test, the unittest.mock.patch() function is used as a context manager to\nreplace the value of sys.stdout with a StringIO object as a substitute. The fake_out\nvariable is the mock object that’s created in this process. This can be used inside the\nbody of the with statement to perform various checks. When the with statement com‐\npletes, patch conveniently puts everything back the way it was before the test ever ran.\nIt’s worth noting that certain C extensions to Python may write directly to standard\noutput, bypassing the setting of sys.stdout. This recipe won’t help with that scenario,\nbut it should work fine with pure Python code (if you need to capture I/O from such C\nextensions, you can do it by opening a temporary file and performing various tricks\ninvolving file descriptors to have standard output temporarily redirected to that file).\nMore information about capturing IO in a string and StringIO objects can be found in\nRecipe 5.6. \n566 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 2172,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": 13,
      "content": "14.2. Patching Objects in Unit Tests\nProblem\nYou’re writing unit tests and need to apply patches to selected objects in order to make\nassertions about how they were used in the test (e.g., assertions about being called with\ncertain parameters, access to selected attributes, etc.).\nSolution\nThe unittest.mock.patch() function can be used to help with this problem. It’s a little\nunusual, but patch() can be used as a decorator, a context manager, or stand-alone. For\nexample, here’s an example of how it’s used as a decorator:\nfrom unittest.mock import patch\nimport example\n@patch('example.func')\ndef test1(x, mock_func):\n    example.func(x)       # Uses patched example.func\n    mock_func.assert_called_with(x)\nIt can also be used as a context manager:\nwith patch('example.func') as mock_func:\n    example.func(x)      # Uses patched example.func\n    mock_func.assert_called_with(x)\nLast, but not least, you can use it to patch things manually:\np = patch('example.func')\nmock_func = p.start()\nexample.func(x)\nmock_func.assert_called_with(x)\np.stop()\nIf necessary, you can stack decorators and context managers to patch multiple objects.\nFor example:\n@patch('example.func1')\n@patch('example.func2')\n@patch('example.func3')\ndef test1(mock1, mock2, mock3):\n    ...\ndef test2():\n    with patch('example.patch1') as mock1, \\\n         patch('example.patch2') as mock2, \\\n         patch('example.patch3') as mock3:\n    ...\n14.2. Patching Objects in Unit Tests \n| \n567",
      "content_length": 1461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": 13,
      "content": "Discussion\npatch() works by taking an existing object with the fully qualified name that you pro‐\nvide and replacing it with a new value. The original value is then restored after the\ncompletion of the decorated function or context manager. By default, values are replaced\nwith MagicMock instances. For example:\n>>> x = 42\n>>> with patch('__main__.x'):\n...     print(x)\n...\n<MagicMock name='x' id='4314230032'>\n>>> x\n42\n>>>\nHowever, you can actually replace the value with anything that you wish by supplying\nit as a second argument to patch():\n>>> x\n42\n>>> with patch('__main__.x', 'patched_value'):\n...     print(x)\n...\npatched_value\n>>> x\n42\n>>>\nThe MagicMock instances that are normally used as replacement values are meant to\nmimic callables and instances. They record information about usage and allow you to\nmake assertions. For example:\n>>> from unittest.mock import MagicMock\n>>> m = MagicMock(return_value = 10)\n>>> m(1, 2, debug=True)\n10\n>>> m.assert_called_with(1, 2, debug=True)\n>>> m.assert_called_with(1, 2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \".../unittest/mock.py\", line 726, in assert_called_with\n    raise AssertionError(msg)\nAssertionError: Expected call: mock(1, 2)\nActual call: mock(1, 2, debug=True)\n>>>\n>>> m.upper.return_value = 'HELLO'\n>>> m.upper('hello')\n'HELLO'\n>>> assert m.upper.called\n568 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": 13,
      "content": ">>> m.split.return_value = ['hello', 'world']\n>>> m.split('hello world')\n['hello', 'world']\n>>> m.split.assert_called_with('hello world')\n>>>\n>>> m['blah']\n<MagicMock name='mock.__getitem__()' id='4314412048'>\n>>> m.__getitem__.called\nTrue\n>>> m.__getitem__.assert_called_with('blah')\n>>>\nTypically, these kinds of operations are carried out in a unit test. For example, suppose\nyou have some function like this:\n# example.py\nfrom urllib.request import urlopen\nimport csv\ndef dowprices():\n    u = urlopen('http://finance.yahoo.com/d/quotes.csv?s=@^DJI&f=sl1')\n    lines = (line.decode('utf-8') for line in u)\n    rows = (row for row in csv.reader(lines) if len(row) == 2)\n    prices = { name:float(price) for name, price in rows }\n    return prices\nNormally, this function uses urlopen() to go fetch data off the Web and parse it. To\nunit test it, you might want to give it a more predictable dataset of your own creation,\nhowever. Here’s an example using patching:\nimport unittest\nfrom unittest.mock import patch\nimport io\nimport example\nsample_data = io.BytesIO(b'''\\\n\"IBM\",91.1\\r\n\"AA\",13.25\\r\n\"MSFT\",27.72\\r\n\\r\n''')\nclass Tests(unittest.TestCase):\n    @patch('example.urlopen', return_value=sample_data)\n    def test_dowprices(self, mock_urlopen):\n        p = example.dowprices()\n        self.assertTrue(mock_urlopen.called)\n        self.assertEqual(p,\n                         {'IBM': 91.1,\n                          'AA': 13.25,\n                          'MSFT' : 27.72})\n14.2. Patching Objects in Unit Tests \n| \n569",
      "content_length": 1521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": 13,
      "content": "if __name__ == '__main__':\n    unittest.main()\nIn this example, the urlopen() function in the example module is replaced with a mock\nobject that returns a BytesIO() containing sample data as a substitute.\nAn important but subtle facet of this test is the patching of example.urlopen instead of\nurllib.request.urlopen. When you are making patches, you have to use the names\nas they are used in the code being tested. Since the example code uses from urllib.re\nquest import urlopen, the urlopen() function used by the dowprices() function is\nactually located in example.\nThis recipe has really only given a very small taste of what’s possible with the uni\nttest.mock module. The official documentation is a must-read for more advanced\nfeatures.\n14.3. Testing for Exceptional Conditions in Unit Tests\nProblem\nYou want to write a unit test that cleanly tests if an exception is raised.\nSolution\nTo test for exceptions, use the assertRaises() method. For example, if you want to test\nthat a function raised a ValueError exception, use this code:\nimport unittest\n# A simple function to illustrate\ndef parse_int(s):\n    return int(s)\nclass TestConversion(unittest.TestCase):\n    def test_bad_int(self):\n        self.assertRaises(ValueError, parse_int, 'N/A')\nIf you need to test the exception’s value in some way, then a different approach is needed.\nFor example:\nimport errno\nclass TestIO(unittest.TestCase):\n    def test_file_not_found(self):\n        try:\n            f = open('/file/not/found')\n        except IOError as e:\n            self.assertEqual(e.errno, errno.ENOENT)\n570 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1626,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": 13,
      "content": "else:\n            self.fail('IOError not raised')\nDiscussion\nThe assertRaises() method provides a convenient way to test for the presence of an\nexception. A common pitfall is to write tests that manually try to do things with excep‐\ntions on their own. For instance:\nclass TestConversion(unittest.TestCase):\n    def test_bad_int(self):\n        try:\n            r = parse_int('N/A')\n        except ValueError as e:\n            self.assertEqual(type(e), ValueError)\nThe problem with such approaches is that it is easy to forget about corner cases, such\nas that when no exception is raised at all. To do that, you need to add an extra check for\nthat situation, as shown here:\nclass TestConversion(unittest.TestCase):\n    def test_bad_int(self):\n        try:\n            r = parse_int('N/A')\n        except ValueError as e:\n            self.assertEqual(type(e), ValueError)\n        else:\n            self.fail('ValueError not raised')\nThe assertRaises() method simply takes care of these details, so you should prefer to\nuse it.\nThe one limitation of assertRaises() is that it doesn’t provide a means for testing the\nvalue of the exception object that’s created. To do that, you have to manually test it, as\nshown. Somewhere in between these two extremes, you might consider using the as\nsertRaisesRegex() method, which allows you to test for an exception and perform a\nregular expression match against the exception’s string representation at the same time.\nFor example:\nclass TestConversion(unittest.TestCase):\n    def test_bad_int(self):\n        self.assertRaisesRegex(ValueError, 'invalid literal .*',\n                                       parse_int, 'N/A')\nA little-known fact about assertRaises() and assertRaisesRegex() is that they can\nalso be used as context managers:\nclass TestConversion(unittest.TestCase):\n    def test_bad_int(self):\n        with self.assertRaisesRegex(ValueError, 'invalid literal .*'):\n            r = parse_int('N/A')\n14.3. Testing for Exceptional Conditions in Unit Tests \n| \n571",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": 13,
      "content": "This form can be useful if your test involves multiple steps (e.g., setup) besides that of\nsimply executing a callable.\n14.4. Logging Test Output to a File\nProblem\nYou want the results of running unit tests written to a file instead of printed to standard\noutput.\nSolution\nA very common technique for running unit tests is to include a small code fragment\nlike this at the bottom of your testing file:\nimport unittest\nclass MyTest(unittest.TestCase):\n    ...\nif __name__ == '__main__':\n    unittest.main()\nThis makes the test file executable, and prints the results of running tests to standard\noutput. If you would like to redirect this output, you need to unwind the main() call a\nbit and write your own main() function like this:\nimport sys\ndef main(out=sys.stderr, verbosity=2):\n    loader = unittest.TestLoader()\n    suite = loader.loadTestsFromModule(sys.modules[__name__])\n    unittest.TextTestRunner(out,verbosity=verbosity).run(suite)\nif __name__ == '__main__':\n    with open('testing.out', 'w') as f:\n        main(f)\nDiscussion\nThe interesting thing about this recipe is not so much the task of getting test results\nredirected to a file, but the fact that doing so exposes some notable inner workings of\nthe unittest module.\nAt a basic level, the unittest module works by first assembling a test suite. This test\nsuite consists of the different testing methods you defined. Once the suite has been\nassembled, the tests it contains are executed.\n572 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 591,
      "chapter": 13,
      "content": "These two parts of unit testing are separate from each other. The unittest.TestLoad\ner instance created in the solution is used to assemble a test suite. The loadTestsFrom\nModule() is one of several methods it defines to gather tests. In this case, it scans a\nmodule for TestCase classes and extracts test methods from them. If you want some‐\nthing more fine-grained, the loadTestsFromTestCase() method (not shown) can be\nused to pull test methods from an individual class that inherits from TestCase.\nThe TextTestRunner class is an example of a test runner class. The main purpose of\nthis class is to execute the tests contained in a test suite. This class is the same test runner\nthat sits behind the unittest.main() function. However, here we’re giving it a bit of\nlow-level configuration, including an output file and an elevated verbosity level.\nAlthough this recipe only consists of a few lines of code, it gives a hint as to how you\nmight further customize the unittest framework. To customize how test suites are\nassembled, you would perform various operations using the TestLoader class. To cus‐\ntomize how tests execute, you could make custom test runner classes that emulate the\nfunctionality of TextTestRunner. Both topics are beyond the scope of what can be cov‐\nered here. However, documentation for the unittest module has extensive coverage\nof the underlying protocols. \n14.5. Skipping or Anticipating Test Failures\nProblem\nYou want to skip or mark selected tests as an anticipated failure in your unit tests.\nSolution\nThe unittest module has decorators that can be applied to selected test methods to\ncontrol their handling. For example:\nimport unittest\nimport os\nimport platform\nclass Tests(unittest.TestCase):\n    def test_0(self):\n        self.assertTrue(True)\n    @unittest.skip('skipped test')\n    def test_1(self):\n        self.fail('should have failed!')\n    @unittest.skipIf(os.name=='posix', 'Not supported on Unix')\n    def test_2(self):\n        import winreg\n14.5. Skipping or Anticipating Test Failures \n| \n573",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": 13,
      "content": "@unittest.skipUnless(platform.system() == 'Darwin', 'Mac specific test')\n    def test_3(self):\n        self.assertTrue(True)\n    @unittest.expectedFailure\n    def test_4(self):\n        self.assertEqual(2+2, 5)\nif __name__ == '__main__':\n    unittest.main()\nIf you run this code on a Mac, you’ll get this output:\n    bash % python3 testsample.py -v\n    test_0 (__main__.Tests) ... ok\n    test_1 (__main__.Tests) ... skipped 'skipped test'\n    test_2 (__main__.Tests) ... skipped 'Not supported on Unix'\n    test_3 (__main__.Tests) ... ok\n    test_4 (__main__.Tests) ... expected failure\n    ----------------------------------------------------------------------\n    Ran 5 tests in 0.002s\n    OK (skipped=2, expected failures=1)\nDiscussion\nThe skip() decorator can be used to skip over a test that you don’t want to run at all.\nskipIf() and skipUnless() can be a useful way to write tests that only apply to certain\nplatforms or Python versions, or which have other dependencies. Use the @expected\nFailure decorator to mark tests that are known failures, but for which you don’t want\nthe test framework to report more information.\nThe decorators for skipping methods can also be applied to entire testing classes. For\nexample:\n@unittest.skipUnless(platform.system() == 'Darwin', 'Mac specific tests')\nclass DarwinTests(unittest.TestCase):\n    ...\n14.6. Handling Multiple Exceptions\nProblem\nYou have a piece of code that can throw any of several different exceptions, and you\nneed to account for all of the potential exceptions that could be raised without creating\nduplicate code or long, meandering code passages.\n574 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 593,
      "chapter": 13,
      "content": "Solution\nIf you can handle different exceptions all using a single block of code, they can be\ngrouped together in a tuple like this:\ntry:\n    client_obj.get_url(url)\nexcept (URLError, ValueError, SocketTimeout):\n    client_obj.remove_url(url)\nIn the preceding example, the remove_url() method will be called if any one of the\nlisted exceptions occurs. If, on the other hand, you need to handle one of the exceptions\ndifferently, put it into its own except clause:\ntry:\n    client_obj.get_url(url)\nexcept (URLError, ValueError):\n    client_obj.remove_url(url)\nexcept SocketTimeout:\n    client_obj.handle_url_timeout(url)\nMany exceptions are grouped into an inheritance hierarchy. For such exceptions, you\ncan catch all of them by simply specifying a base class. For example, instead of writing\ncode like this:\ntry:\n    f = open(filename)\nexcept (FileNotFoundError, PermissionError):\n    ...\nyou could rewrite the except statement as:\ntry:\n    f = open(filename)\nexcept OSError:\n    ...\nThis works because OSError is a base class that’s common to both the FileNotFound\nErrorand PermissionError exceptions.\nDiscussion\nAlthough it’s not specific to handling multiple exceptions per se, it’s worth noting that\nyou can get a handle to the thrown exception using the as keyword:\ntry:\n    f = open(filename)\nexcept OSError as e:\n    if e.errno == errno.ENOENT:\n        logger.error('File not found')\n    elif e.errno == errno.EACCES:\n        logger.error('Permission denied')\n14.6. Handling Multiple Exceptions \n| \n575",
      "content_length": 1510,
      "extraction_method": "Direct"
    },
    {
      "page_number": 594,
      "chapter": 13,
      "content": "else:\n        logger.error('Unexpected error: %d', e.errno)\nIn this example, the e variable holds an instance of the raised OSError. This is useful if\nyou need to inspect the exception further, such as processing it based on the value of an\nadditional status code.\nBe aware that except clauses are checked in the order listed and that the first match\nexecutes. It may be a bit pathological, but you can easily create situations where multiple\nexcept clauses might match. For example:\n>>> f = open('missing')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nFileNotFoundError: [Errno 2] No such file or directory: 'missing'\n>>> try:\n...     f = open('missing')\n... except OSError:\n...     print('It failed')\n... except FileNotFoundError:\n...     print('File not found')\n...\nIt failed\n>>>\nHere the except FileNotFoundError clause doesn’t execute because the OSError is\nmore general, matches the FileNotFoundError exception, and was listed first.\nAs a debugging tip, if you’re not entirely sure about the class hierarchy of a particular\nexception, you can quickly view it by inspecting the exception’s __mro__ attribute. For\nexample:\n>>> FileNotFoundError.__mro__\n(<class 'FileNotFoundError'>, <class 'OSError'>, <class 'Exception'>,\n <class 'BaseException'>, <class 'object'>)\n>>>\nAny one of the listed classes up to BaseException can be used with the except statement.\n14.7. Catching All Exceptions\nProblem\nYou want to write code that catches all exceptions.\nSolution\nTo catch all exceptions, write an exception handler for Exception, as shown here:\n576 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1627,
      "extraction_method": "Direct"
    },
    {
      "page_number": 595,
      "chapter": 13,
      "content": "try:\n   ...\nexcept Exception as e:\n   ...\n   log('Reason:', e)       # Important!\nThis will catch all exceptions save SystemExit, KeyboardInterrupt, and GeneratorEx\nit. If you also want to catch those exceptions, change Exception to BaseException.\nDiscussion\nCatching all exceptions is sometimes used as a crutch by programmers who can’t re‐\nmember all of the possible exceptions that might occur in complicated operations. As\nsuch, it is also a very good way to write undebuggable code if you are not careful.\nBecause of this, if you choose to catch all exceptions, it is absolutely critical to log or\nreport the actual reason for the exception somewhere (e.g., log file, error message print‐\ned to screen, etc.). If you don’t do this, your head will likely explode at some point.\nConsider this example:\ndef parse_int(s):\n    try:\n        n = int(v)\n    except Exception:\n        print(\"Couldn't parse\")\nIf you try this function, it behaves like this:\n>>> parse_int('n/a')\nCouldn't parse\n>>> parse_int('42')\nCouldn't parse\n>>>\nAt this point, you might be left scratching your head as to why it doesn’t work. Now\nsuppose the function had been written like this:\ndef parse_int(s):\n    try:\n        n = int(v)\n    except Exception as e:\n        print(\"Couldn't parse\")\n        print('Reason:', e)\nIn this case, you get the following output, which indicates that a programming mistake\nhas been made:\n>>> parse_int('42')\nCouldn't parse\nReason: global name 'v' is not defined\n>>>\n14.7. Catching All Exceptions \n| \n577",
      "content_length": 1512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 596,
      "chapter": 13,
      "content": "All things being equal, it’s probably better to be as precise as possible in your exception\nhandling. However, if you must catch all exceptions, just make sure you give good di‐\nagnostic information or propagate the exception so that cause doesn’t get lost.\n14.8. Creating Custom Exceptions\nProblem\nYou’re building an application and would like to wrap lower-level exceptions with cus‐\ntom ones that have more meaning in the context of your application.\nSolution\nCreating new exceptions is easy—just define them as classes that inherit from Excep\ntion (or one of the other existing exception types if it makes more sense). For example,\nif you are writing code related to network programming, you might define some custom\nexceptions like this:\nclass NetworkError(Exception):\n    pass\nclass HostnameError(NetworkError):\n    pass\nclass TimeoutError(NetworkError):\n    pass\nclass ProtocolError(NetworkError):\n    pass\nUsers could then use these exceptions in the normal way. For example:\ntry:\n    msg = s.recv()\nexcept TimeoutError as e:\n    ...\nexcept ProtocolError as e:\n    ...\nDiscussion\nCustom exception classes should almost always inherit from the built-in Exception\nclass, or inherit from some locally defined base exception that itself inherits from Ex\nception. Although all exceptions also derive from BaseException, you should not use\nthis as a base class for new exceptions. BaseException is reserved for system-exiting\nexceptions, such as KeyboardInterrupt or SystemExit, and other exceptions that\nshould signal the application to exit. Therefore, catching these exceptions is not the\n578 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 597,
      "chapter": 13,
      "content": "intended use case. Assuming you follow this convention, it follows that inheriting from\nBaseException causes your custom exceptions to not be caught and to signal an im‐\nminent application shutdown! \nHaving custom exceptions in your application and using them as shown makes your\napplication code tell a more coherent story to whoever may need to read the code. One\ndesign consideration involves the grouping of custom exceptions via inheritance. In\ncomplicated applications, it may make sense to introduce further base classes that group\ndifferent classes of exceptions together. This gives the user a choice of catching a nar‐\nrowly specified error, such as this:\ntry:\n    s.send(msg)\nexcept ProtocolError:\n    ...\nIt also gives the ability to catch a broad range of errors, such as the following:\ntry:\n    s.send(msg)\nexcept NetworkError:\n    ...\nIf you are going to define a new exception that overrides the __init__() method of\nException, make sure you always call Exception.__init__() with all of the passed\narguments. For example:\nclass CustomError(Exception):\n    def __init__(self, message, status):\n        super().__init__(message, status)\n        self.message = message\n        self.status = status\nThis might look a little weird, but the default behavior of Exception is to accept all\narguments passed and to store them in the .args attribute as a tuple. Various other\nlibraries and parts of Python expect all exceptions to have the .args attribute, so if you\nskip this step, you might find that your new exception doesn’t behave quite right in\ncertain contexts. To illustrate the use of .args, consider this interactive session with the\nbuilt-in RuntimeError exception, and notice how any number of arguments can be used\nwith the raise statement:\n>>> try:\n...     raise RuntimeError('It failed')\n... except RuntimeError as e:\n...     print(e.args)\n...\n('It failed',)\n>>> try:\n...     raise RuntimeError('It failed', 42, 'spam')\n... except RuntimeError as e:\n14.8. Creating Custom Exceptions \n| \n579",
      "content_length": 2012,
      "extraction_method": "Direct"
    },
    {
      "page_number": 598,
      "chapter": 13,
      "content": "...     print(e.args)\n...\n('It failed', 42, 'spam')\n>>>\nFor more information on creating your own exceptions, see the Python documentation.\n14.9. Raising an Exception in Response to Another\nException\nProblem\nYou want to raise an exception in response to catching a different exception, but want\nto include information about both exceptions in the traceback.\nSolution\nTo chain exceptions, use the raise from statement instead of a simple raise statement.\nThis will give you information about both errors. For example:\n>>> def example():\n...     try:\n...             int('N/A')\n...     except ValueError as e:\n...             raise RuntimeError('A parsing error occurred') from e...\n>>> \nexample()\nTraceback (most recent call last):\n  File \"<stdin>\", line 3, in example\nValueError: invalid literal for int() with base 10: 'N/A'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 5, in example\nRuntimeError: A parsing error occurred\n>>>\nAs you can see in the traceback, both exceptions are captured. To catch such an excep‐\ntion, you would use a normal except statement. However, you can look at the __cause__\nattribute of the exception object to follow the exception chain should you wish. For\nexample:\ntry:\n    example()\nexcept RuntimeError as e:\n    print(\"It didn't work:\", e)\n580 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 599,
      "chapter": 13,
      "content": "if e.__cause__:\n        print('Cause:', e.__cause__)\nAn implicit form of chained exceptions occurs when another exception gets raised in‐\nside an except block. For example:\n>>> def example2():\n...     try:\n...             int('N/A')\n...     except ValueError as e:\n...             print(\"Couldn't parse:\", err)\n...\n>>>\n>>> example2()\nTraceback (most recent call last):\n  File \"<stdin>\", line 3, in example2\nValueError: invalid literal for int() with base 10: 'N/A'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 5, in example2\nNameError: global name 'err' is not defined\n>>>\nIn this example, you get information about both exceptions, but the interpretation is a\nbit different. In this case, the NameError exception is raised as the result of a program‐\nming error, not in direct response to the parsing error. For this case, the __cause__\nattribute of an exception is not set. Instead, a __context__ attribute is set to the prior\nexception.\nIf, for some reason, you want to suppress chaining, use raise from None:\n>>> def example3():\n...     try:\n...             int('N/A')\n...     except ValueError:\n...             raise RuntimeError('A parsing error occurred') from None...\n>>> \nexample3()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 5, in example3\nRuntimeError: A parsing error occurred\n>>>\n14.9. Raising an Exception in Response to Another Exception \n| \n581",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 600,
      "chapter": 13,
      "content": "Discussion\nIn designing code, you should give careful attention to use of the raise statement inside\nof other except blocks. In most cases, such raise statements should probably be\nchanged to raise from statements. That is, you should prefer this style:\ntry:\n   ...\nexcept SomeException as e:\n   raise DifferentException() from e\nThe reason for doing this is that you are explicitly chaining the causes together. That is,\nthe DifferentException is being raised in direct response to getting a SomeExcep\ntion. This relationship will be explicitly stated in the resulting traceback.\nIf you write your code in the following style, you still get a chained exception, but it’s\noften not clear if the exception chain was intentional or the result of an unforeseen\nprogramming error:\ntry:\n   ...\nexcept SomeException:\n   raise DifferentException()\nWhen you use raise from, you’re making it clear that you meant to raise the second\nexception.\nResist the urge to suppress exception information, as shown in the last example. Al‐\nthough suppressing exception information can lead to smaller tracebacks, it also dis‐\ncards information that might be useful for debugging. All things being equal, it’s often\nbest to keep as much information as possible.\n14.10. Reraising the Last Exception\nProblem\nYou caught an exception in an except block, but now you want to reraise it.\nSolution\nSimply use the raise statement all by itself. For example:\n>>> def example():\n...     try:\n...             int('N/A')\n...     except ValueError:\n...             print(\"Didn't work\")\n...             raise\n...\n582 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 601,
      "chapter": 14,
      "content": ">>> example()\nDidn't work\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in example\nValueError: invalid literal for int() with base 10: 'N/A'\n>>>\nDiscussion\nThis problem typically arises when you need to take some kind of action in response to\nan exception (e.g., logging, cleanup, etc.), but afterward, you simply want to propagate\nthe exception along. A very common use might be in catch-all exception handlers:\ntry:\n   ...\nexcept Exception as e:\n   # Process exception information in some way\n   ...\n   # Propagate the exception\n   raise\n14.11. Issuing Warning Messages\nProblem\nYou want to have your program issue warning messages (e.g., about deprecated features\nor usage problems).\nSolution\nTo have your program issue a warning message, use the warnings.warn() function. For\nexample:\nimport warnings\ndef func(x, y, logfile=None, debug=False):\n    if logfile is not None:\n         warnings.warn('logfile argument deprecated', DeprecationWarning)\n    ...\nThe arguments to warn() are a warning message along with a warning class, which is\ntypically one of the following: UserWarning, DeprecationWarning, SyntaxWarning,\nRuntimeWarning, ResourceWarning, or FutureWarning.\nThe handling of warnings depends on how you have executed the interpreter and other\nconfiguration. For example, if you run Python with the -W all option, you’ll get output\nsuch as the following:\n14.11. Issuing Warning Messages \n| \n583",
      "content_length": 1458,
      "extraction_method": "Direct"
    },
    {
      "page_number": 602,
      "chapter": 14,
      "content": "bash % python3 -W all example.py\n    example.py:5: DeprecationWarning: logfile argument is deprecated\n      warnings.warn('logfile argument is deprecated', DeprecationWarning)\nNormally, warnings just produce output messages on standard error. If you want to turn\nwarnings into exceptions, use the -W error option:\n    bash % python3 -W error example.py\n    Traceback (most recent call last):\n      File \"example.py\", line 10, in <module>\n        func(2, 3, logfile='log.txt')\n      File \"example.py\", line 5, in func\n        warnings.warn('logfile argument is deprecated', DeprecationWarning)\n    DeprecationWarning: logfile argument is deprecated\n    bash %\nDiscussion\nIssuing a warning message is often a useful technique for maintaining software and\nassisting users with issues that don’t necessarily rise to the level of being a full-fledged\nexception. For example, if you’re going to change the behavior of a library or framework,\nyou can start issuing warning messages for the parts that you’re going to change while\nstill providing backward compatibility for a time. You can also warn users about prob‐\nlematic usage issues in their code.\nAs another example of a warning in the built-in library, here is an example of a warning\nmessage generated by destroying a file without closing it:\n>>> import warnings\n>>> warnings.simplefilter('always')\n>>> f = open('/etc/passwd')\n>>> del f\n__main__:1: ResourceWarning: unclosed file <_io.TextIOWrapper name='/etc/passwd'\n mode='r' encoding='UTF-8'>\n>>>\nBy default, not all warning messages appear. The -W option to Python can control the\noutput of warning messages. -W all will output all warning messages, -W ignore\nignores all warnings, and -W error turns warnings into exceptions. As an alternative,\nyou can can use the warnings.simplefilter() function to control output, as just\nshown. An argument of always makes all warning messages appear, ignore ignores all\nwarnings, and error turns warnings into exceptions.\nFor simple cases, this is all you really need to issue warning messages. The warnings\nmodule provides a variety of more advanced configuration options related to the fil‐\ntering and handling of warning messages. See the Python documentation for more \ninformation.\n584 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 603,
      "chapter": 14,
      "content": "14.12. Debugging Basic Program Crashes\nProblem\nYour program is broken and you’d like some simple strategies for debugging it.\nSolution\nIf your program is crashing with an exception, running your program as python3 -i\nsomeprogram.py can be a useful tool for simply looking around. The -i option starts\nan interactive shell as soon as a program terminates. From there, you can explore the\nenvironment. For example, suppose you have this code:\n# sample.py\ndef func(n):\n    return n + 10\nfunc('Hello')\nRunning python3 -i produces the following:\nbash % python3 -i sample.py\nTraceback (most recent call last):\n  File \"sample.py\", line 6, in <module>\n    func('Hello')\n  File \"sample.py\", line 4, in func\n    return n + 10\nTypeError: Can't convert 'int' object to str implicitly\n>>> func(10)\n20\n>>>\nIf you don’t see anything obvious, a further step is to launch the Python debugger after\na crash. For example:\n>>> import pdb\n>>> pdb.pm()\n> sample.py(4)func()\n-> return n + 10\n(Pdb) w\n  sample.py(6)<module>()\n-> func('Hello')\n> sample.py(4)func()\n-> return n + 10\n(Pdb) print n\n'Hello'\n(Pdb) q\n>>>\n14.12. Debugging Basic Program Crashes \n| \n585",
      "content_length": 1137,
      "extraction_method": "Direct"
    },
    {
      "page_number": 604,
      "chapter": 14,
      "content": "If your code is deeply buried in an environment where it is difficult to obtain an inter‐\nactive shell (e.g., in a server), you can often catch errors and produce tracebacks yourself.\nFor example:\nimport traceback\nimport sys\ntry:\n    func(arg)\nexcept:\n    print('**** AN ERROR OCCURRED ****')\n    traceback.print_exc(file=sys.stderr)\nIf your program isn’t crashing, but it’s producing wrong answers or you’re mystified by\nhow it works, there is often nothing wrong with just injecting a few print() calls in\nplaces of interest. However, if you’re going to do that, there are a few related techniques\nof interest. First, the traceback.print_stack() function will create a stack track of\nyour program immediately at that point. For example:\n>>> def sample(n):\n...     if n > 0:\n...             sample(n-1)\n...     else:\n...             traceback.print_stack(file=sys.stderr)\n...\n>>> sample(5)\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in sample\n  File \"<stdin>\", line 3, in sample\n  File \"<stdin>\", line 3, in sample\n  File \"<stdin>\", line 3, in sample\n  File \"<stdin>\", line 3, in sample\n  File \"<stdin>\", line 5, in sample\n>>>\nAlternatively, you can also manually launch the debugger at any point in your program\nusing pdb.set_trace() like this:\nimport pdb\ndef func(arg):\n    ...\n    pdb.set_trace()\n    ...\nThis can be a useful technique for poking around in the internals of a large program\nand answering questions about the control flow or arguments to functions. For instance,\nonce the debugger starts, you can inspect variables using print or type a command such\nas w to get the stack traceback.\n586 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 605,
      "chapter": 14,
      "content": "Discussion\nDon’t make debugging more complicated than it needs to be. Simple errors can often\nbe resolved by merely knowing how to read program tracebacks (e.g., the actual error\nis usually the last line of the traceback). Inserting a few selected print() functions in\nyour code can also work well if you’re in the process of developing it and you simply\nwant some diagnostics (just remember to remove the statements later).\nA common use of the debugger is to inspect variables inside a function that has crashed.\nKnowing how to enter the debugger after such a crash has occurred is a useful skill to\nknow.\nInserting statements such as pdb.set_trace() can be useful if you’re trying to unravel\nan extremely complicated program where the underlying control flow isn’t obvious.\nEssentially, the program will run until it hits the set_trace() call, at which point it will\nimmediately enter the debugger. From there, you can try to make more sense of it. \nIf you’re using an IDE for Python development, the IDE will typically provide its own\ndebugging interface on top of or in place of pdb. Consult the manual for your IDE for\nmore information.\n14.13. Profiling and Timing Your Program\nProblem\nYou would like to find out where your program spends its time and make timing\nmeasurements.\nSolution\nIf you simply want to time your whole program, it’s usually easy enough to use something\nlike the Unix time command. For example:\nbash % time python3 someprogram.py\nreal 0m13.937s\nuser 0m12.162s\nsys  0m0.098s\nbash %\nOn the other extreme, if you want a detailed report showing what your program is doing,\nyou can use the cProfile module:\n14.13. Profiling and Timing Your Program \n| \n587",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 606,
      "chapter": 14,
      "content": "bash % python3 -m cProfile someprogram.py\n         859647 function calls in 16.016 CPU seconds\n   Ordered by: standard name\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   263169    0.080    0.000    0.080    0.000 someprogram.py:16(frange)\n      513    0.001    0.000    0.002    0.000 someprogram.py:30(generate_mandel)\n   262656    0.194    0.000   15.295    0.000 someprogram.py:32(<genexpr>)\n        1    0.036    0.036   16.077   16.077 someprogram.py:4(<module>)\n   262144   15.021    0.000   15.021    0.000 someprogram.py:4(in_mandelbrot)\n        1    0.000    0.000    0.000    0.000 os.py:746(urandom)\n        1    0.000    0.000    0.000    0.000 png.py:1056(_readable)\n        1    0.000    0.000    0.000    0.000 png.py:1073(Reader)\n        1    0.227    0.227    0.438    0.438 png.py:163(<module>)\n      512    0.010    0.000    0.010    0.000 png.py:200(group)\n    ...\nbash %\nMore often than not, profiling your code lies somewhere in between these two extremes.\nFor example, you may already know that your code spends most of its time in a few\nselected functions. For selected profiling of functions, a short decorator can be useful.\nFor example:\n# timethis.py\nimport time\nfrom functools import wraps\ndef timethis(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.perf_counter()\n        r = func(*args, **kwargs)\n        end = time.perf_counter()\n        print('{}.{} : {}'.format(func.__module__, func.__name__, end - start))\n        return r\n    return wrapper\nTo use this decorator, you simply place it in front of a function definition to get timings\nfrom it. For example:\n>>> @timethis\n... def countdown(n):\n...     while n > 0:\n...             n -= 1\n...\n>>> countdown(10000000)\n__main__.countdown : 0.803001880645752\n>>>\nTo time a block of statements, you can define a context manager. For example:\n588 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 607,
      "chapter": 14,
      "content": "from contextlib import contextmanager\n@contextmanager\ndef timeblock(label):\n    start = time.perf_counter()\n    try:\n        yield\n    finally:\n        end = time.perf_counter()\n        print('{} : {}'.format(label, end - start))\nHere is an example of how the context manager works:\n>>> with timeblock('counting'):\n...     n = 10000000\n...     while n > 0:\n...             n -= 1\n...\ncounting : 1.5551159381866455\n>>>\nFor studying the performance of small code fragments, the timeit module can be useful.\nFor example:\n>>> from timeit import timeit\n>>> timeit('math.sqrt(2)', 'import math')\n0.1432319980012835\n>>> timeit('sqrt(2)', 'from math import sqrt')\n0.10836604500218527\n>>>\ntimeit works by executing the statement specified in the first argument a million times\nand measuring the time. The second argument is a setup string that is executed to set\nup the environment prior to running the test. If you need to change the number of\niterations, supply a number argument like this:\n>>> timeit('math.sqrt(2)', 'import math', number=10000000)\n1.434852126003534\n>>> timeit('sqrt(2)', 'from math import sqrt', number=10000000)\n1.0270336690009572\n>>>\nDiscussion\nWhen making performance measurements, be aware that any results you get are ap‐\nproximations. The time.perf_counter() function used in the solution provides the\nhighest-resolution timer possible on a given platform. However, it still measures wall-\nclock time, and can be impacted by many different factors, such as machine load.\nIf you are interested in process time as opposed to wall-clock time, use time.pro\ncess_time() instead. For example:\n14.13. Profiling and Timing Your Program \n| \n589",
      "content_length": 1653,
      "extraction_method": "Direct"
    },
    {
      "page_number": 608,
      "chapter": 14,
      "content": "from functools import wraps\ndef timethis(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.process_time()\n        r = func(*args, **kwargs)\n        end = time.process_time()\n        print('{}.{} : {}'.format(func.__module__, func.__name__, end - start))\n        return r\n    return wrapper\nLast, but not least, if you’re going to perform detailed timing analysis, make sure to read\nthe documentation for the time, timeit, and other associated modules, so that you have\nan understanding of important platform-related differences and other pitfalls.\nSee Recipe 13.13 for a related recipe on creating a stopwatch timer class.\n14.14. Making Your Programs Run Faster\nProblem\nYour program runs too slow and you’d like to speed it up without the assistance of more\nextreme solutions, such as C extensions or a just-in-time (JIT) compiler.\nSolution\nWhile the first rule of optimization might be to “not do it,” the second rule is almost\ncertainly “don’t optimize the unimportant.” To that end, if your program is running slow,\nyou might start by profiling your code as discussed in Recipe 14.13.\nMore often than not, you’ll find that your program spends its time in a few hotspots,\nsuch as inner data processing loops. Once you’ve identified those locations, you can use\nthe no-nonsense techniques presented in the following sections to make your program\nrun faster.\nUse functions\nA lot of programmers start using Python as a language for writing simple scripts. When\nwriting scripts, it is easy to fall into a practice of simply writing code with very little\nstructure. For example:\n# somescript.py\nimport sys\nimport csv\nwith open(sys.argv[1]) as f:\n     for row in csv.reader(f):\n590 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 609,
      "chapter": 14,
      "content": "# Some kind of processing\n         ...\nA little-known fact is that code defined in the global scope like this runs slower than\ncode defined in a function. The speed difference has to do with the implementation of\nlocal versus global variables (operations involving locals are faster). So, if you want to\nmake the program run faster, simply put the scripting statements in a function:\n# somescript.py\nimport sys\nimport csv\ndef main(filename):\n    with open(filename) as f:\n         for row in csv.reader(f):\n             # Some kind of processing\n             ...\nmain(sys.argv[1])\nThe speed difference depends heavily on the processing being performed, but in our\nexperience, speedups of 15-30% are not uncommon.\nSelectively eliminate attribute access\nEvery use of the dot (.) operator to access attributes comes with a cost. Under the covers,\nthis triggers special methods, such as __getattribute__() and __getattr__(), which\noften lead to dictionary lookups.\nYou can often avoid attribute lookups by using the from module import name form of\nimport as well as making selected use of bound methods. To illustrate, consider the\nfollowing code fragment:\nimport math\ndef compute_roots(nums):\n    result = []\n    for n in nums:\n        result.append(math.sqrt(n))\n    return result\n# Test\nnums = range(1000000)\nfor n in range(100):\n    r = compute_roots(nums)\nWhen tested on our machine, this program runs in about 40 seconds. Now change the\ncompute_roots() function as follows:\nfrom math import sqrt\ndef compute_roots(nums):\n14.14. Making Your Programs Run Faster \n| \n591",
      "content_length": 1569,
      "extraction_method": "Direct"
    },
    {
      "page_number": 610,
      "chapter": 14,
      "content": "result = []\n    result_append = result.append\n    for n in nums:\n        result_append(sqrt(n))\n    return result\nThis version runs in about 29 seconds. The only difference between the two versions of\ncode is the elimination of attribute access. Instead of using math.sqrt(), the code uses\nsqrt(). The result.append() method is additionally placed into a local variable re\nsult_append and reused in the inner loop.\nHowever, it must be emphasized that these changes only make sense in frequently ex‐\necuted code, such as loops. So, this optimization really only makes sense in carefully \nselected places.\nUnderstand locality of variables\nAs previously noted, local variables are faster than global variables. For frequently ac‐\ncessed names, speedups can be obtained by making those names as local as possible.\nFor example, consider this modified version of the compute_roots() function just\ndiscussed:\nimport math\ndef compute_roots(nums):\n    sqrt = math.sqrt\n    result = []\n    result_append = result.append\n    for n in nums:\n        result_append(sqrt(n))\n    return result\nIn this version, sqrt has been lifted from the math module and placed into a local\nvariable. If you run this code, it now runs in about 25 seconds (an improvement over\nthe previous version, which took 29 seconds). That additional speedup is due to a local\nlookup of sqrt being a bit faster than a global lookup of sqrt.\nLocality arguments also apply when working in classes. In general, looking up a value\nsuch as self.name will be considerably slower than accessing a local variable. In inner\nloops, it might pay to lift commonly accessed attributes into a local variable. For example:\n# Slower\nclass SomeClass:\n    ...\n    def method(self):\n         for x in s:\n             op(self.value)\n# Faster\nclass SomeClass:\n592 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 611,
      "chapter": 14,
      "content": "...\n    def method(self):\n         value = self.value\n         for x in s:\n             op(value)\nAvoid gratuitous abstraction\nAny time you wrap up code with extra layers of processing, such as decorators, prop‐\nerties, or descriptors, you’re going to make it slower. As an example, consider this class:\nclass A:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    @property\n    def y(self):\n        return self._y\n    @y.setter\n    def y(self, value):\n        self._y = value\nNow, try a simple timing test:\n>>> from timeit import timeit\n>>> a = A(1,2)\n>>> timeit('a.x', 'from __main__ import a')\n0.07817923510447145\n>>> timeit('a.y', 'from __main__ import a')\n0.35766440676525235\n>>>\nAs you can observe, accessing the property y is not just slightly slower than a simple\nattribute x, it’s about 4.5 times slower. If this difference matters, you should ask yourself\nif the definition of y as a property was really necessary. If not, simply get rid of it and\ngo back to using a simple attribute instead. Just because it might be common for pro‐\ngrams in another programming language to use getter/setter functions, that doesn’t\nmean you should adopt that programming style for Python.\nUse the built-in containers\nBuilt-in data types such as strings, tuples, lists, sets, and dicts are all implemented in C,\nand are rather fast. If you’re inclined to make your own data structures as a replacement\n(e.g., linked lists, balanced trees, etc.), it may be rather difficult if not impossible to match\nthe speed of the built-ins. Thus, you’re often better off just using them.\nAvoid making unnecessary data structures or copies\nSometimes programmers get carried away with making unnecessary data structures\nwhen they just don’t have to. For example, someone might write code like this:\n14.14. Making Your Programs Run Faster \n| \n593",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 612,
      "chapter": 14,
      "content": "values = [x for x in sequence]\nsquares = [x*x for x in values]\nPerhaps the thinking here is to first collect a bunch of values into a list and then to start\napplying operations such as list comprehensions to it. However, the first list is com‐\npletely unnecessary. Simply write the code like this:\nsquares = [x*x for x in sequence]\nRelated to this, be on the lookout for code written by programmers who are overly\nparanoid about Python’s sharing of values. Overuse of functions such as copy.deep\ncopy() may be a sign of code that’s been written by someone who doesn’t fully under‐\nstand or trust Python’s memory model. In such code, it may be safe to eliminate many\nof the copies.\nDiscussion\nBefore optimizing, it’s usually worthwhile to study the algorithms that you’re using first.\nYou’ll get a much bigger speedup by switching to an O(n log n) algorithm than by\ntrying to tweak the implementation of an an O(n**2) algorithm.\nIf you’ve decided that you still must optimize, it pays to consider the big picture. As a\ngeneral rule, you don’t want to apply optimizations to every part of your program,\nbecause such changes are going to make the code hard to read and understand. Instead,\nfocus only on known performance bottlenecks, such as inner loops.\nYou need to be especially wary interpreting the results of micro-optimizations. For\nexample, consider these two techniques for creating a dictionary:\na = {\n    'name' : 'AAPL',\n    'shares' : 100,\n    'price' : 534.22\n}\nb = dict(name='AAPL', shares=100, price=534.22)\nThe latter choice has the benefit of less typing (you don’t need to quote the key names).\nHowever, if you put the two code fragments in a head-to-head performance battle, you’ll\nfind that using dict() runs three times slower! With this knowledge, you might be\ninclined to scan your code and replace every use of dict() with its more verbose al‐\nternative. However, a smart programmer will only focus on parts of a program where\nit might actually matter, such as an inner loop. In other places, the speed difference just\nisn’t going to matter at all.\nIf, on the other hand, your performance needs go far beyond the simple techniques in\nthis recipe, you might investigate the use of tools based on just-in-time (JIT) compilation\ntechniques. For example, the PyPy project is an alternate implementation of the Python\n594 \n| \nChapter 14: Testing, Debugging, and Exceptions",
      "content_length": 2389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 613,
      "chapter": 14,
      "content": "interpreter that analyzes the execution of your program and generates native machine\ncode for frequently executed parts. It can sometimes make Python programs run an\norder of magnitude faster, often approaching (or even exceeding) the speed of code\nwritten in C. Unfortunately, as of this writing, PyPy does not yet fully support Python\n3. So, that is something to look for in the future. You might also consider the Numba\nproject. Numba is a dynamic compiler where you annotate selected Python functions\nthat you want to optimize with a decorator. Those functions are then compiled into\nnative machine code through the use of LLVM. It too can produce signficant perfor‐\nmance gains. However, like PyPy, support for Python 3 should be viewed as somewhat\nexperimental.\nLast, but not least, the words of John Ousterhout come to mind: “The best performance\nimprovement is the transition from the nonworking to the working state.” Don’t worry\nabout optimization until you need to. Making sure your program works correctly is\nusually more important than making it run fast (at least initially).\n14.14. Making Your Programs Run Faster \n| \n595",
      "content_length": 1136,
      "extraction_method": "Direct"
    },
    {
      "page_number": 614,
      "chapter": 14,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 615,
      "chapter": 14,
      "content": "CHAPTER 15\nC Extensions\nThis chapter looks at the problem of accessing C code from Python. Many of Python’s\nbuilt-in libraries are written in C, and accessing C is an important part of making Python\ntalk to existing libraries. It’s also an area that might require the most study if you’re faced\nwith the problem of porting extension code from Python 2 to 3.\nAlthough Python provides an extensive C programming API, there are actually many\ndifferent approaches for dealing with C. Rather than trying to give an exhaustive ref‐\nerence for every possible tool or technique, the approach is to focus on a small fragment\nof C code along with some representative examples of how to work with the code. The\ngoal is to provide a series of programming templates that experienced programmers\ncan expand upon for their own use.\nHere is the C code we will work with in most of the recipes:\n/* sample.c */_method\n#include <math.h>\n/* Compute the greatest common divisor */\nint gcd(int x, int y) {\n    int g = y;\n    while (x > 0) {\n        g = x;\n        x = y % x;\n        y = g;\n    }\n    return g;\n}\n/* Test if (x0,y0) is in the Mandelbrot set or not */\nint in_mandel(double x0, double y0, int n) {\n  double x=0,y=0,xtemp;\n  while (n > 0) {\n    xtemp = x*x - y*y + x0;\n    y = 2*x*y + y0;\n597",
      "content_length": 1282,
      "extraction_method": "Direct"
    },
    {
      "page_number": 616,
      "chapter": 14,
      "content": "x = xtemp;\n    n -= 1;\n    if (x*x + y*y > 4) return 0;\n  }\n  return 1;\n}\n/* Divide two numbers */\nint divide(int a, int b, int *remainder) {\n  int quot = a / b;\n  *remainder = a % b;\n  return quot;\n}\n/* Average values in an array */\ndouble avg(double *a, int n) {\n  int i;\n  double total = 0.0;\n  for (i = 0; i < n; i++) {\n    total += a[i];\n  }\n  return total / n;\n}\n/* A C data structure */\ntypedef struct Point {\n    double x,y;\n} Point;\n/* Function involving a C data structure */\ndouble distance(Point *p1, Point *p2) {\n   return hypot(p1->x - p2->x, p1->y - p2->y);\n}\nThis code contains a number of different C programming features. First, there are a few\nsimple functions such as gcd() and is_mandel(). The divide() function is an example\nof a C function returning multiple values, one through a pointer argument. The avg()\nfunction performs a data reduction across a C array. The Point and distance() function\ninvolve C structures.\nFor all of the recipes that follow, assume that the preceding code is found in a file named\nsample.c, that definitions are found in a file named sample.h and that it has been com‐\npiled into a library libsample that can be linked to other C code. The exact details of\ncompilation and linking vary from system to system, but that is not the primary focus.\nIt is assumed that if you’re working with C code, you’ve already figured that out.\n598 \n| \nChapter 15: C Extensions",
      "content_length": 1411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 617,
      "chapter": 14,
      "content": "15.1. Accessing C Code Using ctypes\nProblem\nYou have a small number of C functions that have been compiled into a shared library\nor DLL. You would like to call these functions purely from Python without having to\nwrite additional C code or using a third-party extension tool.\nSolution\nFor small problems involving C code, it is often easy enough to use the ctypes module\nthat is part of Python’s standard library. In order to use ctypes, you must first make\nsure the C code you want to access has been compiled into a shared library that is\ncompatible with the Python interpreter (e.g., same architecture, word size, compiler,\netc.). For the purposes of this recipe, assume that a shared library, libsample.so, has\nbeen created and that it contains nothing more than the code shown in the chapter\nintroduction. Further assume that the libsample.so file has been placed in the same\ndirectory as the sample.py file shown next.\nTo access the resulting library, you make a Python module that wraps around it, such\nas the following:\n# sample.py\nimport ctypes\nimport os\n# Try to locate the .so file in the same directory as this file\n_file = 'libsample.so'\n_path = os.path.join(*(os.path.split(__file__)[:-1] + (_file,)))\n_mod = ctypes.cdll.LoadLibrary(_path)\n# int gcd(int, int)\ngcd = _mod.gcd\ngcd.argtypes = (ctypes.c_int, ctypes.c_int)\ngcd.restype = ctypes.c_int\n# int in_mandel(double, double, int)\nin_mandel = _mod.in_mandel\nin_mandel.argtypes = (ctypes.c_double, ctypes.c_double, ctypes.c_int)\nin_mandel.restype = ctypes.c_int\n# int divide(int, int, int *)\n_divide = _mod.divide\n_divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))\n_divide.restype = ctypes.c_int\ndef divide(x, y):\n    rem = ctypes.c_int()\n    quot = _divide(x, y, rem)\n15.1. Accessing C Code Using ctypes \n| \n599",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 618,
      "chapter": 14,
      "content": "return quot,rem.value\n# void avg(double *, int n)\n# Define a special type for the 'double *' argument\nclass DoubleArrayType:\n    def from_param(self, param):\n        typename = type(param).__name__\n        if hasattr(self, 'from_' + typename):\n            return getattr(self, 'from_' + typename)(param)\n        elif isinstance(param, ctypes.Array):\n            return param\n        else:\n            raise TypeError(\"Can't convert %s\" % typename)\n    # Cast from array.array objects\n    def from_array(self, param):\n        if param.typecode != 'd':\n            raise TypeError('must be an array of doubles')\n        ptr, _ = param.buffer_info()\n        return ctypes.cast(ptr, ctypes.POINTER(ctypes.c_double))\n    # Cast from lists/tuples\n    def from_list(self, param):\n        val = ((ctypes.c_double)*len(param))(*param)\n        return val\n    from_tuple = from_list\n    # Cast from a numpy array\n    def from_ndarray(self, param):\n        return param.ctypes.data_as(ctypes.POINTER(ctypes.c_double))\nDoubleArray = DoubleArrayType()\n_avg = _mod.avg\n_avg.argtypes = (DoubleArray, ctypes.c_int)\n_avg.restype = ctypes.c_double\ndef avg(values):\n    return _avg(values, len(values))\n# struct Point { }\nclass Point(ctypes.Structure):\n    _fields_ = [('x', ctypes.c_double),\n                ('y', ctypes.c_double)]\n# double distance(Point *, Point *)\ndistance = _mod.distance\ndistance.argtypes = (ctypes.POINTER(Point), ctypes.POINTER(Point))\ndistance.restype = ctypes.c_double\nIf all goes well, you should be able to load the module and use the resulting C functions.\nFor example:\n600 \n| \nChapter 15: C Extensions",
      "content_length": 1612,
      "extraction_method": "Direct"
    },
    {
      "page_number": 619,
      "chapter": 14,
      "content": ">>> import sample\n>>> sample.gcd(35,42)\n7\n>>> sample.in_mandel(0,0,500)\n1\n>>> sample.in_mandel(2.0,1.0,500)\n0\n>>> sample.divide(42,8)\n(5, 2)\n>>> sample.avg([1,2,3])\n2.0\n>>> p1 = sample.Point(1,2)\n>>> p2 = sample.Point(4,5)\n>>> sample.distance(p1,p2)\n4.242640687119285\n>>>\nDiscussion\nThere are several aspects of this recipe that warrant some discussion. The first issue\nconcerns the overall packaging of C and Python code together. If you are using ctypes\nto access C code that you have compiled yourself, you will need to make sure that the\nshared library gets placed in a location where the sample.py module can find it. One\npossibility is to put the resulting .so file in the same directory as the supporting Python\ncode. This is what’s shown at the first part of this recipe—sample.py looks at the __file__\nvariable to see where it has been installed, and then constructs a path that points to a\nlibsample.so file in the same directory.\nIf the C library is going to be installed elsewhere, then you’ll have to adjust the path\naccordingly. If the C library is installed as a standard library on your machine, you might\nbe able to use the ctypes.util.find_library() function. For example:\n>>> from ctypes.util import find_library\n>>> find_library('m')\n'/usr/lib/libm.dylib'\n>>> find_library('pthread')\n'/usr/lib/libpthread.dylib'\n>>> find_library('sample')\n'/usr/local/lib/libsample.so'\n>>>\nAgain, ctypes won’t work at all if it can’t locate the library with the C code. Thus, you’ll\nneed to spend a few minutes thinking about how you want to install things.\nOnce you know where the C library is located, you use ctypes.cdll.LoadLibrary()\nto load it. The following statement in the solution does this where _path is the full\npathname to the shared library:\n_mod = ctypes.cdll.LoadLibrary(_path)\n15.1. Accessing C Code Using ctypes \n| \n601",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 620,
      "chapter": 14,
      "content": "Once a library has been loaded, you need to write statements that extract specific sym‐\nbols and put type signatures on them. This is what’s happening in code fragments such\nas this:\n# int in_mandel(double, double, int)\nin_mandel = _mod.in_mandel\nin_mandel.argtypes = (ctypes.c_double, ctypes.c_double, ctypes.c_int)\nin_mandel.restype = ctypes.c_int\nIn this code, the .argtypes attribute is a tuple containing the input arguments to a\nfunction, and .restype is the return type. ctypes defines a variety of type objects (e.g.,\nc_double, c_int, c_short, c_float, etc.) that represent common C data types. Attach‐\ning the type signatures is critical if you want to make Python pass the right kinds of\narguments and convert data correctly (if you don’t do this, not only will the code not\nwork, but you might cause the entire interpreter process to crash).\nA somewhat tricky part of using ctypes is that the original C code may use idioms that\ndon’t map cleanly to Python. The divide() function is a good example because it returns\na value through one of its arguments. Although that’s a common C technique, it’s often\nnot clear how it’s supposed to work in Python. For example, you can’t do anything\nstraightforward like this:\n>>> divide = _mod.divide\n>>> divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))\n>>> x = 0\n>>> divide(10, 3, x)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nctypes.ArgumentError: argument 3: <class 'TypeError'>: expected LP_c_int\ninstance instead of int\n>>>\nEven if this did work, it would violate Python’s immutability of integers and probably\ncause the entire interpreter to be sucked into a black hole. For arguments involving\npointers, you usually have to construct a compatible ctypes object and pass it in like\nthis:\n>>> x = ctypes.c_int()\n>>> divide(10, 3, x)\n3\n>>> x.value\n1\n>>>\nHere an instance of a ctypes.c_int is created and passed in as the pointer object. Unlike\na normal Python integer, a c_int object can be mutated. The .value attribute can be\nused to either retrieve or change the value as desired.\n602 \n| \nChapter 15: C Extensions",
      "content_length": 2126,
      "extraction_method": "Direct"
    },
    {
      "page_number": 621,
      "chapter": 14,
      "content": "For cases where the C calling convention is “un-Pythonic,” it is common to write a small\nwrapper function. In the solution, this code makes the divide() function return the\ntwo results using a tuple instead:\n# int divide(int, int, int *)\n_divide = _mod.divide\n_divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))\n_divide.restype = ctypes.c_int\ndef divide(x, y):\n    rem = ctypes.c_int()\n    quot = _divide(x,y,rem)\n    return quot, rem.value\nThe avg() function presents a new kind of challenge. The underlying C code expects\nto receive a pointer and a length representing an array. However, from the Python side,\nwe must consider the following questions: What is an array? Is it a list? A tuple? An\narray from the array module? A numpy array? Is it all of these? In practice, a Python\n“array” could take many different forms, and maybe you would like to support multiple\npossibilities.\nThe DoubleArrayType class shows how to handle this situation. In this class, a single\nmethod from_param() is defined. The role of this method is to take a single parameter\nand narrow it down to a compatible ctypes object (a pointer to a ctypes.c_double, in\nthe example). Within from_param(), you are free to do anything that you wish. In the\nsolution, the typename of the parameter is extracted and used to dispatch to a more\nspecialized method. For example, if a list is passed, the typename is list and a method\nfrom_list() is invoked.\nFor lists and tuples, the from_list() method performs a conversion to a ctypes array\nobject. This looks a little weird, but here is an interactive example of converting a list to\na ctypes array:\n>>> nums = [1, 2, 3]\n>>> a = (ctypes.c_double * len(nums))(*nums)\n>>> a\n<__main__.c_double_Array_3 object at 0x10069cd40>\n>>> a[0]\n1.0\n>>> a[1]\n2.0\n>>> a[2]\n3.0\n>>>\nFor array objects, the from_array() method extracts the underlying memory pointer\nand casts it to a ctypes pointer object. For example:\n15.1. Accessing C Code Using ctypes \n| \n603",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 622,
      "chapter": 14,
      "content": ">>> import array\n>>> a = array.array('d',[1,2,3])\n>>> a\narray('d', [1.0, 2.0, 3.0])\n>>> ptr_ = a.buffer_info()\n>>> ptr\n4298687200\n>>> ctypes.cast(ptr, ctypes.POINTER(ctypes.c_double))\n<__main__.LP_c_double object at 0x10069cd40>\n>>>\nThe from_ndarray() shows comparable conversion code for numpy arrays.\nBy defining the DoubleArrayType class and using it in the type signature of avg(), as\nshown, the function can accept a variety of different array-like inputs:\n>>> import sample\n>>> sample.avg([1,2,3])\n2.0\n>>> sample.avg((1,2,3))\n2.0\n>>> import array\n>>> sample.avg(array.array('d',[1,2,3]))\n2.0\n>>> import numpy\n>>> sample.avg(numpy.array([1.0,2.0,3.0]))\n2.0\n>>>\nThe last part of this recipe shows how to work with a simple C structure. For structures,\nyou simply define a class that contains the appropriate fields and types like this:\nclass Point(ctypes.Structure):\n    _fields_ = [('x', ctypes.c_double),\n                ('y', ctypes.c_double)]\nOnce defined, you can use the class in type signatures as well as in code that needs to\ninstantiate and work with the structures. For example:\n>>> p1 = sample.Point(1,2)\n>>> p2 = sample.Point(4,5)\n>>> p1.x\n1.0\n>>> p1.y\n2.0\n>>> sample.distance(p1,p2)\n4.242640687119285\n>>>\nA few final comments: ctypes is a useful library to know about if all you’re doing is\naccessing a few C functions from Python. However, if you’re trying to access a large\nlibrary, you might want to look at alternative approaches, such as Swig (described in\nRecipe 15.9) or Cython (described in Recipe 15.10).\n604 \n| \nChapter 15: C Extensions",
      "content_length": 1564,
      "extraction_method": "Direct"
    },
    {
      "page_number": 623,
      "chapter": 14,
      "content": "The main problem with a large library is that since ctypes isn’t entirely automatic, you’ll\nhave to spend a fair bit of time writing out all of the type signatures, as shown in the\nexample. Depending on the complexity of the library, you might also have to write a\nlarge number of small wrapper functions and supporting classes. Also, unless you fully\nunderstand all of the low-level details of the C interface, including memory management\nand error handling, it is often quite easy to make Python catastrophically crash with a\nsegmentation fault, access violation, or some similar error.\nAs an alternative to ctypes, you might also look at CFFI. CFFI provides much of the\nsame functionality, but uses C syntax and supports more advanced kinds of C code. As\nof this writing, CFFI is still a relatively new project, but its use has been growing rapidly.\nThere has even been some discussion of including it in the Python standard library in\nsome future release. Thus, it’s definitely something to keep an eye on.\n15.2. Writing a Simple C Extension Module\nProblem\nYou want to write a simple C extension module directly using Python’s extension API\nand no other tools.\nSolution\nFor simple C code, it is straightforward to make a handcrafted extension module. As a\npreliminary step, you probably want to make sure your C code has a proper header file.\nFor example,\n/* sample.h */\n#include <math.h>\nextern int gcd(int, int);\nextern int in_mandel(double x0, double y0, int n);\nextern int divide(int a, int b, int *remainder);\nextern double avg(double *a, int n);\ntypedef struct Point {\n    double x,y;\n} Point;\nextern double distance(Point *p1, Point *p2);\n15.2. Writing a Simple C Extension Module \n| \n605",
      "content_length": 1699,
      "extraction_method": "Direct"
    },
    {
      "page_number": 624,
      "chapter": 14,
      "content": "Typically, this header would correspond to a library that has been compiled separately.\nWith that assumption, here is a sample extension module that illustrates the basics of\nwriting extension functions:\n#include \"Python.h\"\n#include \"sample.h\"\n/* int gcd(int, int) */\nstatic PyObject *py_gcd(PyObject *self, PyObject *args) {\n  int x, y, result;\n  if (!PyArg_ParseTuple(args,\"ii\", &x, &y)) {\n    return NULL;\n  }\n  result = gcd(x,y);\n  return Py_BuildValue(\"i\", result);\n}\n/* int in_mandel(double, double, int) */\nstatic PyObject *py_in_mandel(PyObject *self, PyObject *args) {\n  double x0, y0;\n  int n;\n  int result;\n  if (!PyArg_ParseTuple(args, \"ddi\", &x0, &y0, &n)) {\n    return NULL;\n  }\n  result = in_mandel(x0,y0,n);\n  return Py_BuildValue(\"i\", result);\n}\n/* int divide(int, int, int *) */\nstatic PyObject *py_divide(PyObject *self, PyObject *args) {\n  int a, b, quotient, remainder;\n  if (!PyArg_ParseTuple(args, \"ii\", &a, &b)) {\n    return NULL;\n  }\n  quotient = divide(a,b, &remainder);\n  return Py_BuildValue(\"(ii)\", quotient, remainder);\n}\n/* Module method table */\nstatic PyMethodDef SampleMethods[] = {\n  {\"gcd\",  py_gcd, METH_VARARGS, \"Greatest common divisor\"},\n  {\"in_mandel\", py_in_mandel, METH_VARARGS, \"Mandelbrot test\"},\n  {\"divide\", py_divide, METH_VARARGS, \"Integer division\"},\n  { NULL, NULL, 0, NULL}\n};\n/* Module structure */\nstatic struct PyModuleDef samplemodule = {\n  PyModuleDef_HEAD_INIT,\n606 \n| \nChapter 15: C Extensions",
      "content_length": 1452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 625,
      "chapter": 14,
      "content": "\"sample\",           /* name of module */\n  \"A sample module\",  /* Doc string (may be NULL) */\n  -1,                 /* Size of per-interpreter state or -1 */\n  SampleMethods       /* Method table */\n};\n/* Module initialization function */\nPyMODINIT_FUNC\nPyInit_sample(void) {\n  return PyModule_Create(&samplemodule);\n}\nFor building the extension module, create a setup.py file that looks like this:\n# setup.py\nfrom distutils.core import setup, Extension\nsetup(name='sample',\n      ext_modules=[\n        Extension('sample',\n                  ['pysample.c'],\n                  include_dirs = ['/some/dir'],\n                  define_macros = [('FOO','1')],\n                  undef_macros = ['BAR'],\n                  library_dirs = ['/usr/local/lib'],\n                  libraries = ['sample']\n                  )\n        ]\n)\nNow, to build the resulting library, simply use python3 buildlib.py build_ext --\ninplace. For example:\nbash % python3 setup.py build_ext --inplace\nrunning build_ext\nbuilding 'sample' extension\ngcc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes\n -I/usr/local/include/python3.3m -c pysample.c\n -o build/temp.macosx-10.6-x86_64-3.3/pysample.o\ngcc -bundle -undefined dynamic_lookup\nbuild/temp.macosx-10.6-x86_64-3.3/pysample.o \\\n -L/usr/local/lib -lsample -o sample.so\nbash %\nAs shown, this creates a shared library called sample.so. When compiled, you should\nbe able to start importing it as a module:\n>>> import sample\n>>> sample.gcd(35, 42)\n7\n>>> sample.in_mandel(0, 0, 500)\n1\n>>> sample.in_mandel(2.0, 1.0, 500)\n15.2. Writing a Simple C Extension Module \n| \n607",
      "content_length": 1607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 626,
      "chapter": 14,
      "content": "0\n>>> sample.divide(42, 8)\n(5, 2)\n>>>\nIf you are attempting these steps on Windows, you may need to spend some time fiddling\nwith your environment and the build environment to get extension modules to build\ncorrectly. Binary distributions of Python are typically built using Microsoft Visual\nStudio. To get extensions to work, you may have to compile them using the same or\ncompatible tools. See the Python documentation.\nDiscussion\nBefore attempting any kind of handwritten extension, it is absolutely critical that you\nconsult Python’s documentation on “Extending and Embedding the Python Interpret‐\ner”. Python’s C extension API is large, and repeating all of it here is simply not practical.\nHowever, the most important parts can be easily discussed.\nFirst, in extension modules, functions that you write are all typically written with a\ncommon prototype such as this:\nstatic PyObject *py_func(PyObject *self, PyObject *args) {\n  ...\n}\nPyObject is the C data type that represents any Python object. At a very high level, an\nextension function is a C function that receives a tuple of Python objects (in PyObject\n*args) and returns a new Python object as a result. The self argument to the function\nis unused for simple extension functions, but comes into play should you want to define\nnew classes or object types in C (e.g., if the extension function were a method of a class,\nthen self would hold the instance).\nThe PyArg_ParseTuple() function is used to convert values from Python to a C rep‐\nresentation. As input, it takes a format string that indicates the required values, such as\n“i” for integer and “d” for double, as well as the addresses of C variables in which to place\nthe converted results. PyArg_ParseTuple() performs a variety of checks on the number\nand type of arguments. If there is any mismatch with the format string, an exception is\nraised and NULL is returned. By checking for this and simply returning NULL, an ap‐\npropriate exception will have been raised in the calling code.\nThe Py_BuildValue() function is used to create Python objects from C data types. It\nalso accepts a format code to indicate the desired type. In the extension functions, it is\nused to return results back to Python. One feature of Py_BuildValue() is that it can\nbuild more complicated kinds of objects, such as tuples and dictionaries. In the code\nfor py_divide(), an example showing the return of a tuple is shown. However, here are\na few more examples:\n608 \n| \nChapter 15: C Extensions",
      "content_length": 2491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 627,
      "chapter": 14,
      "content": "return Py_BuildValue(\"i\", 34);      // Return an integer\nreturn Py_BuildValue(\"d\", 3.4);     // Return a double\nreturn Py_BuildValue(\"s\", \"Hello\"); // Null-terminated UTF-8 string\nreturn Py_BuildValue(\"(ii)\", 3, 4); // Tuple (3, 4)\nNear the bottom of any extension module, you will find a function table such as the\nSampleMethods table shown in this recipe. This table lists C functions, the names to use\nin Python, as well as doc strings. All modules are required to specify such a table, as it\ngets used in the initialization of the module.\nThe final function PyInit_sample() is the module initialization function that executes\nwhen the module is first imported. The primary job of this function is to register the\nmodule object with the interpreter.\nAs a final note, it must be stressed that there is considerably more to extending Python\nwith C functions than what is shown here (in fact, the C API contains well over 500\nfunctions in it). You should view this recipe simply as a stepping stone for getting started.\nTo do more, start with the documentation on the PyArg_ParseTuple() and Py_Build\nValue() functions, and expand from there.\n15.3. Writing an Extension Function That Operates on\nArrays\nProblem\nYou want to write a C extension function that operates on contiguous arrays of data, as\nmight be created by the array module or libraries like NumPy. However, you would like\nyour function to be general purpose and not specific to any one array library.\nSolution\nTo receive and process arrays in a portable manner, you should write code that uses the\nBuffer Protocol. Here is an example of a handwritten C extension function that receives\narray data and calls the avg(double *buf, int len) function from this chapter’s in‐\ntroduction:\n/* Call double avg(double *, int) */\nstatic PyObject *py_avg(PyObject *self, PyObject *args) {\n  PyObject *bufobj;\n  Py_buffer view;\n  double result;\n  /* Get the passed Python object */\n  if (!PyArg_ParseTuple(args, \"O\", &bufobj)) {\n    return NULL;\n  }\n  /* Attempt to extract buffer information from it */\n15.3. Writing an Extension Function That Operates on Arrays \n| \n609",
      "content_length": 2120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 628,
      "chapter": 14,
      "content": "if (PyObject_GetBuffer(bufobj, &view,\n      PyBUF_ANY_CONTIGUOUS | PyBUF_FORMAT) == -1) {\n    return NULL;\n  }\n  if (view.ndim != 1) {\n    PyErr_SetString(PyExc_TypeError, \"Expected a 1-dimensional array\");\n    PyBuffer_Release(&view);\n    return NULL;\n  }\n  /* Check the type of items in the array */\n  if (strcmp(view.format,\"d\") != 0) {\n    PyErr_SetString(PyExc_TypeError, \"Expected an array of doubles\");\n    PyBuffer_Release(&view);\n    return NULL;\n  }\n  /* Pass the raw buffer and size to the C function */\n  result = avg(view.buf, view.shape[0]);\n  /* Indicate we're done working with the buffer */\n  PyBuffer_Release(&view);\n  return Py_BuildValue(\"d\", result);\n}\nHere is an example that shows how this extension function works:\n>>> import array\n>>> avg(array.array('d',[1,2,3]))\n2.0\n>>> import numpy\n>>> avg(numpy.array([1.0,2.0,3.0]))\n2.0\n>>> avg([1,2,3])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'list' does not support the buffer interface\n>>> avg(b'Hello')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: Expected an array of doubles\n>>> a = numpy.array([[1.,2.,3.],[4.,5.,6.]])\n>>> avg(a[:,2])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: ndarray is not contiguous\n>>> sample.avg(a)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: Expected a 1-dimensional array\n>>> sample.avg(a[0])\n610 \n| \nChapter 15: C Extensions",
      "content_length": 1486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 629,
      "chapter": 14,
      "content": "2.0\n>>>\nDiscussion\nPassing array objects to C functions might be one of the most common things you would\nwant to do with a extension function. A large number of Python applications, ranging\nfrom image processing to scientific computing, are based on high-performance array\nprocessing. By writing code that can accept and operate on arrays, you can write cus‐\ntomized code that plays nicely with those applications as opposed to having some sort\nof custom solution that only works with your own code.\nThe key to this code is the PyBuffer_GetBuffer() function. Given an arbitrary Python\nobject, it tries to obtain information about the underlying memory representation. If\nit’s not possible, as is the case with most normal Python objects, it simply raises an\nexception and returns -1. The special flags passed to PyBuffer_GetBuffer() give\nadditional hints about the kind of memory buffer that is requested. For example,\nPyBUF_ANY_CONTIGUOUS specifies that a contiguous region of memory is required.\nFor arrays, byte strings, and other similar objects, a Py_buffer structure is filled with\ninformation about the underlying memory. This includes a pointer to the memory, size,\nitemsize, format, and other details. Here is the definition of this structure:\ntypedef struct bufferinfo {\n    void *buf;              /* Pointer to buffer memory */\n    PyObject *obj;          /* Python object that is the owner */\n    Py_ssize_t len;         /* Total size in bytes */\n    Py_ssize_t itemsize;    /* Size in bytes of a single item */\n    int readonly;           /* Read-only access flag */\n    int ndim;               /* Number of dimensions */\n    char *format;           /* struct code of a single item */\n    Py_ssize_t *shape;      /* Array containing dimensions */\n    Py_ssize_t *strides;    /* Array containing strides */\n    Py_ssize_t *suboffsets; /* Array containing suboffsets */\n} Py_buffer;\nIn this recipe, we are simply concerned with receiving a contiguous array of doubles.\nTo check if items are a double, the format attribute is checked to see if the string is\n\"d\". This is the same code that the struct module uses when encoding binary values.\nAs a general rule, format could be any format string that’s compatible with the struct\nmodule and might include multiple items in the case of arrays containing C structures.\nOnce we have verified the underlying buffer information, we simply pass it to the C\nfunction, which treats it as a normal C array. For all practical purposes, it is not con‐\ncerned with what kind of array it is or what library created it. This is how the function\nis able to work with arrays created by the array module or by numpy.\n15.3. Writing an Extension Function That Operates on Arrays \n| \n611",
      "content_length": 2727,
      "extraction_method": "Direct"
    },
    {
      "page_number": 630,
      "chapter": 14,
      "content": "Before returning a final result, the underlying buffer view must be released using \nPyBuffer_Release(). This step is required to properly manage reference counts of\nobjects.\nAgain, this recipe only shows a tiny fragment of code that receives an array. If working\nwith arrays, you might run into issues with multidimensional data, strided data, different\ndata types, and more that will require study. Make sure you consult the official docu‐\nmentation to get more details.\nIf you need to write many extensions involving array handling, you may find it easier\nto implement the code in Cython. See Recipe 15.11. \n15.4. Managing Opaque Pointers in C Extension Modules\nProblem\nYou have an extension module that needs to handle a pointer to a C data structure, but\nyou don’t want to expose any internal details of the structure to Python.\nSolution\nOpaque data structures are easily handled by wrapping them inside capsule objects.\nConsider this fragment of C code from our sample code:\ntypedef struct Point {\n    double x,y;\n} Point;\nextern double distance(Point *p1, Point *p2);\nHere is an example of extension code that wraps the Point structure and distance()\nfunction using capsules:\n/* Destructor function for points */\nstatic void del_Point(PyObject *obj) {\n  free(PyCapsule_GetPointer(obj,\"Point\"));\n}\n/* Utility functions */\nstatic Point *PyPoint_AsPoint(PyObject *obj) {\n  return (Point *) PyCapsule_GetPointer(obj, \"Point\");\n}\nstatic PyObject *PyPoint_FromPoint(Point *p, int must_free) {\n  return PyCapsule_New(p, \"Point\", must_free ? del_Point : NULL);\n}\n/* Create a new Point object */\nstatic PyObject *py_Point(PyObject *self, PyObject *args) {\n612 \n| \nChapter 15: C Extensions",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 631,
      "chapter": 14,
      "content": "Point *p;\n  double x,y;\n  if (!PyArg_ParseTuple(args,\"dd\",&x,&y)) {\n    return NULL;\n  }\n  p = (Point *) malloc(sizeof(Point));\n  p->x = x;\n  p->y = y;\n  return PyPoint_FromPoint(p, 1);\n}\nstatic PyObject *py_distance(PyObject *self, PyObject *args) {\n  Point *p1, *p2;\n  PyObject *py_p1, *py_p2;\n  double result;\n  if (!PyArg_ParseTuple(args,\"OO\",&py_p1, &py_p2)) {\n    return NULL;\n  }\n  if (!(p1 = PyPoint_AsPoint(py_p1))) {\n    return NULL;\n  }\n  if (!(p2 = PyPoint_AsPoint(py_p2))) {\n    return NULL;\n  }\n  result = distance(p1,p2);\n  return Py_BuildValue(\"d\", result);\n}\nUsing these functions from Python looks like this:\n>>> import sample\n>>> p1 = sample.Point(2,3)\n>>> p2 = sample.Point(4,5)\n>>> p1\n<capsule object \"Point\" at 0x1004ea330>\n>>> p2\n<capsule object \"Point\" at 0x1005d1db0>\n>>> sample.distance(p1,p2)\n2.8284271247461903\n>>>\nDiscussion\nCapsules are similar to a typed C pointer. Internally, they hold a generic pointer along\nwith an identifying name and can be easily created using the PyCapsule_New() function.\nIn addition, an optional destructor function can be attached to a capsule to release the\nunderlying memory when the capsule object is garbage collected.\n15.4. Managing Opaque Pointers in C Extension Modules \n| \n613",
      "content_length": 1244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 632,
      "chapter": 14,
      "content": "To extract the pointer contained inside a capsule, use the PyCapsule_GetPointer()\nfunction and specify the name. If the supplied name doesn’t match that of the capsule\nor some other error occurs, an exception is raised and NULL is returned.\nIn this recipe, a pair of utility functions—PyPoint_FromPoint() and PyPoint_As\nPoint()—have been written to deal with the mechanics of creating and unwinding\nPoint instances from capsule objects. In any extension functions, we’ll use these func‐\ntions instead of working with capsules directly. This design choice makes it easier to\ndeal with possible changes to the wrapping of Point objects in the future. For example,\nif you decided to use something other than a capsule later, you would only have to change\nthese two functions.\nOne tricky part about capsules concerns garbage collection and memory management.\nThe PyPoint_FromPoint() function accepts a must_free argument that indicates\nwhether the underlying Point * structure is to be collected when the capsule is de‐\nstroyed. When working with certain kinds of C code, ownership issues can be difficult\nto handle (e.g., perhaps a Point structure is embedded within a larger data structure\nthat is managed separately). Rather than making a unilateral decision to garbage collect,\nthis extra argument gives control back to the programmer. It should be noted that the\ndestructor associated with an existing capsule can also be changed using the PyCap\nsule_SetDestructor() function.\nCapsules are a sensible solution to interfacing with certain kinds of C code involving\nstructures. For instance, sometimes you just don’t care about exposing the internals of\na structure or turning it into a full-fledged extension type. With a capsule, you can put\na lightweight wrapper around it and easily pass it around to other extension functions.\n15.5. Defining and Exporting C APIs from Extension\nModules\nProblem\nYou have a C extension module that internally defines a variety of useful functions that\nyou would like to export as a public C API for use elsewhere. You would like to use these\nfunctions inside other extension modules, but don’t know how to link them together,\nand doing it with the C compiler/linker seems excessively complicated (or impossible).\nSolution\nThis recipe focuses on the code written to handle Point objects, which were presented\nin Recipe 15.4. If you recall, that C code included some utility functions like this:\n/* Destructor function for points */\nstatic void del_Point(PyObject *obj) {\n614 \n| \nChapter 15: C Extensions",
      "content_length": 2537,
      "extraction_method": "Direct"
    },
    {
      "page_number": 633,
      "chapter": 14,
      "content": "free(PyCapsule_GetPointer(obj,\"Point\"));\n}\n/* Utility functions */\nstatic Point *PyPoint_AsPoint(PyObject *obj) {\n  return (Point *) PyCapsule_GetPointer(obj, \"Point\");\n}\nstatic PyObject *PyPoint_FromPoint(Point *p, int must_free) {\n  return PyCapsule_New(p, \"Point\", must_free ? del_Point : NULL);\n}\nThe problem now addressed is how to export the PyPoint_AsPoint() and Py\nPoint_FromPoint() functions as an API that other extension modules could use and\nlink to (e.g., if you have other extensions that also want to use the wrapped Point\nobjects).\nTo solve this problem, start by introducing a new header file for the “sample” extension\ncalled pysample.h. Put the following code in it:\n/* pysample.h */\n#include \"Python.h\"\n#include \"sample.h\"\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n/* Public API Table */\ntypedef struct {\n  Point *(*aspoint)(PyObject *);\n  PyObject *(*frompoint)(Point *, int);\n} _PointAPIMethods;\n#ifndef PYSAMPLE_MODULE\n/* Method table in external module */\nstatic _PointAPIMethods *_point_api = 0;\n/* Import the API table from sample */\nstatic int import_sample(void) {\n  _point_api = (_PointAPIMethods *) PyCapsule_Import(\"sample._point_api\",0);\n  return (_point_api != NULL) ? 1 : 0;\n}\n/* Macros to implement the programming interface */\n#define PyPoint_AsPoint(obj) (_point_api->aspoint)(obj)\n#define PyPoint_FromPoint(obj) (_point_api->frompoint)(obj)\n#endif\n#ifdef __cplusplus\n}\n#endif\n15.5. Defining and Exporting C APIs from Extension Modules \n| \n615",
      "content_length": 1477,
      "extraction_method": "Direct"
    },
    {
      "page_number": 634,
      "chapter": 14,
      "content": "The most important feature here is the _PointAPIMethods table of function pointers. It\nwill be initialized in the exporting module and found by importing modules.\nChange the original extension module to populate the table and export it as follows:\n/* pysample.c */\n#include \"Python.h\"\n#define PYSAMPLE_MODULE\n#include \"pysample.h\"\n...\n/* Destructor function for points */\nstatic void del_Point(PyObject *obj) {\n  printf(\"Deleting point\\n\");\n  free(PyCapsule_GetPointer(obj,\"Point\"));\n}\n/* Utility functions */\nstatic Point *PyPoint_AsPoint(PyObject *obj) {\n  return (Point *) PyCapsule_GetPointer(obj, \"Point\");\n}\nstatic PyObject *PyPoint_FromPoint(Point *p, int free) {\n  return PyCapsule_New(p, \"Point\", free ? del_Point : NULL);\n}\nstatic _PointAPIMethods _point_api = {\n  PyPoint_AsPoint,\n  PyPoint_FromPoint\n};\n...\n/* Module initialization function */\nPyMODINIT_FUNC\nPyInit_sample(void) {\n  PyObject *m;\n  PyObject *py_point_api;\n  m = PyModule_Create(&samplemodule);\n  if (m == NULL)\n    return NULL;\n  /* Add the Point C API functions */\n  py_point_api = PyCapsule_New((void *) &_point_api, \"sample._point_api\", NULL);\n  if (py_point_api) {\n    PyModule_AddObject(m, \"_point_api\", py_point_api);\n  }\n  return m;\n}\n616 \n| \nChapter 15: C Extensions",
      "content_length": 1252,
      "extraction_method": "Direct"
    },
    {
      "page_number": 635,
      "chapter": 14,
      "content": "Finally, here is an example of a new extension module that loads and uses these API\nfunctions:\n/* ptexample.c */\n/* Include the header associated with the other module */\n#include \"pysample.h\"\n/* An extension function that uses the exported API */\nstatic PyObject *print_point(PyObject *self, PyObject *args) {\n  PyObject *obj;\n  Point *p;\n  if (!PyArg_ParseTuple(args,\"O\", &obj)) {\n    return NULL;\n  }\n  /* Note: This is defined in a different module */\n  p = PyPoint_AsPoint(obj);\n  if (!p) {\n    return NULL;\n  }\n  printf(\"%f %f\\n\", p->x, p->y);\n  return Py_BuildValue(\"\");\n}\nstatic PyMethodDef PtExampleMethods[] = {\n  {\"print_point\", print_point, METH_VARARGS, \"output a point\"},\n  { NULL, NULL, 0, NULL}\n};\nstatic struct PyModuleDef ptexamplemodule = {\n  PyModuleDef_HEAD_INIT,\n  \"ptexample\",           /* name of module */\n  \"A module that imports an API\",  /* Doc string (may be NULL) */\n  -1,                 /* Size of per-interpreter state or -1 */\n  PtExampleMethods       /* Method table */\n};\n/* Module initialization function */\nPyMODINIT_FUNC\nPyInit_ptexample(void) {\n  PyObject *m;\n  m = PyModule_Create(&ptexamplemodule);\n  if (m == NULL)\n    return NULL;\n  /* Import sample, loading its API functions */\n  if (!import_sample()) {\n    return NULL;\n  }\n15.5. Defining and Exporting C APIs from Extension Modules \n| \n617",
      "content_length": 1337,
      "extraction_method": "Direct"
    },
    {
      "page_number": 636,
      "chapter": 14,
      "content": "return m;\n}\nWhen compiling this new module, you don’t even need to bother to link against any of\nthe libraries or code from the other module. For example, you can just make a simple\nsetup.py file like this:\n# setup.py\nfrom distutils.core import setup, Extension\nsetup(name='ptexample',\n      ext_modules=[\n        Extension('ptexample',\n                  ['ptexample.c'],\n                  include_dirs = [],  # May need pysample.h directory\n                  )\n        ]\n)\nIf it all works, you’ll find that your new extension function works perfectly with the C\nAPI functions defined in the other module:\n>>> import sample\n>>> p1 = sample.Point(2,3)\n>>> p1\n<capsule object \"Point *\" at 0x1004ea330>\n>>> import ptexample\n>>> ptexample.print_point(p1)\n2.000000 3.000000\n>>>\nDiscussion\nThis recipe relies on the fact that capsule objects can hold a pointer to anything you\nwish. In this case, the defining module populates a structure of function pointers, creates\na capsule that points to it, and saves the capsule in a module-level attribute (e.g., sam\nple._point_api).\nOther modules can be programmed to pick up this attribute when imported and extract\nthe underlying pointer. In fact, Python provides the PyCapsule_Import() utility func‐\ntion, which takes care of all the steps for you. You simply give it the name of the attribute\n(e.g., sample._point_api), and it will find the capsule and extract the pointer all in one\nstep.\nThere are some C programming tricks involved in making exported functions look\nnormal in other modules. In the pysample.h file, a pointer _point_api is used to point\nto the method table that was initialized in the exporting module. A related function\nimport_sample() is used to perform the required capsule import and initialize this\npointer. This function must be called before any functions are used. Normally, it would\n618 \n| \nChapter 15: C Extensions",
      "content_length": 1885,
      "extraction_method": "Direct"
    },
    {
      "page_number": 637,
      "chapter": 14,
      "content": "be called in during module initialization. Finally, a set of C preprocessor macros have\nbeen defined to transparently dispatch the API functions through the method table.\nThe user just uses the original function names, but doesn’t know about the extra indi‐\nrection through these macros.\nFinally, there is another important reason why you might use this technique to link\nmodules together—it’s actually easier and it keeps modules more cleanly decoupled. If\nyou didn’t want to use this recipe as shown, you might be able to cross-link modules\nusing advanced features of shared libraries and the dynamic loader. For example, putting\ncommon API functions into a shared library and making sure that all extension modules\nlink against that shared library. Yes, this works, but it can be tremendously messy in\nlarge systems. Essentially, this recipe cuts out all of that magic and allows modules to\nlink to one another through Python’s normal import mechanism and just a tiny number\nof capsule calls. For compilation of modules, you only need to worry about header files,\nnot the hairy details of shared libraries.\nFurther information about providing C APIs for extension modules can be found in the\nPython documentation.\n15.6. Calling Python from C\nProblem\nYou want to safely execute a Python callable from C and return a result back to C. For\nexample, perhaps you are writing C code that wants to use a Python function as a\ncallback.\nSolution\nCalling Python from C is mostly straightforward, but involves a number of tricky parts.\nThe following C code shows an example of how to do it safely:\n#include <Python.h>\n/* Execute func(x,y) in the Python interpreter.  The\n   arguments and return result of the function must\n   be Python floats */\ndouble call_func(PyObject *func, double x, double y) {\n  PyObject *args;\n  PyObject *kwargs;\n  PyObject *result = 0;\n  double retval;\n  /* Make sure we own the GIL */\n  PyGILState_STATE state = PyGILState_Ensure();\n15.6. Calling Python from C \n| \n619",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 638,
      "chapter": 14,
      "content": "/* Verify that func is a proper callable */\n  if (!PyCallable_Check(func)) {\n    fprintf(stderr,\"call_func: expected a callable\\n\");\n    goto fail;\n  }\n  /* Build arguments */\n  args = Py_BuildValue(\"(dd)\", x, y);\n  kwargs = NULL;\n  /* Call the function */\n  result = PyObject_Call(func, args, kwargs);\n  Py_DECREF(args);\n  Py_XDECREF(kwargs);\n  /* Check for Python exceptions (if any) */\n  if (PyErr_Occurred()) {\n    PyErr_Print();\n    goto fail;\n  }\n  /* Verify the result is a float object */\n  if (!PyFloat_Check(result)) {\n    fprintf(stderr,\"call_func: callable didn't return a float\\n\");\n    goto fail;\n  }\n  /* Create the return value */\n  retval = PyFloat_AsDouble(result);\n  Py_DECREF(result);\n  /* Restore previous GIL state and return */\n  PyGILState_Release(state);\n  return retval;\nfail:\n  Py_XDECREF(result);\n  PyGILState_Release(state);\n  abort();   // Change to something more appropriate\n}\nTo use this function, you need to have obtained a reference to an existing Python callable\nto pass in. There are many ways that you can go about doing that, such as having a\ncallable object passed into an extension module or simply writing C code to extract a\nsymbol from an existing module.\nHere is a simple example that shows calling a function from an embedded Python\ninterpreter:\n#include <Python.h>\n/* Definition of call_func() same as above */\n...\n620 \n| \nChapter 15: C Extensions",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 639,
      "chapter": 14,
      "content": "/* Load a symbol from a module */\nPyObject *import_name(const char *modname, const char *symbol) {\n  PyObject *u_name, *module;\n  u_name = PyUnicode_FromString(modname);\n  module = PyImport_Import(u_name);\n  Py_DECREF(u_name);\n  return PyObject_GetAttrString(module, symbol);\n}\n/* Simple embedding example */\nint main() {\n  PyObject *pow_func;\n  double x;\n  Py_Initialize();\n  /* Get a reference to the math.pow function */\n  pow_func = import_name(\"math\",\"pow\");\n  /* Call it using our call_func() code */\n  for (x = 0.0; x < 10.0; x += 0.1) {\n    printf(\"%0.2f %0.2f\\n\", x, call_func(pow_func,x,2.0));\n  }\n  /* Done */\n  Py_DECREF(pow_func);\n  Py_Finalize();\n  return 0;\n}\nTo build this last example, you’ll need to compile the C and link against the Python\ninterpreter. Here is a Makefile that shows how you might do it (this is something that\nmight require some amount of fiddling with on your machine):\nall::\n        cc -g embed.c -I/usr/local/include/python3.3m \\\n          -L/usr/local/lib/python3.3/config-3.3m -lpython3.3m\nCompiling and running the resulting executable should produce output similar to this:\n0.00 0.00\n0.10 0.01\n0.20 0.04\n0.30 0.09\n0.40 0.16\n...\nHere is a slightly different example that shows an extension function that receives a\ncallable and some arguments and passes them to call_func() for the purposes of\ntesting:\n/* Extension function for testing the C-Python callback */\nPyObject *py_call_func(PyObject *self, PyObject *args) {\n  PyObject *func;\n15.6. Calling Python from C \n| \n621",
      "content_length": 1515,
      "extraction_method": "Direct"
    },
    {
      "page_number": 640,
      "chapter": 14,
      "content": "double x, y, result;\n  if (!PyArg_ParseTuple(args,\"Odd\", &func,&x,&y)) {\n    return NULL;\n  }\n  result = call_func(func, x, y);\n  return Py_BuildValue(\"d\", result);\n}\nUsing this extension function, you could test it as follows:\n>>> import sample\n>>> def add(x,y):\n...     return x+y\n...\n>>> sample.call_func(add,3,4)\n7.0\n>>>\nDiscussion\nIf you are calling Python from C, the most important thing to keep in mind is that C is\ngenerally going to be in charge. That is, C has the responsibility of creating the argu‐\nments, calling the Python function, checking for exceptions, checking types, extracting\nreturn values, and more.\nAs a first step, it is critical that you have a Python object representing the callable that\nyou’re going to invoke. This could be a function, class, method, built-in method, or\nanything that implements the __call__() operation. To verify that it’s callable, use \nPyCallable_Check() as shown in this code fragment:\ndouble call_func(PyObject *func, double x, double y) {\n  ...\n  /* Verify that func is a proper callable */\n  if (!PyCallable_Check(func)) {\n    fprintf(stderr,\"call_func: expected a callable\\n\");\n    goto fail;\n  }\n  ...\nAs an aside, handling errors in the C code is something that you will need to carefully\nstudy. As a general rule, you can’t just raise a Python exception. Instead, errors will have\nto be handled in some other manner that makes sense to your C code. In the solution,\nwe’re using goto to transfer control to an error handling block that calls abort(). This\ncauses the whole program to die, but in real code you would probably want to do some‐\nthing more graceful (e.g., return a status code). Keep in mind that C is in charge here,\nso there isn’t anything comparable to just raising an exception. Error handling is some‐\nthing you’ll have to engineer into the program somehow.\nCalling a function is relatively straightforward—simply use PyObject_Call(), supply‐\ning it with the callable object, a tuple of arguments, and an optional dictionary of\n622 \n| \nChapter 15: C Extensions",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 641,
      "chapter": 15,
      "content": "keyword arguments. To build the argument tuple or dictionary, you can use Py_Build\nValue(), as shown.\ndouble call_func(PyObject *func, double x, double y) {\n  PyObject *args;\n  PyObject *kwargs;\n  ...\n  /* Build arguments */\n  args = Py_BuildValue(\"(dd)\", x, y);\n  kwargs = NULL;\n  /* Call the function */\n  result = PyObject_Call(func, args, kwargs);\n  Py_DECREF(args);\n  Py_XDECREF(kwargs);\n  ...\nIf there are no keyword arguments, you can pass NULL, as shown. After making the\nfunction call, you need to make sure that you clean up the arguments using Py_DE\nCREF() or Py_XDECREF(). The latter function safely allows the NULL pointer to be\npassed (which is ignored), which is why we’re using it for cleaning up the optional\nkeyword arguments.\nAfter calling the Python function, you must check for the presence of exceptions. The \nPyErr_Occurred() function can be used to do this. Knowing what to do in response to\nan exception is tricky. Since you’re working from C, you really don’t have the exception\nmachinery that Python has. Thus, you would have to set an error status code, log the\nerror, or do some kind of sensible processing. In the solution, abort() is called for lack\nof a simpler alternative (besides, hardened C programmers will appreciate the abrupt\ncrash):\n  ...\n  /* Check for Python exceptions (if any) */\n  if (PyErr_Occurred()) {\n    PyErr_Print();\n    goto fail;\n  }\n  ...\nfail:\n  PyGILState_Release(state);\n  abort();\nExtracting information from the return value of calling a Python function is typically\ngoing to involve some kind of type checking and value extraction. To do this, you may\nhave to use functions in the Python concrete objects layer. In the solution, the code\nchecks for and extracts the value of a Python float using PyFloat_Check() and Py\nFloat_AsDouble().\n15.6. Calling Python from C \n| \n623",
      "content_length": 1834,
      "extraction_method": "Direct"
    },
    {
      "page_number": 642,
      "chapter": 15,
      "content": "A final tricky part of calling into Python from C concerns the management of Python’s\nglobal interpreter lock (GIL). Whenever Python is accessed from C, you need to make\nsure that the GIL is properly acquired and released. Otherwise, you run the risk of having\nthe interpreter corrupt data or crash. The calls to PyGILState_Ensure() and PyGIL\nState_Release() make sure that it’s done correctly:\ndouble call_func(PyObject *func, double x, double y) {\n  ...\n  double retval;\n  /* Make sure we own the GIL */\n  PyGILState_STATE state = PyGILState_Ensure();\n  ...\n  /* Code that uses Python C API functions */\n  ...\n  /* Restore previous GIL state and return */\n  PyGILState_Release(state);\n  return retval;\nfail:\n  PyGILState_Release(state);\n  abort();\n}\nUpon return, PyGILState_Ensure() always guarantees that the calling thread has ex‐\nclusive access to the Python interpreter. This is true even if the calling C code is running\na different thread that is unknown to the interpreter. At this point, the C code is free to\nuse any Python C-API functions that it wants. Upon successful completion, PyGIL\nState_Release() is used to restore the interpreter back to its original state.\nIt is critical to note that every PyGILState_Ensure() call must be followed by a matching\nPyGILState_Release() call—even in cases where errors have occurred. In the solution,\nthe use of a goto statement might look like a horrible design, but we’re actually using it\nto transfer control to a common exit block that performs this required step. Think of\nthe code after the fail: lable as serving the same purpose as code in a Python final\nly: block.\nIf you write your C code using all of these conventions including management of the\nGIL, checking for exceptions, and thorough error checking, you’ll find that you can\nreliably call into the Python interpreter from C—even in very complicated programs\nthat utilize advanced programming techniques such as multithreading.\n624 \n| \nChapter 15: C Extensions",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 643,
      "chapter": 15,
      "content": "15.7. Releasing the GIL in C Extensions\nProblem\nYou have C extension code in that you want to execute concurrently with other threads\nin the Python interpreter. To do this, you need to release and reacquire the global in‐\nterpreter lock (GIL).\nSolution\nIn C extension code, the GIL can be released and reacquired by inserting the following\nmacros in the code:\n#include \"Python.h\"\n...\nPyObject *pyfunc(PyObject *self, PyObject *args) {\n   ...\n   Py_BEGIN_ALLOW_THREADS\n   // Threaded C code.  Must not use Python API functions\n   ...\n   Py_END_ALLOW_THREADS\n   ...\n   return result;\n}\nDiscussion\nThe GIL can only safely be released if you can guarantee that no Python C API functions\nwill be executed in the C code. Typical examples where the GIL might be released are\nin computationally intensive code that performs calculations on C arrays (e.g., in ex‐\ntensions such as numpy) or in code where blocking I/O operations are going to be per‐\nformed (e.g., reading or writing on a file descriptor).\nWhile the GIL is released, other Python threads are allowed to execute in the interpreter.\nThe Py_END_ALLOW_THREADS macro blocks execution until the calling threads reacquires\nthe GIL in the interpreter.\n15.8. Mixing Threads from C and Python\nProblem\nYou have a program that involves a mix of C, Python, and threads, but some of the\nthreads are created from C outside the control of the Python interpreter. Moreover,\ncertain threads utilize functions in the Python C API.\n15.7. Releasing the GIL in C Extensions \n| \n625",
      "content_length": 1516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 644,
      "chapter": 15,
      "content": "Solution\nIf you’re going to mix C, Python, and threads together, you need to make sure you\nproperly initialize and manage Python’s global interpreter lock (GIL). To do this, include\nthe following code somewhere in your C code and make sure it’s called prior to creation\nof any threads:\n#include <Python.h>\n  ...\n  if (!PyEval_ThreadsInitialized()) {\n    PyEval_InitThreads();\n  }\n  ...\nFor any C code that involves Python objects or the Python C API, make sure you prop‐\nerly acquire and release the GIL first. This is done using PyGILState_Ensure() and\nPyGILState_Release(), as shown in the following:\n  ...\n  /* Make sure we own the GIL */\n  PyGILState_STATE state = PyGILState_Ensure();\n  /* Use functions in the interpreter */\n  ...\n  /* Restore previous GIL state and return */\n  PyGILState_Release(state);\n  ...\nEvery call to PyGILState_Ensure() must have a matching call to PyGILState_Re\nlease().\nDiscussion\nIn advanced applications involving C and Python, it is not uncommon to have many\nthings going on at once—possibly involving a mix of a C code, Python code, C threads,\nand Python threads. As long as you diligently make sure the interpreter is properly\ninitialized and that C code involving the interpreter has the proper GIL management\ncalls, it all should work.\nBe aware that the PyGILState_Ensure() call does not immediately preempt or interrupt\nthe interpreter. If other code is currently executing, this function will block until that\ncode decides to release the GIL. Internally, the interpreter performs periodic thread\nswitching, so even if another thread is executing, the caller will eventually get to run\n(although it may have to wait for a while first).\n626 \n| \nChapter 15: C Extensions",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 645,
      "chapter": 15,
      "content": "15.9. Wrapping C Code with Swig\nProblem\nYou have existing C code that you would like to access as a C extension module. You\nwould like to do this using the Swig wrapper generator.\nSolution\nSwig operates by parsing C header files and automatically creating extension code. To\nuse it, you first need to have a C header file. For example, this header file for our sample\ncode:\n/* sample.h */\n#include <math.h>\nextern int gcd(int, int);\nextern int in_mandel(double x0, double y0, int n);\nextern int divide(int a, int b, int *remainder);\nextern double avg(double *a, int n);\ntypedef struct Point {\n    double x,y;\n} Point;\nextern double distance(Point *p1, Point *p2);\nOnce you have the header files, the next step is to write a Swig “interface” file. By con‐\nvention, these files have a .i suffix and might look similar to the following:\n// sample.i - Swig interface\n%module sample\n%{\n#include \"sample.h\"\n%}\n/* Customizations */\n%extend Point {\n    /* Constructor for Point objects */\n    Point(double x, double y) {\n        Point *p = (Point *) malloc(sizeof(Point));\n        p->x = x;\n        p->y = y;\n        return p;\n   };\n};\n/* Map int *remainder as an output argument */\n%include typemaps.i\n%apply int *OUTPUT { int * remainder };\n15.9. Wrapping C Code with Swig \n| \n627",
      "content_length": 1274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 646,
      "chapter": 15,
      "content": "/* Map the argument pattern (double *a, int n) to arrays */\n%typemap(in) (double *a, int n)(Py_buffer view) {\n  view.obj = NULL;\n  if (PyObject_GetBuffer($input, &view, PyBUF_ANY_CONTIGUOUS | PyBUF_FORMAT) == -1) {\n    SWIG_fail;\n  }\n  if (strcmp(view.format,\"d\") != 0) {\n    PyErr_SetString(PyExc_TypeError, \"Expected an array of doubles\");\n    SWIG_fail;\n  }\n  $1 = (double *) view.buf;\n  $2 = view.len / sizeof(double);\n}\n%typemap(freearg) (double *a, int n) {\n  if (view$argnum.obj) {\n    PyBuffer_Release(&view$argnum);\n  }\n}\n/* C declarations to be included in the extension module */\nextern int gcd(int, int);\nextern int in_mandel(double x0, double y0, int n);\nextern int divide(int a, int b, int *remainder);\nextern double avg(double *a, int n);\ntypedef struct Point {\n    double x,y;\n} Point;\nextern double distance(Point *p1, Point *p2);\nOnce you have written the interface file, Swig is invoked as a command-line tool:\nbash % swig -python -py3 sample.i\nbash %\nThe output of swig is two files, sample_wrap.c and sample.py. The latter file is what\nusers import. The sample_wrap.c file is C code that needs to be compiled into a sup‐\nporting module called _sample. This is done using the same techniques as for normal\nextension modules. For example, you create a setup.py file like this:\n# setup.py\nfrom distutils.core import setup, Extension\nsetup(name='sample',\n      py_modules=['sample.py'],\n      ext_modules=[\n        Extension('_sample',\n                  ['sample_wrap.c'],\n                  include_dirs = [],\n                  define_macros = [],\n628 \n| \nChapter 15: C Extensions",
      "content_length": 1597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 647,
      "chapter": 15,
      "content": "undef_macros = [],\n                  library_dirs = [],\n                  libraries = ['sample']\n                  )\n        ]\n)\nTo compile and test, run python3 on the setup.py file like this:\nbash % python3 setup.py build_ext --inplace\nrunning build_ext\nbuilding '_sample' extension\ngcc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes\n-I/usr/local/include/python3.3m -c sample_wrap.c\n -o build/temp.macosx-10.6-x86_64-3.3/sample_wrap.o\nsample_wrap.c: In function ‘SWIG_InitializeModule’:\nsample_wrap.c:3589: warning: statement with no effect\ngcc -bundle -undefined dynamic_lookup build/temp.macosx-10.6-x86_64-3.3/sample.o\n build/temp.macosx-10.6-x86_64-3.3/sample_wrap.o -o _sample.so -lsample\nbash %\nIf all of this works, you’ll find that you can use the resulting C extension module in a\nstraightforward way. For example:\n>>> import sample\n>>> sample.gcd(42,8)\n2\n>>> sample.divide(42,8)\n[5, 2]\n>>> p1 = sample.Point(2,3)\n>>> p2 = sample.Point(4,5)\n>>> sample.distance(p1,p2)\n2.8284271247461903\n>>> p1.x\n2.0\n>>> p1.y\n3.0\n>>> import array\n>>> a = array.array('d',[1,2,3])\n>>> sample.avg(a)\n2.0\n>>>\nDiscussion\nSwig is one of the oldest tools for building extension modules, dating back to Python\n1.4. However, recent versions currently support Python 3. The primary users of Swig\ntend to have large existing bases of C that they are trying to access using Python as a\nhigh-level control language. For instance, a user might have C code containing thou‐\nsands of functions and various data structures that they would like to access from\nPython. Swig can automate much of the wrapper generation process.\n15.9. Wrapping C Code with Swig \n| \n629",
      "content_length": 1666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 648,
      "chapter": 15,
      "content": "All Swig interfaces tend to start with a short preamble like this:\n%module sample\n%{\n#include \"sample.h\"\n%}\nThis merely declares the name of the extension module and specifies C header files that\nmust be included to make everything compile (the code enclosed in %{ and %} is pasted\ndirectly into the output code so this is where you put all included files and other defi‐\nnitions needed for compilation).\nThe bottom part of a Swig interface is a listing of C declarations that you want to be\nincluded in the extension. This is often just copied from the header files. In our example,\nwe just pasted in the header file directly like this:\n%module sample\n%{\n#include \"sample.h\"\n%}\n...\nextern int gcd(int, int);\nextern int in_mandel(double x0, double y0, int n);\nextern int divide(int a, int b, int *remainder);\nextern double avg(double *a, int n);\ntypedef struct Point {\n    double x,y;\n} Point;\nextern double distance(Point *p1, Point *p2);\nIt is important to stress that these declarations are telling Swig what you want to include\nin the Python module. It is quite common to edit the list of declarations or to make\nmodifications as appropriate. For example, if you didn’t want certain declarations to be\nincluded, you would remove them from the declaration list.\nThe most complicated part of using Swig is the various customizations that it can apply\nto the C code. This is a huge topic that can’t be covered in great detail here, but a number\nof such customizations are shown in this recipe.\nThe first customization involving the %extend directive allows methods to be attached\nto existing structure and class definitions. In the example, this is used to add a con‐\nstructor method to the Point structure. This customization makes it possible to use the\nstructure like this:\n>>> p1 = sample.Point(2,3)\n>>>\nIf omitted, then Point objects would have to be created in a much more clumsy manner\nlike this:\n630 \n| \nChapter 15: C Extensions",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 649,
      "chapter": 15,
      "content": ">>> # Usage if %extend Point is omitted\n>>> p1 = sample.Point()\n>>> p1.x = 2.0\n>>> p1.y = 3\nThe second customization involving the inclusion of the typemaps.i library and the \n%apply directive is instructing Swig that the argument signature int *remainder is to\nbe treated as an output value. This is actually a pattern matching rule. In all declarations\nthat follow, any time int *remainder is encountered, it is handled as output. This\ncustomization is what makes the divide() function return two values:\n>>> sample.divide(42,8)\n[5, 2]\n>>>\nThe last customization involving the %typemap directive is probably the most advanced\nfeature shown here. A typemap is a rule that gets applied to specific argument patterns\nin the input. In this recipe, a typemap has been written to match the argument pattern\n(double *a, int n). Inside the typemap is a fragment of C code that tells Swig how\nto convert a Python object into the associated C arguments. The code in this recipe has\nbeen written using Python’s buffer protocol in an attempt to match any input argument\nthat looks like an array of doubles (e.g., NumPy arrays, arrays created by the array\nmodule, etc.). See Recipe 15.3.\nWithin the typemap code, substitutions such as $1 and $2 refer to variables that hold\nthe converted values of the C arguments in the typemap pattern (e.g., $1 maps to double\n*a and $2 maps to int n). $input refers to a PyObject * argument that was supplied\nas an input argument. $argnum is the argument number.\nWriting and understanding typemaps is often the bane of programmers using Swig. Not\nonly is the code rather cryptic, but you need to understand the intricate details of both\nthe Python C API and the way in which Swig interacts with it. The Swig documentation\nhas many more examples and detailed information.\nNevertheless, if you have a lot of a C code to expose as an extension module, Swig can\nbe a very powerful tool for doing it. The key thing to keep in mind is that Swig is basically\na compiler that processes C declarations, but with a powerful pattern matching and\ncustomization component that lets you change the way in which specific declarations\nand types get processed. More information can be found at Swig’s website, including\nPython-specific documentation. \n15.9. Wrapping C Code with Swig \n| \n631",
      "content_length": 2299,
      "extraction_method": "Direct"
    },
    {
      "page_number": 650,
      "chapter": 15,
      "content": "15.10. Wrapping Existing C Code with Cython\nProblem\nYou want to use Cython to make a Python extension module that wraps around an\nexisting C library.\nSolution\nMaking an extension module with Cython looks somewhat similar to writing a hand‐\nwritten extension, in that you will be creating a collection of wrapper functions. How‐\never, unlike previous recipes, you won’t be doing this in C—the code will look a lot more\nlike Python.\nAs preliminaries, assume that the sample code shown in the introduction to this chapter\nhas been compiled into a C library called libsample. Start by creating a file named\ncsample.pxd that looks like this:\n# csample.pxd\n#\n# Declarations of \"external\" C functions and structures\ncdef extern from \"sample.h\":\n    int gcd(int, int)\n    bint in_mandel(double, double, int)\n    int divide(int, int, int *)\n    double avg(double *, int) nogil\n    ctypedef struct Point:\n         double x\n         double y\n    double distance(Point *, Point *)\nThis file serves the same purpose in Cython as a C header file. The initial declaration\ncdef extern from \"sample.h\" declares the required C header file. Declarations\nthat follow are taken from that header. The name of this file is csample.pxd, not sam‐\nple.pxd—this is important.\nNext, create a file named sample.pyx. This file will define wrappers that bridge the\nPython interpreter to the underlying C code declared in the csample.pxd file:\n# sample.pyx\n# Import the low-level C declarations\ncimport csample\n# Import some functionality from Python and the C stdlib\nfrom cpython.pycapsule cimport *\n632 \n| \nChapter 15: C Extensions",
      "content_length": 1601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 651,
      "chapter": 15,
      "content": "from libc.stdlib cimport malloc, free\n# Wrappers\ndef gcd(unsigned int x, unsigned int y):\n    return csample.gcd(x, y)\ndef in_mandel(x, y, unsigned int n):\n    return csample.in_mandel(x, y, n)\ndef divide(x, y):\n    cdef int rem\n    quot = csample.divide(x, y, &rem)\n    return quot, rem\ndef avg(double[:] a):\n    cdef:\n        int sz\n        double result\n    sz = a.size\n    with nogil:\n        result = csample.avg(<double *> &a[0], sz)\n    return result\n# Destructor for cleaning up Point objects\ncdef del_Point(object obj):\n    pt = <csample.Point *> PyCapsule_GetPointer(obj,\"Point\")\n    free(<void *> pt)\n# Create a Point object and return as a capsule\ndef Point(double x,double y):\n    cdef csample.Point *p\n    p = <csample.Point *> malloc(sizeof(csample.Point))\n    if p == NULL:\n        raise MemoryError(\"No memory to make a Point\")\n    p.x = x\n    p.y = y\n    return PyCapsule_New(<void *>p,\"Point\",<PyCapsule_Destructor>del_Point)\ndef distance(p1, p2):\n    pt1 = <csample.Point *> PyCapsule_GetPointer(p1,\"Point\")\n    pt2 = <csample.Point *> PyCapsule_GetPointer(p2,\"Point\")\n    return csample.distance(pt1,pt2)\nVarious details of this file will be covered further in the discussion section. Finally, to\nbuild the extension module, create a setup.py file that looks like this:\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\next_modules = [\n    Extension('sample',\n15.10. Wrapping Existing C Code with Cython \n| \n633",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 652,
      "chapter": 15,
      "content": "['sample.pyx'],\n              libraries=['sample'],\n              library_dirs=['.'])]\nsetup(\n  name = 'Sample extension module',\n  cmdclass = {'build_ext': build_ext},\n  ext_modules = ext_modules\n)\nTo build the resulting module for experimentation, type this:\nbash % python3 setup.py build_ext --inplace\nrunning build_ext\ncythoning sample.pyx to sample.c\nbuilding 'sample' extension\ngcc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes\n -I/usr/local/include/python3.3m -c sample.c\n -o build/temp.macosx-10.6-x86_64-3.3/sample.o\ngcc -bundle -undefined dynamic_lookup build/temp.macosx-10.6-x86_64-3.3/sample.o\n  -L. -lsample -o sample.so\nbash %\nIf it works, you should have an extension module sample.so that can be used as shown\nin the following example:\n>>> import sample\n>>> sample.gcd(42,10)\n2\n>>> sample.in_mandel(1,1,400)\nFalse\n>>> sample.in_mandel(0,0,400)\nTrue\n>>> sample.divide(42,10)\n(4, 2)\n>>> import array\n>>> a = array.array('d',[1,2,3])\n>>> sample.avg(a)\n2.0\n>>> p1 = sample.Point(2,3)\n>>> p2 = sample.Point(4,5)\n>>> p1\n<capsule object \"Point\" at 0x1005d1e70>\n>>> p2\n<capsule object \"Point\" at 0x1005d1ea0>\n>>> sample.distance(p1,p2)\n2.8284271247461903\n>>>\n634 \n| \nChapter 15: C Extensions",
      "content_length": 1225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 653,
      "chapter": 15,
      "content": "Discussion\nThis recipe incorporates a number of advanced features discussed in prior recipes, in‐\ncluding manipulation of arrays, wrapping opaque pointers, and releasing the GIL. Each\nof these parts will be discussed in turn, but it may help to review earlier recipes first.\nAt a high level, using Cython is modeled after C. The .pxd files merely contain C defi‐\nnitions (similar to .h files) and the .pyx files contain implementation (similar to a .c file).\nThe cimport statement is used by Cython to import definitions from a .pxd file. This is\ndifferent than using a normal Python import statement, which would load a regular\nPython module.\nAlthough .pxd files contain definitions, they are not used for the purpose of automati‐\ncally creating extension code. Thus, you still have to write simple wrapper functions.\nFor example, even though the csample.pxd file declares int gcd(int, int) as a func‐\ntion, you still have to write a small wrapper for it in sample.pyx. For instance:\ncimport csample\ndef gcd(unsigned int x, unsigned int y):\n    return csample.gcd(x,y)\nFor simple functions, you don’t have to do too much. Cython will generate wrapper code\nthat properly converts the arguments and return value. The C data types attached to the\narguments are optional. However, if you include them, you get additional error checking\nfor free. For example, if someone calls this function with negative values, an exception\nis generated:\n>>> sample.gcd(-10,2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"sample.pyx\", line 7, in sample.gcd (sample.c:1284)\n    def gcd(unsigned int x,unsigned int y):\nOverflowError: can't convert negative value to unsigned int\n>>>\nIf you want to add additional checking to the wrapper, just use additional wrapper code.\nFor example:\ndef gcd(unsigned int x, unsigned int y):\n    if x <= 0:\n        raise ValueError(\"x must be > 0\")\n    if y <= 0:\n        raise ValueError(\"y must be > 0\")\n    return csample.gcd(x,y)\nThe declaration of in_mandel() in the csample.pxd file has an interesting, but subtle\ndefinition. In that file, the function is declared as returning a bint instead of an int.\nThis causes the function to create a proper Boolean value from the result instead of a\nsimple integer. So, a return value of 0 gets mapped to False and 1 to True.\n15.10. Wrapping Existing C Code with Cython \n| \n635",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 654,
      "chapter": 15,
      "content": "Within the Cython wrappers, you have the option of declaring C data types in addition\nto using all of the usual Python objects. The wrapper for divide() shows an example\nof this as well as how to handle a pointer argument.\ndef divide(x,y):\n    cdef int rem\n    quot = csample.divide(x,y,&rem)\n    return quot, rem\nHere, the rem variable is explicitly declared as a C int variable. When passed to the\nunderlying divide() function, &rem makes a pointer to it just as in C.\nThe code for the avg() function illustrates some more advanced features of Cython.\nFirst the declaration def avg(double[:] a) declares avg() as taking a one-dimensional\nmemoryview of double values. The amazing part about this is that the resulting function\nwill accept any compatible array object, including those created by libraries such as\nnumpy. For example:\n>>> import array\n>>> a = array.array('d',[1,2,3])\n>>> import numpy\n>>> b = numpy.array([1., 2., 3.])\n>>> import sample\n>>> sample.avg(a)\n2.0\n>>> sample.avg(b)\n2.0\n>>>\nIn the wrapper, a.size and &a[0] refer to the number of array items and underlying\npointer, respectively. The syntax <double *> &a[0] is how you type cast pointers to a\ndifferent type if necessary. This is needed to make sure the C avg() receives a pointer\nof the correct type. Refer to the next recipe for some more advanced usage of Cython\nmemoryviews.\nIn addition to working with general arrays, the avg() example also shows how to work\nwith the global interpreter lock. The statement with nogil: declares a block of code as\nexecuting without the GIL. Inside this block, it is illegal to work with any kind of normal\nPython object—only objects and functions declared as cdef can be used. In addition to\nthat, external functions must explicitly declare that they can execute without the GIL.\nThus, in the csample.pxd file, the avg() is declared as double avg(double *, int)\nnogil.\nThe handling of the Point structure presents a special challenge. As shown, this recipe\ntreats Point objects as opaque pointers using capsule objects, as described in\nRecipe 15.4. However, to do this, the underlying Cython code is a bit more complicated.\nFirst, the following imports are being used to bring in definitions of functions from the\nC library and Python C API:\n636 \n| \nChapter 15: C Extensions",
      "content_length": 2289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 655,
      "chapter": 15,
      "content": "from cpython.pycapsule cimport *\nfrom libc.stdlib cimport malloc, free\nThe function del_Point() and Point() use this functionality to create a capsule object\nthat wraps around a Point * pointer. The declaration cdef del_Point() declares\ndel_Point() as a function that is only accessible from Cython and not Python. Thus,\nthis function will not be visible to the outside—instead, it’s used as a callback function\nto clean up memory allocated by the capsule. Calls to functions such as PyCap\nsule_New(), PyCapsule_GetPointer() are directly from the Python C API and are used\nin the same way.\nThe distance() function has been written to extract pointers from the capsule objects\ncreated by Point(). One notable thing here is that you simply don’t have to worry about\nexception handling. If a bad object is passed, PyCapsule_GetPointer() raises an ex‐\nception, but Cython already knows to look for it and propagate it out of the dis\ntance() function if it occurs.\nA downside to the handling of Point structures is that they will be completely opaque\nin this implementation. You won’t be able to peek inside or access any of their attributes.\nThere is an alternative approach to wrapping, which is to define an extension type, as\nshown in this code:\n# sample.pyx\ncimport csample\nfrom libc.stdlib cimport malloc, free\n...\ncdef class Point:\n    cdef csample.Point *_c_point\n    def __cinit__(self, double x, double y):\n        self._c_point = <csample.Point *> malloc(sizeof(csample.Point))\n        self._c_point.x = x\n        self._c_point.y = y\n    def __dealloc__(self):\n        free(self._c_point)\n    property x:\n        def __get__(self):\n            return self._c_point.x\n        def __set__(self, value):\n            self._c_point.x = value\n    property y:\n        def __get__(self):\n            return self._c_point.y\n        def __set__(self, value):\n            self._c_point.y = value\n15.10. Wrapping Existing C Code with Cython \n| \n637",
      "content_length": 1942,
      "extraction_method": "Direct"
    },
    {
      "page_number": 656,
      "chapter": 15,
      "content": "def distance(Point p1, Point p2):\n    return csample.distance(p1._c_point, p2._c_point)\nHere, the cdef class Point is declaring Point as an extension type. The class variable\ncdef csample.Point *_c_point is declaring an instance variable that holds a pointer\nto an underlying Point structure in C. The __cinit__() and __dealloc__() methods\ncreate and destroy the underlying C structure using malloc() and free() calls. The\nproperty x and property y declarations give code that gets and sets the underlying\nstructure attributes. The wrapper for distance() has also been suitably modified to\naccept instances of the Point extension type as arguments, but pass the underlying\npointer to the C function.\nMaking this change, you will find that the code for manipulating Point objects is more\nnatural:\n>>> import sample\n>>> p1 = sample.Point(2,3)\n>>> p2 = sample.Point(4,5)\n>>> p1\n<sample.Point object at 0x100447288>\n>>> p2\n<sample.Point object at 0x1004472a0>\n>>> p1.x\n2.0\n>>> p1.y\n3.0\n>>> sample.distance(p1,p2)\n2.8284271247461903\n>>>\nThis recipe has illustrated many of Cython’s core features that you might be able to\nextrapolate to more complicated kinds of wrapping. However, you will definitely want\nto read more of the official documentation to do more.\nThe next few recipes also illustrate a few additional Cython features.\n15.11. Using Cython to Write High-Performance Array\nOperations\nProblem\nYou would like to write some high-performance array processing functions to operate\non arrays from libraries such as NumPy. You’ve heard that tools such as Cython can\nmake this easier, but aren’t sure how to do it.\n638 \n| \nChapter 15: C Extensions",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 657,
      "chapter": 15,
      "content": "Solution\nAs an example, consider the following code which shows a Cython function for clipping\nthe values in a simple one-dimensional array of doubles:\n# sample.pyx (Cython)\ncimport cython\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncpdef clip(double[:] a, double min, double max, double[:] out):\n    '''\n    Clip the values in a to be between min and max. Result in out\n    '''\n    if min > max:\n        raise ValueError(\"min must be <= max\")\n    if a.shape[0] != out.shape[0]:\n        raise ValueError(\"input and output arrays must be the same size\")\n    for i in range(a.shape[0]):\n        if a[i] < min:\n            out[i] = min\n        elif a[i] > max:\n            out[i] = max\n        else:\n            out[i] = a[i]\nTo compile and build the extension, you’ll need a setup.py file such as the following (use\npython3 setup.py build_ext --inplace to build it):\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\next_modules = [\n    Extension('sample',\n              ['sample.pyx'])\n]\nsetup(\n  name = 'Sample app',\n  cmdclass = {'build_ext': build_ext},\n  ext_modules = ext_modules\n)\nYou will find that the resulting function clips arrays, and that it works with many dif‐\nferent kinds of array objects. For example:\n>>> # array module example\n>>> import sample\n>>> import array\n>>> a = array.array('d',[1,-3,4,7,2,0])\n>>> a\n15.11. Using Cython to Write High-Performance Array Operations \n| \n639",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 658,
      "chapter": 15,
      "content": "array('d', [1.0, -3.0, 4.0, 7.0, 2.0, 0.0])\n>>> sample.clip(a,1,4,a)\n>>> a\narray('d', [1.0, 1.0, 4.0, 4.0, 2.0, 1.0])\n>>> # numpy example\n>>> import numpy\n>>> b = numpy.random.uniform(-10,10,size=1000000)\n>>> b\narray([-9.55546017,  7.45599334,  0.69248932, ...,  0.69583148,\n       -3.86290931,  2.37266888])\n>>> c = numpy.zeros_like(b)\n>>> c\narray([ 0.,  0.,  0., ...,  0.,  0.,  0.])\n>>> sample.clip(b,-5,5,c)\n>>> c\narray([-5.        ,  5.        ,  0.69248932, ...,  0.69583148,\n       -3.86290931,  2.37266888])\n>>> min(c)\n-5.0\n>>> max(c)\n5.0\n>>>\nYou will also find that the resulting code is fast. The following session puts our imple‐\nmentation in a head-to-head battle with the clip() function already present in numpy:\n>>> timeit('numpy.clip(b,-5,5,c)','from __main__ import b,c,numpy',number=1000)\n8.093049556000551\n>>> timeit('sample.clip(b,-5,5,c)','from __main__ import b,c,sample',\n...         number=1000)\n3.760528204000366\n>>>\nAs you can see, it’s quite a bit faster—an interesting result considering the core of the\nNumPy version is written in C.\nDiscussion\nThis recipe utilizes Cython typed memoryviews, which greatly simplify code that op‐\nerates on arrays. The declaration cpdef clip() declares clip() as both a C-level and\nPython-level function. In Cython, this is useful, because it means that the function call\nis more efficently called by other Cython functions (e.g., if you want to invoke clip()\nfrom a different Cython function).\nThe typed parameters double[:] a and double[:] out declare those parameters as\none-dimensional arrays of doubles. As input, they will access any array object that\nproperly implements the memoryview interface, as described in PEP 3118. This includes\narrays from NumPy and from the built-in array library.\n640 \n| \nChapter 15: C Extensions",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 659,
      "chapter": 15,
      "content": "When writing code that produces a result that is also an array, you should follow the\nconvention shown of having an output parameter as shown. This places the responsi‐\nbility of creating the output array on the caller and frees the code from having to know\ntoo much about the specific details of what kinds of arrays are being manipulated (it\njust assumes the arrays are already in-place and only needs to perform a few basic sanity\nchecks such as making sure their sizes are compatible). In libraries such as NumPy, it\nis relatively easy to create output arrays using functions such as numpy.zeros() or\nnumpy.zeros_like(). Alternatively, to create uninitialized arrays, you can use num\npy.empty() or numpy.empty_like(). This will be slightly faster if you’re about to over‐\nwrite the array contents with a result.\nIn the implementation of your function, you simply write straightforward looking array\nprocessing code using indexing and array lookups (e.g., a[i], out[i], and so forth).\nCython will take steps to make sure these produce efficient code.\nThe two decorators that precede the definition of clip() are a few optional performance\noptimizations. @cython.boundscheck(False) eliminates all array bounds checking and\ncan be used if you know the indexing won’t go out of range. @cython.wrap\naround(False) eliminates the handling of negative array indices as wrapping around\nto the end of the array (like with Python lists). The inclusion of these decorators can\nmake the code run substantially faster (almost 2.5 times faster on this example when\ntested).\nWhenever working with arrays, careful study and experimentation with the underlying\nalgorithm can also yield large speedups. For example, consider this variant of the clip()\nfunction that uses conditional expressions:\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncpdef clip(double[:] a, double min, double max, double[:] out):\n    if min > max:\n        raise ValueError(\"min must be <= max\")\n    if a.shape[0] != out.shape[0]:\n        raise ValueError(\"input and output arrays must be the same size\")\n    for i in range(a.shape[0]):\n        out[i] = (a[i] if a[i] < max else max) if a[i] > min else min\nWhen tested, this version of the code runs over 50% faster (2.44s versus 3.76s on the\ntimeit() test shown earlier).\nAt this point, you might be wondering how this code would stack up against a hand‐\nwritten C version. For example, perhaps you write the following C function and craft a\nhandwritten extension to using techniques shown in earlier recipes:\nvoid clip(double *a, int n, double min, double max, double *out) {\n  double x;\n  for (; n >= 0; n--, a++, out++) {\n    x = *a;\n15.11. Using Cython to Write High-Performance Array Operations \n| \n641",
      "content_length": 2725,
      "extraction_method": "Direct"
    },
    {
      "page_number": 660,
      "chapter": 15,
      "content": "*out = x > max ? max : (x < min ? min : x);\n  }\n}\nThe extension code for this isn’t shown, but after experimenting, we found that a hand‐\ncrafted C extension ran more than 10% slower than the version created by Cython. The\nbottom line is that the code runs a lot faster than you might think.\nThere are several extensions that can be made to the solution code. For certain kinds of\narray operations, it might make sense to release the GIL so that multiple threads can\nrun in parallel. To do that, modify the code to include the with nogil: statement:\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncpdef clip(double[:] a, double min, double max, double[:] out):\n    if min > max:\n        raise ValueError(\"min must be <= max\")\n    if a.shape[0] != out.shape[0]:\n        raise ValueError(\"input and output arrays must be the same size\")\n    with nogil:\n        for i in range(a.shape[0]):\n            out[i] = (a[i] if a[i] < max else max) if a[i] > min else min\nIf you want to write a version of the code that operates on two-dimensional arrays, here\nis what it might look like:\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncpdef clip2d(double[:,:] a, double min, double max, double[:,:] out):\n    if min > max:\n        raise ValueError(\"min must be <= max\")\n    for n in range(a.ndim):\n        if a.shape[n] != out.shape[n]:\n            raise TypeError(\"a and out have different shapes\")\n    for i in range(a.shape[0]):\n        for j in range(a.shape[1]):\n            if a[i,j] < min:\n                out[i,j] = min\n            elif a[i,j] > max:\n                out[i,j] = max\n            else:\n                out[i,j] = a[i,j]\nHopefully it’s not lost on the reader that all of the code in this recipe is not tied to any\nspecific array library (e.g., NumPy). That gives the code a great deal of flexibility. How‐\never, it’s also worth noting that dealing with arrays can be significantly more complicated\nonce multiple dimensions, strides, offsets, and other factors are introduced. Those top‐\nics are beyond the scope of this recipe, but more information can be found in PEP\n3118. The Cython documentation on “typed memoryviews” is also essential reading.\n642 \n| \nChapter 15: C Extensions",
      "content_length": 2205,
      "extraction_method": "Direct"
    },
    {
      "page_number": 661,
      "chapter": 15,
      "content": "15.12. Turning a Function Pointer into a Callable\nProblem\nYou have (somehow) obtained the memory address of a compiled function, but want\nto turn it into a Python callable that you can use as an extension function.\nSolution\nThe ctypes module can be used to create Python callables that wrap around arbitrary\nmemory addresses. The following example shows how to obtain the raw, low-level ad‐\ndress of a C function and how to turn it back into a callable object:\n>>> import ctypes\n>>> lib = ctypes.cdll.LoadLibrary(None)\n>>> # Get the address of sin() from the C math library\n>>> addr = ctypes.cast(lib.sin, ctypes.c_void_p).value\n>>> addr\n140735505915760\n>>> # Turn the address into a callable function\n>>> functype = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double)\n>>> func = functype(addr)\n>>> func\n<CFunctionType object at 0x1006816d0>\n>>> # Call the resulting function\n>>> func(2)\n0.9092974268256817\n>>> func(0)\n0.0\n>>>\nDiscussion\nTo make a callable, you must first create a CFUNCTYPE instance. The first argument to\nCFUNCTYPE() is the return type. Subsequent arguments are the types of the arguments.\nOnce you have defined the function type, you wrap it around an integer memory address\nto create a callable object. The resulting object is used like any normal function accessed\nthrough ctypes.\nThis recipe might look rather cryptic and low level. However, it is becoming increasingly\ncommon for programs and libraries to utilize advanced code generation techniques like\njust in-time compilation, as found in libraries such as LLVM.\nFor example, here is a simple example that uses the llvmpy extension to make a small\nassembly function, obtain a function pointer to it, and turn it into a Python callable:\n15.12. Turning a Function Pointer into a Callable \n| \n643",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 662,
      "chapter": 15,
      "content": ">>> from llvm.core import Module, Function, Type, Builder\n>>> mod = Module.new('example')\n>>> f = Function.new(mod,Type.function(Type.double(), \\\n                     [Type.double(), Type.double()], False), 'foo')\n>>> block = f.append_basic_block('entry')\n>>> builder = Builder.new(block)\n>>> x2 = builder.fmul(f.args[0],f.args[0])\n>>> y2 = builder.fmul(f.args[1],f.args[1])\n>>> r = builder.fadd(x2,y2)\n>>> builder.ret(r)\n<llvm.core.Instruction object at 0x10078e990>\n>>> from llvm.ee import ExecutionEngine\n>>> engine = ExecutionEngine.new(mod)\n>>> ptr = engine.get_pointer_to_function(f)\n>>> ptr\n4325863440\n>>> foo = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double, ctypes.c_double)(ptr)\n>>> # Call the resulting function\n>>> foo(2,3)\n13.0\n>>> foo(4,5)\n41.0\n>>> foo(1,2)\n5.0\n>>>\nIt goes without saying that doing anything wrong at this level will probably cause the\nPython interpreter to die a horrible death. Keep in mind that you’re directly working\nwith machine-level memory addresses and native machine code—not Python\nfunctions.\n15.13. Passing NULL-Terminated Strings to C Libraries\nProblem\nYou are writing an extension module that needs to pass a NULL-terminated string to a\nC library. However, you’re not entirely sure how to do it with Python’s Unicode string\nimplementation.\nSolution\nMany C libraries include functions that operate on NULL-terminated strings declared\nas type char *. Consider the following C function that we will use for the purposes of\nillustration and testing:\nvoid print_chars(char *s) {\n    while (*s) {\n        printf(\"%2x \", (unsigned char) *s);\n644 \n| \nChapter 15: C Extensions",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 663,
      "chapter": 15,
      "content": "s++;\n    }\n    printf(\"\\n\");\n}\nThis function simply prints out the hex representation of individual characters so that\nthe passed strings can be easily debugged. For example:\nprint_chars(\"Hello\");   // Outputs: 48 65 6c 6c 6f\nFor calling such a C function from Python, you have a few choices. First, you could\nrestrict it to only operate on bytes using \"y\" conversion code to PyArg_ParseTuple()\nlike this:\nstatic PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  char *s;\n  if (!PyArg_ParseTuple(args, \"y\", &s)) {\n    return NULL;\n  }\n  print_chars(s);\n  Py_RETURN_NONE;\n}\nThe resulting function operates as follows. Carefully observe how bytes with embedded\nNULL bytes and Unicode strings are rejected:\n>>> print_chars(b'Hello World')\n48 65 6c 6c 6f 20 57 6f 72 6c 64\n>>> print_chars(b'Hello\\x00World')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: must be bytes without null bytes, not bytes\n>>> print_chars('Hello World')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'str' does not support the buffer interface\n>>>\nIf you want to pass Unicode strings instead, use the \"s\" format code to PyArg_Parse\nTuple() such as this:\nstatic PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  char *s;\n  if (!PyArg_ParseTuple(args, \"s\", &s)) {\n    return NULL;\n  }\n  print_chars(s);\n  Py_RETURN_NONE;\n}\n15.13. Passing NULL-Terminated Strings to C Libraries \n| \n645",
      "content_length": 1451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 664,
      "chapter": 15,
      "content": "When used, this will automatically convert all strings to a NULL-terminated UTF-8\nencoding. For example:\n>>> print_chars('Hello World')\n48 65 6c 6c 6f 20 57 6f 72 6c 64\n>>> print_chars('Spicy Jalape\\u00f1o')  # Note: UTF-8 encoding\n53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f\n>>> print_chars('Hello\\x00World')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: must be str without null characters, not str\n>>> print_chars(b'Hello World')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: must be str, not bytes\n>>>\nIf for some reason, you are working directly with a PyObject * and can’t use PyArg_Par\nseTuple(), the following code samples show how you can check and extract a suitable\nchar * reference, from both a bytes and string object:\n/* Some Python Object (obtained somehow) */\nPyObject *obj;\n/* Conversion from bytes */\n{\n   char *s;\n   s = PyBytes_AsString(o);\n   if (!s) {\n      return NULL;   /* TypeError already raised */\n   }\n   print_chars(s);\n}\n/* Conversion to UTF-8 bytes from a string */\n{\n   PyObject *bytes;\n   char *s;\n   if (!PyUnicode_Check(obj)) {\n       PyErr_SetString(PyExc_TypeError, \"Expected string\");\n       return NULL;\n   }\n   bytes = PyUnicode_AsUTF8String(obj);\n   s = PyBytes_AsString(bytes);\n   print_chars(s);\n   Py_DECREF(bytes);\n}\nBoth of the preceding conversions guarantee NULL-terminated data, but they do not\ncheck for embedded NULL bytes elsewhere inside the string. Thus, that’s something\nthat you would need to check yourself if it’s important.\n646 \n| \nChapter 15: C Extensions",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 665,
      "chapter": 15,
      "content": "Discussion\nIf it all possible, you should try to avoid writing code that relies on NULL-terminated\nstrings since Python has no such requirement. It is almost always better to handle strings\nusing the combination of a pointer and a size if possible. Nevertheless, sometimes you\nhave to work with legacy C code that presents no other option.\nAlthough it is easy to use, there is a hidden memory overhead associated with using the\n\"s\" format code to PyArg_ParseTuple() that is easy to overlook. When you write code\nthat uses this conversion, a UTF-8 string is created and permanently attached to the\noriginal string object. If the original string contains non-ASCII characters, this makes\nthe size of the string increase until it is garbage collected. For example:\n>>> import sys\n>>> s = 'Spicy Jalape\\u00f1o'\n>>> sys.getsizeof(s)\n87\n>>> print_chars(s)     # Passing string\n53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f\n>>> sys.getsizeof(s)   # Notice increased size\n103\n>>>\nIf this growth in memory use is a concern, you should rewrite your C extension code\nto use the PyUnicode_AsUTF8String() function like this:\nstatic PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  PyObject *o, *bytes;\n  char *s;\n  if (!PyArg_ParseTuple(args, \"U\", &o)) {\n    return NULL;\n  }\n  bytes = PyUnicode_AsUTF8String(o);\n  s = PyBytes_AsString(bytes);\n  print_chars(s);\n  Py_DECREF(bytes);\n  Py_RETURN_NONE;\n}\nWith this modification, a UTF-8 encoded string is created if needed, but then discarded\nafter use. Here is the modified behavior:\n>>> import sys\n>>> s = 'Spicy Jalape\\u00f1o'\n>>> sys.getsizeof(s)\n87\n>>> print_chars(s)\n53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f\n>>> sys.getsizeof(s)\n87\n>>>\n15.13. Passing NULL-Terminated Strings to C Libraries \n| \n647",
      "content_length": 1752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 666,
      "chapter": 15,
      "content": "If you are trying to pass NULL-terminated strings to functions wrapped via ctypes, be\naware that ctypes only allows bytes to be passed and that it does not check for embedded\nNULL bytes. For example:\n>>> import ctypes\n>>> lib = ctypes.cdll.LoadLibrary(\"./libsample.so\")\n>>> print_chars = lib.print_chars\n>>> print_chars.argtypes = (ctypes.c_char_p,)\n>>> print_chars(b'Hello World')\n48 65 6c 6c 6f 20 57 6f 72 6c 64\n>>> print_chars(b'Hello\\x00World')\n48 65 6c 6c 6f\n>>> print_chars('Hello World')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nctypes.ArgumentError: argument 1: <class 'TypeError'>: wrong type\n>>>\nIf you want to pass a string instead of bytes, you need to perform a manual UTF-8\nencoding first. For example:\n>>> print_chars('Hello World'.encode('utf-8'))\n48 65 6c 6c 6f 20 57 6f 72 6c 64\n>>>\nFor other extension tools (e.g., Swig, Cython), careful study is probably in order should\nyou decide to use them to pass strings to C code.\n15.14. Passing Unicode Strings to C Libraries\nProblem\nYou are writing an extension module that needs to pass a Python string to a C library\nfunction that may or may not know how to properly handle Unicode.\nSolution\nThere are many issues to be concerned with here, but the main one is that existing C\nlibraries won’t understand Python’s native representation of Unicode. Therefore, your\nchallenge is to convert the Python string into a form that can be more easily understood\nby C libraries.\nFor the purposes of illustration, here are two C functions that operate on string data\nand output it for the purposes of debugging and experimentation. One uses bytes pro‐\nvided in the form char *, int, whereas the other uses wide characters in the form\nwchar_t *, int:\nvoid print_chars(char *s, int len) {\n  int n = 0;\n648 \n| \nChapter 15: C Extensions",
      "content_length": 1817,
      "extraction_method": "Direct"
    },
    {
      "page_number": 667,
      "chapter": 15,
      "content": "while (n < len) {\n    printf(\"%2x \", (unsigned char) s[n]);\n    n++;\n  }\n  printf(\"\\n\");\n}\nvoid print_wchars(wchar_t *s, int len) {\n  int n = 0;\n  while (n < len) {\n    printf(\"%x \", s[n]);\n    n++;\n  }\n  printf(\"\\n\");\n}\nFor the byte-oriented function print_chars(), you need to convert Python strings into\na suitable byte encoding such as UTF-8. Here is a sample extension function that does\nthis:\nstatic PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  char *s;\n  Py_ssize_t  len;\n  if (!PyArg_ParseTuple(args, \"s#\", &s, &len)) {\n    return NULL;\n  }\n  print_chars(s, len);\n  Py_RETURN_NONE;\n}\nFor library functions that work with the machine native wchar_t type, you can write\nextension code such as this:\nstatic PyObject *py_print_wchars(PyObject *self, PyObject *args) {\n  wchar_t *s;\n  Py_ssize_t  len;\n  if (!PyArg_ParseTuple(args, \"u#\", &s, &len)) {\n    return NULL;\n  }\n  print_wchars(s,len);\n  Py_RETURN_NONE;\n}\nHere is an interactive session that illustrates how these functions work:\n>>> s = 'Spicy Jalape\\u00f1o'\n>>> print_chars(s)\n53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f\n>>> print_wchars(s)\n53 70 69 63 79 20 4a 61 6c 61 70 65 f1 6f\n>>>\n15.14. Passing Unicode Strings to C Libraries \n| \n649",
      "content_length": 1221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 668,
      "chapter": 15,
      "content": "Carefully observe how the byte-oriented function print_chars() is receiving UTF-8\nencoded data, whereas print_wchars() is receiving the Unicode code point values.\nDiscussion\nBefore considering this recipe, you should first study the nature of the C library that\nyou’re accessing. For many C libraries, it might make more sense to pass bytes instead\nof a string. To do that, use this conversion code instead:\nstatic PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  char *s;\n  Py_ssize_t  len;\n  /* accepts bytes, bytearray, or other byte-like object */\n  if (!PyArg_ParseTuple(args, \"y#\", &s, &len)) {\n    return NULL;\n  }\n  print_chars(s, len);\n  Py_RETURN_NONE;\n}\nIf you decide that you still want to pass strings, you need to know that Python 3 uses an\nadaptable string representation that is not entirely straightforward to map directly to C\nlibraries using the standard types char * or wchar_t * See PEP 393 for details. Thus,\nto present string data to C, some kind of conversion is almost always necessary. The s#\nand u# format codes to PyArg_ParseTuple() safely perform such conversions.\nOne potential downside is that such conversions cause the size of the original string\nobject to permanently increase. Whenever a conversion is made, a copy of the converted\ndata is kept and attached to the original string object so that it can be reused later. You\ncan observe this effect:\n>>> import sys\n>>> s = 'Spicy Jalape\\u00f1o'\n>>> sys.getsizeof(s)\n87\n>>> print_chars(s)\n53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f\n>>> sys.getsizeof(s)\n103\n>>> print_wchars(s)\n53 70 69 63 79 20 4a 61 6c 61 70 65 f1 6f\n>>> sys.getsizeof(s)\n163\n>>>\nFor small amounts of string data, this might not matter, but if you’re doing large amounts\nof text processing in extensions, you may want to avoid the overhead. Here is an\nalternative implementation of the first extension function that avoids these memory\ninefficiencies:\n650 \n| \nChapter 15: C Extensions",
      "content_length": 1949,
      "extraction_method": "Direct"
    },
    {
      "page_number": 669,
      "chapter": 15,
      "content": "static PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  PyObject *obj, *bytes;\n  char *s;\n  Py_ssize_t   len;\n  if (!PyArg_ParseTuple(args, \"U\", &obj)) {\n    return NULL;\n  }\n  bytes = PyUnicode_AsUTF8String(obj);\n  PyBytes_AsStringAndSize(bytes, &s, &len);\n  print_chars(s, len);\n  Py_DECREF(bytes);\n  Py_RETURN_NONE;\n}\nAvoiding memory overhead for wchar_t handling is much more tricky. Internally,\nPython stores strings using the most efficient representation possible. For example,\nstrings containing nothing but ASCII are stored as arrays of bytes, whereas strings con‐\ntaining characters in the range U+0000 to U+FFFF use a two-byte representation. Since\nthere isn’t a single representation of the data, you can’t just cast the internal array to\nwchar_t * and hope that it works. Instead, a wchar_t array has to be created and text\ncopied into it. The \"u#\" format code to PyArg_ParseTuple() does this for you at the\ncost of efficiency (it attaches the resulting copy to the string object).\nIf you want to avoid this long-term memory overhead, your only real choice is to copy\nthe Unicode data into a temporary array, pass it to the C library function, and then\ndeallocate the array. Here is one possible implementation:\nstatic PyObject *py_print_wchars(PyObject *self, PyObject *args) {\n  PyObject *obj;\n  wchar_t *s;\n  Py_ssize_t len;\n  if (!PyArg_ParseTuple(args, \"U\", &obj)) {\n    return NULL;\n  }\n  if ((s = PyUnicode_AsWideCharString(obj, &len)) == NULL) {\n    return NULL;\n  }\n  print_wchars(s, len);\n  PyMem_Free(s);\n  Py_RETURN_NONE;\n}\nIn this implementation, PyUnicode_AsWideCharString() creates a temporary buffer of\nwchar_t characters and copies data into it. That buffer is passed to C and then released\nafterward. As of this writing, there seems to be a possible bug related to this behavior,\nas described at the Python issues page.\n15.14. Passing Unicode Strings to C Libraries \n| \n651",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 670,
      "chapter": 15,
      "content": "If, for some reason you know that the C library takes the data in a different byte encoding\nthan UTF-8, you can force Python to perform an appropriate conversion using exten‐\nsion code such as the following:\nstatic PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  char *s = 0;\n  int   len;\n  if (!PyArg_ParseTuple(args, \"es#\", \"encoding-name\", &s, &len)) {\n    return NULL;\n  }\n  print_chars(s, len);\n  PyMem_Free(s);\n  Py_RETURN_NONE;\n}\nLast, but not least, if you want to work directly with the characters in a Unicode string,\nhere is an example that illustrates low-level access:\nstatic PyObject *py_print_wchars(PyObject *self, PyObject *args) {\n  PyObject *obj;\n  int n, len;\n  int kind;\n  void *data;\n  if (!PyArg_ParseTuple(args, \"U\", &obj)) {\n    return NULL;\n  }\n  if (PyUnicode_READY(obj) < 0) {\n    return NULL;\n  }\n  len = PyUnicode_GET_LENGTH(obj);\n  kind = PyUnicode_KIND(obj);\n  data = PyUnicode_DATA(obj);\n  for (n = 0; n < len; n++) {\n    Py_UCS4 ch = PyUnicode_READ(kind, data, n);\n    printf(\"%x \", ch);\n  }\n  printf(\"\\n\");\n  Py_RETURN_NONE;\n}\nIn this code, the PyUnicode_KIND() and PyUnicode_DATA() macros are related to the\nvariable-width storage of Unicode, as described in PEP 393. The kind variable encodes\ninformation about the underlying storage (8-bit, 16-bit, or 32-bit) and data points the\nbuffer. In reality, you don’t need to do anything with these values as long as you pass\nthem to the PyUnicode_READ() macro when extracting characters.\nA few final words: when passing Unicode strings from Python to C, you should probably\ntry to make it as simple as possible. If given the choice between an encoding such as\n652 \n| \nChapter 15: C Extensions",
      "content_length": 1686,
      "extraction_method": "Direct"
    },
    {
      "page_number": 671,
      "chapter": 15,
      "content": "UTF-8 or wide characters, choose UTF-8. Support for UTF-8 seems to be much more\ncommon, less trouble-prone, and better supported by the interpreter. Finally, make sure\nyour review the documentation on Unicode handling. \n15.15. Converting C Strings to Python\nProblem\nYou want to convert strings from C to Python bytes or a string object.\nSolution\nFor C strings represented as a pair char *, int, you must decide whether or not you\nwant the string presented as a raw byte string or as a Unicode string. Byte objects can\nbe built using Py_BuildValue() as follows:\nchar *s;     /* Pointer to C string data */\nint   len;   /* Length of data */\n/* Make a bytes object */\nPyObject *obj = Py_BuildValue(\"y#\", s, len);\nIf you want to create a Unicode string and you know that s points to data encoded as\nUTF-8, you can use the following:\nPyObject *obj = Py_BuildValue(\"s#\", s, len);\nIf s is encoded in some other known encoding, you can make a string using PyUni\ncode_Decode() as follows:\nPyObject *obj = PyUnicode_Decode(s, len, \"encoding\", \"errors\");\n/* Examples /*\nobj = PyUnicode_Decode(s, len, \"latin-1\", \"strict\");\nobj = PyUnicode_Decode(s, len, \"ascii\", \"ignore\");\nIf you happen to have a wide string represented as a wchar_t *, len pair, there are a\nfew options. First, you could use Py_BuildValue() as follows:\nwchar_t *w;    /* Wide character string */\nint len;       /* Length */\nPyObject *obj = Py_BuildValue(\"u#\", w, len);\nAlternatively, you can use PyUnicode_FromWideChar():\nPyObject *obj = PyUnicode_FromWideChar(w, len);\nFor wide character strings, no interpretation is made of the character data—it is assumed\nto be raw Unicode code points which are directly converted to Python.\n15.15. Converting C Strings to Python \n| \n653",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 672,
      "chapter": 15,
      "content": "Discussion\nConversion of strings from C to Python follow the same principles as I/O. Namely, the\ndata from C must be explicitly decoded into a string according to some codec. Common\nencodings include ASCII, Latin-1, and UTF-8. If you’re not entirely sure of the encoding\nor the data is binary, you’re probably best off encoding the string as bytes instead.\nWhen making an object, Python always copies the string data you provide. If necessary,\nit’s up to you to release the C string afterward (if required). Also, for better reliability,\nyou should try to create strings using both a pointer and a size rather than relying on\nNULL-terminated data.\n15.16. Working with C Strings of Dubious Encoding\nProblem\nYou are converting strings back and forth between C and Python, but the C encoding\nis of a dubious or unknown nature. For example, perhaps the C data is supposed to be\nUTF-8, but it’s not being strictly enforced. You would like to write code that can handle\nmalformed data in a graceful way that doesn’t crash Python or destroy the string data\nin the process.\nSolution\nHere is some C data and a function that illustrates the nature of this problem:\n/* Some dubious string data (malformed UTF-8) */\nconst char *sdata = \"Spicy Jalape\\xc3\\xb1o\\xae\";\nint slen = 16;\n/* Output character data */\nvoid print_chars(char *s, int len) {\n  int n = 0;\n  while (n < len) {\n    printf(\"%2x \", (unsigned char) s[n]);\n    n++;\n  }\n  printf(\"\\n\");\n}\nIn this code, the string sdata contains a mix of UTF-8 and malformed data. Neverthe‐\nless, if a user calls print_chars(sdata, slen) in C, it works fine.\nNow suppose you want to convert the contents of sdata into a Python string. Further\nsuppose you want to later pass that string to the print_chars() function through an\nextension. Here’s how to do it in a way that exactly preserves the original data even\nthough there are encoding problems:\n654 \n| \nChapter 15: C Extensions",
      "content_length": 1914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 673,
      "chapter": 15,
      "content": "/* Return the C string back to Python */\nstatic PyObject *py_retstr(PyObject *self, PyObject *args) {\n  if (!PyArg_ParseTuple(args, \"\")) {\n    return NULL;\n  }\n  return PyUnicode_Decode(sdata, slen, \"utf-8\", \"surrogateescape\");\n}\n/* Wrapper for the print_chars() function */\nstatic PyObject *py_print_chars(PyObject *self, PyObject *args) {\n  PyObject *obj, *bytes;\n  char *s = 0;\n  Py_ssize_t   len;\n  if (!PyArg_ParseTuple(args, \"U\", &obj)) {\n    return NULL;\n  }\n  if ((bytes = PyUnicode_AsEncodedString(obj,\"utf-8\",\"surrogateescape\"))\n        == NULL) {\n    return NULL;\n  }\n  PyBytes_AsStringAndSize(bytes, &s, &len);\n  print_chars(s, len);\n  Py_DECREF(bytes);\n  Py_RETURN_NONE;\n}\nIf you try these functions from Python, here’s what happens:\n>>> s = retstr()\n>>> s\n'Spicy Jalapeño\\udcae'\n>>> print_chars(s)\n53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f ae\n>>>\nCareful observation will reveal that the malformed string got encoded into a Python\nstring without errors, and that when passed back into C, it turned back into a byte string\nthat exactly encoded the same bytes as the original C string.\nDiscussion\nThis recipe addresses a subtle, but potentially annoying problem with string handling\nin extension modules. Namely, the fact that C strings in extensions might not follow the\nstrict Unicode encoding/decoding rules that Python normally expects. Thus, it’s possible\nthat some malformed C data would pass to Python. A good example might be C strings\nassociated with low-level system calls such as filenames. For instance, what happens if\na system call returns a broken string back to the interpreter that can’t be properly\ndecoded.\n15.16. Working with C Strings of Dubious Encoding \n| \n655",
      "content_length": 1698,
      "extraction_method": "Direct"
    },
    {
      "page_number": 674,
      "chapter": 15,
      "content": "Normally, Unicode errors are often handled by specifying some sort of error policy, such\nas strict, ignore, replace, or something similar. However, a downside of these policies\nis that they irreparably destroy the original string content. For example, if the malformed\ndata in the example was decoded using one of these polices, you would get results such\nas this:\n>>> raw = b'Spicy Jalape\\xc3\\xb1o\\xae'\n>>> raw.decode('utf-8','ignore')\n'Spicy Jalapeño'\n>>> raw.decode('utf-8','replace')\n'Spicy Jalapeño?'\n>>>\nThe surrogateescape error handling policies takes all nondecodable bytes and turns\nthem into the low-half of a surrogate pair (\\udcXX where XX is the raw byte value). For\nexample:\n>>> raw.decode('utf-8','surrogateescape')\n'Spicy Jalapeño\\udcae'\n>>>\nIsolated low surrogate characters such as \\udcae never appear in valid Unicode. Thus,\nthis string is technically an illegal representation. In fact, if you ever try to pass it to\nfunctions that perform output, you’ll get encoding errors:\n>>> s = raw.decode('utf-8', 'surrogateescape')\n>>> print(s)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nUnicodeEncodeError: 'utf-8' codec can't encode character '\\udcae'\nin position 14: surrogates not allowed\n>>>\nHowever, the main point of allowing the surrogate escapes is to allow malformed strings\nto pass from C to Python and back into C without any data loss. When the string is\nencoded using surrogateescape again, the surrogate characters are turned back into\ntheir original bytes. For example:\n>>> s\n'Spicy Jalapeño\\udcae'\n>>> s.encode('utf-8','surrogateescape')\nb'Spicy Jalape\\xc3\\xb1o\\xae'\n>>>\nAs a general rule, it’s probably best to avoid surrogate encoding whenever possible—\nyour code will be much more reliable if it uses proper encodings. However, sometimes\nthere are situations where you simply don’t have control over the data encoding and\nyou aren’t free to ignore or replace the bad data because other functions may need to\nuse it. This recipe shows how to do it.\n656 \n| \nChapter 15: C Extensions",
      "content_length": 2042,
      "extraction_method": "Direct"
    },
    {
      "page_number": 675,
      "chapter": 15,
      "content": "As a final note, many of Python’s system-oriented functions, especially those related to\nfilenames, environment variables, and command-line options, use surrogate encoding.\nFor example, if you use a function such as os.listdir() on a directory containing a\nundecodable filename, it will be returned as a string with surrogate escapes. See\nRecipe 5.15 for a related recipe.\nPEP 383 has more information about the problem addressed by this recipe and surro\ngateescape error handling.\n15.17. Passing Filenames to C Extensions\nProblem\nYou need to pass filenames to C library functions, but need to make sure the filename\nhas been encoded according to the system’s expected filename encoding.\nSolution\nTo write an extension function that receives a filename, use code such as this:\nstatic PyObject *py_get_filename(PyObject *self, PyObject *args) {\n  PyObject *bytes;\n  char *filename;\n  Py_ssize_t len;\n  if (!PyArg_ParseTuple(args,\"O&\", PyUnicode_FSConverter, &bytes)) {\n    return NULL;\n  }\n  PyBytes_AsStringAndSize(bytes, &filename, &len);\n  /* Use filename */\n  ...\n  /* Cleanup and return */\n  Py_DECREF(bytes)\n  Py_RETURN_NONE;\n}\nIf you already have a PyObject * that you want to convert as a filename, use code such\nas the following:\nPyObject *obj;    /* Object with the filename */\nPyObject *bytes;\nchar *filename;\nPy_ssize_t len;\nbytes = PyUnicode_EncodeFSDefault(obj);\nPyBytes_AsStringAndSize(bytes, &filename, &len);\n/* Use filename */\n...\n15.17. Passing Filenames to C Extensions \n| \n657",
      "content_length": 1496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 676,
      "chapter": 15,
      "content": "/* Cleanup */\nPy_DECREF(bytes);\nIf you need to return a filename back to Python, use the following code:\n/* Turn a filename into a Python object */\nchar *filename;       /* Already set */\nint   filename_len;   /* Already set */\nPyObject *obj = PyUnicode_DecodeFSDefaultAndSize(filename, filename_len);\nDiscussion\nDealing with filenames in a portable way is a tricky problem that is best left to Python.\nIf you use this recipe in your extension code, filenames will be handled in a manner that\nis consistent with filename handling in the rest of Python. This includes encoding/\ndecoding of bytes, dealing with bad characters, surrogate escapes, and other complica‐\ntions.\n15.18. Passing Open Files to C Extensions\nProblem\nYou have an open file object in Python, but need to pass it to C extension code that will\nuse the file.\nSolution\nTo convert a file to an integer file descriptor, use PyFile_FromFd(), as shown:\nPyObject *fobj;     /* File object (already obtained somehow) */\nint fd = PyObject_AsFileDescriptor(fobj);\nif (fd < 0) {\n   return NULL;\n}\nThe resulting file descriptor is obtained by calling the fileno() method on fobj. Thus,\nany object that exposes a descriptor in this manner should work (e.g., file, socket, etc.).\nOnce you have the descriptor, it can be passed to various low-level C functions that\nexpect to work with files.\nIf you need to convert an integer file descriptor back into a Python object, use\nPyFile_FromFd() as follows:\nint fd;     /* Existing file descriptor (already open) */\nPyObject *fobj = PyFile_FromFd(fd, \"filename\",\"r\",-1,NULL,NULL,NULL,1);\n658 \n| \nChapter 15: C Extensions",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 677,
      "chapter": 15,
      "content": "The arguments to PyFile_FromFd() mirror those of the built-in open() function. NULL\nvalues simply indicate that the default settings for the encoding, errors, and newline\narguments are being used.\nDiscussion\nIf you are passing file objects from Python to C, there are a few tricky issues to be\nconcerned about. First, Python performs its own I/O buffering through the io module.\nPrior to passing any kind of file descriptor to C, you should first flush the I/O buffers\non the associated file objects. Otherwise, you could get data appearing out of order on\nthe file stream.\nSecond, you need to pay careful attention to file ownership and the responsibility of\nclosing the file in particular. If a file descriptor is passed to C, but still used in Python,\nyou need to make sure C doesn’t accidentally close the file. Likewise, if a file descriptor\nis being turned into a Python file object, you need to be clear about who is responsible\nfor closing it. The last argument to PyFile_FromFd() is set to 1 to indicate that Python\nshould close the file.\nIf you need to make a different kind of file object such as a FILE * object from the C\nstandard I/O library using a function such as fdopen(), you’ll need to be especially\ncareful. Doing so would introduce two completely different I/O buffering layers into\nthe I/O stack (one from Python’s io module and one from C stdio). Operations such\nas fclose() in C could also inadvertently close the file for further use in Python. If given\na choice, you should probably make extension code work with the low-level integer file\ndescriptors as opposed to using a higher-level abstraction such as that provided by\n<stdio.h>.\n15.19. Reading File-Like Objects from C\nProblem\nYou want to write C extension code that consumes data from any Python file-like object\n(e.g., normal files, StringIO objects, etc.).\nSolution\nTo consume data on a file-like object, you need to repeatedly invoke its read() method\nand take steps to properly decode the resulting data.\nHere is a sample C extension function that merely consumes all of the data on a file-like\nobject and dumps it to standard output so you can see it:\n#define CHUNK_SIZE 8192\n15.19. Reading File-Like Objects from C \n| \n659",
      "content_length": 2212,
      "extraction_method": "Direct"
    },
    {
      "page_number": 678,
      "chapter": 15,
      "content": "/* Consume a \"file-like\" object and write bytes to stdout */\nstatic PyObject *py_consume_file(PyObject *self, PyObject *args) {\n  PyObject *obj;\n  PyObject *read_meth;\n  PyObject *result = NULL;\n  PyObject *read_args;\n  if (!PyArg_ParseTuple(args,\"O\", &obj)) {\n    return NULL;\n  }\n  /* Get the read method of the passed object */\n  if ((read_meth = PyObject_GetAttrString(obj, \"read\")) == NULL) {\n    return NULL;\n  }\n  /* Build the argument list to read() */\n  read_args = Py_BuildValue(\"(i)\", CHUNK_SIZE);\n  while (1) {\n    PyObject *data;\n    PyObject *enc_data;\n    char *buf;\n    Py_ssize_t len;\n    /* Call read() */\n    if ((data = PyObject_Call(read_meth, read_args, NULL)) == NULL) {\n      goto final;\n    }\n    /* Check for EOF */\n    if (PySequence_Length(data) == 0) {\n      Py_DECREF(data);\n      break;\n    }\n    /* Encode Unicode as Bytes for C */\n    if ((enc_data=PyUnicode_AsEncodedString(data,\"utf-8\",\"strict\"))==NULL) {\n      Py_DECREF(data);\n      goto final;\n    }\n    /* Extract underlying buffer data */\n    PyBytes_AsStringAndSize(enc_data, &buf, &len);\n    /* Write to stdout (replace with something more useful) */\n    write(1, buf, len);\n    /* Cleanup */\n    Py_DECREF(enc_data);\n    Py_DECREF(data);\n  }\n  result = Py_BuildValue(\"\");\n660 \n| \nChapter 15: C Extensions",
      "content_length": 1297,
      "extraction_method": "Direct"
    },
    {
      "page_number": 679,
      "chapter": 15,
      "content": "final:\n  /* Cleanup */\n  Py_DECREF(read_meth);\n  Py_DECREF(read_args);\n  return result;\n}\nTo test the code, try making a file-like object such as a StringIO instance and pass it in:\n>>> import io\n>>> f = io.StringIO('Hello\\nWorld\\n')\n>>> import sample\n>>> sample.consume_file(f)\nHello\nWorld\n>>>\nDiscussion\nUnlike a normal system file, a file-like object is not necessarily built around a low-level\nfile descriptor. Thus, you can’t use normal C library functions to access it. Instead, you\nneed to use Python’s C API to manipulate the file-like object much like you would in\nPython.\nIn the solution, the read() method is extracted from the passed object. An argument\nlist is built and then repeatedly passed to PyObject_Call() to invoke the method. To\ndetect end-of-file (EOF), PySequence_Length() is used to see if the returned result has\nzero length.\nFor all I/O operations, you’ll need to concern yourself with the underlying encoding\nand distinction between bytes and Unicode. This recipe shows how to read a file in text\nmode and decode the resulting text into a bytes encoding that can be used by C. If you\nwant to read the file in binary mode, only minor changes will be made. For example:\n...\n    /* Call read() */\n    if ((data = PyObject_Call(read_meth, read_args, NULL)) == NULL) {\n      goto final;\n    }\n    /* Check for EOF */\n    if (PySequence_Length(data) == 0) {\n      Py_DECREF(data);\n      break;\n    }\n    if (!PyBytes_Check(data)) {\n      Py_DECREF(data);\n      PyErr_SetString(PyExc_IOError, \"File must be in binary mode\");\n      goto final;\n    }\n15.19. Reading File-Like Objects from C \n| \n661",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 680,
      "chapter": 15,
      "content": "/* Extract underlying buffer data */\n    PyBytes_AsStringAndSize(data, &buf, &len);\n    ...\nThe trickiest part of this recipe concerns proper memory management. When working\nwith PyObject * variables, careful attention needs to be given to managing reference\ncounts and cleaning up values when no longer needed. The various Py_DECREF() calls\nare doing this.\nThe recipe is written in a general-purpose manner so that it can be adapted to other file\noperations, such as writing. For example, to write data, merely obtain the write()\nmethod of the file-like object, convert data into an appropriate Python object (bytes or\nUnicode), and invoke the method to have it written to the file.\nFinally, although file-like objects often provide other methods (e.g., readline(),\nread_into()), it is probably best to just stick with the basic read() and write() meth‐\nods for maximal portability. Keeping things as simple as possible is often a good policy\nfor C extensions.\n15.20. Consuming an Iterable from C\nProblem\nYou want to write C extension code that consumes items from any iterable object such\nas a list, tuple, file, or generator.\nSolution\nHere is a sample C extension function that shows how to consume the items on an\niterable:\nstatic PyObject *py_consume_iterable(PyObject *self, PyObject *args) {\n  PyObject *obj;\n  PyObject *iter;\n  PyObject *item;\n  if (!PyArg_ParseTuple(args, \"O\", &obj)) {\n    return NULL;\n  }\n  if ((iter = PyObject_GetIter(obj)) == NULL) {\n    return NULL;\n  }\n  while ((item = PyIter_Next(iter)) != NULL) {\n    /* Use item */\n    ...\n    Py_DECREF(item);\n  }\n662 \n| \nChapter 15: C Extensions",
      "content_length": 1617,
      "extraction_method": "Direct"
    },
    {
      "page_number": 681,
      "chapter": null,
      "content": "Py_DECREF(iter);\n  return Py_BuildValue(\"\");\n}\nDiscussion\nThe code in this recipe mirrors similar code in Python. The PyObject_GetIter() call\nis the same as calling iter() to get an iterator. The PyIter_Next() function invokes\nthe next method on the iterator returning the next item or NULL if there are no more\nitems. Make sure you’re careful with memory management—Py_DECREF() needs to be\ncalled on both the produced items and the iterator object itself to avoid leaking memory.\n15.21. Diagnosing Segmentation Faults\nProblem\nThe interpreter violently crashes with a segmentation fault, bus error, access violation,\nor other fatal error. You would like to get a Python traceback that shows you where your\nprogram was running at the point of failure.\nSolution\nThe faulthandler module can be used to help you solve this problem. Include the\nfollowing code in your program:\nimport faulthandler\nfaulthandler.enable()\nAlternatively, run Python with the -Xfaulthandler option such as this:\nbash % python3 -Xfaulthandler program.py\nLast, but not least, you can set the PYTHONFAULTHANDLER environment variable.\nWith faulthandler enabled, fatal errors in C extensions will result in a Python trace‐\nback being printed on failures. For example:\n    Fatal Python error: Segmentation fault\n    Current thread 0x00007fff71106cc0:\n      File \"example.py\", line 6 in foo\n      File \"example.py\", line 10 in bar\n      File \"example.py\", line 14 in spam\n      File \"example.py\", line 19 in <module>\n    Segmentation fault\nAlthough this won’t tell you where in the C code things went awry, at least it can tell you\nhow it got there from Python.\n15.21. Diagnosing Segmentation Faults \n| \n663",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 682,
      "chapter": null,
      "content": "Discussion\nThe faulthandler will show you the stack traceback of the Python code executing at\nthe time of failure. At the very least, this will show you the top-level extension function\nthat was invoked. With the aid of pdb or other Python debugger, you can investigate the\nflow of the Python code leading to the error.\nfaulthandler will not tell you anything about the failure from C. For that, you will\nneed to use a traditional C debugger, such as gdb. However, the information from the\nfaulthandler traceback may give you a better idea of where to direct your attention.\nIt should be noted that certain kinds of errors in C may not be easily recoverable. For\nexample, if a C extension trashes the stack or program heap, it may render faulthan\ndler inoperable and you’ll simply get no output at all (other than a crash). Obviously,\nyour mileage may vary.\n664 \n| \nChapter 15: C Extensions",
      "content_length": 890,
      "extraction_method": "Direct"
    },
    {
      "page_number": 683,
      "chapter": null,
      "content": "APPENDIX A\nFurther Reading\nThere are a large number of books and online resources available for learning and\nprogramming Python. However, if like this book, your focus is on the use of Python 3,\nfinding reliable information is made a bit more difficult simply due to the sheer volume\nof existing material written for earlier Python versions.\nIn this appendix, we provide a few selected links to material that may be particularly\nuseful in the context of Python 3 programming and the recipes contained in this book.\nThis is by no means an exhaustive list of resources, so you should definitely check to\nsee if new titles or more up-to-date editions of these books have been published.\nOnline Resources\nhttp://docs.python.org\nIt goes without saying that Python’s own online documentation is an excellent re‐\nsource if you need to delve into the finer details of the language and modules. Just\nmake sure you’re looking at the documentation for Python 3 and not earlier ver‐\nsions.\nhttp://www.python.org/dev/peps\nPython Enhancement Proposals (PEPs) are invaluable if you want to understand\nthe motivation for adding new features to the Python language as well as subtle\nimplementation details. This is especially true for some of the more advanced lan‐\nguage features. In writing this book, the PEPs were often more useful than the\nofficial documentation.\nhttp://pyvideo.org\nThis is a large collection of video presentations and tutorials from past PyCon con‐\nferences, user group meetings, and more. It can be an invaluable resource for learn‐\ning about modern Python development. Many of the videos feature Python core\ndevelopers talking about the new features being added in Python 3.\n665",
      "content_length": 1687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 684,
      "chapter": null,
      "content": "http://code.activestate.com/recipes/langs/python\nThe ActiveState Python recipes site has long been a resource for finding the solution\nto thousands of specific programming problems. As of this writing, it contains\napproximately 300 recipes specific to Python 3. You’ll find that many of its recipes\neither expand upon topics covered in this book or focus on more narrowly defined\ntasks. As such, it’s a good companion.\nhttp://stackoverflow.com/questions/tagged/python\nStack Overflow currently has more than 175,000 questions tagged as Python-related\n(and almost 5000 questions specific to Python 3). Although the quality of the ques‐\ntions and answers varies, there is a lot of good material to be found.\nBooks for Learning Python\nThe following books provide an introduction to Python with a focus on Python 3:\n• Learning Python, 4th Edition, by Mark Lutz, O’Reilly & Associates (2009).\n• The Quick Python Book, 2nd Edition, by Vernon Ceder, Manning (2010).\n• Python Programming for the Absolute Beginner, 3rd Edition, by Michael Dawson,\nCourse Technology PTR (2010).\n• Beginning Python: From Novice to Professional, 2nd Edition, by Magnus Lie Het‐\nland, Apress (2008).\n• Programming in Python 3, 2nd Edition, by Mark Summerfield, Addison-Wesley\n(2010).\nAdvanced Books\nThe following books provide more advanced coverage and include Python 3 topics:\n• Programming Python, 4th Edition, by Mark Lutz, O’Reilly & Associates (2010).\n• Python Essential Reference, 4th Edition, by David Beazley, Addison-Wesley (2009).\n• Core Python Applications Programming, 3rd Edition, by Wesley Chun, Prentice Hall\n(2012).\n• The Python Standard Library by Example, by Doug Hellmann, Addison-Wesley\n(2011).\n• Python 3 Object Oriented Programming, by Dusty Phillips, Packt Publishing (2010).\n• Porting to Python 3, by Lennart Regebro, CreateSpace (2011), http://python3port\ning.com.\n666 \n| \nAppendix A: Further Reading",
      "content_length": 1896,
      "extraction_method": "Direct"
    },
    {
      "page_number": 685,
      "chapter": null,
      "content": "We’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\nIndex\nSymbols\n!r formatting code, 244\n% (percent) operator, 58, 556\nformat() function vs., 88\n* (star) operator\nEBNFs and, 70\n**kwargs, 218\ndecorators and, 331\nenforcing signature on, 364–367\nhelp function and, 219\nwrapped functions and, 353\n*args, 217\ndecorators and, 331\nenforcing signature on, 364–367\nwrapped functions and, 353\n+ (plus) operator, 48, 59\n-O option (interpreter), 343\n-OO option (interpreter), 343\n-W (warnings), 584\nall option, 583\nerror option, 584\nignore option, 584\n. (dot) operator, 49, 591\n< (less than) operator, date comparisons with,\n108\n== operator, 462\n? (question mark) modifier in regular expres‐\nsions, 48\n\\$ (end-marker) in regular expressions, 44\n_ (single underscore)\navoiding clashes with reserved words, 251\nnaming convention and, 250\n__ (double underscore), naming conventions\nand, 250\nA\nabc module, 274\nabspath() (os.path module), 551\nabstract base classes, 274–276\ncollections module and, 283–286\npredefined, 276\nabstract syntax tree, 388–392\nabstraction, gratuitous, 593\n@abstractmethod decorator (abc module), 275\naccepting script via input files, 539\naccessor functions, 238–241\nadjusting decorators with, 336–339\nACCESS_COPY (mmap module), 155\nACLs, 548\nactor model, 516–520\naddHandler() operation (logging module), 559\nadd_argument() method (ArgumentParser\nmodule), 543, 544\nAdvanced Programming in the Unix Environ‐\nment, 2e (Stevens, Rago), 538\nalgorithms, 1–35\nfiltering sequence elements, 26–28\n667",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 686,
      "chapter": null,
      "content": "finding most frequent item (sequences), 20–\n21\nlimited history, keeping, 5–7\nsorting for largest/smallest N items, 7–8\nunpacking, 1–5\nanonymous functions\ndefining, 224–225\nvariables, capturing, 225–227\nApache web server, 451\nappend() method (ElementTree module), 193\n%apply directive (Swig), 631\narchives\ncreating, 549\nformats of, 550\nunpacking, 549\nargparse module, 541, 543, 544\n.args attribute (Exceptions), 579\nArgumentParser instance, 543\narguments\nannotations, 220\ncleaning up in C extensions, 623\nmultiple-dispatch, implementing with, 376–\n382\narrays\ncalculating with large numerical, 97–100\nhigh-performance with Cython, 638–642\nin C extensions, 603\nlarge, sending/receiving, 481–483\nNumPy and, 97–100\noperating on, with extension functions, 609–\n612\nreadinto() method and, 152\nwriting to files from, 147\nassertRaises() method (unittest module), 570,\n571\nassertRaisesRegex() method (unittest module),\n571\nast module, 78, 311, 388–392\nasynchronous processing, 232–235\nattrgetter() function (operator module), 24\nauthentication\nencryption vs., 463\nhmac module, 461–463\npasswords, prompting at runtime, 544\nrequests package and, 439\nsimple, 461–463\ntesting with httpbin.org, 441\nB\nb16decode() function (base64 module), 199\nbinascii functions vs., 198\nb16encode() function (base64 module), 199\nbinascii functions vs., 198\nbase64 module, 199\nbinascii module vs., 198\nBaseException class, 577–579\nexcept statements and, 576\nBaseRequestHandler (socketserver module),\n443\nbasicConfig() function (logging module), 556–\n557\nbin() function, 89–90\nbinary data\narrays of structures, reading/writing, 199–\n203\nencoding with base64 module, 199\nmemory mapping, 153–156\nnested records, reading, 203–213\nreading into mutable buffers, 152–153\nreading/writing, 145–147\nunpack_from() method and, 202\nvariable-sized records, reading, 203–213\nbinary integers, 89–90\nbinascii module, 197–198\nbind() method (Signature objects), 344, 364\nbind_partial() method (Signature objects), 343\nbit_length() method (int type), 92\nBNF, 69\nboundscheck() decorator (Cython), 641\nbuffer attribute (files), 165\nBuffer Protocol, 147, 609\nBufferedWriter object (io module), 164\nbuilt-ins\ncontainers, 593\nexception class, 578\nmethods, overriding, 256\nbyte strings\nas return value of read(), 146\nencoding with base64, 199\nindexing of, 79\nperforming text operations on, 78–81\nprinting, 80\nwriting to text files, 165\nbytes, interpreting as text, 546\nBytesIO() class (io module), 148–149, 570\nbz2 compression format, 550\n668 \n| \nIndex",
      "content_length": 2494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 687,
      "chapter": null,
      "content": "bz2 module, 149–151\ncompresslevel keyword, 150\nC\nC APIs, 612–614\nC extension programming, 566, 597\nAPIs, defining/exporting, 612–614\naquiring GIL in, 624\ncapsule objects in, 612–614\nctypes module and, 599–605\nCython, wrapping code in, 632–638\nfile-like objects, reading, 659–662\nfilenames, passing to, 657–658\nfunction pointers, converting to callables,\n643–644\nGIL and, 515–516\nGIL, releasing in, 625\niterables from C, consuming, 662–663\nloading modules, 601\nmodules, building, 607\nnull-terminated strings, passing, 644–648\nopaque pointers in, 612–614\nopen files, passing to, 658–659\noperating on arrays with, 609–612\nPython callables in, 619–624\nPython/C threads, mixing, 625–626\nsegmentation faults and, 663–664\nstrings, converting to Python objects, 653–\n654\nSwig, wrapping with, 627–631\nUnicode strings, passing, 648–653\nunknown string type and, 654–657\nwriting, modules, 605–609\nC structures, 147\ncalendar module, 108\n__call__() method (metaclasses), 347–349, 356\ncallback functions\ncarrying extra state with, 232–235\ninlining, 235–238\ncapsule objects, 612–614\nC APIs, defining/exporting with, 612–614\nCelery, 457\ncenter() method (str type), 57\ncertificate authorities, 468\nCFFI, 605\nCFUNCTYPE instances (ctypes module), 643\nchain() method (itertools module), 131–132\nchained exceptions, 581, 582\nChainMap class (collections module), 33–35\nupdate() method vs., 35\nchdir() function (os module), 537\ncheck_output() function (subprocess class), 546\nChicago, Illinois, 186, 214\nchoice() function (random module), 102\n__class__ attribute, 302–304\nclass decorators, 279, 355–356\ndata models and, 281\nmixin classes vs., 281\nmixins and, 298\nclasses, 243–327\nabstract base classes, 274–276\nattribute definition order, 359–362\ncaching instances of, 323\nclosures vs., 238–241\ncoding conventions in, 367–370\nconstructors, multiple, 291–292\ncontainers, custom, 283–286\ndata models, implementing, 277–283\ndecorators, 279\ndecorators, defining in, 345–347\ndefining decorators as, 347–349\ndelegating attributes, 287–291\ndescriptors and, 264–267\nextending properties in subclasses, 260–264\nextending with mixins, 294–299\nimplementing state for objects/machines,\n299–305\ninheritance, implementation of, 258\ninitializing members at definition time, 374–\n375\ninitializing of data structures in, 270–274\nlazy attributes and, 267\nmixins, 260, 294–299\nprivate data in, 250–251\nreplacing with functions, 231–232\nstatements, metaclass keyword in, 362–364\nsuper() function and, 256–260\nsupporting comparison operations, 321–323\ntype system, implementing, 277–283\n__init__() method, bypassing, 293–294\n@classmethod decorator, 330\ndecorating class methods with, 350–352\n__func__ attribute and, 334\nclient module (http package), 440\nclock() function (time class), 561\nclose() method (generators), 531\nclosefd argument (open() function), 166\nIndex \n| \n669",
      "content_length": 2830,
      "extraction_method": "Direct"
    },
    {
      "page_number": 688,
      "chapter": null,
      "content": "closures\naccessing variables inside, 238–241\ncapturing state with, 233–234\nclasses vs., 238–241\nconverting classes to functions with, 231\nnonlocal declarations and, 239\ncmath module, 93\ncode readability and string templates, 453\ncoding conventions, 367–370\ncollections module\nChainMap class, 33–35\nCounter class, 20–21\ndefaultdict class, 11\ndeque class, 5–7\nimplementing custom containers with, 283–\n286\nnamedtuple() function, 30\nOrderedDict class, 12–13\ncombinations() function (itertools module), 126\ncombinations_with_replacement() function\n(itertools module), 126\ncombining() function (unicodedata module), 51\ncommand-line options, parsing, 539, 541, 542\ncompare_digest() function (hmac module), 462\ncomparison operations, 321–323\ncompile() function (ast module), 390\ncompile() function (re module), 49\ncomplex() function, 92\ncomplex-valued math, 92–94\ncompress() function (itertools module), 28\ncompressed files, reading/writing, 149–151\nspecifying level of, 150\nconcurrency, 485–538\nactor model and, 516–520\ncoroutines, 524–531\nevent-driven I/O and, 479\nGIL, 513–516\nparallel programming, 509–513\npolling thread queues, 531–534\nthreads, 485–488\nwith generators, 524–531\nCondition object (threading module), 489\nconditionals and NaN values, 95\nconfig file\nPython code vs., 554\nPython source file vs., 553\nConfigParser module, 552–555\nconfiguration files, 552\nconnect() function (sqlite3 module), 196\nconnection (multiprocessing module), 456–458\nconstructors, defining multiple, 291–292\ncontainers, custom, 283–286\niterating over items in separate, 131–132\n__iter__() method and, 114–115\ncontext managers\ndefining, 384–385\nuse instance as, 540\ncontext-management protocols, 246–248\ncontextlib module, 238\n@contextmanager decorator (contextlib mod‐\nule), 238, 385\ncontextmanager module, 248\ncookies, 439\ncopy2() function (shutil module), 548\ncopying directories/files, 547\ncopytree() function (shutil class), 548–549\ncoroutines, 524–531\ncapturing state with, 233\ninlining callback functions with, 235\nCounter objects (collections module), 20–21\ncountry_timezones dictionary (pytz module),\n112\ncProfile module, 587\nCPU-bound programs, 514\nCPUs, restricting use of, 561\ncritical() function (logging module), 556\ncryptography, 103\nCSV files, reading/writing, 175–179\ncsv module, 175–179\nctypes module, 599–605\nalternatives to, 604\nmemory addresses, wrapping, 643–644\ncustom exceptions, 578\nCython, 604\narray handling with, 612\nhigh-performance arrays with, 638–642\nmemoryviews in, 640\nreleasing the GIL, 636\nsetup.py file, 633, 639\nwrapping existing code in, 632–638\nD\ndaemon processes (Unix), 534–538\ndaemonic threads, 486\ndata\npickle module and, 171–174\nserializing, 171–174\n670 \n| \nIndex",
      "content_length": 2690,
      "extraction_method": "Direct"
    },
    {
      "page_number": 689,
      "chapter": null,
      "content": "transforming/reducing simultaneously, 32–\n33\ndata analysis, 178\ndata encapsulation, 250–251\ndata encoding\nCSV files, reading/writing, 175–179\nfor RPCs, 460\nhexadecimal digits, 197–198\nJSON, reading/writing, 179–183\nwith base64 module, 199\nXML, extracting data from, 183–186\ndata models, 277–283\ndata processing\ndictionaries, converting to XML, 189–191\nnested binary records, reading, 203–213\nparsing large XML files, 186–189\nrelational databases and, 195–197\nstatistics, 214–216\nsummarizing, 214–216\nvariable-sized binary records, reading, 203–\n213\nXML namespaces, 193–195\nXML, parsing/modifying/rewriting, 191–193\ndata structures, 1–35, 593\ncyclic, managing memory in, 317–320\ndictionaries\ngrouping records by field, 24–26\nsorting by common keys, 21–23\nheaps, 7–11\ninitializations of, simplifying, 270–274\nmultidicts, 11–12\ndatabase API (Python), 195–197\nstrings and, 197\ndatagrams, 445\ndate calculations, 106–107\ndatetime module, 104–112\ndatabases and, 197\ndate calculations with, 106–107\nreplace() method, 108\nstrptime() method, 109\ntime conversions, 104–105\ntime zones and, 110\ntimedelta object, 104\ndateutil module, 105\ndeadlock avoidance, 503\ndeadlocks, 500–503\ndining philosophers problem, 503\nwatchdog timers and, 503\n__debug__ variable, 343\ndebug() function (logging module), 556\ndebugger, launching, 585\ndebugging, 584\ndecimal module, 84–86\ndatabases and, 197\nformatting output of, 88\ndecode() method (str type), 56\ndecorators, 329–356\nadding function arguments with, 352–354\nadjustable attributes for, 336–339\nclass methods, applying to, 350–352\ndefining as class, 347–349\ndefining in classes, 345–347\nforcing type checking with, 341–345\nmultiple-dispatch, implementing with, 381\noptional arguments for, 339–341\npatching classes with, 355–356\npreserving function metadata in, 331–333\nstatic methods, applying to, 350–352\nunwrapping, 333–334\nwith arguments, 334–336\nwrappers as, 329–331\n__wrapped__ attribute, 332\ndeepcopy() function (copy module), 594\ndefaultdict object (collections module), 11\ngroupby() function vs., 26\nordinary dictionary vs., 12\n__delattr__() method (delegation), 290\ndelegation\ninheritance vs., 289\nof attributes, 287–291\n__delete__() method (descriptors), 265\nDeprecationWarning argument (warn() func‐\ntion), 583\ndeque class (collections module), 5–7\nappending, 6\npopping, 6\ndescriptors\ncreating, 264–267\ndata models, implementing, 277–283\nextending, 263\nlazy attributes and, 267–270\ntype system, implementing, 277–283\ndeserializing data, 293–294\nDesign Patterns: Elements of Reusable Object-\nOriented Software (Gamma, Helm, Johnson,\nand Vlissides), 305\ndest argument (add_argument() method), 544\ndetach() method, 164\nIndex \n| \n671",
      "content_length": 2666,
      "extraction_method": "Direct"
    },
    {
      "page_number": 690,
      "chapter": null,
      "content": "dict() function, 29, 594\ndictionaries\ncomparing, 15–16\nconverting to XML, 189–191\nCounter objects and, 21\ndefaultdict vs., 12\ngrouping records based on field, 24–26\nJSON support for, 179\nkeeping in order, 12–13\nmultiple values for a single key in, 11–12\nremoving duplicates from, 17\nsorting, 13–15\nsorting list of, by common key, 21–23\nsubsets, extracting, 28–29\ndictionary comprehension, 29\ndining philosophers problem (deadlocks), 503\ndirectories\nas runnable scripts, 407–408\ncopying, 547\nmoving, 547\ntemporary, 167–170\ndirectory listings, 158–159\ndis module, 392–395\ndiscarding vs. filtering iterables, 124\ndistributed systems and property calls, 255\ndomain sockets (Unix), 457, 470\nconnecting interpreters with, 472\nDOTALL flag (re module), 49\ndropwhile() function (itertools module), 123\ndump() function (pickle module), 171\ndumps() function (json module), 179\nindent argument, 181\nsort_keys argument, 182\ndumps() function (pickle module), 171\ndup2() function (os module), 537\nE\nEBNF, 69\nElementTree module (xml.etree), 66, 183–186\ncreating XML documents with, 189–191\niterparse() function, 188\nparse() function, 185\nparse_and_remove() function, 189\nparsing namespaces and, 194\nparsing/modifying/rewriting files with, 191–\n193\nempty() method (queue module), 496\nencode() method (str type), 56\nencoding text data, 141–144\nchanging in open files, 163–165\npassing to C libraries, 648–653\npickling and, 174\nencryption vs. authentication, 463\nend argument (print() function), 144\nendswith() method (str type), 38\npattern matching with, 42\n__enter__() method (with statements), 246–248\nenumerate() function, 127–128\nenviron argument (WSGI), 451\nerror messages, terminating program with, 540\nerror() function (logging module), 556\nescape() function (html module), 65\nescape() function (xml.sax.saxutils module), 191\nEvent object (threading module), 488–491\nevent-driven I/O, 475–481\nexcept statement, 576, 580\nchained exceptions and, 581\nreraising exceptions caught by, 582\nException class, 579\nhandler for, 576\nexceptions\ncatching all, 576\ncreating custom, 578\nhandling multiple, 574\nraising in response to another exception, 580\nreraising, 582\nSystemExit, 540\ntesting for, 570\nwith statements and, 247\nexec() function, 386–388\nexecve() function (os module), 546\n__exit__() method (with statements), 246–248\n@expectedFailure decorator, 574\nexponential notation, 87\n%extend directive (Swig), 630\nexternal command\nexecuting, 545\ngetting output, 545\nF\nfactory functions, 323\nfaulthandler module, 663–664\nfdel attribute (properties), 253\nfget attribute (properties), 253\nFieldStorage() class (cgi module), 452\nfile descriptor\npassing between processes, 470–475\n672 \n| \nIndex",
      "content_length": 2669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 691,
      "chapter": null,
      "content": "wrapping in object, 166\nfile-like objects, reading from C, 659–662\nFileInput class, 540\nhelper methods, 540\nfileinput module (built-in), 539\nFileIO class (io module), 164\nFileNotFoundError exception, 575\nfiles, 141–174\naccepting scripts via, 539\nbinary data, reading/writing, 145–147\nbypassing name encoding, 160–161\nbytes, writing to text, 165\nchanging encoding in open, 163–165\ncompressed, reading/writing, 149–151\ncopying, 547\ncreating new, 147–148\ndefining modules in multiple, 401–404\ndetach() method, 164\ndirectory listings, getting, 158–159\nfinding by name, 550\ngetfilesystemencoding() function (sys mod‐\nule), 160–161\nheaders, unpacking, 205\niterating over fixed-sized records in, 151\nline ending, changing, 144–145\nmanipulating pathnames of, 156–157\nmemory mapping, 153–156\nmoving, 547\nmutable buffers, reading into, 152–153\nnames, passing to C extensions, 657–658\nnewlines, 142\nopen() function, 141–144\nopen, passing to C extensions, 658–659\npath module (os), 156–157\nprinting bad filenames for, 161–163\nprinting to, 144\nreading, 408–409\nreadinto() method of, 147\nsaving objects to, 171–174\nseparator character, changing, 144–145\ntemporary, 167–170\ntesting for existence of, 157–158\ntext data from, 141–144\nunpacking, 2\nwrapping descriptors in objects, 166\nx mode of open() function, 147–148\nfiles list, 551\nfilesystem, byte strings and, 80\nfill() function (textwrap module), 65\nfilter() function, 27\nfiltering\ndiscarding iterables vs., 124\nnormalization of Unicode text and, 51\nsequence elements, 26–28\ntoken streams, 68\nfind Unix utility, 551\nfind() method (str type)\npattern matching with, 42\nsearch/replace text and, 46\nfindall() method (re module), 43\nfinditer() method (re module), 44\nfind_library() function (ctypes.util module),\n601\nfloat()\ndecimal arithmetic and, 84–86\nformat() function (built-in), 87–88\nj suffix for, 92\nfnmatch module, 40–42\nfnmatch() function (fnmatch module), 40–42\nfnmatchcase() function (fnmatch module), 40–\n42\nfor loops\nconsuming iterators manually vs., 113–114\ngenerators and, 115–117\nfork() function (os module), 537\nForkingTCPServer objects (socketserver mod‐\nule), 442\nForkingUDPServer objects (socketserver mod‐\nule), 446\nformat() function (built-in )\nand non-decimal integers, 89\nfloats, specifying precision with, 84\nformatting output with, 244\nformat() function (built-in), 87–88\nformat() method (str type), 57\ncustomizing string, 245–246\ninterpolating variables, 61–64\n__format__() method, 245–246\nformat_map() method (str type), 62\nfpectl module, 95\nfractions module, 96–96\nframe hacking, 373\nfrom module import * statement (modules), 398\nreload() function and, 406\nfrom module import name, 591\nfromkeys() method (dict type), 55\nfrom_bytes() method (int type), 91\nfset attribute (properties), 253\nIndex \n| \n673",
      "content_length": 2765,
      "extraction_method": "Direct"
    },
    {
      "page_number": 692,
      "chapter": null,
      "content": "fsum() function (math module), 86\nfull() method (queue module), 496\nfunctions, 217–241\naccepting arbitrary arguments for, 217–218\naccessor functions as attributes of, 238–241\nanonymous/inline, defining, 224–225\nargument annotations, 220\ncalling with fewer arguments, 227–230\nclosures, accessing variables inside, 238–241\ndefault arguments for, 222–224\ndisassembling, 392–395\ninlining callbacks, 235–238\nkeyword arguments only, 219–220\nmultiple return values for, 221\npartial() function and, 227–230\npointers, converting to callables, 643–644\nreplacing classes with, 231–232\nstate, carrying, 232–235\nwrapper layers for, 329–331\nwrapping with Cython, 632–638\nfutures module (concurrent module), 479, 505–\n509\nProcessPoolExecutor class, 509–513\nFutureWarning argument (warn() function),\n583\nG\ngarbage collection\ncaching instances and, 325\ncyclic data structures and, 318\ngauss() function (random module), 103\ngdb debugger, 663–664\nGeneral Decimal Arithmetic Specification\n(IBM), 86\ngenerator expressions\nconcatenating/combining strings and, 60\nfiltering sequence elements with, 26\nflattening nested sequences with, 135–136\nrecursive algorithms vs., 311–317\ntransforming/reducing data with, 32–33\nGenerator Tricks for Systems Programmers\n(Beazley), 135\ngenerator-expression argument, 32\nGeneratorExit exception, 577\ngenerators, 113–139\nconcurrency with, 524–531\ncreating iterator patterns with, 115–117\ndefining with extra states, 120\ninlining callback functions with, 235\nislice() function and, 122–123\niterator protocol, implementing, 117–119\npipelines, creating with, 132–135\nsearch functions and, 6\nslicing, 122–123\nunpacking, 2\nGET requests (HTTP), 437\nget() function (webbrowser class), 563\nget() method (ElementTree module), 185\nget() method (queue module), 491, 495\n__get__() method, 347–349\ndescriptors, 265–266\ngetattr() function, 305\nvisitor patterns and, 310\n__getattr__() method (delegation), 287, 290\ngetboolean() method (ConfigParser module),\n554\ngetdefaultencoding() function (sys module), 142\ngetfilesystemencoding() function (sys module),\n160–161\ngetframe() (sys module), 63\nframe hacking with, 373\n__getitem__() method, 23\ngetLogger() function (logging module), 558\ngetpass module, 544\ngetpass() method (getpass module), 545\ngetrandbits() function (random module), 103\ngetrecursionlimit() function (sys module), 310\ngettempdir() function (tempfile module), 169\ngetuser() (getpass module), 544\nget_archive_formats() function (shutil module),\n550\nget_data() function (pkgutil module), 409\nget_terminal_size() function (os module), 65,\n545\ngevent module, 531\nglob module, 42\nGlobal Interpreter Lock (GIL), 487, 513–516\nC, calling Python from, 624\nC/Python threads, mixing, 625–626\nreleasing in C extensions, 625\nreleasing in Cython, 636\nthread pools and, 508\ngrammar rules, for parsers, 69–78\ngreenlets, 531\n.group() method (re module), 46\ngroupby() function (itertools module), 24–26\ngrouping records, in dictionaries, 12\ngzip compression format, 550\n674 \n| \nIndex",
      "content_length": 2977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 693,
      "chapter": null,
      "content": "gzip module, 149–151\ncompresslevel keyword, 150\nH\nhard limit on resources, 562\nhashes, 17\nheappop() method (heapq module), 8\npriority queues and, 9\nheapq module, 7–8\nmerge() function, 136–137\nheaps, 7–11\nnlargest()/nsmallest() functions and, 8\nreversing order of, 10\nhelp function\nkeyword arguments and, 219\nmetadata for arguments and, 220\nhex() function, 89–90\nhexadecimal digits, encoding, 197–198\nhexadecimal integers, 89–90\nhmac module, 461–463\nHTML entities\nhandling in text, 65–66\nreplacing, 66\nhtml module, 65\nHTTP services\nclients, 437–441\nheaders, custom, 438\ntesting client code, 441\nhttpbin.org service, 441\nI\nI/O, 141–174\ndecoding/encoding functions for, 56\nevent-driven, 475–481\niter() function and, 139\nobjects, serializing, 171–174\noperations on strings, 148–149\npassing open files and, 659\nserial ports, communicating with, 170–171\nthread termination and, 487\nIDEs for Python development, 587\n__init__() method, data structures and, 273\nif-elif-else blocks, 304\nIGNORECASE flag (re module), 47\nignore_patterns() function (shutil module), 548\nignore_types argument (isinstance() function),\n135\nimport hooks, 418–420\npatching modules on import, 428–431\nsys.path_hooks variable and, 420\nimport statement, 412–428\nImportError exceptions, 425\nimportlib module, 421\nimport_module() function (importlib module),\n411, 430\nindex-value pairs, iterating over, 127–128\nIndexError exception, 19\nindexing in CSV files, 175–179\nindices() method (slice), 19\ninfinite values, 94–95\ninfo() function (logging module), 556\ninheritance\nclass decorators and, 356\nclass defined decorators and, 347\ndelegation vs., 289\nimplementation of, 258\n.ini file features, 555\n__init__() method (classes), 579\nbypassing, 293–294\ncoding conventions and, 367–370\ndata structure initialization and, 270–274\nmixins and, 297\nmultiple constructors and, 291–292\noptional arguments and, 363\nsuper() function and, 256–260\ninline functions\ncallbacks, 235–238\ndefining, 224–225\ninput files, 539\ninput() function (fileinput), 540\ninsert() method (ElementTree module), 193\ninspect() module, 364–367\ninstances\ncached, metaclasses and, 358\ncreation, controlling with metaclasses, 356–\n359\ndescriptors and, 264–267\nsaving memory when creating, 248–249\nserializing with JSON, 182\nWSGI applications as, 452\nint type, 90–92\nint() function, 90\ninterface files (Swig), 627\ninterpreters, communication between, 456–458\ninvalidate_caches() function (importlib mod‐\nule), 427\nIndex \n| \n675",
      "content_length": 2446,
      "extraction_method": "Direct"
    },
    {
      "page_number": 694,
      "chapter": null,
      "content": "io module, 148–149, 163–165\nioctl(), 545\nIPv4Address objects (ipaddress module), 448\nis operator, 223\nisinf() function (math module), 94\nisinstance() function, 135\nislice() function (itertools module), 122–123,\n124\nisnan() function (math module), 94\nissuing warning messages, 583\nis_alive() method (Thread class), 486\nitemgetter function (operator module), 22\nitems() method (dictionaries), 16\niter() function\nfixed size records and, 151\nwhile loops vs., 138–139\n__iter__() method (iterators), 114–115\ngenerators with extra states and, 121\niterables\ncustom containers for, 283–286\ndiscarding parts of, 123–125\nfrom C, consuming, 662–663\nsorted/merged, iterating over, 136–137\nunpacking, 1–2\niterating\nall possible combinations/permutations,\n125–127\nchain() method (itertools module), 131–132\niter() function vs. while loops, 138–139\nmerge() function (heapq module), 136–137\nover fixed-sized records, 151\nover index-value pairs, 127–128\nover separate containers, 131–132\nover sorted/merged iterables, 136–137\nreverse, 119\nzip() function and, 129–130\niterators, 113–139\nconsuming manually, 113–114\ncreating patterns with generators, 115–117\ndelegating, 114–115\nislice() function (itertools module), 122–123\nprotocol, implementing, 117–119\nslicing, 122–123\niterparse() function (ElementTree module), 188\niterparse() method (ElementTree module), 194\nitertools module, 123–125, 125–127\ncompress() function, 28\ngroupby() function, 24–26\niter_as() function, 211\nJ\njoin() method (str type), 58–61\nchanging single characters with, 145\njoin() method (Thread class), 486\nJSON (JavaScript Object Notation)\nreading/writing, 179–183\ntypes supported by, 179\njson module, 179–183\njust-in-time (JIT) compiling, 590, 594\nK\nkey argument (sorted() function), 23\nKeyboardInterrupt exception, 577, 578\nkeys() method (dictionaries), 16\nkeyword arguments, 218\nannotations, 220\nfor metaclasses, 362–364\nfunctions that only accept, 219–220\nhelp function and, 219\nL\nlambda expressions, 24, 225\nvariables, capturing, 225–227\nlaunching\nPython debugger, 585\nweb browsers, 562\nlazy imports, 403\nlazy unpacking, 212\nleap years, datetime module and, 104\nlibraries, adding logging to, 558\nlimited history, 5–7\nlinalg subpackage (NumPy module), 101\nlinear algebra, 100–101\nlist comprehension, 26\nlistdir() function (os module), 158–159\nbad filenames, printing, 161–163\nlists\nNumPy vs., 99\nprocessing and star unpacking, 5\nunpacking, 2\nljust() method (str type), 57\nllvmpy extension module, 643\nload() function (pickle module), 171\nuntrusted data and, 172\nLoadLibrary() function (ctypes.cdll module),\n601\nloads() function (json module), 179\n676 \n| \nIndex",
      "content_length": 2618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 695,
      "chapter": null,
      "content": "loads() function (pickle module), 171\nloadTestsFromModule() method (TestLoader\nmodule), 573\nloadTestsFromTestCase() method (TestCase\nmodule), 573\nlocal() method (threading module), 504–505\nlocale module, 88\nlocals() function and exec() function, 386–388\nLock objects (threading module), 497–500\nlogging\nadding to libraries, 558\nadding to simple scripts, 555\noutput of, 557\ntest output to file, 572\nlogging module, 557, 559\ncritical(), 556\ndebug(), 556\nerror(), 556\ninfo(), 556\nwarning(), 556\nlower() method (str type), 54\nlstrip() method (str type), 53\nlxml module, 195\nM\nMagicMock instances, 568\nmakefile() method (socket module), 167\nmake_archive() function (shutil module), 549\nMANIFEST.in file, 434\nmanipulating pathnames, 156–157\nmap() operation (ProcessPoolExecutor class),\n511\nMapping class (collections module), 283\nmappings\nclass definition order and, 362\nconsolidating multiple, 33–35\nmatch() method (re module), 43\nsearch/replace text and, 46\nmath() module, 94\nmatrix calculations, 100–101\nmatrix object (NumPy module), 100\nmax() function, 8, 15\nattrgetter() function and, 24\ngenerator expressions and, 33\nitemgetter() function, 23\nmemory\nmanagement in cyclic data structures, 317–\n320\nmapping, 153–156\nOrderedDict objects and, 13\nparsing XML and, 186–189\nrestricting use of, 561\nwchar_t strings and, 651\n__slots__ attribute and, 248–249\nmemory mapping, 153–156\nMemoryError exceptions, 562\nmemoryview() object (struct module), 153, 206,\n213\nCython-typed, 640\nlarge arrays and, 481–483\nmemory_map() function (mmap module), 154\nmerge() function (heapq module), 136–137\nMersenne Twister algorithm, 103\nmeta path importer, 414–418\nsys.metapath object, 418\nmetaclass keyword (class statement), 362–364\nmetaclasses, 279\ncapturing attribute definition order, 359–362\ncontrolling instance creation, 356–359\nenforcing coding conventions, 367–370\ninitializing members at definition time, 374–\n375\nnon-standardized binary files and, 207\noptional arguments for, 362–364\n__prepare__() method and, 361\nmetaprogramming, 329–395\nadjustable attributes, 336–339\ndecorators, 329–356\ndecorators with arguments, 334–336\ndis module, 392–395\nexec() function vs., 386–388\nfunction metadata in decorators, 331–333\nfunction wrappers, 329–331\nrepetitive property methods and, 382–384\nunwrapping decorators, 333–334\nmethod overloading, 376–382\nmethodcaller() function (operator module), 305\nMicrosoft Excel, CSV encoding rules of, 177\nMicrosoft Visual Studio, 608\nmin() function, 8, 15\nattrgetter() function and, 24\ngenerator expressions and, 33\nitemgetter() function, 23\n__missing__() method (dict module), 62\nmixin classes, 260\nextending unrelated classes with, 294–299\nin descriptors of data models, 280\nmkdtemp() function (tempfile module), 169\nIndex \n| \n677",
      "content_length": 2745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 696,
      "chapter": null,
      "content": "mkstemp() function (tempfile module), 169\nmmap module, 153–156\nmock module (unittest module), 565, 570\nmodule import * statement, 398–399\nmodules, 397–435\ncontrolling import of, 398–399\nfrom module import * statement, 398\nhierarchical packages of, 397–398\nimport hooks, using, 412–428\nimporting with relative names, 399–401\nimporting, using string as name, 411\nimportlib module, 421\nimport_module() function (importlib mod‐\nule), 411\ninvalidate_caches() function (importlib\nmodule), 427\nmeta path importer, 414–418\nnew_module() function (imp module), 421\nobjects, creating, 421\npatching on import, 428–431\nrelative imports vs. absolute names, 400\nreloading, 406–407\nremote machines, loading from, 412–428\nsplitting into multiple files, 401–404\nsys.path, adding directories to, 409–411\nsys.path_importer_cache object, 424\nvirtual environments and, 432–433\n__all__ variable in, 398–399\n__init__.py file, 397\n__main__.py files, 407–408\nmonthrange() function (calendar module), 108\nmonths, finding date ranges of, 107–109\nmoving directories/files, 547\n__mro__ attribute, 576\nmultidicts, 11–12\nmultiple-dispatch, 376–382\nmultiprocessing module, 488\nGIL and, 514\npassing file descriptors with, 470–475\nreduction module, 470–475\nRPCs and, 459\nmutable buffers, reading into, 152–153\nMutableMapping class (collections module),\n283\nMutableSequence class (collections module),\n283\nMutableSet class (collections module), 283\nN\nnamed pipes (Windows), 457, 470, 472\nNamedTemporaryFile() function (tempfile\nmodule), 169\nnamedtuple() function (collections module), 30\nnamedtuple() method (collections module)\nnew_class() function and, 372\nunpacking binary data and, 203\nNameError exception, 581\nnamespace package, checking for, 426\nnamespaces (XML), parsing, 193–195\nnamespaces, multiple directories in, 404–406\nnaming conventions\nfor private data, 250–251\n__ (double underscore) and, 250\nNaN (not a number) values, 94–95\nin JSON, 183\nisnan() function and, 95\nnested sequences, flattening, 135–136\nnetwork programming, 437–483\nconnection (multiprocessing module), 456–\n458\nevent-driven I/O, 475–481\nhmac module, 461–463\ninterpreters, communication between, 456–\n458\nlarge arrays, sending/receiving, 481–483\npassing file descriptors, 470–475\nremote procedure calls, 454–456, 458–461\nsimple authentication, 461–463\nsocketserver module, 441–444\nSSL, implementing, 464–470\nTCP servers, 441–444\nUDP server, implementing, 445–446\nUDPServer class, 446\nXML-RPC, 454–456\n__new__() method (classes)\ncoding conventions and, 367–370\noptional arguments and, 363\nnewlines, 142\nnew_class() function (types module), 370–373\nnew_module() function (imp module), 421\nnext() function (iterators), 113\nnlargest() function (heapq module), 7–8\nnonblocking, supporting with queues, 495\nnoncapture groups (regular expressions), 49\nnonlocal declarations, 239\nadjusting decorators with, 336–339\n678 \n| \nIndex",
      "content_length": 2865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 697,
      "chapter": null,
      "content": "normalize() function (unicodedata module), 51,\n55\nnormalize() method (pytz module), 111\nnormalizing Unicode, 50–51\nsupported forms in Python, 51\nnormpath() (os.path module), 551\nnsmallest() function (heapq module), 7–8\nnull-terminated strings, passing to C, 644–648\nNullHandler() class (logging module), 559\nnumerical operations, 83–112\ncomplex-valued math, 92–94\ndecimal calculations, 84–86\nformating for output, 87–88\ninfinity and, 94–95\nlinear algebra calculations, 100–101\nmatrix calculations, 100–101\nNaNs and, 94–95\nNumPy and, 97–100\non fractions, 96–96\npacking/unpacking integers from byte\nstring, 90–92\nrandom number generators, 102–103\nrounding, 83–84\ntime, 104–112\ntime conversions, 104–105\nwith binary integers, 89–90\nwith hexadecimal integers, 89–90\nwith octal integers, 89–90\nNumPy module, 97–100, 640\ncomplex math and, 93\nCPU-bound programs and, 514\nlinear algebra and, 100–101\nmatrix calculations and, 100–101\nunpacking binary data with, 203\nO\nobjects, 243–327\nand context-management protocols, 246–\n248\ncalling methods on, when named in strings,\n305–306\ncreating large numbers of, 248–249\ndefining default arguments and, 224\nformat() function and, 245–246\nformatting output of, 243–244\nimplementing states for, 299–305\niterator protocol, implementing, 117–119\nJSON dictionary, decoding to, 181\nmemory management, 317–320\nrepresenting in C, 608\nserializing, 171–174\nvisitor pattern, implementing, 306–311\nvisitor patterns without recursion, 311–317\nwith statement and, 246–248\nobject_hook (json.loads() function), 181\nobject_pairs_hook (json.loads() function), 181\noct() function, 89–90\noctal integers, 89–90\nOlson time zone database, 110\nopaque pointers, 612–614\nopen() function, 141–144\nbinary data, reading/writing, 145–147\nclosefd argument, 166\nfile descriptors and, 166\non non-Unix systems, 167\nrb/wb modes of, 145–147\nrt/wt mode, 141–144\nx mode of, 147–148\noptimizating your programs, 590\noptparse vs. argparse, 544\nOrderedDict object (collections module), 12–13\ndecoding JSON data into, 181\nos module\nlistdir() function, 158–159\npath module in, 156–157\nOSError exception, 575–576\nouput sent to stdout, 565\nP\npack() function (structs), 201\npackages, 397–435\ndatafiles, reading, 408–409\ndirectories/zip files, running, 407–408\ndistributing, 433–435\nhierarchical, of modules, 397–398\nMANIFEST.in file, 434\nnamespaces, multiple directories in, 404–406\nper-user directory, installing, 431–432\nsubmodules, importing with relative names,\n399–401\nsys.path, adding directories to, 409–411\nthird-party options for, 435\nvirtual environments and, 432–433\n__init__.py file, 397\n__path__ variable and, 405\npack_into() function (struct module), 153\nPandas package, 178, 214–216\nparallel programming, 509–513\nIndex \n| \n679",
      "content_length": 2728,
      "extraction_method": "Direct"
    },
    {
      "page_number": 698,
      "chapter": null,
      "content": "parse tree, 73\nparse() function (ElementTree module), 185\nparser (html), 66\nparser, recursive descent, 69–78\nparse_and_remove() function (ElementTree\nmodule), 189\nparsing command-line options, 539, 541\npartial() function (functools module), 227–230\ndefining property methods and, 383\nfixed size records and, 151\noptional arguments for decorators and, 341\nstate, carrying with, 234, 235\nPaste, 453\npatch() function (unittest.mock class)\ntesting output with, 565\nunit tests with, 567\npatching objects, 567\npath module (os), 156–157\ntesting for existing files with, 157–158\npathnames, manipulating, 156–157\npattern matching, 42–45\npattern object (regular expressions), 43\npdb debugger, 663–664\nper-user directory, 431–432\nperformance\nast manipulation and, 392\nattrgetter() function and, 24\nCython and, 638–642\ndictionary comprehension vs. dict() type, 29\njoin() method vs. + operator, 60\nkeeping limited history and, 6\nlazy attributes and, 267–270\nnlargest()/nsmallest() functions and, 7\nNumPy module and, 97–100\nof byte vs. text strings, 81\npattern objects and, 45\nprofiling/timing, 587\nsanitizing strings and, 56\nstrptime() method and, 110\nperf_counter() function (time class), 561, 589\nPermissionError exception, 575\npermutations() function (itertools module), 125\npexpect, 547\npickle module, 171–174\nlimits on, 172\nmultiprocessing.connection functions and,\n457\nRPCs and, 458–461\nPipe object (multiprocessing module), 471\npipelines, creating, 132–135\npipes\naccepting script via, 539\nmimicking, 132–135\nPLY, 69, 76\npolling thread queues, 531–534\nPopen class (subprocess module), 547\nPOST method (HTTP), 438\npprint() function (pprint module), 180\npredefined abstract base classes, 276\nprefix variable, 554, 555\n__prepare__() method (classes), 361\nnew_class() function and, 372\noptional arguments and, 363\nprepare_class() function (types module), 373\nprint() function, 243, 542, 586\nend argument, 144\nline ending, changing, 144–145\nredirecting to files, 144\nseparator character, changing, 144–145\nprivate data/methods\ncaching instances within, 326\nnaming conventions for, 250–251\nprobability distributions (random module), 103\nprocess pools, GIL and, 514, 516\nProcessPoolExecutor class (futures module),\n509–513\nprocess_time() function (time class), 561, 589\nprogram crashes, debugging, 584\nprogram profiling, 587\nproperty attributes (classes)\ndelegating, 287–291\nextending in subclasses, 260–264\nrepetitive definitions for, 382–384\n@property decorator, 330, 346\nproxies, delegating attributes with, 288, 290\npublic-facing services, 457\nput() method (queue module), 491, 495\npwd module, 544\nPyArg_ParseTuple() function (C extensions),\n609\nconverting string encoding with, 650\nPyBuffer_GetBuffer() method (Py_buffer ob‐\nject), 611\nPyBuffer_Release() method (Py_buffer object),\n612\nPyCallable_Check() function (C extensions),\n622\n680 \n| \nIndex",
      "content_length": 2838,
      "extraction_method": "Direct"
    },
    {
      "page_number": 699,
      "chapter": null,
      "content": "PyCapsule_GetPointer() function (C exten‐\nsions), 614\nPyCapsule_Import() function (C extensions),\n618\nPyCapsule_New() function (C extensions), 613\nPyCapsule_SetDestructor() function (C exten‐\nsions), 614\nPyErr_Occurred() function (C extensions), 623\nPyFile_FromFd() function (C extensions), 658–\n659\nPyFloat_AsDouble() function (C extensions),\n623\nPyFloat_Check() function (C extensions), 623\nPyGILState_Ensure() function (C extensions),\n624, 626\nPyGILState_Release() function (C extensions),\n624, 626\nPyIter_Next() function (C extensions), 662–663\nPyObject data type (C extension)\nfilenames, passing with, 657–658\nPyObject_BuildValue function (C extensions),\n623\nPyObject_Call() function (C extensions), 622\nfile-like objects and, 661\nPyObject_GetIter() function (C extensions),\n662–663\nPyParsing, 69, 76\nPyPy, 514\nPySequence_Length() function (C extensions),\n661\npySerial package, 170–171\nPython for Data Analysis (McKinney), 216\nPython Package Index, 434\nPython syntax vs. JSON, 180\nPYTHONPATH environment variable, 409\npytz module, 110–112\nPyUnicode_FromWideChar() function (C ex‐\ntensions)\nC strings, converting to Python objects, 653–\n654\npyvenv command, 432–433\nPy_BEGIN_ALLOW_THREADS (Py objects),\n625\nPy_BuildValue() function (C extensions), 609\nC strings, converting to Python objects, 653–\n654\nPy_DECREF() function (C extensions), 623\nPy_END_ALLOW_THREADS (Py objects), 625\nPy_XDECREF() function (C extensions), 623\nQ\nqsize() method (queue module), 496\nqueue module, 491–496\nqueues\nbounding, 495\nstructures, deque() and, 6\nquote() function (shlex class), 546\nR\nrace conditions, avoiding, 497–500\nraise from statement, 580, 582\nNone modifier, 581\nraise statement, 580, 582\nrandint() function (random module), 102\nrandom module, 102–103\nrandom() function (random module), 103\nre module\ncompile() function, 49\nDOTALL flag, 49\nIGNORECASE flag, 47\npattern matching and, 42–45\npatterns, naming, 67\nscanner() method, 67\nsearch/replace text and, 45–46\nsplit() method, 37–38\nsub() function, 54\nUnicode text and, 52–53\nread() function, 139\nfile-like C objects and, 659–662\nreadability, naming slices for, 18–19\nreadinto() method (files), 147\nreading into mutable buffers with, 152–153\nrecursion, 5\ngetrecursionlimit() function (sys module),\n310\nrecursive descent parsers, 69–78\nlimitations on, 76\nrecvfrom() method (socket module), 445\nrecv_handle() function (reduction module),\n470–475\nrecv_into() function (socket module), 153, 483\nredirection, 539\nregex module, 53\nregister() function (atexit module), 538\nregister_function() method (XML-RPC), 455\nregular expressions\n* (star) operator and, 48\n? (question mark) modifier and, 48\nIndex \n| \n681",
      "content_length": 2646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 700,
      "chapter": null,
      "content": "greedy vs. nongreedy, 47–48\nmatching multiple lines, 48–49\nnewline support in, 48–49\non byte strings, 79\norder of tokens in, 68\npattern matching and, 42–45\nre.split() method and, 38\nregex module for, 53\nshortest match with, 47–48\nstripping characters with, 54\nUnicode and, 52–53\nrelational databases, 195–197\nconnecting to, 196\ncursors, creating, 196\nrelative imports vs. absolute names (modules),\n400\nrelativedelta() function (dateutil module), 105\nreload() function (imp module), 406–407, 431\nremote machines\nloading modules from, 412–428\nXML-RPC, 454–456\nremote procedure calls (RPC), 458–461\nexception handling with, 461\nremove() method (ElementTree module), 193\nreplace() method (date module), 108\nreplace() method (datetime module), 108\n_replace() method (namedtuple object), 31\nreplace() method (str type), 54, 54\nperformance and, 56\nsearch/replace text and, 45\nrepr() function (built-in), 243\nrequest module (urllib module), 438–441\nclient package vs., 440\nrequests package (urllib module)\nreturn values of, 439\nreraising exceptions, 582\nreserved words, clashing with, 251\nresource forks, 548\nresource management, 248\nresource module, 561\nResourceWarning argument (warn() function),\n583\nREST-base interface, 449–453\ntesting, 450\nrestricting CPU time, 561\nresult() method (ProcessPoolExecutor class),\n512\nreverse iteration, 119–120\nreversed() function, 119–120\nrjust() method (str type), 57\nRLock objects (threading module), 498\nround() function, 83–84\nrounding numbers, 83–84\nRSS feeds, parsing, 183\nrstrip() method (str type), 53\nRTS-CTS handshaking, 170\nRuntimeWarning argument (warn() function),\n583\nS\nsample() function (random module), 102\nsanitizing text, 54–57\nscanner() method (re module), 67\nsearch\nfor shortest match with regular expressions,\n47–48\nmatching multiple lines, 48–49\nnlargest()/nsmallest() functions (heapq\nmodule), 7–8\nnoncapture groups, 49\nnormalization of Unicode text and, 51\nvisitor pattern and, 311\nsearch/replace text, 45–46\ncase insensitive, 46–47\nsecurity\nimport statement and, 412\npickle and, 172\nRPCs and, 460\nSSL certificates and, 466\nseed() function (random module), 103\nsegmentation faults, 663–664\nselect() function (event driven I/O), 476\nself-signed certificates (SSL), 468\nSemaphore objects (threading module), 490,\n498\nsend() function (socket module), 483\nsend() method (generators), 316\nsendmsg() method (socket module), 473\nsendto() method (socket module), 445\nsend_handle() function (reduction module),\n470–475\nSequence class (collections module), 283\nsequences\nfiltering elements of, 26–28\nflattening nested, 135–136\niterating over multiple, simultaneously, 129–\n130\nmapping names to elements of, 29–32\n682 \n| \nIndex",
      "content_length": 2669,
      "extraction_method": "Direct"
    },
    {
      "page_number": 701,
      "chapter": null,
      "content": "most frequently occurring item in, 20–21\nremoving duplicates from, 17–18\nunpacking, 1–2\nserial ports, communicating with, 170–171\nserve_forever() method (XML-RPC), 455\nSet class (collections module), 283\n__set__() method (descriptors), 265\nin data models, 280\nsetattr() method, 294\n__setattr__() method (delegation), 290\nsetrlimit() function, 562\nsetsid() function (os module), 537\nsetup.py file, 633, 639\ndistributing packages and, 434\nset_trace() function (pdb module), 586–587\nshell, 546\nshell scripts, 539\nshelling out, 550\nshuffle() function (random module), 102\nshutil module\narchives and, 549\ncopying files/directories with, 547\nsig module, 343–345\nsignature objects, 364–367\nsignature() function (inspect), 343\nSIGXCPU signal, 562\nsimple scripts, 555\nsimplefilter() function (warnings class), 584\nSimpleXMLRPCServer (XML-RPC), 456\nsite-packages directories, 410\nvirtual environments vs., 432–433\nskip() decorator, 574\nskipIf() function (unittest module), 574\nskipping test failures, 573\nskipUnless() function (unittest module), 574\nslice() object, 19\nslices, naming, 18–19\n__slots__ attribute\nclasses with, 32\nmemory management and, 249\nsocket module, 444\nipaddress module and, 448\nsending datagrams with, 445\nsocketpair() function (Unix), 532\nsockets\nlarge arrays, sending/receiving, 483\nsetting options on, 443\nthreads and, 532\nsocketserver module\nimplementing TCP servers with, 441–444\nUDP server, implementing, 445–446\nsorted() function\nand objects without comparison support,\n23–24\nitemgetter() function, 22\nsorting\ndictionaries, 13–15\ndictionaries by common key, 21–23\nfinding largest/smallest N items, 7–8\ngroupby() function and, 25\nitemgetter function (operator module), 22\nobjects without comparison support, 23–24\nsort_keys argument (json.dumps() function),\n182\nsource file vs. config file, 553\nspecial characters, escaping, 66\nsplit() method (re object), 37–38\nsplit() method (str type), 37–38\nSQLAlchemy, 197\nsqlite3 module, 195\nsqrt() function (math module), 592\nssh session, 547\nSSL, 464–470\ncertificate authorities, 468\nself-signed certificates, 468\nssl module, 464–470\nrandom module vs., 103\nStackless Python, 531\nstack_size() function (threading module), 509\nstar expressions\ndiscarding values from, 4\nunpacking iterables and, 3\nstart attribute (slice), 19\nstart() method (Thread class), 486\nstartswith() method (str type), 38\npattern matching with, 42\nstart_response argument (WSGI), 452\nstate\ncapturing with closures, 233–234\ncarrying with callback functions, 232–235\nimplementing for objects/machines, 299–\n305\nof mixins, 297\nthread-specific, storing, 504–505\nstates, generators with extra, 120–121\nstatic methods, applying decorators to, 350–352\nIndex \n| \n683",
      "content_length": 2688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 702,
      "chapter": null,
      "content": "@staticmethod decorator, 330\ndecorating class methods with, 350–352\n__func__ attribute and, 334\nstdout object (sys), changing encoding on, 163\nstep attribute (slice), 19\nstop attribute (slice), 19\nStopIteration exception, 113\nstr type\nconverting to datetime objects, 109–110\ndecode() method, 56\nencode() method, 56\nendswith() method, 38\nformat() function, 57\njoin() method, 58–61\nlower() method, 54\npattern matching with, 42\nreplace() method, 54\nsanitizing text with, 54–57\nsplit() method, 37–38\nstartswith() method, 38\nstripping unwanted characters from, 53–54\ntranslate() method, 55\nupper() method, 54\nstr() function, 243\nStreamRequestHandler (socketserver module),\n442–444\nstring module, 246\nstring templates and code readability, 453\nStringIO object (io module), 566\nStringIO() object (io module), 148–149\nstrings, 37–81\naligning, 57–58\nbad encoding in C/Python extensions, 654–\n657\nC, converting to Python objects, 653–654\ncalling object methods when named in, 305–\n306\ncombining, 58–61\nconcatenating, 58–61\nconverting to dates/times, 109–110\ndatabase API and, 197\nI/O operations, performing on, 148–149\ninterpolating variables in, 61–64\nmatching start/end text of, 38–40\nnull-terminated, passing to C, 644–648\nsplitting on delimiters, 37–38\nstripping unwanted characters from, 53–54\nUnicode, passing to C modules, 648–653\nunpacking, 2\nstrip() method (str type), 53–54\nstrptime() method (datetime module), 109\nstruct module, 199–203\nnested binary records, reading, 203–213\npacking/unpacking integers from byte\nstrings and, 91\nstructure codes for, 201\nvariable-sized binary records, reading, 203–\n213\nstructures (data type), 612–614\nsub() function (re module), 54, 63\nsearch/replace text and, 45\nsubmit() operation (ProcessPoolExecutor class),\n512\nsubn() method (re module), 46\nsubprocess module, 547\nsubsitution callback functions, 46\nsuper() function, 256–260\nclass decorators and, 356\ncoding conventions and, 370\nextending properties in subclasses and, 262\nmixin classes and, 298\nsurrogate encoding, 654–657\nSwig, 604, 627–631\nheaders and, 630\nsymbolic links, 548\nbroken, 549\nSyntaxWarning argument (warn() function),\n583\nsys.arg value, 543\nsys.argv (commaand line arguments), 544\nsys.metapath object, 418, 430\nextending import operations and, 422\nsys.modules dictionary, 421\nsys.path\nadding directories to, 409–411\nsite-packages directories, 410\nsys.path_hooks variable (importers), 420\nsys.path_importer_cache object, 424\nsys.stderr, 540\nsys.stdout, 565–566\nsystem-exiting exceptions, 578\nSystemExit exception, 540, 577–580\nT\ntag attribute (ElementTree module), 185\ntarfile compression format, 550\nTCP servers, 441–444\nevent-driven I/O implementation, 477\n684 \n| \nIndex",
      "content_length": 2678,
      "extraction_method": "Direct"
    },
    {
      "page_number": 703,
      "chapter": null,
      "content": "tempfile module, 167–170\ntemporary files, 167–170\nTemporaryDirectory() method (tempfile mod‐\nule), 169\nTemporaryFile (tempfile module), 168\nterminal, finding size of, 65, 545\ntest failures, 573\nTestCase classes (TestLoader module), 573\ntesting\nouput sent to stdout, 565\noutput, logging to file, 572\nunit tests for exceptions, 570\nTestLoader class (unitest module), 573\ntext attribute (ElementTree module), 185\ntext data\nencoding, 141–144\nreading/writing, 141–144\ntext manipulation, 37–81\naligning strings, 57–58\ncase insensitive search/replace, 46–47\ncombining/concatenating, 58–61\nHTML entities, handling in text, 65–66\ninterpolating variables, 61–64\nmatching start/end of strings, 38–40\nof Unicode with regular expressions, 52–53\non byte strings, 78–81\nparsers, implementing, 69–78\npattern matching, 42–45\nreformatting into columns, 64–65\nsanitizing, 54–57\nsearch/replace, 45–46\nstripping unwanted characters, 53–54\ntokenizing, 66–69\nwildcard matcing, 40–42\nXML entities, handling in text, 65–66\nTextIOWrapper object (io module), 163–165\ntextwrap module, 64–65\nThread class (threading module), 485–488\nthread pools\nGIL and, 508\nqueues and, 506\nthreading module, 488\nCondition object, 489\nEvent object, 488–491\nlocal() method, 504–505\nLock objects, 497–500\nSemaphore objects, 490\nstack_size() function, 509\nThreadingMixIn class (socketserver module),\n297\nThreadingTCPServer objects (socketserver\nmodule), 442\nThreadingUDPServer objects (socketserver\nmodule), 446\nThreadPoolExecutor class (futures module),\n505–509\nthreads\nactor model and, 516–520\nC/Python, mixing, 625–626\ncommunication between, 491–496\ncreating/destroying, 485–488\ndaemonic, 486\ndeadlocks between, 500–503\ngenerators as alternative to, 524–531\nlocking critical sections, 497–500\nnonblocking, supporting with queues, 495\npolling multiple queues, 531–534\npools of, 505–509\npriority queues and, 11\nqueue module and, 491–496\nrace conditions, avoiding, 497–500\nstatus of, finding, 488–491\nstoring state of, 504–505\ntimeouts, supporting with queues, 495\nthrow() method (generators), 531\ntime command, 587\ntime module, 590\ntime zones, 110–112\ncountry_timezones dictionary (pytz mod‐\nule), 112\nUTC time, 111\ntime() function (time class), 561\ntime, operations on, 104–112\nconverting strings for, 109–110\ndate calculations, 106–107\ndate ranges, finding, 107–109\npytz module, 110–112\ntime conversions, 104–105\ntime zones, manipulating, 110–112\nUTC time, 111\ntimedelta object (datetime module), 104, 107–\n109\ntimeit module, 589, 590\ntimeouts, supporting with queues, 495\nTimer class, 561\ntokenizing and DOTALL flag (re module), 49\ntokens streams, filtering, 68\nIndex \n| \n685",
      "content_length": 2631,
      "extraction_method": "Direct"
    },
    {
      "page_number": 704,
      "chapter": null,
      "content": "tostring() function (ElementTree module), 190\ntotal_ordering decorator (functools module),\n321–323\nto_bytes() method (int module), 91\ntraceback, segmentation faults and, 663–664\ntraceback.print_stack() function, 586\ntranslate() method (str type), 54, 55\nnumerical output and, 88\nperformance and, 56\ntransmitting data, 155\ntree structures, memory management of, 317–\n320\ntree traversal, 311, 314\nTTYs, 545, 547\ntuples\nand endswith()/startswith() methods, 39\nas return values, 221\nrelational databases and, 195–197\nunpacking, 2\nTwisted package, 238\ntype checking (data)\nabstract base classes and, 274–276\nforcing with decorator functions, 341–345\ntype systems, 277–283\n%typemap directive (Swig), 631\ntypes module, 370–373\nU\nUDP server, 445–446\nevent-driven I/O implementation, 476\nUDPServer class, 446\numask() function (os module), 537\nunescape() function (xml.sax.saxutils module),\n191\nUnicode, 50–53\nbad encoding in C/Python extensions, 654–\n657\nIGNORECASE flag and matching, 47\nregular expressions and, 52–53\nstrings, passing to C modules, 648–653\nunicodedata module, 55\nuniform() function (random module), 103\nunittest module, 565, 572–573\nUnix, 548\ncommands, 548\nfind utility, 551\nunpack() function (struct module), 201\nbinary data and, 205\nunpacking\narchives, 549\ndiscarding values while, 2\nenumerate() function and, 128\nintegers from byte string, 90–92\niterables into separate variables, 1–2\niterables of arbitrary length, 3–5\nsequences into separate variables, 1–2\nstar expressions and, 3\nunpack_archive() function (shutil module), 549\nunpack_from() method (struct module), 202\nuploading using requests module, 440\nupper() method (str type), 54\nurllib module, 413, 437–441\nurlopen() function (urllib module), 569–570\nimport statements vs., 413\nsending query parameters with, 438\nUserWarning argument (warn() function), 583\nUTC time, 111\nV\nvalidation of data, abstract base classes and,\n274–276\nValueError, 570\nunpacking and, 2\nvalues() method (dictionaries), 14, 16\nvariables\ninterpolating, in strings, 61–64\nunderstand locality of, 592\nvars() method, 62\nvirtual environments, 432–433\nvisitor pattern\nimplementing with recursion, 306–311\nimplementing without recursion, 311–317\nW\nwalk() function (os module), 550\nwarn() function (warning class), 583\nwarning messages, issuing, 583\nwarning module, 584\nwarning() function (logging module), 556\nwatchdog timers, 503\nwchar_t * declarations (C), 648–653\nconverting to Python objects, 653\nweak references and caching instances, 325\nweakref module, 317, 320\nweb browsers, launching, 562\n686 \n| \nIndex",
      "content_length": 2549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 705,
      "chapter": null,
      "content": "web programming, 437–483\nHTTP service clients, 437–441\nREST-base interface, 449–453\nurllib module and, 437–441\nwebbrowser module, 563\nWebOb, 453\nwhile loops vs. iter() function, 138–139\nwildcard matcing, strings, 40–42\nWindows, 548\nC extension modules and, 608\nwith nogil: statement (Cython), 636\nwith statement, 246–248, 561, 566\ncontextmanager decorator and, 385\nLock objects and, 497–500\nSemaphore objects and, 499\nwraparound() decorator (Cython), 641\n__wrapped__ attribute, 332\n@wraps decorator (functools)\nfunction metadata and, 331–333\nunwrapping, 333–334\nWSGI standard, 449–453\nreturn value for apps based on, 452\nX\nXML entities\nhandling in text, 65–66\nreplacing, 66\nXML files\ndictionaries, converting to, 189–191\nextracting data from, 183–186\nmodifying, 191–193\nparsing, 191–193\nincrementally, 186–189\nwith namespaces, 193–195\nrewriting, 191–193\ntags, specifying, 185\nXML-RPC, 454–456\nadding SSL to, 466\ndata types handled by, 455\nxml.etree.ElementTree module (see Element‐\nTree module)\nxml.sax.saxutils module, 191\nXMLNamespaces class, 194\nY\nyield from statement, 135–136\nyield statement, 60, 134\nbehavior in generators, 315\nconcurrency implementations and, 524–531\ngenerators and, 116\nsearch functions and, 6\nZ\nZeroMQ, 457\nzip files as runnable scripts, 407–408\nzip() method (dictionaries), 13–15, 129–130\nzipfile compression format, 550\nIndex \n| \n687",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 706,
      "chapter": null,
      "content": "About the Authors\nDavid Beazley is an independent software developer and book author living in the city\nof Chicago. He primarily works on programming tools, providing custom software\ndevelopment, and teaching practical programming courses for software developers,\nscientists, and engineers. He is best known for his work with the Python programming\nlanguage, for which he has created several open source packages (e.g., Swig and PLY)\nand authored the acclaimed Python Essential Reference. He also has significant experi‐\nence with systems programming in C, C++, and assembly language.\nBrian K. Jones is a system administrator in the department of computer science at\nPrinceton University.\nColophon\nThe animal on the cover of Python Cookbook, Third Edition is a springhaas (Pedetes\ncapensis), also known as a spring hare. Springhaas are not hares at all, but rather the\nonly member of the family Pedetidae in the order Rodentia. They are not marsupials,\nbut they are vaguely kangaroo-like, with small front legs, powerful hind legs designed\nfor hopping, jumping, and leaping, and long, strong, bushy (but not prehensile) tails\nused for balance and as a brace when sitting. They grow to be about 14 to 18 inches\nlong, with tails as long as their bodies, and can weigh approximately eight pounds.\nSpringhaas have rich, glossy, tawny, or golden-reddish coats with long, soft fur and white\nunderbellies. Their heads are disproportionately large, and they have long ears (with a\nflap of skin at the base they can close to prevent sand from getting inside while they are\ndigging) and large, dark brown eyes.\nSpringhaas mate throughout the year and have a gestation period of about 78 to 82 days.\nFemales generally give birth to only one baby (which stays with its mother until it is\napproximately seven weeks old) per litter but have three or four litters each year. Babies\nare born with teeth and are fully furred, with their eyes closed and ears open.\nSpringhaas are terrestrial and well-adapted for digging, and they tend to spend their\ndays in the small networks of their burrows and tunnels. They are nocturnal and pri‐\nmarily herbivorous, feeding on bulbs, roots, grains, and occasionally insects. While they\nare foraging, they move about on all fours, but they are able to move 10 to 25 feet in a\nsingle horizontal leap and are capable of quick getaways when frightened. Although they\nare often seen foraging in groups in the wild, they do not form an organized social unit\nand usually nest alone or in breeding pairs. Springhaas can live up to 15 years in captivity.\nThey are found in Zaire, Kenya, and South Africa, in dry, desert, or semiarid areas, and\nthey are a favorite and important food source in South Africa.\nThe cover image is from Animal Creation: Mammalia. The cover font is Adobe ITC\nGaramond. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐\ndensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 2937,
      "extraction_method": "Direct"
    }
  ]
}