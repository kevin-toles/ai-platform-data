{
  "metadata": {
    "title": "effective-performance-engineering",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 117,
    "conversion_date": "2025-12-19T18:45:20.690061",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "effective-performance-engineering.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Getting Started",
      "start_page": 7,
      "end_page": 26,
      "detection_method": "regex_chapter",
      "content": "rying the mantle of performance reported to operations or infra‐ structure teams. Some still do (and that’s okay).\n\nAs hardware became more commoditized and the adoption of vir‐ tual infrastructure and “the cloud” more prevalent, this function took a backseat to development in an effort to deliver business applications and changes faster. It isn’t uncommon now for teams to have multiple environments to support development, test, produc‐ tion, and failover. While certainly more cost-effective than ever, virtualization has given us the false sense that these environments are free.\n\nThe cloud allows service providers to charge a premium for com‐ puter power in exchange for the promise of higher uptime, higher availability, and virtually unlimited capacity. However, the cloud doesn’t promise an optimal user experience. Applications need to be optimized for the cloud in order to maximize the potential return on investment.\n\nSoftware Over the last 30 years, software has transformed from monolithic to highly distributed, and even the concept of model-view-controller (MVC) has evolved to service-oriented (SOA) and micro-service architectures, all in an effort to reduce the number of points of change or failure, and improve the time-to-value when new func‐ tionality is implemented. Isolating components also allows develop‐ ers to test their discrete behavior, which often leaves end-to-end integrated testing out of scope. The assumption here is that if every component behaves as it should, the entire system should perform well. In an isolated environment this may be true, but there are many factors introduced when you’re building large-scale dis‐ tributed systems that impact performance and the end-user experi‐ ence—factors that may not be directly attributed to software, but should be considered nonetheless, such as network latency, individ‐ ual component failures, and client-side behavior. It is important to build and test application components with all of these factors rep‐ resented in order to optimize around them.\n\nCulture Every organization and group has a mission and vision. While they strive to attain these goals, performance becomes implied or\n\nWhat Is Effective Performance Engineering?\n\n|\n\n3\n\nimplicit. But performance needs to be a part of all decisions around the steps taken to achieve a goal; it forms the basis of how an organi‐ zation will embody Performance Engineering throughout their cul‐ ture to achieve their mission and vision.\n\nWe need to treat performance as a design principle, similar to decid‐ ing whether to build applications using MVC or micro-services architectures, or asking why a new epic (or the relative size of a requirement, in Agile terminology) is important to the business, and how performance with the business/technology/end user will make a difference for all stakeholders. Performance needs to be an over‐ arching requirement from the beginning, or we have already started on the wrong foot.\n\nIn order to build a culture that respects the performance require‐ ments of the organization and our end users, there needs to be some incentive to do so. If it doesn’t come from the top down, then we can take a grassroots approach, but first we need to quantify what per‐ formance means to our business, users, and team. We must under‐ stand the impact and cost of every transaction in the system, and seek to optimize that for improved business success.\n\nThroughout this book are sidebars in which we look at five compa‐ nies and examine how performance is built in to their culture. These organizations are from a variety of industries and verticals, showing the diversity of how a performance culture drives everything a busi‐ ness does, enabling it to deliver amazing results. Here are the five companies we will look at in more detail:\n\nGoogle\n\nWegmans\n\nDreamWorks\n\nSalesforce\n\nApple\n\nThe key takeaway here should be that performance is everyone’s responsibility, not just the developers', the testers', or the operations team’s. It needs to be part of our collective DNA. “Performance First” can be a mantra for every stakeholder.\n\n4\n\n|\n\nChapter 1: Getting Started\n\nGoogle: A Performance Culture Based on 10 Things Within Google’s “About Google” section is an area titled “What we believe,” in which the “Ten things we know to be true” live. There are items written when the company was a few years old, which they continue to revisit to hold each other accountable.\n\nThere is a classic video from Google I/O 2014 where Paul Lewis and Lara Swanson talk about the performance culture at Google. Part of the abstract from this video states, “We can be deliberate about per‐ formance and mobile web, make smart use of performance moni‐ toring tools, and cultivate a social atmosphere of collaboratively improving performance for our mobile users.”\n\nTim Kadlec shared his notes on the video:\n\n34% of U.S. adults use a smartphone as their primary means of Internet access.\n\nMobile networks add a tremendous amount of latency.\n\nWe are not our end users. The new devices and fast net‐ works we use are not necessarily what our users are using.\n\n40% of people abandon a site that takes longer than 2–3 sec‐ onds to load.\n\nPerformance cops (developers or designers who enforce performance) is not sustainable. We need to build a perfor‐ mance culture.\n\nThere is no “I” in performance. Performance culture is a team sport.\n\nThe first step is to gather data. Look at your traffic stats, load stats and render stats to better understand the shape of your site and how visitors are using it.\n\nConduct performance experiments on your site to see the impact of performance on user behavior.\n\nTest across devices to experience what your users are experi‐ encing. Not testing on multiple devices can cost much more than the cost of building a device lab.\n\nAdd performance into your build tools to automatically per‐ form optimizations and build a dashboard of performance metrics over time. Etsy notifies developers whenever one of the metrics exceeds a performance goal.\n\nSurfacing your team’s performance data throughout devel‐ opment will improve their work.\n\nWhat Is Effective Performance Engineering?\n\n|\n\n5\n\nCelebrating performance wins both internally and exter‐ nally will make your team more eager to consider perfor‐ mance in their work.\n\nEven the New York Times published an article, quoting Ben Waber, who has a Ph.D. from M.I.T., is the author of People Analytics (FT Press), and is, at 29, the median age of Google employees. His com‐ pany, Sociometric Solutions in Boston, uses data to assess work‐ place interactions. “Google has really been out front in this field,” he said. “They’ve looked at the data to see how people are collaborat‐ ing. Physical space is the biggest lever to encourage collaboration. And the data are clear that the biggest driver of performance in complex industries like software is serendipitous interaction. For this to happen, you also need to shape a community. That means if you’re stressed, there’s someone to help, to take up the slack. If you’re surrounded by friends, you’re happier, you’re more loyal, you’re more productive. Google looks at this holistically. It’s the antithesis of the old factory model, where people were just cogs in a machine.”\n\nThe end-user experience should be at the forefront of thinking when it comes to performance. The satisfaction of your end users will ulti‐ mately drive business success (or failure), and can be quantified by a number of metrics in described in “Metrics for Success” on page 82. The point is that it shouldn’t matter whether your servers can handle 1,000 hits/second or if CPU usage is below 80%. If the experience of the end user is slow or unreliable, the end result should be consid‐ ered a failure.\n\nBusiness What does performance mean to your business? Aberdeen Group surveyed 160 companies with average annual revenue of over $1 bil‐ lion, finding that a one-second delay in response time caused an 11% decrease in page views, a 7% decrease in “conversions,” and a 16% decrease in customer satisfaction.\n\nGoogle conducted experiments on two different page designs, one with 10 results per page and another with 30. The larger design page took a few hundred milliseconds longer to load, reducing search usage by 20%. Traffic at Google is correlated to click-through rates, and click-through rates are correlated with revenue, so the 20% reduction in traffic would have led to a 20% reduction in revenue.\n\n6\n\n|\n\nChapter 1: Getting Started\n\nReducing Google Maps’ 100-kilobyte page weight by almost a third increased traffic by over one-third.\n\nThe correlation between response time and revenue is not restricted to Google. A former employee of Amazon.com discovered that 100 milliseconds of delay reduced revenues by 1%. Whether you are sell‐ ing goods online or providing access to healthcare registration for citizens, there is a direct correlation between the performance of your applications and the success of your business.\n\nWhy Is Effective Performance Engineering Necessary? Over the years, we have all lived with the mantras “Do more with less,” and “Faster, cheaper, and better.” While some organizations have survived, many have not. Now we are faced with a different question: “How do we deliver highest quality, highest value, at the highest speed?”\n\nIn addition, organizations must focus on other key elements, like revenue, competitive advantage, customers, and brand value. Practi‐ ces like Agile and DevOps have evolved and become more widely adopted. Regardless of the lifecycle, Effective Performance Engi‐ neering practices are enabling organizations to accomplish the pre‐ ceding focus areas and goals for their end users. We will touch on each of these focus points next.\n\nRevenue Effective Performance Engineering enables organizations to increase revenue in several ways. In a recent survey, 68% of respondents expected Performance Engineering practices to increase revenues by “ensuring the system can process transactions within the requisite time frame.” While all survey participants were generally in agree‐ ment regarding the tasks required, they had different expectations.\n\nIn addition to the increase in revenue, another result is reduced cost; 62% of the Performance Engineering–focused respondents felt that Performance Engineering practices should serve to avoid “unneces‐ sary hardware acquisition costs.”\n\nBy building in performance, organizations can start optimizing applications before the first piece of code is even written, or before\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n7\n\nthat new capability lights up the hardware, thereby improving the end-user experience and proactively focusing on the business objectives. Cost reductions can be dramatic. As the result of reduced performance-related production incidents, organizations can often handle 30–50 percent more transactions with the same (or less) infrastructure.\n\nSavings can quickly multiply—fewer machines means reduced capi‐ tal expense as the business scales, including lower operational expenses related to power and cooling. Fewer resources are then needed to support the infrastructure. Savings also accumulate through the reduction in performance-related production incidents that need to be managed, which reduces the opportunity cost of putting your most valuable people to work on new features and functions for the end user and your business.\n\nA catalog company, for example, might focus on total revenue from a specific product line or service, then track it after making specific changes to a promotional website to see if revenue increases. In another scenario, the value of the mobile application might be judged by the number of registered users and the frequency with which they access products or services. And, as the backend infra‐ structure (including web servers, middle tiers, and databases) takes on more roles inside the corporation, the metrics that track the per‐ formance of the larger organizational goals better reflect the quality of the supporting technology.\n\nThe rise in importance of Performance Engineering has been driven by practical concerns. At least 50% of respondents admitted that slowdowns and outages were discouraging customers and frustrat‐ ing employees. Many characterized the problems as “repeated,” and said they were often caused by large spikes in traffic that weren’t anticipated when the applications were built.\n\nThe consequences are serious. The average firm that responded to the survey said that a major outage could cost between $100,000 and $500,000 in lost revenue per hour. Some of the larger companies with more than 10,000 employees said they could lose $5 million an hour from website or core system outages.\n\nWhen organizations contemplated the scope and catastrophic range of these failures, they recognized that the traditional development process just wasn’t ready to build a system with adequate provisions for surviving these kinds of issues. Transforming the organization\n\n8\n\n|\n\nChapter 1: Getting Started\n\nand focusing individuals and teams on performance means empow‐ ering them with capabilities to anticipate problems and solve them before they occur. And when problems emerge after deployment, it means giving the team the ability to control failure and mitigate risk.\n\nThis is one of the reasons why a greater understanding of Perfor‐ mance Engineering as a cross-discipline, intra-business mindset is so essential. Revenue is often the first and foremost measurement of why something needs to change in any organization, but it is not always used as a measurement of Effective Performance Engineering —even though it’s often seen as the key differentiator in gaining a competitive advantage related to delivering a product or service faster and better than anyone else in the market.\n\nSalesforce.com: Entrepreneurial, Independent, and Results-Oriented Salesforce provides quick insight to their culture on their website and states the following:\n\nTop talent across the globe comes to salesforce.com for the “change-the-world” mentality; the opportunity to excel in a fast- paced atmosphere; and the chance to be surrounded by peers and leaders that inspire, motivate, and innovate. Salesforce.com offers a unique career opportunity, regardless of what you do or where you do it.\n\nOur employees are entrepreneurial, independent, and results- oriented. If you like working hard in a place where hard work is rewarded, contributing to projects where contributions count, and growing in a company where growth knows no boundaries, salesforce.com is perfect place to do the best work of your career.\n\nThe beneficiaries of our hard work extend into our communities through the Salesforce.com Foundation. Employees are encour‐ aged to give back to the community and get four hours per month, or six full days per year, off with pay to spend on volun‐ teer activities.\n\nLooking to mainstream media, we quickly find Salesforce appears often in the “Fortune Top 100 Places to Work.” When you look at the Employee Ratings, it is easy to see how the right culture can benefit an organization. Adopting Effective Performance Engineer‐ ing capabilities supports many of these metrics.\n\nFigure 1-1 shows Salesforce employee ratings, demonstrating that 93% of employees say their workplace is “great.”\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n9\n\nFigure 1-1. Salesforce employee ratings\n\nOn the Salesforce careers page, the company showcases a hashtag (#dreamjob) that says a lot for its culture. Going a bit further, they add, “A #dreamjob starts with passionate people who do work that matters, win as a team, and celebrate success together. It ends with knowing that together, we are the force innovating the future of business for customers. We are living our #dreamjob. Join us!”\n\nCompetitive Advantage There’s an obvious business reason to focus on the needs of end users. If they’re your customers, they’re consuming your products and/or services and possibly paying you for results. If you’re a provider in a technology chain that defines a complex solution of some sort, you’re still dependent on the satisfaction of users, how‐ ever indirectly.\n\nBut it wasn’t so long ago—say, 20 years—that users of computing systems were expected to live with high latencies, downtimes, and bug workarounds that were common in business systems. That’s because users were employees and had to put up with the buggy, slow, or unpredictable systems their employers provided. Or they were users of a standalone application, such as WordPerfect for writ‐ ing, or Lotus 1-2-3 for spreadsheets.\n\nThat’s right, we’re talking about the pre-Internet age, when very few users imagined doing actual business transactions online. But once e-commerce became a buzzword, and soon simply a word, users\n\n10\n\n|\n\nChapter 1: Getting Started\n\nstopped being just users. They became customers, and it was obvious the best web-based experiences for customers would lead to repeat business.\n\nFast-forward to today’s web-based and mobile business climate, where:\n\nUser experience (UX) is a red-hot topic.\n\nCommoditization of virtually everything is a fact of life.\n\nSocial media is the engine that can quickly sink an online retailer, transportation provider, and so on if the UX is poor— no matter the reason.\n\nThese days, performance is king, and your online or mobile custom‐ ers can either be adoring subjects or a mob of thousands, with the social-media equivalent of torches and pitchforks, at your kingdom’s gate. Or they’ll just go on to the next provider, leaving you alone in the dark ages.\n\nThrill your end users (especially customers) by outperforming their expectations, and you’ll get to keep doing more of whatever made them come to you in the first place, while making your company brand champions.\n\nDreamWorks Animation: Inspiring Audiences to Dream and Laugh Together Going to the DreamWorks website to learn about the company’s culture was an experience in and of itself. We were surprised to see five areas of focus in the “Our Culture” section of the site:\n\n1. Our Culture\n\n2. Education\n\n3. Campus Activities\n\n4. Well-Being\n\n5. Environment\n\nWithin the “Our Culture” area of focus it states, “At the heart of DreamWorks Animation is the desire to tell great stories that inspire audiences to dream and laugh together, pushing the bound‐ aries of both creativity and technology. To do this effectively, DreamWorks is dedicated to providing the best work environment\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n11\n\npossible for the company’s artists, engineers and everyone in between, so that they enjoy creating their work just as much as audiences enjoy watching it.”\n\nGoing to the Indeed website revealed a bit more about the culture at DreamWorks, as nearly all reviews were 5 stars, nearly unheard of at other companies in this industry.\n\nAnother article from ChicagoCreativeSpace’s Max Chopovsky con‐ cludes by stating, “we have found ways to combine these spaces and make them all onsite so that collaboration, creativity and efficiency are maximized.”\n\nMany companies recognize that it’s not enough to call something a glitch, cross their virtual fingers, and hope it never happens again. They need to track their computer systems’ online status; guarantee that they’re responding to customers, partners, and employees; and measure whether they’re delivering the information promptly so no one is clicking, tapping, pounding the return key, and wondering whether they should just go somewhere else.\n\nAs corporate leaders realize the importance of their online and mobile presence and start to measure just how much business comes in through all of their channels, they’re reshaping organiza‐ tions to keep information flowing quickly and accurately. This responsibility is gaining traction under the umbrella of Performance Engineering.\n\nProponents of this new vision believe that enterprises must build a performance-focused culture throughout their organizations—one that measures the experience of end users, both internal and exter‐ nal—and deliver a software and hardware experience that results in efficient performance for these users. Performance must be priori‐ tized from the beginning of the process and be watched vigilantly after the code is deployed.\n\nMore efficient, faster systems leave employees less frustrated, some‐ thing that almost certainly translates to customer satisfaction as well. The rest of the answers indicate a general awareness that the performance and throughput of computer systems are directly\n\n12\n\n|\n\nChapter 1: Getting Started\n\nrelated to competitive advantage and the organization’s ability to retain and attract customers.\n\nIs Performance Engineering DevOps? Putting an end to these academic distinctions, along with the day- to-day finger-pointing between Development and Operations teams (not to mention Technology versus Business teams), is part and parcel what the DevOps movement is all about. It involves an internal cultural shift toward the end user of the system(s) who wants an update, a bug fix, and some assurance the system has the full backing of the organization the customer is depending on.\n\nThere is a relationship between Effective Performance Engineering and DevOps. This relationship is one that DevOps will deliver higher value at higher quality and higher speed if the capabilities of Effective Performance Engineering are enabling the DevOps practices.\n\nThe competitive advantage—driven by faster time-to-market, with higher quality through Effective Performance Engineering—is a basis for how a business attracts and retains customers.\n\nCustomers: Acquisition and Retention Acquiring and retaining customers should be the driving force in an organization, no matter the size. As we have mentioned, Perfor‐ mance Engineering plays a considerable role in enabling your orga‐ nization to succeed in the marketplace, and success is defined in many ways for different organizations, and is not exclusive by industry or organization or product/service. But in general, success is defined by a few factors (and many times a combination), includ‐ ing: revenue, competitive advantage, brand value, and customers (acquisition and retention).\n\nWhen your customers look across the market to discover where they can get a specific product or services, what do they see, and how do they evaluate it? Today, much of this process is accom‐ plished in a number of near real-time and openly accessible formats, including app store reviews, feedback on websites, and even product- or service-specific feedback on your sites.\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n13\n\nA myriad of feedback and comments are available to you regarding how your customers perceive your products and services, and what you need to focus on for your next release to deliver the features and functions they are seeking. The question becomes, “How are you capturing and what are you doing with this data today?” Using effec‐ tive Performance Engineering practices, you can leverage a continu‐ ous feedback loop from production to market, provide the highest value to your customer, and increase your acquisition and retention.\n\nDefining and knowing your customers is crucial. A recent info‐ graphic (cropped) posted by Rigor.com titled “Why Performance Matters to Your Bottom Line” illustrates how impatient online shop‐ pers are, and the impact of a one-second delay (see Figure 1-2).\n\nFigure 1-2. Why performance matters to your bottom line\n\nThe key point to highlight from this infographic is that online shop‐ pers are impatient, and the cost of a one-second delay is substantial; understanding this, and doing something about it for your business, is why we are focusing on building a Performance Engineering cul‐ ture and adopting practices to improve quickly.\n\nOf course, this leads to another important consideration: metrics. Thinking about acquisition and retention of customers involves measuring many key elements, including the feedback in the mar‐ ketplace for your products and/or services. For many companies,\n\n14\n\n|\n\nChapter 1: Getting Started\n\nthis becomes a great starting point and one they leverage to engineer into their practices.\n\nAs organizations consider and implement metrics, they’re driven by the realization that computer system performance and throughput are directly related to competitive advantage and their ability to retain and attract customers. This focus on the customer is helpful both for acquiring new customers and retaining existing ones.\n\nWegmans Believes in Caring On the Wegmans website, under “Our Values & Culture” is the fol‐ lowing text:\n\nBecause we’re all part of the extended Wegmans family, we share a common set of values—we call them “Who We Are.” And by living these values—handed down to Danny, Colleen and Nicole from Robert Wegman—we really have created something special: a great place to work where caring about and respecting our peo‐ ple is the priority.\n\nWegmans believes in:\n\nCaring We care about the well-being and success of every person.\n\nHigh Standards High standards are a way of life. We pursue excellence in every‐ thing we do.\n\nMaking A Difference We make a difference in every community we serve.\n\nRespect We respect and listen to our people.\n\nEmpowerment We empower our people to make decisions that improve their work and benefit our customers and our company\n\nIn an article from the Harvard Business Review titled “Six Compo‐ nents of Culture,” Wegmans is highlighted in component number three, stating:\n\n3. Practices: Of course, values are of little importance unless they are enshrined in a company’s practices. If an organization pro‐ fesses, “people are our greatest asset,” it should also be ready to invest in people in visible ways. Wegmans, for example, heralds values like “caring” and “respect,” promising prospects “a job [they’ll] love.” And it follows through in its company practices, ranked by Fortune as the fifth-best company to work for. Simi‐\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n15\n\nlarly, if an organization values “flat” hierarchy, it must encourage more junior team members to dissent in discussions without fear or negative repercussions. And whatever an organization’s values, they must be reinforced in review criteria and promotion poli‐ cies, and baked into the operating principles of daily life in the firm.\n\nForbes also published an article, “Focus on Your Company Culture, and Earnings Will Follow,” in which Wegmans was also highlighted in the “100 best companies to work for in the U.S.” The author writes, “Many organizations cling to ‘what’s always been done,’ which constantly pushes against innovation; as a result, earnings and other key performance metrics begin to lag. Those dips can make managers do some interesting things in an effort to restore their companies to greatness. As executives become laser-focused on chasing earnings, they may lose sight of the bigger picture. They become focused on treating the surface-level symptoms, never diag‐ nosing the deeper cultural dilemma.”\n\nBrand Value In many cases, businesses continue to invest in both a capability and a culture as they work to build in the practices of Performance Engi‐ neering. These practices enable them to grow faster and become more stable. Done well, Performance Engineering avoids the large- scale catastrophes like the one that hit Best Buy in 2014 (see For‐ tune, CNBC, CNN Money) as well as the soft failures that come when slow services frustrate employees and turn away customers. The big failures may get all the media attention, but it’s the gradual slowdowns that can be even more damaging, as they erode revenue and brand value. By the way, these are quick to be picked up in the media and amplified via social media channels, broadening and accelerating the damage to your brand.\n\nBrand value is often something measured and tied to the stakehold‐ ers within your marketing organization(s). However, as we see the culture of performance and practices of Effective Performance Engi‐ neering, this is becoming relevant to all stakeholders (and for all the right reasons).\n\nKeep your end users (especially customers) thrilled by outperform‐ ing their expectations and alternatives, and you’ll get to keep doing more of whatever made them come to you in the first place, while making them brand champions.\n\n16\n\n|\n\nChapter 1: Getting Started\n\nAll survey respondents viewed the downside of poor performance in much the same way. Together, 66% agreed that poor performance could hurt brand loyalty and brand value. As more users interact with an organization through online and mobile channels, it only makes sense that performance would reflect directly on the brand.\n\nIn conclusion, invest in Effective Performance Engineering, or risk costly failures. You’ll be rewarded with smooth rollouts, lower over‐ head, and higher revenue. What if you don’t invest? Expect damaged brands, lower revenue, and lower employee morale.\n\nApple Creates Wonder That Revolutionizes Entire Industries\n\nApple’s website reads:\n\nThe people here at Apple don’t just create products—they create the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innova‐ tion that runs through everything we do, from amazing technol‐ ogy to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it.\n\nFast Company published an article titled “Tim Cook on Apple’s Future: Everything Can Change Except Values.” In this article, sev‐ eral key elements of Apple and their culture are investigated, show‐ ing how deeply important values were to Steve Jobs and are now to Tim Cook.\n\nTim Cook said of Steve Jobs, “It was his selection of people that hel‐ ped propel the culture. You hear these stories of him walking down a hallway and going crazy over something he sees, and yeah, those things happened. But extending that story to imagine that he did everything at Apple is selling him way short. What he did more than anything was build a culture and pick a great team, that would then pick another great team that would then pick another team, and so on.”\n\nTim Cook goes on to say, “We’ve turned up the volume on collabo‐ ration because it’s so clear that in order for us to be incredibly suc‐ cessful we have to be the best collaborators in the world. The magic of Apple, from a product point of view, happens at this intersection of hardware, software, and services. It’s that intersection. Without collaboration, you get a Windows product. There’s a company that pumps out an operating system, another that does some hardware, and yet another that does something else. That’s what’s now hap‐\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n17\n\npening in Android land. Put it all together and it doesn’t score high on the user experience.”\n\nAll of these elements—values, people, and collaboration—are at the core of Effective Performance Engineering.\n\nThe Harvard Business Review published an article titled “The Defining Elements of a Winning Culture,” which shows how a com‐ pany’s culture can have a powerful impact on its performance.\n\nHBR found a set of seven “performance attributes” that enable the best-performing companies. Here is their list:\n\n1. Honest. There is high integrity in all interactions, with employees, customers, suppliers, and other stakeholders;\n\n2. Performance-focused. Rewards, development, and other talent-management practices are in sync with the underly‐ ing drivers of performance;\n\n3. Accountable and owner-like. Roles, responsibilities, and authority all reinforce ownership over work and results;\n\n4. Collaborative. There’s a recognition that the best ideas come from the exchange and sharing of ideas between individuals and teams;\n\n5. Agile and adaptive. The organization is able to turn on a dime when necessary and adapt to changes in the external environment;\n\n6. Innovative. Employees push the envelope in terms of new ways of thinking; and\n\n7. Oriented toward winning. There is strong ambition focused on objective measures of success, either versus the competi‐ tion or against some absolute standard of excellence.\n\nThe article mentions Apple and Steve Jobs specifically: “Steve Jobs builds a challenging culture at Apple —one where ‘reality is sus‐ pended’ and ‘anything is possible’—and the company becomes the most valuable on the planet.”\n\nFocusing on Business Need The business needs to ensure that revenue, competitive advantage, customer acquisition and retention, and brand goals are achieved. Doing so means expanding products and service offerings and/or businesses either through organic or acquisition approaches, all of\n\n18\n\n|\n\nChapter 1: Getting Started\n\nwhich may depend on an existing or new platform(s), so end users can consume products and services without interruption when, where, and how they want.\n\nAs a result, businesses should always be seeking to adopt Effective Performance Engineering capabilities. How they choose to do so must be led by clear and visible objectives, key metrics and measure‐ ments, and communications. If they support and participate, they will see positive results from the previously defined goals and objec‐ tives—if not, negative results will be sure to follow in one or many of these areas.\n\nFocusing on Business Need\n\n|\n\n19\n\nCHAPTER 2 Overview of Performance Engineering\n\nNow that we understand where to start with Performance Engineer‐ ing, having defined both the term itself and also why it is important, let’s dive into more specifics of how it is applied throughout the lifecycle. As we walk through each commonly defined step within a lifecycle, we will explore where and how companies leverage these capabilities to deliver the results we identified in the last chapter.\n\nPerformance Engineering Throughout the Lifecycle As you start to incorporate Performance Engineering capabilities into your lifecycle, it is important to understand what some of these areas are, and put these into context with some typical flow nomen‐ clature. In the following sections we define each of these key ele‐ ments with specifics—what, why, and how—so you have a more complete understanding of how to add Performance Engineering throughout the lifecycle.\n\nOne of the challenges in building Effective Performance Engineer‐ ing or a performance-first culture is defining who does what, when, and how. This kind of organizational alignment and agreement is as important as the daily scrum meeting of an Agile team. If everyone agrees that performance is important, but not on how to address it, then nothing is done about it.\n\n21\n\nFirst, we need to agree that while everyone is responsible for the performance of our business applications, someone needs to be accountable. One person, or team in a larger organization, needs to make sure everyone is playing along in order to meet our objectives. It could be the Scrum Master, Engineering Team Lead, QA Lead, or a separate role dedicated to performance.\n\nSome organizations have even created a “Chief Performance Offi‐ cer” role to bring visibility and accountability to the position, along with information radiators to show performance results as visual and accountable feedback throughout the process. Once that person or group is identified, it is important to include them in any standup meetings or architectural discussions, in order to raise any red flags early in design and avoid costly rework at later stages.\n\nCulture needs to be built into an organization by design. There are several solid, cross-industry examples included in a Staples Tech Article; we’ll examine these more closely and investigate how their culture is focused on Performance Engineering, and how they have built in these capabilities.\n\nThe following sections cover the what, why, and how of Effective Performance Engineering capabilities, so that as you look at this cul‐ ture and the role(s) of adoption, you can start to understand more specifically how it might apply within your own organization.\n\nRequirements Features and functions, along with capabilities both for new applica‐ tions and maintaining legacy, all fall into this section, where we will highlight some specific elements for consideration as you are adopt‐ ing Effective Performance Engineering practices. Take a look at these items and understand the what, why, and how of each, so as you begin to transform, you can ensure consideration for each spe‐ cific element is being considered and adopted.\n\nComplete Stories In defining the changes we are going to implement, complete and understood requirements or stories are a solid starting point. Mike Cohn, founder of Mountain Goat Software, and founding member of the Scrum Alliance and Agile Alliance, has created a user story template, shown in Table 2-1.\n\n22\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nTable 2-1. Mike Cohn’s user story template\n\nAs a/an moderator\n\nmoderator\n\nestimator\n\nmoderator\n\nI want to... create a new game by entering a name and an optional description invite estimators by giving them a URL where they can access the game join a game by entering my name on the page I received the URL for start a round by entering an item in a single multiline text field see the item we’re estimating\n\nSo that... I can start inviting estimators\n\nwe can start the game\n\nI can participate\n\nwe can estimate it\n\nestimator\n\nestimator\n\nmoderator\n\nmoderator\n\nsee all items we will try to estimate this session\n\nsee all items we try to estimate this session\n\nselect an item to be estimated or re-estimated\n\nI know what I’m giving an estimate for I have a feel for the sizes of the various items I can answer questions about the current story such as “does this include ___?” the team sees that item and can estimate it\n\nUsing a thoughtful approach to stories has many benefits. With an incomplete definition of requirements and features, an individual or team is left to define what they believe the end user wants and needs. If Performance Engineering is not considered as part of a complete story, a technical component or architecture could vary considera‐ bly, resulting in underperforming or unutilized capabilities.\n\nOrganizations continue to evolve the way they create complete sto‐ ries using models like Mike Cohn’s user story template, and also by adopting prototyping or wireframe capabilities to accelerate the delivery of high-quality results to the end users.\n\nBreakdown of Epic to Tasks with Acceptance Criteria It’s important to plan for the size, relationship, and priority of requirements and features, along with building in performance that shows the relationship to epic to task, in order to enable teams to collaborate and consider Performance Engineering needs and capa‐ bilities from the start.\n\nFigure 2-1 shows the relationship and breakdown from epic to tasks, along with a story card with Story on front and Acceptance Test Cri‐ teria on the back.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n23",
      "page_number": 7
    },
    {
      "number": 2,
      "title": "Overview of Performance Engineering",
      "start_page": 27,
      "end_page": 71,
      "detection_method": "regex_chapter",
      "content": "Figure 2-1. Breakdown of Epic to Story, and example\n\nIn many cases, we observe a trend of more myopic or task-level views into stories. This practice limits the view and consideration across tasks, and limits the ability to build higher performing plat‐ forms, especially in the much-distributed and shared complex appli‐ cations and systems we operate within today.\n\nIn Figure 2-1, you can see how the story is on the front of the card (typically only used portion) and on the back are the acceptance criteria, which is where you include Performance Engineering con‐ siderations. An example from a recent story about a login page is, “Perform with 180,000 people logging in per hour with 50% on varying mobile network conditions from 5 major US and 2 major International locations with the remaining from good WiFi and LAN connections with a maximum transaction response time of <5 seconds.”\n\nDoneness Criteria A proven practice among organizations is defining a shared under‐ standing and standard for what “done” means. Creating a “Feature Doneness” definition for all teams is critical, and Performance Engi‐ neering considerations need to be built in.\n\nThe standard for speed and quality must be a known and shared value across individuals, teams, business units, and organizations. Perhaps there are only 5–10 core criteria defined and agreed upon at a complete organization level, but this is the delivered standard.\n\n24\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nThis will enable a level of doneness in order to meet shared expecta‐ tions and deliver within the agreed-upon time and quality.\n\nA recent example of how a Dev/Test organization put together their doneness criteria is outlined below. Here, the organization had only eight criteria, of which the italicized one builds performance into their process:\n\nThe code is included in the proper branch in the source code control system.\n\nThe code compiles from a clean checkout without errors using production branch [proposed: and is part of the AHP Build Life which is finally tested in QA].\n\nThe code is appropriately covered with unit tests and all tests are passing using the production branch.\n\nThe code has been peer reviewed by another developer.\n\nDatabase changes have been reviewed and approved by a DBA.\n\nThe code has passed integration, regression, stress, and load test‐ ing.\n\nApplication Support is aware of the backlog item and the system impacts.\n\nDeployment and rollback instructions are defined, tested, and documented.\n\nUntil these are all true, the feature is simply unfinished inventory.\n\nFunctional Within the functional tests you run today, think about how to lever‐ age these (typically single-user) tests to get performance results and share them with all stakeholders. This can take place at any stage throughout your lifecycle from unit to production, and the value of these results benefits the team throughout the lifecycle. A specific example could be the way performance is built in to your automated functional unit tests, as demonstrated in Figure 2-2, which illustrates how long a specific set of automated functional unit tests took to run and set a pass/fail threshold within Jenkins.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n25\n\nThe reasons why this is important are numerous. One of the com‐ mon areas we focus on is speed with quality. As you are increasing the number of builds per hour/day/week/month, these Performance Engineering practices enable early and frequent feedback on quality. The incremental value delivered with every build enables quick feedback loops and opportunities for DevTest teams to deliver higher quality faster by building in performance within their auto‐ mated functional tests.\n\nFigure 2-2. Functional automated test results shown in Jenkins\n\nThe practice of gaining performance results from functional tests during the Performance Engineering adoption should be carried out throughout an organization. Just focusing on the automated func‐ tional tests immediately post-build, we can see this information ena‐ bles a build-over-build continuous comparison, allowing the business to see trends over time. Adding response time along with percentage of errors variables is another way to quickly get a lot more value from functional tests you are already running with little to no effort, effectively driving performance results with immediate feed‐ back.\n\nFigure 2-3 shows performance trend results from JMeter that illus‐ trate the history of performance from build to build.\n\n26\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nFigure 2-3. Performance trend results\n\nSecurity Security is at the forefront of many organizations’ priorities, and is addressed by a dedicated CISO (chief information security officer) and their team of professionals, often in an isolated group across the technology stack. Building performance into the security practices, and vice versa, becomes quite critical, enabling organizations not only to get actual results they may not have previously been able to achieve, but also to provide those results to a broader group of stakeholders earlier and more often in order to quickly mitigate vulnerabilities.\n\nFixing security vulnerabilities earlier in the lifecycle by adoptiong Performance Engineering practices reduces risk for the business and the end users. Providing the DevTest team with immediate, automa‐ ted insight enables the organization to deliver more, faster, with higher quality. This advantage accrues when performance scenarios are run and security results come in from the tests, as well as when security has the opportunity to run their tests under more accurate, repeatable conditions.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n27\n\nThere are two immediate ways that we can think about security and Performance Engineering practices. First is the ability to provide security results within the performance results throughout the life‐ cycle. Second is enabling performance conditions, for example, in network conditions (latency, bandwidth, packet loss, jitter), so when security tests are executed the results delivered are actual. This detail is often overlooked in security tests in two specific vulnerability areas: cross-site scripting and SQL injection. These have become popular due to mobile network conditions (needing to hold connec‐ tions open longer due to higher latency conditions, and dynamic sessions not working).\n\nYou can capture security risks and vulnerabilities by providing automated and prioritized results, captured in a flat file and stored with the automated build results, so they can be remediated and results shown as a trend over time for the given code base/release candidate. Figure 2-4 shows how to get security results while you are running your automated tests using Watcher within Fiddler2.\n\nFigure 2-4. Security test results using Watcher in Fiddler2\n\n28\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nPerformance Too often we observe organizations treating performance as a checkbox. If this is the area where you need to adopt Performance Engineering practices, there are several ways to start. With Perfor‐ mance Engineering practices the performance team(s) no longer has to accept running the same scenario and watching the same results and reporting pass/fail, but instead can dive a lot deeper into what is going on along with how it is working in production, so they can provide more relevant and actual results to help optimize system performance.\n\nHere are four key starting points in adopting Performance Engi‐ neering practices within performance, which we will identify now and define a bit more later:\n\nProduction incidents\n\nEnable you to see what is and is not working, along with creat‐ ing some business value on the impact when something is not working; allow teams to see trends on where the trouble areas are, and show the value added as improvements are reflected in a reduced number of incidents.\n\nInstrumenting\n\nEnables additional and broader insight into what is happening within an application and across a system, in order to visualize and simplify finding the “needle in the haystack”; helps with finding not just the first issue, but also a path of areas needing attention.\n\nVirtualization\n\nEnables the re-creation of virtual users, services, networks, and data. Using virtualization technologies, you can very quickly re- create—with a high level of accuracy and low cost—the produc‐ tion environments anywhere in the world at any time for any period of time.\n\nMonitoring\n\nGives a visual performance dashboard of actual results in pro‐ duction, and when also used to observe the pre-production environments, provides teams assurance that they are building solutions that will scale and be resilient, especially as they are deployed to the end users.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n29\n\nFigure 2-5 shows how monitoring can be leveraged throughout all environments to build in performance and provide results to all stakeholders.\n\nFigure 2-5. Using monitoring to increase performance\n\nWe could dive into any of these topics and quickly observe the value of adopting Performance Engineering practices in performance. Looking a little closer at monitoring, we start to see a number of dif‐ ferent areas where these capabilities used both in production and pre-production environments can greatly enable the adoption of Performance Engineering practices within the performance team(s) and beyond:\n\nSynthetic monitoring provides the ability to simulate applica‐ tion performance many ways, ensuring you can deliver consis‐ tent and predictable performance to your end users.\n\nReal user monitoring enables real-time application performance of all users all the time, allowing you to automatically discover underlying infrastructure and classify user actions.\n\nMobile app monitoring provides insight into the performance, stability, and resource usage of mobile apps. It gives you insight into what the user does, where they exited your app, and why.\n\nDeep-dive diagnostics allows you to drill into your backend performance to quickly isolate and diagnose bottlenecks to resolve issues.\n\n30\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nTransaction monitoring provides visualized process flows over the entire application and infrastructure environment so you can assess key business metrics.\n\nTransaction management enables you to track and confirm the path, step-by-step timing, and content or payload of each trans‐ action, so you can understand the impact of critical transactions on business outcomes.\n\nFigure 2-6 illustrates how Hewlett-Packard Enterprise Business Ser‐ vice Management (HPE BSM) can be used throughout your build life along with Service-Level Agreements (SLAs) to quickly show status across a variety of areas, leveraging these capabilities in both pre-production and production environments, and enabling the performance team(s) to adopt Performance Engineering practices.\n\nFigure 2-6. Using HPE BSM to monitor status\n\nUsability Usability for the purpose of this discussion is defined as the ease of use and learnability of a human-made object. Looking at the corre‐ lation of performance and facets of usability is a way to begin adopt‐ ing Performance Engineering in this increasingly important and vested area.\n\nIt is important to assess your position in the market relative to your competition, and to measure usability as a trend across releases over time. Net Promoter Score (NPS) and User Sentiment Score (USS) are common elements used to measure usability. As we look at usa‐\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n31\n\nbility and Performance Engineering, it is important to note this practice should be applied throughout the lifecycle, not just at a sin‐ gle point late on the way to production deployment.\n\nThe NPS and USS Scoring Systems The Net Promoter Score (NPS) system was developed by Fred Reichheld, Bain & Company, and Satmetrix. The NPS is a way of measuring user satisfaction. The methodology relies on the so- called “ultimate question,” which measures overall satisfaction and, even more importantly, loyalty to the service or provider. A second question establishes the root causes for the given score.\n\nThe User Sentiment Score (USS), developed by HPE, is created by a sentiment analysis service that scans end-user comments on your applications, categorizes them, and provides a weighted score. You can go to http://apppulse.io/#/ and type in “Target” to see example results.\n\nCorrelation between performance and user satisfaction with usabil‐ ity is high. So, if you can make your interface and design easier to use, people will like it more—this seems obvious, but not everyone takes the time to implement this capability. There are several reasons interest in usability and performance has increased, including increases in competitive advantage and customer acquisition and retention. In addition, with the brand damage that can be inflicted through app store reviews and social media, getting usability right is now a mainstream challenge.\n\nThere are a variety of ways organizations can implement Perfor‐ mance Engineering within usability. A few key metrics to consider in this area include:\n\nThe time it takes for an end user to complete a task. So, if an end user is seeking to make a purchase, what was the amount of time it took for that task end to end, and how long did each step take, and why?\n\nWorkflow simplicity reflects the experience of the end user in being able to easily learn and navigate the interface to perform the task they need to complete.\n\nSuccess rate is a measurement of what percentage of end users were able to complete the task they set out to achieve.\n\n32\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nSubjective feedback tends to be a narrative of how users felt about the overall experience.\n\nError rate is the percentage of time an end user received an error or failure while using the interface to perform a task.\n\nHeat map and eye tracking are another way to capture seem‐ ingly subjective feedback in an objective way, knowing where your end users are spending their time looking and clicking or tapping on the interface to perform a task.\n\nFigure 2-7 is an example of a usability dashboard in Google Analyt‐ ics, showing how to capture some of these key metrics both in pre- production and production in order to adopt Performance Engineering practices in usability.\n\nFigure 2-7. Usability dashboard in Google Analytics\n\nDesign As we look at the complexity of our systems and applications today, much of which we no longer control or house within our data cen‐ ters, the need for Effective Performance Engineering increases con‐ siderably. Being able to adopt these capabilities throughout your lifecycle enables you to consider prototype options and build SLAs and performance budgets into each component you are designing, regardless of the stage of life of the component or system.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n33\n\nSystem design\n\nA critical step in Performance Engineering is defining the many dependent pieces of a system, including data, user interfaces, inter‐ nal and external dependencies, modules, components, architecture, and much more.\n\nWithout a design map and plan, teams are often left to what they know, and the results can be discarded or need significant rework. With an Effective Performance Engineering approach, performance considerations are built in, and collaboration across teams enables a more complete design from the start.\n\nOften, an Enterprise Architecture team performs this step. However, as we can see, this is a critical element for which infrastructure architecture and application architecture must be designed, so involvement from the broader team, with knowledge of challenges and opportunities, is imperative.\n\nInfrastructure architecture\n\nRelated to and often integrated with following system design is the need to define the required infrastructure in order to support the needs of the applications and forecasted business levels, and thus be able to deliver the required resources, often left to a few infrastruc‐ ture people to create and verify.\n\nA continuation of the overall system design is the infrastructure architecture. An Effective Performance Engineering approach pro‐ vides for understanding and prototyping the various components and dependencies, along with configuration and sizing needs for the overall systems.\n\nThis is often delegated to R&D, product management, and product development to execute and deliver; however, to bring Performance Engineering practices into the picture, the team needs to be broader earlier and often include prototyping to find initial optimization.\n\nApplication architecture\n\nThe goal here is to design a composite architecture that will be scala‐ ble, available, manageable, and reliable. This involves many consid‐ erations and technological risks, along with software requirements and configurations, that must be integrated with those of the overall system design and infrastructure architecture.\n\n34\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nAnother extension of the overall system design is the application architecture. An Effective Performance Engineering approach pro‐ vides for understanding and prototyping the various components and dependencies, along with configuration and sizing needs for the overall systems.\n\nThere may be some limitations around existing or defined stand‐ ards, especially given the complexity and integrated nature of com‐ posite applications. Using the approach of prototyping enables quick build and running of a few quick performance scenarios to get results showing how these prototypes perform. Seeing how they per‐ form is the first step. Knowing the underlying infrastructure and network components can also play a major role in being able best to architect a high performing solution. A collaboration between these functions must be taken to optimize architectures and overall sys‐ tem design for performance.\n\nDeployment models\n\nDeployment, or moving code and artifacts through environments to production, can range from manual to fully automated, and can be limited to only certain environments or all of them, and anything in between.\n\nThis step enables you to achieve fast feedback and increase stability across your environments, thereby eliminating many of the variables usually associated with deployment.\n\nThere are a number of open source and commercial capabilities available to help automate this process from check-in to build to deploy, and to enable the tracking of build quality through the life of the release.\n\nResiliency\n\nWhen thinking about design and resiliency as they relate to Perfor‐ mance Engineering, we specifically are looking at the steps you take as you design your solution, so that when your product or service experiences a disruption it can continue to deliver value to your end user.\n\nEnd users today expect to be able to access your products or services at any time, anywhere in the world. In order to enable this capabil‐ ity, we must consider how we design our technology to meet these expectations.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n35\n\nAsking “if this component is down or not available, how will it con‐ tinue to deliver value to our end user?” is a great place to start the design process. Defining the top five revenue-generating workflows for your business, then designing how you can ensure they are resil‐ ient, is a good strategy.\n\nScalability\n\nScalability involves the considerations for how a system or applica‐ tion will be designed to handle a growing amount of work. Over the years, we have heard claims about how certain cloud environments enable exponential and linear growth; how your applications and systems are designed will enable you to see and understand the level of truth in such statements.\n\nIncreased demand for products and services by the end user of your business is a good thing, and if your technology can support and meet or exceed this demand, even better. However, too many times we’ve seen organizations make great assumptions that systems will scale as needed in a linear model. They provide more and more infrastructure capabilities to such a system, only to have it fail signif‐ icantly because it was simply not designed to scale.\n\nStarting with the share services and synchronous capabilities helps us to identify the two most common areas for scalability. Still, the complexities within your composite application architecture will likely also see middle-tier services and database constraints. Consid‐ ering this during the design phase enables you to be ready to proto‐ type early in the process.\n\nDevelopment Practices of Effective Performance Engineering vary across teams and divisions as much as they can across industries and organiza‐ tions. Following some basic principles and practices enables you to create and maintain sustainable, scalable, and high-performing applications on resilient infrastructure. With these guidelines, we will see how to do this, and deliver it quickly with quality.\n\nCode, frameworks, and service reuse\n\nWhen developing new applications and systems, it’s important to leverage ways to accelerate delivery of capabilities. Some of these ways manifest themselves in code repositories, frameworks, plat‐\n\n36\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nforms, libraries, and service components reuse. You can help DevTest teams maximize the focus on new and modified elements, and drive higher quality in a faster timeline, within the defined, shared standards.\n\nAs you move from a sole developer to development teams, then from many development teams to business units of development teams, and finally to many business units, challenges arise around standards and technology approaches and architectures. Many com‐ panies do not consider performance from an early perspective (if ever) and build it in.\n\nImagine having a code repository within a framework and service host that have already been built and optimized within the organiza‐ tional standards, which you can leverage in building the new capa‐ bilities in front of you. Utilizing this approach saves you from spending time building and optimizing core capabilities and shared services.\n\nMetadata repository\n\nDetermining how to quickly and easily leverage needed DevTest data continues to plague many teams and organizations. Attaining frontend data, data in motion, and backend data can be a significant challenge, and often one not easily solved. You must consider many facets in order to create and deliver to end users accurate, working applications and systems to consume that data.\n\nData continues to be one of the bigger challenges facing many today. Without accurate and reflective data, your confidence in application and system success is limited. In addition, to achieve speed and quality of automated lifecycle processes, it’s critical to have quickly refreshing environments, especially those including data. Having a repository of maintained and ready data increases the quality and speed with which you can deliver optimized systems.\n\nMaking data a priority is a great way to start. Bringing in and assign‐ ing accountability to the business analysts and user acceptance test‐ ing teams to define and create data and data models is a parallel approach. Obfuscation of production data to be included, along with data dependencies, and storage must also be planned in order to ensure compliance with all regulatory and other required practi‐ ces. In addition, providing your end users with early access and cap‐\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n37\n\nturing the data and flows they use is another key way you can start to meet this challenge.\n\nAutomated tests\n\nTesting throughout the lifecycle is an important step; however, determining specifically how to do it within DevTest is key, espe‐ cially as continuous testing will be pushing more builds than before, and speed to verify and validate accurately is critical.\n\nYou must proceed with automated tests early, often, and throughout the lifecycle, especially given the need for repetitive and consistent results and measurements, in order to be able to see trends and opti‐ mization opportunities. If tests are not done in an automated way, you cannot meet goals of speed and quality, along with quality gates and build quality, automating more of the lifecycle.\n\nMany open source and commercial capabilities are available to enable automated tests. In addition, using automation test frame‐ works will allow you to build tests that can be merged and versioned with code through the build and test process.\n\nLifecycle virtualization\n\nCreating production-like environments within your pre-production environments is often expensive and labor-intensive. However, hav‐ ing a controlled and stable environment is important, and the ability to make it quickly available and refreshed in a timely manner is even more important. Virtualizing the users, services, network, and data gives you these capabilities, so you can deliver a better quality prod‐ uct faster.\n\nReducing costs, increasing speed, and improving quality are three is key. Having reasons why utilizing production-like environments enables you to more accurately pre‐ dict how things will perform once deployed. Given the complexity of our systems and applications today, this continues to be one of the bigger challenges, and lifecycle virtualization is a way to close the gap significantly.\n\nlifecycle virtualization\n\nMany open source and commercial products are available to provide you with these capabilities. Virtual users, for instance, are commonly found in functional and performance testing tools and enable you to create synthetic end users, performing transactions across a variety\n\n38\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nof workflows as they would in production, across all of your envi‐ ronments.\n\nThen you can move to service virtualization, which enables you to capture and re-create services from production across all of your environments. Virtual networks enable you to capture network con‐ ditions in production (and all other locations) and re-create these in any environment so you can optimize your app or application to perform well, even over poorly performing network conditions. Last is data virtualization, which leverages some metadata repository capabilities to make it quick and easy to refresh data across your pre-production environments.\n\nQuality gates\n\nQuality gates are specific quality thresholds for each stage or envi‐ ronment. These thresholds are then used to automate the build/ deploy/test steps and proceed to the next stage or environment. This allows for little to no manual intervention, and delivering high- quality assets in an automated and timely way. These thresholds enable you to set criteria on the build quality (Figure 2-8 shows a variety of “% Tests Pass” criteria) before proceeding on the path to production.\n\nAs you increase builds and automated tests and deployments, you will see things start to speed up. However, quality is a metric you also need to define, and measure at specific milestones throughout the lifecycle. Tracking quality enables you to observe a build as it moves through the lifecycle, and only intervene when it meets a cer‐ tain quality threshold and you need someone to execute manual tests against the product.\n\nThere are several approaches for adding quality gates, many of which use open source or commercial products to deliver a frame‐ work that enables you to build in the performance, quality, and speed considerations. Some work will be needed to build and sup‐ port this application lifestyle automation solution for your specific environment. Figure 2-8 shows how you can implement automated quality gates with virtualized and physical environments throughout the CI/CD process and where performance results are built in.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n39\n\n40\n\n|\n\nChapter 2: Overview of Performance Engineering\n\ns e t a g\n\ny t i l a u Q\n\n.\n\n8 - 2 e r u g i F\n\nTest Minimizing the risk to the business and your end users in produc‐ tion is paramount. Adopting Effective Performance Engineering practices ensures that you are most efficiently doing this early and throughout your lifecycle, and that you’ll have many opportunities to verify, validate, provide feedback, optimize, and then continu‐ ously execute. The following sections cover specific, proven practi‐ ces you can adopt to enable Performance Engineering capabilities.\n\nBuild results\n\nAfter the commit/build/deploy/test process, the results are what matter, and how we capture these results and make them available to stakeholders is crucial, along with the summary performance met‐ rics continue and live with the build life.\n\nBuilding in this automated feedback throughout the lifecycle, start‐ ing immediately after a successful build, enables teams to respond early and often, which in turn accelerates delivery speed and increa‐ ses quality.\n\nThere are a number of different ways to implement this capability. As a starting point, a basic automated delivery framework needs to be in place. Then, you can use automated unit, functional, or perfor‐ mance scripts to compare execution times at a minimum. Now you will be able to track detailed results from build to build, identifying any outliers, and potentially flag them from your trunk and/or main branch until they can be remediated for performance optimization.\n\nRegression\n\nAs the continuous build/deploy/test cycle evolves into more mature builds and potential release candidates, capabilities and features grow, and you must run quick but complete automated scenarios to ensure no core functionality has been broken or degregated as a result of the new capabilities.\n\nSpeed and quality increase as more performance engineering capa‐ bilities are built into your automated processes. With the adoption of these capabilities comes the need to progress through a variety of environments on the release path. Building automated performance regression suites enables higher confidence and meets automated\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n41\n\nquality gates to enable the potential release candidates to move through the release cycle efficiently.\n\nWhen you’re starting down the path of implementing these capabili‐ ties, you must have an understanding of the key systems, applica‐ tions, and transactions—those critical from both a regulatory and end-user perspective—so that you can implement these first within your automated performance regression suite. In the “lower level” environments (also known as BVT or build validation test), the results should be an automated performance regression suite that takes less than 15 minutes to execute, and provides results to ensure your most critical functions work as designed.\n\nAutomated service oriented\n\nAs the complexity of the composite application and system architec‐ ture grows, so will the dependencies on internal and external serv‐ ices. Automating the verification and validation of these services throughout the lifecycle, and especially testing, is key. You must also consider how to get initial results and flags for performance-related metrics, along with how to re-create these in a valuable and cost- efficient manner.\n\nThe explosive increase in composite application and system archi‐ tectures has resulted in organizations with an exponential number of services. These services change only occasionally, compared to the frequently changing user interface, enabling a more stable and core technology to test with less maintenance.\n\nYou should start with a basic test harness that identifies the service and protocol mapping, then run through a barrage of positive and negative scenarios, delivering results for those that pass and fail. The objective of the team should be continuing to develop until all results pass.\n\nCapability mapping and standardization\n\nOrganizations looking to scale Performance Engineering practices need to standardize core capabilities in order to enable a growing number of teams and individuals to leverage a shared model (map‐ ping). Doing this integrates the new capabilities across IDEs, CI/CD systems, configuration, environments, and release management, just to mention a few key elements.\n\n42\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nHaving disparate and nonintegrated tools to do specific and individ‐ ualized tasks is a mess at a small scale, and once this goes beyond a few teams, it becomes a challenge to manage from many different angles. Finding the capabilities that matter to your organization and standardizing them enables you to scale more efficiently. These standards also simplify the education of new team members and increase the stability of your integrated tools, so you can focus on delivering value to your end users.\n\nFigure 2-9 shows how tool integration can be mapped to support processes to implement Effective Performance Engineering practi‐ ces. As you continue down this path, it may be necessary to create an abstraction layer across the top, so all capabilities can be visual‐ ized and used as an information radiator for several stakeholders across your portfolio.\n\nFigure 2-9. Mapping simple tool integration to support processes\n\nDeployment Releasing code and builds throughout a lifecycle is an art, and it’s a key component to enabling Effective Performance Engineering. In this sectio we will highlight many of the critical ways this can be done, the required controls, and how to get continuous automated feedback throughout. The end objective is for deploying to produc‐ tion not to be an event, but simply another deployment in a series that have already been executed and delivered throughout the lifecy‐ cle in a rigorous yet fully automated process.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n43\n\nAutomation throughout the lifecycle\n\nMoving through the lifecycle with everything we’ve described previ‐ ously is a huge success, but ensuring all that effort isn’t for nothing is also important; we have seen several examples in which deployment to production is still is a manual process, which has its own draw‐ backs and risks.\n\nReducing the manual effort required increases accuracy and repeat‐ ability, and ensures deployment has been tested before the product deploys to production. You can automate deployments of builds and release candidates with commercial or open source products, but implementations vary a bit depending on your environment. The biggest challenge, however, is that of organizational and individual behaviors limiting the continued utilization of automated deploy‐ ments to production.\n\nShowing these organizations and individuals how deployment auto‐ mation has been achieved and tested through pre-production envi‐ ronments, and how IP addresses and credentials can be secured, will go a long way toward utilizing automated deployments to pro‐ duction.\n\nLive/live or blue/green\n\nLive/live or blue/green is a deployment approach in which you have two production environments, enabling you to potentially take one offline, deploy to that environment, and introduce a small popula‐ tion of users to ensure it performs as expected, then either shutting that instance off or leave it on and deploy to the second environ‐ ment so you are running two production environments again.\n\nThere are many reasons why you would want to leverage a live/live or blue/green approach. It provides resiliency and scalability, and increases performance. It also allows you to easily perform canary tests in a contained environment. All of these are key Effective Per‐ formance Engineering practices.\n\nHow this approach is implemented varies, depending on the organi‐ zation. Figure 2-10 shows one way we have done it and opens a dis‐ cussion of which specific Effective Performance Engineering practices are performed where. You will also see a stand-in mode added into this model, available in case the production environ‐ ments (n and n+1) have issues (a stand-in providee end users with access to the products and services without interruption).\n\n44\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nFigure 2-10. Live/live or blue/green deployment\n\nCanary\n\nIn the canary deployment approach, you roll out a new capability to a very focused group of your population, then observe that group’s performance and feedback, and finally decide whether to continue, stop, or pull back that deployment or release to a more general pop‐ ulation.\n\nGetting fast feedback and market-testing new capabilities is impor‐ tant to many organizations so they can test their theories and best guesses as to what the end user needs next. The canary test may also be a first time on a new infrastructure or application architecture, so they can benefit teams greatly to see how the architecture per‐ forms at a smaller scale in production. They can then apply their new knowledge to the next release, and deploy an incremental qual‐ ity increase for the next round of canary testing, feedback, and measurements.\n\nThis kind of test is often limited to a single environment (perhaps a single cluster) and often utilizes a live/live or blue/green production environment, in order to further isolate the underlying technology and limit access to specific end users.\n\nFail over, fail back, and fail forward\n\nThe fail over, fail back, and fail forward approaches involve answer‐ ing the question, “When we have problems, how do we plan to man‐ age them?” The beauty of this process is it allows you to test your\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n45\n\ndesired approach in your later-stage environments (pre-production or disaster recovery, perhaps). The fail over/fail back approach means you have a secondary production environment to fail over and back. The fail forward approach means your intent is to simply push a next release over the prior to resolve the issue.\n\nIn a production disaster recovery scenario, these processes are often defined in the run book and tested on how to fail over, but not always how to fail back. When you are failing back there are many challenges, along with many dependencies on databases and exter‐ nal services, that you need to consider. Not doing so, and as we have observed in several cases, can result in data loss and other more seri‐ ous incidents, making for upset customers and regulatory issues.\n\nTo build these approaches into your tests, you must take the time to plan for them, and they will aid you greatly in ensuring you have a cross-functional team with shared responsibilities and goals. Find‐ ing ways to build in fail over, fail back, and fail forward into your overall deployment approach is a great way to start. Perhaps asking this question today will help you understand how you do or do not plan for problems, and imagining what might happen if the worst occurs, will get you to adopt this Effective Performance Engineering practice.\n\nMonitoring In many organizations, monitoring is a practice adopted only within production, and is often referred to as application performance mon‐ itoring (APM). However, reactive performance monitoring happens too late to have an impact on revenue, brand, competitive advan‐ tage, and customers. All those factors have already been determined once they’re identified in production. In Effective Performance Engineering, we leverage several monitoring capabilities, explored in more detail in the following sections.\n\nContinuous monitoring and feedback\n\nMonitoring environments and components within your architecture and perhaps outside, both within production and pre-production systems, helps you learn about and improve capacity, resiliency, per‐ formance, and scale. In addition, continuous monitoring and feed‐ back provides you with ways to observe and capture conditions in which production incidents occur, so you can re-create them in pre-\n\n46\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nproduction and validate a fix prior to pushing it back into produc‐ tion.\n\nWithout feedback throughout the lifecycle, you only have a best guess and a hope that a solution will work after it is deployed. We often see this in practice, and yet with Effective Performance Engi‐ neering practices, it does not need to be this way.\n\nHow this process is done varies by the organization and its maturity, but it starts with monitoring both in production and pre-production environments, then moving to measure the key performance indica‐ tors (KPIs) both from a technical and business perspective. Once these have been achieved, you can start by showing these results for the release candidate and then move to build-level results.\n\nEnd-user feedback and analysis\n\nEnd-user feedback can be measured in both objective and subjective results: objective from transactions completed, conversion rates, and response times, and subjective through reviews and interview and survey feedback.\n\nYour end user is the person that matters the most; getting their feed‐ back and analyzing the observations is a key piece of monitoring.\n\nThere is a vast array of ways to gather both objective and subjective feedback from your end users. These range from formal studies in which you invite participants to join you in a lab-like setting to view some new capabilities and features, asking them for feedback, to simply pushing a few new capabilities in production to your canary group, and observing the KPI metrics based on how they use the system. In some cases you might capture and measure heart rate, eye movement, screen tapping, and other behavioral and cognizant responses. This data will all be analyzed so it can be fed back to the teams, and who will use it to create the next iteration of the capabil‐ ity, before eventually deploying it to production.\n\nPredictive: pre-production and production\n\nYou can use the data you’ve gathered and analyzed from production and pre-production environments to predict under what conditions degradation or system failure will occur, so you can operate proac‐ tively. This predictive data enables you to mitigate these issues throughout the lifecycle, so you can better deliver new feature func‐ tion to your end users uninterrupted.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n47\n\nGetting your product wrong is a huge expense in many ways. So, having insight that provides predictive recommendations on what will go wrong and when is a huge advantage to your organiza‐ tion. This is used throughout pre-production and production envi‐ ronments.\n\nWhen leveraged with correlation and big data analytics, predictive data provides us with insight into what could go wrong based on what has gone wrong historically. Results such as Fundex and User Sentiment indicate how your end users will and are responding to new capabilities.\n\nSupport Effective Performance Engineering does not end at production; it is a continuous, iterative practice of integrating feedback, improving the entire lifecycle with every new piece of information, and auto‐ mating this cycle. Advancements in big data and predictive analyt‐ ics, combined with these practices, enable a more stable, high- performance experience for end users.\n\nThreshold analysis and automated mitigation implementation\n\nThreshold analysis starts with defining specific thresholds within your systems and applications for your application performance monitoring capabilities to alert and alarm, then building in rules- based automated mitigation implementation, which allows the prod‐ uct or service your end user is accessing to continue with minimal interruption. Utilizing this approach increases the resiliency of your architecture.\n\nMany companies today have set up threshold analysis in the form of a network operations center (NOC), staffed by a few individuals around the clock, with dozens of monitors projecting thousands of alerts per day and sometimes hourly. Having this automated with built-in and tested mitigation implementation simplifies and ensures the business is protected, enabling the technology to grow and automatically scale with the company’s needs.\n\nWhile this area is still maturing from a solutions perspective, it can be done and delivered successfully when implemented by a team that considers it a collaborative goal.\n\n48\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nIncident management, root-cause analysis, and reporting\n\nIncident management is the practice of actively managing produc‐ tion (and in some cases pre-production) incidents. Its chief purpose is capturing the “current state” and enabling the re-creation of the incident in pre-production, while finding and fixing the root cause. This root-cause analysis, along with tracking and reporting trends, will support the technical debt and investment needed to ensure your architecture is performing well.\n\nHistory does repeat itself. This is especially true for a technology- enabled business that follows a defined set of instructions the same way every time. Thus, we must build in learning cycles in order to understand why something happened and prevent it from recurring.\n\nMany solutions exist off the shelf today. However, first you must assign accountability and responsibility for this activity. We have often seen an incident management team that owns the process of the production incident, but who owns the learning and feedback to prevent that incident from happening again is not always clear. In Effective Performance Engineering, each task must first have a defined process and owners, and only then be enabled with technol‐ ogy.\n\nRe-create production incidents pre-production\n\nAll too often we see pre-production teams working on and attempt‐ ing to fix production incidents. When production incident fixes are applied in production, what gets released are non-integrated updates that are not in any way validated or tested. Not only is this a risky approach for production, but now you have changes that are not being implemented into the existing pre-production assets for future deployments. This means these incidents will happen again. It is also helpful to ask, “Did these hotfixes really fix the problem, or just push them off to another day?”\n\nLeveraging Effective Performance Engineering practices enables you to quickly capture a snapshot of what happened in production, then spin up the environments that pre-production deployed and re- create the incident, so you can follow your normal process to make quality fixes to your production release.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n49\n\nStakeholders With Effective Performance Engineering, the stakeholders are from all walks of life. Whereas with traditional Performance Testing, the sole responsibility for performance fell on a select group of individ‐ uals, in Performance Engineering, it is the entire team’s responsibil‐ ity to work in a broad and collaborative manner encapsulated by the organizational culture.\n\nOnce you drive incremental value and success within a group or team, this culture accelerates. Both as you start adopting these capa‐ bilities and as you continue to integrate them, it is critical that you understand your stakeholders, keep their best interests in mind, and communicate with them frequently. We will dissect what each stake‐ holder group looks like in the following subsections.\n\nDevelopment Performance Engineering is often not a top priority for a typical development team, especially given all the unique and expanding challenges related to the complexity of composite architectures, dis‐ tributed organizations, and end users. The reason for development’s growing importance is to ensure they deliver the highest quality and performing product, and provide continuous feedback and optimi‐ zation recommendations, so other teams can deliver quickly and in fully automated ways.\n\nTesting and Quality Assurance The responsibility of a testing and quality assurance team independ‐ ent or integrated into teams is to ensure the delivery of a quality product. How this happens and where the focus is depends on who you are speaking with, and how they are aligned. For example, some organizations align by specialty (security, performance, functional, usability, and so on) and others have an integrated team. With effec‐ tive Performance Engineering practices, a group of individuals is accountable for the delivery of a high-performing solution through‐ out the development process, and delivers that with clear measure‐ ments and communications throughout the lifecycle.\n\n50\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nOperations Operations teams are focused on ensuring the product is running and service is available. “Less is more” is this stakeholder’s perspec‐ tive. In other words, the less impact a change has on the production environment, the better. This is why defining and validating what is moving into production is so important; the operations stakeholders will want to see how and why a capability is ready for the production environment. The practices of Effective Performance Engineering involve these individuals throughout, allowing them to contribute and be a part of the team steering a high-quality product through the environments and into production.\n\nBusiness The focus of this stakeholder is on ensuring that revenue, competi‐ tive advantage, customer, and brand goals are achieved. This includes expanding offerings and/or businesses either through organic or acquisition approaches, all of which may depend on existing or new platforms, so end users can consume products and services without interruption when, where, and how they want to.\n\nEnd Users End users ensure feedback is delivered, and value from product and/or service is realized. Users simply do not want security, perfor‐ mance, functional, or usability issues while interacting with your products or services. Making them a champion for your brand should be your goal; you do so by continuously meeting or exceed‐ ing their expectations, and asking them to share their positive expe‐ riences with others frequently.\n\nOf course, you will encounter other stakeholders, but start by identi‐ fying which of these five existing roles fit into your organization. Begin thinking about how you can apply some of the Effective Per‐ formance Engineering practices in your organization today, aligning them with the interests of these stakeholders. Include your stake‐ holders in these conversations, ask them for feedback on what they get and what they need, and show them your desire to make them your biggest supporter and champion.\n\nStakeholders\n\n|\n\n51\n\nBuilding in Performance Performance cannot be an afterthought; it needs to be at the fore‐ front of your team’s thinking from the very beginning and through‐ out. This often comes into conflict with the “release faster” mentality that drives many businesses today, but it doesn’t have to. The speed/ accuracy tradeoff that is often cited in Lean startup principles doesn’t necessarily apply to the performance of the applications we build, but rather to the speed with which we deliver and the accu‐ racy of our delivery compared to what the market needs. When it comes to Lean startups, it isn’t necessary to hit the mark 100% right away. Fast feedback allows us to adjust our course and reiterate very quickly. But what happens if the products we deliver to the market perform poorly? There won’t be a second chance to get things right if the customers’ first impression is of a slow or unresponsive app.\n\nIt is our responsibility to bring these capabilities and practices to our business owners and CxOs so they can understand why they’re important and how others are delivering compelling results to their end users, and will enable your business to do the same or more.\n\nThe List: 102 Questions to Ask Performance Engineering is a complex discipline encompassing applications, infrastructure, security, and more. To truly optimize your performance, your organization needs to address a broad range of issues. To make informed decisions, individuals and organiza‐ tions need to start asking some or all of the following 102 questions.\n\nThis list is intended to be inclusive but not exclusive, and apply across all DevTestOps approaches. No matter what your role, per‐ sona, and interest, this list should help you understand how your solution is engineered for performance, stability, and scalability.\n\nOur goal is to help you, your team, and your stakeholders gain a common nomenclature for Performance Engineering so you can define your path and direction together. We intend the following as a checklist of actionable items, so you can ask informed questions as you start collaborating and adopting Effective Performance Engi‐ neering practices throughout your lifecycle.\n\n52\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nServer sizing\n\nHow many application servers are needed to support the cus‐ tomer base?\n\nWhat is the optimal ratio of users to web servers?\n\nWhat is the optimal web server–to–application server ratio?\n\nWhat is the maximum number of users per server?\n\nWhat is the maximum number of transactions per server?\n\nServer tuning and optimization\n\nWhich specific hardware configurations provide the best performance?\n\nHow can vendor default configurations be tuned to suit this specific infrastructure and application?\n\nWhat\n\nsystem\n\nresources need\n\ntuning\n\nto give optimal\n\nperformance?\n\nCapacity planning\n\nWhat is the current production server capability?\n\nIs there room for growth?\n\nWhat hardware or software can be added to achieve the next level of performance or capacity?\n\nIs there excess capacity? Can a server be removed without com‐ promising performance?\n\nThird-party validation\n\nWhat is the current ISP and network capacity?\n\nCan the ISP deliver on the service-level agreement that was signed?\n\nSecurity exposure\n\nCan system vulnerabilities be identified and minimized?\n\nBuilding in Performance\n\n|\n\n53\n\nWhat is the failover for firewalls?\n\nAre there new vulnerabilities when excess user load is added to the application?\n\nHow susceptible is the system to DoS attacks?\n\nIf a DoS attack occurs, how will the system respond?\n\nIf the system goes down due to a hacker attack, how effective are the recovery procedures?\n\nInfrastructure The following are some questions you should ask regarding the infrastructure.\n\nBrowser/user profile issues\n\nThis subsystem is known as the user community profile and consists of business process definitions.\n\nWhat do the users do? (These are business-process definitions.)\n\nHow fast do the users do it? What are the transaction rates of each business process?\n\nWhen do they do it? What time of day are most users using it?\n\nWhat major geographic locations are they doing it from?\n\nIs the application browser- or interface-dependent?\n\nIs modem, WAN, or LAN emulation necessary?\n\nAre there asynchronous communications between the browser/ client and the backend servers?\n\nAre there any non-HTTP(s) communications between the browser/client and backend servers?\n\nInternet issues\n\nWhat are the peering issues associated with the client’s hosting/ bandwidth provider?\n\nWhat is the hosting strategy?\n\n54\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nSite web pipe issues\n\nHow much bandwidth does the site have?\n\nWho is the client’s bandwidth provider? (Peering issues)\n\nAre there multiple web pipes?\n\nBorder router issues\n\nWhat kind of load-balancing are the multiple pipes configured for?\n\nDoes it use the same inbound pipe as outbound pipe?\n\nIs there equal distribution for outbound regardless of inbound pipe?\n\nIs there the same outbound pipe regardless of the inbound pipe?\n\nAre there multiple border routers?\n\nWhat is the failover configuration for multiple border routers?\n\nLoad-balance issues\n\nWhat type of load-balancing scheme is used? (Round robin, sticky IP, least connections, subnet based?)\n\nWhat is the timeout of LB table?\n\nDoes it do any connection pooling?\n\nIs it doing any content filtering?\n\nIs it checking for HTTP response status?\n\nAre there application dependencies associated with the LB time‐ out settings?\n\nWhat failover strategies are employed?\n\nWhat is the connection persistence timeout?\n\nAre there application dependencies associated with the LB time‐ out settings?\n\nWhat are the timeouts for critical functions?\n\nBuilding in Performance\n\n|\n\n55\n\nPeripheral systems issues\n\nIs the LAN/WAN system dedicated or shared with other appli‐ cations?\n\nAre there any shared production resources?\n\nAre there any web pipes, ERP systems, mail servers, filesystems, DNS servers, and so on?\n\nDoes it share databases with other applications?\n\nDoes it share hardware with other applications?\n\nExternal systems issues\n\nAre there any outside vendors that provide content distribution systems (CDS) for the architecture?\n\nDistributed hosting issues\n\nAre these multiple mirrored sites?\n\nIs any site configured for failover operation?\n\nHow is the traffic load-balanced across the sites?\n\nAre\n\nthere architecture components on\n\nshared WAN\n\nconnections?\n\nWhat is the failover and recovery behavior?\n\nFirewall issues\n\nWhat is the throughput capacity?\n\nWhat is the connection capacity and rate?\n\nWhat is the DMZ operation?\n\nWhat are the throughput policies from a single IP?\n\nWhat are the connection policies from a single IP?\n\nIDS: Intrusion detection systems\n\nIs there statistical content sampling?\n\n56\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nIs there an inverse relationship between throughput and security?\n\nHow is content filtering achieved?\n\nApplication Here are some of the questions you should be prepared to ask regarding the application.\n\nWeb server issues\n\nHow many connections can the server handle?\n\nHow many open file descriptors or handles is the server config‐ ured to handle?\n\nHow many processes or threads is the server configured to handle?\n\nDoes it release and renew threads and connections correctly?\n\nHow large is the server’s listen queue?\n\nWhat is the server’s “page push” capacity?\n\nWhat type of caching is done?\n\nIs there any page construction done here?\n\nIs there dynamic browsing?\n\nWhat type of server-side scripting is done? (ASP, JSP, Perl, Java‐ Script, PHP, and so on)\n\nAre there any SSL acceleration devices in front of the web server?\n\nAre there any content caching devices in front of the web server?\n\nCan server extensions and their functions be validated? (ASP, JSP, PHP, Perl, CGI, servlets, ISAPI filter/app, and so on)\n\nMonitoring (Pools: threads, processes, connections, and so on; queues: ASP, sessions, and so on; general: CPU, memory, I/O, context switch rate, paging, and so on)\n\nBuilding in Performance\n\n|\n\n57\n\nApplication server issues\n\nIs there any page construction done here?\n\nHow is session management done and what is the capacity?\n\nAre there any clustered configurations?\n\nIs there any load-balancing done?\n\nIf there is software load-balancing, which one is the load- balancer?\n\nWhat is the page construction capacity?\n\nDo components have a specific interface to peripheral and external systems?\n\nDatabase server issues\n\nHave both small and large data sets been tested?\n\nWhat is the connection pooling configuration?\n\nWhat are its upper limits?\n\nSecurity Here are some of the questions to ask when addressing security issues.\n\nFirewalls and multiple DMZs\n\nDoes the firewall do content filtering?\n\nIs it sensitive to inbound and/or outbound traffic?\n\nWhat is its upper connection limit?\n\nAre there policies associated with maximum connection or throughput per IP address?\n\nAre there multiple firewalls in the architecture (multiple DMZs)?\n\nIf it has multiple DMZs, is it sensitive to data content?\n\n58\n\n|\n\nChapter 2: Overview of Performance Engineering\n\nIDS: Intrusion detection system\n\nIs there any content filtering?\n\nIs the system sensitive to inbound and/or outbound traffic?\n\nWhat are the alert thresholds?\n\nWhat are the acceptable security thresholds?\n\nNaturally, these questions are only a starting point—you also need to come up with answers—and they don’t cover every possible issue in Performance Engineering. How will you use these questions? What would you add to the list?\n\nBuilding in Performance\n\n|\n\n59\n\nCHAPTER 3 Proven Practices of Performance Engineering\n\nTo explore the proven practices of Performance Engineering, we will start with the requirements, architecture, and design; hit some of the highlights of implementation; and walk you through a real-life sce‐ nario. The objective of this chapter is to present a complete case study for each practice so you can begin to understand what it means for you, and to provide you with a story you can use and share with your team or organization.\n\nRequirements, Architecture, and Design Here is a list of proven practices for requirements, architecture, and design:\n\nIdentify components\n\nSet performance budgets\n\nEstablish acceptance criteria\n\nPlan for outliers\n\nBuild in performance culture\n\nPrototype (and test)\n\n61\n\nIntroduction One of the questions many people ask themselves while adopting Effective Performance Engineering practices is, “How do I engineer configuration and applications before starting development?”\n\nThe building in Figure 3-1 represents the requirements of software and architecture architecture and design.\n\nFigure 3-1. The requirements of architecture and design represented as a building\n\nToday, teams architect and design within their own pillars, typically within a development or architecture team, and sometimes seen within a “Sprint 0” or other phase if the project or system is new.\n\nThe increasing complexity of composite applications, and the multi‐ tude of end users and ways of consuming products and services, has compounded the root-cause issues described in Chapter 2. Effective Performance Engineering provides a new way of thinking about software and hardware systems and how to architect and design them so your end user has a great experience, and you have a high- performing and resilient capability supporting your products and services.\n\nIn many cases these are complex and dependent systems—some‐ times new, but often existing and integrated throughout both the frontend and backend. Knowing this, you may be focused initially on “big risk/big impact” systems like web and core systems.\n\n62\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nAlthough the risk and reach is high, so is the value you can deliver to the business and your end users.\n\nScenario Company ABC is a large financial services institution that has grown over the years to 9 million+ customers and continues to grow at a high rate both organically and through acquisition. They have been transforming to integrated and self-managed Agile teams for nearly a year. A new epic has been prioritized for a new capability that will be rolled out to all customers across all channels, and although it is a vendor commercial-off-the-shelf (COTS) product, it has not been previously deployed at this scale by the vendor or any other known enterprise organizations.\n\nChallenges\n\nAs you might imagine, this task is not without its challenges—some known and some not. To help you spot these potential challenges, here are some additional details.\n\nThis is the largest Internet-based bank in the United States. The new epic has to do with a login security capability, in which end users will no longer be using their PIN but an image + phrase + login + password to gain access to their accounts. This is a very good brand, having earned the business of 9 million+ Americans and seeing low double-digit growth rates of new customers. 180,000 logins per hour across 7 total major geographic locations worldwide (5 of which are in the US). Of the 9 million+ customers, 40%+ most often access their accounts via mobile with a range of mobile conditions across 2.5G, 3G, and 4G connection types. When rolled out, 100% of all customers upon login will be requested to set up and complete the new login procedure prior to getting access to their accounts. This is planned as a software-only install, with no new hardware or other infrastructure upgrades needed, per the vendor.\n\nYou can imagine some of the complexities that might exist or quickly begin to surface with this scenario.\n\nOption 1\n\nRoll out the new login to all customers simultaneously, and provide additional infrastructure to support the initial spike in activity.\n\nRequirements, Architecture, and Design\n\n|\n\n63\n\nThe pros of Option 1 include:\n\nIncreased marketability of the new feature (enhanced security capability for end users)\n\nShortest perceived time to first implementation\n\nShows urgency in response to partnering technology with busi‐ ness\n\nThe cons of Option 1 include:\n\nIncreased risk of failure\n\nPotentially high infrastructure cost\n\nIntroduced elasticity\n\nrequirement,\n\nforcing potential\n\narchitecture of the application\n\nDependency on potential unknown cloud service provider to scale\n\nOption 2\n\nRoll out the new login to a reduced segment of the population, using a staggered approach.\n\nThe pros of Option 2 are as follows:\n\nPerceived reduced risk of failure\n\nAbility to monitor the affected install base and make quick deci‐ sions to fix if needed\n\nMeasurable incremental scaling of capacity monitored and observed with ramp-up\n\nAnd the cons of Option 2 include:\n\nDuplicate infrastructure during the rollout to support dual authentication mechanisms\n\nPotential database and parallel user login profiles during transi‐ tion and possibly higher risk remediation\n\nMay take longer to roll out to large-scale customer base\n\n64\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nre-\n\nOption 3\n\nAllow customers to opt in to the new login method over a period of several days or weeks.\n\nHere are the pros of Option 3:\n\nLowest potential risk of impact from high adoption rates of new login credentials\n\nAnd here are the cons of Option 3:\n\nNo incentive for customers to switch\n\nHigh likelihood that customers will not switch, leading to longer time to support both login types\n\nSample size of data through conversions potentially too small for any meaningful indication of future impact\n\nProbably will need to push a force to transition in the future, which could lead to massive customer demand\n\nPerceived risk in the market to customers and/or regulatory impact\n\nRecommendation Given these three options, we recommend Option 2. It provides a balanced approach by deploying the capability in a timely manner, while mitigating both the risk of overcapacity from demand as well as the business risk from brand and regulatory impact.\n\nSummary This scenario was based on a real-world situation in which the resulting impact was nearly catastrophic to the organization. At the time, there was no indication that a well-established commercial product would have such a detrimental impact.\n\nHere we have the benefit of hindsight to tell us there were better ways to roll this change out, capture relevant usage and impact data from a subset of the production users, and make the necessary adjustments. It is important to ask, “What will be the impact if this fails?” and somehow mitigate that without adding weeks or months to the project. Architects rarely get to see their vision brought to life.\n\nRequirements, Architecture, and Design\n\n|\n\n65\n\nIn the real-world scenario, what happened was a staggered approach to deployment. However, what was not known or predicted was the users’ behavior: specifically, that nearly every user would scroll through all 100+ images to see which related most to them, and that each image was 700KB in size with 5 images per page displayed. As a result, even with only 1% of the population (90,000 customers) set‐ ting up their new login credentials, each pulled a huge amount of data (3.5MB per page, at 20+ pages per customer) through the data center network lines. The result was a massive production incident that taxed nearly all systems due to overcapacity issues. This was compounded by the fact that ~40% of users were on mobile connec‐ tions, causing the sessions to remain open and ultimately run out of connections throughout all components.\n\nHow-To In this scenario, you can see why the proven practices of Perfor‐ mance Engineering for requirements, architecture, and design should play a significant role within your organization. The first step is identifying the components, you should consider within the infra‐ structure and application architecture, knowing some will be inter‐ nal and others external, and some private while others are shared. The next consideration is setting performance budgets, or allocating milliseconds per component to target, in order to deliver the desired end-user experience. Defining the acceptance criteria for each com‐ ponent, specifying the conditions and use case (e.g., network condi‐ tions and image sizes), is critical. You can see how planning for outliers might have led the bank to make a bit more capacity avail‐ able, and a more conservative deployment approach. Building in per‐ formance culture would have helped a lot here, especially in considering vendors for commercial off-the-shelf (COTS) products and capabilities, and establishing performance criteria as part of the interrogation criteria prior to acquisition through the procurement process—let alone production deployment. This is where also proto‐ typing and testing would have provided significant insight and infor‐ mation proactively, which the bank could have then applied and used as feedback in determining the best technology approach with the business for the end user.\n\n66\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nKey Implementation Considerations\n\nOrganizational\n\n— Where do people sit (different teams versus integrated)?\n\n— Who is accountable for performance?\n\nCultural\n\n— Place value on performance\n\n— Hold people accountable for performance\n\n— Tie compensation to performance\n\nTechnical\n\n— Build performance into the story\n\n— Build performance into the architecture\n\nProven Practices for DevTest Continuing our journey into the proven practices of Performance Engineering, next we describe a DevTest scenario and explore some of the highlights. In this section, we once again start with a list of proven practices, then walk through an introduction, followed by a scenario, summary, how-to, and key considerations.\n\nHere are the proven practices of Performance Engineering for DevTest:\n\nBuild performance into your UNIT tests\n\nBuild performance into your build validation tests (BVT), get‐ ting results after every build\n\nTrack the trend of results for systems and components\n\nAutomate quality gates in the build in order to avoid perpetuat‐ ing poorly performing components\n\nConsider a branching strategy that enables you to “keep out” pieces that will break the code and/or make it perform poorly\n\nProven Practices for DevTest\n\n|\n\n67\n\nIntroduction Performance Engineering during application development consists of testing the application in as realistic an environment as is avail‐ able, without impacting the velocity of the team, and getting rele‐ vant performance feedback into development in an automated way. Assuming we have the necessary goals defined from requirements, architecture, and design, this feedback should provide KPIs to sup‐ port those goals. For example:\n\nThe application needs to support 10,000 active users with sub- second responses for key transactions such as Login, Search, or Confirm Order.\n\nThe application must support a peak volume of 1,000 transac‐ tions per second with processing time of no more than 500ms.\n\nThese KPIs should reflect the goals of the business, and provide a target for success and improvement such that every build and release of the application is measured consistently against these goals.\n\nMoreover, the process by which these KPIs are measured should reflect the goals and delivery method best suited to support the busi‐ ness. This means an application that follows an Agile or hybrid approach to development and testing should not be held up by the activities of the team or individuals responsible for performance. This is critical, and should define the level of automation and meas‐ urement that can be achieved in order to provide the greatest level of feedback with the least amount of impact to the application deliv‐ ery chain.\n\nTo support this statement, we will use an example scenario in which an application team is tasked with reaching certain business goals for performance, but is forced to make tradeoffs in order to main‐ tain their release velocity.\n\nScenario Company XYZ is about to launch a new version of their online ship‐ ping application, which currently services 2,000 businesses and 95,000 individuals via mobile and web clients in North America.\n\n68\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nThe new version is required to support their European launch, adding an estimated 150,000 new customers as well as 15 new regional shipping services and 5 new payment vendors across 28 countries.\n\nThe service-level objectives (SLO) of the business are to process all orders within 2 seconds, although key stakeholders do not have a good sense of peak user volumes. Estimates from business analysts in the Marketing team estimate an additional 45,000–65,000 indi‐ vidual users.\n\nChallenges\n\nFor the Performance Engineering team to validate the SLO, they need an estimated three weeks to build a production-like environ‐ ment, and another four weeks to complete testing and analysis. Even if a reasonable amount of overlap is achieved, this release delay is unacceptable to the business. The developers don’t believe this work is needed, as they are confident in their architecture decisions and in the quality of their code. They also believe that any issues found in production can be fixed quickly enough to minimize the impact. In this example organization, all teams are independent and dis‐ tributed groups, and Development carries enough influence that this line of thinking is gaining popularity.\n\nConversely, the media has run stories about the company’s expan‐ sion into Europe, primarily led by competitor messaging that the company is likely to fail to meet the demands of such a broad and diverse region. Many in the company are aware of the risk of failure to launch, and support the need to validate performance. You can present one of three options in hopes of avoiding a major failure during this important launch.\n\nOption 1\n\nTest the end-to-end application in a completely integrated and scaled environment at the end of development.\n\nThe pros of Option 1 include:\n\nComplete picture of application performance before go-live date\n\nAbility to identify bottlenecks within and beyond the company’s infrastructure\n\nProven Practices for DevTest\n\n|\n\n69",
      "page_number": 27
    },
    {
      "number": 3,
      "title": "Proven Practices of Performance Engineering",
      "start_page": 72,
      "end_page": 86,
      "detection_method": "regex_chapter",
      "content": "The cons of Option 1 are as follows:\n\nMassive delays to the project\n\nDifficult to resolve issues late in development\n\nLittle room for additional delays or retesting\n\nOption 2\n\nTest the end-to-end application in a scaled-down and virtualized environment using service levels provided by third-party services to represent external dependencies.\n\nHere are the pros of Option 2:\n\nReasonable facsimile of production performance\n\nAbility to virtualize external dependencies reliably and capture “what if” scenarios\n\nFewer delays to the project timeline, more time for re-testing\n\nAnd here are the cons of Option 2:\n\nAssumption-based approach to capturing third-party depend‐ ency performance\n\nStill difficult to resolve issues late in development\n\nOption 3\n\nTest individual application components throughout development, with virtualized internal and external dependencies. The end-to-end performance will be tested at the end of each sprint/cycle.\n\nThe pros of Option 3 are:\n\nFast feedback to the development team\n\nLittle to no delays to the project\n\nReasonable facsimile of production performance\n\n70\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nThe cons of Option 3 are:\n\nAssumption-based approach to capturing third-party depend‐ ency performance\n\nRecommendation The recommendation is Option 3. Application changes tend to focus on a few components that can be scaled close to production, while the surrounding dependencies are outside of our control. These may belong to other teams or organizations, or are cost-prohibitive to build to scale in a DevTest environment. Leveraging virtualization for these dependencies is key to isolating and identifying the impact to the components under our control.\n\nSummary In reality, the best-fit solution for testing any application involves some tradeoffs between the completeness of our picture of perfor‐ mance and the time in which we deliver. There is no one answer that fits every application, environment, and organization, but in most situations we should strive to deliver as much information as possi‐ ble, as quickly as possible. In general, there are three factors you should consider when deciding how to test: cost, quality, and time. We can choose which of these is most important, but the closer we get to one, the more we sacrifice of the other two.\n\nFigure 3-2 shows the triangle of three key factors that everyone wants; however, it is often said you can choose only two.\n\nFigure 3-2. The three key factors\n\nProven Practices for DevTest\n\n|\n\n71\n\nHow-To One approach to delivering critical performance information to the Development team without slowing or halting their progress is to automate the execution of performance testing during each and every build. The goal is to understand the performance delta between builds, with a focus on KPI trends rather than the accuracy of individual metrics.\n\nIn order to achieve rapid feedback in a continuous integration envi‐ ronment, you must reuse assets and virtualize dependencies cap‐ tured during previous iterations. Automated execution and analysis are critical. Wherever possible, automation should extend to the analysis of tests as well. Most testing tools, such as Jenkins and Bam‐ boo, extend continuous integration platforms to provide perfor‐ mance trending and feedback after each build is created. Figure 3-3 shows a typical lifecycle representation along with call-out boxes of how performance can be built in at specific stages, demonstrating a possible flow through development that provides quick time-to- value and will not significantly impede development velocity.\n\nFigure 3-3. Typical lifecycle representation\n\nThe most significant feedback comes from performance testing, which should happen in three stages:\n\n1. Execute the baseline test after each and every build.\n\n72\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\n2. Add/change incremental functionality to the test library as it becomes available.\n\n3. Add new/changed features to the baseline at the end of each release.\n\nIn this context, the baseline test refers to the previously accepted release criteria, or some level of test that represents the most com‐ mon usage and behavior of the application. BVT for performance may be contentious for some organizations that have not bought into the value of performance to the company. In these cases, it is important to understand:\n\nWho is responsible for the performance and stability of the build?\n\nWhat information can we deliver that will add value for that person or group?\n\nEvery team, technology stack, and application will be different, but we can strive to provide relevant, repeatable, and realistic data in a reasonable timeframe, to avoid blocking the development process.\n\nParallel to automated test execution and analysis, Performance Engi‐ neering teams should execute a series of focused tests to validate the scalability and resilience of the application. These are often run against a pre-production or dedicated performance environment (for the very fortunate), and account for the overarching perfor‐ mance requirements that are not easily captured in a single set of release requirements.\n\nIn addition to testing, there needs to be some sort of continuous feedback from production. In the following sections, we will discuss some of the ways that monitoring can be used to provide critical information to development and testing, and impact the design phase as well.\n\nKey Implementation Considerations\n\nOrganizational\n\n— Who is responsible for your builds?\n\n— Are they considering performance?\n\n— What does “pass” mean?\n\nProven Practices for DevTest\n\n|\n\n73\n\nCultural\n\n— What happens when you break a build?\n\n— How is progress/status socialized?\n\nTechnical\n\n— How are you enabling automation across tools, teams, and\n\nroles?\n\nProven Practices for Operations Lastly, in our exploration of proven practices of Performance Engi‐ neering, we will elaborate on Operations, highlighting, and illustrat‐ ing important points through a real-life scenario. Again we start with a list of proven practices, walk through an introduction, and follow with a scenario, summary, how-to, and key considerations.\n\nHere are the proven practices of Performance Engineering for Oper‐ ations:\n\nMake sure disaster recovery, capacity planning, and resiliency are all known, tested, and observed.\n\nProvide continuous deployment and operations from pre- production environments through to production.\n\nSet up a production environment with: canary, live/live, or blue/ green deployment approaches.\n\nEstablish predictive, scaled growth, feature and configuration tracking, and continuous feedback.\n\nMaintain an inclusive monitoring strategy with complete and continuous feedback across all environments.\n\nIdentify the most commonly used features and functionality, updated and used across the stack and lifecycle.\n\nIdentify the “hot” areas: software, hardware, configurations, and so on.\n\nEstablish a production incident review process focused on learning and long-term stability.\n\n74\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nIntroduction Operations is often considered the most important part of the business, as this group is responsible for ensuring the Production team is able to provide the products and services that end users want and for maintaining very high uptime and an extremely low incident rate.\n\nSeveral organizations (possible even a majority) have sought to reduce overall operational expenses by outsourcing operations tasks to a partner in a long-term contract (often 10+ years), in exchange for a guaranteed savings rate per year.\n\nThere is nothing wrong with this strategy. That said, many organiza‐ tions have limited time and resources allotted for transitioning operations to the partner, and often that partner has not yet adopted Effective Performance Engineering practices.\n\nThat’s a recipe for less-than-desirable results, and often becomes a point of contention wherein both parties spend much energy and effort negotiating the contract rather than focusing on the end user.\n\nLet’s take a look at such a scenario, and how you can mitigate these challenges with Effective Performance Engineering practices.\n\nScenario Company 123 is a large international business with 9 reportable seg‐ ments (Agriculture and Nutrition, Nylon Enterprise, Performance Coatings and Polymers, Pharmaceuticals, Pigments and Chemicals, Pioneer, Polyester Enterprise, Specialty Fibers, and Specialty Poly‐ mers) spanning more than 70 countries worldwide. They make 49% of consolidated sales to customers outside the US, employ a total of 93,000 people, and have a $68 billion market cap. In addition, they operate the largest Electronic Data Interchange (EDI) operations and are the largest SAP customer in the world.\n\nThey are seeking to optimize their growing operations and support systems, as well as reduce the associated costs, while still maintain‐ ing the service level for their end users. Their intent is to partner and outsource the global information systems and technology infra‐ structures in order to provide selected applications, software serv‐ ices, and information systems solutions designed to enhance manufacturing, marketing, distribution, and customer service.\n\nProven Practices for Operations\n\n|\n\n75\n\nThe company is seeking to reach a 10-year agreement with a partner to transition all operations and support with a guaranteed savings of 10% per year for the 10 years of the initial contract term. Transition is scheduled to take place over a 3-month period and at least 70% of the existing company resources will transition to the new partner organization as part of this operations outsourcing agreement.\n\nChallenges\n\nIf you have ever been a part of something like this, then you know: the sheer size of this maneuver is scary. Now add the fact you are outsourcing your “global information systems and technology” busi‐ ness for 10 years to a partner (where your talent is going), and it gets exponentially more interesting.\n\nNeedless to say, this transition was not without several challenges, and we will see how this organization became a cautionary tale for the importance of implementing Effective Performance Engineering capabilities that benefit all parties. Of course, let’s not forget Com‐ pany 123 is nearly 100 years old, comprises 9 segments in 70+ coun‐ tries, serves 93,000 people, and has a $93 billion market cap.\n\nOption 1\n\nSingle partner.\n\nThe pros of Option 1 are as follows:\n\nSingle neck to wring if needed\n\nLittle ambiguity or dependency on partner’s ability to deliver on the contract to company\n\nTalent goes to a single-partner organization\n\nThe cons of Option 1 are as follows:\n\nLimited diversification, consolidated risk\n\nOption 2\n\nTwo partners.\n\nThe pros of Option 2 are:\n\nCompetitive environment\n\n76\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nDiversity of service, redundancy\n\nThe cons of Option 2 are:\n\nMultiple players\n\nIncreased possibility for finger-pointing\n\nTalent dispersed over two partner organizations\n\nOption 3\n\nHybrid (split: Company 123 resources and one or more partners).\n\nHere are the pros of Option 3:\n\nCompany can control speed of transition\n\nCompetitive environment\n\nIterative learning and feedback\n\nAnd here are the cons of Option 3:\n\nMay take longer to transition\n\nReduction of operational expense delayed\n\nMore company effort required\n\nRecommendation The recommendation is Option 3. Option 3 enables Company 123 to begin to realize the benefits and value of transitioning operations to a partner, and adapt and change their business practices over time, while observing the positive and negative effects on end users.\n\nSummary In reality, Company 123 moved forward with Option 2. As a result, Partner 1 contracted to operate a majority of Company 123’s global information systems and technology infrastructure, and provide selected application and software services; and Partner 2 contracted to provide information systems solutions designed to enhance Com‐ pany 123’s manufacturing, marketing, distribution, and customer service.\n\nProven Practices for Operations\n\n|\n\n77\n\nThe company reached a 10-year agreement with both partners to transition nearly all operations and support with a guaranteed sav‐ ings of 10% per year for the 10 years of the initial contract term. Transition was scheduled to take place over a 3-month period with at least 80% of the existing company resources transitioning to the new partner organization as part of this operations outsourcing agreement.\n\nDue to many of the identified “cons” of Option 2, this approach was riddled with challenges throughout the term of the contract, with the transition working out for one partner and not the other, and Company 123 ended up taking back much of the outsourced sup‐ port and operations. The end users suffered the most, followed by Company 123’s businesses; today, some 15 years later, the company has a $46 billion market cap—approximately 50% smaller than it had before the partnership.\n\nHow-To In this scenario, you can see why the proven practices of Perfor‐ mance Engineering for operations should play a significant role within an organization. These practices start with disaster recovery, capacity planning, and resiliency, which should all be known, tested, and observed for critical capabilities and functions. Continuous deployment and operations spanning from the earliest pre- production environments through to production enables you to test the deployments, control and protect the deployment procedures, and measure the quality of the release candidate. A production envi‐ ronment enabling canary,live/live, or blue/green-type deployment approaches can mitigate the risks associated with traditional, single- data-center, big-bang deployments. Predictive models allow you the chance to gain insight into how a capability will scale under growth, track features and configuration, and get continuous feedback from pre-production through production and back, supported by an inclusive monitoring strategy with complete and continuous feedback across all environments. Other practices include identifying the most commonly used features and functionality, in order to show when updates happen and how they’re used across and beyond the stack and lifecycle, and identifying where the “hot” areas are related to software, hardware, and configurations. Lastly, establish a solid cross-team production incident review process rooted in and focused on learning and the long-term stability of the team and all environ‐\n\n78\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nments, in order to make products and services continuously avail‐ able to the business and end users.\n\nFigure 3-4 shows the intersection of workflow, toolchain, and arti‐ facts, along with the automated flow from left to right of a common automated build/test/deploy cycle. It highlights the (often automa‐ ted) check and release-to-production steps, as well as the integration of performance considerations and feedback throughout the cycle.\n\nKey Implementation Considerations\n\nOrganizational\n\n— Who does delivery through DevTest versus ProdOps?\n\n— When a delivery goes wrong, who is responsible?\n\nHow are you measuring the quality of a release?\n\n— Stakeholders (business/technical/customer)\n\n— Are all parts considered?\n\n— Visualized and communicated\n\nCultural\n\n— What happens when you have a failed delivery?\n\n— Is rollback tested every time prior to your release?\n\n— What are your performance metrics?\n\n— Ops = Chaos\n\n— Frequency of alarms, acceptable\n\n— Performance Engineering teams > time in production\n\nTechnical\n\n— How are you mitigating quality and performance with fre‐\n\nquency of release?\n\n— Are you using canary deployment approaches to minimize\n\nimpact to production?\n\n— How are your blue/green releases?\n\n— Backup and recovery\n\n— Load-balance and CDN\n\n— End-to-end (client, server, network, and app layer)\n\nProven Practices for Operations\n\n|\n\n79\n\n80\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\ne l c y c\n\ny o l p e d / t s e t / d l i u b a n i\n\ns t c a f i t r a d n a\n\n,\n\nn i a h c l o o t\n\n,\n\nw o fl k r o w\n\nf o n o i t c e s r e t n I\n\n.\n\n4 - 3 e r u g i F\n\nCHAPTER 4 Tying It All Together\n\nAs more businesses experience devastating production incidents, they are recognizing that they need to change, and are working to implement Effective Performance Engineering practices. They’re restructuring their teams and redefining jobs such that some team members are focused on ensuring that the essential computer infra‐ structure and applications deliver good, stable performance at all times. They’re embracing practices in Performance Engineering and treating them as critical, adopting an organizational culture sup‐ porting this transformation, and rewarding individuals for their contributions.\n\nKeep in mind that Performance Engineering doesn’t refer only to a specific job, such as a “performance engineer.” More generally, it refers to the set of skills and practices that are gradually being understood across organizations that focus on achieving higher lev‐ els of performance in technology, in the business, and for end users.\n\nMany naive observers often take the same attitude toward Perfor‐ mance Engineering: it’s simply a matter of making sure the systems run fast. If possible, make them run really fast. When in doubt, just make them run really, really fast. And if that doesn’t work right away, throw money at the problem by buying more hardware to make the systems go really fast.\n\nBut just as there’s more to winning a track meet than being fast, there’s more to building a constellation of quick, efficient web servers and databases than being fast. Just as athletes can’t win without a sophisticated mixture of strategy, form, attitude, tactics,\n\n81\n\nand speed, Performance Engineering requires a good collection of metrics and tools to deliver the desired business results. When they’re combined correctly, the results are systems that satisfy both customers and employees, enabling everyone on the team to win.\n\nMetrics for Success One critical element of integrating a Performance Engineering cul‐ ture within an organization is to determine what performance met‐ rics you need to track and assess whether you can measure them with confidence.\n\nHow often do we hear development and testing organizations and even managers refer to lines of code written, scripts passed and exe‐ cuted, defects discovered, and test use cases as a measure of their commitment to software quality?\n\nAt the end of the day, these measurements are useless when it comes to delivering results that matter to your end users, that keep them coming back for more of your products and services. Think about it. Who cares how many defects you’ve found in pre-production? What does that measure?\n\nWe want to make a fairly bold statement: these old, standalone test metrics don’t matter anymore.\n\nWhen it comes to quality in development, testing, and overall oper‐ ations, these are the questions you should be asking yourself:\n\nHow many stories have we committed to?\n\nHow many of these were delivered with high quality to the end user?\n\nHow much time did it take to deliver from business or customer concept to production?\n\nFinally, ask yourself this: “Are our end users consuming the capabili‐ ties they asked for?”\n\nActivities Versus Results The difference between this sort of focus and a purely technical focus is the difference between activities and results. If our team commits to 80 story points at the beginning of a 4-week sprint but\n\n82\n\n|\n\nChapter 4: Tying It All Together\n\nwe deliver only 60, then we’re not meeting our commitment to our‐ selves, the business, or the customer. It also means our team is pre‐ venting another team from delivering on their commitments. The release will instead be put on hold and pushed to our next release. Ultimately, the business results are going to be less than what we promised.\n\nOver the last several years, improvements in development and test‐ ing have provided an opportunity for organizations to apply new metrics that can lead to genuine transformation. The most common of these proven concepts is Agile development practices. When executed well, Agile methods can enable a team to quickly deliver high-quality software with a focus on the highest priority for the business and end user. As teams transform, having a few key meas‐ urements and producing results helps the organization evolve in an informed manner, with continuous feedback from the investments they’re making.\n\nWithout these types of metrics, organizations will simply attempt their transformation blindly, with limited capacity to show results, including the business outcomes demanded of today’s technology organizations.\n\nTop Five Software Quality Metrics Here are the top five quality metrics that really matter:\n\nCommitted stories versus delivered results meeting doneness criteria\n\nRemember the last time someone committed to do something for you and either failed to deliver or didn’t meet your stand‐ ards? It caused delays and extra work, along with a lot of frus‐ tration. In software development, stories are the pieces of work that are committed to and, ideally, delivered on time and to a certain spec.\n\nAs you may know, stories represent the simple, high-level descriptions that form a use case, such as a user inserting a credit card into an airline kiosk. Each story needs to be deliv‐ ered at a specific level of quality or “doneness” criteria. As teams continuously plan, elaborate, plan again, commit, and deliver, the ultimate goal should be to deliver these results in alignment with the broader team’s doneness criteria. When that can be measured, the team can showcase its abilities to meet its com‐ mitments on schedule and with the highest standards.\n\nMetrics for Success\n\n|\n\n83\n\nQuality across the lifecycle\n\nThe demand for software delivery speed continues to increase along with the demand for reduced costs. But how can you ach‐ ieve these goals when you don’t have the time and resources to manually test every build? When you can’t afford to wait and find those defects in your late-stage releases? The answer is to follow the build lifecycle from story to code on a developer desktop. Next, you should check, build, and unit test. Continue by using automation through the rest of the process, including automated functional, performance, security, and other modes of testing. This enables teams to show the quality of a build throughout the lifecycle with quality metrics and automated pass/fail gates.\n\nGiven the frequency of these builds and automated tests, build- life results can be created and measured in seconds, minutes, and hours. Now, your most frequent tests are fully automated, and you’re only doing manual tests on the highest quality relea‐ ses that make it through the automated lifecycle. This results in automated build-life quality metrics that cover the full lifecycle, enabling your team to deliver with speed and quality, while reducing costs through higher efficiency.\n\nProduction incidents over time and recurrence\n\nJust as it’s important to show the quality of the release over time, it’s also important to minimize production incidents and their recurrence over subsequent releases. Table 4-1 illustrates a tech‐ nique we’ve used to compare team performance over time. Imagine you are working with five teams over three completed releases; this shows how an information radiator can be used with simple and minimal key data to visually represent impor‐ tant results, such as “% Commit Done” and “# Prod Incidents,” delivered across teams.\n\nThe target for this typical (though imaginary) organization is 95% of committed stories delivered and zero production inci‐ dents. Teams that didn’t meet these goals are highlighted in bold red. Often, production incident numbers are found within an incident management process. Defining the root cause and implementing corrective measures enables continuous improve‐ ment and prevents recurrence of the same issue in subsequent releases. With these quality metrics in place, you can learn\n\n84\n\n|\n\nChapter 4: Tying It All Together",
      "page_number": 72
    },
    {
      "number": 4,
      "title": "Tying It All Together",
      "start_page": 87,
      "end_page": 117,
      "detection_method": "regex_chapter",
      "content": "which teams meet specific goals. Finally, you can look across teams and discover why proven concepts work.\n\nTable 4-1. Using an information radiator to visualize results\n\nTeams\n\nReleases\n\n% Commit Done Alpha # Prod Incidents % Commit Done Beta # Prod Incidents % Commit Done Gamma # Prod Incidents % Commit Done Delta # Prod Incidents % Commit Done # Prod Incidents % Commit Done # Prod Incidents\n\nEpsilon\n\nTotals\n\nTeam Averages 2016-Jan 2016-Feb 2016-Mar 2016-Apr 98% 97% 0 2 95% 94.33% 2 6 100% 100% 0 1 100% 93.33% 0 2 95% 92.33% 0 1 97.6% 95.398% 2 12\n\n96% 1 92% 3 100% 0 100% 0 85% 0 94.6% 4\n\n97% 1 96% 1 100% 1 80% 2 97% 1 94% 6\n\nUser sentiment\n\nGet to know your end users by measuring how they feel when interacting with an application or system. By capturing and dis‐ secting the feedback they provide regarding new or improved capabilities, you can incorporate their needs into an upcoming sprint. At the very least, you can develop a plan to deliver some‐ thing in response to those needs.\n\nOn a larger scale, your analysis and incorporation of user senti‐ ment can expand to a more general market sentiment, which can broaden your impact and market presence. Several compo‐ nents of quality can be covered via this metric, including sim‐ plicity, stability, usability, and brand value.\n\nContinuous improvement\n\nFollowing retrospectives, allow time and effort to implement prioritized, continuous improvement stories. This enables the team to self-organize and be accountable for improving the quality of their process. When you allocate this time and make it visible to all, the team and stakeholders can show their imme‐ diate impact. They can demonstrate how one team, compared to others, has delivered results at increased speed, with higher quality and value to the end user. This allows team leads to ask\n\nMetrics for Success\n\n|\n\n85\n\nand possibly answer these questions: are there certain practices that need to be shared? How do teams perform over time with certain changes injected? The continuous improvement metric can also justify recent or proposed investments in the team.\n\nWhat Really Matters It’s amazing to see how many teams are still working the old- fashioned way. In fact, the empathy and sympathy poured out from others in the field is overwhelming. We hear and share the same sto‐ ries we shared 20+ years ago. For example, have you heard this lately, “I have 3,896 test cases, and I’m 30% complete on test execu‐ tion”? We should all ask, “So, what does that mean for time, quality, and cost, along with on-time delivery to the end user?” It’s genuinely shocking when we hear from a VP about their mobile-testing pro‐ cess, only to learn that the company’s mobile strategy is a “mobile guy” who does manual testing by putting the application on his phone and playing with it—maybe even wrapping it in aluminum foil and walking up and down some hills or taking the elevator to simulate real-world users and weak network conditions.\n\nLet’s start focusing on metrics that really matter. We need results that center on the value and quality we deliver to our end users. In the process, let’s not forget how to deliver. We need teams to con‐ tribute creatively and improve the practices they have, while meas‐ uring quality via metrics they can use to evaluate, modify, and improve processes over time.\n\nWhat happens when we insist on the old style of quality metrics?\n\nWell, for one thing, it helps explain why so many CIOs hold their positions for less than two years or why a third of them lose their jobs after a failed project. We’ve seen this before: a new CIO or senior leader comes in, fires a few mid-level managers, reorganizes a couple of things, and brings in a new partner, and suddenly they’re trying to measure results. Unfortunately, they don’t have the right metrics in place to show how the team is delivering. Command and control fails again. Sadly, this fails the business, shareholders, pas‐ sionate individuals, and ultimately the end user: the customer.\n\nYou do not want to fail your customer.\n\n86\n\n|\n\nChapter 4: Tying It All Together\n\nOther Performance Engineering Metrics The top five quality metrics are a foundational and important start‐ ing point for Effective Performance Engineering. In addition, there are a variety of other Performance Engineering metrics that come into play:\n\nRelease quality\n\nThroughput\n\nWorkflow and transaction response time\n\nAutomated performance regression success rate\n\nForecasted release confidence and quality level\n\nBreaking point versus current production as a multiplier\n\nDefect density\n\nThis drive to explore new metrics and find better ways of under‐ standing how software is succeeding (and failing) is going to con‐ tinue and grow even more intense. Software engineers understand that it’s not enough to simply focus on the narrow job of going fast. The challenge is capturing just how the software is helping the com‐ pany, its employees, and its customers. If they succeed, then the soft‐ ware is a success.\n\nThere are big differences in the ways companies are approaching the challenge. They’re mixing enterprise, commercial, and open source tools, and using a wide range of metrics to understand their results. We’ve seen key metrics that are accepted by all groups of stakehold‐ ers—metrics that all businesses can start using today. However, there’s nothing like enabling the team to also measure what matters to them, because what matters to your team may matter deeply to your success.\n\nAutomation Automation can mean different things to different people. In this section, we explore why performance testing is not enough, investi‐ gate the four key areas to focus on as a performance engineer, and discuss how to apply these practices in the real world. You will see how automation plays a critically important role in Performance Engineering.\n\nAutomation\n\n|\n\n87\n\nPerformance Testing Isn’t Enough Software, hardware, and the needs of application users have all changed radically in recent years, so why are the best practices many developers use to ensure software quality seemingly frozen in time? The world has evolved toward Performance Engineering, but too many developers still rely on performance testing alone. This can lead to disaster.\n\nThe initial failures of the Healthcare.gov website revealed how frag‐ ile underlying systems and integrated dependencies can be. Simple performance testing isn’t enough. If you don’t develop and test using a Performance Engineering approach, the results can be both costly and ineffective.\n\nWhat went wrong with Healthcare.gov? A report in Forbes cited these eight reasons for the site’s massive failure:\n\nUnrealistic requirements\n\nTechnical complexity\n\nIntegration responsibility\n\nFragmented authority\n\nLoose metrics\n\nInadequate testing\n\nAggressive schedules\n\nAdministrative blindness\n\nPresident Obama, the CEO in this scenario, received widespread criticism over the troubled launch, which should have been a high point for his presidency. Instead, the site’s poor performance tainted the public’s perception of the program. When you embarrass your boss, you don’t always get a second chance. In the case of Health‐ care.gov, the Obama administration had to bring in new blood.\n\nSo, how do failures like this happen?\n\nWhen developers and testers were working in a mainframe or client-server environment, the traditional performance testing prac‐ tices were good enough. As the evolution of technology accelerated, however, teams have had to work with a mix of on-premises, third- party, and other cloud-based services, and components over which\n\n88\n\n|\n\nChapter 4: Tying It All Together\n\nthey often have little or no control. Meanwhile, users increasingly expect quick access anywhere, anytime, and on any device.\n\nFour Key Areas of Focus Performance Engineering practices help developers and testers solve these problems and mitigate risks by focusing on high performance and delivering valuable capabilities to the business.\n\nThe key is to start by focusing on four key areas:\n\nBuilding in continuous business feedback and improvement. You accomplish this by integrating a continuous feedback and improvement loop into the process right from the beginning.\n\nDeveloping a simple and lightweight process that enables auto‐ mated, built-in performance. In this way, the application, sys‐ tem, and infrastructure are optimized throughout the process.\n\nOptimizing applications for business needs.\n\nFocusing on quality.\n\nApplying the four key areas\n\nYour team can head off unrealistic requirements by asking for and using feedback and improvement recommendations. To avoid technical complexity, your team must share a common goal to quickly define, overcome, and verify that all systems are engineered with resiliency and optimized for business and customer needs. Integration responsibility must be built into all environments, along with end-to-end automated performance validation. This should even include simulations for services and components that are not yet available.\n\nThe issue of fragmented authority won’t come up if you create a col‐ laborative and interactive team, and you can avoid the problem of loose metrics by using metrics that provide stakeholders the infor‐ mation they need to make informed business decisions. Inadequate testing will never be an issue if you build in automated testing, including functional testing for:\n\nPerformance\n\nSecurity\n\nAutomation\n\n|\n\n89\n\nUsability\n\nDisaster recovery\n\nCapacity planning\n\nOverly aggressive schedules are unlikely to occur if you provide automated quality results reports that highlight risks and offer opti‐ mization recommendations to support informed decision making. Finally, to prevent administrative blindness, focus on business out‐ comes, communicate with all stakeholders throughout the process, and build in accountability and responsibility for delivery.\n\nIt’s your responsibility to ensure that your organization is moving from antiquated methodologies based on performance testing only to more comprehensive Performance Engineering practices. After all, no one wants to be the next Healthcare.gov.\n\nBig Data for Performance Performance Engineering has long been a practice adopted in the world of high-performance automotive. One of the results we often see in our “Performance Engineering” Google Alert is Lingenfelter Performance Engineering. When you go to the “About us” section of their website, it states:\n\nLingenfelter Performance Engineering was founded over 43 years ago and is a globally recognized brand in the performance engi‐ neering industry. The company offers engine building, engine and chassis tuning components and installation for vehicle owners; component product development; services to manufacturers, after‐ market and original equipment suppliers; prototype and prepara‐ tion of product development vehicles; late product life-cycle performance improvements; durability testing; and show and media event vehicles.\n\nLooking at high-performance automotive organizations like Lingen‐ felter (and many others), it is easy to see a direct correlation between all of the components and engineered elements that make a high- performance automobile and these of our business systems (and between their drivers and our end users). The parallel that we want you to recognize is the now available “Big Data for Performance,” which the high-performance automotive industry has been leverag‐ ing for many years, yet we as Performance Engineers are only start‐ ing to utilize. This big data and the accompanying predictive analytics, both of which leverage the capabilities of Performance\n\n90\n\n|\n\nChapter 4: Tying It All Together\n\nEngineering, will enable us to best support our businesses and end users through technology.\n\nTo finish out this analogy, do organizations like Lingenfelter only wait until final deployment to see how the automobile they are optimizing will perform? No, they have adopted practices for look‐ ing as a team at each component along the way, making decisions, and optimizing the components based on data to ensure they are high quality.\n\nPerformance as a Team Sport Over the last few years, organizations have started to define and embrace the capabilities of Performance Engineering, recognizing that their systems are growing so complex that it’s not enough to simply tell the computers or the individuals behind them to “run fast.” This capability must be built into the organization’s culture and behavior, and it must include activities for developers, database administrators, designers, and all stakeholders—each coordinating to orchestrate a system that works well, starting early in the lifecycle and building it in throughout. Each of the parts may be good enough on its own, but without the attention of good engineering practices, they won’t work well enough together.\n\nMarket Solutions As you look across the market, you will see there are a number of analysts, partners, and software tool vendors actively marketing their Performance Engineering capabilities.\n\nTo simplify the decision-making and implementation process for you, we’ve provided some Performance Engineering topics with links to key information at http://www.effectiveperformanceengineer ing.com.\n\nIn addition, we’ve included the results of a Performance Engineering survey that gives a lot more detail about what is going on in the market now.\n\nMarket Solutions\n\n|\n\n91\n\nPerformance Engineering Survey Results Hewlett Packard Enterprise has been working to support Perfor‐ mance Engineering in all organizations. In 2015, it contracted You‐ Gov, an independent research organization, to survey 400 engineers and managers to understand how organizations are using tools and metrics to measure and evolve their Performance Engineering prac‐ tices. The survey was conducted blind so that no one knew that Hewlett Packard Enterprise commissioned it.\n\nThe sample consisted of 50% performance engineers and perfor‐ mance testers, 25% application development managers, and 25% IT operations managers. All came from companies with at least 500 employees in the US. The results reveal a wide range of techniques and broad approaches to Performance Engineering and some of the practices through which organizations are using tools and metrics.\n\nThe survey asked, “When you look to the future of Performance Engineering, what types of tools do you and your stakeholders plan to acquire?” In response, 52% of large companies (those with 10,000+ employees) indicated “more enterprise and proven” tools; 37% of the larger companies said they expected “more open source and home-grown”; and the remaining 11% said they were planning “more hybrid of open source and enterprise.” The responses from companies of different sizes followed a similar pattern, but with a bit more balance (see Figure 4-1).\n\nWhen the results were analyzed based on roles, the majority of respondents planned to acquire “more enterprise and proven” tools, with those identifying as “performance engineer/performance tester” (41%), application development manager (44%), and IT operations manager (51%), as shown in Figure 4-2.\n\nWhen it comes to testing, an increasing number of companies are concentrating on burst testing to push their software closer to the breaking point. They’re spinning up a large number of virtual users and then pointing them at the systems under test in a large burst over a period of time. This simulates heavy traffic generated from sales, promotions, big events, or retail days like Black Friday or Cyber Monday, when a heavy load can wreak havoc on a system (Figure 4-3).\n\n92\n\n|\n\nChapter 4: Tying It All Together\n\nMarket Solutions\n\n|\n\ne z i s n o i t a z i n a g r o y b n o i t i s i u q c a l o o t\n\ne r u t u F\n\n.\n\n1 - 4 e r u g i F\n\n93\n\n94\n\n|\n\nChapter 4: Tying It All Together\n\ne l o r b o j\n\ny b n o i t i s i u q c a l o o t\n\ne r u t u F\n\n.\n\n2 - 4 e r u g i F\n\nMarket Solutions\n\n|\n\ne z i s n o i t a z i n a g r o y b g n i t s e t\n\nt s r u B\n\n.\n\n3 - 4 e r u g i F\n\n95\n\nOne of the most important options among tools like the ones just cited is the ability to deploy an army of machines to poke and prod at an organization’s systems. The cloud is often the best source for these machines, because many modern cloud companies rent virtual machines by the minute. Those working on performance tests can start up a test for a short amount of time and pay only for the minutes they use.\n\nThe value of the cloud is obvious in the answers to the questions about the average size and duration of a load test. Only 3% of respondents reported testing with fewer than 100 simulated users. At least 80% of the respondents used 500 or more users, and 14% wanted to test their software with at least 10,000 users. They feel that this is the only way to be prepared for the number of real users com‐ ing their way when the software is deployed (Figure 4-4).\n\nGrowth in load testing points to the cloud.\n\nThis demand will almost certainly increase. When asked how big they expect their load tests to be in just two years, 27% of respond‐ ents said that they expect they’ll need at least 10,000 simulated users. They mentioned much larger numbers, too; 8% predicted they’ll be running tests with more than 100,000 simulated users, and 2% could foresee tests with 500,000 users or more.\n\nWhile the number of simulated users is growing, duration isn’t long enough to make a dedicated test facility economical. The tests are usually not very long; only 8% reported running tests that routinely lasted more than 24 hours. Most of the survey respondents (54%) said that their tests ran between 4 and 12 hours (Figure 4-5).\n\nThe largest companies are also the ones that are most likely to be using the cloud. Only 9% said that they don’t use the cloud for test‐ ing, typically because their security policies didn’t permit them to expose their data to the cloud (Figure 4-6).\n\n96\n\n|\n\nChapter 4: Tying It All Together\n\nMarket Solutions\n\n|\n\ne z i s n o i t a z i n a g r o y b e z i s\n\nt s e t d a o l\n\nm u m i x a M\n\n.\n\n4 - 4 e r u g i F\n\n97\n\n98\n\n|\n\nChapter 4: Tying It All Together\n\ne z i s n o i t a z i n a g r o y b n o i t a r u d m u m i x a M\n\n.\n\n5 - 4 e r u g i F\n\nMarket Solutions\n\n|\n\ne t i s n o i t a z i n a g r o y b s r e d i v o r p e c i v r e s d u o l c\n\nf o e s U\n\n.\n\n6 - 4 e r u g i F\n\n99\n\nHow to Choose a Solution Now comes the time for you to start defining what a solution looks like for you. As you begin, we suggest you take a three-step approach: define your goals and objectives, define a timeline, and identify partners. In the following sections, we go through this pro‐ cess in a bit more detail, so you can get started on your path to choosing your Performance Engineering solution.\n\nDefine your goals and objectives\n\nTransforming is a complex exercise and one that should have some thought behind it. When thinking about goals and objectives, begin with five key aspects of your teams and organization:\n\nCulture\n\nTechnology\n\nSpeed\n\nQuality\n\nCost\n\nEach of these considerations factors into the overall goals and objec‐ tives for Effective Performance Engineering, and decisions must be made now.\n\nIt is a journey and will take some time, and the path will not always be straight; however, getting started in a focused area with some support, adopting a few key practices, and sharing the results is the right approach.\n\nCelebrate the success, examine the results, and then continue along the journey, never losing sight of the end user. With this collabora‐ tion and continued guidance and direction, you’ll attain success and make forward progress, as you transform into an Effective Perfor‐ mance Engineering organization.\n\nDefine your timeline\n\nTimelines are relative. By contrast, value to your end users and stakeholders is more objective and within your control, so you should focus on defining what is important there before setting your timelines.\n\n100\n\n|\n\nChapter 4: Tying It All Together\n\nFrom a purely leadership and budget/time perspective, it is impor‐ tant to define a timeline with clear goals and objectives within the given budgetary cycle. Doing so enables you to share and communi‐ cate results delivered in the prior period, activities being performed in the current period with their forecasted results, and commit‐ ments for the future period with their forecasted results.\n\nA timeline should visually represent key milestones along with incremental measures indicating what should be achieved within these milestones. It should also map each task or activity to the value it will deliver to the end user and business.\n\nFigure 4-7 illustrates what this could look like at a high level for you and your organization on your journey to Effective Performance Engineering.\n\nIdentify your partners\n\nPartners and thought leaders are often a great resource to provide additional insight, experience, and practical advice from the market, in order to get you aligned with current trends and able to accelerate as desired.\n\nNext we take a deeper look at some thought leaders, consulting part‐ ners, and analyst partners, with specific details and links to existing capabilities and assets, so you can quickly get a better and broader idea of some of the Effective Performance Engineering resources available to you.\n\nTop thought leaders of today Microsoft, Google, IBM, Apple, and Hewlett Packard Enterprise comprise the set of top five thought leaders and influencers around Performance Engineering and testing today.\n\nThis set of five is consistent by audience (performance engineers/ testers, application development managers, and IT operations man‐ agers), as well as by organization size (Figure 4-8).\n\nMarket Solutions\n\n|\n\n101\n\n102\n\n|\n\nChapter 4: Tying It All Together\n\ng n i r e e n i g n E e c n a m r o f r e P\n\nf o n o i t u l o v E\n\n.\n\n7 - 4 e r u g i F\n\nFigure 4-8. Top thought leaders\n\nPreferred partners for Performance Engineering Accenture, Infosys, Deloitte, HCL, and Tech Mahindra are the top five service providers most often chosen as Performance Engineer‐ ing partners by the organizations surveyed.\n\nNote that “None of the Above” only represented 7% of all other responses (Figure 4-9).\n\nFigure 4-9. Preferred partners\n\nConclusion Performance Engineering practices define a culture that enables teams to deliver fast, efficient, and responsive systems architected for large-scale populations of customers, employees, regulators, managers, and more. The careful application of these principles makes it possible for corporations to please customers, support employees, and boost revenues, all at the same time.\n\nConclusion\n\n|\n\n103\n\nThere is more to Performance Engineering than just testing. Done right, Performance Engineering means understanding how all the parts of the system fit together, and building in performance from the first design.\n\nMaking the journey from performance testing to Performance Engi‐ neering isn’t easy. But the proven practices established over years of observation can help you on your way.\n\nThe Path to Performance Engineering One of the first tasks that budding programmers are given is to write a program that produces the text “Hello world.”\n\nNext you start to play with the program and try to do more, to see how quickly it delivers data or answers queries, and try to optimize for the highest performance with the least amount of code. The requests come in, the responses go out, and you see results on a screen. Take this and add a long-time run script for performance testing, a script you run every time you push out your latest release. It’s pretty easy when you’re the author and the user.\n\nPerformance Engineering, though, is a broad set of processes, and it’s also a culture. Performance Engineering is an art based on years of observation that have led to proven practices.\n\nBut moving from performance testing to Performance Engineering isn’t an easy process. The team must be ready to move from simply running a checkbox performance test script and focusing on parts to studying the way that all parts of the system work together. These pieces encompass hardware, software, configuration, performance, security, usability, business value, and the customer. The process is about collaborating and iterating on the highest-value items, and delivering them quickly, at high quality, so you can exceed the expectations of your end user.\n\nHere’s a roadmap for making the trip from performance testing to Performance Engineering. Essentially, these are the steps to become a hero and change agent—and how you can enable your organiza‐ tion to deliver with proven Performance Engineering practices and the accompanying culture.\n\n104\n\n|\n\nChapter 4: Tying It All Together\n\nDefine a culture\n\nThe success of a team depends heavily on the way leaders are nur‐ turing the professional environment and enabling individuals to col‐ laborate. Building this type of environment will inspire the formation of cross-functional teams and logical thinking.\n\nBuild a team\n\nA Performance Engineering team means that technology, business, and user representatives work together. They focus on the perfor‐ mance nature of everything they’re working on and figure out together how they can build in these capabilities. They need to know what specific area to focus on first, as well as how to measure along the way. They need to agree on the desired outcome. They must constantly remind themselves that the end goal of adopting Perfor‐ mance Engineering is to benefit the organization and end user.\n\nChoose metrics\n\nWe often encourage teams to start with a manual metrics process, perhaps a whiteboard (we know, not really high tech for a technolo‐ gist) and a few key metrics, then measure them over time and see why they matter (or don’t). You’ll quickly get a core set of metrics that matter for you and your organization, which have grown out of your cross-functional teams. Your people have the passion and understanding behind these, so trust them. They offer a good way to judge results against the desired outcome.\n\nOnce you have figured out enough of this manually, and individuals are starting to adopt and believe in them, take a look at your existing technology capabilities and see how you can get to automated reporting of these results fairly simply. These metrics will be key to your way of measuring what you do and the results you’re able to deliver. Make sure you have a solid baseline, and take regular measurements.\n\nAdd technology\n\nPerformance Engineering requires a new way of thinking, related to your existing software and infrastructure, including the existing tools and capabilities. This is how you shape and form quick, auto‐ mated results.\n\nConclusion\n\n|\n\n105\n\nDefine what your scope of effort is going to be and quickly learn what technology capabilities you already have available to you and your team. This will be an interesting experience, because you’ll learn about the capabilities that other siloed teams have available to them. Now, with a shared vision of how you want to deliver Per‐ formance Engineering throughout the organization, you can lever‐ age the technology to launch a single approach that aggregates these capabilities.\n\nPerhaps there are a few technology areas you want to start thinking about from the lifecycle virtualization space, such as user virtualiza‐ tion, service virtualization, network virtualization, and data virtuali‐ zation. These are the core capabilities that will enable your team to accelerate the transformation to Performance Engineering.\n\nBuild in telemetry\n\nNow that you’ve started with culture, team, and technology, it’s time to start integrating the telemetry and its data.\n\nFor example, how are you capturing the APM (application perfor‐ mance monitoring) data from production, and how about pre- production? Can you begin to examine these results and understand more about the behavior patterns of your users, systems, and trans‐ actions? From a cross-functional perspective, this will also pique the interest of the IT operations manager; so you’ll continue to broaden your network, and you’ll enable them to reduce the number of pro‐ duction incidents. This is just one example.\n\nThink about other quick wins or simple integrations for your exist‐ ing technology that will enable you to build more bridges. Correlate these types of results across your team so you can promote the cul‐ ture and desired outcomes of Performance Engineering by building in telemetry.\n\nLook for indirect metrics\n\nThere are hundreds of metrics available that you can use to estimate the success of a new capability or feature being released. As systems take on more roles inside a company, metrics that track perfor‐ mance become more readily available, and these enable you to begin partnering with your business peers to find out what metrics they watch and how they get these results.\n\n106\n\n|\n\nChapter 4: Tying It All Together\n\nStart looking at and asking about indirect metrics within the busi‐ ness that would show results related to revenue, customers (attrac‐ tion and retention), competitive advantage, and brand value. These are important to measure as you make the transition to Performance Engineering.\n\nFocus on stakeholders\n\nGet to know your stakeholders. Who on your team has the most interest in delivering the highest-value items to the end user most quickly and with great results? Find these people and get to know them well. Remember, you’re looking for your executive-level spon‐ sors and peer champions, so you can transform the practices and culture of an organization to become a Performance Engineering delivery machine.\n\nStart gathering information and sharing initial prototypes for the type of results, reports, and dashboards you want to show to your stakeholders on a regular basis. Typically, this would be a monthly show-and-tell exercise; however, as it matures it may become a set of automated results delivered with every build, consistently available if stakeholders want to review it. Also, you should consider regular, quarterly presentations to the executive board in which you share last quarter’s results, talk about the current quarter, and seek fund‐ ing for the next one.\n\nStay focused. Remember your objective. Find your champions. Deliver results.\n\nCreate stable environments\n\nOne of the earliest challenges will involve enabling teams with the capabilities they require. Some of this will come as you build these teams and the cross-functional tools, capabilities, and associated skills come together. But in the beginning, having a “like produc‐ tion” environment for Performance Engineering is key.\n\nBy leveraging the aforementioned lifecycle virtualization—including user virtualization, service virtualization, network virtualization, and data virtualization—you can quickly re-create production envi‐ ronments at a significant fraction of the cost, and you can duplicate them as many times as required. There are several other stable envi‐ ronment proven practices that have emerged along the way, which you can also learn and share through others.\n\nConclusion\n\n|\n\n107\n\nCelebrate wins\n\nRemember the old forming, storming, norming, and performing program developed by Bruce Tuckman? He believed these were the four phases necessary to building teams. If you’re a leader or a team member, you’ll see this in action.\n\nIt’s important to remember why you’re doing this, and know it’s all part of the transformation. Stay focused on the business and end- user objectives, so you can measure your progress and keep your eye on the prize.\n\nJust imagine what it will be like once you have delivered these capa‐ bilities to your end user. Conduct proper retrospectives, track your progress with your metrics, and celebrate the wins!\n\nAdd gamification\n\nAs you mature the capabilities just listed, think about how you can add gamification into the results. In other words, how do you make the results you’re delivering fun and visual, and how do you make a positive impact on your end users and the organization in the process?\n\nRajat Paharia created the gamification industry in 2007. In his book Loyalty 3.0 (McGraw-Hill) Rajat explains, “how to revolutionize customer and employee engagement with Big Data and gamifica‐ tion” and defines these “10 key mechanics of gamification”:\n\n1. Fast feedback\n\n2. Transparency\n\n3. Goals\n\n4. Badges\n\n5. Leveling up\n\n6. Onboarding\n\n7. Competition\n\n8. Collaboration\n\n9. Community\n\n10. Points\n\n108\n\n|\n\nChapter 4: Tying It All Together\n\nOf course, you also want to ensure that you highlight the opportuni‐ ties for improvement and show the wins and losses. You can also gamify Performance Engineering itself at a team level to encourage a little healthy competition within your group, and well beyond, then broadly share the results. This also enables you to leverage these results as information radiators for all stakeholders, showing how teams, systems, and applications are performing against defined baselines and goals.\n\nStart small\n\nWhen you first begin to incorporate Performance Engineering, you may be tackling a long-neglected maintenance list, or a new, up- and-coming hot project. Either can benefit from the focus of a Per‐ formance Engineering culture. Don’t try to take on too much at first.\n\nAs you begin to elaborate on your requirements, stories, and fea‐ tures, it’s important to remember that your whole team is working to define the what, why, and how of each item. As you continue down the Performance Engineering path, you will learn from each other’s domain expertise,” keeping in mind these learnings and results are from small experiments to show quick incremental value.\n\nStart early\n\nPerformance Engineering works best when the team starts thinking about it from the beginning. The earlier the team begins addressing performance in the product lifecycle, the likelier it is that the final system will run quickly, smoothly, and efficiently. But if it can’t be done from the very beginning, it’s still possible to add the process to the redesign and reengineering work done to develop the next itera‐ tion or generation of a product.\n\nConclusion\n\n|\n\n109\n\nAbout the Authors\n\nTodd DeCapua is the Chief Technology Evangelist with Hewlett Packard Enterprise and cofounder of TechBeacon.com thought leadership site for IT Heros.\n\nDeCapua is a seasoned software professional with 20+ years of expe‐ rience in IT applications development, IT operations, technology integrations, channels operations, and business development in sev‐ eral domains, including Mobile, Agile, Cloud, and Performance.\n\nOver the years Todd has transformed three organizations to Agile/ DevOps, consulted with 100+ organizations worldwide, and amassed a variety of perspectives and practical experiences. He has earned an MBA in Finance and a BS; has been recognized with sev‐ eral industry certifications and awards; and is an industry-renowned leader, speaker, and author.\n\nShane Evans is an experienced IT Manager with over 12 years in the industry. His primary focus has been Performance Engineering and Performance Management, and he spent 7 years managing these for a major financial institution in Canada before joining Hewlett- Packard in 2009 as a Presales Solution Architect. After three years in the field helping ensure the success of customers across the country, he is now part of the Product Management team. Shane is an active member of the Performance Engineering community, and regularly contributes to the discussions on the HP Forums as well as Google Groups, Yahoo!, and LinkedIn.\n\nAcknowledgments\n\nWe recognize Performance Engineering as both an art and a science. Thank you to those with whom we have been able to practice our art, and to those who continue to define the science with us.\n\nThis book is dedicated to our families, friends, and colleagues.\n\nPage Number\n\nComment/Key Learning/Action\n\nFor more information, join us online:\n\nhttp://www.EffectivePerformanceEngineering.com • @EffPerfEng on Twitter\n\nNotes\n\n111",
      "page_number": 87
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Effective Performance Engineering\n\nTodd DeCapua & Shane Evans",
      "content_length": 61,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Effective Performance Engineering\n\nTodd DeCapua and Shane Evans\n\nBeijing Beijing\n\nBoston Boston\n\nFarnham Sebastopol Farnham Sebastopol\n\nTokyo Tokyo",
      "content_length": 147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Effective Performance Engineering by Todd DeCapua and Shane Evans\n\nCopyright © 2016 O’Reilly Media, Inc. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://safaribooksonline.com). For more sales department: 800-998-9938 or corporate@oreilly.com.\n\ninformation,\n\ncontact our\n\ncorporate/institutional\n\nEditor: Brian Anderson Production Editor: Colleen Lobner Copyeditor: Colleen Toporek Proofreader: Rachel Monaghan\n\nInterior Designer: David Futato Cover Designer: Randy Comer Illustrator: Rebecca Demarest\n\nJune 2016:\n\nFirst Edition\n\nRevision History for the First Edition 2016-05-16: First Release 2016-07-11: Second Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491950869 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Effective Perfor‐ mance Engineering, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nWhile the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is sub‐ ject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-491-95086-9\n\n[LSI]",
      "content_length": 1848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Table of Contents\n\n1. Getting Started. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 What Is Effective Performance Engineering? 2 Why Is Effective Performance Engineering Necessary? 7 Focusing on Business Need 18\n\n2. Overview of Performance Engineering. . . . . . . . . . . . . . . . . . . . . . . . . 21 Performance Engineering Throughout the Lifecycle 21 Stakeholders 50 Building in Performance 52\n\n3. Proven Practices of Performance Engineering. . . . . . . . . . . . . . . . . . 61 Requirements, Architecture, and Design 61 Proven Practices for DevTest 67 Proven Practices for Operations 74\n\n4. Tying It All Together. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Metrics for Success 82 Automation 87 Market Solutions 91 Conclusion 103\n\nv",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "CHAPTER 1 Getting Started\n\nHow do we get started with Effective Performance Engineering?\n\nLet’s start by defining it. When speaking with different individuals and organizations, we’ve found the definition of “Effective Perfor‐ mance Engineering” or “Performance Engineering” varies greatly, so we wanted to define it upfront.\n\nPerformance Engineering represents a cultural shift in the way organizations view their essential processes. It embraces practices and capabilities that build quality and performance throughout an organization. This enables organizations to increase revenue, cus‐ tomer attraction and retention, brand value, and competitive advan‐ tage—all while focusing on meeting and exceeding the expectations of their end users.\n\nLet’s go back to see where performance was first introduced in the modern computer era in technology history: when the Z1 was cre‐ ated by German Konrad Zuse in his parents’ living room between 1936 and 1938. The Z1 is considered to be the first electro- mechanical binary programmable computer, and the first really functional modern computer. This marked the start of “Perfor‐ mance Engineering” for hardware and software related to the modern computer, some nearly 80 years ago.\n\nPrior to the first functional modern computer, there are many examples of other performance-related topics associated with crops, livestock, medicine, mechanics, and plenty more. As with anything the challenge remains similar, but the practices and capabilities\n\n1",
      "content_length": 1493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "change. Many of these practices were handed down from mentor to mentee and through apprenticeship or learned through individual real-world experiences.\n\nWe will take a look at the Effective Performance Engineering of today, and how it relates to future technology and business capabili‐ ties (including computer and associated hardware and software) to serve the end user, and several other impacting factors and opportu‐ nities as we continue to accelerate.\n\nWhat Is Effective Performance Engineering? While Performance Engineering is often defined narrowly as ensur‐ ing that nonfunctional requirements are met (such as response times, resource utilization, and throughput), the trend has moved toward a much broader application of the term.\n\n“Performance Engineering” doesn’t refer only to a specific role. More generally, it refers to the set of skills and practices that are gradually being understood and adopted across organizations that focus on achieving higher levels of performance in technology, in the business, and for end users.\n\nPerformance Engineering embraces practices and capabilities that build in quality and performance throughout an organization, including functional requirements, security, usability, technology platform management, devices, third-party services, the cloud, and more.\n\nStakeholders of Performance Engineering run the gamut, including Business, Operations, Development, Testing/Quality Assurance, and End Users.\n\nWe’ll explore several different facets of Performance Engineering in this book, providing a well-rounded overview of the practices and capabilities that make up Effective Performance Engineering.\n\nHardware The traditional goal of Performance Engineering between the 1970s and the late 90s was to optimize the application hardware to suit the needs of the business or, more accurately, the IT organization that provided the services to the business. This activity was really more of a capacity planning function, and many teams charged with car‐\n\n2\n\n|\n\nChapter 1: Getting Started",
      "content_length": 2033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "rying the mantle of performance reported to operations or infra‐ structure teams. Some still do (and that’s okay).\n\nAs hardware became more commoditized and the adoption of vir‐ tual infrastructure and “the cloud” more prevalent, this function took a backseat to development in an effort to deliver business applications and changes faster. It isn’t uncommon now for teams to have multiple environments to support development, test, produc‐ tion, and failover. While certainly more cost-effective than ever, virtualization has given us the false sense that these environments are free.\n\nThe cloud allows service providers to charge a premium for com‐ puter power in exchange for the promise of higher uptime, higher availability, and virtually unlimited capacity. However, the cloud doesn’t promise an optimal user experience. Applications need to be optimized for the cloud in order to maximize the potential return on investment.\n\nSoftware Over the last 30 years, software has transformed from monolithic to highly distributed, and even the concept of model-view-controller (MVC) has evolved to service-oriented (SOA) and micro-service architectures, all in an effort to reduce the number of points of change or failure, and improve the time-to-value when new func‐ tionality is implemented. Isolating components also allows develop‐ ers to test their discrete behavior, which often leaves end-to-end integrated testing out of scope. The assumption here is that if every component behaves as it should, the entire system should perform well. In an isolated environment this may be true, but there are many factors introduced when you’re building large-scale dis‐ tributed systems that impact performance and the end-user experi‐ ence—factors that may not be directly attributed to software, but should be considered nonetheless, such as network latency, individ‐ ual component failures, and client-side behavior. It is important to build and test application components with all of these factors rep‐ resented in order to optimize around them.\n\nCulture Every organization and group has a mission and vision. While they strive to attain these goals, performance becomes implied or\n\nWhat Is Effective Performance Engineering?\n\n|\n\n3",
      "content_length": 2231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "implicit. But performance needs to be a part of all decisions around the steps taken to achieve a goal; it forms the basis of how an organi‐ zation will embody Performance Engineering throughout their cul‐ ture to achieve their mission and vision.\n\nWe need to treat performance as a design principle, similar to decid‐ ing whether to build applications using MVC or micro-services architectures, or asking why a new epic (or the relative size of a requirement, in Agile terminology) is important to the business, and how performance with the business/technology/end user will make a difference for all stakeholders. Performance needs to be an over‐ arching requirement from the beginning, or we have already started on the wrong foot.\n\nIn order to build a culture that respects the performance require‐ ments of the organization and our end users, there needs to be some incentive to do so. If it doesn’t come from the top down, then we can take a grassroots approach, but first we need to quantify what per‐ formance means to our business, users, and team. We must under‐ stand the impact and cost of every transaction in the system, and seek to optimize that for improved business success.\n\nThroughout this book are sidebars in which we look at five compa‐ nies and examine how performance is built in to their culture. These organizations are from a variety of industries and verticals, showing the diversity of how a performance culture drives everything a busi‐ ness does, enabling it to deliver amazing results. Here are the five companies we will look at in more detail:\n\nGoogle\n\nWegmans\n\nDreamWorks\n\nSalesforce\n\nApple\n\nThe key takeaway here should be that performance is everyone’s responsibility, not just the developers', the testers', or the operations team’s. It needs to be part of our collective DNA. “Performance First” can be a mantra for every stakeholder.\n\n4\n\n|\n\nChapter 1: Getting Started",
      "content_length": 1907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Google: A Performance Culture Based on 10 Things Within Google’s “About Google” section is an area titled “What we believe,” in which the “Ten things we know to be true” live. There are items written when the company was a few years old, which they continue to revisit to hold each other accountable.\n\nThere is a classic video from Google I/O 2014 where Paul Lewis and Lara Swanson talk about the performance culture at Google. Part of the abstract from this video states, “We can be deliberate about per‐ formance and mobile web, make smart use of performance moni‐ toring tools, and cultivate a social atmosphere of collaboratively improving performance for our mobile users.”\n\nTim Kadlec shared his notes on the video:\n\n34% of U.S. adults use a smartphone as their primary means of Internet access.\n\nMobile networks add a tremendous amount of latency.\n\nWe are not our end users. The new devices and fast net‐ works we use are not necessarily what our users are using.\n\n40% of people abandon a site that takes longer than 2–3 sec‐ onds to load.\n\nPerformance cops (developers or designers who enforce performance) is not sustainable. We need to build a perfor‐ mance culture.\n\nThere is no “I” in performance. Performance culture is a team sport.\n\nThe first step is to gather data. Look at your traffic stats, load stats and render stats to better understand the shape of your site and how visitors are using it.\n\nConduct performance experiments on your site to see the impact of performance on user behavior.\n\nTest across devices to experience what your users are experi‐ encing. Not testing on multiple devices can cost much more than the cost of building a device lab.\n\nAdd performance into your build tools to automatically per‐ form optimizations and build a dashboard of performance metrics over time. Etsy notifies developers whenever one of the metrics exceeds a performance goal.\n\nSurfacing your team’s performance data throughout devel‐ opment will improve their work.\n\nWhat Is Effective Performance Engineering?\n\n|\n\n5",
      "content_length": 2028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Celebrating performance wins both internally and exter‐ nally will make your team more eager to consider perfor‐ mance in their work.\n\nEven the New York Times published an article, quoting Ben Waber, who has a Ph.D. from M.I.T., is the author of People Analytics (FT Press), and is, at 29, the median age of Google employees. His com‐ pany, Sociometric Solutions in Boston, uses data to assess work‐ place interactions. “Google has really been out front in this field,” he said. “They’ve looked at the data to see how people are collaborat‐ ing. Physical space is the biggest lever to encourage collaboration. And the data are clear that the biggest driver of performance in complex industries like software is serendipitous interaction. For this to happen, you also need to shape a community. That means if you’re stressed, there’s someone to help, to take up the slack. If you’re surrounded by friends, you’re happier, you’re more loyal, you’re more productive. Google looks at this holistically. It’s the antithesis of the old factory model, where people were just cogs in a machine.”\n\nThe end-user experience should be at the forefront of thinking when it comes to performance. The satisfaction of your end users will ulti‐ mately drive business success (or failure), and can be quantified by a number of metrics in described in “Metrics for Success” on page 82. The point is that it shouldn’t matter whether your servers can handle 1,000 hits/second or if CPU usage is below 80%. If the experience of the end user is slow or unreliable, the end result should be consid‐ ered a failure.\n\nBusiness What does performance mean to your business? Aberdeen Group surveyed 160 companies with average annual revenue of over $1 bil‐ lion, finding that a one-second delay in response time caused an 11% decrease in page views, a 7% decrease in “conversions,” and a 16% decrease in customer satisfaction.\n\nGoogle conducted experiments on two different page designs, one with 10 results per page and another with 30. The larger design page took a few hundred milliseconds longer to load, reducing search usage by 20%. Traffic at Google is correlated to click-through rates, and click-through rates are correlated with revenue, so the 20% reduction in traffic would have led to a 20% reduction in revenue.\n\n6\n\n|\n\nChapter 1: Getting Started",
      "content_length": 2330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Reducing Google Maps’ 100-kilobyte page weight by almost a third increased traffic by over one-third.\n\nThe correlation between response time and revenue is not restricted to Google. A former employee of Amazon.com discovered that 100 milliseconds of delay reduced revenues by 1%. Whether you are sell‐ ing goods online or providing access to healthcare registration for citizens, there is a direct correlation between the performance of your applications and the success of your business.\n\nWhy Is Effective Performance Engineering Necessary? Over the years, we have all lived with the mantras “Do more with less,” and “Faster, cheaper, and better.” While some organizations have survived, many have not. Now we are faced with a different question: “How do we deliver highest quality, highest value, at the highest speed?”\n\nIn addition, organizations must focus on other key elements, like revenue, competitive advantage, customers, and brand value. Practi‐ ces like Agile and DevOps have evolved and become more widely adopted. Regardless of the lifecycle, Effective Performance Engi‐ neering practices are enabling organizations to accomplish the pre‐ ceding focus areas and goals for their end users. We will touch on each of these focus points next.\n\nRevenue Effective Performance Engineering enables organizations to increase revenue in several ways. In a recent survey, 68% of respondents expected Performance Engineering practices to increase revenues by “ensuring the system can process transactions within the requisite time frame.” While all survey participants were generally in agree‐ ment regarding the tasks required, they had different expectations.\n\nIn addition to the increase in revenue, another result is reduced cost; 62% of the Performance Engineering–focused respondents felt that Performance Engineering practices should serve to avoid “unneces‐ sary hardware acquisition costs.”\n\nBy building in performance, organizations can start optimizing applications before the first piece of code is even written, or before\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n7",
      "content_length": 2095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "that new capability lights up the hardware, thereby improving the end-user experience and proactively focusing on the business objectives. Cost reductions can be dramatic. As the result of reduced performance-related production incidents, organizations can often handle 30–50 percent more transactions with the same (or less) infrastructure.\n\nSavings can quickly multiply—fewer machines means reduced capi‐ tal expense as the business scales, including lower operational expenses related to power and cooling. Fewer resources are then needed to support the infrastructure. Savings also accumulate through the reduction in performance-related production incidents that need to be managed, which reduces the opportunity cost of putting your most valuable people to work on new features and functions for the end user and your business.\n\nA catalog company, for example, might focus on total revenue from a specific product line or service, then track it after making specific changes to a promotional website to see if revenue increases. In another scenario, the value of the mobile application might be judged by the number of registered users and the frequency with which they access products or services. And, as the backend infra‐ structure (including web servers, middle tiers, and databases) takes on more roles inside the corporation, the metrics that track the per‐ formance of the larger organizational goals better reflect the quality of the supporting technology.\n\nThe rise in importance of Performance Engineering has been driven by practical concerns. At least 50% of respondents admitted that slowdowns and outages were discouraging customers and frustrat‐ ing employees. Many characterized the problems as “repeated,” and said they were often caused by large spikes in traffic that weren’t anticipated when the applications were built.\n\nThe consequences are serious. The average firm that responded to the survey said that a major outage could cost between $100,000 and $500,000 in lost revenue per hour. Some of the larger companies with more than 10,000 employees said they could lose $5 million an hour from website or core system outages.\n\nWhen organizations contemplated the scope and catastrophic range of these failures, they recognized that the traditional development process just wasn’t ready to build a system with adequate provisions for surviving these kinds of issues. Transforming the organization\n\n8\n\n|\n\nChapter 1: Getting Started",
      "content_length": 2458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "and focusing individuals and teams on performance means empow‐ ering them with capabilities to anticipate problems and solve them before they occur. And when problems emerge after deployment, it means giving the team the ability to control failure and mitigate risk.\n\nThis is one of the reasons why a greater understanding of Perfor‐ mance Engineering as a cross-discipline, intra-business mindset is so essential. Revenue is often the first and foremost measurement of why something needs to change in any organization, but it is not always used as a measurement of Effective Performance Engineering —even though it’s often seen as the key differentiator in gaining a competitive advantage related to delivering a product or service faster and better than anyone else in the market.\n\nSalesforce.com: Entrepreneurial, Independent, and Results-Oriented Salesforce provides quick insight to their culture on their website and states the following:\n\nTop talent across the globe comes to salesforce.com for the “change-the-world” mentality; the opportunity to excel in a fast- paced atmosphere; and the chance to be surrounded by peers and leaders that inspire, motivate, and innovate. Salesforce.com offers a unique career opportunity, regardless of what you do or where you do it.\n\nOur employees are entrepreneurial, independent, and results- oriented. If you like working hard in a place where hard work is rewarded, contributing to projects where contributions count, and growing in a company where growth knows no boundaries, salesforce.com is perfect place to do the best work of your career.\n\nThe beneficiaries of our hard work extend into our communities through the Salesforce.com Foundation. Employees are encour‐ aged to give back to the community and get four hours per month, or six full days per year, off with pay to spend on volun‐ teer activities.\n\nLooking to mainstream media, we quickly find Salesforce appears often in the “Fortune Top 100 Places to Work.” When you look at the Employee Ratings, it is easy to see how the right culture can benefit an organization. Adopting Effective Performance Engineer‐ ing capabilities supports many of these metrics.\n\nFigure 1-1 shows Salesforce employee ratings, demonstrating that 93% of employees say their workplace is “great.”\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n9",
      "content_length": 2344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Figure 1-1. Salesforce employee ratings\n\nOn the Salesforce careers page, the company showcases a hashtag (#dreamjob) that says a lot for its culture. Going a bit further, they add, “A #dreamjob starts with passionate people who do work that matters, win as a team, and celebrate success together. It ends with knowing that together, we are the force innovating the future of business for customers. We are living our #dreamjob. Join us!”\n\nCompetitive Advantage There’s an obvious business reason to focus on the needs of end users. If they’re your customers, they’re consuming your products and/or services and possibly paying you for results. If you’re a provider in a technology chain that defines a complex solution of some sort, you’re still dependent on the satisfaction of users, how‐ ever indirectly.\n\nBut it wasn’t so long ago—say, 20 years—that users of computing systems were expected to live with high latencies, downtimes, and bug workarounds that were common in business systems. That’s because users were employees and had to put up with the buggy, slow, or unpredictable systems their employers provided. Or they were users of a standalone application, such as WordPerfect for writ‐ ing, or Lotus 1-2-3 for spreadsheets.\n\nThat’s right, we’re talking about the pre-Internet age, when very few users imagined doing actual business transactions online. But once e-commerce became a buzzword, and soon simply a word, users\n\n10\n\n|\n\nChapter 1: Getting Started",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "stopped being just users. They became customers, and it was obvious the best web-based experiences for customers would lead to repeat business.\n\nFast-forward to today’s web-based and mobile business climate, where:\n\nUser experience (UX) is a red-hot topic.\n\nCommoditization of virtually everything is a fact of life.\n\nSocial media is the engine that can quickly sink an online retailer, transportation provider, and so on if the UX is poor— no matter the reason.\n\nThese days, performance is king, and your online or mobile custom‐ ers can either be adoring subjects or a mob of thousands, with the social-media equivalent of torches and pitchforks, at your kingdom’s gate. Or they’ll just go on to the next provider, leaving you alone in the dark ages.\n\nThrill your end users (especially customers) by outperforming their expectations, and you’ll get to keep doing more of whatever made them come to you in the first place, while making your company brand champions.\n\nDreamWorks Animation: Inspiring Audiences to Dream and Laugh Together Going to the DreamWorks website to learn about the company’s culture was an experience in and of itself. We were surprised to see five areas of focus in the “Our Culture” section of the site:\n\n1. Our Culture\n\n2. Education\n\n3. Campus Activities\n\n4. Well-Being\n\n5. Environment\n\nWithin the “Our Culture” area of focus it states, “At the heart of DreamWorks Animation is the desire to tell great stories that inspire audiences to dream and laugh together, pushing the bound‐ aries of both creativity and technology. To do this effectively, DreamWorks is dedicated to providing the best work environment\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n11",
      "content_length": 1696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "possible for the company’s artists, engineers and everyone in between, so that they enjoy creating their work just as much as audiences enjoy watching it.”\n\nGoing to the Indeed website revealed a bit more about the culture at DreamWorks, as nearly all reviews were 5 stars, nearly unheard of at other companies in this industry.\n\nAnother article from ChicagoCreativeSpace’s Max Chopovsky con‐ cludes by stating, “we have found ways to combine these spaces and make them all onsite so that collaboration, creativity and efficiency are maximized.”\n\nMany companies recognize that it’s not enough to call something a glitch, cross their virtual fingers, and hope it never happens again. They need to track their computer systems’ online status; guarantee that they’re responding to customers, partners, and employees; and measure whether they’re delivering the information promptly so no one is clicking, tapping, pounding the return key, and wondering whether they should just go somewhere else.\n\nAs corporate leaders realize the importance of their online and mobile presence and start to measure just how much business comes in through all of their channels, they’re reshaping organiza‐ tions to keep information flowing quickly and accurately. This responsibility is gaining traction under the umbrella of Performance Engineering.\n\nProponents of this new vision believe that enterprises must build a performance-focused culture throughout their organizations—one that measures the experience of end users, both internal and exter‐ nal—and deliver a software and hardware experience that results in efficient performance for these users. Performance must be priori‐ tized from the beginning of the process and be watched vigilantly after the code is deployed.\n\nMore efficient, faster systems leave employees less frustrated, some‐ thing that almost certainly translates to customer satisfaction as well. The rest of the answers indicate a general awareness that the performance and throughput of computer systems are directly\n\n12\n\n|\n\nChapter 1: Getting Started",
      "content_length": 2059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "related to competitive advantage and the organization’s ability to retain and attract customers.\n\nIs Performance Engineering DevOps? Putting an end to these academic distinctions, along with the day- to-day finger-pointing between Development and Operations teams (not to mention Technology versus Business teams), is part and parcel what the DevOps movement is all about. It involves an internal cultural shift toward the end user of the system(s) who wants an update, a bug fix, and some assurance the system has the full backing of the organization the customer is depending on.\n\nThere is a relationship between Effective Performance Engineering and DevOps. This relationship is one that DevOps will deliver higher value at higher quality and higher speed if the capabilities of Effective Performance Engineering are enabling the DevOps practices.\n\nThe competitive advantage—driven by faster time-to-market, with higher quality through Effective Performance Engineering—is a basis for how a business attracts and retains customers.\n\nCustomers: Acquisition and Retention Acquiring and retaining customers should be the driving force in an organization, no matter the size. As we have mentioned, Perfor‐ mance Engineering plays a considerable role in enabling your orga‐ nization to succeed in the marketplace, and success is defined in many ways for different organizations, and is not exclusive by industry or organization or product/service. But in general, success is defined by a few factors (and many times a combination), includ‐ ing: revenue, competitive advantage, brand value, and customers (acquisition and retention).\n\nWhen your customers look across the market to discover where they can get a specific product or services, what do they see, and how do they evaluate it? Today, much of this process is accom‐ plished in a number of near real-time and openly accessible formats, including app store reviews, feedback on websites, and even product- or service-specific feedback on your sites.\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n13",
      "content_length": 2064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "A myriad of feedback and comments are available to you regarding how your customers perceive your products and services, and what you need to focus on for your next release to deliver the features and functions they are seeking. The question becomes, “How are you capturing and what are you doing with this data today?” Using effec‐ tive Performance Engineering practices, you can leverage a continu‐ ous feedback loop from production to market, provide the highest value to your customer, and increase your acquisition and retention.\n\nDefining and knowing your customers is crucial. A recent info‐ graphic (cropped) posted by Rigor.com titled “Why Performance Matters to Your Bottom Line” illustrates how impatient online shop‐ pers are, and the impact of a one-second delay (see Figure 1-2).\n\nFigure 1-2. Why performance matters to your bottom line\n\nThe key point to highlight from this infographic is that online shop‐ pers are impatient, and the cost of a one-second delay is substantial; understanding this, and doing something about it for your business, is why we are focusing on building a Performance Engineering cul‐ ture and adopting practices to improve quickly.\n\nOf course, this leads to another important consideration: metrics. Thinking about acquisition and retention of customers involves measuring many key elements, including the feedback in the mar‐ ketplace for your products and/or services. For many companies,\n\n14\n\n|\n\nChapter 1: Getting Started",
      "content_length": 1468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "this becomes a great starting point and one they leverage to engineer into their practices.\n\nAs organizations consider and implement metrics, they’re driven by the realization that computer system performance and throughput are directly related to competitive advantage and their ability to retain and attract customers. This focus on the customer is helpful both for acquiring new customers and retaining existing ones.\n\nWegmans Believes in Caring On the Wegmans website, under “Our Values & Culture” is the fol‐ lowing text:\n\nBecause we’re all part of the extended Wegmans family, we share a common set of values—we call them “Who We Are.” And by living these values—handed down to Danny, Colleen and Nicole from Robert Wegman—we really have created something special: a great place to work where caring about and respecting our peo‐ ple is the priority.\n\nWegmans believes in:\n\nCaring We care about the well-being and success of every person.\n\nHigh Standards High standards are a way of life. We pursue excellence in every‐ thing we do.\n\nMaking A Difference We make a difference in every community we serve.\n\nRespect We respect and listen to our people.\n\nEmpowerment We empower our people to make decisions that improve their work and benefit our customers and our company\n\nIn an article from the Harvard Business Review titled “Six Compo‐ nents of Culture,” Wegmans is highlighted in component number three, stating:\n\n3. Practices: Of course, values are of little importance unless they are enshrined in a company’s practices. If an organization pro‐ fesses, “people are our greatest asset,” it should also be ready to invest in people in visible ways. Wegmans, for example, heralds values like “caring” and “respect,” promising prospects “a job [they’ll] love.” And it follows through in its company practices, ranked by Fortune as the fifth-best company to work for. Simi‐\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n15",
      "content_length": 1937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "larly, if an organization values “flat” hierarchy, it must encourage more junior team members to dissent in discussions without fear or negative repercussions. And whatever an organization’s values, they must be reinforced in review criteria and promotion poli‐ cies, and baked into the operating principles of daily life in the firm.\n\nForbes also published an article, “Focus on Your Company Culture, and Earnings Will Follow,” in which Wegmans was also highlighted in the “100 best companies to work for in the U.S.” The author writes, “Many organizations cling to ‘what’s always been done,’ which constantly pushes against innovation; as a result, earnings and other key performance metrics begin to lag. Those dips can make managers do some interesting things in an effort to restore their companies to greatness. As executives become laser-focused on chasing earnings, they may lose sight of the bigger picture. They become focused on treating the surface-level symptoms, never diag‐ nosing the deeper cultural dilemma.”\n\nBrand Value In many cases, businesses continue to invest in both a capability and a culture as they work to build in the practices of Performance Engi‐ neering. These practices enable them to grow faster and become more stable. Done well, Performance Engineering avoids the large- scale catastrophes like the one that hit Best Buy in 2014 (see For‐ tune, CNBC, CNN Money) as well as the soft failures that come when slow services frustrate employees and turn away customers. The big failures may get all the media attention, but it’s the gradual slowdowns that can be even more damaging, as they erode revenue and brand value. By the way, these are quick to be picked up in the media and amplified via social media channels, broadening and accelerating the damage to your brand.\n\nBrand value is often something measured and tied to the stakehold‐ ers within your marketing organization(s). However, as we see the culture of performance and practices of Effective Performance Engi‐ neering, this is becoming relevant to all stakeholders (and for all the right reasons).\n\nKeep your end users (especially customers) thrilled by outperform‐ ing their expectations and alternatives, and you’ll get to keep doing more of whatever made them come to you in the first place, while making them brand champions.\n\n16\n\n|\n\nChapter 1: Getting Started",
      "content_length": 2362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "All survey respondents viewed the downside of poor performance in much the same way. Together, 66% agreed that poor performance could hurt brand loyalty and brand value. As more users interact with an organization through online and mobile channels, it only makes sense that performance would reflect directly on the brand.\n\nIn conclusion, invest in Effective Performance Engineering, or risk costly failures. You’ll be rewarded with smooth rollouts, lower over‐ head, and higher revenue. What if you don’t invest? Expect damaged brands, lower revenue, and lower employee morale.\n\nApple Creates Wonder That Revolutionizes Entire Industries\n\nApple’s website reads:\n\nThe people here at Apple don’t just create products—they create the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innova‐ tion that runs through everything we do, from amazing technol‐ ogy to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it.\n\nFast Company published an article titled “Tim Cook on Apple’s Future: Everything Can Change Except Values.” In this article, sev‐ eral key elements of Apple and their culture are investigated, show‐ ing how deeply important values were to Steve Jobs and are now to Tim Cook.\n\nTim Cook said of Steve Jobs, “It was his selection of people that hel‐ ped propel the culture. You hear these stories of him walking down a hallway and going crazy over something he sees, and yeah, those things happened. But extending that story to imagine that he did everything at Apple is selling him way short. What he did more than anything was build a culture and pick a great team, that would then pick another great team that would then pick another team, and so on.”\n\nTim Cook goes on to say, “We’ve turned up the volume on collabo‐ ration because it’s so clear that in order for us to be incredibly suc‐ cessful we have to be the best collaborators in the world. The magic of Apple, from a product point of view, happens at this intersection of hardware, software, and services. It’s that intersection. Without collaboration, you get a Windows product. There’s a company that pumps out an operating system, another that does some hardware, and yet another that does something else. That’s what’s now hap‐\n\nWhy Is Effective Performance Engineering Necessary?\n\n|\n\n17",
      "content_length": 2392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "pening in Android land. Put it all together and it doesn’t score high on the user experience.”\n\nAll of these elements—values, people, and collaboration—are at the core of Effective Performance Engineering.\n\nThe Harvard Business Review published an article titled “The Defining Elements of a Winning Culture,” which shows how a com‐ pany’s culture can have a powerful impact on its performance.\n\nHBR found a set of seven “performance attributes” that enable the best-performing companies. Here is their list:\n\n1. Honest. There is high integrity in all interactions, with employees, customers, suppliers, and other stakeholders;\n\n2. Performance-focused. Rewards, development, and other talent-management practices are in sync with the underly‐ ing drivers of performance;\n\n3. Accountable and owner-like. Roles, responsibilities, and authority all reinforce ownership over work and results;\n\n4. Collaborative. There’s a recognition that the best ideas come from the exchange and sharing of ideas between individuals and teams;\n\n5. Agile and adaptive. The organization is able to turn on a dime when necessary and adapt to changes in the external environment;\n\n6. Innovative. Employees push the envelope in terms of new ways of thinking; and\n\n7. Oriented toward winning. There is strong ambition focused on objective measures of success, either versus the competi‐ tion or against some absolute standard of excellence.\n\nThe article mentions Apple and Steve Jobs specifically: “Steve Jobs builds a challenging culture at Apple —one where ‘reality is sus‐ pended’ and ‘anything is possible’—and the company becomes the most valuable on the planet.”\n\nFocusing on Business Need The business needs to ensure that revenue, competitive advantage, customer acquisition and retention, and brand goals are achieved. Doing so means expanding products and service offerings and/or businesses either through organic or acquisition approaches, all of\n\n18\n\n|\n\nChapter 1: Getting Started",
      "content_length": 1967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "which may depend on an existing or new platform(s), so end users can consume products and services without interruption when, where, and how they want.\n\nAs a result, businesses should always be seeking to adopt Effective Performance Engineering capabilities. How they choose to do so must be led by clear and visible objectives, key metrics and measure‐ ments, and communications. If they support and participate, they will see positive results from the previously defined goals and objec‐ tives—if not, negative results will be sure to follow in one or many of these areas.\n\nFocusing on Business Need\n\n|\n\n19",
      "content_length": 608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "CHAPTER 2 Overview of Performance Engineering\n\nNow that we understand where to start with Performance Engineer‐ ing, having defined both the term itself and also why it is important, let’s dive into more specifics of how it is applied throughout the lifecycle. As we walk through each commonly defined step within a lifecycle, we will explore where and how companies leverage these capabilities to deliver the results we identified in the last chapter.\n\nPerformance Engineering Throughout the Lifecycle As you start to incorporate Performance Engineering capabilities into your lifecycle, it is important to understand what some of these areas are, and put these into context with some typical flow nomen‐ clature. In the following sections we define each of these key ele‐ ments with specifics—what, why, and how—so you have a more complete understanding of how to add Performance Engineering throughout the lifecycle.\n\nOne of the challenges in building Effective Performance Engineer‐ ing or a performance-first culture is defining who does what, when, and how. This kind of organizational alignment and agreement is as important as the daily scrum meeting of an Agile team. If everyone agrees that performance is important, but not on how to address it, then nothing is done about it.\n\n21",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "First, we need to agree that while everyone is responsible for the performance of our business applications, someone needs to be accountable. One person, or team in a larger organization, needs to make sure everyone is playing along in order to meet our objectives. It could be the Scrum Master, Engineering Team Lead, QA Lead, or a separate role dedicated to performance.\n\nSome organizations have even created a “Chief Performance Offi‐ cer” role to bring visibility and accountability to the position, along with information radiators to show performance results as visual and accountable feedback throughout the process. Once that person or group is identified, it is important to include them in any standup meetings or architectural discussions, in order to raise any red flags early in design and avoid costly rework at later stages.\n\nCulture needs to be built into an organization by design. There are several solid, cross-industry examples included in a Staples Tech Article; we’ll examine these more closely and investigate how their culture is focused on Performance Engineering, and how they have built in these capabilities.\n\nThe following sections cover the what, why, and how of Effective Performance Engineering capabilities, so that as you look at this cul‐ ture and the role(s) of adoption, you can start to understand more specifically how it might apply within your own organization.\n\nRequirements Features and functions, along with capabilities both for new applica‐ tions and maintaining legacy, all fall into this section, where we will highlight some specific elements for consideration as you are adopt‐ ing Effective Performance Engineering practices. Take a look at these items and understand the what, why, and how of each, so as you begin to transform, you can ensure consideration for each spe‐ cific element is being considered and adopted.\n\nComplete Stories In defining the changes we are going to implement, complete and understood requirements or stories are a solid starting point. Mike Cohn, founder of Mountain Goat Software, and founding member of the Scrum Alliance and Agile Alliance, has created a user story template, shown in Table 2-1.\n\n22\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Table 2-1. Mike Cohn’s user story template\n\nAs a/an moderator\n\nmoderator\n\nestimator\n\nmoderator\n\nI want to... create a new game by entering a name and an optional description invite estimators by giving them a URL where they can access the game join a game by entering my name on the page I received the URL for start a round by entering an item in a single multiline text field see the item we’re estimating\n\nSo that... I can start inviting estimators\n\nwe can start the game\n\nI can participate\n\nwe can estimate it\n\nestimator\n\nestimator\n\nmoderator\n\nmoderator\n\nsee all items we will try to estimate this session\n\nsee all items we try to estimate this session\n\nselect an item to be estimated or re-estimated\n\nI know what I’m giving an estimate for I have a feel for the sizes of the various items I can answer questions about the current story such as “does this include ___?” the team sees that item and can estimate it\n\nUsing a thoughtful approach to stories has many benefits. With an incomplete definition of requirements and features, an individual or team is left to define what they believe the end user wants and needs. If Performance Engineering is not considered as part of a complete story, a technical component or architecture could vary considera‐ bly, resulting in underperforming or unutilized capabilities.\n\nOrganizations continue to evolve the way they create complete sto‐ ries using models like Mike Cohn’s user story template, and also by adopting prototyping or wireframe capabilities to accelerate the delivery of high-quality results to the end users.\n\nBreakdown of Epic to Tasks with Acceptance Criteria It’s important to plan for the size, relationship, and priority of requirements and features, along with building in performance that shows the relationship to epic to task, in order to enable teams to collaborate and consider Performance Engineering needs and capa‐ bilities from the start.\n\nFigure 2-1 shows the relationship and breakdown from epic to tasks, along with a story card with Story on front and Acceptance Test Cri‐ teria on the back.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n23",
      "content_length": 2131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Figure 2-1. Breakdown of Epic to Story, and example\n\nIn many cases, we observe a trend of more myopic or task-level views into stories. This practice limits the view and consideration across tasks, and limits the ability to build higher performing plat‐ forms, especially in the much-distributed and shared complex appli‐ cations and systems we operate within today.\n\nIn Figure 2-1, you can see how the story is on the front of the card (typically only used portion) and on the back are the acceptance criteria, which is where you include Performance Engineering con‐ siderations. An example from a recent story about a login page is, “Perform with 180,000 people logging in per hour with 50% on varying mobile network conditions from 5 major US and 2 major International locations with the remaining from good WiFi and LAN connections with a maximum transaction response time of <5 seconds.”\n\nDoneness Criteria A proven practice among organizations is defining a shared under‐ standing and standard for what “done” means. Creating a “Feature Doneness” definition for all teams is critical, and Performance Engi‐ neering considerations need to be built in.\n\nThe standard for speed and quality must be a known and shared value across individuals, teams, business units, and organizations. Perhaps there are only 5–10 core criteria defined and agreed upon at a complete organization level, but this is the delivered standard.\n\n24\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "This will enable a level of doneness in order to meet shared expecta‐ tions and deliver within the agreed-upon time and quality.\n\nA recent example of how a Dev/Test organization put together their doneness criteria is outlined below. Here, the organization had only eight criteria, of which the italicized one builds performance into their process:\n\nThe code is included in the proper branch in the source code control system.\n\nThe code compiles from a clean checkout without errors using production branch [proposed: and is part of the AHP Build Life which is finally tested in QA].\n\nThe code is appropriately covered with unit tests and all tests are passing using the production branch.\n\nThe code has been peer reviewed by another developer.\n\nDatabase changes have been reviewed and approved by a DBA.\n\nThe code has passed integration, regression, stress, and load test‐ ing.\n\nApplication Support is aware of the backlog item and the system impacts.\n\nDeployment and rollback instructions are defined, tested, and documented.\n\nUntil these are all true, the feature is simply unfinished inventory.\n\nFunctional Within the functional tests you run today, think about how to lever‐ age these (typically single-user) tests to get performance results and share them with all stakeholders. This can take place at any stage throughout your lifecycle from unit to production, and the value of these results benefits the team throughout the lifecycle. A specific example could be the way performance is built in to your automated functional unit tests, as demonstrated in Figure 2-2, which illustrates how long a specific set of automated functional unit tests took to run and set a pass/fail threshold within Jenkins.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n25",
      "content_length": 1767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "The reasons why this is important are numerous. One of the com‐ mon areas we focus on is speed with quality. As you are increasing the number of builds per hour/day/week/month, these Performance Engineering practices enable early and frequent feedback on quality. The incremental value delivered with every build enables quick feedback loops and opportunities for DevTest teams to deliver higher quality faster by building in performance within their auto‐ mated functional tests.\n\nFigure 2-2. Functional automated test results shown in Jenkins\n\nThe practice of gaining performance results from functional tests during the Performance Engineering adoption should be carried out throughout an organization. Just focusing on the automated func‐ tional tests immediately post-build, we can see this information ena‐ bles a build-over-build continuous comparison, allowing the business to see trends over time. Adding response time along with percentage of errors variables is another way to quickly get a lot more value from functional tests you are already running with little to no effort, effectively driving performance results with immediate feed‐ back.\n\nFigure 2-3 shows performance trend results from JMeter that illus‐ trate the history of performance from build to build.\n\n26\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 1332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Figure 2-3. Performance trend results\n\nSecurity Security is at the forefront of many organizations’ priorities, and is addressed by a dedicated CISO (chief information security officer) and their team of professionals, often in an isolated group across the technology stack. Building performance into the security practices, and vice versa, becomes quite critical, enabling organizations not only to get actual results they may not have previously been able to achieve, but also to provide those results to a broader group of stakeholders earlier and more often in order to quickly mitigate vulnerabilities.\n\nFixing security vulnerabilities earlier in the lifecycle by adoptiong Performance Engineering practices reduces risk for the business and the end users. Providing the DevTest team with immediate, automa‐ ted insight enables the organization to deliver more, faster, with higher quality. This advantage accrues when performance scenarios are run and security results come in from the tests, as well as when security has the opportunity to run their tests under more accurate, repeatable conditions.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n27",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "There are two immediate ways that we can think about security and Performance Engineering practices. First is the ability to provide security results within the performance results throughout the life‐ cycle. Second is enabling performance conditions, for example, in network conditions (latency, bandwidth, packet loss, jitter), so when security tests are executed the results delivered are actual. This detail is often overlooked in security tests in two specific vulnerability areas: cross-site scripting and SQL injection. These have become popular due to mobile network conditions (needing to hold connec‐ tions open longer due to higher latency conditions, and dynamic sessions not working).\n\nYou can capture security risks and vulnerabilities by providing automated and prioritized results, captured in a flat file and stored with the automated build results, so they can be remediated and results shown as a trend over time for the given code base/release candidate. Figure 2-4 shows how to get security results while you are running your automated tests using Watcher within Fiddler2.\n\nFigure 2-4. Security test results using Watcher in Fiddler2\n\n28\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Performance Too often we observe organizations treating performance as a checkbox. If this is the area where you need to adopt Performance Engineering practices, there are several ways to start. With Perfor‐ mance Engineering practices the performance team(s) no longer has to accept running the same scenario and watching the same results and reporting pass/fail, but instead can dive a lot deeper into what is going on along with how it is working in production, so they can provide more relevant and actual results to help optimize system performance.\n\nHere are four key starting points in adopting Performance Engi‐ neering practices within performance, which we will identify now and define a bit more later:\n\nProduction incidents\n\nEnable you to see what is and is not working, along with creat‐ ing some business value on the impact when something is not working; allow teams to see trends on where the trouble areas are, and show the value added as improvements are reflected in a reduced number of incidents.\n\nInstrumenting\n\nEnables additional and broader insight into what is happening within an application and across a system, in order to visualize and simplify finding the “needle in the haystack”; helps with finding not just the first issue, but also a path of areas needing attention.\n\nVirtualization\n\nEnables the re-creation of virtual users, services, networks, and data. Using virtualization technologies, you can very quickly re- create—with a high level of accuracy and low cost—the produc‐ tion environments anywhere in the world at any time for any period of time.\n\nMonitoring\n\nGives a visual performance dashboard of actual results in pro‐ duction, and when also used to observe the pre-production environments, provides teams assurance that they are building solutions that will scale and be resilient, especially as they are deployed to the end users.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n29",
      "content_length": 1933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Figure 2-5 shows how monitoring can be leveraged throughout all environments to build in performance and provide results to all stakeholders.\n\nFigure 2-5. Using monitoring to increase performance\n\nWe could dive into any of these topics and quickly observe the value of adopting Performance Engineering practices in performance. Looking a little closer at monitoring, we start to see a number of dif‐ ferent areas where these capabilities used both in production and pre-production environments can greatly enable the adoption of Performance Engineering practices within the performance team(s) and beyond:\n\nSynthetic monitoring provides the ability to simulate applica‐ tion performance many ways, ensuring you can deliver consis‐ tent and predictable performance to your end users.\n\nReal user monitoring enables real-time application performance of all users all the time, allowing you to automatically discover underlying infrastructure and classify user actions.\n\nMobile app monitoring provides insight into the performance, stability, and resource usage of mobile apps. It gives you insight into what the user does, where they exited your app, and why.\n\nDeep-dive diagnostics allows you to drill into your backend performance to quickly isolate and diagnose bottlenecks to resolve issues.\n\n30\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 1347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Transaction monitoring provides visualized process flows over the entire application and infrastructure environment so you can assess key business metrics.\n\nTransaction management enables you to track and confirm the path, step-by-step timing, and content or payload of each trans‐ action, so you can understand the impact of critical transactions on business outcomes.\n\nFigure 2-6 illustrates how Hewlett-Packard Enterprise Business Ser‐ vice Management (HPE BSM) can be used throughout your build life along with Service-Level Agreements (SLAs) to quickly show status across a variety of areas, leveraging these capabilities in both pre-production and production environments, and enabling the performance team(s) to adopt Performance Engineering practices.\n\nFigure 2-6. Using HPE BSM to monitor status\n\nUsability Usability for the purpose of this discussion is defined as the ease of use and learnability of a human-made object. Looking at the corre‐ lation of performance and facets of usability is a way to begin adopt‐ ing Performance Engineering in this increasingly important and vested area.\n\nIt is important to assess your position in the market relative to your competition, and to measure usability as a trend across releases over time. Net Promoter Score (NPS) and User Sentiment Score (USS) are common elements used to measure usability. As we look at usa‐\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n31",
      "content_length": 1427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "bility and Performance Engineering, it is important to note this practice should be applied throughout the lifecycle, not just at a sin‐ gle point late on the way to production deployment.\n\nThe NPS and USS Scoring Systems The Net Promoter Score (NPS) system was developed by Fred Reichheld, Bain & Company, and Satmetrix. The NPS is a way of measuring user satisfaction. The methodology relies on the so- called “ultimate question,” which measures overall satisfaction and, even more importantly, loyalty to the service or provider. A second question establishes the root causes for the given score.\n\nThe User Sentiment Score (USS), developed by HPE, is created by a sentiment analysis service that scans end-user comments on your applications, categorizes them, and provides a weighted score. You can go to http://apppulse.io/#/ and type in “Target” to see example results.\n\nCorrelation between performance and user satisfaction with usabil‐ ity is high. So, if you can make your interface and design easier to use, people will like it more—this seems obvious, but not everyone takes the time to implement this capability. There are several reasons interest in usability and performance has increased, including increases in competitive advantage and customer acquisition and retention. In addition, with the brand damage that can be inflicted through app store reviews and social media, getting usability right is now a mainstream challenge.\n\nThere are a variety of ways organizations can implement Perfor‐ mance Engineering within usability. A few key metrics to consider in this area include:\n\nThe time it takes for an end user to complete a task. So, if an end user is seeking to make a purchase, what was the amount of time it took for that task end to end, and how long did each step take, and why?\n\nWorkflow simplicity reflects the experience of the end user in being able to easily learn and navigate the interface to perform the task they need to complete.\n\nSuccess rate is a measurement of what percentage of end users were able to complete the task they set out to achieve.\n\n32\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Subjective feedback tends to be a narrative of how users felt about the overall experience.\n\nError rate is the percentage of time an end user received an error or failure while using the interface to perform a task.\n\nHeat map and eye tracking are another way to capture seem‐ ingly subjective feedback in an objective way, knowing where your end users are spending their time looking and clicking or tapping on the interface to perform a task.\n\nFigure 2-7 is an example of a usability dashboard in Google Analyt‐ ics, showing how to capture some of these key metrics both in pre- production and production in order to adopt Performance Engineering practices in usability.\n\nFigure 2-7. Usability dashboard in Google Analytics\n\nDesign As we look at the complexity of our systems and applications today, much of which we no longer control or house within our data cen‐ ters, the need for Effective Performance Engineering increases con‐ siderably. Being able to adopt these capabilities throughout your lifecycle enables you to consider prototype options and build SLAs and performance budgets into each component you are designing, regardless of the stage of life of the component or system.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n33",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "System design\n\nA critical step in Performance Engineering is defining the many dependent pieces of a system, including data, user interfaces, inter‐ nal and external dependencies, modules, components, architecture, and much more.\n\nWithout a design map and plan, teams are often left to what they know, and the results can be discarded or need significant rework. With an Effective Performance Engineering approach, performance considerations are built in, and collaboration across teams enables a more complete design from the start.\n\nOften, an Enterprise Architecture team performs this step. However, as we can see, this is a critical element for which infrastructure architecture and application architecture must be designed, so involvement from the broader team, with knowledge of challenges and opportunities, is imperative.\n\nInfrastructure architecture\n\nRelated to and often integrated with following system design is the need to define the required infrastructure in order to support the needs of the applications and forecasted business levels, and thus be able to deliver the required resources, often left to a few infrastruc‐ ture people to create and verify.\n\nA continuation of the overall system design is the infrastructure architecture. An Effective Performance Engineering approach pro‐ vides for understanding and prototyping the various components and dependencies, along with configuration and sizing needs for the overall systems.\n\nThis is often delegated to R&D, product management, and product development to execute and deliver; however, to bring Performance Engineering practices into the picture, the team needs to be broader earlier and often include prototyping to find initial optimization.\n\nApplication architecture\n\nThe goal here is to design a composite architecture that will be scala‐ ble, available, manageable, and reliable. This involves many consid‐ erations and technological risks, along with software requirements and configurations, that must be integrated with those of the overall system design and infrastructure architecture.\n\n34\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Another extension of the overall system design is the application architecture. An Effective Performance Engineering approach pro‐ vides for understanding and prototyping the various components and dependencies, along with configuration and sizing needs for the overall systems.\n\nThere may be some limitations around existing or defined stand‐ ards, especially given the complexity and integrated nature of com‐ posite applications. Using the approach of prototyping enables quick build and running of a few quick performance scenarios to get results showing how these prototypes perform. Seeing how they per‐ form is the first step. Knowing the underlying infrastructure and network components can also play a major role in being able best to architect a high performing solution. A collaboration between these functions must be taken to optimize architectures and overall sys‐ tem design for performance.\n\nDeployment models\n\nDeployment, or moving code and artifacts through environments to production, can range from manual to fully automated, and can be limited to only certain environments or all of them, and anything in between.\n\nThis step enables you to achieve fast feedback and increase stability across your environments, thereby eliminating many of the variables usually associated with deployment.\n\nThere are a number of open source and commercial capabilities available to help automate this process from check-in to build to deploy, and to enable the tracking of build quality through the life of the release.\n\nResiliency\n\nWhen thinking about design and resiliency as they relate to Perfor‐ mance Engineering, we specifically are looking at the steps you take as you design your solution, so that when your product or service experiences a disruption it can continue to deliver value to your end user.\n\nEnd users today expect to be able to access your products or services at any time, anywhere in the world. In order to enable this capabil‐ ity, we must consider how we design our technology to meet these expectations.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n35",
      "content_length": 2091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Asking “if this component is down or not available, how will it con‐ tinue to deliver value to our end user?” is a great place to start the design process. Defining the top five revenue-generating workflows for your business, then designing how you can ensure they are resil‐ ient, is a good strategy.\n\nScalability\n\nScalability involves the considerations for how a system or applica‐ tion will be designed to handle a growing amount of work. Over the years, we have heard claims about how certain cloud environments enable exponential and linear growth; how your applications and systems are designed will enable you to see and understand the level of truth in such statements.\n\nIncreased demand for products and services by the end user of your business is a good thing, and if your technology can support and meet or exceed this demand, even better. However, too many times we’ve seen organizations make great assumptions that systems will scale as needed in a linear model. They provide more and more infrastructure capabilities to such a system, only to have it fail signif‐ icantly because it was simply not designed to scale.\n\nStarting with the share services and synchronous capabilities helps us to identify the two most common areas for scalability. Still, the complexities within your composite application architecture will likely also see middle-tier services and database constraints. Consid‐ ering this during the design phase enables you to be ready to proto‐ type early in the process.\n\nDevelopment Practices of Effective Performance Engineering vary across teams and divisions as much as they can across industries and organiza‐ tions. Following some basic principles and practices enables you to create and maintain sustainable, scalable, and high-performing applications on resilient infrastructure. With these guidelines, we will see how to do this, and deliver it quickly with quality.\n\nCode, frameworks, and service reuse\n\nWhen developing new applications and systems, it’s important to leverage ways to accelerate delivery of capabilities. Some of these ways manifest themselves in code repositories, frameworks, plat‐\n\n36\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "forms, libraries, and service components reuse. You can help DevTest teams maximize the focus on new and modified elements, and drive higher quality in a faster timeline, within the defined, shared standards.\n\nAs you move from a sole developer to development teams, then from many development teams to business units of development teams, and finally to many business units, challenges arise around standards and technology approaches and architectures. Many com‐ panies do not consider performance from an early perspective (if ever) and build it in.\n\nImagine having a code repository within a framework and service host that have already been built and optimized within the organiza‐ tional standards, which you can leverage in building the new capa‐ bilities in front of you. Utilizing this approach saves you from spending time building and optimizing core capabilities and shared services.\n\nMetadata repository\n\nDetermining how to quickly and easily leverage needed DevTest data continues to plague many teams and organizations. Attaining frontend data, data in motion, and backend data can be a significant challenge, and often one not easily solved. You must consider many facets in order to create and deliver to end users accurate, working applications and systems to consume that data.\n\nData continues to be one of the bigger challenges facing many today. Without accurate and reflective data, your confidence in application and system success is limited. In addition, to achieve speed and quality of automated lifecycle processes, it’s critical to have quickly refreshing environments, especially those including data. Having a repository of maintained and ready data increases the quality and speed with which you can deliver optimized systems.\n\nMaking data a priority is a great way to start. Bringing in and assign‐ ing accountability to the business analysts and user acceptance test‐ ing teams to define and create data and data models is a parallel approach. Obfuscation of production data to be included, along with data dependencies, and storage must also be planned in order to ensure compliance with all regulatory and other required practi‐ ces. In addition, providing your end users with early access and cap‐\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n37",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "turing the data and flows they use is another key way you can start to meet this challenge.\n\nAutomated tests\n\nTesting throughout the lifecycle is an important step; however, determining specifically how to do it within DevTest is key, espe‐ cially as continuous testing will be pushing more builds than before, and speed to verify and validate accurately is critical.\n\nYou must proceed with automated tests early, often, and throughout the lifecycle, especially given the need for repetitive and consistent results and measurements, in order to be able to see trends and opti‐ mization opportunities. If tests are not done in an automated way, you cannot meet goals of speed and quality, along with quality gates and build quality, automating more of the lifecycle.\n\nMany open source and commercial capabilities are available to enable automated tests. In addition, using automation test frame‐ works will allow you to build tests that can be merged and versioned with code through the build and test process.\n\nLifecycle virtualization\n\nCreating production-like environments within your pre-production environments is often expensive and labor-intensive. However, hav‐ ing a controlled and stable environment is important, and the ability to make it quickly available and refreshed in a timely manner is even more important. Virtualizing the users, services, network, and data gives you these capabilities, so you can deliver a better quality prod‐ uct faster.\n\nReducing costs, increasing speed, and improving quality are three is key. Having reasons why utilizing production-like environments enables you to more accurately pre‐ dict how things will perform once deployed. Given the complexity of our systems and applications today, this continues to be one of the bigger challenges, and lifecycle virtualization is a way to close the gap significantly.\n\nlifecycle virtualization\n\nMany open source and commercial products are available to provide you with these capabilities. Virtual users, for instance, are commonly found in functional and performance testing tools and enable you to create synthetic end users, performing transactions across a variety\n\n38\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "of workflows as they would in production, across all of your envi‐ ronments.\n\nThen you can move to service virtualization, which enables you to capture and re-create services from production across all of your environments. Virtual networks enable you to capture network con‐ ditions in production (and all other locations) and re-create these in any environment so you can optimize your app or application to perform well, even over poorly performing network conditions. Last is data virtualization, which leverages some metadata repository capabilities to make it quick and easy to refresh data across your pre-production environments.\n\nQuality gates\n\nQuality gates are specific quality thresholds for each stage or envi‐ ronment. These thresholds are then used to automate the build/ deploy/test steps and proceed to the next stage or environment. This allows for little to no manual intervention, and delivering high- quality assets in an automated and timely way. These thresholds enable you to set criteria on the build quality (Figure 2-8 shows a variety of “% Tests Pass” criteria) before proceeding on the path to production.\n\nAs you increase builds and automated tests and deployments, you will see things start to speed up. However, quality is a metric you also need to define, and measure at specific milestones throughout the lifecycle. Tracking quality enables you to observe a build as it moves through the lifecycle, and only intervene when it meets a cer‐ tain quality threshold and you need someone to execute manual tests against the product.\n\nThere are several approaches for adding quality gates, many of which use open source or commercial products to deliver a frame‐ work that enables you to build in the performance, quality, and speed considerations. Some work will be needed to build and sup‐ port this application lifestyle automation solution for your specific environment. Figure 2-8 shows how you can implement automated quality gates with virtualized and physical environments throughout the CI/CD process and where performance results are built in.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n39",
      "content_length": 2138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "40\n\n|\n\nChapter 2: Overview of Performance Engineering\n\ns e t a g\n\ny t i l a u Q\n\n.\n\n8 - 2 e r u g i F",
      "content_length": 101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Test Minimizing the risk to the business and your end users in produc‐ tion is paramount. Adopting Effective Performance Engineering practices ensures that you are most efficiently doing this early and throughout your lifecycle, and that you’ll have many opportunities to verify, validate, provide feedback, optimize, and then continu‐ ously execute. The following sections cover specific, proven practi‐ ces you can adopt to enable Performance Engineering capabilities.\n\nBuild results\n\nAfter the commit/build/deploy/test process, the results are what matter, and how we capture these results and make them available to stakeholders is crucial, along with the summary performance met‐ rics continue and live with the build life.\n\nBuilding in this automated feedback throughout the lifecycle, start‐ ing immediately after a successful build, enables teams to respond early and often, which in turn accelerates delivery speed and increa‐ ses quality.\n\nThere are a number of different ways to implement this capability. As a starting point, a basic automated delivery framework needs to be in place. Then, you can use automated unit, functional, or perfor‐ mance scripts to compare execution times at a minimum. Now you will be able to track detailed results from build to build, identifying any outliers, and potentially flag them from your trunk and/or main branch until they can be remediated for performance optimization.\n\nRegression\n\nAs the continuous build/deploy/test cycle evolves into more mature builds and potential release candidates, capabilities and features grow, and you must run quick but complete automated scenarios to ensure no core functionality has been broken or degregated as a result of the new capabilities.\n\nSpeed and quality increase as more performance engineering capa‐ bilities are built into your automated processes. With the adoption of these capabilities comes the need to progress through a variety of environments on the release path. Building automated performance regression suites enables higher confidence and meets automated\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n41",
      "content_length": 2120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "quality gates to enable the potential release candidates to move through the release cycle efficiently.\n\nWhen you’re starting down the path of implementing these capabili‐ ties, you must have an understanding of the key systems, applica‐ tions, and transactions—those critical from both a regulatory and end-user perspective—so that you can implement these first within your automated performance regression suite. In the “lower level” environments (also known as BVT or build validation test), the results should be an automated performance regression suite that takes less than 15 minutes to execute, and provides results to ensure your most critical functions work as designed.\n\nAutomated service oriented\n\nAs the complexity of the composite application and system architec‐ ture grows, so will the dependencies on internal and external serv‐ ices. Automating the verification and validation of these services throughout the lifecycle, and especially testing, is key. You must also consider how to get initial results and flags for performance-related metrics, along with how to re-create these in a valuable and cost- efficient manner.\n\nThe explosive increase in composite application and system archi‐ tectures has resulted in organizations with an exponential number of services. These services change only occasionally, compared to the frequently changing user interface, enabling a more stable and core technology to test with less maintenance.\n\nYou should start with a basic test harness that identifies the service and protocol mapping, then run through a barrage of positive and negative scenarios, delivering results for those that pass and fail. The objective of the team should be continuing to develop until all results pass.\n\nCapability mapping and standardization\n\nOrganizations looking to scale Performance Engineering practices need to standardize core capabilities in order to enable a growing number of teams and individuals to leverage a shared model (map‐ ping). Doing this integrates the new capabilities across IDEs, CI/CD systems, configuration, environments, and release management, just to mention a few key elements.\n\n42\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "Having disparate and nonintegrated tools to do specific and individ‐ ualized tasks is a mess at a small scale, and once this goes beyond a few teams, it becomes a challenge to manage from many different angles. Finding the capabilities that matter to your organization and standardizing them enables you to scale more efficiently. These standards also simplify the education of new team members and increase the stability of your integrated tools, so you can focus on delivering value to your end users.\n\nFigure 2-9 shows how tool integration can be mapped to support processes to implement Effective Performance Engineering practi‐ ces. As you continue down this path, it may be necessary to create an abstraction layer across the top, so all capabilities can be visual‐ ized and used as an information radiator for several stakeholders across your portfolio.\n\nFigure 2-9. Mapping simple tool integration to support processes\n\nDeployment Releasing code and builds throughout a lifecycle is an art, and it’s a key component to enabling Effective Performance Engineering. In this sectio we will highlight many of the critical ways this can be done, the required controls, and how to get continuous automated feedback throughout. The end objective is for deploying to produc‐ tion not to be an event, but simply another deployment in a series that have already been executed and delivered throughout the lifecy‐ cle in a rigorous yet fully automated process.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n43",
      "content_length": 1513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Automation throughout the lifecycle\n\nMoving through the lifecycle with everything we’ve described previ‐ ously is a huge success, but ensuring all that effort isn’t for nothing is also important; we have seen several examples in which deployment to production is still is a manual process, which has its own draw‐ backs and risks.\n\nReducing the manual effort required increases accuracy and repeat‐ ability, and ensures deployment has been tested before the product deploys to production. You can automate deployments of builds and release candidates with commercial or open source products, but implementations vary a bit depending on your environment. The biggest challenge, however, is that of organizational and individual behaviors limiting the continued utilization of automated deploy‐ ments to production.\n\nShowing these organizations and individuals how deployment auto‐ mation has been achieved and tested through pre-production envi‐ ronments, and how IP addresses and credentials can be secured, will go a long way toward utilizing automated deployments to pro‐ duction.\n\nLive/live or blue/green\n\nLive/live or blue/green is a deployment approach in which you have two production environments, enabling you to potentially take one offline, deploy to that environment, and introduce a small popula‐ tion of users to ensure it performs as expected, then either shutting that instance off or leave it on and deploy to the second environ‐ ment so you are running two production environments again.\n\nThere are many reasons why you would want to leverage a live/live or blue/green approach. It provides resiliency and scalability, and increases performance. It also allows you to easily perform canary tests in a contained environment. All of these are key Effective Per‐ formance Engineering practices.\n\nHow this approach is implemented varies, depending on the organi‐ zation. Figure 2-10 shows one way we have done it and opens a dis‐ cussion of which specific Effective Performance Engineering practices are performed where. You will also see a stand-in mode added into this model, available in case the production environ‐ ments (n and n+1) have issues (a stand-in providee end users with access to the products and services without interruption).\n\n44\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Figure 2-10. Live/live or blue/green deployment\n\nCanary\n\nIn the canary deployment approach, you roll out a new capability to a very focused group of your population, then observe that group’s performance and feedback, and finally decide whether to continue, stop, or pull back that deployment or release to a more general pop‐ ulation.\n\nGetting fast feedback and market-testing new capabilities is impor‐ tant to many organizations so they can test their theories and best guesses as to what the end user needs next. The canary test may also be a first time on a new infrastructure or application architecture, so they can benefit teams greatly to see how the architecture per‐ forms at a smaller scale in production. They can then apply their new knowledge to the next release, and deploy an incremental qual‐ ity increase for the next round of canary testing, feedback, and measurements.\n\nThis kind of test is often limited to a single environment (perhaps a single cluster) and often utilizes a live/live or blue/green production environment, in order to further isolate the underlying technology and limit access to specific end users.\n\nFail over, fail back, and fail forward\n\nThe fail over, fail back, and fail forward approaches involve answer‐ ing the question, “When we have problems, how do we plan to man‐ age them?” The beauty of this process is it allows you to test your\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n45",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "desired approach in your later-stage environments (pre-production or disaster recovery, perhaps). The fail over/fail back approach means you have a secondary production environment to fail over and back. The fail forward approach means your intent is to simply push a next release over the prior to resolve the issue.\n\nIn a production disaster recovery scenario, these processes are often defined in the run book and tested on how to fail over, but not always how to fail back. When you are failing back there are many challenges, along with many dependencies on databases and exter‐ nal services, that you need to consider. Not doing so, and as we have observed in several cases, can result in data loss and other more seri‐ ous incidents, making for upset customers and regulatory issues.\n\nTo build these approaches into your tests, you must take the time to plan for them, and they will aid you greatly in ensuring you have a cross-functional team with shared responsibilities and goals. Find‐ ing ways to build in fail over, fail back, and fail forward into your overall deployment approach is a great way to start. Perhaps asking this question today will help you understand how you do or do not plan for problems, and imagining what might happen if the worst occurs, will get you to adopt this Effective Performance Engineering practice.\n\nMonitoring In many organizations, monitoring is a practice adopted only within production, and is often referred to as application performance mon‐ itoring (APM). However, reactive performance monitoring happens too late to have an impact on revenue, brand, competitive advan‐ tage, and customers. All those factors have already been determined once they’re identified in production. In Effective Performance Engineering, we leverage several monitoring capabilities, explored in more detail in the following sections.\n\nContinuous monitoring and feedback\n\nMonitoring environments and components within your architecture and perhaps outside, both within production and pre-production systems, helps you learn about and improve capacity, resiliency, per‐ formance, and scale. In addition, continuous monitoring and feed‐ back provides you with ways to observe and capture conditions in which production incidents occur, so you can re-create them in pre-\n\n46\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "production and validate a fix prior to pushing it back into produc‐ tion.\n\nWithout feedback throughout the lifecycle, you only have a best guess and a hope that a solution will work after it is deployed. We often see this in practice, and yet with Effective Performance Engi‐ neering practices, it does not need to be this way.\n\nHow this process is done varies by the organization and its maturity, but it starts with monitoring both in production and pre-production environments, then moving to measure the key performance indica‐ tors (KPIs) both from a technical and business perspective. Once these have been achieved, you can start by showing these results for the release candidate and then move to build-level results.\n\nEnd-user feedback and analysis\n\nEnd-user feedback can be measured in both objective and subjective results: objective from transactions completed, conversion rates, and response times, and subjective through reviews and interview and survey feedback.\n\nYour end user is the person that matters the most; getting their feed‐ back and analyzing the observations is a key piece of monitoring.\n\nThere is a vast array of ways to gather both objective and subjective feedback from your end users. These range from formal studies in which you invite participants to join you in a lab-like setting to view some new capabilities and features, asking them for feedback, to simply pushing a few new capabilities in production to your canary group, and observing the KPI metrics based on how they use the system. In some cases you might capture and measure heart rate, eye movement, screen tapping, and other behavioral and cognizant responses. This data will all be analyzed so it can be fed back to the teams, and who will use it to create the next iteration of the capabil‐ ity, before eventually deploying it to production.\n\nPredictive: pre-production and production\n\nYou can use the data you’ve gathered and analyzed from production and pre-production environments to predict under what conditions degradation or system failure will occur, so you can operate proac‐ tively. This predictive data enables you to mitigate these issues throughout the lifecycle, so you can better deliver new feature func‐ tion to your end users uninterrupted.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n47",
      "content_length": 2315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Getting your product wrong is a huge expense in many ways. So, having insight that provides predictive recommendations on what will go wrong and when is a huge advantage to your organiza‐ tion. This is used throughout pre-production and production envi‐ ronments.\n\nWhen leveraged with correlation and big data analytics, predictive data provides us with insight into what could go wrong based on what has gone wrong historically. Results such as Fundex and User Sentiment indicate how your end users will and are responding to new capabilities.\n\nSupport Effective Performance Engineering does not end at production; it is a continuous, iterative practice of integrating feedback, improving the entire lifecycle with every new piece of information, and auto‐ mating this cycle. Advancements in big data and predictive analyt‐ ics, combined with these practices, enable a more stable, high- performance experience for end users.\n\nThreshold analysis and automated mitigation implementation\n\nThreshold analysis starts with defining specific thresholds within your systems and applications for your application performance monitoring capabilities to alert and alarm, then building in rules- based automated mitigation implementation, which allows the prod‐ uct or service your end user is accessing to continue with minimal interruption. Utilizing this approach increases the resiliency of your architecture.\n\nMany companies today have set up threshold analysis in the form of a network operations center (NOC), staffed by a few individuals around the clock, with dozens of monitors projecting thousands of alerts per day and sometimes hourly. Having this automated with built-in and tested mitigation implementation simplifies and ensures the business is protected, enabling the technology to grow and automatically scale with the company’s needs.\n\nWhile this area is still maturing from a solutions perspective, it can be done and delivered successfully when implemented by a team that considers it a collaborative goal.\n\n48\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Incident management, root-cause analysis, and reporting\n\nIncident management is the practice of actively managing produc‐ tion (and in some cases pre-production) incidents. Its chief purpose is capturing the “current state” and enabling the re-creation of the incident in pre-production, while finding and fixing the root cause. This root-cause analysis, along with tracking and reporting trends, will support the technical debt and investment needed to ensure your architecture is performing well.\n\nHistory does repeat itself. This is especially true for a technology- enabled business that follows a defined set of instructions the same way every time. Thus, we must build in learning cycles in order to understand why something happened and prevent it from recurring.\n\nMany solutions exist off the shelf today. However, first you must assign accountability and responsibility for this activity. We have often seen an incident management team that owns the process of the production incident, but who owns the learning and feedback to prevent that incident from happening again is not always clear. In Effective Performance Engineering, each task must first have a defined process and owners, and only then be enabled with technol‐ ogy.\n\nRe-create production incidents pre-production\n\nAll too often we see pre-production teams working on and attempt‐ ing to fix production incidents. When production incident fixes are applied in production, what gets released are non-integrated updates that are not in any way validated or tested. Not only is this a risky approach for production, but now you have changes that are not being implemented into the existing pre-production assets for future deployments. This means these incidents will happen again. It is also helpful to ask, “Did these hotfixes really fix the problem, or just push them off to another day?”\n\nLeveraging Effective Performance Engineering practices enables you to quickly capture a snapshot of what happened in production, then spin up the environments that pre-production deployed and re- create the incident, so you can follow your normal process to make quality fixes to your production release.\n\nPerformance Engineering Throughout the Lifecycle\n\n|\n\n49",
      "content_length": 2223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Stakeholders With Effective Performance Engineering, the stakeholders are from all walks of life. Whereas with traditional Performance Testing, the sole responsibility for performance fell on a select group of individ‐ uals, in Performance Engineering, it is the entire team’s responsibil‐ ity to work in a broad and collaborative manner encapsulated by the organizational culture.\n\nOnce you drive incremental value and success within a group or team, this culture accelerates. Both as you start adopting these capa‐ bilities and as you continue to integrate them, it is critical that you understand your stakeholders, keep their best interests in mind, and communicate with them frequently. We will dissect what each stake‐ holder group looks like in the following subsections.\n\nDevelopment Performance Engineering is often not a top priority for a typical development team, especially given all the unique and expanding challenges related to the complexity of composite architectures, dis‐ tributed organizations, and end users. The reason for development’s growing importance is to ensure they deliver the highest quality and performing product, and provide continuous feedback and optimi‐ zation recommendations, so other teams can deliver quickly and in fully automated ways.\n\nTesting and Quality Assurance The responsibility of a testing and quality assurance team independ‐ ent or integrated into teams is to ensure the delivery of a quality product. How this happens and where the focus is depends on who you are speaking with, and how they are aligned. For example, some organizations align by specialty (security, performance, functional, usability, and so on) and others have an integrated team. With effec‐ tive Performance Engineering practices, a group of individuals is accountable for the delivery of a high-performing solution through‐ out the development process, and delivers that with clear measure‐ ments and communications throughout the lifecycle.\n\n50\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Operations Operations teams are focused on ensuring the product is running and service is available. “Less is more” is this stakeholder’s perspec‐ tive. In other words, the less impact a change has on the production environment, the better. This is why defining and validating what is moving into production is so important; the operations stakeholders will want to see how and why a capability is ready for the production environment. The practices of Effective Performance Engineering involve these individuals throughout, allowing them to contribute and be a part of the team steering a high-quality product through the environments and into production.\n\nBusiness The focus of this stakeholder is on ensuring that revenue, competi‐ tive advantage, customer, and brand goals are achieved. This includes expanding offerings and/or businesses either through organic or acquisition approaches, all of which may depend on existing or new platforms, so end users can consume products and services without interruption when, where, and how they want to.\n\nEnd Users End users ensure feedback is delivered, and value from product and/or service is realized. Users simply do not want security, perfor‐ mance, functional, or usability issues while interacting with your products or services. Making them a champion for your brand should be your goal; you do so by continuously meeting or exceed‐ ing their expectations, and asking them to share their positive expe‐ riences with others frequently.\n\nOf course, you will encounter other stakeholders, but start by identi‐ fying which of these five existing roles fit into your organization. Begin thinking about how you can apply some of the Effective Per‐ formance Engineering practices in your organization today, aligning them with the interests of these stakeholders. Include your stake‐ holders in these conversations, ask them for feedback on what they get and what they need, and show them your desire to make them your biggest supporter and champion.\n\nStakeholders\n\n|\n\n51",
      "content_length": 2019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Building in Performance Performance cannot be an afterthought; it needs to be at the fore‐ front of your team’s thinking from the very beginning and through‐ out. This often comes into conflict with the “release faster” mentality that drives many businesses today, but it doesn’t have to. The speed/ accuracy tradeoff that is often cited in Lean startup principles doesn’t necessarily apply to the performance of the applications we build, but rather to the speed with which we deliver and the accu‐ racy of our delivery compared to what the market needs. When it comes to Lean startups, it isn’t necessary to hit the mark 100% right away. Fast feedback allows us to adjust our course and reiterate very quickly. But what happens if the products we deliver to the market perform poorly? There won’t be a second chance to get things right if the customers’ first impression is of a slow or unresponsive app.\n\nIt is our responsibility to bring these capabilities and practices to our business owners and CxOs so they can understand why they’re important and how others are delivering compelling results to their end users, and will enable your business to do the same or more.\n\nThe List: 102 Questions to Ask Performance Engineering is a complex discipline encompassing applications, infrastructure, security, and more. To truly optimize your performance, your organization needs to address a broad range of issues. To make informed decisions, individuals and organiza‐ tions need to start asking some or all of the following 102 questions.\n\nThis list is intended to be inclusive but not exclusive, and apply across all DevTestOps approaches. No matter what your role, per‐ sona, and interest, this list should help you understand how your solution is engineered for performance, stability, and scalability.\n\nOur goal is to help you, your team, and your stakeholders gain a common nomenclature for Performance Engineering so you can define your path and direction together. We intend the following as a checklist of actionable items, so you can ask informed questions as you start collaborating and adopting Effective Performance Engi‐ neering practices throughout your lifecycle.\n\n52\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 2233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Server sizing\n\nHow many application servers are needed to support the cus‐ tomer base?\n\nWhat is the optimal ratio of users to web servers?\n\nWhat is the optimal web server–to–application server ratio?\n\nWhat is the maximum number of users per server?\n\nWhat is the maximum number of transactions per server?\n\nServer tuning and optimization\n\nWhich specific hardware configurations provide the best performance?\n\nHow can vendor default configurations be tuned to suit this specific infrastructure and application?\n\nWhat\n\nsystem\n\nresources need\n\ntuning\n\nto give optimal\n\nperformance?\n\nCapacity planning\n\nWhat is the current production server capability?\n\nIs there room for growth?\n\nWhat hardware or software can be added to achieve the next level of performance or capacity?\n\nIs there excess capacity? Can a server be removed without com‐ promising performance?\n\nThird-party validation\n\nWhat is the current ISP and network capacity?\n\nCan the ISP deliver on the service-level agreement that was signed?\n\nSecurity exposure\n\nCan system vulnerabilities be identified and minimized?\n\nBuilding in Performance\n\n|\n\n53",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "What is the failover for firewalls?\n\nAre there new vulnerabilities when excess user load is added to the application?\n\nHow susceptible is the system to DoS attacks?\n\nIf a DoS attack occurs, how will the system respond?\n\nIf the system goes down due to a hacker attack, how effective are the recovery procedures?\n\nInfrastructure The following are some questions you should ask regarding the infrastructure.\n\nBrowser/user profile issues\n\nThis subsystem is known as the user community profile and consists of business process definitions.\n\nWhat do the users do? (These are business-process definitions.)\n\nHow fast do the users do it? What are the transaction rates of each business process?\n\nWhen do they do it? What time of day are most users using it?\n\nWhat major geographic locations are they doing it from?\n\nIs the application browser- or interface-dependent?\n\nIs modem, WAN, or LAN emulation necessary?\n\nAre there asynchronous communications between the browser/ client and the backend servers?\n\nAre there any non-HTTP(s) communications between the browser/client and backend servers?\n\nInternet issues\n\nWhat are the peering issues associated with the client’s hosting/ bandwidth provider?\n\nWhat is the hosting strategy?\n\n54\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Site web pipe issues\n\nHow much bandwidth does the site have?\n\nWho is the client’s bandwidth provider? (Peering issues)\n\nAre there multiple web pipes?\n\nBorder router issues\n\nWhat kind of load-balancing are the multiple pipes configured for?\n\nDoes it use the same inbound pipe as outbound pipe?\n\nIs there equal distribution for outbound regardless of inbound pipe?\n\nIs there the same outbound pipe regardless of the inbound pipe?\n\nAre there multiple border routers?\n\nWhat is the failover configuration for multiple border routers?\n\nLoad-balance issues\n\nWhat type of load-balancing scheme is used? (Round robin, sticky IP, least connections, subnet based?)\n\nWhat is the timeout of LB table?\n\nDoes it do any connection pooling?\n\nIs it doing any content filtering?\n\nIs it checking for HTTP response status?\n\nAre there application dependencies associated with the LB time‐ out settings?\n\nWhat failover strategies are employed?\n\nWhat is the connection persistence timeout?\n\nAre there application dependencies associated with the LB time‐ out settings?\n\nWhat are the timeouts for critical functions?\n\nBuilding in Performance\n\n|\n\n55",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Peripheral systems issues\n\nIs the LAN/WAN system dedicated or shared with other appli‐ cations?\n\nAre there any shared production resources?\n\nAre there any web pipes, ERP systems, mail servers, filesystems, DNS servers, and so on?\n\nDoes it share databases with other applications?\n\nDoes it share hardware with other applications?\n\nExternal systems issues\n\nAre there any outside vendors that provide content distribution systems (CDS) for the architecture?\n\nDistributed hosting issues\n\nAre these multiple mirrored sites?\n\nIs any site configured for failover operation?\n\nHow is the traffic load-balanced across the sites?\n\nAre\n\nthere architecture components on\n\nshared WAN\n\nconnections?\n\nWhat is the failover and recovery behavior?\n\nFirewall issues\n\nWhat is the throughput capacity?\n\nWhat is the connection capacity and rate?\n\nWhat is the DMZ operation?\n\nWhat are the throughput policies from a single IP?\n\nWhat are the connection policies from a single IP?\n\nIDS: Intrusion detection systems\n\nIs there statistical content sampling?\n\n56\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Is there an inverse relationship between throughput and security?\n\nHow is content filtering achieved?\n\nApplication Here are some of the questions you should be prepared to ask regarding the application.\n\nWeb server issues\n\nHow many connections can the server handle?\n\nHow many open file descriptors or handles is the server config‐ ured to handle?\n\nHow many processes or threads is the server configured to handle?\n\nDoes it release and renew threads and connections correctly?\n\nHow large is the server’s listen queue?\n\nWhat is the server’s “page push” capacity?\n\nWhat type of caching is done?\n\nIs there any page construction done here?\n\nIs there dynamic browsing?\n\nWhat type of server-side scripting is done? (ASP, JSP, Perl, Java‐ Script, PHP, and so on)\n\nAre there any SSL acceleration devices in front of the web server?\n\nAre there any content caching devices in front of the web server?\n\nCan server extensions and their functions be validated? (ASP, JSP, PHP, Perl, CGI, servlets, ISAPI filter/app, and so on)\n\nMonitoring (Pools: threads, processes, connections, and so on; queues: ASP, sessions, and so on; general: CPU, memory, I/O, context switch rate, paging, and so on)\n\nBuilding in Performance\n\n|\n\n57",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Application server issues\n\nIs there any page construction done here?\n\nHow is session management done and what is the capacity?\n\nAre there any clustered configurations?\n\nIs there any load-balancing done?\n\nIf there is software load-balancing, which one is the load- balancer?\n\nWhat is the page construction capacity?\n\nDo components have a specific interface to peripheral and external systems?\n\nDatabase server issues\n\nHave both small and large data sets been tested?\n\nWhat is the connection pooling configuration?\n\nWhat are its upper limits?\n\nSecurity Here are some of the questions to ask when addressing security issues.\n\nFirewalls and multiple DMZs\n\nDoes the firewall do content filtering?\n\nIs it sensitive to inbound and/or outbound traffic?\n\nWhat is its upper connection limit?\n\nAre there policies associated with maximum connection or throughput per IP address?\n\nAre there multiple firewalls in the architecture (multiple DMZs)?\n\nIf it has multiple DMZs, is it sensitive to data content?\n\n58\n\n|\n\nChapter 2: Overview of Performance Engineering",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "IDS: Intrusion detection system\n\nIs there any content filtering?\n\nIs the system sensitive to inbound and/or outbound traffic?\n\nWhat are the alert thresholds?\n\nWhat are the acceptable security thresholds?\n\nNaturally, these questions are only a starting point—you also need to come up with answers—and they don’t cover every possible issue in Performance Engineering. How will you use these questions? What would you add to the list?\n\nBuilding in Performance\n\n|\n\n59",
      "content_length": 463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "CHAPTER 3 Proven Practices of Performance Engineering\n\nTo explore the proven practices of Performance Engineering, we will start with the requirements, architecture, and design; hit some of the highlights of implementation; and walk you through a real-life sce‐ nario. The objective of this chapter is to present a complete case study for each practice so you can begin to understand what it means for you, and to provide you with a story you can use and share with your team or organization.\n\nRequirements, Architecture, and Design Here is a list of proven practices for requirements, architecture, and design:\n\nIdentify components\n\nSet performance budgets\n\nEstablish acceptance criteria\n\nPlan for outliers\n\nBuild in performance culture\n\nPrototype (and test)\n\n61",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Introduction One of the questions many people ask themselves while adopting Effective Performance Engineering practices is, “How do I engineer configuration and applications before starting development?”\n\nThe building in Figure 3-1 represents the requirements of software and architecture architecture and design.\n\nFigure 3-1. The requirements of architecture and design represented as a building\n\nToday, teams architect and design within their own pillars, typically within a development or architecture team, and sometimes seen within a “Sprint 0” or other phase if the project or system is new.\n\nThe increasing complexity of composite applications, and the multi‐ tude of end users and ways of consuming products and services, has compounded the root-cause issues described in Chapter 2. Effective Performance Engineering provides a new way of thinking about software and hardware systems and how to architect and design them so your end user has a great experience, and you have a high- performing and resilient capability supporting your products and services.\n\nIn many cases these are complex and dependent systems—some‐ times new, but often existing and integrated throughout both the frontend and backend. Knowing this, you may be focused initially on “big risk/big impact” systems like web and core systems.\n\n62\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 1379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Although the risk and reach is high, so is the value you can deliver to the business and your end users.\n\nScenario Company ABC is a large financial services institution that has grown over the years to 9 million+ customers and continues to grow at a high rate both organically and through acquisition. They have been transforming to integrated and self-managed Agile teams for nearly a year. A new epic has been prioritized for a new capability that will be rolled out to all customers across all channels, and although it is a vendor commercial-off-the-shelf (COTS) product, it has not been previously deployed at this scale by the vendor or any other known enterprise organizations.\n\nChallenges\n\nAs you might imagine, this task is not without its challenges—some known and some not. To help you spot these potential challenges, here are some additional details.\n\nThis is the largest Internet-based bank in the United States. The new epic has to do with a login security capability, in which end users will no longer be using their PIN but an image + phrase + login + password to gain access to their accounts. This is a very good brand, having earned the business of 9 million+ Americans and seeing low double-digit growth rates of new customers. 180,000 logins per hour across 7 total major geographic locations worldwide (5 of which are in the US). Of the 9 million+ customers, 40%+ most often access their accounts via mobile with a range of mobile conditions across 2.5G, 3G, and 4G connection types. When rolled out, 100% of all customers upon login will be requested to set up and complete the new login procedure prior to getting access to their accounts. This is planned as a software-only install, with no new hardware or other infrastructure upgrades needed, per the vendor.\n\nYou can imagine some of the complexities that might exist or quickly begin to surface with this scenario.\n\nOption 1\n\nRoll out the new login to all customers simultaneously, and provide additional infrastructure to support the initial spike in activity.\n\nRequirements, Architecture, and Design\n\n|\n\n63",
      "content_length": 2087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "The pros of Option 1 include:\n\nIncreased marketability of the new feature (enhanced security capability for end users)\n\nShortest perceived time to first implementation\n\nShows urgency in response to partnering technology with busi‐ ness\n\nThe cons of Option 1 include:\n\nIncreased risk of failure\n\nPotentially high infrastructure cost\n\nIntroduced elasticity\n\nrequirement,\n\nforcing potential\n\narchitecture of the application\n\nDependency on potential unknown cloud service provider to scale\n\nOption 2\n\nRoll out the new login to a reduced segment of the population, using a staggered approach.\n\nThe pros of Option 2 are as follows:\n\nPerceived reduced risk of failure\n\nAbility to monitor the affected install base and make quick deci‐ sions to fix if needed\n\nMeasurable incremental scaling of capacity monitored and observed with ramp-up\n\nAnd the cons of Option 2 include:\n\nDuplicate infrastructure during the rollout to support dual authentication mechanisms\n\nPotential database and parallel user login profiles during transi‐ tion and possibly higher risk remediation\n\nMay take longer to roll out to large-scale customer base\n\n64\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\nre-",
      "content_length": 1188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Option 3\n\nAllow customers to opt in to the new login method over a period of several days or weeks.\n\nHere are the pros of Option 3:\n\nLowest potential risk of impact from high adoption rates of new login credentials\n\nAnd here are the cons of Option 3:\n\nNo incentive for customers to switch\n\nHigh likelihood that customers will not switch, leading to longer time to support both login types\n\nSample size of data through conversions potentially too small for any meaningful indication of future impact\n\nProbably will need to push a force to transition in the future, which could lead to massive customer demand\n\nPerceived risk in the market to customers and/or regulatory impact\n\nRecommendation Given these three options, we recommend Option 2. It provides a balanced approach by deploying the capability in a timely manner, while mitigating both the risk of overcapacity from demand as well as the business risk from brand and regulatory impact.\n\nSummary This scenario was based on a real-world situation in which the resulting impact was nearly catastrophic to the organization. At the time, there was no indication that a well-established commercial product would have such a detrimental impact.\n\nHere we have the benefit of hindsight to tell us there were better ways to roll this change out, capture relevant usage and impact data from a subset of the production users, and make the necessary adjustments. It is important to ask, “What will be the impact if this fails?” and somehow mitigate that without adding weeks or months to the project. Architects rarely get to see their vision brought to life.\n\nRequirements, Architecture, and Design\n\n|\n\n65",
      "content_length": 1651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "In the real-world scenario, what happened was a staggered approach to deployment. However, what was not known or predicted was the users’ behavior: specifically, that nearly every user would scroll through all 100+ images to see which related most to them, and that each image was 700KB in size with 5 images per page displayed. As a result, even with only 1% of the population (90,000 customers) set‐ ting up their new login credentials, each pulled a huge amount of data (3.5MB per page, at 20+ pages per customer) through the data center network lines. The result was a massive production incident that taxed nearly all systems due to overcapacity issues. This was compounded by the fact that ~40% of users were on mobile connec‐ tions, causing the sessions to remain open and ultimately run out of connections throughout all components.\n\nHow-To In this scenario, you can see why the proven practices of Perfor‐ mance Engineering for requirements, architecture, and design should play a significant role within your organization. The first step is identifying the components, you should consider within the infra‐ structure and application architecture, knowing some will be inter‐ nal and others external, and some private while others are shared. The next consideration is setting performance budgets, or allocating milliseconds per component to target, in order to deliver the desired end-user experience. Defining the acceptance criteria for each com‐ ponent, specifying the conditions and use case (e.g., network condi‐ tions and image sizes), is critical. You can see how planning for outliers might have led the bank to make a bit more capacity avail‐ able, and a more conservative deployment approach. Building in per‐ formance culture would have helped a lot here, especially in considering vendors for commercial off-the-shelf (COTS) products and capabilities, and establishing performance criteria as part of the interrogation criteria prior to acquisition through the procurement process—let alone production deployment. This is where also proto‐ typing and testing would have provided significant insight and infor‐ mation proactively, which the bank could have then applied and used as feedback in determining the best technology approach with the business for the end user.\n\n66\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 2354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Key Implementation Considerations\n\nOrganizational\n\n— Where do people sit (different teams versus integrated)?\n\n— Who is accountable for performance?\n\nCultural\n\n— Place value on performance\n\n— Hold people accountable for performance\n\n— Tie compensation to performance\n\nTechnical\n\n— Build performance into the story\n\n— Build performance into the architecture\n\nProven Practices for DevTest Continuing our journey into the proven practices of Performance Engineering, next we describe a DevTest scenario and explore some of the highlights. In this section, we once again start with a list of proven practices, then walk through an introduction, followed by a scenario, summary, how-to, and key considerations.\n\nHere are the proven practices of Performance Engineering for DevTest:\n\nBuild performance into your UNIT tests\n\nBuild performance into your build validation tests (BVT), get‐ ting results after every build\n\nTrack the trend of results for systems and components\n\nAutomate quality gates in the build in order to avoid perpetuat‐ ing poorly performing components\n\nConsider a branching strategy that enables you to “keep out” pieces that will break the code and/or make it perform poorly\n\nProven Practices for DevTest\n\n|\n\n67",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Introduction Performance Engineering during application development consists of testing the application in as realistic an environment as is avail‐ able, without impacting the velocity of the team, and getting rele‐ vant performance feedback into development in an automated way. Assuming we have the necessary goals defined from requirements, architecture, and design, this feedback should provide KPIs to sup‐ port those goals. For example:\n\nThe application needs to support 10,000 active users with sub- second responses for key transactions such as Login, Search, or Confirm Order.\n\nThe application must support a peak volume of 1,000 transac‐ tions per second with processing time of no more than 500ms.\n\nThese KPIs should reflect the goals of the business, and provide a target for success and improvement such that every build and release of the application is measured consistently against these goals.\n\nMoreover, the process by which these KPIs are measured should reflect the goals and delivery method best suited to support the busi‐ ness. This means an application that follows an Agile or hybrid approach to development and testing should not be held up by the activities of the team or individuals responsible for performance. This is critical, and should define the level of automation and meas‐ urement that can be achieved in order to provide the greatest level of feedback with the least amount of impact to the application deliv‐ ery chain.\n\nTo support this statement, we will use an example scenario in which an application team is tasked with reaching certain business goals for performance, but is forced to make tradeoffs in order to main‐ tain their release velocity.\n\nScenario Company XYZ is about to launch a new version of their online ship‐ ping application, which currently services 2,000 businesses and 95,000 individuals via mobile and web clients in North America.\n\n68\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "The new version is required to support their European launch, adding an estimated 150,000 new customers as well as 15 new regional shipping services and 5 new payment vendors across 28 countries.\n\nThe service-level objectives (SLO) of the business are to process all orders within 2 seconds, although key stakeholders do not have a good sense of peak user volumes. Estimates from business analysts in the Marketing team estimate an additional 45,000–65,000 indi‐ vidual users.\n\nChallenges\n\nFor the Performance Engineering team to validate the SLO, they need an estimated three weeks to build a production-like environ‐ ment, and another four weeks to complete testing and analysis. Even if a reasonable amount of overlap is achieved, this release delay is unacceptable to the business. The developers don’t believe this work is needed, as they are confident in their architecture decisions and in the quality of their code. They also believe that any issues found in production can be fixed quickly enough to minimize the impact. In this example organization, all teams are independent and dis‐ tributed groups, and Development carries enough influence that this line of thinking is gaining popularity.\n\nConversely, the media has run stories about the company’s expan‐ sion into Europe, primarily led by competitor messaging that the company is likely to fail to meet the demands of such a broad and diverse region. Many in the company are aware of the risk of failure to launch, and support the need to validate performance. You can present one of three options in hopes of avoiding a major failure during this important launch.\n\nOption 1\n\nTest the end-to-end application in a completely integrated and scaled environment at the end of development.\n\nThe pros of Option 1 include:\n\nComplete picture of application performance before go-live date\n\nAbility to identify bottlenecks within and beyond the company’s infrastructure\n\nProven Practices for DevTest\n\n|\n\n69",
      "content_length": 1962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "The cons of Option 1 are as follows:\n\nMassive delays to the project\n\nDifficult to resolve issues late in development\n\nLittle room for additional delays or retesting\n\nOption 2\n\nTest the end-to-end application in a scaled-down and virtualized environment using service levels provided by third-party services to represent external dependencies.\n\nHere are the pros of Option 2:\n\nReasonable facsimile of production performance\n\nAbility to virtualize external dependencies reliably and capture “what if” scenarios\n\nFewer delays to the project timeline, more time for re-testing\n\nAnd here are the cons of Option 2:\n\nAssumption-based approach to capturing third-party depend‐ ency performance\n\nStill difficult to resolve issues late in development\n\nOption 3\n\nTest individual application components throughout development, with virtualized internal and external dependencies. The end-to-end performance will be tested at the end of each sprint/cycle.\n\nThe pros of Option 3 are:\n\nFast feedback to the development team\n\nLittle to no delays to the project\n\nReasonable facsimile of production performance\n\n70\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "The cons of Option 3 are:\n\nAssumption-based approach to capturing third-party depend‐ ency performance\n\nRecommendation The recommendation is Option 3. Application changes tend to focus on a few components that can be scaled close to production, while the surrounding dependencies are outside of our control. These may belong to other teams or organizations, or are cost-prohibitive to build to scale in a DevTest environment. Leveraging virtualization for these dependencies is key to isolating and identifying the impact to the components under our control.\n\nSummary In reality, the best-fit solution for testing any application involves some tradeoffs between the completeness of our picture of perfor‐ mance and the time in which we deliver. There is no one answer that fits every application, environment, and organization, but in most situations we should strive to deliver as much information as possi‐ ble, as quickly as possible. In general, there are three factors you should consider when deciding how to test: cost, quality, and time. We can choose which of these is most important, but the closer we get to one, the more we sacrifice of the other two.\n\nFigure 3-2 shows the triangle of three key factors that everyone wants; however, it is often said you can choose only two.\n\nFigure 3-2. The three key factors\n\nProven Practices for DevTest\n\n|\n\n71",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "How-To One approach to delivering critical performance information to the Development team without slowing or halting their progress is to automate the execution of performance testing during each and every build. The goal is to understand the performance delta between builds, with a focus on KPI trends rather than the accuracy of individual metrics.\n\nIn order to achieve rapid feedback in a continuous integration envi‐ ronment, you must reuse assets and virtualize dependencies cap‐ tured during previous iterations. Automated execution and analysis are critical. Wherever possible, automation should extend to the analysis of tests as well. Most testing tools, such as Jenkins and Bam‐ boo, extend continuous integration platforms to provide perfor‐ mance trending and feedback after each build is created. Figure 3-3 shows a typical lifecycle representation along with call-out boxes of how performance can be built in at specific stages, demonstrating a possible flow through development that provides quick time-to- value and will not significantly impede development velocity.\n\nFigure 3-3. Typical lifecycle representation\n\nThe most significant feedback comes from performance testing, which should happen in three stages:\n\n1. Execute the baseline test after each and every build.\n\n72\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 1352,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "2. Add/change incremental functionality to the test library as it becomes available.\n\n3. Add new/changed features to the baseline at the end of each release.\n\nIn this context, the baseline test refers to the previously accepted release criteria, or some level of test that represents the most com‐ mon usage and behavior of the application. BVT for performance may be contentious for some organizations that have not bought into the value of performance to the company. In these cases, it is important to understand:\n\nWho is responsible for the performance and stability of the build?\n\nWhat information can we deliver that will add value for that person or group?\n\nEvery team, technology stack, and application will be different, but we can strive to provide relevant, repeatable, and realistic data in a reasonable timeframe, to avoid blocking the development process.\n\nParallel to automated test execution and analysis, Performance Engi‐ neering teams should execute a series of focused tests to validate the scalability and resilience of the application. These are often run against a pre-production or dedicated performance environment (for the very fortunate), and account for the overarching perfor‐ mance requirements that are not easily captured in a single set of release requirements.\n\nIn addition to testing, there needs to be some sort of continuous feedback from production. In the following sections, we will discuss some of the ways that monitoring can be used to provide critical information to development and testing, and impact the design phase as well.\n\nKey Implementation Considerations\n\nOrganizational\n\n— Who is responsible for your builds?\n\n— Are they considering performance?\n\n— What does “pass” mean?\n\nProven Practices for DevTest\n\n|\n\n73",
      "content_length": 1762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Cultural\n\n— What happens when you break a build?\n\n— How is progress/status socialized?\n\nTechnical\n\n— How are you enabling automation across tools, teams, and\n\nroles?\n\nProven Practices for Operations Lastly, in our exploration of proven practices of Performance Engi‐ neering, we will elaborate on Operations, highlighting, and illustrat‐ ing important points through a real-life scenario. Again we start with a list of proven practices, walk through an introduction, and follow with a scenario, summary, how-to, and key considerations.\n\nHere are the proven practices of Performance Engineering for Oper‐ ations:\n\nMake sure disaster recovery, capacity planning, and resiliency are all known, tested, and observed.\n\nProvide continuous deployment and operations from pre- production environments through to production.\n\nSet up a production environment with: canary, live/live, or blue/ green deployment approaches.\n\nEstablish predictive, scaled growth, feature and configuration tracking, and continuous feedback.\n\nMaintain an inclusive monitoring strategy with complete and continuous feedback across all environments.\n\nIdentify the most commonly used features and functionality, updated and used across the stack and lifecycle.\n\nIdentify the “hot” areas: software, hardware, configurations, and so on.\n\nEstablish a production incident review process focused on learning and long-term stability.\n\n74\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 1456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Introduction Operations is often considered the most important part of the business, as this group is responsible for ensuring the Production team is able to provide the products and services that end users want and for maintaining very high uptime and an extremely low incident rate.\n\nSeveral organizations (possible even a majority) have sought to reduce overall operational expenses by outsourcing operations tasks to a partner in a long-term contract (often 10+ years), in exchange for a guaranteed savings rate per year.\n\nThere is nothing wrong with this strategy. That said, many organiza‐ tions have limited time and resources allotted for transitioning operations to the partner, and often that partner has not yet adopted Effective Performance Engineering practices.\n\nThat’s a recipe for less-than-desirable results, and often becomes a point of contention wherein both parties spend much energy and effort negotiating the contract rather than focusing on the end user.\n\nLet’s take a look at such a scenario, and how you can mitigate these challenges with Effective Performance Engineering practices.\n\nScenario Company 123 is a large international business with 9 reportable seg‐ ments (Agriculture and Nutrition, Nylon Enterprise, Performance Coatings and Polymers, Pharmaceuticals, Pigments and Chemicals, Pioneer, Polyester Enterprise, Specialty Fibers, and Specialty Poly‐ mers) spanning more than 70 countries worldwide. They make 49% of consolidated sales to customers outside the US, employ a total of 93,000 people, and have a $68 billion market cap. In addition, they operate the largest Electronic Data Interchange (EDI) operations and are the largest SAP customer in the world.\n\nThey are seeking to optimize their growing operations and support systems, as well as reduce the associated costs, while still maintain‐ ing the service level for their end users. Their intent is to partner and outsource the global information systems and technology infra‐ structures in order to provide selected applications, software serv‐ ices, and information systems solutions designed to enhance manufacturing, marketing, distribution, and customer service.\n\nProven Practices for Operations\n\n|\n\n75",
      "content_length": 2203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "The company is seeking to reach a 10-year agreement with a partner to transition all operations and support with a guaranteed savings of 10% per year for the 10 years of the initial contract term. Transition is scheduled to take place over a 3-month period and at least 70% of the existing company resources will transition to the new partner organization as part of this operations outsourcing agreement.\n\nChallenges\n\nIf you have ever been a part of something like this, then you know: the sheer size of this maneuver is scary. Now add the fact you are outsourcing your “global information systems and technology” busi‐ ness for 10 years to a partner (where your talent is going), and it gets exponentially more interesting.\n\nNeedless to say, this transition was not without several challenges, and we will see how this organization became a cautionary tale for the importance of implementing Effective Performance Engineering capabilities that benefit all parties. Of course, let’s not forget Com‐ pany 123 is nearly 100 years old, comprises 9 segments in 70+ coun‐ tries, serves 93,000 people, and has a $93 billion market cap.\n\nOption 1\n\nSingle partner.\n\nThe pros of Option 1 are as follows:\n\nSingle neck to wring if needed\n\nLittle ambiguity or dependency on partner’s ability to deliver on the contract to company\n\nTalent goes to a single-partner organization\n\nThe cons of Option 1 are as follows:\n\nLimited diversification, consolidated risk\n\nOption 2\n\nTwo partners.\n\nThe pros of Option 2 are:\n\nCompetitive environment\n\n76\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 1586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Diversity of service, redundancy\n\nThe cons of Option 2 are:\n\nMultiple players\n\nIncreased possibility for finger-pointing\n\nTalent dispersed over two partner organizations\n\nOption 3\n\nHybrid (split: Company 123 resources and one or more partners).\n\nHere are the pros of Option 3:\n\nCompany can control speed of transition\n\nCompetitive environment\n\nIterative learning and feedback\n\nAnd here are the cons of Option 3:\n\nMay take longer to transition\n\nReduction of operational expense delayed\n\nMore company effort required\n\nRecommendation The recommendation is Option 3. Option 3 enables Company 123 to begin to realize the benefits and value of transitioning operations to a partner, and adapt and change their business practices over time, while observing the positive and negative effects on end users.\n\nSummary In reality, Company 123 moved forward with Option 2. As a result, Partner 1 contracted to operate a majority of Company 123’s global information systems and technology infrastructure, and provide selected application and software services; and Partner 2 contracted to provide information systems solutions designed to enhance Com‐ pany 123’s manufacturing, marketing, distribution, and customer service.\n\nProven Practices for Operations\n\n|\n\n77",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "The company reached a 10-year agreement with both partners to transition nearly all operations and support with a guaranteed sav‐ ings of 10% per year for the 10 years of the initial contract term. Transition was scheduled to take place over a 3-month period with at least 80% of the existing company resources transitioning to the new partner organization as part of this operations outsourcing agreement.\n\nDue to many of the identified “cons” of Option 2, this approach was riddled with challenges throughout the term of the contract, with the transition working out for one partner and not the other, and Company 123 ended up taking back much of the outsourced sup‐ port and operations. The end users suffered the most, followed by Company 123’s businesses; today, some 15 years later, the company has a $46 billion market cap—approximately 50% smaller than it had before the partnership.\n\nHow-To In this scenario, you can see why the proven practices of Perfor‐ mance Engineering for operations should play a significant role within an organization. These practices start with disaster recovery, capacity planning, and resiliency, which should all be known, tested, and observed for critical capabilities and functions. Continuous deployment and operations spanning from the earliest pre- production environments through to production enables you to test the deployments, control and protect the deployment procedures, and measure the quality of the release candidate. A production envi‐ ronment enabling canary,live/live, or blue/green-type deployment approaches can mitigate the risks associated with traditional, single- data-center, big-bang deployments. Predictive models allow you the chance to gain insight into how a capability will scale under growth, track features and configuration, and get continuous feedback from pre-production through production and back, supported by an inclusive monitoring strategy with complete and continuous feedback across all environments. Other practices include identifying the most commonly used features and functionality, in order to show when updates happen and how they’re used across and beyond the stack and lifecycle, and identifying where the “hot” areas are related to software, hardware, and configurations. Lastly, establish a solid cross-team production incident review process rooted in and focused on learning and the long-term stability of the team and all environ‐\n\n78\n\n|\n\nChapter 3: Proven Practices of Performance Engineering",
      "content_length": 2491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "ments, in order to make products and services continuously avail‐ able to the business and end users.\n\nFigure 3-4 shows the intersection of workflow, toolchain, and arti‐ facts, along with the automated flow from left to right of a common automated build/test/deploy cycle. It highlights the (often automa‐ ted) check and release-to-production steps, as well as the integration of performance considerations and feedback throughout the cycle.\n\nKey Implementation Considerations\n\nOrganizational\n\n— Who does delivery through DevTest versus ProdOps?\n\n— When a delivery goes wrong, who is responsible?\n\nHow are you measuring the quality of a release?\n\n— Stakeholders (business/technical/customer)\n\n— Are all parts considered?\n\n— Visualized and communicated\n\nCultural\n\n— What happens when you have a failed delivery?\n\n— Is rollback tested every time prior to your release?\n\n— What are your performance metrics?\n\n— Ops = Chaos\n\n— Frequency of alarms, acceptable\n\n— Performance Engineering teams > time in production\n\nTechnical\n\n— How are you mitigating quality and performance with fre‐\n\nquency of release?\n\n— Are you using canary deployment approaches to minimize\n\nimpact to production?\n\n— How are your blue/green releases?\n\n— Backup and recovery\n\n— Load-balance and CDN\n\n— End-to-end (client, server, network, and app layer)\n\nProven Practices for Operations\n\n|\n\n79",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "80\n\n|\n\nChapter 3: Proven Practices of Performance Engineering\n\ne l c y c\n\ny o l p e d / t s e t / d l i u b a n i\n\ns t c a f i t r a d n a\n\n,\n\nn i a h c l o o t\n\n,\n\nw o fl k r o w\n\nf o n o i t c e s r e t n I\n\n.\n\n4 - 3 e r u g i F",
      "content_length": 230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "CHAPTER 4 Tying It All Together\n\nAs more businesses experience devastating production incidents, they are recognizing that they need to change, and are working to implement Effective Performance Engineering practices. They’re restructuring their teams and redefining jobs such that some team members are focused on ensuring that the essential computer infra‐ structure and applications deliver good, stable performance at all times. They’re embracing practices in Performance Engineering and treating them as critical, adopting an organizational culture sup‐ porting this transformation, and rewarding individuals for their contributions.\n\nKeep in mind that Performance Engineering doesn’t refer only to a specific job, such as a “performance engineer.” More generally, it refers to the set of skills and practices that are gradually being understood across organizations that focus on achieving higher lev‐ els of performance in technology, in the business, and for end users.\n\nMany naive observers often take the same attitude toward Perfor‐ mance Engineering: it’s simply a matter of making sure the systems run fast. If possible, make them run really fast. When in doubt, just make them run really, really fast. And if that doesn’t work right away, throw money at the problem by buying more hardware to make the systems go really fast.\n\nBut just as there’s more to winning a track meet than being fast, there’s more to building a constellation of quick, efficient web servers and databases than being fast. Just as athletes can’t win without a sophisticated mixture of strategy, form, attitude, tactics,\n\n81",
      "content_length": 1611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "and speed, Performance Engineering requires a good collection of metrics and tools to deliver the desired business results. When they’re combined correctly, the results are systems that satisfy both customers and employees, enabling everyone on the team to win.\n\nMetrics for Success One critical element of integrating a Performance Engineering cul‐ ture within an organization is to determine what performance met‐ rics you need to track and assess whether you can measure them with confidence.\n\nHow often do we hear development and testing organizations and even managers refer to lines of code written, scripts passed and exe‐ cuted, defects discovered, and test use cases as a measure of their commitment to software quality?\n\nAt the end of the day, these measurements are useless when it comes to delivering results that matter to your end users, that keep them coming back for more of your products and services. Think about it. Who cares how many defects you’ve found in pre-production? What does that measure?\n\nWe want to make a fairly bold statement: these old, standalone test metrics don’t matter anymore.\n\nWhen it comes to quality in development, testing, and overall oper‐ ations, these are the questions you should be asking yourself:\n\nHow many stories have we committed to?\n\nHow many of these were delivered with high quality to the end user?\n\nHow much time did it take to deliver from business or customer concept to production?\n\nFinally, ask yourself this: “Are our end users consuming the capabili‐ ties they asked for?”\n\nActivities Versus Results The difference between this sort of focus and a purely technical focus is the difference between activities and results. If our team commits to 80 story points at the beginning of a 4-week sprint but\n\n82\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 1806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "we deliver only 60, then we’re not meeting our commitment to our‐ selves, the business, or the customer. It also means our team is pre‐ venting another team from delivering on their commitments. The release will instead be put on hold and pushed to our next release. Ultimately, the business results are going to be less than what we promised.\n\nOver the last several years, improvements in development and test‐ ing have provided an opportunity for organizations to apply new metrics that can lead to genuine transformation. The most common of these proven concepts is Agile development practices. When executed well, Agile methods can enable a team to quickly deliver high-quality software with a focus on the highest priority for the business and end user. As teams transform, having a few key meas‐ urements and producing results helps the organization evolve in an informed manner, with continuous feedback from the investments they’re making.\n\nWithout these types of metrics, organizations will simply attempt their transformation blindly, with limited capacity to show results, including the business outcomes demanded of today’s technology organizations.\n\nTop Five Software Quality Metrics Here are the top five quality metrics that really matter:\n\nCommitted stories versus delivered results meeting doneness criteria\n\nRemember the last time someone committed to do something for you and either failed to deliver or didn’t meet your stand‐ ards? It caused delays and extra work, along with a lot of frus‐ tration. In software development, stories are the pieces of work that are committed to and, ideally, delivered on time and to a certain spec.\n\nAs you may know, stories represent the simple, high-level descriptions that form a use case, such as a user inserting a credit card into an airline kiosk. Each story needs to be deliv‐ ered at a specific level of quality or “doneness” criteria. As teams continuously plan, elaborate, plan again, commit, and deliver, the ultimate goal should be to deliver these results in alignment with the broader team’s doneness criteria. When that can be measured, the team can showcase its abilities to meet its com‐ mitments on schedule and with the highest standards.\n\nMetrics for Success\n\n|\n\n83",
      "content_length": 2241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Quality across the lifecycle\n\nThe demand for software delivery speed continues to increase along with the demand for reduced costs. But how can you ach‐ ieve these goals when you don’t have the time and resources to manually test every build? When you can’t afford to wait and find those defects in your late-stage releases? The answer is to follow the build lifecycle from story to code on a developer desktop. Next, you should check, build, and unit test. Continue by using automation through the rest of the process, including automated functional, performance, security, and other modes of testing. This enables teams to show the quality of a build throughout the lifecycle with quality metrics and automated pass/fail gates.\n\nGiven the frequency of these builds and automated tests, build- life results can be created and measured in seconds, minutes, and hours. Now, your most frequent tests are fully automated, and you’re only doing manual tests on the highest quality relea‐ ses that make it through the automated lifecycle. This results in automated build-life quality metrics that cover the full lifecycle, enabling your team to deliver with speed and quality, while reducing costs through higher efficiency.\n\nProduction incidents over time and recurrence\n\nJust as it’s important to show the quality of the release over time, it’s also important to minimize production incidents and their recurrence over subsequent releases. Table 4-1 illustrates a tech‐ nique we’ve used to compare team performance over time. Imagine you are working with five teams over three completed releases; this shows how an information radiator can be used with simple and minimal key data to visually represent impor‐ tant results, such as “% Commit Done” and “# Prod Incidents,” delivered across teams.\n\nThe target for this typical (though imaginary) organization is 95% of committed stories delivered and zero production inci‐ dents. Teams that didn’t meet these goals are highlighted in bold red. Often, production incident numbers are found within an incident management process. Defining the root cause and implementing corrective measures enables continuous improve‐ ment and prevents recurrence of the same issue in subsequent releases. With these quality metrics in place, you can learn\n\n84\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 2324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "which teams meet specific goals. Finally, you can look across teams and discover why proven concepts work.\n\nTable 4-1. Using an information radiator to visualize results\n\nTeams\n\nReleases\n\n% Commit Done Alpha # Prod Incidents % Commit Done Beta # Prod Incidents % Commit Done Gamma # Prod Incidents % Commit Done Delta # Prod Incidents % Commit Done # Prod Incidents % Commit Done # Prod Incidents\n\nEpsilon\n\nTotals\n\nTeam Averages 2016-Jan 2016-Feb 2016-Mar 2016-Apr 98% 97% 0 2 95% 94.33% 2 6 100% 100% 0 1 100% 93.33% 0 2 95% 92.33% 0 1 97.6% 95.398% 2 12\n\n96% 1 92% 3 100% 0 100% 0 85% 0 94.6% 4\n\n97% 1 96% 1 100% 1 80% 2 97% 1 94% 6\n\nUser sentiment\n\nGet to know your end users by measuring how they feel when interacting with an application or system. By capturing and dis‐ secting the feedback they provide regarding new or improved capabilities, you can incorporate their needs into an upcoming sprint. At the very least, you can develop a plan to deliver some‐ thing in response to those needs.\n\nOn a larger scale, your analysis and incorporation of user senti‐ ment can expand to a more general market sentiment, which can broaden your impact and market presence. Several compo‐ nents of quality can be covered via this metric, including sim‐ plicity, stability, usability, and brand value.\n\nContinuous improvement\n\nFollowing retrospectives, allow time and effort to implement prioritized, continuous improvement stories. This enables the team to self-organize and be accountable for improving the quality of their process. When you allocate this time and make it visible to all, the team and stakeholders can show their imme‐ diate impact. They can demonstrate how one team, compared to others, has delivered results at increased speed, with higher quality and value to the end user. This allows team leads to ask\n\nMetrics for Success\n\n|\n\n85",
      "content_length": 1848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "and possibly answer these questions: are there certain practices that need to be shared? How do teams perform over time with certain changes injected? The continuous improvement metric can also justify recent or proposed investments in the team.\n\nWhat Really Matters It’s amazing to see how many teams are still working the old- fashioned way. In fact, the empathy and sympathy poured out from others in the field is overwhelming. We hear and share the same sto‐ ries we shared 20+ years ago. For example, have you heard this lately, “I have 3,896 test cases, and I’m 30% complete on test execu‐ tion”? We should all ask, “So, what does that mean for time, quality, and cost, along with on-time delivery to the end user?” It’s genuinely shocking when we hear from a VP about their mobile-testing pro‐ cess, only to learn that the company’s mobile strategy is a “mobile guy” who does manual testing by putting the application on his phone and playing with it—maybe even wrapping it in aluminum foil and walking up and down some hills or taking the elevator to simulate real-world users and weak network conditions.\n\nLet’s start focusing on metrics that really matter. We need results that center on the value and quality we deliver to our end users. In the process, let’s not forget how to deliver. We need teams to con‐ tribute creatively and improve the practices they have, while meas‐ uring quality via metrics they can use to evaluate, modify, and improve processes over time.\n\nWhat happens when we insist on the old style of quality metrics?\n\nWell, for one thing, it helps explain why so many CIOs hold their positions for less than two years or why a third of them lose their jobs after a failed project. We’ve seen this before: a new CIO or senior leader comes in, fires a few mid-level managers, reorganizes a couple of things, and brings in a new partner, and suddenly they’re trying to measure results. Unfortunately, they don’t have the right metrics in place to show how the team is delivering. Command and control fails again. Sadly, this fails the business, shareholders, pas‐ sionate individuals, and ultimately the end user: the customer.\n\nYou do not want to fail your customer.\n\n86\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 2235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Other Performance Engineering Metrics The top five quality metrics are a foundational and important start‐ ing point for Effective Performance Engineering. In addition, there are a variety of other Performance Engineering metrics that come into play:\n\nRelease quality\n\nThroughput\n\nWorkflow and transaction response time\n\nAutomated performance regression success rate\n\nForecasted release confidence and quality level\n\nBreaking point versus current production as a multiplier\n\nDefect density\n\nThis drive to explore new metrics and find better ways of under‐ standing how software is succeeding (and failing) is going to con‐ tinue and grow even more intense. Software engineers understand that it’s not enough to simply focus on the narrow job of going fast. The challenge is capturing just how the software is helping the com‐ pany, its employees, and its customers. If they succeed, then the soft‐ ware is a success.\n\nThere are big differences in the ways companies are approaching the challenge. They’re mixing enterprise, commercial, and open source tools, and using a wide range of metrics to understand their results. We’ve seen key metrics that are accepted by all groups of stakehold‐ ers—metrics that all businesses can start using today. However, there’s nothing like enabling the team to also measure what matters to them, because what matters to your team may matter deeply to your success.\n\nAutomation Automation can mean different things to different people. In this section, we explore why performance testing is not enough, investi‐ gate the four key areas to focus on as a performance engineer, and discuss how to apply these practices in the real world. You will see how automation plays a critically important role in Performance Engineering.\n\nAutomation\n\n|\n\n87",
      "content_length": 1778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Performance Testing Isn’t Enough Software, hardware, and the needs of application users have all changed radically in recent years, so why are the best practices many developers use to ensure software quality seemingly frozen in time? The world has evolved toward Performance Engineering, but too many developers still rely on performance testing alone. This can lead to disaster.\n\nThe initial failures of the Healthcare.gov website revealed how frag‐ ile underlying systems and integrated dependencies can be. Simple performance testing isn’t enough. If you don’t develop and test using a Performance Engineering approach, the results can be both costly and ineffective.\n\nWhat went wrong with Healthcare.gov? A report in Forbes cited these eight reasons for the site’s massive failure:\n\nUnrealistic requirements\n\nTechnical complexity\n\nIntegration responsibility\n\nFragmented authority\n\nLoose metrics\n\nInadequate testing\n\nAggressive schedules\n\nAdministrative blindness\n\nPresident Obama, the CEO in this scenario, received widespread criticism over the troubled launch, which should have been a high point for his presidency. Instead, the site’s poor performance tainted the public’s perception of the program. When you embarrass your boss, you don’t always get a second chance. In the case of Health‐ care.gov, the Obama administration had to bring in new blood.\n\nSo, how do failures like this happen?\n\nWhen developers and testers were working in a mainframe or client-server environment, the traditional performance testing prac‐ tices were good enough. As the evolution of technology accelerated, however, teams have had to work with a mix of on-premises, third- party, and other cloud-based services, and components over which\n\n88\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "they often have little or no control. Meanwhile, users increasingly expect quick access anywhere, anytime, and on any device.\n\nFour Key Areas of Focus Performance Engineering practices help developers and testers solve these problems and mitigate risks by focusing on high performance and delivering valuable capabilities to the business.\n\nThe key is to start by focusing on four key areas:\n\nBuilding in continuous business feedback and improvement. You accomplish this by integrating a continuous feedback and improvement loop into the process right from the beginning.\n\nDeveloping a simple and lightweight process that enables auto‐ mated, built-in performance. In this way, the application, sys‐ tem, and infrastructure are optimized throughout the process.\n\nOptimizing applications for business needs.\n\nFocusing on quality.\n\nApplying the four key areas\n\nYour team can head off unrealistic requirements by asking for and using feedback and improvement recommendations. To avoid technical complexity, your team must share a common goal to quickly define, overcome, and verify that all systems are engineered with resiliency and optimized for business and customer needs. Integration responsibility must be built into all environments, along with end-to-end automated performance validation. This should even include simulations for services and components that are not yet available.\n\nThe issue of fragmented authority won’t come up if you create a col‐ laborative and interactive team, and you can avoid the problem of loose metrics by using metrics that provide stakeholders the infor‐ mation they need to make informed business decisions. Inadequate testing will never be an issue if you build in automated testing, including functional testing for:\n\nPerformance\n\nSecurity\n\nAutomation\n\n|\n\n89",
      "content_length": 1796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Usability\n\nDisaster recovery\n\nCapacity planning\n\nOverly aggressive schedules are unlikely to occur if you provide automated quality results reports that highlight risks and offer opti‐ mization recommendations to support informed decision making. Finally, to prevent administrative blindness, focus on business out‐ comes, communicate with all stakeholders throughout the process, and build in accountability and responsibility for delivery.\n\nIt’s your responsibility to ensure that your organization is moving from antiquated methodologies based on performance testing only to more comprehensive Performance Engineering practices. After all, no one wants to be the next Healthcare.gov.\n\nBig Data for Performance Performance Engineering has long been a practice adopted in the world of high-performance automotive. One of the results we often see in our “Performance Engineering” Google Alert is Lingenfelter Performance Engineering. When you go to the “About us” section of their website, it states:\n\nLingenfelter Performance Engineering was founded over 43 years ago and is a globally recognized brand in the performance engi‐ neering industry. The company offers engine building, engine and chassis tuning components and installation for vehicle owners; component product development; services to manufacturers, after‐ market and original equipment suppliers; prototype and prepara‐ tion of product development vehicles; late product life-cycle performance improvements; durability testing; and show and media event vehicles.\n\nLooking at high-performance automotive organizations like Lingen‐ felter (and many others), it is easy to see a direct correlation between all of the components and engineered elements that make a high- performance automobile and these of our business systems (and between their drivers and our end users). The parallel that we want you to recognize is the now available “Big Data for Performance,” which the high-performance automotive industry has been leverag‐ ing for many years, yet we as Performance Engineers are only start‐ ing to utilize. This big data and the accompanying predictive analytics, both of which leverage the capabilities of Performance\n\n90\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 2230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Engineering, will enable us to best support our businesses and end users through technology.\n\nTo finish out this analogy, do organizations like Lingenfelter only wait until final deployment to see how the automobile they are optimizing will perform? No, they have adopted practices for look‐ ing as a team at each component along the way, making decisions, and optimizing the components based on data to ensure they are high quality.\n\nPerformance as a Team Sport Over the last few years, organizations have started to define and embrace the capabilities of Performance Engineering, recognizing that their systems are growing so complex that it’s not enough to simply tell the computers or the individuals behind them to “run fast.” This capability must be built into the organization’s culture and behavior, and it must include activities for developers, database administrators, designers, and all stakeholders—each coordinating to orchestrate a system that works well, starting early in the lifecycle and building it in throughout. Each of the parts may be good enough on its own, but without the attention of good engineering practices, they won’t work well enough together.\n\nMarket Solutions As you look across the market, you will see there are a number of analysts, partners, and software tool vendors actively marketing their Performance Engineering capabilities.\n\nTo simplify the decision-making and implementation process for you, we’ve provided some Performance Engineering topics with links to key information at http://www.effectiveperformanceengineer ing.com.\n\nIn addition, we’ve included the results of a Performance Engineering survey that gives a lot more detail about what is going on in the market now.\n\nMarket Solutions\n\n|\n\n91",
      "content_length": 1745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Performance Engineering Survey Results Hewlett Packard Enterprise has been working to support Perfor‐ mance Engineering in all organizations. In 2015, it contracted You‐ Gov, an independent research organization, to survey 400 engineers and managers to understand how organizations are using tools and metrics to measure and evolve their Performance Engineering prac‐ tices. The survey was conducted blind so that no one knew that Hewlett Packard Enterprise commissioned it.\n\nThe sample consisted of 50% performance engineers and perfor‐ mance testers, 25% application development managers, and 25% IT operations managers. All came from companies with at least 500 employees in the US. The results reveal a wide range of techniques and broad approaches to Performance Engineering and some of the practices through which organizations are using tools and metrics.\n\nThe survey asked, “When you look to the future of Performance Engineering, what types of tools do you and your stakeholders plan to acquire?” In response, 52% of large companies (those with 10,000+ employees) indicated “more enterprise and proven” tools; 37% of the larger companies said they expected “more open source and home-grown”; and the remaining 11% said they were planning “more hybrid of open source and enterprise.” The responses from companies of different sizes followed a similar pattern, but with a bit more balance (see Figure 4-1).\n\nWhen the results were analyzed based on roles, the majority of respondents planned to acquire “more enterprise and proven” tools, with those identifying as “performance engineer/performance tester” (41%), application development manager (44%), and IT operations manager (51%), as shown in Figure 4-2.\n\nWhen it comes to testing, an increasing number of companies are concentrating on burst testing to push their software closer to the breaking point. They’re spinning up a large number of virtual users and then pointing them at the systems under test in a large burst over a period of time. This simulates heavy traffic generated from sales, promotions, big events, or retail days like Black Friday or Cyber Monday, when a heavy load can wreak havoc on a system (Figure 4-3).\n\n92\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 2231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Market Solutions\n\n|\n\ne z i s n o i t a z i n a g r o y b n o i t i s i u q c a l o o t\n\ne r u t u F\n\n.\n\n1 - 4 e r u g i F\n\n93",
      "content_length": 125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "94\n\n|\n\nChapter 4: Tying It All Together\n\ne l o r b o j\n\ny b n o i t i s i u q c a l o o t\n\ne r u t u F\n\n.\n\n2 - 4 e r u g i F",
      "content_length": 124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Market Solutions\n\n|\n\ne z i s n o i t a z i n a g r o y b g n i t s e t\n\nt s r u B\n\n.\n\n3 - 4 e r u g i F\n\n95",
      "content_length": 107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "One of the most important options among tools like the ones just cited is the ability to deploy an army of machines to poke and prod at an organization’s systems. The cloud is often the best source for these machines, because many modern cloud companies rent virtual machines by the minute. Those working on performance tests can start up a test for a short amount of time and pay only for the minutes they use.\n\nThe value of the cloud is obvious in the answers to the questions about the average size and duration of a load test. Only 3% of respondents reported testing with fewer than 100 simulated users. At least 80% of the respondents used 500 or more users, and 14% wanted to test their software with at least 10,000 users. They feel that this is the only way to be prepared for the number of real users com‐ ing their way when the software is deployed (Figure 4-4).\n\nGrowth in load testing points to the cloud.\n\nThis demand will almost certainly increase. When asked how big they expect their load tests to be in just two years, 27% of respond‐ ents said that they expect they’ll need at least 10,000 simulated users. They mentioned much larger numbers, too; 8% predicted they’ll be running tests with more than 100,000 simulated users, and 2% could foresee tests with 500,000 users or more.\n\nWhile the number of simulated users is growing, duration isn’t long enough to make a dedicated test facility economical. The tests are usually not very long; only 8% reported running tests that routinely lasted more than 24 hours. Most of the survey respondents (54%) said that their tests ran between 4 and 12 hours (Figure 4-5).\n\nThe largest companies are also the ones that are most likely to be using the cloud. Only 9% said that they don’t use the cloud for test‐ ing, typically because their security policies didn’t permit them to expose their data to the cloud (Figure 4-6).\n\n96\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 1923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Market Solutions\n\n|\n\ne z i s n o i t a z i n a g r o y b e z i s\n\nt s e t d a o l\n\nm u m i x a M\n\n.\n\n4 - 4 e r u g i F\n\n97",
      "content_length": 122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "98\n\n|\n\nChapter 4: Tying It All Together\n\ne z i s n o i t a z i n a g r o y b n o i t a r u d m u m i x a M\n\n.\n\n5 - 4 e r u g i F",
      "content_length": 128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Market Solutions\n\n|\n\ne t i s n o i t a z i n a g r o y b s r e d i v o r p e c i v r e s d u o l c\n\nf o e s U\n\n.\n\n6 - 4 e r u g i F\n\n99",
      "content_length": 135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "How to Choose a Solution Now comes the time for you to start defining what a solution looks like for you. As you begin, we suggest you take a three-step approach: define your goals and objectives, define a timeline, and identify partners. In the following sections, we go through this pro‐ cess in a bit more detail, so you can get started on your path to choosing your Performance Engineering solution.\n\nDefine your goals and objectives\n\nTransforming is a complex exercise and one that should have some thought behind it. When thinking about goals and objectives, begin with five key aspects of your teams and organization:\n\nCulture\n\nTechnology\n\nSpeed\n\nQuality\n\nCost\n\nEach of these considerations factors into the overall goals and objec‐ tives for Effective Performance Engineering, and decisions must be made now.\n\nIt is a journey and will take some time, and the path will not always be straight; however, getting started in a focused area with some support, adopting a few key practices, and sharing the results is the right approach.\n\nCelebrate the success, examine the results, and then continue along the journey, never losing sight of the end user. With this collabora‐ tion and continued guidance and direction, you’ll attain success and make forward progress, as you transform into an Effective Perfor‐ mance Engineering organization.\n\nDefine your timeline\n\nTimelines are relative. By contrast, value to your end users and stakeholders is more objective and within your control, so you should focus on defining what is important there before setting your timelines.\n\n100\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 1618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "From a purely leadership and budget/time perspective, it is impor‐ tant to define a timeline with clear goals and objectives within the given budgetary cycle. Doing so enables you to share and communi‐ cate results delivered in the prior period, activities being performed in the current period with their forecasted results, and commit‐ ments for the future period with their forecasted results.\n\nA timeline should visually represent key milestones along with incremental measures indicating what should be achieved within these milestones. It should also map each task or activity to the value it will deliver to the end user and business.\n\nFigure 4-7 illustrates what this could look like at a high level for you and your organization on your journey to Effective Performance Engineering.\n\nIdentify your partners\n\nPartners and thought leaders are often a great resource to provide additional insight, experience, and practical advice from the market, in order to get you aligned with current trends and able to accelerate as desired.\n\nNext we take a deeper look at some thought leaders, consulting part‐ ners, and analyst partners, with specific details and links to existing capabilities and assets, so you can quickly get a better and broader idea of some of the Effective Performance Engineering resources available to you.\n\nTop thought leaders of today Microsoft, Google, IBM, Apple, and Hewlett Packard Enterprise comprise the set of top five thought leaders and influencers around Performance Engineering and testing today.\n\nThis set of five is consistent by audience (performance engineers/ testers, application development managers, and IT operations man‐ agers), as well as by organization size (Figure 4-8).\n\nMarket Solutions\n\n|\n\n101",
      "content_length": 1746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "102\n\n|\n\nChapter 4: Tying It All Together\n\ng n i r e e n i g n E e c n a m r o f r e P\n\nf o n o i t u l o v E\n\n.\n\n7 - 4 e r u g i F",
      "content_length": 130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Figure 4-8. Top thought leaders\n\nPreferred partners for Performance Engineering Accenture, Infosys, Deloitte, HCL, and Tech Mahindra are the top five service providers most often chosen as Performance Engineer‐ ing partners by the organizations surveyed.\n\nNote that “None of the Above” only represented 7% of all other responses (Figure 4-9).\n\nFigure 4-9. Preferred partners\n\nConclusion Performance Engineering practices define a culture that enables teams to deliver fast, efficient, and responsive systems architected for large-scale populations of customers, employees, regulators, managers, and more. The careful application of these principles makes it possible for corporations to please customers, support employees, and boost revenues, all at the same time.\n\nConclusion\n\n|\n\n103",
      "content_length": 785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "There is more to Performance Engineering than just testing. Done right, Performance Engineering means understanding how all the parts of the system fit together, and building in performance from the first design.\n\nMaking the journey from performance testing to Performance Engi‐ neering isn’t easy. But the proven practices established over years of observation can help you on your way.\n\nThe Path to Performance Engineering One of the first tasks that budding programmers are given is to write a program that produces the text “Hello world.”\n\nNext you start to play with the program and try to do more, to see how quickly it delivers data or answers queries, and try to optimize for the highest performance with the least amount of code. The requests come in, the responses go out, and you see results on a screen. Take this and add a long-time run script for performance testing, a script you run every time you push out your latest release. It’s pretty easy when you’re the author and the user.\n\nPerformance Engineering, though, is a broad set of processes, and it’s also a culture. Performance Engineering is an art based on years of observation that have led to proven practices.\n\nBut moving from performance testing to Performance Engineering isn’t an easy process. The team must be ready to move from simply running a checkbox performance test script and focusing on parts to studying the way that all parts of the system work together. These pieces encompass hardware, software, configuration, performance, security, usability, business value, and the customer. The process is about collaborating and iterating on the highest-value items, and delivering them quickly, at high quality, so you can exceed the expectations of your end user.\n\nHere’s a roadmap for making the trip from performance testing to Performance Engineering. Essentially, these are the steps to become a hero and change agent—and how you can enable your organiza‐ tion to deliver with proven Performance Engineering practices and the accompanying culture.\n\n104\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 2075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Define a culture\n\nThe success of a team depends heavily on the way leaders are nur‐ turing the professional environment and enabling individuals to col‐ laborate. Building this type of environment will inspire the formation of cross-functional teams and logical thinking.\n\nBuild a team\n\nA Performance Engineering team means that technology, business, and user representatives work together. They focus on the perfor‐ mance nature of everything they’re working on and figure out together how they can build in these capabilities. They need to know what specific area to focus on first, as well as how to measure along the way. They need to agree on the desired outcome. They must constantly remind themselves that the end goal of adopting Perfor‐ mance Engineering is to benefit the organization and end user.\n\nChoose metrics\n\nWe often encourage teams to start with a manual metrics process, perhaps a whiteboard (we know, not really high tech for a technolo‐ gist) and a few key metrics, then measure them over time and see why they matter (or don’t). You’ll quickly get a core set of metrics that matter for you and your organization, which have grown out of your cross-functional teams. Your people have the passion and understanding behind these, so trust them. They offer a good way to judge results against the desired outcome.\n\nOnce you have figured out enough of this manually, and individuals are starting to adopt and believe in them, take a look at your existing technology capabilities and see how you can get to automated reporting of these results fairly simply. These metrics will be key to your way of measuring what you do and the results you’re able to deliver. Make sure you have a solid baseline, and take regular measurements.\n\nAdd technology\n\nPerformance Engineering requires a new way of thinking, related to your existing software and infrastructure, including the existing tools and capabilities. This is how you shape and form quick, auto‐ mated results.\n\nConclusion\n\n|\n\n105",
      "content_length": 1999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Define what your scope of effort is going to be and quickly learn what technology capabilities you already have available to you and your team. This will be an interesting experience, because you’ll learn about the capabilities that other siloed teams have available to them. Now, with a shared vision of how you want to deliver Per‐ formance Engineering throughout the organization, you can lever‐ age the technology to launch a single approach that aggregates these capabilities.\n\nPerhaps there are a few technology areas you want to start thinking about from the lifecycle virtualization space, such as user virtualiza‐ tion, service virtualization, network virtualization, and data virtuali‐ zation. These are the core capabilities that will enable your team to accelerate the transformation to Performance Engineering.\n\nBuild in telemetry\n\nNow that you’ve started with culture, team, and technology, it’s time to start integrating the telemetry and its data.\n\nFor example, how are you capturing the APM (application perfor‐ mance monitoring) data from production, and how about pre- production? Can you begin to examine these results and understand more about the behavior patterns of your users, systems, and trans‐ actions? From a cross-functional perspective, this will also pique the interest of the IT operations manager; so you’ll continue to broaden your network, and you’ll enable them to reduce the number of pro‐ duction incidents. This is just one example.\n\nThink about other quick wins or simple integrations for your exist‐ ing technology that will enable you to build more bridges. Correlate these types of results across your team so you can promote the cul‐ ture and desired outcomes of Performance Engineering by building in telemetry.\n\nLook for indirect metrics\n\nThere are hundreds of metrics available that you can use to estimate the success of a new capability or feature being released. As systems take on more roles inside a company, metrics that track perfor‐ mance become more readily available, and these enable you to begin partnering with your business peers to find out what metrics they watch and how they get these results.\n\n106\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 2201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Start looking at and asking about indirect metrics within the busi‐ ness that would show results related to revenue, customers (attrac‐ tion and retention), competitive advantage, and brand value. These are important to measure as you make the transition to Performance Engineering.\n\nFocus on stakeholders\n\nGet to know your stakeholders. Who on your team has the most interest in delivering the highest-value items to the end user most quickly and with great results? Find these people and get to know them well. Remember, you’re looking for your executive-level spon‐ sors and peer champions, so you can transform the practices and culture of an organization to become a Performance Engineering delivery machine.\n\nStart gathering information and sharing initial prototypes for the type of results, reports, and dashboards you want to show to your stakeholders on a regular basis. Typically, this would be a monthly show-and-tell exercise; however, as it matures it may become a set of automated results delivered with every build, consistently available if stakeholders want to review it. Also, you should consider regular, quarterly presentations to the executive board in which you share last quarter’s results, talk about the current quarter, and seek fund‐ ing for the next one.\n\nStay focused. Remember your objective. Find your champions. Deliver results.\n\nCreate stable environments\n\nOne of the earliest challenges will involve enabling teams with the capabilities they require. Some of this will come as you build these teams and the cross-functional tools, capabilities, and associated skills come together. But in the beginning, having a “like produc‐ tion” environment for Performance Engineering is key.\n\nBy leveraging the aforementioned lifecycle virtualization—including user virtualization, service virtualization, network virtualization, and data virtualization—you can quickly re-create production envi‐ ronments at a significant fraction of the cost, and you can duplicate them as many times as required. There are several other stable envi‐ ronment proven practices that have emerged along the way, which you can also learn and share through others.\n\nConclusion\n\n|\n\n107",
      "content_length": 2188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Celebrate wins\n\nRemember the old forming, storming, norming, and performing program developed by Bruce Tuckman? He believed these were the four phases necessary to building teams. If you’re a leader or a team member, you’ll see this in action.\n\nIt’s important to remember why you’re doing this, and know it’s all part of the transformation. Stay focused on the business and end- user objectives, so you can measure your progress and keep your eye on the prize.\n\nJust imagine what it will be like once you have delivered these capa‐ bilities to your end user. Conduct proper retrospectives, track your progress with your metrics, and celebrate the wins!\n\nAdd gamification\n\nAs you mature the capabilities just listed, think about how you can add gamification into the results. In other words, how do you make the results you’re delivering fun and visual, and how do you make a positive impact on your end users and the organization in the process?\n\nRajat Paharia created the gamification industry in 2007. In his book Loyalty 3.0 (McGraw-Hill) Rajat explains, “how to revolutionize customer and employee engagement with Big Data and gamifica‐ tion” and defines these “10 key mechanics of gamification”:\n\n1. Fast feedback\n\n2. Transparency\n\n3. Goals\n\n4. Badges\n\n5. Leveling up\n\n6. Onboarding\n\n7. Competition\n\n8. Collaboration\n\n9. Community\n\n10. Points\n\n108\n\n|\n\nChapter 4: Tying It All Together",
      "content_length": 1389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Of course, you also want to ensure that you highlight the opportuni‐ ties for improvement and show the wins and losses. You can also gamify Performance Engineering itself at a team level to encourage a little healthy competition within your group, and well beyond, then broadly share the results. This also enables you to leverage these results as information radiators for all stakeholders, showing how teams, systems, and applications are performing against defined baselines and goals.\n\nStart small\n\nWhen you first begin to incorporate Performance Engineering, you may be tackling a long-neglected maintenance list, or a new, up- and-coming hot project. Either can benefit from the focus of a Per‐ formance Engineering culture. Don’t try to take on too much at first.\n\nAs you begin to elaborate on your requirements, stories, and fea‐ tures, it’s important to remember that your whole team is working to define the what, why, and how of each item. As you continue down the Performance Engineering path, you will learn from each other’s domain expertise,” keeping in mind these learnings and results are from small experiments to show quick incremental value.\n\nStart early\n\nPerformance Engineering works best when the team starts thinking about it from the beginning. The earlier the team begins addressing performance in the product lifecycle, the likelier it is that the final system will run quickly, smoothly, and efficiently. But if it can’t be done from the very beginning, it’s still possible to add the process to the redesign and reengineering work done to develop the next itera‐ tion or generation of a product.\n\nConclusion\n\n|\n\n109",
      "content_length": 1644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "About the Authors\n\nTodd DeCapua is the Chief Technology Evangelist with Hewlett Packard Enterprise and cofounder of TechBeacon.com thought leadership site for IT Heros.\n\nDeCapua is a seasoned software professional with 20+ years of expe‐ rience in IT applications development, IT operations, technology integrations, channels operations, and business development in sev‐ eral domains, including Mobile, Agile, Cloud, and Performance.\n\nOver the years Todd has transformed three organizations to Agile/ DevOps, consulted with 100+ organizations worldwide, and amassed a variety of perspectives and practical experiences. He has earned an MBA in Finance and a BS; has been recognized with sev‐ eral industry certifications and awards; and is an industry-renowned leader, speaker, and author.\n\nShane Evans is an experienced IT Manager with over 12 years in the industry. His primary focus has been Performance Engineering and Performance Management, and he spent 7 years managing these for a major financial institution in Canada before joining Hewlett- Packard in 2009 as a Presales Solution Architect. After three years in the field helping ensure the success of customers across the country, he is now part of the Product Management team. Shane is an active member of the Performance Engineering community, and regularly contributes to the discussions on the HP Forums as well as Google Groups, Yahoo!, and LinkedIn.\n\nAcknowledgments\n\nWe recognize Performance Engineering as both an art and a science. Thank you to those with whom we have been able to practice our art, and to those who continue to define the science with us.\n\nThis book is dedicated to our families, friends, and colleagues.",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Page Number\n\nComment/Key Learning/Action\n\nFor more information, join us online:\n\nhttp://www.EffectivePerformanceEngineering.com • @EffPerfEng on Twitter\n\nNotes\n\n111",
      "content_length": 164,
      "extraction_method": "Unstructured"
    }
  ]
}