{
  "metadata": {
    "title": "Real World Natural Language Processing",
    "author": "Masato Hagiwara",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 337,
    "conversion_date": "2025-12-25T18:17:06.513381",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Real World Natural Language Processing.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nMasato Hagiwara\n\n\n302\nThe development cycle of NLP applications\nR\nE\nS\nE\nA\nR\nC\nH\nO\nP\nE\nR\nA\nT\nI\nO\nN\nS\nD\nE\nV\nE\nL\nO\nP\nM\nE\nN\nT\nImplementation\nDeploying\nNLP system \ndevelopment\nprocess\nTraining\nMonitoring\nData\ncollection\nAnalysis &\nexperimenting\n\n\nReal-World Natural\nLanguage Processing\nMASATO HAGIWARA\nM A N N I N G\nSHELTER ISLAND\n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2021 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nThe author and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The author and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \nof the information herein.\nManning Publications Co.\nDevelopment editor: Karen Miller\n20 Baldwin Road\nTechnical development editor: Mike Shepard\nPO Box 761\nReview editor: Adriana Sabo\nShelter Island, NY 11964\nProduction editor: Deirdre S. Hiam\nCopy editor: Pamela Hunt\nProofreader: Keri Hales\nTechnical proofreader: Mayur Patil\nTypesetter and cover designer: Marija Tudor\nISBN 9781617296420\nPrinted in the United States of America\n\n\nTo Daphne, Laurel, and Lynn\n\n\niv\n \n\n\nv\ncontents\npreface\nxi\nacknowledgments\nxiii\nabout this book\nxiv\nabout the author\nxvii\nabout the cover illustration\nxviii\nPART 1\nBASICS ............................................................ 1\n1 Introduction to natural language processing\n3\n1.1\nWhat is natural language processing (NLP)?\n4\nWhat is NLP?\n4\n■What is not NLP?\n6\n■AI, ML, DL, and \nNLP\n8\n■Why NLP?\n10\n1.2\nHow NLP is used\n12\nNLP applications\n13\n■NLP tasks\n15\n1.3\nBuilding NLP applications\n21\nDevelopment of NLP applications\n21\n■Structure of NLP \napplications\n24\n2 Your first NLP application\n26\n2.1\nIntroducing sentiment analysis\n27\n2.2\nWorking with NLP datasets\n28\nWhat is a dataset?\n28\n■Stanford Sentiment Treebank\n29\nTrain, validation, and test sets\n30\n■Loading SST datasets using \nAllenNLP\n33\n\n\nCONTENTS\nvi\n2.3\nUsing word embeddings\n34\nWhat are word embeddings?\n34\n■Using word embeddings \nfor sentiment analysis\n36\n2.4\nNeural networks\n37\nWhat are neural networks?\n37\n■Recurrent neural networks \n(RNNs) and linear layers\n38\n■Architecture for sentiment \nanalysis\n39\n2.5\nLoss functions and optimization\n41\n2.6\nTraining your own classifier\n43\nBatching\n43\n■Putting everything together\n44\n2.7\nEvaluating your classifier\n45\n2.8\nDeploying your application\n46\nMaking predictions\n46\n■Serving predictions\n46\n3 Word and document embeddings\n49\n3.1\nIntroducing embeddings\n50\nWhat are embeddings?\n50\n■Why are embeddings \nimportant?\n50\n3.2\nBuilding blocks of language: Characters, words, \nand phrases\n52\nCharacters\n52\n■Words, tokens, morphemes, and phrases\n53\nN-grams\n53\n3.3\nTokenization, stemming, and lemmatization\n54\nTokenization\n54\n■Stemming\n55\n■Lemmatization\n56\n3.4\nSkip-gram and continuous bag of words (CBOW)\n57\nWhere word embeddings come from\n57\n■Using word \nassociations\n58\n■Linear layers\n59\n■Softmax\n61\nImplementing Skip-gram on AllenNLP\n62\n■Continuous \nbag of words (CBOW) model\n67\n3.5\nGloVe\n68\nHow GloVe learns word embeddings\n68\n■Using pretrained \nGloVe vectors\n69\n3.6\nfastText\n72\nMaking use of subword information\n72\n■Using the \nfastText toolkit\n73\n3.7\nDocument-level embeddings\n74\n3.8\nVisualizing embeddings\n76\n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-17)",
      "start_page": 9,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "CONTENTS\nvii\n4 Sentence classification\n80\n4.1\nRecurrent neural networks (RNNs)\n81\nHandling variable-length input\n81\n■RNN abstraction\n82\nSimple RNNs and nonlinearity\n84\n4.2\nLong short-term memory units (LSTMs) and gated \nrecurrent units (GRUs)\n88\nVanishing gradients problem\n88\n■Long short-term memory \n(LSTM)\n90\n■Gated recurrent units (GRUs)\n92\n4.3\nAccuracy, precision, recall, and F-measure\n93\nAccuracy\n93\n■Precision and recall\n94\n■F-measure\n96\n4.4\nBuilding AllenNLP training pipelines\n96\nInstances and fields\n97\n■Vocabulary and token \nindexers\n98\n■Token embedders and RNNs\n99\nBuilding your own model\n100\n■Putting it all \ntogether\n101\n4.5\nConfiguring AllenNLP training pipelines\n102\n4.6\nCase study: Language detection\n105\nUsing characters as input\n106\n■Creating a dataset \nreader\n106\n■Building the training pipeline\n108\nRunning the detector on unseen instances\n110\n5 Sequential labeling and language modeling\n112\n5.1\nIntroducing sequential labeling\n113\nWhat is sequential labeling?\n113\n■Using RNNs \nto encode sequences\n115\n■Implementing a Seq2Seq \nencoder in AllenNLP\n117\n5.2\nBuilding a part-of-speech tagger\n118\nReading a dataset\n118\n■Defining the model and the \nloss\n119\n■Building the training pipeline\n121\n5.3\nMultilayer and bidirectional RNNs\n122\nMultilayer RNNs\n122\n■Bidirectional RNNs\n124\n5.4\nNamed entity recognition\n126\nWhat is named entity recognition?\n127\n■Tagging \nspans\n128\n■Implementing a named entity \nrecognizer\n128\n5.5\nModeling a language\n130\nWhat is a language model?\n130\n■Why are language models \nuseful?\n131\n■Training an RNN language model\n132\n\n\nCONTENTS\nviii\n5.6\nText generation using RNNs\n133\nFeeding characters to an RNN\n134\n■Evaluating text using \na language model\n134\n■Generating text using a language \nmodel\n136\nPART 2\nADVANCED MODELS ..................................... 139\n6 Sequence-to-sequence models\n141\n6.1\nIntroducing sequence-to-sequence models\n142\n6.2\nMachine translation 101\n144\n6.3\nBuilding your first translator\n147\nPreparing the datasets\n148\n■Training the model\n150\nRunning the translator\n153\n6.4\nHow Seq2Seq models work\n154\nEncoder\n154\n■Decoder\n156\n■Greedy decoding\n158\nBeam search decoding\n161\n6.5\nEvaluating translation systems\n163\nHuman evaluation\n163\n■Automatic evaluation\n163\n6.6\nCase study: Building a chatbot\n165\nIntroducing dialogue systems\n165\n■Preparing a dataset\n166\nTraining and running a chatbot\n167\n■Next steps\n169\n7 Convolutional neural networks\n171\n7.1\nIntroducing convolutional neural networks (CNNs)\n172\nRNNs and their shortcomings\n172\n■Pattern matching \nfor sentence classification\n173\n■Convolutional neural \nnetworks (CNNs)\n174\n7.2\nConvolutional layers\n174\nPattern matching using filters\n175\n■Rectified linear unit \n(ReLU)\n176\n■Combining scores\n178\n7.3\nPooling layers\n179\n7.4\nCase study: Text classification\n180\nReview: Text classification\n180\n■Using CnnEncoder\n181\nTraining and running the classifier\n182\n8 Attention and Transformer\n184\n8.1\nWhat is attention?\n185\nLimitation of vanilla Seq2Seq models\n185\n■Attention \nmechanism\n186\n\n\nCONTENTS\nix\n8.2\nSequence-to-sequence with attention\n187\nEncoder-decoder attention\n188\n■Building a Seq2Seq \nmachine translation with attention\n189\n8.3\nTransformer and self-attention\n192\nSelf-attention\n192\n■Transformer\n195\n■Experiments\n197\n8.4\nTransformer-based language models\n200\nTransformer as a language model\n200\n■Transformer-XL\n203\nGPT-2\n205\n■XLM\n207\n8.5\nCase study: Spell-checker\n208\nSpell correction as machine translation\n208\n■Training a spell-\nchecker\n210\n■Improving a spell-checker\n213\n9 Transfer learning with pretrained language models\n218\n9.1\nTransfer learning\n219\nTraditional machine learning\n219\n■Word embeddings\n220\nWhat is transfer learning?\n220\n9.2\nBERT\n222\nLimitations of word embeddings\n222\n■Self-supervised learning\n224\nPretraining BERT\n225\n■Adapting BERT\n226\n9.3\nCase study 1: Sentiment analysis with BERT\n229\nTokenizing input\n230\n■Building the model\n232\nTraining the model\n233\n9.4\nOther pretrained language models\n236\nELMo\n236\n■XLNet\n237\n■RoBERTa\n239\nDistilBERT\n240\n■ALBERT\n241\n9.5\nCase study 2: Natural language inference with BERT\n243\nWhat is natural language inference?\n243\n■Using BERT \nfor sentence-pair classification\n244\n■Using Transformers \nwith AllenNLP\n246\nPART 3\nPUTTING INTO PRODUCTION ....................... 253\n10 Best practices in developing NLP applications\n255\n10.1\nBatching instances\n256\nPadding\n256\n■Sorting\n257\n■Masking\n259\n10.2\nTokenization for neural models\n261\nUnknown words\n261\n■Character models\n262\n■Subword \nmodels\n263\n\n\nCONTENTS\nx\n10.3\nAvoiding overfitting\n265\nRegularization\n265\n■Early stopping\n268\n■Cross-\nvalidation\n269\n10.4\nDealing with imbalanced datasets\n270\nUsing appropriate evaluation metrics\n270\n■Upsampling and \ndownsampling\n271\n■Weighting losses\n272\n10.5\nHyperparameter tuning\n273\nExamples of hyperparameters\n274\n■Grid search vs. random \nsearch\n275\n■Hyperparameter tuning with Optuna\n276\n11 Deploying and serving NLP applications\n280\n11.1\nArchitecting your NLP application\n281\nBefore machine learning\n282\n■Choosing the right \narchitecture\n282\n■Project structure\n283\n■Version \ncontrol\n285\n11.2\nDeploying your NLP model\n286\nTesting\n286\n■Train-serve skew\n288\n■Monitoring\n289\nUsing GPUs\n289\n11.3\nCase study: Serving and deploying NLP applications\n292\nServing models with TorchServe\n292\n■Deploying models with \nSageMaker\n296\n11.4\nInterpreting and visualizing model predictions\n298\n11.5\nWhere to go from here\n302\nindex\n305\n\n\nxi\npreface\nHaving worked at the intersection of machine learning (ML), natural language pro-\ncessing (NLP), and education for the last two decades, I have always been passionate\nabout education and helping people learn new technologies. That’s why I didn’t think\ntwice when I heard about the opportunity of publishing a book on NLP.\n The field of artificial intelligence (AI) went through a lot of changes over the past\nseveral years, including the explosive popularization of neural network–based meth-\nods and the advent of large, pretrained language models. This change made advanced\nlanguage technologies possible, many of which you interact with daily—voice-based\nvirtual assistants, speech recognition, and machine translation, to name a few. How-\never, the “technology stack” of NLP, characterized by the use of pretrained models\nand transfer learning, has finally stabilized in the last few years and is expected to\nremain so, at least for the next couple of years. This is why I think now is a good time\nto start learning about NLP.\n Developing a book on AI is never easy. It feels like you are chasing a moving target\nthat doesn’t slow down and wait for you. When I started writing this book, the Trans-\nformer had just been published, and BERT did not yet exist. Over the course of writ-\ning, AllenNLP, the main NLP framework we use in this book, went through two major\nupdates. Few people were using Hugging Face Transformer, a widely popular deep\nNLP library currently used by many practitioners all over the world. Within two years,\nthe landscape of the NLP field changed completely, due to the advent of the Trans-\nformer and pretrained language models such as BERT. The good news is that the\nbasics of modern machine learning, including word and sentence embeddings, RNNs,\nand CNNs, have not become obsolete and remain important. This book intends to cap-\nture this “core” of ideas and concepts that help you build real-world NLP applications. \n\n\nPREFACE\nxii\n Many great books about ML and deep learning in general are on the market, but\nsome of them focus heavily on math and theories. There’s a gap between what’s\ntaught in books and what the industry needs. I hope this book will serve to bridge\nthis gap.\n \n \n\n\nxiii\nacknowledgments\nThis book would not be possible without the help of many people. I must start by thank-\ning Karen Miller, the development editor at Manning Publications. Thank you for your\nsupport and patience during the development of this book. I’m also grateful for the rest\nof the Manning team: technical development editor Mike Shepard, review editor Adri-\nana Sabo, production editor Deirdre Hiam, copy editor Pamela Hunt, proofreader Keri\nHales, and technical proofreader Mayur Patil. Denny (http://www.designsonline.id/)\nalso created some of the high-quality illustrations you see in this book.\n I’d also like to thank the reviewers who gave valuable feedback after reading the\nmanuscript of this book: Al Krinker, Alain Lompo, Anutosh Ghosh, Brian S. Cole, Cass\nPetrus, Charles Soetan, Dan Sheikh, Emmanuel Medina Lopez, Frédéric Flayol,\nGeorge L. Gaines, James Black, Justin Coulston, Lin Chen, Linda Ristevski, Luis\nMoux, Marc-Anthony Taylor, Mike Rosencrantz, Nikos Kanakaris, Ninoslav Čerkez,\nRichard Vaughan, Robert Diana, Roger Meli, Salvatore Campagna, Shanker Janakira-\nman, Stuart Perks, Taylor Delehanty, and Tom Heiman.\n I’d like to acknowledge the AllenNLP team at the Allen Institute for Artificial Intel-\nligence. I’ve had great discussions with the team, namely, Matt Gardner, Mark Neu-\nmann, and Michael Schmitz. I always look up to their great work that makes deep NLP\ntechnologies easy and accessible to the world.\n Last but not least, I’d like to thank my awesome wife, Lynn. She not only helped\nme choose the right cover image for this book but has also been understanding and\nsupportive of my work throughout the development of this book. \n\n\nxiv\nabout this book\nReal-World Natural Language Processing is not a typical NLP textbook. We focus on build-\ning real-world NLP applications. Real-world’s meaning here is twofold: first, we pay\nattention to what it takes to build real-world NLP applications. As a reader, you will\nlearn not just how to train NLP models but also how to design, develop, deploy, and\nmonitor them. Along the way, you will also learn the basic building blocks of modern\nNLP models, as well as recent developments in the NLP field that are useful for build-\ning NLP applications. Second, unlike most introductory books, we take a top-down\napproach to teaching. Instead of a bottom-up approach, spending page after page\nshowing neural network theories and mathematical formulae, we focus on quickly\nbuilding NLP applications that “just work.” We then dive deeper into individual con-\ncepts and models that make up NLP applications. You’ll also learn how to build\nend-to-end custom NLP applications tailored to your needs using these basic building\nblocks.\nWho should read this book\nThis book is written mainly for software engineers and programmers who are looking\nto learn the basics of NLP and how to build NLP applications. We assume that you, the\nreader, have basic programming and software engineering skills in Python. This book\nalso comes in handy if you are already working on machine learning but would like to\nmove into the NLP field. Either way, you don’t need any prior knowledge of ML or\nNLP. You don’t need any math knowledge to read this book, although basic under-\nstanding of linear algebra might be helpful. There is not a single mathematical for-\nmula in this book.\n\n\nABOUT THIS BOOK\nxv\nHow this book is organized: A roadmap\nThis book consists of three parts that span a total of 11 chapters. Part 1 covers the\nbasics of NLP, where we learn how to quickly build an NLP application with AllenNLP\nfor basic tasks such as sentiment analysis and sequence labeling.\nChapter 1 begins by introducing the “what” and “why” of NLP—what is NLP,\nwhat is not NLP, how NLP technologies are used, and how NLP is related to other\nfields of AI.\nChapter 2 demonstrates how to build your very first NLP application, a sentiment\nanalyzer, and introduces the basics of modern NLP models—word embeddings\nand recurrent neural networks (RNNs)—along the way.\nChapter 3 introduces two important building blocks of NLP applications, word\nand sentence embeddings, and demonstrates how to use and train them.\nChapter 4 discusses one of the simplest but most important NLP tasks, sentence\nclassification, and how to use RNNs for this task.\nChapter 5 covers sequence labeling tasks such as part-of-speech tagging and\nnamed entity extraction. It also touches upon a related technique, language\nmodeling.\nPart 2 covers advanced NLP topics including sequence-to-sequence models, the Trans-\nformer, and how to leverage transfer learning and pretrained language models to\nbuild powerful NLP applications.\nChapter 6 introduces sequence-to-sequence models, which transform one\nsequence into another. We build a simple machine translation system and a\nchatbot within an hour.\nChapter 7 discusses another type of popular neural network architecture, con-\nvolutional neural networks (CNNs).\nChapter 8 provides a deep dive into the Transformer, one of the most import-\nant NLP models today. We’ll demonstrate how to build an improved machine\ntranslation system and a spell-checker using the Transformer.\nChapter 9 builds upon the previous chapter and discusses transfer learning, a pop-\nular technique in modern NLP, with pretrained language models such as BERT.\nPart 3 covers topics that become relevant when you develop NLP applications that are\nrobust to real-world data, and deploy and serve them.\nChapter 10 details best practices when developing NLP applications, including\nbatching and padding, regularization, and hyperparameter optimization.\nChapter 11 concludes the book by covering how to deploy and serve NLP mod-\nels. It also covers how to explain and interpret ML models.\nAbout the code \nThis book contains many examples of source code both in numbered listings and in\nline with normal text. In both cases, source code is formatted in a fixed-width font\n",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 18-25)",
      "start_page": 18,
      "end_page": 25,
      "detection_method": "topic_boundary",
      "content": "ABOUT THIS BOOK\nxvi\nlike this to separate it from ordinary text. Sometimes code is also in bold to high-\nlight code that has changed from previous steps in the chapter, such as when a new\nfeature adds to an existing line of code.\n In many cases, the original source code has been reformatted; we’ve added line\nbreaks and reworked indentation to accommodate the available page space in the\nbook. In rare cases, even this was not enough, and listings include line-continuation\nmarkers (➥). Additionally, comments in the source code have often been removed\nfrom the listings when the code is described in the text. Code annotations accompany\nmany of the listings, highlighting important concepts.\n The code for the examples in this book is available for download from the Manning\nwebsite at https://www.manning.com/books/real-world-natural-language-processing\nand from GitHub at https://github.com/mhagiwara/realworldnlp.\n Most of the code can also be run on Google Colab, which is a free web-based plat-\nform where you can run your machine learning code on hardware accelerators,\nincluding GPUs. \nliveBook discussion forum\nPurchase of Real-World Natural Language Processing includes free access to a private web\nforum run by Manning Publications where you can make comments about the book,\nask technical questions, and receive help from the author and from other users.\nTo access the forum, go to https://livebook.manning.com/book/real-world-natural\n-language-processing/discussion. You can also learn more about Manning’s forums\nand the rules of conduct at https://livebook.manning.com/#!/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking the author some challenging questions lest his interest stray! The\nforum and the archives of previous discussions will be accessible from the publisher’s\nwebsite as long as the book is in print.\nOther online resources\nThe two NLP frameworks we use heavily in this book, AllenNLP and Hugging Face\nTransformers, both have great online courses (https://guide.allennlp.org/ and\nhttps://huggingface.co/course) where you can learn the basics of NLP and how to\nuse the libraries to solve a variety of NLP tasks.\n\n\nxvii\nabout the author\nMASATO HAGIWARA received a PhD in computer science from\nNagoya University in 2009, focusing on natural language processing\nand machine learning. He has interned at Google and Microsoft\nResearch and worked at Baidu, Rakuten Institute of Technology,\nand Duolingo, as an engineer and a researcher. He now runs his\nown research and consultancy company, Octanove Labs, focusing\non educational applications of NLP.\n\n\nxviii\nabout the cover illustration\nThe figure on the cover of Real-World Natural Language Processing is captioned “Bulgare,”\nor a man from Bulgaria. The illustration is taken from a collection of dress costumes\nfrom various countries by Jacques Grasset de Saint-Sauveur (1757–1810), titled Costumes\nde Différents Pays, published in France in 1797. Each illustration is finely drawn and col-\nored by hand. The rich variety of Grasset de Saint-Sauveur’s collection reminds us viv-\nidly of how culturally apart the world’s towns and regions were just 200 years ago.\nIsolated from each other, people spoke different dialects and languages. In the streets\nor in the countryside, it was easy to identify where they lived and what their trade or sta-\ntion in life was just by their dress. \n The way we dress has changed since then and the diversity by region, so rich at the\ntime, has faded away. It is now hard to tell apart the inhabitants of different conti-\nnents, let alone different towns, regions, or countries. Perhaps we have traded cultural\ndiversity for a more varied personal life—certainly for a more varied and fast-paced\ntechnological life. \n At a time when it is hard to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nGrasset de Saint-Sauveur’s pictures.\n\n\nPart 1\nBasics\nWelcome to the beautiful and exciting world of natural language process-\ning (NLP)! NLP is a subfield of artificial intelligence (AI) that concerns compu-\ntational approaches to processing, understanding, and generating human\nlanguages. NLP is used in many technologies you interact with in your daily life—\nspam filtering, conversational assistants, search engines, and machine translation.\nThis first part of the book is intended to give you a gentle introduction to the field\nand bring you up to speed with how to build practical NLP applications.\n In chapter 1, we’ll begin by introducing the “what” and “why” of NLP—what\nis NLP, what is not NLP, how NLP technologies are used, and how it’s related to\nother fields of AI.\n In chapter 2, you’ll build a complete, working NLP application—a sentiment\nanalyzer—within an hour with the help of a powerful NLP framework, Allen-\nNLP. You’ll also learn to use basic machine learning (ML) concepts, including\nword embeddings and recurrent neural networks (RNNs). Don’t worry if this sounds\nintimidating—we’ll introduce you to the concepts gradually and provide an\nintuitive explanation.\n Chapter 3 provides a deep dive into the one of the most important concepts\nfor deep learning approaches to NLP—word and sentence embeddings. The\nchapter demonstrates how to use and even train them using your own data.\n Chapters 4 and 5 cover fundamental NLP tasks, sentence classification and\nsequence labeling. Though simple, these tasks have a wide range of applica-\ntions, including sentiment analysis, part-of-speech tagging, and named entity\nrecognition. \n This part familiarizes you with some basic concepts of modern NLP and we’ll\nbuild useful NLP applications along the way.\n\n\n2\nCHAPTER \n\n\n3\nIntroduction to natural\nlanguage processing\nThis is not an introductory book to machine learning or deep learning. You won’t\nlearn how to write neural networks in mathematical terms or how to compute gra-\ndients, for example. But don’t worry, even if you don’t have any idea what they are.\nI’ll explain those concepts as needed, not mathematically but conceptually. In fact,\nthis book contains no mathematical formulae—not a single one. Also, thanks to\nmodern deep learning libraries, you don’t really need to understand the math to\nbuild practical NLP applications. If you are interested in learning the theories and\nthe math behind machine learning and deep learning, you can find a number of\ngreat resources out there.\nThis chapter covers\nWhat natural language processing (NLP) is, what it is not, \nand why it’s such an interesting, yet challenging, field\nHow NLP relates to other fields, including artificial \nintelligence (AI) and machine learning (ML)\nWhat typical NLP applications and tasks are\nHow a typical NLP application is developed and structured\n\n\n4\nCHAPTER 1\nIntroduction to natural language processing\n But you do need to be at least comfortable enough to write in Python and know its\necosystems. However, you don’t need to be an expert in software engineering topics.\nIn fact, this book’s purpose is to introduce software engineering best practices for\ndeveloping NLP applications. You also don’t need to know NLP in advance. Again,\nthis book is designed to be a gentle introduction to the field. \n You need Python version 3.6.1 or higher and AllenNLP 2.5.0 or higher to run the\ncode examples in this book. Note that we do not support Python 2, mainly because\nAllenNLP (https://allennlp.org/), the deep natural language processing framework\nI’m going to heavily use in this book, supports only Python 3. If you haven’t done so, I\nstrongly recommend upgrading to Python 3 and familiarizing yourself with the latest\nlanguage features such as type hints and new string-formatting syntax. This will be\nhelpful, even if you are developing non-NLP applications.\n Don’t worry if you don’t have a Python development environment ready. Most of\nthe examples in this book can be run via the Google Colab platform (https://\ncolab.research.google.com). You need only a web browser to build and experiment\nwith NLP models!\n This book will use PyTorch (https://pytorch.org/) as its main choice of deep learn-\ning framework. This was a difficult decision for me, because several deep learning\nframeworks are equally great choices for building NLP applications, namely, Tensor-\nFlow, Keras, and Chainer. A few factors make PyTorch stand out among those\nframeworks—it’s a flexible and dynamic framework that makes it easier to prototype\nand debug NLP models; it’s becoming increasingly popular within the research com-\nmunity, so it’s easy to find open source implementations of major models; and the\ndeep NLP framework AllenNLP mentioned earlier is built on top of PyTorch.\n1.1\nWhat is natural language processing (NLP)?\nNLP is a principled approach to processing human language. Formally, it is a subfield\nof artificial intelligence (AI) that refers to computational approaches to process,\nunderstand, and generate human language. The reason it is part of AI is because lan-\nguage processing is considered a huge part of human intelligence. The use of lan-\nguage is arguably the most salient skill that separates humans from other animals.\n1.1.1\nWhat is NLP?\nNLP includes a range of algorithms, tasks, and problems that take human-produced\ntext as an input and produce some useful information, such as labels, semantic repre-\nsentations, and so on, as an output. Other tasks, such as translation, summarization,\nand text generation, directly produce text as output. In any case, the focus is on pro-\nducing some output that is useful per se (e.g., a translation) or as input to other down-\nstream tasks (e.g., parsing). I’ll touch upon some popular NLP applications and tasks\nin section 1.3.\n You might wonder why NLP explicitly has “natural” in its name. What does it mean\nfor a language to be natural? Are there any unnatural languages? Is English natural?\nWhich is more natural: Spanish or French?\n\n\n5\nWhat is natural language processing (NLP)?\n The word “natural” here is used to contrast natural languages with formal lan-\nguages. In this sense, all the languages humans speak are natural. Many experts\nbelieve that language emerged naturally tens of thousands of years ago and has\nevolved organically ever since. Formal languages, on the other hand, are types of lan-\nguages that are invented by humans and have strictly and explicitly defined syntax\n(i.e., what is grammatical) and semantics (i.e., what it means).\n Programming languages such as C and Python are good examples of formal lan-\nguages. These languages are defined in such a strict way that it is always clear what is\ngrammatical and ungrammatical. When you run a compiler or an interpreter on the\ncode you write in those languages, you either get a syntax error or not. The compiler\nwon’t say something like, “Hmm, this code is maybe 50% grammatical.” Also, the behav-\nior of your program is always the same if it’s run on the same code, assuming external\nfactors such as the random seed and the system states remain constant. Your interpreter\nwon’t show one result 50% of the time and another the other 50% of the time.\n This is not the case for human languages. You can write a sentence that is maybe\ngrammatical. For example, do you consider the phrase “The person I spoke to”\nungrammatical? There are some grammar topics where even experts disagree with\neach other. This is what makes human languages interesting but challenging, and why\nthe entire field of NLP even exists. Human languages are ambiguous, meaning that\ntheir interpretation is often not unique. Both structures (how sentences are formed)\nand semantics (what sentences mean) can have ambiguities in human language. As an\nexample, let’s take a close look at the next sentence:\n He saw a girl with a telescope.\nWhen you read this sentence, who do you think has a telescope? Is it the boy, who’s\nusing a telescope to see a girl (from somewhere far), or the girl, who has a telescope\nand is seen by the boy? There seem to be at least two interpretations of this sentence\nas shown in figure 1.1.\nFigure 1.1\nTwo interpretations of “He saw a girl with a telescope.”\n",
      "page_number": 18
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 26-33)",
      "start_page": 26,
      "end_page": 33,
      "detection_method": "topic_boundary",
      "content": "6\nCHAPTER 1\nIntroduction to natural language processing\nThe reason you are confused upon reading this sentence is because you don’t know\nwhat the phrase “with a telescope” is about. More technically, you don’t know what\nthis prepositional phrase (PP) modifies. This is called a PP-attachment problem and is a\nclassic example of syntactic ambiguity. A syntactically ambiguous sentence has more\nthan one interpretation of how the sentence is structured. You can interpret the sen-\ntence in multiple ways, depending on which structure of the sentence you believe.\n Another type of ambiguity that may arise in natural language is semantic ambiguity.\nThis is when the meaning of a word or a sentence, not its structure, is ambiguous. For\nexample, let’s look at the following sentence:\n I saw a bat.\nThere is no question how this sentence is structured. The subject of the sentence is “I”\nand the object is “a bat,” connected by the verb “saw.” In other words, there is no syn-\ntactical ambiguity in it. But how about its meaning? “Saw” has at least two meanings.\nOne is the past tense of the verb “to see.” The other is to cut some object with a saw. Sim-\nilarly, “a bat” can mean two very different things: is it a nocturnal flying mammal or a\npiece of wood used to hit a ball? All in all, does this sentence mean that I observed a noc-\nturnal flying mammal or that I cut a baseball or cricket bat? Or even (cruelly) that I cut\na nocturnal animal with a saw? You never know, at least from this sentence alone.\n Ambiguity is what makes natural languages rich but also challenging to process.\nWe can’t simply run a compiler or an interpreter on a piece of text and just “get it.”\nWe need to face the complexities and subtleties of human languages. We need a scien-\ntific, principled approach to deal with them. That’s what NLP is all about.\n Welcome to the beautiful world of natural languages.\n1.1.2\nWhat is not NLP?\nNow let’s consider the following scenario and think how you’d approach this prob-\nlem: you are working as a junior developer at a midsized company that has a\nconsumer-facing product line. It’s 3 p.m. on a Friday. The rest of the team is becoming\nmore and more restless as the weekend approaches. That’s when your boss drops by at\nyour cubicle.\n “Hey, got a minute? I’ve got something interesting to show you. I just sent it to\nyou.”\n Your boss just sent you an email with a huge zip file attached to it.\n “OK, so this is a giant TSV file. It contains all the responses to the survey questions\nabout our product. I just got this data from the marketing team.”\n Obviously, the marketing team has been collecting user opinions about one of the\nproducts through a series of survey questions online. \n “The survey questions include standard ones like ‘How did you know about our\nproduct?’ and ‘How do you like our product?’ There is also a free-response question,\nwhere our customers can write whatever they feel about our product. The thing is, the\nmarketing team realized there was a bug in the online system and the answers to the\nsecond question were not recorded in the database at all.”\n\n\n7\nWhat is natural language processing (NLP)?\n “Wait, so there’s no way to tell how the customers are feeling about our product?”\nThis sounds weirdly familiar. This must be a copy-and-paste error. When you first cre-\nated an online data-collection interface, you copied and pasted the backend code\nwithout modifying the ID parameters, resulting in a loss of some data fields. \n “So,” your boss continues. “I was wondering if we can recover the lost data some-\nhow. The marketing team is a little desperate now because they need to report the\nresults to the VP early next week.” \n At this point, your bad feeling has been confirmed. Unless you come up with a way\nto get this done as quickly as possible, your weekend plans will be ruined. \n “Didn’t you say you were interested in some machine learning? I think this is a per-\nfect project for you. Anyway, it’d be great if you can give it a stab and let me know what\nyou find. Do you think you can have some results by Monday?”\n “Well, I’ll give it a try.” \n You know “no” is not an acceptable answer here. Satisfied with your answer, your\nboss smiles and walks off. \n You start by skimming the TSV file. To your relief, its structure is fairly standard—it\nhas several fields such as timestamps and submission IDs. At the end of each line is a\nlengthy field for the free-response question. Here they are, you think. At least you\nknow where to look for some clues.\n After a quick glance over the field, you find responses such as “A very good prod-\nuct!” and “Very bad. It crashes all the time!” Not too bad, you think. At least you can\ncapture these simple cases. You start by writing the following method that captures\nthose two cases:\ndef get_sentiment(text):\n    \"\"\"Return 1 if text is positive, -1 if negative.\n       Otherwise, return 0.\"\"\"\n    if 'good' in text:\n        return 1\n    elif 'bad' in text:\n        return -1\n    return 0\nThen you run this method on the responses in the file and log the results, along with\nthe original input. As intended, this method seems to be able to capture a dozen or so\nof the responses that contains “good” or “bad.” \n But then you start to see something alarming, as shown next:\n “I can’t think of a single good reason to use this product”: positive\n “It’s not bad.”: negative\nOops, you think. Negation. Yeah, of course. But this is pretty easy to deal with. You\nmodify the method as follows:\ndef get_sentiment(text):\n    \"\"\"Return 1 if text is positive, -1 if negative.\n       Otherwise, return 0.\"\"\"\n    sentiment = 0\n\n\n8\nCHAPTER 1\nIntroduction to natural language processing\n    if 'good' in text:\n        sentiment = 1\n    elif 'bad' in text:\n        sentiment = -1\n    if 'not' in text or \"n't\" in text:\n        sentiment *= -1\n    return sentiment\nYou run the script again. This time, it seems to be behaving as intended, until you see\nan even more complicated example:\n “The product is not only cheap but also very good!”: negative\nHmm, you think. This is probably not as straightforward as I initially thought after all.\nMaybe the negation has to be somewhere near “good” or “bad” for it to be effective.\nWondering what steps you could take next, you scroll down to see more examples,\nwhich is when you see responses like these:\n “I always wanted this feature badly!”: negative\n “It’s very badly made.”: negative\nYou silently curse to yourself. How could a single word in a language have two com-\npletely opposite meanings? At this point, your little hope for enjoying the weekend\nhas already disappeared. You are already wondering what excuses you use with your\nboss next Monday.\n \n As a reader of this book, you’ll know better. You’ll know that NLP is not about\nthrowing a bunch of ifs and thens at natural language text. It is a more principled\napproach to processing natural language. In the following chapters, you’ll learn how\nyou should approach this problem before writing a single line of code and how to\nbuild a custom-made NLP application just for your task at hand.\n1.1.3\nAI, ML, DL, and NLP\nBefore delving into the details of NLP, it’d be useful to clarify how it relates to other,\nsimilar fields. Most of you have at least heard about artificial intelligence (AI) and\nmachine learning (ML). You may also have heard of deep learning (DL), because it’s\ngenerating a lot of buzz in popular media these days. Figure 1.2 illustrates how those\ndifferent fields overlap with each other.\n Artificial intelligence (AI) is a broad umbrella field that is concerned with achiev-\ning human-like intelligence using machines. It encompasses a wide range of subfields,\nincluding machine learning, natural language processing, computer vision, and\nspeech recognition. The field also includes subfields such as reasoning, planning, and\nsearch, which do not fall under either machine learning or natural language process-\ning and are not in the scope of this book.\n Machine learning (ML) is usually considered a subfield of artificial intelligence that\nis about improving computer algorithms through experience and data. This includes\nlearning a general function that maps inputs to outputs based on past experience\n\n\n9\nWhat is natural language processing (NLP)?\n(supervised learning), drawing hidden patterns and structures from data (unsuper-\nvised learning), and learning how to act in a dynamic environment based on indirect\nrewards (reinforcement learning). Throughout this book, we’ll make a heavy use of\nsupervised machine learning, which is the main paradigm for training NLP models.\n Deep learning (DL) is a subfield of machine learning that usually uses deep neural\nnetworks. These neural network models are called “deep” because they consist of a\nnumber of layers. A layer is just a fancy word for a substructure of neural networks. By\nhaving many stacked layers, deep neural networks can learn complex representations\nof data and can capture highly complicated relationships between the input and the\noutput.\n As the amount of available data and computational resources increases, modern\nNLP makes a heavier and heavier use of machine learning and deep learning. Modern\nNLP applications and tasks are usually built on top of machine learning pipelines and\ntrained from data. But also notice in figure 1.2 that a part of NLP does not overlap with\nmachine learning. Traditional methods such as counting words and measuring similar-\nities between text are usually not considered to be machine learning techniques per se,\nalthough they can be important building blocks for ML-based models.\n I’d also like to mention some other fields that are related to NLP. One such field is\ncomputational linguistics (CL). As its name suggests, computational linguistics is a sub-\nfield of linguistics that uses computational approaches to study human language. The\nmain distinction between CL and NLP is that the former encompasses scientific\napproaches to study language, whereas the latter is concerned with engineering\nArtificial\nintelligence (AI)\nMachine\nlearning (ML)\nNatural\nlanguage\nprocessing\n (NLP) \nDeep\nlearning (DL)\nFigure 1.2\nThe relationship \namong different fields: AI, \nML, DL, and NLP\n\n\n10\nCHAPTER 1\nIntroduction to natural language processing\napproaches for making computers perform something useful related to language.\nPeople often use those terms interchangeably, partly due to some historical reasons.\nFor example, the most prestigious conference in the field of NLP is called ACL, which\nactually stands for “the Association for Computational Linguistics!”\n Another related field is text mining. Text mining is a type of data mining targeted at\ntextual data. Its focus is on drawing useful insights from unstructured textual data,\nwhich is a type of text data that is not formatted in a form that is easily interpretable by\ncomputers. Such data is usually collected from various sources, such as crawling the\nweb and social media. Although its purpose is slightly different from that of NLP,\nthese two fields are similar, and we can use the same tools and algorithms for both. \n1.1.4\nWhy NLP?\nIf you are reading this, you have at least some interest in NLP. Why is NLP exciting?\nWhy is it worth learning more about NLP and, specifically, real-world NLP?\n The first reason is that NLP is booming. Even without the recent AI and ML boom,\nNLP is more important than ever. We are witnessing the advent of practical NLP appli-\ncations in our daily lives, such as conversational agents (think Apple Siri, Amazon\nAlexa, and Google Assistant) and near human-level machine translation (think Google\nTranslate). A number of NLP applications are already an integral part of our day-to-day\nactivities, such as spam filtering, search engines, and spelling correction, as we’ll discuss\nlater. The number of Stanford students enrolled in NLP classes grew fivefold from\n2008 to 2018 (http://realworldnlpbook.com/ch1.html#tweet1). Similarly, the number\nof attendees for EMNLP (Empirical Methods in Natural Language Processing), one\nof the top NLP conferences, doubled within just one year (http://realworldnlpbook\n.com/ch1.html#tweet2). Other major NLP conferences are also experiencing similar\nincreases in participants and paper submissions (http://realworldnlpbook.com/ch1\n.html#nivre17).\n The second reason is that NLP is an evolving field. The field of NLP itself has a\nlong history. The first experiment to build a machine translation system, called The\nGeorgetown–IBM Experiment, was attempted back in 1954. For more than 30 years since\nthis experiment, most NLP systems relied on handwritten rules. Yes, it was not much\ndifferent from what you just saw in section 1.1.1. The first milestone, which came in\nthe late 1980s, was the use of statistical methods and machine learning for NLP. Many\nNLP systems started leveraging statistical models trained from data. This led to some\nrecent successes in NLP, including, most notably, IBM Watson. The second milestone\nwas more drastic. Starting around the late 2000s, the use of so-called deep learning,\nthat is, deep neural network models, took the field by storm. By the mid-2010s, deep\nneural network models became the new standard in the field. \n This second milestone was so drastic and fast that it’s worth noting here. New neural\nnetwork–based NLP models are not only more effective but also a lot simpler. For exam-\nple, it used to take a lot of expertise and effort to replicate even a simple, baseline\nmachine translation model. One of the most popular open source software packages\nfor statistical machine translation, called Moses (http://www.statmt.org/moses/), is a\n\n\n11\nWhat is natural language processing (NLP)?\nbehemoth, consisting of 100,000s of lines of code and dozens of supporting modules\nand tools. Experts spent hours just installing the software and making it work. On the\nother hand, as of 2018, anyone with some prior programming experience could run a\nneural machine translation system more powerful than traditional statistical models\nwith a fraction of the code size—less than a few thousand lines of code (e.g., see Tensor-\nFlow’s neural machine translation tutorial at https://github.com/tensorflow/nmt).\nAlso, the new neural network models are trained “end-to-end,” which means that those\nbig, monolithic networks take the input and directly produce the output. Entire models\nare trained to match the desired output. On the other hand, traditional machine learn-\ning models consist of (at least) several submodules. These submodules are trained sep-\narately using different machine learning algorithms. In this book, I’ll mainly discuss\nmodern neural network–based approaches to NLP but also touch upon some tradi-\ntional concepts as well.\n The third and final reason is that NLP is challenging. Understanding and produc-\ning language is the central problem of artificial intelligence, as we saw in the previous\nsection. The accuracy and performance in major NLP tasks such as speech recogni-\ntion and machine translation got drastically better in the past decade or so. But\nhuman-level understanding of language is far from being solved.\n To verify this quickly, open up your favorite machine translation service (or simply\nGoogle Translate), and type this sentence: “I saw her duck.” Try to translate it to Span-\nish or some other language you understand. You should see words like “pato,” which\nmeans “a duck” in Spanish. But did you notice another interpretation of this sen-\ntence? See figure 1.3 for the two interpretations. The word “duck” here could be a\nverb meaning “to crouch down.” Try adding another sentence after this, such as “She\ntried to avoid a flying ball.” Did the machine translation change the first translation in\nany way? The answer is probably no. You should still see the same “pato” in the transla-\ntion. As you can see, most (if not all) commercial machine translation systems that are\navailable as of today do not understand the context outside of the sentence that is\nbeing translated. A lot of research effort is spent on this problem in academia, but this\nis still one of many problems in NLP that is considered unsolved.\nFigure 1.3\nTwo interpretations \nof “I saw her duck.”\n\n\n12\nCHAPTER 1\nIntroduction to natural language processing\nCompared to other AI fields such as robotics and computer vision, language has its\nown quirks. Unlike images, utterances and sentences have variable length. You can say\na very short sentence (“Hello.”) or a very long one (“A quick brown fox . . .”). Most\nmachine learning algorithms are not good at dealing with something of variable\nlength, and you need to come up with ways to represent languages with something\nmore fixed. If you look back at the history of the field, NLP is largely concerned with\nthe problem of how to represent language mathematically. Vector space models and\nword embeddings (discussed in chapter 3) are some examples of this. \n Another characteristic of language is that it is discrete. What this means is that\nthings in languages are separate as concepts. For example, if you take a word “rat” and\nchange its first letter to the next one, you’ll have “sat.” In computer memory, the dif-\nference is just a single bit. However, there is no relationship between those two words\nexcept they both end with “at,” and maybe a rat can sit. There is no such thing as\nsomething that is in between “rat” and “sat.” These two are totally discrete, separate\nconcepts that happen to have similar spelling. On the other hand, if you take an\nimage of a car and change the value of a pixel by a single bit, you still have a car that is\nalmost identical to the one before this change. Maybe it has a slightly different color.\nIn other words, images and sounds are continuous, meaning that you can make small\nmodifications without greatly affecting what they are. Many mathematical toolkits,\nsuch as vectors, matrices, and functions, are good at dealing with something continu-\nous. The history of NLP is actually a history of challenging this discreteness of lan-\nguage, and only recently have we begun to see some successes on this front, for\nexample, with word embeddings.\n1.2\nHow NLP is used\nAs I mentioned previously, NLP is already an integral part of our daily life. In modern\nlife, a larger and larger portion of our daily communication is done online, and our\nonline communication is still largely conducted in natural language text. Think of\nyour favorite social networking services, such as Facebook and Twitter. Although you\ncan post photos and videos, a large portion of communication is still in text. As long\nas you are dealing with text, there is a need for NLP. For example, how do you know if\na particular post is spam? How do you know which posts are the ones you are most\nlikely to “like?” How do you know which ads you are most likely to click?\n Because many large internet companies need to deal with text in one way or\nanother, chances are many of them are already using NLP. You can also confirm this\nfrom their “careers” page—you’ll see that they are always hiring NLP engineers and\ndata scientists. NLP is also used to a varying extent in many other industries and prod-\nucts including, but not limited to, customer service, e-commerce, education, enter-\ntainment, finance, and health care, which all involve text in some ways.\n Many NLP systems and services can be classified into or built by combining some\nmajor types of NLP applications and tasks. In this section, I’m going to introduce\nsome of the most popular applications of NLP as well as common NLP tasks.\n\n\n13\nHow NLP is used\n1.2.1\nNLP applications\nAn NLP application is a software application whose main purpose is to process natural\nlanguage text and draw some useful information from it. Similar to general software\napplications, it can be implemented in various ways, such as an offline data-processing\nscript, an offline standalone application, a backend service, or a full-stack service with\na frontend, depending on its scope and use cases. It can be built for end users to use\ndirectly, for other backend services to consume its output, or for other businesses to\nuse as a SaaS (software as a service). \n You can use many NLP applications out of the box, such as machine translation\nsoftware and major SaaS products (e.g., Google Cloud API), if your requirement is\ngeneric and doesn’t require a high level of customization. You can also build your own\nNLP applications if you need customizations and/or you need to deal with a specific\ntarget domain. This is exactly what you’ll learn throughout this book!\nMACHINE TRANSLATION\nMachine translation is probably one of the most popular and easy-to-understand NLP\napplications. Machine translation (MT) systems translate a given text from one lan-\nguage to another language. An MT system can be implemented as a full-stack service\n(e.g., Google Translate), as well as a pure backend service (e.g., NLP SaaS products).\nThe language the input text is written in is called the source language, whereas the one\nfor the output is called the target language. MT encompasses a wide range of NLP prob-\nlems, including language understanding and generation, because MT systems need\nto understand the input and then generate the output. MT is one of the most well-\nstudied areas in NLP, and it was one of the earliest applications of NLP as well.\n One challenge in MT is the tradeoff between fluency and adequacy. Translation\nneeds to be fluent, meaning that the output has to sound natural in the target lan-\nguage. Translation also needs to be adequate, meaning that the output has to reflect\nthe meaning expressed by the input as closely as possible. These two are often in con-\nflict, especially when the source and the target languages are not very similar (e.g.,\nEnglish and Mandarin Chinese). You can write a sentence that is a precise, verbatim\ntranslation of the input, but doing so often leads to something that doesn’t sound nat-\nural in the target language. On the other hand, you can make up something that\nsounds natural but might not reflect the precise meaning. Good human translators\naddress this tradeoff in a creative way. It’s their job to come up with translations that\nare natural in the target language while reflecting the meaning of the original.\nGRAMMATICAL AND SPELLING ERROR CORRECTION\nMost major web browsers nowadays support spelling correction. Even if you forget\nhow to spell “Mississippi,” you can do your best and type what you remember, and the\nbrowser highlights it with a correction. Some word-processing software applications,\nincluding recent versions of Microsoft Word, do more than just correct spelling. They\npoint out grammatical errors such as uses of “it’s” instead of “its.” This is not an easy\nfeat, because both words are, in a sense, “correct” (no mistakes in spelling), and the\nsystem needs to infer whether they are used correctly from the context. Some\n",
      "page_number": 26
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 34-45)",
      "start_page": 34,
      "end_page": 45,
      "detection_method": "topic_boundary",
      "content": "14\nCHAPTER 1\nIntroduction to natural language processing\ncommercial products (most notably, Grammarly, https://www.grammarly.com/) spe-\ncialize in grammatical error correction. Some products go a long way and point out\nincorrect usage of punctuation and even writing styles. These products are popular\namong both native and non-native speakers of the language.\n Research into grammatical error correction has been active due to the increasing\nnumber of non-native English speakers. Traditionally, grammatical error correction\nsystems for non-native English speakers dealt with individual types of mistakes one by\none. For example, you could think of a subcomponent of the system that detects and\ncorrects only incorrect uses of articles (a, an, the, etc.), which is very common among\nnon-native English speakers. More recent approaches to grammatical error correction\nare similar to the ones for machine translation. You can think of the (potentially\nincorrect) input as one language and the corrected output as another. Then your job\nis to “translate” between these two languages! \nSEARCH ENGINE\nAnother application of NLP that is already an integral part of our daily lives is search\nengines. Few people would think of search engines as an NLP application, yet NLP\nplays such an important role in making search engines useful that they are worth men-\ntioning here.\n Page analysis is one area where NLP is heavily used for search engines. Ever won-\nder why you don’t see any “hot dog” pages when you search for “dogs?” If you have any\nexperience building your own full-text search engines using open source software\nsuch as Solr and Elasticsearch, and if you simply used a word-based index, your search\nresult pages would be littered with “hot dogs,” even when you want just “dogs.” Major\ncommercial search engines solve this problem by running the page content being\nindexed through NLP pipelines that recognize that “hot dogs” are not a type of\n“dogs.” But the extent and types of NLP pipelines that go into page analysis is confi-\ndential information for search engines and is difficult to know.\n Query analysis is another NLP application in search engines. If you have noticed\nGoogle showing a box with pictures and bios when you search for a celebrity or a box\nwith the latest news stories when you search for certain current events, that’s query anal-\nysis in action. Query analysis identifies the intent (what the user wants) of the query and\nshows relevant information accordingly. A common way to implement query analysis is\nto make it a classification problem, where an NLP pipeline classifies queries into classes\nof intent (e.g., celebrity, news, weather, videos), although again, the details of how com-\nmercial search engines run query analysis are usually highly confidential.\n Finally, search engines are not only about analyzing pages and classifying queries.\nThey have many other functionalities that make your searches easier, one of which is\nquery correction. This comes into play when you make a spelling or a grammatical mis-\ntake when formulating the query, and Google and other major search engines show cor-\nrections with labels such as “showing results for:” and “Did you mean.” How this works\nis somewhat similar to grammatical error correction that I mentioned earlier, except it\nis optimized for the types of mistakes and queries that search engine users use.\n\n\n15\nHow NLP is used\nDIALOG SYSTEMS\nDialog systems are machines that humans can have conversations with. The field of\ndialog systems has a long history. One of the earliest dialog systems, ELIZA, was devel-\noped in 1966. \n But it’s only recently that dialog systems have found their ways into our daily lives.\nWe have seen an almost exponential increase in their popularity in recent years, mainly\ndriven by the availability of consumer-facing “conversational AI” products such as Ama-\nzon Alexa and Google Assistant. In fact, according to a survey in 2018, 20% of US homes\nalready own a smart speaker. You may also remember being mind-blown watching the\nkeynote at Google IO in 2018, where Google’s conversational AI called Google Duplex\nwas shown making a phone call to a hair salon and a restaurant, having natural conver-\nsations with the staff at the business, and making an appointment on behalf of its user. \n The two main types of dialog systems are task-oriented and chatbots. Task-oriented\ndialog systems are used to achieve specific goals (for example, reserving a plane\nticket), obtaining some information, and, as we saw, making a reservation at a restau-\nrant. Task-oriented dialog systems are usually built as an NLP pipeline consisting of\nseveral components, including speech recognition, language understanding, dialog\nmanagement, response generation, and speech synthesis, which are usually trained\nseparately. Similar to machine translation, though, there are new deep learning\napproaches where dialog systems (or their subsystems) are trained end-to-end.\n The other type of dialog system is chatbots, whose main purpose is to have conver-\nsations with humans. Traditional chatbots are usually managed by a set of handwritten\nrules (e.g., when the human says this, say that). Recently, the use of deep neural net-\nworks, particularly sequence-to-sequence models and reinforcement learning, has\nbecome increasingly popular. However, because the chatbots do not serve particular\npurposes, the evaluation of chatbots, that is, assessing how good a particular chatbot\nis, remains an open question.\n1.2.2\nNLP tasks\nBehind the scenes, many NLP applications are built by combining multiple NLP com-\nponents that solve different NLP problems. In this section, I introduce some notable\nNLP tasks that are commonly used in NLP applications.\nTEXT CLASSIFICATION\nText classification is the process of classifying pieces of text into different categories.\nThis NLP task is one of the simplest yet most widely used. You might not have heard of\nthe term “text classification” before, but I bet most of you benefit from this NLP task\nevery day. For example, spam filtering is one type of text classification. It classifies\nemails (or other types of text, such as web pages) into two categories—spam or not\nspam. This is why you get very few spam emails when you use Gmail and you see so few\nspammy (low-quality) web pages when you use Google. \n Another type of text classification is called sentiment analysis, which is what we saw\nin section 1.1. Sentiment analysis is used to automatically identify subjective informa-\ntion, such as opinions, emotions, and feelings, within text. \n\n\n16\nCHAPTER 1\nIntroduction to natural language processing\nPART-OF-SPEECH TAGGING\nA part of speech (POS) is a category of words that share the similar grammatical proper-\nties. In English, for example, nouns describe the names of things like objects, animals,\npeople, and concepts, among many other things. A noun can be used as a subject of a\nverb, an object of a verb, and an object of a preposition. Verbs, in contrast, describe\nactions, states, and occurrences. Other English parts of speech include adjectives\n(green, furious), adverbs (cheerfully, almost), determiners (a, the, this, that), prepositions\n(in, from, with), conjunctions (and, yet, because), and many others. Almost all languages\nhave nouns and verbs, but other parts of speech differ from language to language. For\nexample, many languages, such as Hungarian, Turkish, and Japanese, have postposi-\ntions instead of prepositions, which are placed after words to add some extra meaning\nto them. A group of NLP researchers came up with a set of tags that cover frequent\nparts of speech that exist in most languages, called a universal part-of-speech tagset\n(http://realworldnlpbook.com/ch1.html#universal-pos). This tagset is widely used for\nlanguage-independent tasks.\n Part-of-speech tagging is the process of tagging each word in a sentence with a cor-\nresponding part-of-speech tag. Some of you may have done this at school. As an exam-\nple, let’s take the sentence “I saw a girl with a telescope.” The POS tags for this\nsentence are shown in figure 1.4.\nThese tags come from the Penn Treebank POS tagset, which is the most popular stan-\ndard corpus for training and evaluating various NLP tasks such as POS tagging and\nparsing. Traditionally, POS tagging was solved by sequential labeling algorithms such\nas hidden Markov models (HMMs) and conditional random fields (CRFs). Recently,\nInput sentence\nPOS tags\nPRP\nI\nsaw\na\nVBD\nDT\ngirl\nNN\nwith\nIN\na\ntelescope\n.\nDT\nNN\n.\nPOS tag\nDT\nIN\nDescription\nDeterminer\nPreposition\nNN\nNoun (singular or mass)\nPRP\nPronoun\nVBD\nVerb (past tense)\nFigure 1.4\nPart-of-\nspeech (POS) tagging\n\n\n17\nHow NLP is used\nrecurrent neural networks (RNNs) have become a popular and practical choice for\ntraining a POS tagger with high accuracy. The results of POS tagging are often used as\nthe input to other downstream NLP tasks, such as machine translation and parsing.\nI’ll cover part-of-speech tagging in more detail in chapter 5.\nPARSING\nParsing is the task of analyzing the structure of a sentence. Broadly speaking, there are\ntwo main types of parsing, constituency parsing and dependency parsing, which we’ll dis-\ncuss in detail next.\n Constituency parsing uses context-free grammars to represent natural language sen-\ntences. (See http://mng.bz/GO5q for a brief introduction to context-free grammars).\nA context-free grammar is a way to specify how smaller building blocks of a language\n(e.g., words) are combined to form larger building blocks (e.g., phrases and clauses)\nand eventually sentences. To put it another way, it specifies how the largest unit (a sen-\ntence) is broken down to phrases and clauses and all the way down to words. The ways\nthe linguistic units interact with each other are specified by a set of production rules as\nfollows:\nS -> NP VP\nNP -> DT NN | PRN | NP PP\nVP -> VBD NP | VBD PN PP\nPP -> IN NP\nDT -> a\nIN -> with\nNN -> girl | telescope\nPRN -> I\nVBD -> saw\nA production rule describes a transformation from the symbol on the left-hand side\n(e.g., “S”) to the symbols on the right-hand side (e.g., “NP VP”). The first rule means\nthat a sentence is a noun phrase (NP) followed by a verb phrase (VP). Some of the\nsymbols (e.g., DT, NN, VBD) may look familiar to you—yes, they are the POS tags we\njust saw in the POS tagging section. In fact, you can consider POS tags as the smallest\ngrammatical categories that behave in similar ways (because they are!).\n Now the parser’s job is to figure out how to reach the final sym-\nbol (in this case, “S”) starting from the raw words in the sentence.\nYou can think of those rules as transformation rules from the sym-\nbols on the right to the ones on the left by traversing the arrow\nbackward. For example, using the rule “DT  a” and “NN  girl,”\nyou can convert “a girl” to “DT NN.” Then, if you use “NP  DT\nNN,” you can reduce the entire phrase to “NP.” If you illustrate this\nprocess in a tree-like diagram, you get something like the one\nshown in figure 1.5.\nNP\nDT\nNN\na\ngirl\nFigure 1.5\nSubtree for \n“a girl”\n\n\n18\nCHAPTER 1\nIntroduction to natural language processing\nTree structures that are created in the process of parsing are called parse trees, or sim-\nply parses. The figure is a subtree because it doesn’t cover the entirety of the tree (i.e.,\nit doesn’t show all the way from “S” to words). Using the sentence “I saw a girl with a\ntelescope” that we discussed earlier and see if you can parse it by hand. If you keep\nbreaking down the sentence using the production rules until you get the final “S” sym-\nbol, you get the tree-like structure shown in figure 1.6.\nFigure 1.6\nParse tree for “I saw a girl with a telescope.”\nDon’t worry if the tree in figure 1.6 is different from what you got. Actually, there’s\nanother parse tree that is a valid parse of this sentence, shown in figure 1.7.\nFigure 1.7\nAnother parse tree for “I saw a girl with a telescope.”\nNP\nVP\nNP\nPRN\nVP\nVBD\nDT\nNN\na\ngirl\nsaw\nI\nNP\nPP\nDT\nNN\na\nwith\ntelescope\nIN\nS\nNP\nNP\nVP\nNP\nPRN\nVP\nVBD\nDT\nNN\na\ngirl\nsaw\nI\nNP\nPP\nDT\nNN\na\nwith\ntelescope\nIN\nS\n\n\n19\nHow NLP is used\nIf you look at those two trees carefully, you’ll notice a difference where the “PP” (prep-\nositional phrase) is located, or attached. In fact, these two parse trees correspond to\nthe two different interpretations of this sentence we discussed in section 1.1. The first\ntree (figure 1.6), where the PP attaches the verb “saw,” corresponds to the interpreta-\ntion where the boy is using a telescope to see the girl. In the second tree (figure 1.7),\nwhere the PP attaches to the noun “a girl,” the boy saw the girl who has a telescope.\nParsing is a great step forward to reveal the structure and the semantics of a sentence,\nbut in cases like this one, parsing alone cannot uniquely decide what is the single most\nlikely interpretation of a sentence.\n The other type of parsing is called dependency parsing. Dependency parsing uses\ndependency grammars to describe the structure of sentences, not in terms of phrases\nbut in terms of words and the binary relations between them. For example, the result\nof dependency parsing of the earlier sentence is shown in figure 1.8.\nFigure 1.8\nDependency parse for “I saw a girl with a telescope.”\nNotice that each relation is directional and labeled. A relation specifies which word\ndepends on which word and the type of relationship between the two. For example,\nthe relation connecting “a” to “girl” is labeled “det,” meaning the first word is the\ndeterminer of the second. If you take the most central word, “saw,” and pull it upward,\nyou’ll notice that these words and relations form a tree. Such trees are called depen-\ndency trees.\n One advantage of dependency grammars is that they are agnostic regarding some\nword-order changes, meaning that the order of certain words in the sentence will not\nchange the dependency tree. For example, in English, there is some freedom as to\nwhere to put an adverb in a sentence, especially when the adverb describes the man-\nner in which the action referred to by the verb is done. For example, “I carefully\npainted the house” and “I painted the house carefully” are both acceptable and mean\nthe same thing. If you represent these sentences by a dependency grammar, the word\n“carefully” always modifies the verb “painted,” and the two sentences have completely\nidentical dependency trees. Dependency grammars capture more than just phrasal\nstructures of sentences—they capture something more fundamental about the rela-\ntionship of the words. Therefore, dependency parsing is considered an important step\ntoward semantic analysis of natural language. A group of researchers is working on a\nI\nsaw\na\ngirl\nwith\na\ntelescope\nnsubj\ndet\ndet\ncase\ndobj\nnmod\n\n\n20\nCHAPTER 1\nIntroduction to natural language processing\nformal language-independent dependency grammar, called Universal Dependencies,\nthat is linguistically motivated and applicable to many languages, similar to the univer-\nsal POS tagset.\nTEXT GENERATION\nText generation, also called natural language generation (NLG), is the process of gener-\nating natural language text from something else. In a broader sense, machine transla-\ntion, which we discussed previously, involves a text-generation problem, because MT\nsystems need to generate text in the target language. Similarly, summarization, text\nsimplification, and grammatical error correction all produce natural language text as\noutput and are instances of text-generation tasks. Because all of these tasks take natu-\nral language text as their input, they are called text-to-text generation. \n Another class of text-generation task is called data-to-text generation. For those\ntasks, the input is data that is not text. For example, a dialog system needs to generate\nnatural utterances based on the current state of the conversation. A publisher may\nwish to generate news text based on events such as sports game outcomes and weather.\nThere is also a growing interest in generating natural language text that best describes\na given image, called image captioning.\n Finally, a third class of text classification is unconditional text generation, where\nnatural language text is generated randomly from a model. You can train models so\nthat they can generate random academic papers, Linux source code, or even poems\nand play scripts. For example, Andrej Karpathy trained an RNN model from all of\nShakespeare’s works and succeeded in generating pieces of text that look exactly like\nhis work (http://realworldnlpbook.com/ch1.html#karpathy15), as shown next:\nPANDARUS:\nAlas, I think he shall be come approached and the day\nWhen little srain would be attain'd into being never fed,\nAnd who is but a chain and subjects of his death,\nI should not sleep.\nSecond Senator:\nThey are away this miseries, produced upon my soul,\nBreaking and strongly should be buried, when I perish\nThe earth and thoughts of many states.\nDUKE VINCENTIO:\nWell, your wit is in the care of side and that.\nSecond Lord:\nThey would be ruled after this chamber, and\nmy fair nues begun out of the fact, to be conveyed,\nWhose noble souls I'll have the heart of the wars.\nClown:\nCome, sir, I will make did behold your worship.\nVIOLA:\nI'll drink it.\n\n\n21\nBuilding NLP applications\nTraditionally, text generation has been solved by handcrafted templates and rules for\ngenerating text from some information. You can think of this as the reverse of parsing,\nwhere rules are used to infer information about natural language text, as we discussed\nearlier. In recent years, neural network models are an increasingly popular choice for\nnatural language generation, be it text-to-text generation (sequence-to-sequence\nmodels), data-to-text generation (encoder-decoder models), and unconditional text\ngeneration (neural language models and generative adversarial networks, or GANs).\nWe’ll discuss text generation more in chapter 5.\n1.3\nBuilding NLP applications\nIn this section, I’m going to show you how NLP applications are typically developed\nand structured. Although details may vary on a case-by-case basis, understanding the\ntypical process helps you plan and budget before you start developing an application.\nIt also goes a long way if you know best practices in developing NLP applications\nbeforehand.\n1.3.1\nDevelopment of NLP applications\nThe development of NLP applications is a highly iterative process, consisting of many\nphases of research, development, and operations (figure 1.9). Most learning materials\nsuch as books and online tutorials focus mainly on the training phase, although all the\nother phases of application development are equally important for real-world NLP\nR\nE\nS\nE\nA\nR\nC\nH\nO\nP\nE\nR\nA\nT\nI\nO\nN\nS\nD\nE\nV\nE\nL\nO\nP\nM\nE\nN\nT\nImplementation\nDeploying\nNLP system \ndevelopment\nprocess\nTraining\nMonitoring\nData\ncollection\nAnalysis &\nexperimenting\nFigure 1.9\nThe development \ncycle of NLP applications\n\n\n22\nCHAPTER 1\nIntroduction to natural language processing\napplications. In this section, I briefly introduce what each stage involves. Note that no\nclear boundary exists between these phases. It is not uncommon that application\ndevelopers (researchers, engineers, managers, and other stakeholders) go back and\nforth between some of these phases through trial and error.\nDATA COLLECTION\nMost modern NLP applications are based on machine learning. Machine learning, by\ndefinition, requires data on which NLP models are trained (remember the definition\nof ML we talked about previously—it’s about improving algorithms through data). In\nthis phase, NLP application developers discuss how to formulate the application as an\nNLP/ML problem and what kind of data should be collected. Data can be collected\nfrom humans (e.g., by hiring in-house annotators and having them go through a bunch\nof text instances), crowdsourcing (e.g., using platforms such as Amazon Mechanical\nTurk), or automated mechanisms (e.g., from application logs or clickstreams). \n You may choose not to use machine learning approaches for your NLP application\nat first, which could totally be the right choice depending on various factors, such as\ntime, budgets, the complexity of the task, and the expected amount of data you might\nbe able to collect. Even in that case, it may be a good idea to collect a small amount of\ndata for validation purposes. I’ll talk more about training, validation, and testing of\nNLP applications in chapter 11.\nANALYSIS AND EXPERIMENTING\nAfter collecting the data, you move on to the next phase where you analyze and run\nsome experiments. For analyses, you usually look for signals such as: What are the\ncharacteristics of the text instances? How are the training labels distributed? Can you\ncome up with signals that are correlated with the training labels? Can you come up\nwith some simple rules that can predict the training labels with reasonable accuracy?\nShould we even use ML? This list goes on and on. This analysis phase includes aspects\nof data science, where various statistical techniques may come in handy. \n You run experiments to try a number of prototypes quickly. The goal in this phase\nis to narrow down the possible set of approaches to a couple of promising ones, before\nyou go all-in and start training a gigantic model. By running experiments, you wish to\nanswer questions including: What types of NLP tasks and approaches are appropriate\nfor this NLP application? Is this a classification, parsing, sequence labeling, regression,\ntext generation, or some other problem? What is the performance of the baseline\napproach? What is the performance of the rule-based approach? Should we even use\nML? What is the estimate of training and serving time for the promising approaches?\n I call these first two phases the “research” phase. The existence of this phase is\narguably the biggest difference between NLP applications and other generic software\nsystems. Due to its nature, it is difficult to predict the performance and the behavior\nof a machine learning system, or an NLP system, for that matter. At this point, you\nmight not have written a single line of production code, and that’s totally fine. The\npoint of this research phase is to prevent you from wasting your effort writing produc-\ntion code that turns out to be useless at a later stage. \n\n\n23\nBuilding NLP applications\nTRAINING\nAt this point you have pretty clear ideas what the approaches will be for your NLP\napplication. This is when you start adding more data and computational resources\n(e.g., GPUs) for training your model. It is not uncommon for modern NLP models to\ntake days if not weeks to train, especially if they are based on neural network models.\nIt is always a good practice to gradually ramp up the amount of the data and the size\nof the model you train. You don’t want to spend weeks training a gigantic neural net-\nwork model only to find that a smaller and simpler model performs just as well, or\neven worse, that you introduced a bug in the model and that the model you spent\nweeks training is simply useless!\n It is critical at this phase that you keep your training pipeline reproducible.\nChances are, you will need to run this several times with different sets of hyperparam-\neters, which are tuning values set before starting the model’s learning process. It is\nalso likely that you will need to run this pipeline several months later, if not years. I’ll\ntouch upon some best practices when training NLP/ML models in chapter 10.\nIMPLEMENTATION\nWhen you have a model that is working with acceptable performance, you move on to\nthe implementation phase. This is when you start making your application “produc-\ntion ready.” This process basically follows software engineering best practices, includ-\ning: writing unit and integration tests for your NLP modules, refactoring your code,\nhaving your code reviewed by other developers, improving the performance of your\nNLP modules, and dockerizing your application. I’ll talk more about this process in\nchapter 11.\nDEPLOYING\nYour NLP application is finally ready to deploy. You can deploy your NLP application\nin many ways—it can be an online service, a recurring batch job, an offline applica-\ntion, or an offline one-off task. If this is an online service that needs to serve its predic-\ntions in real time, it is a good idea to make this a microservice to make it loosely coupled\nwith other services. In any case, it is a good practice to use continuous integration\n(CI) for your application, where you run tests and verify that your code and model are\nworking as intended every time you make changes to your application.\nMONITORING\nAn important final step for developing NLP applications is monitoring. This not only\nincludes monitoring the infrastructure such as server CPU, memory, and request\nlatency, but also higher-level ML statistics such as the distributions of the input and the\npredicted labels. Some of the important questions to ask at this stage are: What do the\ninput instances look like? Are they what you expected when you built your model?\nWhat do the predicted labels look like? Does the predicted label distribution match\nthe one in the training data? The purpose of the monitoring is to check that the model\nyou built is behaving as intended. If the incoming text or data instances or the pre-\ndicted labels do not match your expectation, you may have an out-of-domain problem,\nmeaning that the domain of the natural language data you are receiving is different\n\n\n24\nCHAPTER 1\nIntroduction to natural language processing\nfrom the one in which your model is trained. Machine learning models are usually not\ngood at dealing with out-of-domain data, and the prediction accuracy may suffer. If this\nissue becomes obvious, it may be a good idea to repeat the whole process again, start-\ning from collecting more in-domain data.\n1.3.2\nStructure of NLP applications\nThe structures of modern, machine learning–based NLP applications are becoming\nsurprisingly similar for two main reasons—one is that most modern NLP applications\nrely on machine learning to some degree, and they should follow best practices for\nmachine learning applications. The other is that, due to the advent of neural network\nmodels, a number of NLP tasks, including text classification, machine translation, dia-\nlog systems, and speech recognition, can now be trained end-to-end, as I mentioned\nbefore. Some of these tasks used to be hairy, enormous monsters with dozens of com-\nponents with complex plumbing. Now, however, some of these tasks can be solved by\nless than 1,000 lines of Python code, provided that there’s enough data to train the\nmodel end-to-end.\n Figure 1.10 illustrates the typical structure of a modern NLP application. There are\ntwo main infrastructures: the training and the serving infrastructure. The training\ninfrastructure is usually offline and serves the purpose of training the machine learning\nmodel necessary for the application. It takes the training data, converts it to some data\nstructure that can be handled by the pipeline, and further processes it by transforming\nthe data and extracting the features. This part varies greatly from task to task. Finally,\nTraining data \nTraining infrastructure\nDataset\nreader\nTransformer\nTrainer\nOptimizer\nModel\nBatching\nServing infrastructure\nDataset\nreader\nTransformer\nPredictor \nModel\nPrediction \nNew instance\nFigure 1.10\nStructure of typical NLP applications\n\n\n25\nSummary\nif the model is a neural network, data instances are batched and fed to the model, which\nis optimized to minimize the loss. Don’t worry if you don’t understand what I’m talking\nabout in that last sentence—we’ll talk about those technical terms used with neural net-\nworks in chapter 2. The trained model is usually serialized and stored to be passed to\nthe serving infrastructure.\n The serving infrastructure’s job is to, given a new instance, produce the prediction,\nsuch as classes, tags, or translations. The first part of this infrastructure, which reads the\ninstance and transforms it into some numbers, is similar to the one for training. In fact,\nyou must keep the dataset reader and the transformer identical. Otherwise, discrepan-\ncies will arise in the way those two process the data, also known as training-serving skew.\nAfter the instance is processed, it’s fed to the pretrained model to produce the predic-\ntion. I’ll talk more about designing your NLP applications in chapter 11.\nSummary\nNatural language processing (NLP) is a subfield of artificial intelligence (AI)\nthat refers to computational approaches to process, understand, and generate\nhuman language.\nOne of the challenges for NLP is ambiguity in natural languages. There is syn-\ntactic and semantic ambiguity.\nWhere there is text, there is NLP. Many tech companies use NLP to draw infor-\nmation from a large amount of text. Typical NLP applications include machine\ntranslation, grammatical error correction, search engines, and dialog systems.\nNLP applications are developed in an iterative way, with more emphasis on the\nresearch phase.\nMany modern NLP applications rely heavily on machine learning (ML) and are\nstructurally similar to ML systems.\n",
      "page_number": 34
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 46-53)",
      "start_page": 46,
      "end_page": 53,
      "detection_method": "topic_boundary",
      "content": "26\nYour first NLP application\nIn section 1.1.2, we saw how not to do NLP. In this chapter, we are going to discuss\nhow to do NLP in a more principled, modern way. Specifically, we’d like to build a\nsentiment analyzer using a neural network. Even though the sentiment analyzer we\nare going to build is a simple application and the library (AllenNLP) takes care of\nmost heavy lifting, it is a full-fledged NLP application that covers a lot of basic com-\nponents of modern NLP and machine learning. I’ll introduce important terms and\nconcepts along the way. Don’t worry if you don’t understand some concepts at first.\nWe will revisit most of the concepts introduced here in later chapters.\nThis chapter covers\nBuilding a sentiment analyzer using AllenNLP\nApplying basic machine learning concepts \n(datasets, classification, and regression)\nEmploying neural network concepts (word \nembeddings, recurrent neural networks, linear \nlayers)\nTraining the model through reducing loss\nEvaluating and deploying your model\n\n\n27\nIntroducing sentiment analysis\n2.1\nIntroducing sentiment analysis\nIn the scenario described in section 1.1.2, you wanted to extract users’ subjective opin-\nions from online survey results. You have a collection of textual data in response to a\nfree-response question, but you are missing the answers to the “How do you like our\nproduct?” question, which you’d like to recover from the text. This task is called senti-\nment analysis, which is a text analytic technique used in the automatic identification\nand categorization of subjective information within text. The technique is widely used\nin quantifying opinions, emotions, and so on that are written in an unstructured way\nand, thus, hard to quantify otherwise. Sentiment analysis is applied to a wide variety of\ntextual resources such as survey, reviews, and social media posts.\n In machine learning, classification means categorizing something into a set of pre-\ndefined, discrete categories. One of the most basic tasks in sentiment analysis is the\nclassification of polarity, that is, to classify whether the expressed opinion is positive,\nnegative, or neutral. You could use more than three classes, for example, strongly pos-\nitive, positive, neutral, negative, or strongly negative. This may sound familiar to you if\nyou have used a website (such as Amazon) where people can review things using a five-\npoint scale expressed by the number of stars.\n Classification of polarity is one type of sentence classification task. Another type of\nsentence classification task is spam filtering, where each sentence is categorized into\ntwo classes—spam or not spam. It’s called binary classification if there are only two\nclasses. If there are more than two classes (the five-star classification system mentioned\nearlier, for example), it’s called multiclass classification. \n In contrast, when the prediction is a continuous value instead of discrete catego-\nries, it’s called regression. If you’d like to predict the price of a house based on its prop-\nerties, such as its neighborhood, numbers of bedrooms and bathrooms, and square\nfootage, it’s a regression problem. If you attempt to predict stock prices based on the\ninformation collected from news articles and social media posts, it’s also a regression\nproblem. (Disclaimer: I’m not suggesting this is an appropriate approach to stock\nprice prediction. I’m not even sure if it works.) As I mentioned earlier, most linguistic\nunits such as characters, words, and part-of-speech tags are discrete. For this reason,\nmost uses of machine learning in NLP are classification, not regression. \nNOTE \nLogistic regression, a widely used statistical model, is usually used for clas-\nsification, even though it has “regression” in its name. Yes, I know it’s confusing!\nMany modern NLP applications, including the sentiment analyzer we are going to\nbuild in this chapter (shown in figure 2.1), are built based on the supervised machine\nlearning paradigm. Supervised machine learning is one type of machine learning\nwhere the algorithm is trained with data that has supervision signals—the desired out-\ncome for individual input. The algorithm is trained in such a way that it reproduces\nthe signals as closely as possible. For sentiment analysis, this means that the system is\ntrained on data that contains the desired labels for each input sentence.\n \n\n\n28\nCHAPTER 2\nYour first NLP application\nFigure 2.1\nSentiment analysis pipeline\n2.2\nWorking with NLP datasets\nAs we discussed in the previous section, many modern NLP applications are devel-\noped using supervised machine learning, where algorithms are trained from data\nannotated with desired outcomes, instead of using handwritten rules. Almost by defi-\nnition, data is a critical part for machine learning, and it is important to understand\nhow it is structured and used with machine learning algorithms. \n2.2.1\nWhat is a dataset?\nA dataset simply means a collection of data. If you are familiar with relational data-\nbases, you can think of a dataset as a dump of one table. It consists of pieces of data\nthat follow the same format. In database terms, each piece of the data corresponds to\na record, or a row in a table. A record can have any number of fields, which corre-\nspond to columns in a database.\n In NLP, records in a dataset are usually some type of linguistic units, such as words,\nsentences, or documents. A dataset of natural language texts is called a corpus (plural:\ncorpora). As an example, let’s think of a (hypothetical) dataset for spam filtering. Each\nrecord in this dataset is a pair of a piece of text and a label, where the text is a sen-\ntence or a paragraph (e.g., from an email) and the label specifies whether the text is\nspam. Both the text and the label are the fields of a record.\n Some NLP datasets and corpora have more complex structures. For example, a\ndataset may contain a collection of sentences, where each sentence is annotated with\ndetailed linguistic information, such as part-of-speech tags, parse trees, dependency\nstructures, and semantic roles. If a dataset contains a collection of sentences anno-\ntated with their parse trees, the dataset is called a treebank. The most famous example\nof this is Penn Treebank (PTB) (http://realworldnlpbook.com/ch2.html#ptb), which\nhas been serving as the de facto standard dataset for training and evaluating NLP tasks\nsuch as part-of-speech (POS) tagging and parsing.\n A closely related term to a record is an instance. In machine learning, an instance is\na basic unit for which the prediction is made. For example, in the spam-filtering task\nmentioned earlier, an instance is one piece of text, because predictions (spam or not\nLinear\nlayer \nStrongly \npositive \nStrongly \nnegative \nPositive\nNegative \nNeutral\nRecurrent neural network (RNN) \nThis\nis\nthe\never\nWord\nembeddings\n!\n\n\n29\nWorking with NLP datasets\nspam) are made for individual texts. An instance is usually created from a record in a\ndataset, as is the case for the spam-filtering task, but this is not always the case—for\nexample, if you take a treebank and use it to train an NLP task that detects all nouns\nin a sentence, then each word, not a sentence, becomes an instance, because predic-\ntion (noun or not noun) is made for each word. Finally, a label is a piece of informa-\ntion attached to some linguistic unit in a dataset. A spam-filtering dataset has labels\nthat correspond to whether each text is a spam. A treebank may have one label per\nword for its part of speech. Labels are usually used as training signals (i.e., answers for\nthe training algorithm) in a supervised machine learning setting. See figure 2.2 for a\ndepiction of these parts of a dataset.\nFigure 2.2\nDatasets, records, fields, instances, and labels\n2.2.2\nStanford Sentiment Treebank\nTo build a sentiment analyzer, we are going to use the Stanford Sentiment Treebank\n(SST; https://nlp.stanford.edu/sentiment/), one of the most widely used sentiment\nanalysis datasets as of today. Go ahead and download the dataset from the Train,\nDev, Test Splits in PTB Tree Format link. One feature that differentiates SST from\nother datasets is the fact that sentiment labels are assigned not only to sentences but\nalso to every word and phrase in sentences. For example, some excerpts from the\ndataset follow:\n(4\n  (2 (2 Steven) (2 Spielberg))\n    (4\n      (2 (2 brings) (3 us))\n      (4 (2 another) (4 masterpiece))))\nField\nField\nField\nField\nRecord\nField\nField\nField\nField\nRecord\nDataset\nDataset\nLabel\nInstance\nLabel\nInstance\nLabel\nInstance\nField\nField\nField\nField\nRecord\n\n\n30\nCHAPTER 2\nYour first NLP application\n(1\n  (2 It)\n  (1\n    (1 (2 (2 's) (1 not))\n      (4 (2 a) (4 (4 great) (2 (2 monster) (2 movie)))))\n    (2 .)))\nDon’t worry about the details for now—these trees are written in S-expressions that\nare painfully hard to read for humans (unless you are a Lisp programmer). Notice the\nfollowing:\nEach sentence is annotated with sentiment labels (4 and 1).\nEach word is also annotated, for example, (4 masterpiece) and (1 not).\nEvery single phrase is also annotated, for example, (4 (2 another) \n(4 masterpiece)).\nThis property of the dataset enables us to study the complex semantic interactions\nbetween words and phrases. For example, let’s consider the polarity of the following\nsentence as a whole:\n The movie was actually neither that funny, nor super witty.\nThe above statement would definitely be a negative, although, if you focus on the indi-\nvidual words (such as funny, witty), you might be fooled into thinking it’s a positive. If\nyou built a simple classifier that takes “votes” from individual words (e.g., the sentence\nis positive if a majority of its words are positive), such classifiers would have difficulties\nclassifying this example correctly. To correctly classify the polarity of this sentence, you\nneed to understand the semantic impact of the negation “neither . . . nor.” For this\nproperty, SST has been used as the standard benchmark for neural network models\nthat can capture the syntactic structures of sentences (http://realworldnlpbook.com/\nch2.html#socher13). However, in this chapter, we are going to ignore all the labels\nassigned to internal phrases and use only labels for sentences.\n2.2.3\nTrain, validation, and test sets\nBefore we move on to show how to use SST datasets and start building our own senti-\nment analyzer, I’d like to touch upon some important concepts in machine learning.\nIn NLP and ML, it is common to use a couple of different types of datasets to develop\nand evaluate models. A widely used best practice is to use three different types of data-\nset splits—train, validation, and test sets.\n A train (or training) set is the main dataset used to train the NLP/ML models.\nInstances from the train set are usually fed to the ML training pipeline directly and\nused to learn parameters of the model. Train sets are usually the biggest among the\nthree types of splits discussed here.\n A validation set (also called a dev or development set) is used for model selection. Model\nselection is a process where appropriate NLP/ML models are selected among all pos-\nsible models that can be trained using the train set, and here’s why it’s necessary. Let’s\nthink of a situation where you have two machine learning algorithms, A and B, with\n\n\n31\nWorking with NLP datasets\nwhich you want to train an NLP model. You use both algorithms and obtain models A\nand B. Now, how can you know which model is better?\n “That’s easy,” you might say. “Evaluate them both on the train set.” At first glance,\nthis may sound like a good idea. You run both models A and B on the train set and see\nhow they perform in terms of metrics such as accuracy. Why do people bother to use a\nseparate validation set for selecting models?\n The answer is overfitting—another important concept in NLP and ML. Overfitting is\na situation where a trained model fits the train set so well that it loses its generalizability.\nLet’s think of an extreme case to illustrate the point here. Assume algorithm B is a very,\nvery powerful one that remembers everything as-is. Think of it as a big associative array\n(or dict in Python) that can store all the pairs of instances and labels it has ever\nencountered. For the spam-filtering task, this means that the model stores the exact\ntexts and their labels as they are presented when the model is being trained. If the exact\nsame text is presented when the model is evaluated, it just returns what’s stored as its\nlabel. On the other hand, if the presented text is even slightly different from any other\ntexts it has in memory, the model has no clue, because it’s never seen it before.\n How do you think this model would perform if it was evaluated on the train set? The\nanswer is . . . yes, 100%! Because the model remembers all the instances from the train\nset, it can just “replay” the entire dataset and classify it perfectly. Now, would this algo-\nrithm make a good spam filter if you installed it on your email software? Absolutely not!\nBecause countless spam emails look very similar to existing ones but are slightly differ-\nent, or completely new, the model has no clue if the input email is even one character\ndifferent from what’s stored in the memory, and the model would be useless when\ndeployed in production. In other words, it has poor (in fact, zero) generalizability.\n How could you prevent choosing such a model? By using a validation set! A valida-\ntion set consists of separate instances that are collected in a similar way to the train set.\nBecause they are independent from the train set, if you run your trained model on the\nvalidation set, you’ll get a good idea how the model would perform outside the train\nset. In other words, the validation set gives a proxy for the model’s generalizability.\nImagine if the model trained by the earlier “remember all” algorithm was evaluated\non a validation set. Because the instances in the validation set are similar to but inde-\npendent from the ones in the train set, you’d get very low accuracy and know the\nmodel would perform poorly, even before deploying it.\n The validation set is also used for tuning hyperparameters. A hyperparameter is a\nparameter about a machine learning algorithm or about a model that is being trained.\nFor example, if you repeat the training loop (also called an epoch—see later for more\nexplanation) for N times, this N is a hyperparameter. If you increase the number of lay-\ners of the neural network, you just changed one hyperparameter about the model.\nMachine learning algorithms and models usually have a number of hyperparameters,\nand it is crucial to tune them for them to perform optimally. You can do this by training\nmultiple models with different hyperparameters and evaluating them on a validation\nset. In fact, you can think of models with different hyperparameters as separate models,\neven if they have the same structure, and hyperparameter tuning can be considered\none type of model selection.\n\n\n32\nCHAPTER 2\nYour first NLP application\n Finally, a test set is used to evaluate the model using a new, unseen set of data. It\nconsists of instances that are independent from the train and validation sets. It gives\nyou a good idea how the model would perform “in the wild.” \n You might wonder why yet another separate dataset is necessary for evaluating the\nmodel’s generalizability. Can’t you just use the validation set for this? Again, you\nshouldn’t rely solely on a train set and a validation set to measure the generalizability\nof your model, because your model could also overfit to the validation set in a subtle\nway. This point is less intuitive, but let me give you an example. Imagine you are fran-\ntically experimenting with a ton of different spam-filtering models. You wrote a script\nthat automatically trains a spam-filtering model. The script also automatically evalu-\nates the trained models on the validation set. If you run this script 1,000 times with dif-\nferent combinations of algorithms and hyperparameters and pick one model with the\nhighest validation set performance, would it also perform the best on the completely\nnew, unseen instances? Probably not. If you try a large number of models, some of\nthem happen to perform relatively well on the validation set purely by chance\n(because the predictions inherently have some noise, and/or because those models\nhappen to have some characteristics that make them perform better on the validation\nset), but this is no guarantee that those models perform well outside of the validation\nset. In other words, it could be possible to overfit the model to the validation set.\n In summary, when training NLP models, use a train set to train your model candi-\ndates, use a validation set to choose good ones, and use a test set to evaluate them.\nMany public datasets used for NLP and ML evaluation are already split into train/\nvalidation/test sets. If you just have a single dataset, you can split it into those three\ndatasets by yourself. An 80:10:10 split is commonly used. Figure 2.3 depicts the train/\nvalidation/test split as well as the entire training pipeline.\nTrain (80%) \nValidation  \n(10%)\nTest  \n(10%)\nDataset\nTraining\nalgorithm 1\nTraining\n algorithm 2\nTraining\n algorithm N\nModel\n selection\nEvaluation\nModel 1\nModel 2\nModel N\nMetrics\nBest \nmodel\nFigure 2.3\nTrain/validation/test \nsplit and the training pipeline\n\n\n33\nWorking with NLP datasets\n2.2.4\nLoading SST datasets using AllenNLP\nFinally, let’s see how we can actually load datasets in code. In the remainder of this\nchapter, we assume that you have already installed AllenNLP (version 2.5.0) and the\ncorresponding version of the allennlp-models package by running the following:\npip install allennlp==2.5.0\npip install allennlp-models==2.5.0\nand imported necessary classes and modules as shown here:\nfrom itertools import chain\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder, \nPytorchSeq2VecWrapper\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, \nBasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training import GradientDescentTrainer\nfrom allennlp.training.metrics import CategoricalAccuracy, F1Measure\nfrom allennlp_models.classification.dataset_readers.stanford_sentiment_\ntree_bank import \\\n    StanfordSentimentTreeBankDatasetReader\nUnfortunately, as of this writing, AllenNLP does not officially support Windows. But\ndon’t worry—all the code in this chapter (and all the code in this book, for that mat-\nter) is available as Google Colab notebooks (http://www.realworldnlpbook.com/\nch2.html#sst-nb), where you can run and modify the code and see the results. \n You also need to define the following two constants used in the code snippets:\nEMBEDDING_DIM = 128\nHIDDEN_DIM = 128\nAllenNLP already supports an abstraction called DatasetReader, which takes care of\nreading a dataset from the original format (be it raw text or some exotic XML-based\nformat) and returns it as a collection of instances. We are going to use Stanford-\nSentimentTreeBankDatasetReader(), which is a type of DatasetReader that\nspecifically deals with SST datasets, as shown here:\nreader = StanfordSentimentTreeBankDatasetReader()\ntrain_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/train.txt'\ndev_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/dev.txt'\n",
      "page_number": 46
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 54-64)",
      "start_page": 54,
      "end_page": 64,
      "detection_method": "topic_boundary",
      "content": "34\nCHAPTER 2\nYour first NLP application\nThis snippet will create a dataset reader for SST datasets and define the paths for the\ntrain and dev text files.\n2.3\nUsing word embeddings\nFrom this section on, we’ll start building the neural network architecture for the senti-\nment analyzer. Architecture is just another word for the structure of neural networks.\nBuilding neural networks is a lot like building structures such as houses. The first step\nis to figure out how to feed the input (e.g., sentences for sentiment analysis) into the\nnetwork.\n As we have seen previously, everything in NLP is discrete, meaning there is no pre-\ndictable relationship between the forms and the meanings (remember “rat” and “sat”).\nOn the other hand, neural networks are best at dealing with something numerical and\ncontinuous, meaning everything in neural networks needs to be float numbers. How\ncan we “bridge” between these two worlds—discrete and continuous? The key is the use\nof word embeddings, which we are going to discuss in detail in this section.\n2.3.1\nWhat are word embeddings?\nWord embeddings are one of the most important concepts in modern NLP. Technically,\nan embedding is a continuous vector representation of something that is usually dis-\ncrete. A word embedding is a continuous vector representation of a word. If you are\nnot familiar with the concept of vectors, vector is a mathematical name for single-\ndimensional arrays of numbers. In simpler terms, word embeddings are a way to rep-\nresent each word with a 300-element array (or an array of any other size) filled with\nnonzero float numbers. It is conceptually very simple. Then, why has it been so\nimportant and prevalent in modern NLP?\n As I mentioned in chapter 1, the history of NLP is actually the history of continu-\nous battle against “discreteness” of language. In the eyes of computers, “cat” is no\ncloser to “dog” than it is to “pizza.” One way to deal with discrete words programmati-\ncally is to assign indices to individual words as follows (here we simply assume that\nthese indices are assigned alphabetically):\nindex(\"cat\") = 1\nindex(\"dog\") = 2\nindex(\"pizza\") = 3\n…\nThese assignments are usually managed by a lookup table. The entire, finite set of\nwords that one NLP application or task deals with is called vocabulary. But this method\nisn’t any better than dealing with raw words. Just because words are now represented\nby numbers doesn’t mean you can do arithmetic operations on them and conclude\nthat “cat” is equally similar to “dog” (difference between 1 and 2), as “dog” is to\n“pizza” (difference between 2 and 3). Those indices are still discrete and arbitrary.\n  “What if we can represent them on a numerical scale?” some NLP researchers\nwondered decades ago. Can we think of some sort of numerical scale where words are\n\n\n35\nUsing word embeddings\nrepresented as points, so that\nsemantically closer words (e.g.,\n“dog” and “cat,” which are both\nanimals) are also geometrically\ncloser? Conceptually, the numeri-\ncal scale would look like the one\nshown in figure 2.4.\n This is a step forward. Now we can represent the fact that “cat” and “dog” are more\nsimilar to each other than “pizza” is to those words. But still, “pizza” is slightly closer to\n“dog” than it is to “cat.” What if you wanted to place it somewhere that is equally far\nfrom “cat” and “dog?” Maybe only one dimension is too limiting. How about adding\nanother dimension to this, as shown in figure 2.5?\n Much better! Because computers are really good at dealing with multidimensional\nspaces (because you can just represent points by arrays), you can simply keep doing\nthis until you have a sufficient number of dimensions. Let’s have three dimensions. In\nthis 3-D space, you can represent those three words as follows:\nvec(\"cat\") = [0.7, 0.5, 0.1] \nvec(\"dog\") = [0.8, 0.3, 0.1]\nvec(\"pizza\") = [0.1, 0.2, 0.8]\nFigure 2.6 illustrates this three-dimensional space.\n The x-axis (the first element) here represents some concept of “animal-ness” and\nthe z-axis (the third dimension) corresponds to “food-ness.” (I’m making these num-\nbers up, but you get the point.) This is essentially what word embeddings are. You just\nembedded those words in a three-dimensional space. By using those vectors, you\nalready “know” how the basic building blocks of the language work. For example, if\nyou wanted to identify animal names, then you would just look at the first element of\n \nPizza\nDog\nx\nCat\nFigure 2.4\nWord embeddings in a 1-D space\nx\ny\nz\nCat\nPizza\nDog\nFigure 2.6\nWord embeddings in a \n3-D space\nx\ny\nCat\nPizza\nDog\nFigure 2.5\nWord embeddings in a \n2-D space\n\n\n36\nCHAPTER 2\nYour first NLP application\neach word vector and see if the value is high enough. This is a great jump start com-\npared to the raw word indices!\n You may be wondering where those numbers come from in practice. These num-\nbers are actually “learned” using some machine learning algorithms and a large text\ndataset. We’ll discuss this further in chapter 3.\n By the way, we have a much simpler method to “embed” words into a multidimen-\nsional space. Think of a multidimensional space that has as many dimensions as there\nare words. Then, give to each word a vector that is filled with zeros but just one 1, as\nshown next:\nvec(\"cat\") = [1, 0, 0] \nvec(\"dog\") = [0, 1, 0]\nvec(\"pizza\") = [0, 0, 1]\nNotice that each vector has only one 1 at the position corresponding to the word’s\nindex. These special vectors are called one-hot vectors. These vectors are not very useful\nthemselves in representing semantic relationship between those words—the three\nwords are all at the equal distance from each other—but they are still (a very dumb\nkind of) embeddings. They are often used as the input to a machine learning algo-\nrithm when embeddings are not available.\n2.3.2\nUsing word embeddings for sentiment analysis\nFirst, we create dataset loaders that take care of loading data and passing it to the\ntraining pipeline, as shown next (more discussion on this data later in this chapter):\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(reader, train_path,\n                                           batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(reader, dev_path,\n                                           batch_sampler=sampler)\nAllenNLP provides a useful Vocabulary class that manages mappings from some lin-\nguistic units (such as characters, words, and labels) to their IDs. You can tell the class\nto create a Vocabulary instance from a set of instances as follows:\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(),\n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\nThen, you need to initialize an Embedding instance, which takes care of converting\nIDs to embeddings, as shown in the next code snippet. The size (dimension) of the\nembeddings is determined by EMBEDDING_DIM:\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_DIM)\nFinally, you need to specify which index names correspond to which embeddings and\npass it to BasicTextFieldEmbedder as follows:\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n\n\n37\nNeural networks\nNow you can use word_embeddings to convert words (or more precisely, tokens,\nwhich I’ll talk more about in chapter 3) to their embeddings. \n2.4\nNeural networks\nAn increasingly large number of modern NLP applications are built using neural net-\nworks. You may have seen many amazing things that modern neural network models\ncan achieve in the domain of computer vision and game playing (such as self-driving\ncars and Go-playing algorithms defeating human champions), and NLP is no excep-\ntion. We are going to use neural networks for most of the NLP examples and applica-\ntions we are going to build in this book. In this section, we discuss what neural\nnetworks are and why they are powerful.\n2.4.1\nWhat are neural networks?\nNeural networks are at the core of modern NLP (and many other related AI fields,\nsuch as computer vision). It is such an important, vast research topic that it’d take a\nbook (or maybe several books) to fully explain what it is and all the related models,\nalgorithms, and so on. In this section, I’ll briefly explain the gist of it and will go into\nmore details in later chapters as needed.\n In short, a neural network (also called an artificial neural network) is a generic mathe-\nmatical model that transforms a vector to another vector. That’s it. Contrary to what\nyou may have read and heard in popular media, its essence is simple. If you are famil-\niar with programming terms, think of it as a function that takes a vector, does some\ncomputation inside, and produces another vector as the return value. Then why is it\nsuch as big deal? How is it different from normal functions in programming?\n The first difference is that neural networks are trainable. Think of it not just as a\nfixed function but more as a “template” for a set of related functions. If you use a pro-\ngramming language and write a function that includes several mathematical equa-\ntions with some constants, you always get the same result if you feed the same input.\nOn the contrary, neural networks can receive “feedback” (how close the output is to\nyour desired output) and adjust their internal constants. Those “magic” constants are\ncalled weights or, more generally, parameters. Next time you run it, you expect that its\nanswer is closer to what you want. \n The second difference is its mathematical power. It’d be overly complicated if you\nwere to use your favorite programming language and write a function that does, for\nexample, sentiment analysis, if at all possible. (Remember the poor software engineer\nfrom chapter 1?) In theory, given enough model power and training data, neural net-\nworks are known to be able to approximate any continuous functions. This means\nthat, whatever your problem is, neural networks can solve it if there’s a relationship\nbetween the input and the output and if you provide the model with enough compu-\ntational power and training data.\n Neural networks achieve this by learning functions that are not linear. What does it\nmean for a function to be linear? A linear function is a function where, if you change\nthe input by x, the output will always change by c * x, where c is a constant number.\n\n\n38\nCHAPTER 2\nYour first NLP application\nFor example, 2.0 * x is linear, because the return value always increases by 2.0 if you\nchange x by 1.0. If you plot this on a graph, the relationship between the input and\nthe output forms a straight line, which is why it’s called linear. On the other hand,\n2.0 * x * x is not linear, because how much the return value changes depends not\nonly on how much you change x but also on the value of x.\n What this means is that a linear function cannot capture a more complex relation-\nship between the input and the output and between the input variables. On the con-\ntrary, natural phenomena such as language are highly nonlinear. If you change the\ninput by x (e.g., a word in a sentence), how much the output changes depends not\nonly on how much x is changed but also on many other factors such as the value of x\nitself (e.g., what word you changed x to) and what other variables (e.g., the context\nof x) are. Neural networks, which are nonlinear mathematic models, have the poten-\ntial to capture such complex interactions.\n2.4.2\nRecurrent neural networks (RNNs) and linear layers\nTwo special types of neural network components are important for sentiment\nanalysis—recurrent neural networks (RNNs) and linear layers. I’ll explain them in\ndetail in later chapters, but I’ll briefly describe what they are and their roles in senti-\nment analysis (or in general, sentence classification).\n A recurrent neural network (RNN) is a neural network with loops, as shown in figure\n2.7. It has an internal structure that is applied to the input again and again. Using\nthe programming analogy, it’s like writing a function that contains for word in\nsentence: that loops over each word in the input sentence. It can either output the\ninterim values of the internal variables of the loop, or the final values of the variables\nafter the loop is finished, or both. If you just take the final values, you can use an RNN\nas a function that transforms a sentence to a vector with a fixed length. In many NLP\ntasks, you can use an RNN to transform a sentence to an embedding of the sentence.\nRemember word embeddings? They were fixed-length representation of words. Simi-\nlarly, RNNs can produce fixed-length representation of sentences.\nFigure 2.7\nRecurrent neural network (RNN)\nRecurrent neural network (RNN) \nSentence \nembedding\nThis\nis\nthe\never\n!\n...\n\n\n39\nNeural networks\n Another type of neural network component we’ll be using here is linear layers. A\nlinear layer, also called a fully connected layer, transforms a vector to another vector in a\nlinear fashion. As mentioned earlier, layer is just a fancier term for a substructure of\nneural networks, because you can stack them on top of each other to form a larger\nstructure.\n Remember, neural networks can learn nonlinear relationships between the input\nand the output. Why would we want something that is more constrained (linear) at\nall? Linear layers are used for compressing (or less often, expanding) vectors by\nreducing (or increasing) the dimensionality. For example, assume you receive a 64-\ndimensional vector (an array of 64 float numbers) from an RNN as a sentence embed-\nding, but all you care about is a smaller number of values that are essential for your\nprediction. In sentiment analysis, you may care about only five values that correspond\nto five different sentiment labels, namely, strongly positive, positive, neutral, negative,\nand strongly negative. But you have no idea how to extract those five values from the\nembedded 64 values. This is exactly where a linear layer comes in handy—you can add\na layer that transforms a 64-dimensional vector to a 5-dimensional one, and the neural\nnetworks figure out how to do that well, as shown in figure 2.8.\nFigure 2.8\nLinear layer\n2.4.3\nArchitecture for sentiment analysis\nNow you are ready to put the components together to build the neural network for\nthe sentiment analyzer. First, you need to create the RNN as follows: \nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nDon’t worry too much about PytorchSeq2VecWrapper and batch_first=True.\nHere, you are creating an RNN (or more specifically, one type of RNN called LSTM,\nwhich stands for long short-term memory). The size of the input vector is EMBEDDING_DIM,\nwhich we saw earlier, and that of the output vector is HIDDEN_DIM. \nLinear\nlayer \nStrongly positive \nStrongly negative \nPositive\nNegative \nNeutral\nSentence \nembedding\n\n\n40\nCHAPTER 2\nYour first NLP application\n Next, you need to create a linear layer, as shown here:\nself.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n                              out_features=vocab.get_vocab_size('labels'))\nThe size of the input vector is defined by in_features, whereas out_features is\nthat of the output vector. Because we are transforming the sentence embedding to a\nvector whose elements correspond to five sentiment labels, we need to specify the size\nof the encoder output and obtain the total number of labels from vocab.\n Finally, we can connect those components and build a model as shown in the fol-\nlowing code.\nclass LstmClassifier(Model):\n    def __init__(self,\n                 word_embeddings: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n        super().__init__(vocab)\n        self.word_embeddings = word_embeddings\n        self.encoder = encoder\n        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n                                      \nout_features=vocab.get_vocab_size('labels'))\n        self.loss_function = torch.nn.CrossEntropyLoss()   \n    def forward(self,    \n                tokens: Dict[str, torch.Tensor],\n                label: torch.Tensor = None) -> torch.Tensor:\n        mask = get_text_field_mask(tokens)\n        embeddings = self.word_embeddings(tokens)\n        encoder_out = self.encoder(embeddings, mask)\n        logits = self.linear(encoder_out)\n        output = {\"logits\": logits}\n        if label is not None:\n            self.accuracy(logits, label)\n            self.f1_measure(logits, label)\n            output[\"loss\"] = self.loss_function(logits, label)  \n        return output\nI want you to focus on the forward() function which is the most important function\nthat every neural network model has. Its role is to take the input, pass it through sub-\ncomponents of the neural network, and produce the output. Although the function\nhas some unfamiliar logics that we haven’t covered yet (such as mask and loss),\nListing 2.1\nBuilding a sentiment analyzer model\nDefines the \nloss function \n(cross entropy)\nThe forward() function \nis where most of the \ncomputation happens \nin a model.\nComputes the loss and\nassigns it to the “loss” key\nof the returned dict\n\n\n41\nLoss functions and optimization\nwhat’s important here is the fact that you can chain the subcomponents of the model\n(word embeddings, RNN, and the linear layer) as if they were functions that trans-\nform the input (tokens), and you get something called logits at the end of the pipe-\nline. Logit is a term in statistics that has a specific meaning, but here, you can think of\nit as something like a score for a class. The higher the score is for a specific label, the\nmore confident that the label is the correct one.\n2.5\nLoss functions and optimization\nNeural networks are trained using supervised learning. As mentioned earlier, super-\nvised learning is a type of machine learning that learns a function that maps inputs to\noutputs based on a large amount of labeled data. So far, I covered only about how\nneural networks take an input and produce an output. How can we make it so that\nneural networks produce the output that we actually want?\n Neural networks are not just like regular functions that you usually write in pro-\ngramming languages. They are trainable, meaning that they can receive some feedback\nand change their internal parameters so that they can produce more accurate out-\nputs, even for the same inputs next time around. Notice there are two parts to this—\nreceiving feedback and adjusting parameters, which are done through loss functions\nand optimization, respectively, which I’ll explain next.\n A loss function is a function that measures how far an output of a machine learning\nmodel is from a desired one. The difference between an actual output and a desired\none is called the loss. Loss is also called cost in some contexts. Either way, the bigger\nthe loss, the worse it is, and you want it as close to zero as possible. Take sentiment\nanalysis, for example. If the model thinks a sentence is 100% negative, but the train-\ning data says it’s strongly positive, the loss will be big. On the other hand, if the model\nthinks a sentence is maybe 80% negative and the training label is indeed negative, the\nloss will be small. It will be zero if both match exactly.\n PyTorch provides a wide range of functions to compute losses. What we need here\nis called cross-entropy loss, which is often used for classification problems, as shown\nhere:\nself.loss_function = torch.nn.CrossEntropyLoss()\nIt can be used later by passing a prediction and labels from the training set as follows:\noutput[\"loss\"] = self.loss_function(logits, label)\nThen, this is where the magic happens. Neural networks, thanks to their mathematical\nproperties, know how to change their internal parameters to make the loss smaller.\nUpon receiving some large loss, the neural network goes, “Oops, sorry, that was my\nmistake, but I’ll do better next round!” and changes its parameters. Remember I\ntalked about a function that you write in a programming language that has some\nmagic constants in it? Neural networks act like that function but know exactly how to\nchange the magic constants to reduce the loss. They do this for each and every\n\n\n42\nCHAPTER 2\nYour first NLP application\ninstance in the training data, so that they can produce more correct answers for as\nmany instances as possible. Of course, they can’t reach the perfect answer after adjust-\ning the parameters only once. It requires multiple passes, called epochs, over the train-\ning data. Figure 2.9 shows the overall training procedure for neural networks.\nFigure 2.9\nOverall training procedure for neural networks\nThe process where a neural network computes an output from an input using the cur-\nrent set of parameters is called the forward pass. This is why the main function in listing\n2.1 is called forward(). The way the loss is fed back to the neural network is called\nbackpropagation. An algorithm called stochastic gradient descent (SGD) is often used to\nminimize the loss. The process where the loss is minimized is called optimization, and\nthe algorithm (such as SGD) used to achieve this is called the optimizer. You can initial-\nize an optimizer using PyTorch as follows:\noptimizer = optim.Adam(model.parameters())\nHere, we are using one type of optimizer called Adam. There are many types of opti-\nmizers proposed in the neural network community, but the consensus is that there is\nLoss\nLoss function\nBackpropagation\nOptimizer\nForward pass\nModel\nUpdated\nmodel\nModel\nPrediction\nLabel\nInstance\nLabel\nInstance\n× len(train set)\n=\n1 epoch\n\n\n43\nTraining your own classifier\nno single optimization algorithm that works well for any problem, and you should be\nready to experiment with multiple ones for your own problem. \n OK, that was a lot of technical terms. You don’t need to know the details of those\nalgorithms for now, but it’d be helpful if you learn just the terms and what they\nroughly mean. If you write the entire training process in Python pseudocode, it will\nappear as shown in listing 2.2. Note that there are two nested loops, one over epochs\nand another over instances.\nMAX_EPOCHS = 100\nmodel = Model()\nfor epoch in range(MAX_EPOCHS):\n    for instance, label in train_set:\n        prediction = model.forward(instance)\n        loss = loss_function(prediction, label)\n        new_model = optimizer(model, loss)\n        model = new_model\n2.6\nTraining your own classifier\nIn this section, we are going to train our own classifier using AllenNLP’s training\nframework. I’ll also touch upon the concept of batching, an important practical con-\ncept that is used in training neural network models.\n2.6.1\nBatching\nSo far, I have left out one piece of detail—batching. We assumed that an optimization\nstep happens for each and every instance, as you saw in the earlier pseudocode. In\npractice, however, we usually group a number of instances together and feed them to\na neural network, updating model parameters per each group, not per each instance.\nWe call this group of instances a batch.\n Batching is a good idea for a couple of reasons. The first is stability. Any data is\ninherently noisy. Your dataset may contain sampling and labeling errors. If you update\nyour model parameters for every instance, and if some instances contain errors, the\nupdate is influenced too much by the noise. But if you group instances into batches\nand compute the loss for the entire batch, not for individual instances, you can “aver-\nage out” small errors and the feedback to your model stabilizes. \n The second reason is speed. Training neural networks involves a huge number of\narithmetic operations such as matrix additions and multiplications, and it is often\ndone on GPUs (graphics processing units). Because GPUs are designed so that they\ncan process a huge number of arithmetic operations in parallel, it is often efficient if\nyou pass a large amount of data and process it at once instead of passing instances one\nby one. Think of a GPU as a factory overseas that manufactures products based on\nyour specifications. Because factories are often optimized for manufacturing a small\nvariety of products at a large quantity, and there is overhead in communicating and\nListing 2.2\nPseudocode for the neural network training loop\n\n\n44\nCHAPTER 2\nYour first NLP application\nshipping products, it is more efficient if you make a small number of orders for manu-\nfacturing a large quantity of products instead of making a large number of orders for\nmanufacturing a small quantity of products, even if you want the same quantity of\nproducts in total in either way.\n It is easy to group instances into batches using AllenNLP. The framework uses\nPyTorch’s DataLoader abstraction, which takes care of receiving instances and\nreturning batches. We’ll use a BucketBatchSampler that groups instances into\nbuckets of similar lengths, as shown in the next code snippet. I’ll discuss why it’s\nimportant in later chapters:\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(reader, train_path, \nbatch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(reader, dev_path, \nbatch_sampler=sampler)\nThe parameter batch_size specifies the size of the batch (the number of instances\nin a batch). There is often a “sweet spot” in adjusting this parameter. It should be\nlarge enough to have any effect of the batching I mentioned earlier, but also small\nenough so that batches fit in the GPU memory, because factories have the maximum\ncapacity of products they can manufacture at once. \n2.6.2\nPutting everything together\nNow you are ready to train your sentiment analyzer. We assume that you already\ndefined and initialized your model as follows:\nmodel = LstmClassifier(word_embeddings, encoder, vocab)\nSee the full code listing (http://www.realworldnlpbook.com/ch2.html#sst-nb) for\nwhat the model looks like and how to use it.\n AllenNLP provides the Trainer class, which acts as a framework for putting all the\ncomponents together and managing the training pipeline, as shown here:\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\ntrainer.train()\nYou provide the model, optimizer, iterator, train set, dev set, and the number of\nepochs you want to the trainer and invoke the train method. The last parameter,\ncuda_device, tells the trainer which device (CPU or GPU) to use to use for training.\nHere, we are explicitly using the CPU. This will run the neural network training loop\ndescribed in listing 2.2 and display the progress, including the evaluation metrics. \n",
      "page_number": 54
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 65-72)",
      "start_page": 65,
      "end_page": 72,
      "detection_method": "topic_boundary",
      "content": "45\nEvaluating your classifier\n2.7\nEvaluating your classifier\nWhen training an NLP/ML model, you should always monitor how the loss changes\nover time. If the training is working as expected, you should see the loss decrease over\ntime. It doesn’t always decrease each epoch, but it should decrease as a general trend,\nbecause this is exactly what you told the optimizer to do. If it’s increasing or showing\nweird values (such as NaN), it’s usually a sign that your model is too limiting or there’s\na bug in your code.\n In addition to the loss, it is important to monitor other evaluation metrics you care\nabout in your task. Loss is a purely mathematical concept that measures the closeness\nbetween your model and the answer, but smaller losses do not always guarantee better\nperformance in the NLP task. \n You can use a number of evaluation metrics, depending on the nature of your NLP\ntask, but some that you need to know no matter what task you are working on include\naccuracy, precision, recall, and F-measure. Roughly speaking, these metrics measure\nhow precisely your model’s predictions match the expected answers defined by the\ndataset. For now, it suffices to know that they are used to measure how good your clas-\nsifier is (more details coming in chapter 4). \n To monitor and report evaluation metrics during training using AllenNLP, you\nneed to implement the get_metrics() method in your model class, which returns a\ndict from metric names to their values, as shown next.\n   def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {'accuracy': self.accuracy.get_metric(reset),\n                **self.f1_measure.get_metric(reset)}\nself.accuracy and self.f1_measure are defined in __init__() as follows:\n    self.accuracy = CategoricalAccuracy()\n    self.f1_measure = F1Measure(positive_index)\nWhen you run trainer.train() with the metrics defined, you’ll see progress bars\nlike these after every epoch:\naccuracy: 0.7268, precision: 0.8206, recall: 0.8703, f1: 0.8448, batch_loss: \n0.7609, loss: 0.7194 ||: 100%|##########| 267/267 [00:13<00:00, 19.28it/s]\naccuracy: 0.3460, precision: 0.3476, recall: 0.3939, f1: 0.3693, batch_loss: \n1.5834, loss: 1.9942 ||: 100%|##########| 35/35 [00:00<00:00, 119.53it/s]\nYou can see that the training framework reports these metrics both for the train and the\nvalidation sets. This is useful not only for evaluating your model but also for monitoring\nthe progress of the training. If you see any unusual values, such as extremely low or high\nnumbers, you’ll know that something is wrong, even before the training completes.\n You may have noticed a large gap between the train and the validation metrics.\nSpecifically, the metrics for the train set are a lot higher than those for the validation\nListing 2.3\n Defining evaluation metrics\n\n\n46\nCHAPTER 2\nYour first NLP application\nset. This is a common symptom of overfitting, which I mentioned earlier, where a\nmodel fits to a train set so well that it loses generalizability outside of it. This is why it’s\nimportant to monitor the metrics using a validation set as well, because you won’t\nknow if it’s just doing well or overfitting only by looking at the training set metrics!\n2.8\nDeploying your application\nThe final step in making your own NLP application is deploying it. Training your\nmodel is only half the story. You need to set it up so that it can make predictions for\nnew instances it has never seen. Making sure the model is serving predictions is criti-\ncal in real-world NLP applications, and a lot of development efforts may go into this\nstage. In this section, I’m going to show what it’s like to deploy the model we just\ntrained using AllenNLP. This topic is discussed in more detail in chapter 11.\n2.8.1\nMaking predictions\nTo make predictions for new instances your model has never seen (called test\ninstances), you need to pass them through the same neural network pipeline as you\ndid for training. It has to be exactly the same—otherwise, you’ll risk skewing the\nresult. This is called training-serving skew, which I’ll explain in chapter 11.\n AllenNLP provides a convenient abstraction called predictors, whose job it is to\nreceive an input in its raw form (e.g., raw string), pass it through the preprocessing\nand neural network pipeline, and give back the result. I wrote a specific predictor for\nSST called SentenceClassifierPredictor (http://realworldnlpbook.com/ch2\n.html#predictor), which you can call as follows:\npredictor = SentenceClassifierPredictor(model, dataset_reader=reader)\nlogits = predictor.predict('This is the best movie ever!')['logits']\nNote that the predictor returns the raw output from the model, which is logits in this\ncase. Remember, logits are some sort of scores corresponding to target labels, so if you\nwant the predicted label itself, you need to convert it to the label. You don’t need to\nunderstand all the details for now, but this can be done by first taking the argmax of\nthe logits, which returns the index of the logit with the maximum value, and then by\nlooking up the label by the ID, as follows:\nlabel_id = np.argmax(logits)\nprint(model.vocab.get_token_from_index(label_id, 'labels'))\nIf this prints out a “4,” congratulations! Label “4” corresponds to “very positive,” so\nyour sentiment analyzer just predicted that the sentence “This is the best movie ever!”\nis very positive, which is indeed correct.\n2.8.2\nServing predictions\nFinally, you can easily deploy the trained model using AllenNLP. If you use a JSON\nconfiguration file (which I’ll explain in chapter 4), you can save your trained model\nonto disk and then quickly fire up a web-based interface where you can make requests\n\n\n47\nDeploying your application\nto your model. To do this, you need to install allennlp-server, a plugin for Allen-\nNLP that provides a web interface for prediction, as follows:\ngit clone https:/./github.com/allenai/allennlp-server\npip install --editable allennlp-server\nAssuming your model is saved under examples/sentiment/model, you can run a\nPython-based web application using the following AllenNLP command:\n$ allennlp serve \\ \n    --archive-path examples/sentiment/model/model.tar.gz \\\n    --include-package examples.sentiment.sst_classifier \\\n    --predictor sentence_classifier_predictor \\\n    --field-name sentence\nIf you open http:/./localhost:8000/ using your browser, you’ll see the interface shown\nin figure 2.10.\nFigure 2.10\nRunning the sentiment analyzer on a web browser\nTry typing some sentences in the sentence text box, and click Predict. You should see\nthe logits values on the right side of the screen. They are just a raw array of logits and\nhard to read, but you can see that the fourth value (which corresponds to the label\n“very positive”) is the largest and the model is working as expected.\n You can also directly make POST requests to the backend from the command line\nas follows:\ncurl -d '{\"sentence\": \"This is the best movie ever!\"}'\n    -H \"Content-Type: application/json\" \\\n    -X POST http:/./localhost:8000/predict\n\n\n48\nCHAPTER 2\nYour first NLP application\nThis should return the same JSON as you saw above:\n{\"logits\":[-0.2549717128276825,-0.35388273000717163,\n-0.0826418399810791,0.7183976173400879,0.23161858320236206]}\nOK, that’s it for now. We covered a lot in this chapter, but don’t worry—I just wanted\nto show you that it is easy to build an NLP application that actually works. You may\nhave found some books or online tutorials about neural networks and deep learning\nintimidating, or you may have even given up on learning before creating anything\nthat works. Notice that I didn’t even mention any concepts such as neurons, activa-\ntions, gradient, and partial derivatives, which other learning materials teach at the\nvery beginning. These concepts are indeed important and helpful to know, but thanks\nto powerful frameworks such as AllenNLP, you are also able to build practical NLP\napplications without fully understanding their details. In later chapters, I’ll go into\nmore details and discuss these concepts as needed.\nSummary\nSentiment analysis is a text analytic technique to automatically identify subjec-\ntive information within text, such as its polarity (positive or negative).\nTrain, dev, and test sets are used to train, choose, and evaluate machine learn-\ning models.\nWord embeddings represent the meaning of words using vectors of real\nnumbers.\nRecurrent neural networks (RNNs) and linear layers are used to convert a vec-\ntor to another vector of different size.\nNeural networks are trained using an optimizer so that the loss (discrepancy\nbetween the actual and the desired output) is minimized.\nIt is important to monitor the metrics for the train and the dev sets during\ntraining to avoid overfitting.\n\n\n49\nWord and document\nembeddings\nIn chapter 2, I pointed out that neural networks can deal only with numbers,\nwhereas almost everything in natural language is discrete (i.e., separate concepts).\nTo use neural networks in your NLP application, you need to convert linguistic\nunits to numbers, such as vectors. For example, if you wish to build a sentiment\nanalyzer, you need to convert the input sentence (sequence of words) into a\nsequence of vectors. In this chapter, we’ll discuss word embeddings, which are the\nkey to achieving this bridging. We’ll also touch upon a couple of fundamental\nThis chapter covers\nWhat word embeddings are and why they \nare important\nHow the Skip-gram model learns word \nembeddings and how to implement it\nWhat GloVe embeddings are and how to use \npretrained vectors\nHow to use Doc2Vec and fastText to train more \nadvanced embeddings\nHow to visualize word embeddings\n\n\n50\nCHAPTER 3\nWord and document embeddings\nlinguistic components that are important in understanding embeddings and neural\nnetworks in general.\n3.1\nIntroducing embeddings\nAs we discussed in chapter 2, an embedding is a real-valued vector representation of\nsomething that is usually discrete. In this section, we’ll revisit what embeddings are\nand discuss in detail what roles they play in NLP applications.\n3.1.1\nWhat are embeddings?\nA word embedding is a real-valued vector representation of a word. If you find the\nconcept of vectors intimidating, think of them as single-dimensional arrays of float\nnumbers, like the following:\nvec(\"cat\") = [0.7, 0.5, 0.1] \nvec(\"dog\") = [0.8, 0.3, 0.1]\nvec(\"pizza\") = [0.1, 0.2, 0.8]\nBecause each array contains three elements, you can plot them as points in a 3-D\nspace as in figure 3.1. Notice that semantically-related words (“cat” and “dog”) are\nplaced close to each other.\nNOTE\nIn fact, you can embed (i.e., represent by a list of numbers) not just\nwords but also almost anything—characters, sequences of characters, sen-\ntences, or categories. You can embed any categorical variables using the same\nmethod, although in this chapter, we’ll focus on two of the most important\nconcepts in NLP—words and sentences.\n3.1.2\nWhy are embeddings important?\nWhy are embeddings important? Well, word embeddings are not just important but\nessential for using neural networks to solve NLP tasks. Neural networks are pure math-\nematical computation models that can deal only with numbers. They can’t do symbolic\noperations, such as concatenating two strings or conjugating a verb to past tense, unless\nx\ny\nz\nCat\nPizza\nDog\nFigure 3.1\nWord embeddings on a 3-D space\n\n\n51\nIntroducing embeddings\nthese items are all represented by numbers and arithmetic operations. On the other\nhand, almost everything in NLP, such as words and labels, is symbolic and discrete. This\nis why you need to bridge these two worlds, and using embeddings is a way to do it. See\nfigure 3.2 for an overview on how to use word embeddings for an NLP application.\nWord embeddings, just like any other neural network models, can be trained, because\nthey are simply a collection of parameters (or “magic constants,” which we talked\nabout in the previous chapter). Embeddings are used with your NLP model in the fol-\nlowing three scenarios:\nScenario 1: Train word embeddings and your model at the same time using the\ntrain set for your task.\nScenario 2: First, train word embeddings independently using a larger text data-\nset. Alternatively, obtain pretrained word embeddings from somewhere else.\nThen initialize your model using the pretrained word embeddings, and train\nthem and your model at the same time using the train set for your task.\nScenario 3: Same as scenario 2, except you fix word embeddings while you train\nyour model.\nIn the first scenario, word embeddings are initialized randomly and trained in con-\njunction with your NLP model using the same dataset. This is basically how we built the\nsentiment analyzer in chapter 2. Using an analogy, this is like having a dance teacher\nteach a baby to walk and dance at the same time. It is not an entirely impossible feat (in\nfact, some babies might end up being better, maybe more creative dancers by skipping\nthe walking part, but don’t try this at home), but rarely a good idea. Babies would\nNLP\nmodel\n \nPrediction\nTraining data\nLarge text data\nWord \nembeddings\nWords\ndog\nchocolate\nbark\ncat\nFigure 3.2\nUsing word \nembeddings with NLP models\n\n\n52\nCHAPTER 3\nWord and document embeddings\nprobably have a much better chance if they are taught how to stand and walk properly\nfirst, and then how to dance.\n Similarly, it’s not uncommon to train an NLP model and word embeddings as its\nsubcomponent at the same time. But many large-scale, high-performance NLP mod-\nels usually rely on external word embeddings that are pretrained using larger datasets\n(scenarios 2 and 3). Word embeddings can be learned from unlabeled large text\ndatasets—that is, a large amount of plain text data (e.g., Wikipedia dumps), which are\nusually more readily available than the train datasets for your task (e.g., the Stanford\nSentiment Treebank). By leveraging such large textual data, you can teach your model\na lot about how natural language works even before it sees a single instance from the\ndataset for your task. Training a machine learning model on one task and repurposing\nit for another task is called transfer learning, which is becoming increasingly popular in\nmany machine learning domains, NLP included. We’ll further discuss transfer learn-\ning in chapter 9.\n Using the dancing baby analogy again, most healthy babies figure out how to stand\nand walk themselves. They may get some help from adults, usually from their close\ncaregivers such as parents. This form of “help,” however, is usually a lot more abun-\ndant and cheaper than the “training signal” you get from a hired dance teacher, which\nis why it’s a lot more effective if they learn how to walk first, then move on to dancing.\nMany skills used for walking transfer to dancing.\n The difference between scenarios 2 and 3 is whether the word embeddings are\nadjusted, or fine-tuned, while your NLP model is trained. Whether or not this is effec-\ntive may depend on your task and the dataset. Teaching your toddler ballet may have a\ngood effect on how they walk (e.g., by improving their posture), which in turn could\nhave a positive effect on how they dance, but scenario 3 doesn’t allow this to happen.\n The only remaining question you might have is: Where do embeddings come\nfrom? I mentioned earlier that they can be trained from a large amount of plain text.\nThis chapter explains how this is possible and what models are used to achieve this.\n3.2\nBuilding blocks of language: Characters, words, and phrases\nBefore I explain word-embedding models, I’m going to touch upon some basic\nconcepts of language, such as characters, words, and phrases. It helps to understand\nthese concepts when you design the structure of your NLP application. Figure 3.3\nshows some examples of those concepts.\n3.2.1\nCharacters\nA character (also called a grapheme in linguistics) is the smallest unit of a writing system.\nIn written English, “a,” “b,” and “z” are characters. Characters do not necessarily carry\nmeaning by themselves or represent any fixed sound when spoken, although in some\nlanguages (e.g., Chinese), most do. A typical character in many languages can be rep-\nresented by a single Unicode codepoint (by string literals such as \"\\uXXXX\" in Python),\nbut this is not always the case. Many languages use a combination of more than one Uni-\ncode codepoint (e.g., accent marks) to represent a single character. Punctuation\nmarks, such as “.” (period), “,” (comma), and “?” (question mark), are also characters.\n",
      "page_number": 65
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 73-80)",
      "start_page": 73,
      "end_page": 80,
      "detection_method": "topic_boundary",
      "content": "53\nBuilding blocks of language: Characters, words, and phrases\n3.2.2\nWords, tokens, morphemes, and phrases\nA word is the smallest unit in a language that can be uttered independently and that\nusually carries some meaning. In English, “apple,” “banana,” and “zebra” are words.\nIn most written languages that use alphabetic scripts, words are usually separated by\nspaces or punctuation marks. In some languages, like Chinese, Japanese, and Thai,\nhowever, words are not explicitly delimited by spaces and require a preprocessing step\ncalled word segmentation to identify words in a sentence.\n A closely related concept to a word in NLP is a token. A token is a string of contigu-\nous characters that play a certain role in a written language. Most words (“apple,”\n“banana,” “zebra”) are also tokens when written. Punctuation marks such as the excla-\nmation mark (“!”) are tokens but not words, because you can’t utter them in isolation.\nWord and token are often used interchangeably in NLP. In fact, when you see “word”\nin NLP text (including this book), it often means “token,” because most NLP tasks\ndeal only with written text that is processed in an automatic way. Tokens are the out-\nput of a process called tokenization, which I’ll explain more below.\n Another closely related concept is morpheme. A morpheme is the smallest unit of\nmeaning in a language. A typical word consists of one or more morphemes. For exam-\nple, “apple” is a word and also a morpheme. “Apples” is a word comprised of two mor-\nphemes, “apple” and “-s,” which is used to signify the noun is plural. English contains\nmany other morphemes, including “-ing,” “-ly,” “-ness,” and “un-.” The process for iden-\ntifying morphemes in a word or a sentence is called morphological analysis, and it has a\nwide range of NLP/linguistics applications, but this is outside the scope of this book.\n A phrase is a group of words that play a certain grammatical role. For example, “the\nquick brown fox” is a noun phrase (a group of words that behaves like a noun),\nwhereas “jumps over the lazy dog” is a verb phrase. The concept of phrase may be\nused somewhat liberally in NLP to simply mean any group of words. For example, in\nmany NLP literatures and tasks, words like “Los Angeles” are treated as phrases,\nalthough, linguistically speaking, they are closer to a word.\n3.2.3\nN-grams\nFinally, you may encounter the concept of n-grams in NLP. An n-gram is a contiguous\nsequence of one or more occurrences of linguistic units, such as characters and words.\nFor example, a word n-gram is a contiguous sequence of words, such as “the” (one\nCharacters\nWords\nMorphemes\nTokens\nPhrases\nWord n-grams\nThe quick brown fox jumps over the lazy dog.\nA\nbrown\nThe quick brown fox\nthe lazy dog\nThe fox the .\nbrown\njump\n-s\ndog\nover\nb\nq\n.\nquick brown\nbrown fox jumps\nFigure 3.3\nBuilding blocks \nof language used in NLP \n\n\n54\nCHAPTER 3\nWord and document embeddings\nword), “quick brown” (two words), “brown fox jumps” (three words). Similarly, a char-\nacter n-gram is composed of characters, such as “b” (one character), “br” (two charac-\nters), “row” (three characters), and so on, which are all character n-grams made from\n“brown.” An n-gram of size 1 (when n = 1) is called a unigram. N-grams of size 2 and 3\nare called a bigram and a trigram, respectively.\n Word n-grams are often used as proxies for phrases in NLP, because if you enumer-\nate all the n-grams of a sentence, they often contain linguistically interesting units that\ncorrespond to phrases such as “Los Angeles” and “take off.” In a similar vein, we use\ncharacter n-grams when we want to capture subword units that roughly correspond to\nmorphemes. In NLP, when you see “n-grams” (without a qualifier), they are often word\nn-grams.\nNOTE\nInterestingly, in search and information retrieval, n-grams often mean\ncharacter n-grams used for indexing documents. Be mindful which type of\nn-grams are implied by the context when you read papers.\n3.3\nTokenization, stemming, and lemmatization\nWe covered some basic linguistic units often encountered in NLP. In this section, I\nintroduce some steps where linguistic units are processed in a typical NLP pipeline. \n3.3.1\nTokenization\nTokenization is a process where the input text is split into smaller units. There are two\ntypes of tokenization: word and sentence tokenization. Word tokenization splits a sen-\ntence into tokens (rough equivalent to words and punctuation), which I mentioned\nearlier. Sentence tokenization, on the other hand, splits a piece of text that may include\nmore than one sentence into individual sentences. If you say tokenization, it usually\nmeans word tokenization in NLP.\n Many NLP libraries and frameworks support tokenization out of the box, because\nit is one of the most fundamental and widely used preprocessing steps in NLP. In what\nfollows, I show you how to do tokenization using two popular NLP libraries—NLTK\n(https://www.nltk.org/) and spaCy (https://spacy.io/).\nNOTE\nBefore running the example code in this section, make sure the libraries\nare both installed. In a typical Python environment, they can be installed by run-\nning pip install nltk and pip install spacy. After installation, you need\nto download the necessary data and models by running python -c \"import\nnltk; nltk.download('punkt')\" for NLTK, and python -m spacy down-\nload en for spaCy from the command line. You can also run all the examples in\nthis section via Google Colab (http://realworldnlpbook.com/ch3.html#tokeni\nzation) without installing any Python environments or dependencies.\nTo use the default word and sentence tokenizers from NLTK, you can import them\nfrom nltk.tokenize package as follows:\n>>> import nltk\n>>> from nltk.tokenize import word_tokenize, sent_tokenize\n\n\n55\nTokenization, stemming, and lemmatization\nYou can call these methods with a string, and they return a list of words or sentences as\nfollows:\n>>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of \nthem.\\n\\nThanks.'''\n>>> word_tokenize(s)\n['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please',\n 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n>>> sent_tokenize(s)\n['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.',\n 'Thanks.']\nNLTK implements a wide range of tokenizers in addition to the default one we used\nhere. Its documentation page (https://www.nltk.org/api/nltk.tokenize.html) is a\ngood starting point if you are interested in exploring more options.\n You can tokenize words and sentences as follows using spaCy:\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_sm')\n>>> doc = nlp(s)\n>>> [token.text for token in doc]\n['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'in', 'New', 'York', '.', ' ',\n 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.']\n>>> [sent.string.strip() for sent in doc.sents]\n['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.', 'Thanks.']\nNote that the results from NLTK and spaCy are slightly different. For example, spaCy’s\nword tokenizer leaves newlines ('\\n') intact. The behavior of tokenizers differs from\none implementation to another, and there is no single standard solution that every\nNLP practitioner agrees upon. Although standard libraries such as NLTK and spaCy\ngive a good baseline, be ready to experiment depending on your task and data. Also, if\nyou are dealing with languages other than English, your options may vary (and might\nbe quite limited depending on the language). If you are familiar with the Java ecosys-\ntem, Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/) is another good\nNLP framework worth checking out. \n Finally, an increasingly popular and important tokenization method for neural\nnetwork-based NLP models is byte-pair encoding (BPE). Byte-pair encoding is a purely\nstatistical technique to split text into sequences of characters in any language, relying\nnot on heuristic rules (such as spaces and punctuations) but only on character statis-\ntics from the dataset. We’ll study byte-pair encoding more in depth in chapter 10.\n3.3.2\nStemming\nStemming is a process for identifying word stems. A word stem is the main part of a word\nafter stripping off its affixes (prefixes and suffixes). For example, the word stem of\n“apples” (plural) is “apple.” The stem of “meets” (with a third-person singular s) is\n\n\n56\nCHAPTER 3\nWord and document embeddings\n“meet.” The word stem of “unbelievable” is “believe.” It is often a part that remains\nunchanged after inflection.\n Stemming—that is, normalizing words to something closer to their original\nforms—has great benefits in many NLP applications. In search, for example, you can\nimprove the chance of retrieving relevant documents if you index documents using\nword stems instead of words. In many feature-based NLP pipelines, you’d be able to\nalleviate the OOV (out-of-vocabulary) problem by dealing with word stems instead.\nFor example, even if your dictionary doesn’t have an entry for “apples,” you can\ninstead use its stem “apple” as a proxy. \n The most popular algorithm used for stemming English words is called the Porter\nstemming algorithm, originally written by Martin Porter. It consists of a number of rules\nfor rewriting affixes (e.g., if a word ends with “-ization,” change it to “-ize”). NLTK\nimplements a version of the algorithm as the PorterStemmer class, which can be\nused as follows:\n>>> from nltk.stem.porter import PorterStemmer\n>>> stemmer = PorterStemmer()\n>>> words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n...          'died', 'agreed', 'owned', 'humbled', 'sized',\n...          'meetings', 'stating', 'siezing', 'itemization',\n...          'sensational', 'traditional', 'reference', 'colonizer',\n...          'plotted']\n>>> [stemmer.stem(word) for word in words]\n['caress', 'fli', 'die', 'mule', 'deni',\n 'die', 'agre', 'own', 'humbl', 'size',\n 'meet', 'state', 'siez', 'item',\n 'sensat', 'tradit', 'refer', 'colon',\n 'plot']\nStemming is not without its limitations. In many cases, it can be too aggressive. For\nexample, as you can see from the previous example, the Porter stemming algorithm\nchanges both “colonizer” and “colonize” to just “colon.” I can’t imagine many applica-\ntions would be happy to treat those three words as an identical entry. Also, many stem-\nming algorithms do not consider the context or even the parts of speech. In the\nprevious example, “meetings” is changed to “meet,” but you could argue that “meet-\nings” as a plural noun should be stemmed to “meeting,” not “meet.” For those rea-\nsons, as of today, few NLP applications use stemming.\n3.3.3\nLemmatization\nA lemma is the original form of a word that you often find as a head word in a dictio-\nnary. It is also the base form of the word before inflection. For example, the lemma of\n“meetings” (as a plural noun) is “meeting.” The lemma of “met” (a verb past form) is\n“meet.” Notice that it differs from stemming, which simply strips off affixes from a\nword and cannot deal with such irregular verbs and nouns.\n It is straightforward to run lemmatization using NLTK, as shown here:\n>>> from nltk.stem import WordNetLemmatizer\n>>> lemmatizer = WordNetLemmatizer()\n\n\n57\nSkip-gram and continuous bag of words (CBOW)\n>>> [lemmatizer.lemmatize(word) for word in words]\n['caress', 'fly', 'dy', 'mule', 'denied',\n 'died', 'agreed', 'owned', 'humbled', 'sized',\n 'meeting', 'stating', 'siezing', 'itemization',\n 'sensational', 'traditional', 'reference', 'colonizer',\n 'plotted']\nAnd the spaCy code looks like this:\n>>> doc = nlp(' '.join(words))\n>>> [token.lemma_ for token in doc]\n['caress', 'fly', 'die', 'mule', 'deny',\n 'die', 'agree', 'own', 'humble', 'sized',\n 'meeting', 'state', 'siezing', 'itemization',\n 'sensational', 'traditional', 'reference', 'colonizer',\n 'plot']\nNote that lemmatization inherently requires that you know the part of speech of the\ninput word, because the lemma depends on it. For example, “meeting” as a noun\nshould be lemmatized to “meeting,” whereas the result should be “meet” if it’s a verb.\nWordNetLemmatizer in NLTK treats everything as a noun by default, which is why\nyou see many unlemmatized words in the result (“agreed,” “owned,” etc.). On the\nother hand, spaCy automatically infers parts of speech from the word form and the\ncontext, which is why most of the lemmatized words are correct in its result. Lemmati-\nzation is more resource-intensive than stemming because it requires statistical analysis\nof the input and/or some form of linguistic resources such as dictionaries, but it has a\nwider range of applications in NLP due to its linguistic correctness.\n3.4\nSkip-gram and continuous bag of words (CBOW)\nIn previous sections, I explained what word embeddings are and how they are used in\nNLP applications. In this section, we’ll start exploring how to calculate word embed-\ndings from large textual data using two popular algorithms—Skip-gram and CBOW.\n3.4.1\nWhere word embeddings come from\nIn section 3.1, I explained that word embeddings represent each word in the vocabu-\nlary using a single-dimensional array of float numbers:\nvec(\"cat\") = [0.7, 0.5, 0.1] \nvec(\"dog\") = [0.8, 0.3, 0.1]\nvec(\"pizza\") = [0.1, 0.2, 0.8]\nNow, there’s one important piece of information missing from the discussion so far.\nWhere do those numbers come from? Do we hire a group of experts and have them\ncome up with those numbers? It would be virtually impossible to assign them by hand.\nHundreds of thousands of unique words exist in a typical large corpus, and the arrays\nshould be at least around 100-dimensional long to be effective, which means you need\nto tweak more than tens of millions of numbers.\n More importantly, what should those numbers look like? How do you determine\nwhether you should assign a 0.8 to the first element of the “dog” vector, or 0.7, or any\nother number?\n\n\n58\nCHAPTER 3\nWord and document embeddings\n The answer is that those numbers are also trained using a training dataset and a\nmachine learning model like any other model in this book. In what follows, I’ll intro-\nduce and implement one of the most popular models to train word embeddings—the\nSkip-gram model.\n3.4.2\nUsing word associations\nFirst, let’s step back and think how humans learn concepts such as “a dog.” I don’t\nthink any of you have ever been explicitly taught what a dog is. You knew this thing\ncalled “dog” since you were a toddler without anyone else telling you, “Oh by the way,\nthere’s this thing called ‘dog’ in this world. It’s a four-legged animal that barks.” How\nis this possible? You acquire the concept through a large amount of physical (touch-\ning and smelling dogs), cognitive (seeing and hearing dogs), and linguistic (reading\nand hearing about dogs) interactions with the external world.\n Now let’s think about what it takes to teach the concept of “dog” to a computer.\nCan we get a computer to “experience” interactions with the external world related to\nthe concept of a dog? Although typical computers cannot move around and have\ninteractions with actual dogs (well, not yet, as of this writing), one possible way to do\nthis without teaching the computer what “dog” means is to use association relative to\nother words. For example, what words tend to appear together with the word “dog” if\nyou look at its appearances in a large text corpus? “Pet,” “tail,” “smell,” “bark,”\n“puppy”—there can be countless options. How about “cat”? Maybe “pet,” “tail,” “fur,”\n“meow,” “kitten,” and so on. Because “dog” and “cat” have a lot in common conceptu-\nally (they are both popular pet animals with a tail, etc.), these two sets of context\nwords also have large overlap. In other words, you can guess how close two words are\nto each other by looking at what other words appear in the same context. This is\ncalled the distributional hypothesis, and it has a long history in NLP.\nNOTE\nThere’s a related term used in artificial\nintelligence—distributed representations. Distributed\nrepresentations of words are simply another name\nfor word embeddings. Yes, it’s confusing, but both\nterms are commonly used in NLP.\nWe are now one step closer. If two words have a lot of\ncontext words in common, we can give similar vectors\nto those two words. You can think of a word vector as a\n“compressed” representation of its context words. Then\nthe question becomes: how can you “decompress” a\nword vector to obtain its context words? How can you\neven represent a set of context words mathematically?\nConceptually, we’d like to come up with a model that\ndoes something like the one in figure 3.4.\n One way to represent a set of words mathematically\nis to assign a score to each word in the vocabulary.\ndog\nDecompressor\nWord \nembeddings\nContext words\nbark\npet\nsmell\npuppy\nFigure 3.4\nDecompressing a \nword vector\n\n\n59\nSkip-gram and continuous bag of words (CBOW)\nInstead of representing context words as a set, we can think of them as an associative\narray (dict in Python) from words to their “scores” that correspond to how related\neach word is to “dog,” as shown next:\n{\"bark\": 1.4,\n \"chocolate\": 0.1,\n ...,\n \"pet\": 1.2,\n ...,\n \"smell\": 0.6,\n ...}\nThe only remaining piece of the model is how to come up with those “scores.” If you\nsort this list by word IDs (which may be assigned alphabetically), the scores can be\nconveniently represented by an N-dimensional vector, where N is the size of the entire\nvocabulary (the number of unique context words we consider), as follows:\n[1.4, 0.1, ..., 1.2, ..., 0.6, ...]\nAll the “decompressor” needs to do is expand the word-embedding vector (which has\nthree dimensions) to another vector of N dimensions.\n This may sound very familiar to some of you—yes, it’s exactly what linear layers\n(aka fully connected layers) do. I briefly talked about linear layers in section 2.4.2, but\nthis is a perfect time to go deeper into what they really do.\n3.4.3\nLinear layers\nLinear layers transform a vector into another vector in a linear fashion, but how\nexactly do they do this? Before talking about vectors, let’s simplify and start with num-\nbers. How would you write a function (say, a method in Python) that transforms a\nnumber into another one in a linear fashion? Remember, being linear means the out-\nput always changes by a fixed amount (say, w) if you change the input by 1, no matter\nwhat the value of the input is. For example, 2.0 * x is a linear function, because the\nvalue always increases by 2.0 if you increase x by 1, no matter what value x is. You can\nwrite a general version of such a function as follows:\ndef linear(x):\n    return w * x + b\nLet’s now assume the parameters w and b are fixed and defined somewhere else. You\ncan confirm that the output (the return value) always changes by w if you increase or\ndecrease x by 1. b is the value of the output when x = 0. It is called a bias in machine\nlearning.\n Now, what if there are two input variables, say, x1 and x2? Can you still write a\nfunction that transforms two input variables into another number in a linear way? Yes,\nand there’s very little change required to do this, as shown next:\ndef linear2(x1, x2):\n    return w1 * x1 + w2 * x2 + b\n\n\n60\nCHAPTER 3\nWord and document embeddings\nYou can confirm its linearity by checking that the output changes by w1 if you\nchange x1 by 1, and the output changes by w2 if you change x2 by 1, regardless of\nthe value of the other variable. Bias b is still the value of the output when x1 and x2\nare both 0.\n For example, assume we have w1 = 2.0, w2 = -1.0, and b = 1. For an input (1, 1),\nthe function returns 2. If you increase x1 by 1 and give (2, 1) as the input, you’ll get\n4, which is w1 more than 2. If you increase x2 by 1 and give (1, 2) as the input, you’ll\nget 1, which is 1 less (or w2 more) than 2.\n At this point, we can start thinking about generalizing this to vectors. What if there\nare two output variables, say, y1 and y2? Can you still write a linear function with\nrespect to the two inputs? Yes, you can simply duplicate the linear transformation\ntwice, with different weights and biases, as follows:\ndef linear3(x1, x2):\n    y1 = w11 * x1 + w12 * x2 + b1\n    y2 = w21 * x1 + w22 * x2 + b2\n    return [y1, y2]\nOK, it’s getting a little bit complicated, but you effectively wrote a function for a linear\nlayer that transforms a two-dimensional vector into another two-dimensional vector! If\nyou increase the input dimension (the number of input variables), this method would\nget horizontally long (i.e., more additions per line), whereas if you increase the out-\nput dimension, this method would get vertically long (i.e., more lines). \n In practice, deep learning libraries and frameworks implement linear layers in a\nmore efficient, generic way, and often most of the computation happens on a GPU.\nHowever, knowing how linear layers—the most important, simplest form of neural\nnetworks—work conceptually should be essential for understanding more complex\nneural network models.\nNOTE\nIn AI literature, you may encounter the concept of perceptrons. A per-\nceptron is a linear layer with only one output variable, applied to a classifica-\ntion problem. If you stack multiple linear layers (= perceptrons), you get a\nmultilayer perceptron, which is basically another name for a feedforward neural\nnetwork with some specific structures.\nFinally, you may be wondering where the constants w and b you saw in this section\ncome from. These are exactly the “magic constants” that I talked about in section\n2.4.1. You adjust these constants so that the output of the linear layer (and the neural\nnetwork as a whole) gets closer to what you want through a process called optimization.\nThese magic constants are also called parameters of a machine learning model.\n Putting this all together, the structure we want for the Skip-gram model is shown in\nfigure 3.5. This network is very simple. It takes a word embedding as an input and\nexpands it via a linear layer to a set of scores, one for each context word. Hopefully\nthis is not as intimidating as many people think! \n",
      "page_number": 73
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 81-89)",
      "start_page": 81,
      "end_page": 89,
      "detection_method": "topic_boundary",
      "content": "61\nSkip-gram and continuous bag of words (CBOW)\n3.4.4\nSoftmax\nNow let’s talk about how to “train” the Skip-gram model and learn the word embed-\ndings we want. The key here is to turn this into a classification task, where the model\npredicts what words appear in the context. The “context” here simply means a window\nof a fixed size (e.g., 5 + 5 words on both sides) centered around the target word (e.g.,\n“dog”). See figure 3.6 for an illustration when the window size is 2. This is actually a\n“fake” task because we are not interested\nin the prediction of the model per se, but\nrather in the by-product (word embed-\ndings) produced while training the\nmodel. In machine learning and NLP, we\noften make up a fake task to train some-\nthing else as a by-product. \nNOTE\nThis setting of machine learning, where the training labels are auto-\nmatically created from a given dataset, can also be called self-supervised learn-\ning. Recent popular techniques, such as word embeddings and language\nmodeling, all use self-supervision. \nIt is relatively easy to make a neural network solve a classification task. You need to do\nthe following two things:\nModify the network so that it produces a probability distribution.\nUse cross entropy as the loss function (we’ll cover this in detail shortly).\nWord\nembeddings\nbark\nchocolate\npet\nsmell\n…\n…\nLinear\nlayer\n \nScores\ndog\nFigure 3.5\nSkip-gram model structure\nTarget word\nContext words\nContext words\nI heard a dog barking in the distance.\nFigure 3.6\nTarget word and context words \n(when window size = 2)\n\n\n62\nCHAPTER 3\nWord and document embeddings\n You can use something called softmax to do the first. Softmax is a function that\nturns a vector of K float numbers to a probability distribution, by first “squashing” the\nnumbers so that they fit a range between 0.0–1.0, and then normalizing them so that\nthe sum equals 1. If you are not familiar with the concept of probabilities, replace\nthem with confidence. A probability distribution is a set of confidence values that the\nnetwork places on individual predictions (in this case, context words). Softmax does\nall this while preserving the relative ordering of the input float numbers, so large\ninput numbers still have large probability values in the output distribution. Figure 3.7\nillustrates this conceptually.\nAnother component required to turn a neural network into a classifier is cross entropy.\nCross entropy is a loss function used to measure the distance between two probability\ndistributions. It returns zero if two distributions match exactly and higher values if the\ntwo diverge. For classification tasks, we use cross entropy to compare the following:\nPredicted probability distribution produced by the neural network (output of\nsoftmax) \nTarget probability distribution, where the probability of the correct class is 1.0\nand everything else is 0.0\nThe predictions made by the Skip-gram model get closer and closer to the actual con-\ntext words, and word embeddings are learned at the same time.\n3.4.5\nImplementing Skip-gram on AllenNLP\nIt is relatively straightforward to turn this model into working code using AllenNLP.\nNote that all the code listed in this section can be executed on the Google Colab note-\nbook (http://realworldnlpbook.com/ch3.html#word2vec-nb). First, you need to\nimplement a dataset reader that reads a plain text corpus and turns it into a set of\ninstances that can be consumed by the Skip-gram model. The details of the dataset\nreader are not critical to the discussion here, so I’m going to omit the full code listing.\nYou can clone the code repository of this book (https://github.com/mhagiwara/\nrealworldnlp) and import it as follows:\nfrom examples.embeddings.word2vec import SkipGramReader\nA\nB\nC\nD\n…\n…\nScores\nProbability \ndistribution\nSoftmax\n…\nA\nB\nC\nD\n…\nFigure 3.7\nSoftmax\n\n\n63\nSkip-gram and continuous bag of words (CBOW)\nAlternatively, if you are interested, you can see the full code from http://realworldnlp-\nbook.com/ch3.html#word2vec. You can use the reader as follows:\nreader = SkipGramReader()\ntext8 = reader.read('https:/./realworldnlpbook.s3.amazonaws.com/data/text8/\ntext8')\nAlso, be sure to import all the necessary modules and define some constants in this\nexample, as shown next:\nfrom collections import Counter\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import SimpleDataLoader\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training.trainer import GradientDescentTrainer\nfrom torch.nn import CosineSimilarity\nfrom torch.nn import functional\nEMBEDDING_DIM = 256\nBATCH_SIZE = 256\nWe are going to use the text8 (http://mattmahoney.net/dc/textdata) dataset in this\nexample. The dataset is an excerpt from Wikipedia and is often used for training toy\nword embedding and language models. You can iterate over the instances in the data-\nset. token_in is the input token to the model, and token_out is the output (the\ncontext word): \n>>> for inst in text8:\n>>>     print(inst)\n...\nInstance with fields:\n  token_in: LabelField with label: ideas in namespace: 'token_in'.'\n  token_out: LabelField with label: us in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: ideas in namespace: 'token_in'.'\n  token_out: LabelField with label: published in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: ideas in namespace: 'token_in'.'\n  token_out: LabelField with label: journal in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: in in namespace: 'token_in'.'\n  token_out: LabelField with label: nature in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: in in namespace: 'token_in'.'\n  token_out: LabelField with label: he in namespace: 'token_out'.'\n\n\n64\nCHAPTER 3\nWord and document embeddings\nInstance with fields:\n  token_in: LabelField with label: in in namespace: 'token_in'.'\n  token_out: LabelField with label: announced in namespace: 'token_out'.'\n...\nThen, you can build the vocabulary, as we did in chapter 2, as shown next:\nvocab = Vocabulary.from_instances(\n    text8, min_count={'token_in': 5, 'token_out': 5})\nNote that we are using min_count, which sets the lower bound on the number of\noccurrences for each token. Also, let’s define the data loader we use for training as\nfollows:\ndata_loader = SimpleDataLoader(text8, batch_size=BATCH_SIZE)\ndata_loader.index_with(vocab)\nLet’s then define an Embedding object that holds all the word embeddings we’d like\nto learn:\nembedding_in = Embedding(num_embeddings=vocab.get_vocab_size('token_in'),\n                         embedding_dim=EMBEDDING_DIM)\nHere, EMBEDDING_DIM is the length of each word vector (number of float numbers).\nA typical NLP application uses word vectors of a couple hundred dimensions long (in\nthis example, 256), but this value depends greatly on the task and the datasets. It is\noften suggested that you use longer word vectors as your training data grows.\n Finally, you need to implement the body of the Skip-gram model, as shown next.\nclass SkipGramModel(Model):   \n    def __init__(self, vocab, embedding_in):\n        super().__init__(vocab)\n        self.embedding_in = embedding_in   \n        self.linear = torch.nn.Linear(\n            in_features=EMBEDDING_DIM,\n            out_features=vocab.get_vocab_size('token_out'),\n            bias=False)   \n    def forward(self, token_in, token_out):   \n        \n        embedded_in = self.embedding_in(token_in)   \n        \n        logits = self.linear(embedded_in)   \n        loss = functional.cross_entropy(logits, token_out)  \n        return {'loss': loss}\nListing 3.1\nSkip-gram model implemented in AllenNLP\nAllenNLP requires every model \nto be inherited from Model.\nThe embedding object is passed from \noutside rather than defined inside.\nThis creates a linear\nlayer (note that we\ndon’t need biases).\nThe body of neural network computation \nis implemented in forward().\nConverts input tensors \n(word IDs) to word \nembeddings\nApplies the\nlinear layer\nComputes \nthe loss\n\n\n65\nSkip-gram and continuous bag of words (CBOW)\nA few things to note:\nAllenNLP requires every model to be inherited from Model, which can be\nimported from allennlp.models.\nModel’s initializer (__init__) takes a Vocabulary instance and any other\nparameters or submodels defined externally. It also defines any internal param-\neters or models.\nThe main computation of the model is defined in forward(). It takes all the\nfields from instances (in this example, token_in and token_out) as tensors\n(multidimensional arrays) and returns a dict that contains the 'loss' key,\nwhich will be used by the optimizer to train the model.\nYou can train this model using the following code.\nreader = SkipGramReader()\ntext8 = reader.read(' https:/./realworldnlpbook.s3.amazonaws.com/data/text8/\ntext8')\nvocab = Vocabulary.from_instances(\n    text8, min_count={'token_in': 5, 'token_out': 5})\ndata_loader = SimpleDataLoader(text8, batch_size=BATCH_SIZE)\ndata_loader.index_with(vocab)\nembedding_in = Embedding(num_embeddings=vocab.get_vocab_size('token_in'),\n                         embedding_dim=EMBEDDING_DIM)\nmodel = SkipGramModel(vocab=vocab,\n                      embedding_in=embedding_in)\noptimizer = optim.Adam(model.parameters())\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=data_loader,\n    num_epochs=5,\n    cuda_device=CUDA_DEVICE)\ntrainer.train()\nTraining takes a while, so I recommend truncating the training data first, say, by using\nonly the first one million tokens. You can do this by inserting text8 = list(text8)\n[:1000000] after reader.read(). After the training is finished, you can get related\nwords (words with the same meanings) using the method shown in listing 3.3. This\nmethod first obtains the word vector for a given word (token), then computes how\nsimilar it is to every other word vector in the vocabulary. The similarity is calculated\nusing something called the cosine similarity. In simple terms, the cosine similarity is the\nopposite of the angle between two vectors. If two vectors are identical, the angle\nbetween them is zero, and the similarity will be 1, which is the largest possible value. If\nListing 3.2\nCode for training the Skip-gram model\n\n\n66\nCHAPTER 3\nWord and document embeddings\ntwo vectors are perpendicular, the angle is 90 degrees, and the cosine will be 0. If the\nvectors are in totally opposite directions, the cosine will be –1.\ndef get_related(token: str, embedding: Model, vocab: Vocabulary, \n                num_synonyms: int = 10):\n    token_id = vocab.get_token_index(token, 'token_in')\n    token_vec = embedding.weight[token_id]\n    cosine = CosineSimilarity(dim=0)\n    sims = Counter()\n    for index, token in \nvocab.get_index_to_token_vocabulary('token_in').items():\n        sim = cosine(token_vec, embedding.weight[index]).item()\n        sims[token] = sim\n    return sims.most_common(num_synonyms)\nIf you run this for words “one” and “december,” you get the lists of related words\nshown in table 3.1. Although you can see some words that are not related to the query\nword, overall, the results look good. \nOne final note: you need to implement a couple of techniques if you want to use Skip-\ngram to train high-quality word vectors in practice, namely, negative sampling and\nsubsampling of high-frequency words. Although they are important concepts, they\ncan be a distraction if you are just starting out and would like to learn the basics of\nNLP. If you are interested in learning more, check out this blog post that I wrote on\nthis topic: http://realworldnlpbook.com/ch3.html#word2vec-blog.\nListing 3.3\nMethod to obtain related words using word embeddings\nTable 3.1\nRelated words for “one” and “december\"\n“one”\n“december”\none\ndecember\nnine\njanuary\neight\nnixus\nsix\nlondini\nfive\nplantarum\nseven\njune\nthree\nsmissen\nfour\nfebruary\nd\nqanuni\nactress\noctober\n\n\n67\nSkip-gram and continuous bag of words (CBOW)\n3.4.6\nContinuous bag of words (CBOW) model\nAnother word-embedding model that is often mentioned along with the Skip-gram\nmodel is the continuous bag of words (CBOW) model. As a close sibling of the Skip-\ngram model, proposed at the same time (http://realworldnlpbook.com/ch3.html#\nmikolov13), the architecture of the CBOW model looks similar to that of the Skip-\ngram model but flipped upside down. The “fake” task the model is trying to solve is\nto predict the target word from a set of its context words. This is also similar to fill-in-\nthe-blank type of questions. For example, if you see a sentence “I heard a ___ barking\nin the distance,” most of you can probably guess the answer “dog” instantly. Figure\n3.8 shows the structure of this model.\nFigure 3.8\nContinuous bag of words (CBOW) model\nI’m not going to implement the CBOW model from scratch here for a couple of rea-\nsons. It should be straightforward to implement if you understand the Skip-gram\nmodel. Also, the accuracy of the CBOW model measured on word semantic tasks is\na\nin\nWord\nembeddings\ndog\nchocolate\nwolf\ncat\n…\n…\nJoin\nLinear\nlayer\n \nScores\nbarking\nheard\n\n\n68\nCHAPTER 3\nWord and document embeddings\nusually slightly lower than that of Skip-gram, and CBOW is less often used in NLP than\nSkip-gram. Both models are implemented in the original Word2vec (https://code\n.google.com/archive/p/word2vec/) toolkit, if you want to try them yourself, although\nthe vanilla Skip-gram and CBOW models are less and less often used nowadays because\nof the advent of more recent, powerful word-embedding models (such as GloVe and\nfastText) that are covered in the rest of this chapter. \n3.5\nGloVe\nIn the previous section, I implemented Skip-gram and showed how you can train\nyour word embeddings using large text data. But what if you wanted to build your\nown NLP applications leveraging high-quality word embeddings while skipping all\nthe hassle? What if you couldn’t afford the computation and data required to train\nword embeddings? \n Instead of training word embeddings, you can always download pretrained word\nembeddings published by somebody else, which many NLP practitioners do. In this\nsection, I’m going to introduce another popular word-embedding model—GloVe,\nnamed after Global Vectors. Pretrained word embeddings generated by GloVe are prob-\nably the most widely used embeddings in NLP applications today. \n3.5.1\nHow GloVe learns word embeddings\nThe main difference between the two models described earlier and GloVe is that the\nformer is local. To recap, Skip-gram uses a prediction task where a context word\n(“bark”) is predicted from the target word (“dog”). CBOW basically does the opposite\nof this. This process is repeated as many times as there are word tokens in the dataset.\nIt basically scans the entire dataset and asks the question, “Can this word be predicted\nfrom this other word?” for every single occurrence of words in the dataset.\n Let’s think how efficient this algorithm is. What if there were two or more identical\nsentences in the dataset? Or very similar sentences? In that case, Skip-gram would\nrepeat the exact same set of updates multiple times. “Can ‘bark’ be predicted from\n‘dog’?” you might ask. But chances are you already asked that exact same question a\ncouple of hundred sentences ago. If you know that the words “dog” and “bark” appear\ntogether in the context N times in the entire dataset, why repeat this N times? It’s as if\nyou were adding “1” N times to something else (x + 1 + 1 + 1 + ... + 1) when you could\nsimply add N to it (x + N). Could we somehow use this global information directly?\n The design of GloVe is motivated by this insight. Instead of using local word\nco-occurrences, it uses aggregated word co-occurrence statistics in the entire dataset.\nLet’s say “dog” and “bark” co-occur N times in the dataset. I’m not going into the\ndetails of the model, but roughly speaking, the GloVe model tries to predict this num-\nber N from the embeddings of both words. Figure 3.9 illustrates this prediction task. It\nstill makes some predictions about word relations, but notice that it makes one predic-\ntion per a combination of word types, but Skip-gram does so for every combination of\nword tokens!\n\n\n69\nGloVe\nTOKEN AND TYPE\nAs mentioned in section 3.3.1, a token is an occurrence of a\nword in text. There may be multiple occurrences of the same word in a cor-\npus. A type, on the other hand, is a distinctive, unique word. For example, in\nthe sentence “A rose is a rose is a rose,” there are eight tokens but only three\ntypes (“a,” “rose,” and “is”). If you are familiar with object-oriented program-\nming, they are roughly equivalent to instance and class. There can be multi-\nple instances of a class, but there is only one class for a concept.\n3.5.2\nUsing pretrained GloVe vectors\nIn fact, not many NLP practitioners train GloVe vectors from scratch by themselves.\nMore often, we download and use word embeddings, which are pretrained using large\ntext corpora. This is not only quick but usually beneficial in making your NLP applica-\ntions more accurate, because those pretrained word embeddings (often made public\nby the inventor of word-embedding algorithms) are usually trained using larger data-\nsets and more computational power than most of us can afford. By using pretrained\nword embeddings, you can “stand on the shoulders of giants” and quickly leverage\nhigh-quality linguistic knowledge distilled from large text corpora.\n In the rest of this section, let’s see how we can download and search for similar\nwords using pretrained GloVe embeddings. First, you need to download the data file.\nThe official GloVe website (https://nlp.stanford.edu/projects/glove/) provides multi-\nple word-embedding files trained using different datasets and vector sizes. You can\npick any one you like (although the file size could be large, depending on which one\nyou choose) and unzip it. In what follows, we assume you save it under the relative\npath data/glove/.\n Most word-embedding files are formatted in a similar way. Each line contains a\nword, followed by a sequence of numbers that correspond to its word vector. There\nare as many numbers as there are dimensions (in the GloVe files distributed on the\nwebsite above, you can tell the dimensionality from the filename suffix in the form of\ndog\nbark\nWord\nembeddings\nCo-occurrence\nN(“dog”, “bark”)\nGloVe\nFigure 3.9\nGloVe\n",
      "page_number": 81
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 90-97)",
      "start_page": 90,
      "end_page": 97,
      "detection_method": "topic_boundary",
      "content": "70\nCHAPTER 3\nWord and document embeddings\nxxxd). Each field is delimited by a space. Here is an excerpt from one of the GloVe\nword-embedding files:\n...\nif 0.15778 0.17928 -0.45811 -0.12817 0.367 0.18817 -4.5745 0.73647 ...\none 0.38661 0.33503 -0.25923 -0.19389 -0.037111 0.21012 -4.0948 0.68349 ...\nhas 0.08088 0.32472 0.12472 0.18509 0.49814 -0.27633 -3.6442 1.0011 ...\n...\nAs we did in section 3.4.5, what we’d like to do is to take a query word (say, “dog”) and\nfind its neighbors in the N-dimensional space. One way to do this is to calculate the\nsimilarity between the query word and every other word in the vocabulary and sort the\nwords by their similarities, as shown in listing 3.3. Depending on the size of the vocab-\nulary, this approach could be very slow. It’s like linearly scanning an array to find an\nelement instead of using binary search. \n Instead, we’ll use approximate nearest neighbor algorithms to quickly search for\nsimilar words. In a nutshell, these algorithms enable us to quickly retrieve nearest\nneighbors without computing the similarity between every word pair. In particular,\nwe’ll use Annoy (https://github.com/spotify/annoy), a library for approximate\nneighbor search released from Spotify. You can install it by running pip install\nannoy. It implements a popular approximate nearest neighbor algorithm called\nlocally sensitive hashing (LSH) using random projection.\n To use Annoy to search similar words, you first need to build an index, which can be\ndone as shown in listing 3.4. Note that we are also building a dict from word indices\nto words and saving it to a separate file to facilitate the word lookup later (listing 3.5).\nfrom annoy import AnnoyIndex\nimport pickle\nEMBEDDING_DIM = 300\nGLOVE_FILE_PREFIX = 'data/glove/glove.42B.300d{}'\ndef build_index():\n    num_trees = 10\n    idx = AnnoyIndex(EMBEDDING_DIM)\n    index_to_word = {}\n    with open(GLOVE_FILE_PREFIX.format('.txt')) as f:\n        for i, line in enumerate(f):\n            fields = line.rstrip().split(' ')\n            vec = [float(x) for x in fields[1:]]\n            idx.add_item(i, vec)\n            index_to_word[i] = fields[0]\n    idx.build(num_trees)\n    idx.save(GLOVE_FILE_PREFIX.format('.idx'))\n    pickle.dump(index_to_word,\n                open(GLOVE_FILE_PREFIX.format('.i2w'), mode='wb'))\nListing 3.4\nBuilding an Annoy index\n\n\n71\nGloVe\nReading a GloVe embedding file and building an Annoy index can be quite slow, but\nonce it’s built, accessing it and retrieving similar words can be performed very quickly.\nThis configuration is similar to search engines, where an index is built to achieve near\nreal-time retrieval of documents. This is suitable for applications where retrieval of\nsimilar items in real time is required but update of the dataset happens less frequently.\nExamples include search engines and recommendation engines.\ndef search(query, top_n=10):\n    idx = AnnoyIndex(EMBEDDING_DIM)\n    idx.load(GLOVE_FILE_PREFIX.format('.idx'))\n    index_to_word = pickle.load(open(GLOVE_FILE_PREFIX.format('.i2w'),\n                                     mode='rb'))\n    word_to_index = {word: index for index, word in index_to_word.items()}\n    \n    query_id = word_to_index[query]\n    word_ids = idx.get_nns_by_item(query_id, top_n)\n    for word_id in word_ids:\n        print(index_to_word[word_id])\nIf you run this for the words “dog” and “december,” you’ll get the lists of the 10 most-\nrelated words shown in table 3.2.\nYou can see that each list contains many related words to the query word. You see the\nidentical words at the top of each list—this is because the cosine similarity of two iden-\ntical vectors is always 1, its maximum possible value. \nListing 3.5\nUsing an Annoy index to retrieve similar words\nTable 3.2\nRelated words for “dog” and “december\"\n“dog”\n“december”\ndog\ndecember\npuppy\njanuary\ncat\noctober\ncats\nnovember\nhorse\nseptember\nbaby\nfebruary\nbull\naugust\nkid\njuly\nkids\napril\nmonkey\nmarch\n\n\n72\nCHAPTER 3\nWord and document embeddings\n3.6\nfastText\nIn the previous section, we saw how to download pretrained word embeddings and\nretrieve related words. In this section, I’ll explain how to train word embeddings using\nyour own text data using fastText, a popular word-embedding toolkit. This is handy\nwhen your textual data is not in a general domain (e.g., medical, financial, legal, and\nso on) and/or is not in English.\n3.6.1\nMaking use of subword information\nAll the word-embedding methods we’ve seen so far in this chapter assign a distinct\nword vector for each word. For example, word vectors for “dog” and “cat” are treated\ndistinctly and are independently trained at the training time. At first glance, there\nseems to be nothing wrong about this. After all, they are separate words. But what if\nthe words were, say, “dog” and “doggy?” Because “-y” is an English suffix that denotes\nsome familiarity and affection (other examples include “grandma” and “granny” and\n“kitten” and “kitty”), these pairs of words have some semantic connection. However,\nword-embedding algorithms that treat words as distinct cannot make this connection.\nIn the eyes of these algorithms, “dog” and “doggy” are nothing more than, say,\nword_823 and word_1719.\n This is obviously limiting. In most languages, there’s a strong connection between\nword orthography (how you write words) and word semantics (what they mean). For\nexample, words that share the same stems (e.g., “study” and “studied,” “repeat” and\n“repeatedly,” and “legal” and “illegal”) are often related. By treating them as separate\nwords, word-embedding algorithms are losing a lot of information. How can they\nleverage word structures and reflect the similarities in the learned word embeddings? \n fastText, an algorithm and a word-embedding library developed by Facebook, is\none such model. It uses subword information, which means information about linguistic\nunits that are smaller than words, to train higher-quality word embeddings. Specifi-\ncally, fastText breaks words down to character n-grams (section 3.2.3) and learns\nembeddings for them. For example, if the target word is “doggy,” it first adds special\nsymbols at the beginning and end of the word and learns embeddings for <do, dog,\nogg, ggy, gy>, when n = 3. The vector for “doggy” is simply the sum of all these vec-\ntors. The rest of the architecture is quite similar to that of Skip-gram. Figure 3.10\nshows the structure of the fastText model.\n Another benefit in leveraging subword information is that it can alleviate the out-of-\nvocabulary (OOV) problem. Many NLP applications and models assume a fixed vocab-\nulary. For example, a typical word-embedding algorithm such as Skip-gram learns word\nvectors only for the words that were encountered in the train set. However, if a test set\ncontains words that did not appear in the train set (which are called OOV words), the\nmodel would be unable to assign any vectors to them. For example, if you train Skip-\ngram word embeddings from books published in the 1980s and apply them to modern\nsocial media text, how would it know what vectors to assign to “Instagram”? It won’t.\nOn the other hand, because fastText uses subword information (character n-grams), it\ncan assign word vectors to any OOV words, as long as they contain character n-grams\n\n\n73\nfastText\nseen in the training data (which is almost always the case). It can potentially guess it’s\nrelated to something quick (“Insta”) and pictures (“gram”).\n3.6.2\nUsing the fastText toolkit\nFacebook provides the open source for the fastText toolkit, a library for training the\nword-embedding model discussed in the previous section. In the remainder of this\nsection, let’s see how it feels like to use this library to train word embeddings.\n First, go to their official documentation (http://realworldnlpbook.com/ch3.html\n#fasttext) and follow the instruction to download and compile the library. It is just a\nmatter of cloning the GitHub repository and running make from the command line\nin most environments. After compilation is finished, you can run the following com-\nmand to train a Skip-gram-based fastText model:\n$ ./fasttext skipgram -input ../data/text8 -output model\nWe are assuming there’s a text data file under ../data/text8 that you’d like to use\nas the training data, but change this if necessary. This will create a model.bin file,\nwhich is a binary representation of the trained model. After training the model, you\nCharacter n-gram\nembeddings\nbark\nchocolate\npet\nsmell\n…\n…\nAdd\nLinear\nlayer\n \nScores\ndog\ngy>\nggy\n<do\nFigure 3.10\nArchitecture\nof fastText\n\n\n74\nCHAPTER 3\nWord and document embeddings\ncan obtain word vectors for any words, even for the ones that you’ve never seen in the\ntraining data, as follows:\n$ echo \"supercalifragilisticexpialidocious\" \\\n| ./fasttext print-word-vectors model.bin\nsupercalifragilisticexpialidocious 0.032049 0.20626 -0.21628 -0.040391 -\n0.038995 0.088793 -0.0023854 0.41535 -0.17251 0.13115 ...\n3.7\nDocument-level embeddings\nAll the models I have described so far learn embeddings for individual words. If you are\nconcerned only with word-level tasks such as inferring word relationships, or if they are\ncombined with more powerful neural network models such as recurrent neural net-\nworks (RNNs), they can be very useful tools. However, if you wish to solve NLP tasks that\nare concerned with larger linguistic structures such as sentences and documents using\nword embeddings and traditional machine learning tools such as logistic regression\nand support vector machines (SVMs), word-level embedding methods are still limited.\nHow can you represent larger linguistic units such as sentences using vector represen-\ntations? How can you use word embeddings for sentiment analysis, for example?\n One way to achieve this is to simply use the average of all word vectors in a sen-\ntence. You can average vectors by taking the average of first elements, second ele-\nments, and so on and make a new vector by combining these averaged numbers. You\ncan use this new vector as an input to traditional machine learning models. Although\nthis method is simple and can be effective, it is also very limiting. The biggest issue is\nthat it cannot take word order into consideration. For example, both sentences “Mary\nloves John.” and “John loves Mary.” would have exactly the same vectors if you simply\naveraged word vectors for each word in the sentence.\n NLP researchers have proposed models and algorithms that can specifically address\nthis issue. One of the most popular is Doc2Vec, originally proposed by Le and Mikolov\nin 2014 (https://cs.stanford.edu/~quocle/paragraph_vector.pdf). This model, as its\nname suggests, learns vector representations for documents. In fact, “document” here\nsimply means any variable-length piece of text that contains multiple words. Similar\nmodels are also called under many similar names such as Sentence2Vec, Paragraph2Vec,\nparagraph vectors (this is what the authors of the original paper used), but in essence,\nthey all refer to the variations of the same model.\n In the rest of this section, I’m going to discuss one of the Doc2Vec models called\ndistributed memory model of paragraph vectors (PV-DM). The model looks very similar to\nCBOW, which we studied earlier in this chapter, but with one key difference—one\nadditional vector, called paragraph vector, is added as an input. The model predicts the\ntarget word from a set of context words and the paragraph vector. Each paragraph is\nassigned a distinct paragraph vector. Figure 3.11 shows the structure of this PV-DM\nmodel. Also, PV-DM uses only context words that come before the target word for pre-\ndiction, but this is a minor difference.\n What effect would this paragraph vector have on the prediction task? Now you\nhave some extra information from the paragraph vector for predicting the target\nword. As the model tries to maximize the prediction accuracy, you can expect that the\n\n\n75\nDocument-level embeddings\nparagraph vector is updated so that it provides some useful “context” information in\nthe sentence that is not collectively captured by the context word vectors. As a by-\nproduct, the model learns something that reflects the overall meaning of each para-\ngraph, along with word vectors.\n Several open source libraries and packages support Doc2Vec models, but one of\nthe most widely used is Gensim (https://radimrehurek.com/gensim/), which can be\ninstalled by running pip install gensim. Gensim is a popular NLP toolkit that sup-\nports a wide range of vector and topic models such as TF-IDF (term frequency and\ninverse document frequency), LDA (latent semantic analysis), and word embeddings. \n To train a Doc2Vec model using Gensim, you first need to read a dataset and con-\nvert documents to TaggedDocument. This can be done using the read_corpus()\nmethod shown here:\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.doc2vec import TaggedDocument\ndef read_corpus(file_path):\n    with open(file_path) as f:\n        for i, line in enumerate(f):\n            yield TaggedDocument(simple_preprocess(line), [i])\nI\na\nWord\nembeddings\nParagraph\nembedding\n \ndog\nchocolate\nwolf\ncat\n…\n…\nJoin\nLinear\nlayer\n \nScores\nheard\nparagraph_id\nFigure 3.11\nDistributed memory \nmodel of paragraph \nvectors\n\n\n76\nCHAPTER 3\nWord and document embeddings\nWe are going to use a small dataset consisting of the first 200,000 English sentences\ntaken from the Tatoeba project (https://tatoeba.org/). You can download the dataset\nfrom http://mng.bz/7l0y. Then you can use Gensim’s Doc2Vec class to train the\nDoc2Vec model and retrieve similar documents based on the trained paragraph vec-\ntors, as demonstrated next.\n    from gensim.models.doc2vec import Doc2Vec\n    \n    train_set = list(read_corpus('data/mt/sentences.eng.200k.txt'))\n    model = Doc2Vec(vector_size=256, min_count=3, epochs=30)\n    model.build_vocab(train_set)\n    model.train(train_set,\n                total_examples=model.corpus_count,\n                epochs=model.epochs)\n    query_vec = model.infer_vector(\n        ['i', 'heard', 'a', 'dog', 'barking', 'in', 'the', 'distance'])\n    sims = model.docvecs.most_similar([query_vec], topn=10)\n    for doc_id, sim in sims:\n        print('{:3.2f} {}'.format(sim, train_set[doc_id].words)) \nThis will show you a list of documents similar to the input document “I heard a dog\nbarking in the distance,” as follows:\n0.67 ['she', 'was', 'heard', 'playing', 'the', 'violin']\n0.65 ['heard', 'the', 'front', 'door', 'slam']\n0.61 ['we', 'heard', 'tigers', 'roaring', 'in', 'the', 'distance']\n0.61 ['heard', 'dog', 'barking', 'in', 'the', 'distance']\n0.60 ['heard', 'the', 'door', 'open']\n0.60 ['tom', 'heard', 'the', 'door', 'open']\n0.60 ['she', 'heard', 'dog', 'barking', 'in', 'the', 'distance']\n0.59 ['heard', 'the', 'door', 'close']\n0.59 ['when', 'he', 'heard', 'the', 'whistle', 'he', 'crossed', 'the', 'street']\n0.58 ['heard', 'the', 'telephone', 'ringing']\nNotice that most of the retrieved sentences here are related to hearing sound. In fact,\nan identical sentence is in the list, because I took the query sentence from Tatoeba in\nthe first place! Gensim’s Doc2Vec class has a number of hyperparameters that you can\nuse to tweak the model. You can read further about the class on their reference page\n(https://radimrehurek.com/gensim/models/doc2vec.html).\n3.8\nVisualizing embeddings\nIn the final section of this chapter, we are going to shift our focus on visualizing word\nembeddings. As we’ve done earlier, retrieving similar words given a query word is a\ngreat way to quickly check if word embeddings are trained correctly. But it gets tiring\nand time-consuming if you need to check a number of words to see if the word\nembeddings are capturing semantic relationships between words as a whole.\nListing 3.6\nTraining a Doc2Vec model and retrieving similar documents\n\n\n77\nVisualizing embeddings\n As mentioned earlier, word embeddings are simply N-dimensional vectors, which\nare also “points” in an N-dimensional space. We were able to see those points visually\nin a 3-D space in figure 3.1 because N was 3. But N is typically a couple of hundred in\nmost word embeddings, and we cannot simply plot them on an N-dimensional space.\n A solution is to reduce the dimension down to something that we can see (two or\nthree dimensions) while preserving relative distances between points. This technique\nis called dimensionality reduction. We have a number of ways to reduce dimensionality,\nincluding PCA (principal component analysis) and ICA (independent component\nanalysis), but by far the most widely used visualization technique for word embeddings\nis called t-SNE (t-distributed Stochastic Neighbor Embedding, pronounced “tee-snee).\nAlthough the details of t-SNE are outside the scope of this book, the algorithm tries to\nmap points to a lower-dimensional space by preserving the relative neighboring rela-\ntionship between points in the original high-dimensional space.\n The easiest way to use t-SNE is to use Scikit-Learn (https://scikit-learn.org/), a\npopular Python library for machine learning. After installing it (usually just a matter\nof running pip install scikit-learn), you can use it to visualize the GloVe vec-\ntors read from a file as shown next (we use Matplotlib to draw the plot).\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\ndef read_glove(file_path):\n    with open(file_path) as f:\n        for i, line in enumerate(f):\n            fields = line.rstrip().split(' ')\n            vec = [float(x) for x in fields[1:]]\n            word = fields[0]\n            yield (word, vec)\nwords = []\nvectors = []\nfor word, vec in read_glove('data/glove/glove.42B.300d.txt'):\n    words.append(word)\n    vectors.append(vec)\nmodel = TSNE(n_components=2, init='pca', random_state=0)\ncoordinates = model.fit_transform(vectors)\nplt.figure(figsize=(8, 8))\nfor word, xy in zip(words, coordinates):\n    plt.scatter(xy[0], xy[1])\n    plt.annotate(word,\n                 xy=(xy[0], xy[1]),\n                 xytext=(2, 2),\n                 textcoords='offset points')\nplt.xlim(25, 55)\nplt.ylim(-15, 15)\nplt.show()\nListing 3.7\nUsing t-SNE to visualize GloVe embeddings\n",
      "page_number": 90
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 98-107)",
      "start_page": 98,
      "end_page": 107,
      "detection_method": "topic_boundary",
      "content": "78\nCHAPTER 3\nWord and document embeddings\nIn listing 3.7, I used xlim() and ylim() to limit the plotted range to magnify some\nareas that are of interest to us. You may want to try different values to focus on other\nareas in the plot.\n The code in listing 3.7 generates the plot shown in figure 3.12. There’s a lot of\ninteresting stuff going on here, but at a quick glance, you will notice the following\nclusters of words that are semantically related:\nBottom-left: web-related words (posts, article, blog, comments, . . . ).\nUpper-left: time-related words (day, week, month, year, . . . ).\nMiddle: numbers (0, 1, 2, . . . ). Surprisingly, these numbers are lined up in an\nincreasing order toward the bottom. GloVe figured out which numbers are\nlarger purely from a large amount of textual data.\nBottom-right: months (january, february, . . . ) and years (2004, 2005, . . . ).\nAgain, the years seem to be lined up in an increasing order, almost in parallel\nwith the numbers (0, 1, 2, . . . ).\nFigure 3.12\nGloVe embeddings visualized by t-SNE\n\n\n79\nSummary\nIf you think about it, it’s an incredible feat for a purely mathematical model to figure\nout these relationships among words, all from a large amount of text data. Hopefully,\nnow you know how much of an advantage it is if the model knows “july” and “june” are\nclosely related compared to needing to figure everything out starting from word_823\nand word_1719.\nSummary\nWord embeddings are numeric representations of words, and they help convert\ndiscrete units (words and sentences) to continuous mathematical objects\n(vectors).\nThe Skip-gram model uses a neural network with a linear layer and softmax to\nlearn word embeddings as a by-product of the “fake” word-association task.\nGloVe makes use of global statistics of word co-occurrence to train word embed-\ndings efficiently.\nDoc2Vec and fastText learn document-level embeddings and word embeddings\nwith subword information, respectively.\nYou can use t-SNE to visualize word embeddings.\n\n\n80\nSentence classification\nIn this chapter, we are going to study the task of sentence classification, where an\nNLP model receives a sentence and assigns some label to it. A spam filter is an\napplication of sentence classification. It receives an email message and assigns\nwhether or not it is spam. If you want to classify news articles into different topics\nThis chapter covers\nHandling variable-length input with recurrent neural \nnetworks (RNN)\nWorking with RNNs and their variants (LSTMs and \nGRUs)\nUsing common evaluation metrics for classification \nproblems\nDeveloping and configuring a training pipeline using \nAllenNLP\nBuilding a language detector as a sentence \nclassification task\n\n\n81\nRecurrent neural networks (RNNs)\n(business, politics, sports, and so on), it’s also a sentence-classification task. Sentence\nclassification is one of the simplest NLP tasks that has a wide range of applications,\nincluding document classification, spam filtering, and sentiment analysis. Specifically,\nwe are going to revisit the sentiment classifier we introduced in chapter 2 and discuss\nits components in detail. At the end of this section, we are going to study another\napplication of sentence classification—language detection.\n4.1\nRecurrent neural networks (RNNs)\nThe first step in sentence classification is to represent variable-length sentences using\nneural networks (RNNs). In this section, I’m going to present the concept of recur-\nrent neural networks, one of the most important concepts in deep NLP. Many modern\nNLP models use RNNs in some way. I’ll explain why they are important, what they do,\nand introduce their simplest variant.\n4.1.1\nHandling variable-length input\nThe Skip-gram network structure shown in the previous chapter was simple. It takes a\nword vector of a fixed size, runs it through a linear layer, and obtains a distribution of\nscores over all the context words. The structure and the size of the input, output, and\nthe network are all fixed throughout the training.\n However, many, if not most, of what we deal with in NLP are sequences of variable\nlengths. For example, words, which are sequences of characters, can be short (“a,”\n“in”) or long (“internationalization”). Sentences (sequences of words) and docu-\nments (sequences of sentences) can be of any lengths. Even characters, if you look at\nthem as sequences of strokes, can be simple (e.g., “O” and “L” in English) or more\ncomplex (e.g., “鬱” is a Chinese character meaning “depression” which, depressingly,\nhas 29 strokes). \n As we discussed in the previous chapter, neural networks can handle only numbers\nand arithmetic operations. That was why we needed to convert words and documents\nto numbers through embeddings. We used linear layers to convert a fixed-length vec-\ntor into another. But to do something similar with variable-length inputs, we need to\nfigure out how to structure the neural networks so that they can handle them.\n One idea is to first convert the input (e.g., a sequence of words) to embeddings,\nthat is, a sequence of vectors of floating-point numbers, then average them. Let’s\nassume the input sentence is sentence = [\"john\", \"loves\", \"mary\", \".\"], and\nyou already know word embeddings for each word in the sentence v(\"john\"),\nv(\"loves\"), and so on. The average can be obtained with the following code and\nillustrated in figure 4.1:\nresult = (v(\"john\") + v(\"loves\") + v(\"mary\") + v(\".\")) / 4\n\n\n82\nCHAPTER 4\nSentence classification\nThis method is quite simple and is actually used in many NLP applications. However,\nit has one critical issue, which is that it cannot take word order into account. Because\nthe order of input elements doesn’t affect the result of averaging, you’d get the same\nvector for both “Mary loves John” and “John loves Mary.” Although it’s up to the task\nin hand, it’s hard to imagine many NLP applications would want this kind of behavior.\n If we step back and reflect how we humans read language, this “averaging” is far\nfrom actuality. When we read a sentence, we don’t usually read individual words in iso-\nlation and remember them first, then move on to figuring out what the sentence\nmeans. We usually scan the sentence from the beginning, one word at a time, while\nholding what the “partial” sentence means up until the part you are reading in our\nshort-term memory. In other words, you maintain some sort of mental representation\nof the sentence while you read it. When you reach the end of the sentence, the mental\nrepresentation is its meaning. \n Can we design a neural network structure that simulates this incremental reading\nof the input? The answer is a resounding yes. That structure is called recurrent neural\nnetworks (RNNs), which I’ll explain in detail next.\n4.1.2\nRNN abstraction\nIf you break down the reading process mentioned earlier, its core is the repetition of\nthe following series of operations:\n1\nRead a word.\n2\nBased on what has been read so far (your “mental state”), figure out what the\nword means.\n3\nUpdate the mental state.\n4\nMove on to the next word.\nLet’s see how this works using a concrete example. If the input sentence is sentence\n= [\"john\", \"loves\", \"mary\", \".\"], and each word is already represented as a\nword-embedding vector. Also, let’s denote your “mental state” as state, which is ini-\ntialized by init_state(). Then, the reading process is represented by the following\nincremental operations:\nstate = init_state()\nstate = update(state, v(\"john\"))\nAverage\nresult\nsentence\nv(\"john\") v(\"loves\") v(\"mary\")\nv(\".\")\nFigure 4.1\nAveraging \nembedding vectors\n\n\n83\nRecurrent neural networks (RNNs)\nstate = update(state, v(\"loves\"))\nstate = update(state, v(\"mary\"))\nstate = update(state, v(\".\"))\nThe final value of state becomes the representation of the entire sentence from this\nprocess. Notice that if you change the order in which these words are processed (e.g.,\nby flipping “John” and “Mary”), the final value of state also changes, meaning that the\nstate also encodes some information about the word order.\n You can achieve something similar if you can design a network substructure that is\napplied to each element of the input while updating some internal states. RNNs are neu-\nral network structures that do exactly this. In a nutshell, an RNN is a neural network with\na loop. At its core is an operation that is applied to every element in the input as they\ncome in. If you wrote what RNNs do in Python pseudocode, it’d be like the following:\ndef rnn(words):\n    state = init_state()\n    for word in words:\n        state = update(state, word)\n    return state\nNotice that there is state that gets initialized first and passed around during the iter-\nation. For every input word, state is updated based on the previous state and the\ninput using the function update. The network substructure corresponding to this\nstep (the code block inside the loop) is called a cell. This stops when the input is\nexhausted, and the final value of state becomes the result of this RNN. See figure 4.2\nfor the illustration. \n You can see the parallelism here. When you are reading a sentence (sequence of\nwords), your internal mental representation of the sentence, state, is updated after\nreading each word. You can assume that the final state encodes the representation of\nthe entire sentence. \n The only remaining work is to design two functions—init_state() and\nupdate(). The state is usually initialized with zero (i.e., a vector filled with zeros), so\ninit_state()\nstate\nupdate\nupdate\nupdate\nupdate\nv(\"john\")\nv(\"loves\")\nv(\"mary\")\nv(\".\")\nRecurrent neural network (RNN)\nFigure 4.2\nRNN abstraction\n\n\n84\nCHAPTER 4\nSentence classification\nyou usually don’t have to worry about how to go about defining the former. It’s more\nimportant how you design update(), which determines the characteristics of the RNN.\n4.1.3\nSimple RNNs and nonlinearity\nIn section 3.4.3, I talked about how to implement a linear layer with an arbitrary num-\nber of inputs and outputs. Can we do something similar and implement update(),\nwhich is basically a function that takes two input variables and produces one output\nvariable? After all, a cell is a neural network with its own input and output, right? The\nanswer is yes, and it looks like this:\ndef update_simple(state, word):\n    return f(w1 * state + w2 * word + b)\nNotice that this is strikingly similar to the linear2() function in section 3.4.3. In\nfact, if you ignore the difference in variable names, it’s exactly the same except for the\nf() function. An RNN defined by this type of the update function is called a simple\nRNN or Elman RNN, which is, as its name suggests, one of the simplest RNN structures.\n You may be wondering, then, what is this function f() doing here? What does it\nlook like? Do we need it here at all? The function, called an activation function or non-\nlinearity, takes a single input (or a vector) and transforms it (or every element of a vec-\ntor) in a nonlinear fashion. Many kinds of nonlinearities exist, and they play an\nindispensable role in making neural networks truly powerful. What they exactly do and\nwhy they are important requires some math to understand, which is outside the scope\nof this book, but I’ll attempt an intuitive explanation with a simple example next.\n Imagine you are building an RNN that recognizes “grammatical” English sentences.\nDifferentiating grammatical sentences from ungrammatical ones is itself a difficult NLP\nproblem, which is, in fact, a well-established research area (see section 1.2.1), but here,\nlet’s simplify it and consider agreement only between the subject and the verb. Let’s fur-\nther simplify it and assume that there are only four words in this “language”—“I,” “you,”\n“am,” and “are.” If the sentence is either “I am” or “you are,” it’s grammatically correct.\nThe other two combinations, “I are” and “you am,” are incorrect. What you want to\nbuild is an RNN that outputs 1 for the correct sentences while producing 0 for the\nincorrect ones. How would you go about building such a neural network?\n The first step in almost every modern NLP model is to represent words with\nembeddings. As mentioned in the previous chapter, they are usually learned from a\nlarge dataset of natural language text, but here, we are simply going to give them\nsome predefined values, as shown in figure 4.3.\n Now, let’s imagine there was no activation function. The previous update_\nsimple() function simplifies to the following:\ndef update_simple_linear(state, word):\n    return w1 * state + w2 * word + b\nWe will assume the initial value of state is simply [0, 0], because the specific initial val-\nues are not relevant to the discussion here. The RNN takes the first word embedding,\n\n\n85\nRecurrent neural networks (RNNs)\nx1, updates state, takes the second word embedding, x2, then produces the final\nstate, which is a two-dimensional vector. Finally, the two elements in this vector are\nsummed and converted to result. If result is close to 1, the sentence is grammati-\ncal. Otherwise, it is not. If you apply the update_simple_linear() function twice\nand simplify it a little bit, you get the following function, which is all this RNN does,\nafter all:\nw1 * w2 * x1 + w2 * x2 + w1 * b + b\nRemember, w1, w2, and b are parameters of the model (aka “magic constants”) that\nneed to be trained (adjusted). Here, instead of adjusting these parameters using a\ntraining dataset, let’s assign some arbitrary values and see what happens. For example,\nwhen w1 = [1, 0], w2 = [0, 1], and b = [0, 0], the input and the output of this RNN\nwill be as shown in figure 4.4.\nFigure 4.4\nInput and output when w1 = [1, 0], w2 = [0, 1], and b = [0, 0] without \nan activation function\nWord\nI\nyou\nam\nEmbeddings\n[-1, 1]\n[1, -1]\n[-1, -1]\n[1, 1]\nare\nWord embeddings\ninit_state()\nstate\nresult\n+\nupdate\nupdate\nv(\"i\")\nv(\"you\")\nv(\"am\")\nv(\"are\")\nRecurrent neural network (RNN)\nFigure 4.3\nRecognizing grammatical English sentences using an RNN\nWord 1\nI\nI\nyou\nx1\nw1 = [1, 0], w2 = [0, 1], b = [0, 0]\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\nstate\nresult\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n[0, -1]\n[0, 1]\n[0, -1]\n[0, 1]\n-1\n1\n-1\n1\n1\n0\n0\n1\nyou\nWord 2\nDesired\nam\nare\nam\nare\n\n\n86\nCHAPTER 4\nSentence classification\nIf you look at the values of result, this RNN groups ungrammatical sentences (e.g.,\n“I are”) with grammatical ones (e.g., “you are”), which is not the desired behavior.\nHow about we try another set of values for the parameters? Let’s use w1 = [1, 0], w2\n= [-1, 0], and b = [0, 0] and see what happens (see figure 4.5 for the result).\nFigure 4.5\nInput and output when w1 = [1, 0], w2 = [-1, 0], and b = [0, 0] \nwithout an activation function\nThis is much better, because the RNN is successful in grouping ungrammatical sen-\ntences by assigning 0 to both “I are” and “you am.” However, it also assigns completely\nopposite values (2 and –2) to grammatical sentences (“I am” and “you are”). \n I’m going to stop here, but as it turns out, you cannot use this neural network to\ndifferentiate grammatical sentences from ungrammatical ones, no matter how hard\nyou try. Despite the values you assign to the parameters, this RNN cannot produce\nresults that are close enough to the desired values and, thus, are able to group sen-\ntences by their grammaticality.\n Let’s step back and think why this is the case. If you look at the previous update\nfunction, all it does is multiply the input by some value and add them up. In more spe-\ncific terms, it only transforms the input in a linear fashion. The result of this neural net-\nwork always changes by some constant amount when you change the value of the\ninput by some amount. But this is obviously not desirable—you want the result to be 1\nonly when the input variables are some specific values. In other words, you don’t want\nthis RNN to be linear; you want it to be nonlinear.\n To use an analogy, imagine you can use only assignment (“=”), addition (“+”), and\nmultiplication (“*”) in your programming language. You can tweak the input values to\nsome degree to come up with the result, but you can’t write more complex logic in\nsuch a restricted setting. \n Now let’s put the activation function f() back and see what happens. The specific\nactivation function we are going to use is called the hyperbolic tangent function, or more\ncommonly, tanh, which is one of the most commonly used activation functions in neu-\nral networks. The details of this function are not important in this discussion, but in a\nWord 1\nI\nI\nyou\nx1\nw1 = [1, 0], w2 = [-1, 0], b = [0, 0]\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\nstate\nresult\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n[2, 0]\n[0, 0]\n[0, 0]\n[-2, 0]\n2\n0\n0\n-2\n1\n0\n0\n1\nyou\nWord 2\nDesired\nam\nare\nam\nare\n\n\n87\nRecurrent neural networks (RNNs)\nnutshell, it behaves as follows: tanh doesn’t do much to the input when it is close to\nzero, for example, 0.3 or –0.2. In other words, the input passes through the function\nalmost unchanged. When the input is far from zero, tanh tries to squeeze it between\n–1 and 1. For example, when the input is large (say, 10.0), the output becomes very\nclose to 1.0, whereas when it is small (say, –10.0), the output becomes almost –1.0.\nThis creates an effect similar to the OR logical gate (or an AND gate, depending on\nthe weights) if two or more variables are fed into the activation function. The output\nof the gate becomes ON (~1) and OFF (~–1) depending on the input.\n When w1 = [-1, 2], w2 = [-1, 2], b = [0, 1], and the tanh activation func-\ntion is used, the result of the RNN becomes a lot closer to what we desire (see figure\n4.6). If you round them to the closest integers, the RNN successfully groups sentences\nby their grammaticality.\nFigure 4.6\nInput and output when w1 = [-1, 2], w2 = [-1, 2], and b = [0, 1] \nwith an activation function\nTo use the same analogy, using activation functions in your neural networks is like\nusing ANDs and ORs and IFs in your programming language, in addition to basic\nmath operations like addition and multiplication. In this way, you can write complex\nlogic and model complex interactions between input variables, like the example in\nthis section. \nNOTE\nThis example I use in this section is a slightly modified version of the\npopular XOR (or exclusive-or) example commonly seen in deep learning text-\nbooks. This is the most basic and simplest example that can be solved by neu-\nral networks but not by other linear models.\nSome final notes on RNNs—they are trained just like any other neural networks. The\nfinal outcome is compared with the desired outcome using the loss function, then the\ndifference between the two—the loss—is used for updating the “magic constants.”\nThe magic constants are, in this case, w1, w2, and b in the update_simple() func-\ntion. Note that the update function and its magic constants are identical across all the\nWord 1\nI\nI\nyou\nx1\nw1 = [-1, 2], w2 = [-1, 2], b = [0, 1]\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\nstate\nresult\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n[0.23, 0.75]\n[-0.94, 0.99]\n[0.94, -0.98]\n[-0.23, 0.90]\n0.99\n0.06\n-0.04\n0.67\n1\n0\n0\n1\nyou\nWord 2\nDesired\nam\nare\nam\nare\n",
      "page_number": 98
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 108-116)",
      "start_page": 108,
      "end_page": 116,
      "detection_method": "topic_boundary",
      "content": "88\nCHAPTER 4\nSentence classification\ntimesteps in the loop. This means that what RNNs are learning is a general form of\nupdates that can be applied to any situation. \n4.2\nLong short-term memory units (LSTMs) \nand gated recurrent units (GRUs)\nIn fact, the simple RNNs that we discussed earlier are rarely used in real-world NLP\napplications due to one problem called the vanishing gradients problem. In this section,\nI’ll show the issue associated with simple RNNs and how more popular RNN architec-\ntures, namely LSTMs and GRUs, solve this particular problem.\n4.2.1\nVanishing gradients problem\nJust like any programming language, if you know the length of the input, you can\nrewrite a loop without using one. An RNN can also be rewritten without using a loop,\nwhich makes it look just like a regular neural network with many layers. For example,\nif you know that there are only six words in the input, the rnn() from earlier can be\nrewritten as follows:\ndef rnn(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n    \n    state = update(state, word1)\n    state = update(state, word2)\n    state = update(state, word3)\n    state = update(state, word4)\n    state = update(state, word5)\n    state = update(state, word6)\n    return state\nRepresenting RNNs without loops is called unrolling. Now we know what update()\nlooks like for a simple RNN (update_simple), so we can replace the function calls\nwith their bodies, as shown here:\ndef rnn_simple(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n    state = f(w1 * f(w1 * f(w1 * f(w1 * f(w1 * f(w1 * state + w2 * word1 + b) \n+ w2 * word2 + b) + w2 * word3 + b) + w2 * word4 + b) + w2 * word5 + b) \n+ w2 * word6 + b)\n    return state\nThis is getting a bit ugly, but I just want you to notice the very deeply nested function\ncalls and multiplications. Now, recall the task we wanted to accomplish in the previous\nsection—classifying grammatical English sentences by recognizing subject-verb agree-\nment. Let’s say the input is sentence = [\"The\", \"books\", \"I\", \"read\", \"yes-\nterday\", \"were\"]. In this case, the innermost function call processes the first word\n\n\n89\nLong short-term memory units (LSTMs) and gated recurrent units (GRUs)\n“The,” the next one processes the second word “books,” and so on, all the way to the\noutermost function call, which processes “were.” If we rewrite the previous pseudocode\nslightly, as shown in the next code snippet, you can understand it more intuitively:\ndef is_grammatical(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n    state = process_main_verb(w1 *\n        process_adverb(w1 *\n            process_relative_clause_verb(w1 *\n                process_relative_clause_subject(w1 *\n                    process_main_subject(w1 *\n                        process_article(w1 * state + w2 * word1 + b) +\n                    w2 * word2 + b) +\n                w2 * word3 + b) +\n            w2 * word4 + b) + \n        w2 * word5 + b) + \n    w2 * word6 + b)\n    return state\nTo recognize that the input is indeed a grammatical English sentence (or a prefix of a\nsentence), the RNN needs to retain the information about the subject (“the books”)\nin state until it sees the verb (“were”) without being distracted by anything in\nbetween (“I read yesterday”). In the previous pseudocode, the states are represented\nby the return values of function calls, so the information about the subject (return\nvalue of process_main_subject) needs to propagate all the way up in this chain\nuntil it reaches the outermost function (process_main_verb). This is starting to\nsound like a difficult task.\n Things don’t look any better when it comes to training this RNN. RNNs, as well as\nany other neural networks, are trained using an algorithm called backpropagation. Back-\npropagation is a process where the components of a neural network communicate with\nprevious components on how to adjust the parameters to minimize the loss. This is how\nit works for this particular case. First, you look at the outcome, that is, the return value\nof is_grammatical()and compare it with what you desire. The difference between\nthese two is called the loss. The outermost function, is_grammatical(), basically has\nfour ways to decrease the loss to make its output closer to what is desired—1) adjust w1\nwhile fixing the return value of the nested function process_adverb(), 2) adjust w2,\n3) adjust b, or 4) adjust the return value of process_adverb() while fixing the param-\neters. Adjusting the parameters (w1, w2, and b) is the easy part because the function\nknows the exact effect of adjusting each parameter to its return value. Adjusting the\nreturn value of the previous function, however, is not easy, because the caller has no\nidea about the inner workings of the function. Because of this, the caller tells the pre-\nvious function (callee) to adjust its return values to minimize the loss. See figure 4.7 for\nhow the loss is propagated back to the parameters and previous functions.\n\n\n90\nCHAPTER 4\nSentence classification\n \nFigure 4.7\nBackpropagation of loss\nThe nested function calls repeat this process and plays the Telephone game until the\nmessage reaches the innermost function. By that time, because the message needs to\npass through many layers, it becomes so weak and obscure (or so strong and skewed\nbecause of some misunderstanding) that the inner functions have a difficult time fig-\nuring out what they did wrong.\n Technically, the deep learning literature calls this the vanishing gradients problem. A\ngradient is a mathematical term that corresponds to the message signal that each func-\ntion receives from the next one that states how exactly they should improve their pro-\ncess (how to change their magic constants). The reverse Telephone game, where\nmessages are passed backward from the final function (= loss function), is called back-\npropagation. I’m not going into the mathematical details of these terms, but it is use-\nful to understand them at least conceptually.\n Because of the vanishing gradients problem, simple RNNs are difficult to train and\nrarely used in practice nowadays. \n4.2.2\nLong short-term memory (LSTM)\nThe way the nested functions mentioned earlier process information about grammar\nseems too inefficient. After all, why doesn’t the outermost function (is_grammati-\ncal) tell the particular function in charge (e.g., process_main_subject) what\nwent wrong directly, instead of playing the Telephone game? It can’t, because the mes-\nsage can change its shape entirely after each function call because of w2 and f(). The\noutermost function cannot tell which function was responsible for which part of the\nmessage from only the final output. \n How could we address this inefficiency? Instead of passing the information\nthrough an activation function every time and changing its shape completely, how\nabout adding and subtracting information relevant to the part of the sentence being\nprocessed at each step? For example, if process_main_subject() can directly add\nv(\"were\")\nw1\nw2\nb\n*\n...\n+\n*\nstate\nstate\nis_grammatical()\nprocess_adverb()\nf()\nloss\n1)\n2)\n3)\n4)\n\n\n91\nLong short-term memory units (LSTMs) and gated recurrent units (GRUs)\ninformation about the subject to some type of “memory,” and the network can make\nsure the memory passes through the intermediate functions intact, is_grammati-\ncal()will have a much easier time telling the previous functions what to do to adjust\nits output.\n Long short-term memory units (LSTMs) are a type of RNN cell that is proposed based\non this insight. Instead of passing around states, LSTM cells share a “memory” that\neach cell can remove old information from and/or add new information to, some-\nthing like an assembly line in a manufacturing factory. Specifically, LSTM RNNs use\nthe following function for updating states:\ndef update_lstm(state, word):\n    cell_state, hidden_state = state\n    cell_state *= forget(hidden_state, word)\n    cell_state += add(hidden_state, word)\n    hidden_state = update_hidden(hidden_state, cell_state, word)\n    return (cell_state, hidden_state)\nAlthough this looks relatively complicated compared to its simple version, if you break\nit down to subcomponents, it’s not that difficult to understand what is going on here,\nas described next and shown in figure 4.8:\nThe LSTM states comprise two halves—the cell state (the “memory” part) and\nthe hidden state (the “mental representation” part).\nThe function forget() returns a value between 0 and 1, so multiplying by this\nnumber means erasing old memory from cell_state. How much to erase is\nforget\n*\n+\ncell\nstate\nLSTM cell\nhidden\nstate\ncell\nstate\nhidden\nstate\nword\nadd\nupdate\nhidden\nFigure 4.8\nLSTM update function\n\n\n92\nCHAPTER 4\nSentence classification\ndetermined from hidden_state and word (input). Controlling the flow of\ninformation by multiplying by a value between 0 and 1 is called gating. LSTMs\nare the first RNN architecture that uses this gating mechanism.\nThe function add()returns a new value added to the memory. The value again\nis determined from hidden_state and word.\nFinally, hidden_state is updated using a function, whose value is computed\nfrom the previous hidden state, the updated memory, and the input word.\nI abstracted the update function by hiding some mathematical details in the functions\nforget(), add(), and update_hidden(), which are not important for the discus-\nsion here. If you are interested in understanding LSTMs more deeply, I refer you to a\nwonderful blog post Chris Olah wrote on this topic (http://colah.github.io/posts/\n2015-08-Understanding-LSTMs/). \n Because LSTMs have this cell state that stays constant across different timesteps\nunless explicitly modified, they are easier to train and relatively well behaved. Because\nyou have a shared “memory” and functions are adding and removing information\nrelated to different parts of the input sentence, it is easier to pinpoint which function\ndid what and what went wrong. The error signal from the outermost function can\nreach responsible functions more directly.\n A word on the terminology—LSTM refers to one particular type of architecture\nmentioned here, but people use “LSTMs” to mean RNNs with LSTM cells. Also, “RNN”\nis often used to mean “simple RNN,” introduced in section 4.1.3. When you see “RNNs”\nin the literature, you need to be aware of which exact architectures they are using.\n4.2.3\nGated recurrent units (GRUs)\nAnother RNN architecture, called Gated Recurrent Units (GRUs), uses the gating\nmechanism. The philosophy behind GRUs is similar to that of LSTMs, but GRUs\nuse only one set of states instead of two halves. The update function for GRUs is\nshown next:\ndef update_gru(state, word): \n    new_state = update_hidden(state, word)\n    switch = get_switch(state, word)\n    state = swtich * new_state + (1 – switch) * state\n    return state\nInstead of erasing or updating the memory, GRUs use a switching mechanism. The\ncell first computes the new state from the old state and the input. It then computes\nswitch, a value between 0 and 1. The state is chosen between the new state and the\nold one based on the value of switch. If it’s 0, the old state passes through intact. If\nit’s 1, it’s overwritten by the new state. If it’s somewhere in between, the state will be a\nmix of two. See figure 4.9 for an illustration of the GRU update function.\n\n\n93\nAccuracy, precision, recall, and F-measure\nFigure 4.9\nGRU update function\nNotice that the update function for GRUs is much simpler than that for the LSTMs.\nIndeed, it has fewer parameters (magic constants) that need to be trained compared\nto LSTMs. Because of this, GRUs are faster to train than LSTMs.\n Finally, although we introduced two different types of RNN architecture, LSTM\nand GRU, there’s no consensus in the community on which type of architecture is the\nbest for all applications. You often need to treat them as a hyperparameter and exper-\niment with different configurations. Fortunately, it is easy to experiment with different\ntypes of RNN cells as long as you are using modern deep learning frameworks such as\nPyTorch and TensorFlow. \n4.3\nAccuracy, precision, recall, and F-measure\nIn section 2.7, I briefly talked about some metrics that we use for evaluating the per-\nformance of a classification task. Before we move on to actually building a sentence\nclassifier, I’d like to further discuss the evaluation metrics we are going to use—what\nthey mean and what they actually measure.\n4.3.1\nAccuracy\nAccuracy is probably the simplest of all the evaluation metrics that we talk about here.\nIn a classification setting, accuracy is the fraction of instances that your model got\nright. For example, if there are 10 emails and your spam-filtering model got 8 of them\ncorrect, the accuracy of your prediction is 0.8, or 80% (see figure 4.10).\n Though simple, accuracy is not without its limitations. Specifically, accuracy can be\nmisleading when the test set is imbalanced. An imbalanced dataset contains multiple\nclass labels that greatly differ in their numbers. For example, if a spam-filtering dataset\nis imbalanced, it may contain 90% nonspam emails and 10% spams. In such a case,\nupdate\nhidden\n \nstate\nnew_state\nGRU cell\nstate\nx\nswitch\n\n\n94\nCHAPTER 4\nSentence classification\neven a stupid classifier that labels everything as nonspam would be able to achieve an\naccuracy of 90%. As an example, if a “stupid” classifier classifies everything as “non-\nspam” in figure 4.10, it would still achieve an accuracy of 70% (7 out of 10 instances).\nIf you look at this number in isolation, you might be fooled into thinking the perfor-\nmance of the classifier is actually great. When you are using accuracy as a metric, it is\nalways a good idea to compare with the hypothetical, stupid classifier (majority vote) as\na baseline.\n4.3.2\nPrecision and recall\nThe rest of the metrics—precision, recall, and F-measure—are used in a binary classi-\nfication setting. The goal of a binary classification task is to identify one class (called a\npositive class) from the other (called a negative class). In the spam-filtering setting, the\npositive class is spam, whereas the negative class is nonspam.\n The Venn diagram in figure 4.11 contains four subregions: true positives, false pos-\nitives, false negatives, and true negatives. True positives (TP) are instances that are\nEmail 1\nEmail 2\nEmail 3\nEmail 4\nEmail 5\nEmail 6\nEmail 7\nEmail 8\nEmail 9\nEmail 10\nInstances\nLabels\nPredictions\nCorrect?\nSpam\nNonspam\nNonspam\nSpam\nNonspam\nNonspam\nNonspam\nNonspam\nNonspam\nSpam\nSpam\nNonspam\nNonspam\nNonspam\nNonspam\nNonspam\nNonspam\nAccuracy = 8/10 = 80%\nSpam\nNonspam\nSpam\nFigure 4.10\nCalculating accuracy\n\n\n95\nAccuracy, precision, recall, and F-measure\npredicted as positive (= spam) and are indeed in the positive class. False positives (FP)\nare instances that are predicted as positive (= spam) but are actually not in the positive\nclass. These are noises in the prediction, that is, innocent nonspam emails that are mis-\ntakenly caught by the spam filter and end up in the spam folder of your email client.\n On the other hand, false negatives (FN) are instances that are predicted as nega-\ntive but are actually in the positive class. These are spam emails that slip through the\nspam filter and end up in your inbox. Finally, true negatives (TN) are instances that\nare predicted as negative and are indeed in the negative class (nonspam emails in\nyour inbox).\n Precision is the fraction of instances that the model classifies as positive that are\nindeed correct. For example, if your spam filter identifies three emails as spam, and\ntwo of them are indeed spam, the precision will be 2/3, or about 66%. \n Recall is somewhat opposite of precision. It is the fraction of positive instances in\nyour dataset that are identified as positive by your model. Again, using spam filtering\nas an example, if your dataset contains three spam emails and your model identifies\ntwo of them as spam successfully, the recall will be 2/3, or about 66%.\n Figure 4.11 shows the relationship between predicted and true labels as well as\nrecall and precision. \nFigure 4.11\nPrecision and recall\nTrue\npositives\n(TP)\nFalse\npositives\n(FP)\nPredicted\npositives\nTrue\nlabels\nFalse\nnegatives\n(FN)\nTrue\nnegatives\n(TN)\nPrecision =\nTP\nTP + FP\nRecall =\nTP\nTP + FN\nAccuracy =\nTP + TN\nTP + FP + FN + TN\n\n\n96\nCHAPTER 4\nSentence classification\n4.3.3\nF-measure\nYou may have noticed a tradeoff between precision and recall. Imagine there’s a spam\nfilter that is very, very careful in classifying emails. It outputs only one out of several\nthousand emails as spam, but when it does, it is always correct. This is not a difficult\ntask, because some spam emails are pretty obvious—if they contain a word “v1@gra”\nin the text and it’s sent from someone in the spam blacklist, it should be pretty safe to\nmark it as a spam. What would the precision of this spam filter be? 100%. Similarly,\nthere’s another spam filter that is very, very careless in classifying emails. It classifies\nevery single email as spam, including the ones from your colleagues and friends. Its\nrecall? 100%. Would any of these two spam filters be useful? Hardly!\n As you’ve seen, improving precision or recall alone while ignoring the other is not\na good practice, because of the tradeoff between them. It’s like you were looking only\nat your body weight when you are on a diet. You lost 10 pounds? Great! But what if you\nare seven feet tall? Not so much. You need to take into account both your height and\nweight—how much is too much depends on the other variable. That’s why there are\nmeasures like BMI (body mass index) that take both measures into account. Similarly,\nresearchers came up with this metric called F-measure, which is an average (or, more pre-\ncisely speaking, a harmonic mean) of precision and recall. Most often, a special case\ncalled F1-measure is used, which is the equally weighted version of F-measure. In a clas-\nsification setting, it is a good practice to measure and try to maximize the F-measure.\n4.4\nBuilding AllenNLP training pipelines\nIn this section, we are going to revisit the sentiment analyzer we built in chapter 2 and\ndiscuss how to build its training pipeline in more detail. Although I already showed\nthe important steps for building an NLP application using AllenNLP, in this section\nwe will dive deep into some important concepts and abstractions. Understanding\nthese concepts is important not just in using AllenNLP but also in designing NLP\napplications in general, because NLP applications are usually built using these abstrac-\ntions in some way or the other. \n To run the code in this section, you need to import the necessary classes and mod-\nules, as shown in the following code snippet (the code examples in this section can also\nbe accessed via Google Colab, http://www.realworldnlpbook.com/ch2.html#sst-nb):\nfrom itertools import chain\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder, \nPytorchSeq2VecWrapper\n",
      "page_number": 108
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 117-124)",
      "start_page": 117,
      "end_page": 124,
      "detection_method": "topic_boundary",
      "content": "97\nBuilding AllenNLP training pipelines\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, \nBasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training.metrics import CategoricalAccuracy, F1Measure\nfrom allennlp.training.trainer import GradientDescentTrainer\nfrom allennlp_models.classification.dataset_readers.stanford_sentiment_tree_\nbank import \\\n    StanfordSentimentTreeBankDatasetReader\n4.4.1\nInstances and fields\nAs mentioned in section 2.2.1, an instance is the atomic unit for which a prediction is\nmade by a machine learning algorithm. A dataset is a collection of instances of the\nsame form. The first step in most NLP applications is to read in or receive some data\n(e.g., from a file or via network requests) and convert them to instances so that the\nNLP/ML algorithm can consume them.\n AllenNLP supports an abstraction called DatasetReader whose job is to read in\nsome input (raw strings, CSV files, JSON data structures from network requests, and so\non) and convert it to instances. AllenNLP already provides a wide range of dataset read-\ners for major formats used in NLP, such as the CoNLL format (used in popular shared\ntasks for language analysis) and the Penn Treebank (a popular dataset for syntactic\nparsing). To read the Standard Sentiment Treebank, you can use the built-in\nStanfordSentimentTreeBankDatasetReader, which we used earlier in chapter 2.\nYou can also write your own dataset reader just by overriding some core methods from\nDatasetReader.\n The AllenNLP class Instance represents a single instance. An instance can have\none or more fields, which hold some type of data. For example, an instance for the\nsentiment analysis task has two fields—the text body and the label—which can be cre-\nated by passing a dictionary of fields to its constructor as follows:\nInstance({'tokens': TextField(tokens),\n          'label': LabelField(sentiment)})\nHere we assumed that you already created tokens, which is a list of tokens, and sen-\ntiment, a string label corresponding to the sentiment class, from reading the input\nfile. AllenNLP supports other types of fields, depending on the task. \n The read()method of DatasetReader returns an iterator over instances, which\nenables you to enumerate the generated instances and visually check them, as shown\nin the following snippet:\nreader = StanfordSentimentTreeBankDatasetReader()\ntrain_dataset = reader.read('path/to/sst/dataset/train.txt')\ndev_dataset = reader.read('path/to/sst/dataset/dev.txt')\nfor inst in train_dataset + dev_dataset:\n    print(inst)\n\n\n98\nCHAPTER 4\nSentence classification\nIn many cases, you access your dataset readers through data loaders. A data loader is\nan AllenNLP abstraction (which is really a thin wrapper around PyTorch’s data load-\ners) that handles the data and iterates over batched instances. You can specify how\ninstances are sorted, grouped into batches, and fed to the training algorithm by sup-\nplying a batch sampler. Here, we are using a BucketBatchSampler, which does this\nby sorting instances by their length and grouping instances with similar lengths into a\nsingle batch, as shown next: \nreader = StanfordSentimentTreeBankDatasetReader()\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\n4.4.2\nVocabulary and token indexers\nThe second step in many NLP applications is to build the vocabulary. In computer sci-\nence, vocabulary is a theoretical concept that represents the set of all possible words in\na language. In NLP, though, it often means just the set of all unique tokens that\nappeared in a dataset. It is simply impossible to know all the possible words in a lan-\nguage, nor is it necessary for an NLP application. What is stored in a vocabulary is\ncalled a vocabulary item (or just an item). A vocabulary item is usually a word, although\ndepending on the task at hand, it can be any form of linguistic units, including charac-\nters, character n-grams, and labels for linguistic annotation.\n AllenNLP provides a class called Vocabulary. It not only takes care of storing\nvocabulary items that appeared in a dataset, but it also holds mappings between\nvocabulary items and their IDs. As mentioned earlier, neural networks and machine\nlearning models in general can deal only with numbers, and there needs to be a way\nto map discrete items such as words to some numerical representations such as word\nIDs. The vocabulary is also used to map the results of an NLP model back to the origi-\nnal words and labels so that humans can actually read them.\n You can create a Vocabulary object from instances as follows:\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), \n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\nA couple of things to note here: first, because we are dealing with iterators (returned\nby the data loaders’ iter_instances()method), we need to use the chain method\nfrom itertools to enumerate all the instances in both datasets. \n Second, AllenNLP’s Vocabulary class supports namespaces, which are a system to\nseparate different sets of items so that they don’t get mixed up. Here’s why they are\nuseful—say you are building a machine translation system, and you just read a dataset\nthat contains English and French translations. Without namespaces, you’d have just\none set that contains all words in English and French. This is usually not a big issue\n\n\n99\nBuilding AllenNLP training pipelines\nbecause English words (“hi,” “thank you,” “language”) and French words (“bonjour,”\n“merci, “langue”) look quite different in most cases. However, a number of words look\nexactly the same in both languages. For example, “chat” means “talk” in English and\n“cat” in French, but it’s hard to imagine anybody wanting to mix those two words and\nassign the same ID (and embeddings). To avoid this conflict, Vocabulary imple-\nments namespaces and assigns separate sets of items of different types.\n You may have noticed the form_instances() function call has a min_count\nargument. For each namespace, this specifies the minimum number of occurrences in\nthe dataset that is necessary for an item to be included in the vocabulary. All the items\nthat appear less frequently than this threshold are treated as “unknown” items. Here’s\nwhy this is a good idea: in a typical language, a very small number of words appear a\nlot (in English: “the,” “a,” “of”) and a very large number of words appear very infre-\nquently. This usually exhibits a long tail distribution of word frequencies. But it is not\nlikely that these super infrequent words add anything useful to the model, and pre-\ncisely because they appear infrequently, it is difficult to learn any useful patterns from\nthem anyway. Also, because there are so many of them, they inflate the size of the\nvocabulary and the number of model parameters. In such a case, a common practice\nin NLP is to cut this long tail and collapse all the infrequent words to a single entity\n<UNK> (for “unknown” words).\n Finally, a token indexer is an AllenNLP abstraction that takes in a token and returns\nits index, or a list of indices that represent the token. In most cases, there’s a one-to-\none mapping between unique tokens and their indices, but depending on your\nmodel, you may need more advanced ways to index the tokens (such as using charac-\nter n-grams).\n After you create a vocabulary, you can tell the data loaders to index the tokens with\nthe specified vocabulary, as shown in the next code snippet. This means that the\ntokens that the data loaders read from the datasets are converted to integer IDs\naccording to the vocabulary’s mappings:\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n4.4.3\nToken embedders and RNNs\nAfter you index words using a vocabulary and token indexers, you need to convert them\nto embeddings. An AllenNLP abstraction called TokenEmbedder takes word indices as\nan input and produces word embedding vectors as an output. You can embed words\nusing continuous vectors in many ways, but if all you want is to map unique tokens to\nembedding vectors one-to-one, you can use the Embedding class as follows:\ntoken_embedding = Embedding(\n    num_embeddings=vocab.get_vocab_size('tokens'),\n    embedding_dim=EMBEDDING_DIM)\nThis will create an Embedding instance that takes word IDs and converts them to\nfixed-length vectors in a one-to-one fashion. The number of unique words this\n\n\n100\nCHAPTER 4\nSentence classification\ninstance can support is given by num_embeddings, which is equal to the size of the\ntokens vocabulary namespace. The dimensionality of embeddings (i.e., the length of\nembedded vectors) is given by embedding_dim.\n Next, let’s define our RNN and convert a variable-length input (a list of embedded\nwords) to a fixed-length vector representation of the input. As we discussed in section\n4.1, you can think of an RNN as a neural network structure that consumes a sequence\nof things (words) and returns a fixed-length vector. AllenNLP abstracts such models\ninto the Seq2VecEncoder class, and you can create an LSTM RNN by using\nPytorchSeq2VecWrapper as follows:\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nA lot is happening here, but essentially this wraps PyTorch’s LSTM implementation\n(torch.nn.LSTM) and makes it pluggable to the rest of the AllenNLP pipeline. The\nfirst argument to torch.nn.LSTM() is the dimensionality of the input vector, and the\nsecond one is that of LSTM’s internal state. The last one, batch_first, specifies the\nstructure of the input/output tensors for batching, but you usually don’t have to\nworry about its details as long as you are using AllenNLP.\nNOTE\nIn AllenNLP, everything is batch first, meaning that the first dimen-\nsion of any tensor is always equal to the number of instances in a batch.\n4.4.4\nBuilding your own model\nNow that we defined all the subcomponents, we are ready to build the model that exe-\ncutes the prediction. Thanks to AllenNLP’s well-designed abstractions, you can easily\nbuild your model by inheriting AllenNLP’s Model class and overriding the\nforward() method. You don’t usually need to be aware of details such as the shapes\nand dimensions of tensors. The following listing defines the LSTM RNN used for clas-\nsifying sentences.\n@Model.register(\"lstm_classifier\")\nclass LstmClassifier(Model):   \n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n        super().__init__(vocab)\n        self.embedder = embedder\n        self.encoder = encoder\n        self.linear = torch.nn.Linear(   \n            in_features=encoder.get_output_dim(),\n            out_features=vocab.get_vocab_size('labels'))\nListing 4.1\nLSTM sentence classifier\nAllenNLP models \ninherit Model.\nCreates a linear layer to convert the RNN \noutput to a vector of another length\n\n\n101\nBuilding AllenNLP training pipelines\n        positive_index = vocab.get_token_index(\n            positive_label, namespace='labels')\n        self.accuracy = CategoricalAccuracy()\n        self.f1_measure = F1Measure(positive_index)    \n        self.loss_function = torch.nn.CrossEntropyLoss()  \n    def forward(self,    \n                tokens: Dict[str, torch.Tensor],\n                label: torch.Tensor = None) -> torch.Tensor:\n        mask = get_text_field_mask(tokens)\n        embeddings = self.embedder(tokens)\n        encoder_out = self.encoder(embeddings, mask)\n        logits = self.linear(encoder_out)\n        output = {\"logits\": logits}   \n        if label is not None:\n            self.accuracy(logits, label)\n            self.f1_measure(logits, label)\n            output[\"loss\"] = self.loss_function(logits, label)\n        return output\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {'accuracy': self.accuracy.get_metric(reset),    \n                **self.f1_measure.get_metric(reset)}\nEvery AllenNLP Model inherits from PyTorch’s Module class, meaning you can use\nPyTorch’s low-level operations if necessary. This gives you a lot of flexibility in defining\nyour model while leveraging AllenNLP’s high-level abstractions.  \n4.4.5\nPutting it all together\nFinally, we finish this section by implementing the entire pipeline to train the senti-\nment analyzer, as shown next. \nEMBEDDING_DIM = 128\nHIDDEN_DIM = 128\nreader = StanfordSentimentTreeBankDatasetReader()\ntrain_path = 'path/to/sst/dataset/train.txt'\ndev_path = 'path/to/sst/dataset/dev.txt'\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(  \n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\nListing 4.2\nTraining pipeline for the sentiment analyzer\nF1Measure() requires the label \nID for the positive class. '4' \nmeans “very positive.”\nCross-entropy loss \nis used for \nclassification tasks. \nCrossEntropyLoss \ndirectly takes logits \n(no softmax needed).\nInstances are \ndestructed to \nindividual fields \nand passed to \nforward().\nOutput of forward() is a dict, \nwhich contains a “loss” key.\nReturns accuracy, \nprecision, recall, \nand F1-measure \nas the metrics\nDefines how to construct \nthe data loaders\n\n\n102\nCHAPTER 4\nSentence classification\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), \n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\ntoken_embedding = Embedding(\n    num_embeddings=vocab.get_vocab_size('tokens'),\n    embedding_dim=EMBEDDING_DIM)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nmodel = LstmClassifier(word_embeddings, encoder, vocab)   \noptimizer = optim.Adam(model.parameters())   \ntrainer = GradientDescentTrainer(   \n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\ntrainer.train()\nThe training pipeline completes when the Trainer instance is created and invoked\nwith train(). You pass all the ingredients that you need for training—the model,\noptimizer, data loaders, datasets, and a bunch of hyperparameters.\n An optimizer implements an algorithm for adjusting the parameters of the model\nto minimize the loss. Here, we are using one type of optimizer called Adam, which is a\ngood “default” optimizer to use as your first option. However, as I mentioned in chap-\nter 2, you often need to experiment with many different optimizers that work best for\nyour model.\n4.5\nConfiguring AllenNLP training pipelines\nYou may have noticed that very little of listing 4.2 is actually specific to the sentence-\nclassification problem. Indeed, loading datasets, initializing a model, and plugging an\niterator and an optimizer into the trainer are all common steps across almost every\nNLP training pipeline. What if you want to reuse the same training pipeline for many\nrelated tasks without writing the training script from scratch? Also, what if you want to\nexperiment with different sets of configurations (e.g., different hyperparameters, neu-\nral network architectures) and save the exact configurations you tried?\n For those problems, AllenNLP provides a convenient framework where you can\nwrite configuration files in the JSON format. The idea is that you write the specifics of\nyour training pipeline—for example, which dataset reader to use, which models and\nInitializes \nthe model\nDefines the \noptimizer\nInitializes \nthe trainer\n\n\n103\nConfiguring AllenNLP training pipelines\ntheir subcomponents to use, and what hyper-parameters to use for training—in a\nJSON-formatted file (more precisely, AllenNLP uses a format called Jsonnet, which is a\nsuperset of JSON). Instead of rewriting your model file or the training script, you feed\nthe configuration file to the AllenNLP executable, and the framework takes care of\nrunning the training pipeline. If you want to try a different configuration for your\nmodel, you simply change the configuration file (or make a new one) and run the\npipeline again, without changing the Python code. This is a great practice for making\nyour experiment manageable and reproducible. You need to manage only the config-\nuration files and their results—the same configuration always yields the same result.\n A typical AllenNLP configuration file consists of three main parts—the dataset,\nyour model, and the training pipeline. The first part, shown next, specifies which\ndataset files to use and how:\n\"dataset_reader\": {\n    \"type\": \"sst_tokens\"\n  },\n  \"train_data_path\": \"https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/train.txt\",\n  \"validation_data_path\": \"https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/dev.txt\"\nThree keys are in this part: dataset_reader, train_data_path, and valida-\ntion_data_path. The first key, dataset_reader, specifies which DatasetReader\nto use to read the files. Dataset readers, models, and predictors, as well as many other\ntypes of modules in AllenNLP, can be registered using the decorator syntax and be\nreferred to from configuration files. For example, if you peek at the following code\nwhere StanfordSentimentTreeBankDatasetReader is defined\n@DatasetReader.register(\"sst_tokens\")\nclass StanfordSentimentTreeBankDatasetReader(DatasetReader): \n    ...\nyou notice that it is decorated by @DatasetReader.register(\"sst_tokens\").\nThis registers StanfordSentimentTreeBankDatasetReader under the name\nsst_tokens, which allows you to refer it by \"type\": \"sst_tokens\" from the con-\nfiguration files.\n In the second part of the configuration file, you specify the main model to be\ntrained as follows:\n\"model\": {\n    \"type\": \"lstm_classifier\",\n    \"embedder\": {\n      \"token_embedders\": {\n        \"tokens\": {\n          \"type\": \"embedding\",\n          \"embedding_dim\": embedding_dim\n        }\n      }\n    },\n\n\n104\nCHAPTER 4\nSentence classification\n    \"encoder\": {\n      \"type\": \"lstm\",\n      \"input_size\": embedding_dim,\n      \"hidden_size\": hidden_dim\n    }\n}\nAs mentioned before, models in AllenNLP can be registered using the decorator syn-\ntax and be referred from the configuration files via the type key. For example, the\nLstmClassifier class referred here is defined as follows:\n@Model.register(\"lstm_classifier\")\nclass LstmClassifier(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\nOther keys in the model definition JSON dict correspond to the names of the\nparameters of the model constructor. In the previous definition, because Lstm-\nClassifier’s constructor takes two parameters, word_embeddings and encoder\n(in addition to vocab, which is passed by default and can be omitted, and\npositive_label, for which we are going to use the default value), the model defini-\ntion has two corresponding keys, the values of which are also model definitions and\nfollow the same convention.\n In the final part of the configuration file, the data loader and the trainer are speci-\nfied. The convention here is similar to the model definition—you specify the type of\nthe class along with other parameters passed to the constructor as follows:\n  \"data_loader\": {\n    \"batch_sampler\": {\n      \"type\": \"bucket\",\n      \"sorting_keys\": [\"tokens\"],\n      \"padding_noise\": 0.1,\n      \"batch_size\" : 32\n    }\n  },\n  \"trainer\": {\n    \"optimizer\": \"adam\",\n    \"num_epochs\": 20,\n    \"patience\": 10\n  }\nYou can see the full JSON configuration file in the code repository (http://realworld-\nnlpbook.com/ch4.html#sst-json). Once you define the JSON configuration file, you\ncan simply feed it to the allennlp command as follows:\nallennlp train examples/sentiment/sst_classifier.jsonnet \\\n    --serialization-dir sst-model \\\n    --include-package examples.sentiment.sst_classifier\n",
      "page_number": 117
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 125-134)",
      "start_page": 125,
      "end_page": 134,
      "detection_method": "topic_boundary",
      "content": "105\nCase study: Language detection\nThe --serialization-dir specifies where the trained model (along with addi-\ntional information such as serialized vocabulary data) is going to be stored. You also\nneed to specify the module path to LstmClassifier using --include-package so\nthat the configuration file can find the registered class.\n As we saw in chapter 2, when the training is finished, you can launch a simple web-\nbased demo interface using the following command:\n$ allennlp serve \\ \n    --archive-path sst-model/model.tar.gz \\\n    --include-package examples.sentiment.sst_classifier \\\n    --predictor sentence_classifier_predictor \\\n    --field-name sentence\n4.6\nCase study: Language detection\nIn this final section of the chapter, we are going to discuss another scenario—\nlanguage detection—which can also be formulated as a sentence-classification task. A\nlanguage-detection system, given a piece of text, detects the language the text is writ-\nten in. It has a wide range of uses in other NLP applications. For example, a web\nsearch engine may want to detect the language a web page is written in before process-\ning and indexing it. Google Translate also switches the source language automatically\nbased on what is typed in the input textbox.\n Let’s see what this actually looks like. Can you tell the language of each of the fol-\nlowing lines? These sentences are all taken from the Tatoeba project (https://\ntatoeba.org/).\n Contamos con tu ayuda.\n Bitte überleg es dir.\n Parti için planları tartıştılar.\n Je ne sais pas si je peux le faire.\n Você estava em casa ontem, não estava?\n Ĝi estas rapida kaj efika komunikilo.\n Ha parlato per un'ora.\n Szeretnék elmenni és meginni egy italt.\n Ttwaliɣ nezmer ad nili d imeddukal.\n \nThe answer is: Spanish, German, Turkish, French, Portuguese, Esperanto, Italian,\nHungarian, and Berber. I chose them from the top 10 most popular languages on\nTatoeba that are written in the roman alphabet. You may not be familiar with some of\nthe languages listed here. For those of you who are not, Esperanto is a constructed\nauxiliary language invented in the late 19th century. Berber is actually a group of\nrelated languages spoken in some parts of North Africa that are cousins of Semitic lan-\nguages such as Arabic.\n Maybe you were able to recognize some of these languages, even though you\ndon’t actually speak them. I’d like you to step back and think how you did it. It’s quite\n\n\n106\nCHAPTER 4\nSentence classification\ninteresting that people can do this without actually being able to speak the language,\nbecause these languages are all written in the roman alphabet and could look quite\nsimilar to each other. You may have recognized some unique diacritic marks (accents)\nfor some of the languages—for example, “ü” for German and “ã” for Portuguese.\nThese are a strong clue for these languages. Or you just knew some words—for exam-\nple, “ayuda” for Spanish (meaning “help”) and “pas” in French (“ne . . . pas” is a\nFrench negation syntax). It appears that every language has its own characteristics—\nbe it some unique characters or words—that makes it easy to tell it apart from others.\nThis is starting to sound a lot like a kind of problem that machine learning is good at\nsolving. Can we build an NLP system that can do this automatically? How should we go\nabout building it?\n4.6.1\nUsing characters as input\nA language detector can also be built in a similar way to the sentiment analyzer. You\ncan use an RNN to read the input text and convert it to some internal representation\n(hidden states). You can then use a linear layer to convert them to a set of scores cor-\nresponding to how likely the text is written in each language. Finally, you can use\ncross-entropy loss to train the model.\n One major difference between the sentiment analyzer and the language detector\nis how you feed the input into an RNN. When building the sentiment analyzer, we\nused the Stanford Sentiment Treebank and were able to assume that the input text is\nalways English and already tokenized. But this is not the case for language detection.\nIn fact, you don’t even know whether the input text is written in a language that can\nbe tokenized easily—what if the sentence is written in Chinese? Or in Finnish, which is\ninfamous for its complex morphology? You could use a tokenizer that is specific to the\nlanguage if you know what language it is, but we are building the language detector\nbecause we don’t know what language it is in the first place. This sounds like a typical\nchicken-and-egg problem.\n To address this issue, we are going to use characters instead of tokens as the input\nto an RNN. The idea is to break down the input into individual characters, even includ-\ning whitespace and punctuation, and feed them to the RNN one at a time. Using char-\nacters is a common practice used when the input can be better represented as a\nsequence of characters (such as Chinese, or of an unknown origin), or when you’d like\nto make the best use of internal structures of words (such as the fastText model we men-\ntioned in chapter 3). The RNN’s powerful representational power can still capture inter-\nactions between characters and some common words and n-grams mentioned earlier.\n4.6.2\nCreating a dataset reader\nFor this language-detection task, I created the train and the validation datasets from\nthe Tatoeba project by taking the 10 most popular languages on Tatoeba that use the\nroman alphabet and by sampling 10,000 sentences for the train set and 1,000 for the\nvalidation set. An excerpt of this dataset follows:\n\n\n107\nCase study: Language detection\npor\nDe entre os designers, ele escolheu um jovem ilustrador e deu-lhe a \ntarefa.\npor\nA apresentação me fez chorar.\ntur\nBunu denememize gerek var mı?\ntur\nO korkutucu bir parçaydı.\nber\nTebḍamt aɣrum-nni ɣef sin, naɣ?\nber\nAd teddud ad twalid taqbuct n umaḍal n tkurt n uḍar deg Brizil?\neng\nTom works at Harvard.\neng\nThey fixed the flat tire by themselves.\nhun\nAz arca hirtelen elpirult.\nhun\nMiért aggodalmaskodsz? Hiszen még csak egy óra van!\nepo\nSidiĝu sur la benko.\nepo\nTiu ĉi kutime funkcias.\nfra\nVu d'avion, cette île a l'air très belle.\nfra\nNous boirons à ta santé.\ndeu\nDas Abnehmen fällt ihm schwer.\ndeu\nTom war etwas besorgt um Maria.\nita\nSono rimasto a casa per potermi riposare.\nita\nLe due più grandi invenzioni dell'uomo sono il letto e la bomba atomica: \nil primo ti tiene lontano dalle noie, la seconda le elimina.\nspa\nHe visto la película.\nspa\nHas hecho los deberes.\nThe first field is a three-letter language code that describes which language the text is\nwritten in. The second field is the text itself. The fields are delimited by a tab character.\nYou can obtain the datasets from the code repository (https://github.com/mhagiwara/\nrealworldnlp/tree/master/data/tatoeba).\n The first step in building a language detector is to prepare a dataset reader that can\nread datasets in this format. In the previous example (the sentiment analyzer), because\nAllenNLP already provides StanfordSentimentTreeBankDatasetReader, you\njust needed to import and use it. In this scenario, however, you need to write your\nown. Fortunately, writing a dataset reader that can read this particular format is not\nthat difficult. To write a dataset reader, you just need to do the following three things:\nCreate your own dataset reader class by inheriting DatasetReader.\nOverride the text_to_instance() method that takes raw text and converts it\nto an instance object.\nOverride the_read() method that reads the content of a file and yields\ninstances, by calling text_to_instance() above.\nThe complete dataset reader for the language detector is shown in listing 4.3. We also\nassume that you already imported necessary modules and classes as follows:\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\n\n\n108\nCHAPTER 4\nSentence classification\nfrom allennlp.data.dataset_readers import DatasetReader\nfrom allennlp.data.fields import LabelField, TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training import GradientDescentTrainer\nfrom overrides import overrides\nfrom examples.sentiment.sst_classifier import LstmClassifier\nclass TatoebaSentenceReader(DatasetReader):    \n    def __init__(self,\n                 token_indexers: Dict[str, TokenIndexer]=None):\n        super().__init__()\n        self.tokenizer = CharacterTokenizer()   \n        self.token_indexers = token_indexers or {'tokens': \nSingleIdTokenIndexer()}\n    @overrides\n    def text_to_instance(self, tokens, label=None):  \n        fields = {}\n        fields['tokens'] = TextField(tokens, self.token_indexers)\n        if label:\n            fields['label'] = LabelField(label)\n        return Instance(fields)\n    @overrides\n    def _read(self, file_path: str):\n        file_path = cached_path(file_path)   \n        with open(file_path, \"r\") as text_file:\n            for line in text_file:\n                lang_id, sent = line.rstrip().split('\\t')\n                tokens = self.tokenizer.tokenize(sent)\n                yield self.text_to_instance(tokens, lang_id)    \nNote that the dataset reader in listing 4.3 uses CharacterTokenizer() to tokenize\ntext into characters. Its tokenize() method returns a list of tokens, which are Allen-\nNLP objects that represent tokens but actually contain characters in this scenario. \n4.6.3\nBuilding the training pipeline\nOnce you build the dataset reader, the rest of the training pipeline looks similar to\nthat of the sentiment analyzer. In fact, we can reuse the LstmClassifier class we\nListing 4.3\nDataset reader for the language detector \nEvery new dataset \nreader inherits \nDatasetReader.\nUses CharacterTokenizer() \nto tokenize text into \ncharacters\nLabel will be None \nat test time.\nIf file_path is an URL, returns the \nactual path to a cached file on disk\nYields instances using\ntext_to_instance(),\ndefined earlier\n\n\n109\nCase study: Language detection\ndefined previously without any modification. The entire training pipeline is shown in\nlisting 4.4. You can access the Google Colab notebook of the entire code from here:\nhttp://realworldnlpbook.com/ch4.html#langdetect.\nEMBEDDING_DIM = 16\nHIDDEN_DIM = 16\nreader = TatoebaSentenceReader()\ntrain_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/tatoeba/\nsentences.top10langs.train.tsv'\ndev_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/tatoeba/\nsentences.top10langs.dev.tsv'\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\nvocab = Vocabulary.from_instances(train_data_loader.iter_instances(),\n                                  min_count={'tokens': 3})\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_DIM)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nmodel = LstmClassifier(word_embeddings,\n                       encoder,\n                       vocab,\n                       positive_label='eng')\ntrain_dataset.index_with(vocab)\ndev_dataset.index_with(vocab)\noptimizer = optim.Adam(model.parameters())\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\ntrainer.train()\nListing 4.4\nTraining pipeline for the language detector\n\n\n110\nCHAPTER 4\nSentence classification\nWhen you run this training pipeline, you’ll get the metrics on the dev set that are in\nthe ballpark of the following:\naccuracy: 0.9461, precision: 0.9481, recall: 0.9490, f1_measure: 0.9485, \nloss: 0.1560\nThis is not bad at all! This means that the trained detector makes only one mistake out\nof about 20 sentences. Precision of 0.9481 means there’s only one false positive (non-\nEnglish sentence) out of 20 instances that are classified as English. Recall of 0.9490\nmeans there’s only one false negative (English sentence that was missed by the detec-\ntor) out of 20 true English instances.\n4.6.4\nRunning the detector on unseen instances\nFinally, let’s try running the detector we just trained on a set of unseen instances\n(instances that didn’t appear either in the train or the validation sets). It is always a\ngood idea to try feeding a small number of instances to your model and observe how\nit behaves. \n The recommended way for feeding instances into a trained AllenNLP model is to\nuse a predictor, as we did in chapter 2. But here I’d like to do something simpler and\ninstead write a method that, given a piece of text and a model, runs the prediction\npipeline. To run a model on arbitrary instances, you can call the model’s forward_\non_instances() method, as shown in the following snippet:\ndef classify(text: str, model: LstmClassifier):\n    tokenizer = CharacterTokenizer()\n    token_indexers = {'tokens': SingleIdTokenIndexer()}\n    tokens = tokenizer.tokenize(text)\n    instance = Instance({'tokens': TextField(tokens, token_indexers)})\n    logits = model.forward_on_instance(instance)['logits']\n    label_id = np.argmax(logits)\n    label = model.vocab.get_token_from_index(label_id, 'labels')\n    print('text: {}, label: {}'.format(text, label))\nThis method first takes the input (text and model) and passes it through a tokenizer\nto create an instance object. Then it calls model’s forward_on_instance() method\nto retrieve the logits, the scores for target labels (languages). It gets the label ID that\ncorresponds to the maximum logit value by calling np.argmax and then converts it to\nthe label text by using the vocabulary object associated with the model. \n When I ran this method on some sentences that are not in the two datasets, I got\nthe following results. Note that the result you get may be different from mine due to\nsome randomness:\ntext: Take your raincoat in case it rains., label: fra\ntext: Tu me recuerdas a mi padre., label: spa\ntext: Wie organisierst du das Essen am Mittag?, label: deu\ntext: Il est des cas où cette règle ne s'applique pas., label: fra\ntext: Estou fazendo um passeio em um parque., label: por\n\n\n111\nSummary\ntext: Ve, postmorgaŭ jam estas la limdato., label: epo\ntext: Credevo che sarebbe venuto., label: ita\ntext: Nem tudja, hogy én egy macska vagyok., label: hun\ntext: Nella ur nli qrib acemma deg tenwalt., label: ber\ntext: Kurşun kalemin yok, deǧil mi?, label: tur\nThese predictions are almost perfect, except the very first sentence—it is English, not\nFrench. It is surprising that the model makes a mistake on such a seemingly easy sen-\ntence while it predicts more difficult languages (such as Hungarian) perfectly. But\nremember, how difficult the language is for English speakers has nothing to do with\nhow difficult it is for a computer to classify. In fact, some of the “difficult” languages\nsuch as Hungarian and Turkish here have very clear signals (accent marks and unique\nwords) that make it easy to detect them. On the other hand, lack of clear signals in the\nfirst sentence may have made it more difficult to classify it from other languages. \n As a next step, you could try a couple of things: for example, you can tweak some\nof the hyperparameters to see how the evaluation metrics and the final prediction\nresults change. You can also try a larger number of test instances to see how exactly\nthe mistakes are distributed (e.g., between which two languages). You can also zero in\non some of the instances and see why the model made such mistakes. These are all\nimportant practices when you are working on real-world NLP applications. I’ll discuss\nthese topics in detail in chapter 10.\nSummary\nA recurrent neural network (RNN) is a neural network with a loop. It can trans-\nform a variable-length input to a fixed-length vector.\nNonlinearity is a crucial component that makes neural networks truly powerful.\nLSTMs and GRUs are two variants of RNN cells and are easier to train than\nvanilla RNNs.\nYou use accuracy, precision, recall, and F-measure for classification problems.\nAllenNLP provides useful NLP abstractions such as dataset readers, instances,\nand vocabulary. It also provides a way to configure the training pipeline in the\nJSON format.\nYou can build a language detector as a sentence-classification application simi-\nlar to the sentiment analyzer.\n\n\n112\nSequential labeling\nand language modeling\nIn this chapter, we are going to discuss sequential labeling—an important NLP\nframework where systems tag individual words with corresponding labels. Many\nNLP applications, such as part-of-speech tagging and named entity recognition,\ncan be framed as sequential-labeling tasks. In the second half of the chapter, I’ll\nintroduce the concept of language models, one of the most fundamental yet excit-\ning topics in NLP. I’ll talk about why they are important and how to use them to\nevaluate and even generate some natural language text.\nThis chapter covers\nSolving part-of-speech (POS) tagging and named \nentity recognition (NER) using sequential labeling\nMaking RNNs more powerful—multilayer and \nbidirectional recurrent neural networks (RNNs)\nCapturing statistical properties of language using \nlanguage models\nUsing language models to evaluate and generate \nnatural language text\n\n\n113\nIntroducing sequential labeling\n5.1\nIntroducing sequential labeling\nIn the previous chapter, we discussed sentence classification, where the task is to\nassign some label to a given sentence. Spam filtering, sentiment analysis, and lan-\nguage detection are some concrete examples of sentence classification. Although\nmany real-world NLP problems can be formulated as a sentence-classification task,\nthis method can also be quite limited, because the model, by definition, allows us to\nassign only a single label to the whole sentence. But what if you wanted something\nmore granular? For example, what if you wanted to do something with individual\nwords, not just with the sentence? The most typical scenario you encounter is when\nyou want to extract something from the sentence, which cannot be easily solved by\nsentence classification. This is where sequential labeling comes into play.\n5.1.1\nWhat is sequential labeling?\nSequential labeling is an NLP task where, given a sequence such as a sentence, the NLP\nsystem assigns a label to each element (e.g., word) of the input sequence. This con-\ntrasts with sentence classification, where a label is assigned just to the input sentence.\nFigure 5.1 illustrates this contrast.\nFigure 5.1\nSentence classification vs. sequential labeling\nBut why is this even a good idea? When do we need a label per word? A typical scenario\nwhere sequential labeling comes in handy is when you want to analyze a sentence and\nproduce some linguistic information per word. For example, part-of-speech (POS) tag-\nging, which I mentioned in chapter 1, produces a POS tag such as nouns, verbs, and\nprepositions for each word in the input sentence and is a perfect match for sequential\nlabeling. See figure 5.2 for an illustration.\n POS tagging is one of the most fundamental, important NLP tasks. Many English\nwords (and words in many other languages as well) are ambiguous, meaning that they\nhave multiple possible interpretations. For example, the word “book” can be used to\ndescribe a physical or electronic object consisting of pages (“I read a book”) or an\nSentence classification\nSequential labeling\nInput sentence\nword1\nword2\nword3\nwordn\nlabel\n...\nInput sentence\nlabel1\nword1\nword2\nword3\nwordn\n...\nlabel2\nlabel3\nlabeln\n\n\n114\nCHAPTER 5\nSequential labeling and language modeling\naction for reserving something (“I need to book a flight”). Downstream NLP tasks, such\nas parsing and classification, benefit greatly by knowing what each appearance of\n“book” actually means to process the input sentence. If you were to build a speech syn-\nthesis system, you must know the POS of certain words to pronounce them correctly—\n“lead” as a noun (a kind of metal) rhymes with “bed,” whereas “lead” as a verb (to direct,\nguide) rhymes with “bead.” POS tagging is an important first step toward solving this\nambiguity.\n Another scenario is when you want to extract some pieces of information from a\nsentence. For example, if you want to extract subsequences (phrases) such as noun\nphrases and verb phrases, this is also a sequential-labeling task. How can you achieve\nextraction using labeling? The idea is to mark the beginning and the end (or the\nbeginning and the continuation, depending on how you represent it) of the desired\npiece of information using labeling. An example of this is named entity recognition\n(NER), which is a task to identify mentions to real-world entities, such as proper\nnouns and numerical expressions, from a sentence (illustrated in figure 5.3.). \nFigure 5.3\nNamed entity recognition (NER) using sequential labeling\nNotice that all the words that are not part of any named entities are tagged as O (for\n“Outside”). For now, you can ignore some cryptic labels in figure 5.3 such as B-GPE\nand I-MONEY. I’ll talk more about how to formulate NER as a sequential-labeling\nproblem in section 5.4.\nPart-of-speech (POS) tagging\nInput sentence\nPOS tags\nNOUN\nTime\nflies\nlike\nVERB\nADP\nan\nDET\narrow\nNOUN\n.\nPUNKT\nFigure 5.2\nPart-of-speech (POS) \ntagging using sequential labeling\nNamed entity recognition (NER)\nApple\nis\nlooking\nto\nbuy\nUK\nstartup for\n$1\nbillion\nB-ORG\nO\nO\nO\nO\nB-GPE\nO\nO\nB-MONEYI-MONEY\nInput sentence\nNER tags\n",
      "page_number": 125
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 135-142)",
      "start_page": 135,
      "end_page": 142,
      "detection_method": "topic_boundary",
      "content": "115\nIntroducing sequential labeling\n5.1.2\nUsing RNNs to encode sequences\nIn sentence classification, we used recurrent neural networks (RNNs) to convert an\ninput of variable length into a fixed-length vector. The fixed-length vector, which is\nconverted to a set of “scores” by a linear layer, captures the information about the\ninput sentence that is necessary for deriving the sentence label. As a reminder, what\nthis RNN does can be represented by the following pseudocode and the diagram\nshown in figure 5.4: \ndef rnn_vec(words):\n    state = init_state()\n    for word in words:\n        state = update(state, word)\n    return state\nFigure 5.4\nRecurrent neural network (RNN) for sentence classification\nWhat kind of neural network could be used for sequential tagging? We seem to need\nsome information for every input word in the sentence, not just at the end. If you look\nat the pseudocode for rnn_vec() carefully, you can notice that we already have infor-\nmation for every word in the input, which is captured by state. The function just\nhappens to return only the final value of state, but there is no reason we can’t store\nintermediate values of state and return them as a list instead, as in the following\nfunction:\ndef rnn_seq(words):\n    state = init_state()\n    states = []\n    for word in words:\n        state = update(state, word)\n        states.append(state)\n    return states\ninit_state()\nstate\nupdate\nupdate\nupdate\nupdate\nv(\"john\")\nv(\"loves\")\nv(\"mary\")\nv(\".\")\nRecurrent neural network (RNN)\n\n\n116\nCHAPTER 5\nSequential labeling and language modeling\nIf you apply this function to the “time flies” example shown in figure 5.2 and unroll\nit—that is, write it without using a loop—it will look like the following:\nstate = init_state()\nstates = []\nstate = update(state, v(\"time\"))\nstates.append(state)\nstate = update(state, v(\"flies\"))\nstates.append(state)\nstate = update(state, v(\"like\"))\nstates.append(state)\nstate = update(state, v(\"an\"))\nstates.append(state)\nstate = update(state, v(\"arrow\"))\nstates.append(state)\nstate = update(state, v(\".\"))\nstates.append(state)\nNote that v() here is a function that returns the embedding for the given word. This\ncan be visualized as shown in figure 5.5. Notice that for each input word word, the net-\nwork produces the corresponding state that captures some information about word.\nThe length of the list states is the same as that of words. The final value of states,\nthat is, states[-1], is identical to the return value of rnn_vec() from earlier.\nFigure 5.5\nRecurrent neural network (RNN) for sequential labeling\nIf you think of this RNN as a black box, it takes a sequence of something (e.g., word\nembeddings) and converts it to a sequence of vectors that encode some information\nabout individual words in the input, so this architecture is called a Seq2Seq (for\n“sequence-to-sequence”) encoder in AllenNLP.\n The final step is to apply a linear layer to each state of this RNN to derive a set of\nscores that correspond to how likely each label is. If this is a part-of-speech tagger, we\nRecurrent \nneural \nnetwork \n(RNN)\ninit_state()\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\"arrow\")\nv(\".\")\nstate\nstate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\nstate\nstate\nstate\n\n\n117\nIntroducing sequential labeling\nneed one score for the label NOUN,\nanother for VERB, and so on for\neach and every word. This conver-\nsion is illustrated in figure 5.6. Note\nthat the same linear layer (with the\nsame set of parameters) is applied to\nevery state.\n To sum up, we can use almost the\nsame structure for sequential label-\ning as the one we used for sentence\nclassification. The only difference is\nthe former produces a hidden state\nper each word, not just per sentence.\nTo derive scores used for determin-\ning labels, a linear layer is applied to\nevery hidden state. \n5.1.3\nImplementing a Seq2Seq encoder \nin AllenNLP\nAllenNLP implements an abstract\nclass called Seq2SeqEncoder for\nabstracting all Seq2Seq encoders that\ntake in a sequence of vectors and\nreturn another sequence of modified\nvectors. In theory, you can inherit the\nclass and implement your own Seq2Seq encoder. In practice, however, you most likely\nwill use one of the off-the-shelf implementations that PyTorch/AllenNLP provide, such\nas LSTM and GRU. Remember, when we built the encoder for the sentiment analyzer,\nwe used PyTorch’s built-in torch.nn.LSTM and wrapped it with PytorchSeq2Vec-\nWrapper, as shown next, which makes it compatible with AllenNLP’s abstraction:\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nAllenNLP also implements PytorchSeq2SeqWrapper, which takes one of PyTorch’s\nbuilt-in RNN implementations and makes it compliant with AllenNLP’s Seq2Seq-\nEncoder, so there’s very little change you need to initialize a Seq2Seq encoder, as\nshown here:\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nThat’s it! There are a couple more things to note, but there’s surprisingly few changes\nyou need to make to the sentence classification code to make it work for sequential\nlabeling. This is thanks to the powerful abstraction of AllenNLP—most of the time\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nstate\nstate\nlogits\nRNN\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\n \n…\nNOUN VERB …\nFigure 5.6\nApplying a linear layer to RNN\n\n\n118\nCHAPTER 5\nSequential labeling and language modeling\nyou need to worry only about how individual components interact with each other,\nnot about how these components work internally. \n5.2\nBuilding a part-of-speech tagger\nIn this section, we are going to build our first sequential-labeling application—a part-\nof-speech (POS) tagger. You can see the entire code for this section on the Google\nColab notebook (http://realworldnlpbook.com/ch5.html#pos-nb). We assume that\nyou have already imported all necessary dependencies as follows:\nfrom itertools import chain\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, \nPytorchSeq2SeqWrapper\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, \nBasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask, \nsequence_cross_entropy_with_logits\nfrom allennlp.training.metrics import CategoricalAccuracy\nfrom allennlp.training import GradientDescentTrainer\nfrom \nallennlp_models.structured_prediction.dataset_readers.universal_dependen\ncies import UniversalDependenciesDatasetReader\nfrom realworldnlp.predictors import UniversalPOSPredictor\n5.2.1\nReading a dataset\nAs we saw in chapter 1, a part of speech (POS) is a category of words that share similar\ngrammatical properties. Part-of-speech tagging is the process of tagging each word in\na sentence with a corresponding part-of-speech tag. A training set for POS tagging fol-\nlows a tagset, which is a set of predefined POS tags for the language.\n To train a POS tagger, we need a dataset where every word in every sentence is\ntagged with its corresponding POS tag. In this experiment, we are going to use the\nEnglish Universal Dependencies (UD) dataset. Universal Dependencies is a language-\nindependent dependency grammar framework developed by a group of researchers.\nUD also defines a tagset called the Universal part-of-speech tagset (http://realworldnlp-\nbook.com/ch1.html#universal-pos). The use of UD and the Universal POS tagset has\nbeen very popular in the NLP community, especially for language-independent tasks\nand models such as POS tagging and parsing.\n\n\n119\nBuilding a part-of-speech tagger\n We are going to use one subcorpus of UD called A Gold Standard Universal Depen-\ndencies Corpus for English, which is built on top of the English Web Treebank (EWT)\n(http://realworldnlpbook.com/ch5.html#ewt) and can be used under a Creative\nCommons license. You can download the entire dataset from the dataset page (http://\nrealworldnlpbook.com/ch5.html#ewt-data), if needed.\n Universal Dependencies datasets are distributed in a format called the CoNLL-U\nformat (http://universaldependencies.org/docs/format.html). The AllenNLP models\npackage already implements a dataset reader called UniversalDependencies-\nDatasetReader that reads datasets in this format and returns a collection of\ninstances that include information like word forms, POS tags, and dependency rela-\ntionship, so all you need to do is initialize and use it as follows:\nreader = UniversalDependenciesDatasetReader()\ntrain_path = ('https:/./s3.amazonaws.com/realworldnlpbook/data/'\n              'ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu')\ndev_path = ('https:/./s3.amazonaws.com/realworldnlpbook/'\n            'data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-dev.conllu')\nAlso, don’t forget to initialize data loaders and a Vocabulary instance, too, as shown\nnext:\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"words\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(),\n                                        dev_data_loader.iter_instances()))\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n5.2.2\nDefining the model and the loss\nThe next step for building a POS tagger is to define the model. In the previous sec-\ntion, we already saw that you can initialize a Seq2Seq encoder with very little modifica-\ntion using AllenNLP’s built-in PytorchSeq2VecWrapper. Let’s define other\ncomponents (word embeddings and LSTM) and some variables necessary for the\nmodel as follows:\nEMBEDDING_SIZE = 128\nHIDDEN_SIZE = 128\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_SIZE)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\nlstm = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))\n\n\n120\nCHAPTER 5\nSequential labeling and language modeling\nNow we are ready to define the body of the POS tagger model, as shown here.\nclass LstmTagger(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2SeqEncoder,\n                 vocab: Vocabulary) -> None:\n        super().__init__(vocab)\n        self.embedder = embedder\n        self.encoder = encoder\n        \n        self.linear = torch.nn.Linear(\n            in_features=encoder.get_output_dim(),\n            out_features=vocab.get_vocab_size('pos'))\n        \n        self.accuracy = CategoricalAccuracy()    \n    def forward(self,\n                words: Dict[str, torch.Tensor],\n                pos_tags: torch.Tensor = None,\n                **args) -> Dict[str, torch.Tensor]:   \n        mask = get_text_field_mask(words)\n        embeddings = self.embedder(words)\n        encoder_out = self.encoder(embeddings, mask)\n        tag_logits = self.linear(encoder_out)\n        output = {\"tag_logits\": tag_logits}\n        if pos_tags is not None:\n            self.accuracy(tag_logits, pos_tags, mask)\n            output[\"loss\"] = sequence_cross_entropy_with_logits(\n                tag_logits, pos_tags, mask)   \n        return output\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {\"accuracy\": self.accuracy.get_metric(reset)}\nNotice that the code shown in listing 5.1 is very similar to the code for LstmClassi-\nfier (listing 4.1), which we used for building a sentiment analyzer. In fact, except for\nsome naming differences, only one fundamental difference exists—the type of loss\nfunction.\n Recall that we used a loss function called cross entropy for sentence classification\ntasks, which basically measures how far apart two distributions are. If the model pro-\nduces a high probability for the true label, the loss will be low. Otherwise, it will be\nhigh. But this assumed that there is only one label per sentence. How can we measure\nhow far the prediction is from the true label when there is one label per word?\nListing 5.1\nPOS tagger model\nWe use accuracy to \nevaluate the POS tagger.\nWe need **args to capture \nunnecessary instance \nfields that AllenNLP \nautomatically destructures.\nThe Seq2Seq encoder is\ntrained using a sequence\ncross-entropy loss.\n\n\n121\nBuilding a part-of-speech tagger\n The answer is: still use the\ncross entropy, but average it\nover all the elements in the\ninput sequence. For POS tag-\nging, you compute the cross\nentropy per word as if it were\nan \nindividual \nclassification\ntask, sum it over all the words\nin the input sentence, and\ndivide by the length of the sen-\ntence. This will give you a num-\nber reflecting how well your\nmodel is predicting the POS\ntags for the input sentence on\naverage. See figure 5.7 for an\nillustration.\n As for the evaluation met-\nric, POS taggers are usually\nevaluated \nusing \naccuracy,\nwhich we are going to use here.\nAverage human performance\non POS tagging is around 97%,\nwhereas \nthe \nstate-of-the-art\nPOS taggers slightly outper-\nform this (http://realworldnlp\nbook.com/ch5.html#pos-sota).\nYou need to note that accuracy\nis not without a problem, however—assume there is a relatively rare POS tag (e.g.,\nSCONJ, which means subordinating conjugation), which accounts for only 2% of total\ntokens, and a POS tagger messes it up every time it appears. If the tagger gets the rest\nof the tokens all correct, it still achieves 98% accuracy. \n5.2.3\nBuilding the training pipeline\nNow we are ready to move on to building the training pipeline. As with the previous\ntasks, training pipelines in AllenNLP look very similar to each other. See the next list-\ning for the training code.\nmodel = LstmTagger(word_embeddings, encoder, vocab)\noptimizer = optim.Adam(model.parameters())\nListing 5.2\nTraining pipeline for POS tagger\nstate\nstate\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\n \nLinear\nlayer\n \nCross\nentropy\n \nLinear\nlayer\n \nRNN\nlabel\nCross\nentropy\n \nlabel\nCross\nentropy\n \nlabel\nlogits\n+\n+\n=\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nTotal\nloss\nFigure 5.7\nComputing loss for sequence\n\n\n122\nCHAPTER 5\nSequential labeling and language modeling\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=10,\n    cuda_device=-1)\ntrainer.train()\nWhen you run this code, AllenNLP alternates between two phases: 1) training the\nmodel using the train set, and 2) evaluating it using the validation set for each epoch,\nwhile monitoring the loss and accuracy on both sets. Validation set accuracy plateaus\naround 88% after several epochs. After the training is over, you can run the model for\nan unseen instance as shown next:\npredictor = UniversalPOSPredictor(model, reader)\ntokens = ['The', 'dog', 'ate', 'the', 'apple', '.']\nlogits = predictor.predict(tokens)['tag_logits']\ntag_ids = np.argmax(logits, axis=-1)\nprint([vocab.get_token_from_index(tag_id, 'pos') for tag_id in tag_ids])\nThis code uses UniversalPOSPredictor, a predictor that I wrote for this particular\nPOS tagger. Although its details are not important, you can look at its code if you are\ninterested (http://realworldnlpbook.com/ch5#upos-predictor). If successful, this will\nshow a list of POS tags: ['DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'PUNCT'],\nwhich is indeed a correct POS tag sequence for the input sentence.\n5.3\nMultilayer and bidirectional RNNs\nAs we’ve seen so far, RNNs are a powerful tool for building NLP applications. In this\nsection, I talk about their structural variants—multilayer and bidirectional\nRNNs—which are even more powerful components for building highly accurate NLP\napplications.\n5.3.1\nMultilayer RNNs\nIf you look at an RNN as a black box, it is a neural network structure that converts a\nsequence of vectors (word embeddings) into another sequence of vectors (hidden\nstates). The input and output sequences are of the same length, usually the number of\ninput tokens. This means that you can repeat this “encoding” process multiple times by\nstacking RNNs on top of each other. The output (hidden states) of one RNN becomes\nthe input of another RNN that is just above the previous one. A substructure (such as\na single RNN) of a bigger neural network is called a layer, because you can stack them\ntogether like layers. The structure of a two-layered RNN is shown in figure 5.8.\n",
      "page_number": 135
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 143-151)",
      "start_page": 143,
      "end_page": 151,
      "detection_method": "topic_boundary",
      "content": "123\nMultilayer and bidirectional RNNs\nWhy is this a good idea? If you think of a layer of RNN as a machine that takes in some-\nthing concrete (e.g., word embeddings) and extracts some abstract concepts (e.g.,\nscores for POS tags), you can expect that, by repeating this process, RNNs are able to\nextract increasingly more abstract concepts as the number of layers increases.\nAlthough not fully theoretically proven, many real-world NLP applications use multi-\nlayer RNNs. For example, Google’s Neural Machine Translation (NMT) system uses a\nstacked RNN consisting of eight layers for both the encoder and the decoder (http://\nrealworldnlpbook.com/ch5.html#nmt-paper).\n To use multilayer RNNs in your NLP application, all you need to do is change how\nthe encoder is initialized. Specifically, you need to specify only the number of layers\nLayer 1\nLayer 2\ninit_state()\n...\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\ninit_state()\nstate\nstate\nstate\nstate\nstate\nlogits\nLinear\nlayer\n \n…\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\"arrow\")\nv(\".\")\nNOUN VERB …\nFigure 5.8\nTwo-layered RNN\n\n\n124\nCHAPTER 5\nSequential labeling and language modeling\nusing the num_layers parameter, as shown in the next code snippet, and AllenNLP\nmakes sure that the rest of the training pipeline works as-is:\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(\n        EMBEDDING_SIZE, HIDDEN_SIZE, num_layers=2, batch_first=True))\nIf you change this line and rerun the POS tagger training pipeline, you will notice that\naccuracy on the validation set is almost unchanged or slightly lower than the previous\nmodel with a single-layer RNN. This is not surprising—information required for POS\ntagging is mostly superficial, such as the identity of the word being tagged and neigh-\nboring words. Very rarely does it require deep understanding of the input sentence.\nOn the other hand, adding layers to an RNN is not without additional cost. It slows\ndown the training and inference and increases the number of parameters, which\nmakes it susceptible to overfitting. For this small experiment, adding layers to the\nRNN seems to do more harm than good. When you change the structure of the net-\nwork, always remember to verify its effect on a validation set.\n5.3.2\nBidirectional RNNs\nSo far, we’ve been feeding words to RNNs as they come in—from the beginning of the\nsentence to the end. This means that when an RNN is processing a word, it can lever-\nage only the information it has encountered so far, which is the word’s left context.\nTrue, you can get a lot of information from a word’s left context. For example, if a\nword is preceded by a modal verb (e.g., “can”), it is a strong signal that the next word\nis a verb. However, the right context holds a lot of information as well. For example, if\nyou know that the next word is a determiner (e.g., “a”), it is a strong signal that “book”\non its left is a verb, not a noun. \n Bidirectional RNNs (or simply biRNNs) solve this problem by combining two\nRNNs with opposite directions. A forward RNN is a forward-facing RNN that we’ve\nbeen using so far in this book—it scans the input sentence left to right and uses the\ninput word and all the information on its left to update the state. A backward RNN, on\nthe other hand, scans the input sentence right to left. It uses the input word and all\nthe information on its right to update the state. This is equivalent to flipping the\norder of the input sentence and feeding it to a forward RNN. The final hidden states\nproduced by biRNNs are concatenations of hidden states from the forward and back-\nward RNNs. See figure 5.9 for an illustration.\n Let’s use a concrete example to illustrate this. Assume the input sentence is “time\nflies like an arrow” and you’d like to know the POS tag for the word “like” in the mid-\ndle of this sentence. The forward RNN processes “time” and “flies,” and by the time it\nreaches “like,” its internal state (A in figure 5.9) encodes all the information about\n“time flies like.” Similarly, the backward RNN processes “arrow” and “an,” and by\nthe time it reaches “like,” the internal state (B in figure 5.9) has encoded all the infor-\nmation about “like an arrow.” The internal state from the biRNN for “like” is the\n\n\n125\nMultilayer and bidirectional RNNs\nconcatenation of these two states (A + B). You literally stitch together two vectors—no\nmathematical operations involved. As a result, the internal state for “like” encodes all\nthe information from the entire sentence. This is a great improvement over just know-\ning half the sentence!\n Implementing a biRNN is similarly easy—you just need to add the bidirec-\ntional=True flag when initializing the RNN as follows:\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(\n        EMBEDDING_SIZE, HIDDEN_SIZE, bidirectional=True, batch_first=True))\nIf you train the POS tagger with this change, the validation set accuracy will jump\nfrom ~88% to 91%. This implies that incorporating the information on both sides of\nthe word is effective for POS tagging.\n Note that you can combine the two techniques introduced in this section by stack-\ning bidirectional RNNs. The output from one layer of biRNN (concatenation of a\nforward and a backward layer) becomes the input to another layer of biRNN (see\nfigure 5.10). You can implement this by specifying both flags—num_layers and\nbidirectional—when initializing the RNN in PyTorch/AllenNLP.\ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nstate\nstate\nstate\nstate\nForward\nlayer\n \n \ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\n \nBackward\nlayer\n \nlogits\nLinear\nlayer\n \nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\".\")\n…\nNOUN VERB …\n(A)\n(B)\nFigure 5.9\nBidirectional RNN\n\n\n126\nCHAPTER 5\nSequential labeling and language modeling\nFigure 5.10\nTwo-layered bidirectional RNN\n5.4\nNamed entity recognition\nSequential labeling can be applied to many information extraction tasks, not just to\npart-of-speech tagging. In this section, I’ll introduce the task of named entity recogni-\ntion (NER) and demonstrate how to build an NER tagger using sequential labeling.\nThe code for this section can be viewed and executed via the Google Colab platform\n(http://realworldnlpbook.com/ch5#ner-nb).\ninit_state()\ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nstate\nstate\nstate\nstate\nForward\nlayer\n \n \ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\n \nBackward\nlayer\n \nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n \n \ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\nForward\nlayer\n \nBackward\nlayer\n \nLayer 1\nLayer 2\nlogits\nLinear\nlayer\n \nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\".\")\n…\nNOUN VERB …\n\n\n127\nNamed entity recognition\n5.4.1\nWhat is named entity recognition?\nAs mentioned earlier, named entities are mentions of real-world entities such as\nproper nouns. Common named entities that are usually covered by NER systems\ninclude the following:\nPersonal name (PER): Alan Turing, Lady Gaga, Elon Musk\nOrganization (ORG): Google, United Nations, Giants\nLocation (LOC): Mount Rainer, Bali Island, Nile\nGeopolitical entity (GPE): UK, San Francisco, Southeast Asia\nHowever, different NER systems deal with different sets of named entities. The con-\ncept of named entities is a bit overloaded in NLP to mean any mentions that are of\ninterest to the application’s user. For example, in the medical domain, you may want\nto extract mentions to names of drugs and chemical compounds. In the financial\ndomain, companies, products, and stock symbols may be of interest. In many\ndomains, numerical and temporal expressions are also considered.\n Identifying named entities is in itself important, because named entities (who,\nwhat, where, when, and so on) are often what most people are interested in. But NER\nis also an important first step for many other NLP applications. One such task is rela-\ntion extraction: extracting all relations between named entities from the given docu-\nment. For example, given a press release document, you may want to extract an event\ndescribed in the release, such as which company purchased which other company for\nwhat price. This often assumes that all the parties are already identified via NER.\nAnother task that is closely related to NER is entity linking, where mentions of named\nentities are linked to some knowledge base, such as Wikipedia. When Wikipedia is\nused as a knowledge base, entity linking is also called Wikification.\n But you may be wondering, what’s so difficult about simply extracting named enti-\nties? If they are just proper nouns, can you simply compile a dictionary of, say, all the\ncelebrities (or all the countries, or whatever you are interested in) and use it? The\nidea is, whenever the system encounters a noun, it would run the name through this\ndictionary and tag the mention if it appears in it. Such dictionaries are called gazetteers,\nand many NER systems do use them as a component. \n However, relying solely on such dictionaries has one major issue—ambiguity. Ear-\nlier we saw that a single word type could have multiple parts of speech (e.g., “book” as\na noun and a verb), and named entities are no exception. For example, “Georgia” can\nbe the name of a country, a US state, towns and communities across the United States\n(Georgia, Indiana; Georgia, Nebraska), a film, a number of songs, ships, and a per-\nsonal name. Simple words like “book” could also be named entities, including: Book\n(a community in Louisiana), Book/Books (a surname), The Books (an American\nband), and so on. Simply matching mentions against dictionaries would tell you noth-\ning about their identities if they are ambiguous. \n Fortunately, sentences often offer clues that can be used to disambiguate the men-\ntions. For example, if the sentence reads “I live in Georgia,” it’s usually a strong signal\nthat “Georgia” is a name of a place, not a film or a person’s name. NER systems use a\n\n\n128\nCHAPTER 5\nSequential labeling and language modeling\ncombination of signals about the mentions themselves (e.g., whether they are in a pre-\ndefined dictionary) and about their context (whether they are preceded or followed\nby certain words) to determine their tags. \n5.4.2\nTagging spans\nUnlike POS tagging, where each word is assigned a POS tag, mentions to named enti-\nties can span over more than one word, for example, “the United States” and “World\nTrade Organization.” A span in NLP is simply a range over one or more contiguous\nwords. How can we use the same sequential tagging framework to model spans?\n A common practice in NLP is to use some form of encoding to convert spans into\nper-word tags. The most common encoding scheme used in NER is called IOB2 tag-\nging. It represents spans by a combination of the positional tag and the category tag.\nThree types of positional tags follow:\nB (Beginning): assigned to the first (or the only) token of a span\nI (Inside): assigned to all but the first token of a span\nO (Outside): assigned to all words outside of any spans\nNow, let’s take a look at the NER example we saw earlier and is shown in figure 5.11.\nThe token “Apple” is the first (and the only) token of ORG (for “organization”), and it\nis assigned a B-ORG tag. Similarly, “UK”, the first and the only token of GPE (for “geo-\npolitical entity”), is assigned B-GPE. For “$1” and “billion,” the first and the second\ntokens of a monetary expression (MONEY), B-MONEY and I-MONEY are assigned,\nrespectively. All the other tokens are given O. \nFigure 5.11\nNamed entity recognition (NER) using sequential labeling\nThe rest of the pipeline for solving NER is very similar to that of part-of-speech tag-\nging: both are concerned with assigning an appropriate tag for each word and can be\nsolved by RNNs. In the next section, we are going to build a simple NER system using\nneural networks. \n5.4.3\nImplementing a named entity recognizer\nTo build an NER system, we use the Annotated Corpus for Named Entity Recognition\nprepared by Abhinav Walia published on Kaggle (http://realworldnlpbook.com/\nNamed entity recognition (NER)\nApple\nis\nlooking\nto\nbuy\nUK\nstartup for\n$1\nbillion\nB-ORG\nO\nO\nO\nO\nB-GPE\nO\nO\nB-MONEYI-MONEY\nInput sentence\nNER tags\n\n\n129\nNamed entity recognition\nch5.html#ner-data). In what follows, I’m going to assume that you downloaded and\nexpanded the dataset under data/entity-annotated-corpus. Alternatively, you\ncan use the copy of the dataset I uploaded to S3 (http://realworldnlpbook.com/\nch5.html#ner-data-s3), which is what the following code does. I wrote a dataset reader\nfor this dataset (http://realworldnlpbook.com/ch5.html#ner-reader), so you can sim-\nply import (or copy and paste) it and use it:\nreader = NERDatasetReader('https:/./s3.amazonaws.com/realworldnlpbook/'\n                          'data/entity-annotated-corpus/ner_dataset.csv')\nBecause the dataset is not separated into train, validation, and test sets, the dataset\nreader will separate it into train and validation splits for you. All you need to do is\nspecify which split you want when you initialize data loaders, as shown here:\nsampler = BucketBatchSampler(batch_size=16, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, 'train', batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, 'dev', batch_sampler=sampler)\nThe RNN-based sequential tagging model and the rest of the training pipeline look\nalmost the same as the previous example (POS tagger). The only difference is how we\nevaluate our NER model. Because most of the tags for a typical NER dataset are simply\n“O,” using tag accuracy is misleading—a stupid system that tags everything “O” would\nachieve very high accuracy. Instead, NER is usually evaluated as an information\nextraction task, where the goal is to extract named entities from texts, not just to tag\nthem. We’d like to evaluate NER systems based on the “cleanness” of retrieved named\nentities (how many of them are actual entities) and their “completeness” (how many of\nactual entities the system was able to retrieve). Does any of this sound familiar to you?\nYes, these are the definition of recall and precision we talked about in section 4.3.\nBecause there are usually multiple types of named entities, these metrics (precision,\nrecall, and F1-measure) are computed per entity type. \nNOTE\nIf these metrics are computed while ignoring entity types, it’s called a\nmicro average. For example, the micro-averaged precision is the total number\nof true positives of all types divided by the total number of retrieved named\nentities regardless of the type. On the other hand, if these metrics are com-\nputed per entity type and are then averaged, it’s called a macro average. For\nexample, if the precision for PER and GPE is 80% and 90%, respectively, its\nmacro average is 85%. What AllenNLP computes in the following is the micro\naverage.\nAllenNLP implements SpanBasedF1Measure, which computes per-type metrics\n(precision, recall, and F1-measure) as well as the average. You can define the metric in\n__init__() of your model as follows:\nself.f1 = SpanBasedF1Measure(vocab, tag_namespace='labels')\n\n\n130\nCHAPTER 5\nSequential labeling and language modeling\nAnd use it to get metrics during training and validation, as shown next:\ndef get_metrics(self, reset: bool = False) -> Dict[str, float]:\n    f1_metrics = self.f1.get_metric(reset)\n    return {'accuracy': self.accuracy.get_metric(reset),\n            'prec': f1_metrics['precision-overall'],\n            'rec': f1_metrics['recall-overall'],\n            'f1': f1_metrics['f1-measure-overall']}\nIf you run this training pipeline, you get an accuracy around 0.97, and precision,\nrecall, F1-measure will all hover around 0.83. You can also use the predict() method\nto obtain named entity tags for an unseen sentence as \ntokens = ['Apple', 'is', 'looking', 'to', 'buy', 'UK', 'startup',\n          'for', '$1', 'billion', '.']\nlabels = predict(tokens, model)\nprint(' '.join('{}/{}'.format(token, label)\n               for token, label in zip(tokens, labels)))\nwhich produces the following:\nApple/B-org is/O looking/O to/O buy/O UK/O startup/O for/O $1/O billion/O ./O\nThis is not perfect—the NER tagger got the first named entity (“Apple”) correct but\nmissed two others (“UK” and “$1 billion”). If you look at the training data, the men-\ntion “UK” never appears, and no monetary values are tagged. It is not surprising that\nthe system is struggling to tag entities that it has never seen before. In NLP (and also\nmachine learning in general), the characteristic of the test instances needs to match\nthat of the train data for the model to be fully effective. \n5.5\nModeling a language\nIn this section, I’ll switch gears a little bit and introduce language models, which is one\nof the most important concepts in NLP. We’ll discuss what they are, why they are\nimportant, and how to train them using the neural network components we’ve intro-\nduced so far.\n5.5.1\nWhat is a language model?\nImagine you are asked to predict what word comes next given a partial sentence: “My\ntrip to the beach was ruined by bad ___.” What words could come next? Many things\ncould ruin a trip to a beach, but most likely it’s bad weather. Maybe it’s bad-mannered\npeople at the beach, or maybe it’s bad food that the person had eaten before the trip,\nbut most would agree that “weather” is a likely word that comes after this partial sen-\ntence. Few other nouns (people, food, dogs) and words of other parts of speech (be, the,\nrun, green) are as appropriate as “weather” in this context.\n What you just did is to assign some belief (or probability) to an English sentence. You\njust compared several alternatives and judged how likely they are as English sentences.\nMost people would agree that the probability of “My trip to the beach was ruined by bad\nweather” is a lot higher than “My trip to the beach was ruined by bad dogs.”\n\n\n131\nModeling a language\n Formally, a language model is a statistical model that gives a probability to a piece of\ntext. An English language model would assign higher probabilities to sentences that\nlook like English. For example, an English language model would give a higher prob-\nability to “My trip to the beach was ruined by bad weather” than it does to “My trip to\nthe beach was ruined by bad dogs” or even “by weather was trip my bad beach the\nruined to.” The more grammatical and the more “sense” the sentence makes, the\nhigher the probability is.\n5.5.2\nWhy are language models useful?\nYou may be wondering what use such a statistical model has. Although predicting the\nnext word might come in handy when you are answering fill-in-the-blank questions for\nan exam, what particular roles do language models play in NLP?\n The answer is, it is essential for any systems that generate natural language. For\nexample, machine translation systems, which generate a sentence in a language given\na sentence in another language, would benefit greatly from high-quality language mod-\nels. Why? Let’s say we’d like to translate a Spanish sentence “Está lloviendo fuerte” into\nEnglish (“It is raining hard”). The last word “fuerte” has several English equivalents—\nstrong, sharp, loud, heavy, and so on. How would you determine which English equivalent\nis the most appropriate in this context? There could be many approaches to solve this\nproblem, but one of the simplest is to use an English language model and rerank sev-\neral different translation candidates. Assuming you’ve finished translating up to “It is\nraining,” you would simply replace the word “fuerte” with all the equivalents you can\nfind in a Spanish–English dictionary, which generates “It is raining strong,” “It is raining\nsharp,” “It is raining loud,” “It is raining hard.” Then all you need to do is ask the lan-\nguage model which one of these candidates has the highest probability.\nNOTE\nIn fact, neural machine translation models can be thought of as a vari-\nation of a language model that generates sentences in the target language\nconditioned on its input (sentences in the source language). Such a language\nmodel is a called a conditional language model as opposed to an unconditional\nlanguage model, which we discuss here. We’ll discuss machine translation mod-\nels in chapter 6.\nA similar situation arises in speech recognition, too, which is another task that gener-\nates text given spoken audio input. For example, if somebody uttered “You’re right,”\nhow would a speech recognition system know it’s actually “you’re right?” Because\n“you’re” and “your” can have the same pronunciation, and so can “right” and “write”\nand even “Wright” and “rite,” the system output could be any one of “You’re write,”\n“You’re Wright,” “You’re rite,” “Your right,” “Your write,” “Your Wright,” and so on.\nAgain, the simplest approach to resolving this ambiguity is to use a language model.\nAn English language model would properly rerank these candidates and determine\n“you’re right” is the most likely transcription.\n In fact, humans do this type of disambiguation all the time, though unconsciously.\nWhen you are having a conversation with somebody else at a large party, the actual\n",
      "page_number": 143
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 152-159)",
      "start_page": 152,
      "end_page": 159,
      "detection_method": "topic_boundary",
      "content": "132\nCHAPTER 5\nSequential labeling and language modeling\naudio signal you receive is often very noisy. Most people can still understand each\nother without any issues because people’s language models help them “correct” what\nyou hear and interpolate any missing parts. You’ll notice this most if you try to con-\nverse in a less proficient, second language—you’d have a lot harder time understand-\ning the other person in a noisy environment, because your language model is not as\ngood as your first language’s.\n5.5.3\nTraining an RNN language model\nAt this point, you may be wondering what the connection is between predicting the\nnext word and assigning a probability to a sentence. These two are actually equivalent.\nInstead of explaining the theory behind it, which requires you to understand some\nmath (especially probability theory), I’ll attempt an intuitive example next without\ngoing into mathematical details.\n Imagine you want to estimate the chance of tomorrow’s weather being rainy and\nthe ground wet. Let’s simplify this and assume there are only two types of weather,\nsunny and rainy. There are only two outcomes for the ground: dry or wet. This is\nequivalent to estimating the probably of a sequence: [rain, wet].\n Further assume that there’s a 50-50 chance of rain on a given day. After raining,\nthe ground is wet with a 90% chance. Then, what is the probability of the rain and the\nground being wet? It’s simply 50% times 90%, which is 45%, or 0.45. If we know the\nprobability of one event happening after another, you can simply multiply two proba-\nbilities to get the total probability for the sequence. This is called the chain rule in\nprobability theory.\n Similarly, if you can correctly estimate the probability of one word occurring after a\npartial sentence, you can simply multiply it with the probability of the partial sen-\ntence. Starting from the first word, you can keep doing this until you reach the end of\nthe sentence. For example, if you’d like to compute the probability for “The trip to\nthe beach was . . . ,” you can multiply the following:\nThe probability of “The” occurring at the beginning of a sentence\nThe probability of “trip” occurring after “The”\nThe probability of “to” occurring after “The trip”\nThe probability of “the” occurring after “The trip to”\nAnd so on\nThis means that to build a language model, you need a model that predicts the proba-\nbility (or, more precisely, the probability distribution) of the next word given the con-\ntext. You may have noticed that this sounds a little familiar. Indeed, what’s done here is\nvery similar to the sequential-labeling models that we’ve been talking about in this\nchapter. For example, a part-of-speech (POS) tagging model predicts the probability\ndistribution over the possible POS tags given the context. A named entity recognition\n(NER) model does it for the possible named entity tags. The difference is that a lan-\nguage model does it for the possible next words, given what the model has encountered\nso far. Hopefully it’s starting to make some sense why I talk about language models in\nthis chapter!\n\n\n133\nText generation using RNNs\n In summary, to build a language model, you tweak an RNN-based sequence-labeling\nmodel a little bit so that it gives the estimates for the next word, instead of POS or NER\ntags. In chapter 3, I talked about the Skip-gram model, which predicts the words in a\ncontext given the target word. Notice the similarity here—both models predict the\nprobability over possible words. The input to the Skip-gram model is just a single word,\nwhereas the input to the language model is the partial sequence. You can use a similar\nmechanism for converting one vector to another using a linear layer, then converting\nit to a probability distribution using softmax, as we discussed in chapter 3. The archi-\ntecture is shown in figure 5.12.\nFigure 5.12\nArchitecture of RNN-based language model\nThe way RNN-based language models are trained is similar to other sequential-labeling\nmodels. The loss function we use is the sequential cross-entropy loss, which measures\nhow “off” the predicted words are from actual words. The cross-entropy loss is com-\nputed per word and averaged over all words in the sentence.\n5.6\nText generation using RNNs\nWe saw that language models give probabilities to natural language sentences. But the\nmore fun part is you can generate natural language sentences from scratch using a\nlanguage model! In the final section of this chapter, we are going to build a language\nmodel. You can use the trained model to evaluate and generate English sentences.\n<START>\nThe\nWord\nembeddings\n \ntrip\nweather\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\n<END>\n.\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nSoftmax\ntrip\ndog\nway\nkey\n…\n…\nProbabilities\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nThe\nto\n\n\n134\nCHAPTER 5\nSequential labeling and language modeling\nYou can find the entire script for this subsection on a Google Colab notebook (http://\nrealworldnlpbook.com/ch5.html#lm-nb).\n5.6.1\nFeeding characters to an RNN\nIn the first half of this section, we are going to build an English language model and\ntrain it using a generic English corpus. Before we start, we note that the RNN lan-\nguage model we build in this chapter operates on characters, not on words or tokens.\nAll the RNN models we’ve seen so far operate on words, which means the input to the\nRNN was always sequences of words. On the other hand, the RNN we are going to use\nin this section takes sequences of characters as the input. \n In theory, RNNs can operate on sequences of anything, be it tokens or characters\nor something completely different (e.g., waveform for speech recognition), as long as\nthey are something that can be turned into vectors. In building language models,\nwe often feed characters, even including whitespace and punctuations as the input,\ntreating them as words of length 1. The rest of the model works exactly the same—\nindividual characters are first embedded (converted to vectors) and then fed into the\nRNN, which is in turn trained so that it can best predict the distribution over the char-\nacters that are likely to come next. \n You have a couple of considerations when you are deciding whether you should\nfeed words or characters to an RNN. Using characters will definitely make the RNN\nless efficient, meaning that it would need more computation to “figure out” the same\nconcept. For example, a word-based RNN can receive the word “dog” at a timestep\nand update its internal states, whereas a character-based RNN would not able to do it\nuntil it receives three elements d, o, and g, and probably “_” (whitespace). A character-\nbased RNN needs to “learn” that a sequence of these three characters means some-\nthing special (the concept of “dog”).\n On the other hand, by feeding characters to RNNs, you can bypass many issues\narising from dealing with tokens. One such issue is related to out-of-vocabulary (or\nOOV) words. When training a word-based RNN, you usually fix the entire set of vocab-\nulary, often by enumerating all words that appeared in the train set. But whenever it\nencounters an OOV word in the test set, it doesn’t know what to do with it. Often-\ntimes, it assigns a special token <UNK> to all OOV words and treats them in the same\nway, which is not ideal. A character-based RNN, on the contrary, can still operate on\nindividual characters, so it may be able to figure out what “doggy” means, for example,\nbased on the rules it has learned by observing “dog” in the train set, even though it\nhas never seen the exact word “doggy” before.\n5.6.2\nEvaluating text using a language model\nLet’s start building a character-based language model. The first step is to read a plain\ntext dataset file and generate instances for training the model. I’m going to show how\nto construct an instance without using a dataset reader for a demonstration purpose.\nSuppose you have a Python string object text that you’d like to turn into an instance\nfor training a language model. First you need to segment it into characters using\nCharacterTokenizer as follows:\n\n\n135\nText generation using RNNs\nfrom allennlp.data.tokenizers import CharacterTokenizer\ntokenizer = CharacterTokenizer()\ntokens = tokenizer.tokenize(text)\nNote that tokens here is a list of Token objects. Each Token object contains a single\ncharacter, instead of a single word. Then you insert the <START> and <END> symbols\nat the beginning and at the end of the list as shown next:\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL\ntokens.insert(0, Token(START_SYMBOL))\ntokens.append(Token(END_SYMBOL))\nInserting special symbols like these at the beginning and end of each sentence is a\ncommon practice in NLP. With these symbols, models can distinguish between occur-\nrences of a token in the middle of a sentence versus at the beginning/end of a sentence.\nFor example, a period is a lot more likely to occur at the end of a sentence (“. <END>”)\nthan the beginning (“<START> .”), to which a language model can give two very differ-\nent probabilities, which is impossible to do without the use of these symbols.\n Finally, you can construct an instance by specifying individual text fields. Notice\nthat the “output” of a language model is identical to the input, simply shifted by one\ntoken, as shown here:\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.instance import Instance\ninput_field = TextField(tokens[:-1], token_indexers)\noutput_field = TextField(tokens[1:], token_indexers)\ninstance = Instance({'input_tokens': input_field,\n                     'output_tokens': output_field})\nHere token_indexers specifies how individual tokens are mapped into IDs. We sim-\nply use SingleIdTokenIndexer we’ve been using so far as follows:\nfrom allennlp.data.token_indexers import TokenIndexer \ntoken_indexers = {'tokens': SingleIdTokenIndexer()}\nFigure 5.13 shows an instance created from this process. \nFigure 5.13\nInstance for training a language model\ninstance\nT\nh\ne\n<ST>\n<ED>\n_\nq\nu\nT\nh\ne\n_\nq\nu\ni\n.\ng\n.\ng\no\ninput_tokens\noutput_tokens\n…\n\n\n136\nCHAPTER 5\nSequential labeling and language modeling\nThe rest of the training pipeline, as well as the model, is very similar to that for sequen-\ntial labeling mentioned earlier in this chapter. See the Colab notebook for more details.\nAs shown in the next code snippet, after the model is fully trained, you can construct\ninstances from new texts, turn them into instances, and compute the loss, which basi-\ncally measures how successful the model was in predicting what comes next:\npredict('The trip to the beach was ruined by bad weather.', model)\n{'loss': 1.3882852}\npredict('The trip to the beach was ruined by bad dogs.', model)\n{'loss': 1.5099115}\npredict('by weather was trip my bad beach the ruined to.', model)\n{'loss': 1.8084583}\nThe loss here is the cross-entropy loss between the predicted and the expected charac-\nters. The more “unexpected” the characters there are, the higher the values will be, so\nyou can use these values to measure how natural the input is as English text. As\nexpected, natural sentences (such as the first one) are given scores that are lower than\nunnatural sentences (such as the last one).\nNOTE\nIf you calculate 2 to the power of the cross entropy, the value is called\nperplexity. Given a fixed natural language text, perplexity becomes lower\nbecause the language model is better at predicting what comes next, so it is\ncommonly used for evaluating the quality of language models in the literature.\n5.6.3\nGenerating text using a language model\nThe most interesting aspect of (fully trained) language models is that they can predict\npossible characters that may appear next given some context. Specifically, they can give\nyou a probability distribution over possible characters that may come next, from which\nyou choose to determine the next character. For example, if the model has generated\n“t” and “h,” and the LM is trained on generic English text, it would probably assign a\nhigh probability on the letter “e,” generating common English words including the,\nthey, them, and so on. If you start this process from the <START> token and keep doing\nthis until you reach the end of the sentence (i.e., by generating <END>), you can gen-\nerate an English sentence from scratch. By the way, this is another reason why tokens\nsuch as <START> and <END> are useful—you need something to feed to the RNN to\nkick off the generation, and you also need to know when the sentence stops.\n Let’s look at this process in a Python-like pseudocode next:\ndef generate():\n    state = init_state()\n    token = <START>\n    tokens = [<START>]\n    while token != <END>:\n        state = update(state, token)\n        probs = softmax(linear(state))\n        token = sample(probs)\n\n\n137\nText generation using RNNs\n        tokens.append(token)\n    return tokens\nThis loop looks very similar to the one for updating RNNs with one key difference:\nhere, we are not receiving any input but instead are generating characters and feed-\ning them as the input. In other words, the RNN operates on the sequence of charac-\nters that the RNN itself generated so far. Such models that operate on past sequences\nthey produced are called autoregressive models. See figure 5.14 for an illustration of this.\nFigure 5.14\nGenerating text using an RNN\nIn the previous code snippet, init_state() and update() functions are the ones\nthat initialize and update the hidden states of the RNN, as we’ve seen earlier. In gener-\nating text, we assume that the model and its parameters are already trained on a large\namount of natural language text. softmax() is a function to run Softmax on the\ngiven vector, and linear() is the linear layer to expand/shrink the size of the vector.\nThe function sample() returns a character according to the given probability distri-\nbution. For example, if the distribution is “a”: 0.6, “b”: 0.3, “c”: 0.1, it will choose “a”\n60% of the time, “b” 30% of the time, and “c” 10% of the time. This ensures that the\ngenerated string is different every time while every string is likely to look like a real\nEnglish sentence. \nNOTE\nYou can use PyTorch’s torch.multinomial() for sampling an ele-\nment from a probability distribution.\nIf you train this language model using the English sentences from Tatoeba and gener-\nate sentences according to this algorithm, the system will produce something similar\nto the following cherry-picked examples:\nYou can say that you don't know it, and why decided of yourself.\nPike of your value is to talk of hubies.\n<START>\nT\nT\nCharacter\nembeddings\n \nh\ng\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\nh\ne\n.\n<END>\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nT h e _ q u i c k _ b r … _ d o g .\n\n\n138\nCHAPTER 5\nSequential labeling and language modeling\nThe meeting despoit from a police?\nThat's a problem, but us?\nThe sky as going to send nire into better.\nWe'll be look of the best ever studented.\nThere's you seen anything every's redusention day.\nHow a fail is to go there.\nIt sad not distaples with money.\nWhat you see him go as famous to eat!\nThis is not a bad start! If you look at these sentences, there are many words and\nphrases that make sense as valid English (You can say that, That’s a problem, to go there, see\nhim go, etc.). Even when the system generates peculiar words (despoit, studented, redusen-\ntion, distaples), they look almost like real English words because they all basically follow\nmorphological and phonological rules of English. This means that the language\nmodel was successful in learning the basic building blocks of English, such as how to\narrange letters (orthography), how to form words (morphology), and how to form\nbasic sentence structures (syntax).\n However, if you look at sentences as a whole, few of them make any sense (e.g.,\nWhat you see him go as famous to eat!). This means the language model we trained falls\nshort of modeling semantic consistency of sentences. This is potentially because our\nmodel is not powerful enough (our LSTM-RNN needs to compress everything about\nthe sentence into a 256-dimensional vector) or the training dataset is too small (just\n10,000 sentences), or both. But you can easily imagine that if we keep increasing the\nmodel capacity as well as the size of the train set, the model gets incredibly good at\nproducing realistic natural language text. In February 2019, OpenAI announced that\nit developed a huge language model based on the Transformer model (which we’ll\ncover in chapter 8) trained on 40 GB of internet text. The model shows that it can\nproduce realistic-looking text that shows near-perfect grammar and long-term topical\nconsistency given a prompt. In fact, the model was so good that OpenAI decided not\nto release the large model they had trained due to their concerns about malicious use\nof the technology. But it is important to keep in mind that, no matter how intelligent\nthe output looks, their model is trained on the same principle as our toy example in\nthis chapter—just trying to predict the next character!\nSummary\nSequential-labeling models tag each word in the input with a label, which can\nbe achieved by recurrent neural networks (RNNs).\nPart-of-speech (POS) tagging and named entity recognition (NER) are two\ninstances of sequential-labeling tasks.\nMultilayer RNNs stack multiple layers of RNNs, whereas bidirectional RNNs\ncombine forward and backward RNNs to encode the entire sentence.\nLanguage models assign probabilities to natural language text, which is\nachieved by predicting the next word.\nYou can use a trained language model to assess how “natural” a natural lan-\nguage sentence is or even to generate realistic-looking text from scratch.\n\n\nPart 2\nAdvanced models\nThe field of NLP has seen rapid progress in the past few years. Specifically,\nthe advent of the Transformer and pretrained language models such as BERT\nhave completely changed the landscape of the field and how practitioners build\nNLP applications. This part of the book will help you catch up with these latest\ndevelopments.\n Chapter 6 introduces sequence-to-sequence models, an important class of\nmodels that will enable you to build more complex applications such as machine\ntranslation systems and chatbots. Chapter 7 discusses another type of popular\nneural network architecture, convolutional neural networks (CNNs).\n Chapters 8 and 9 are arguably the most important and exciting chapters of this\nbook. They cover the Transformer and transfer learning methods (such as BERT)\nrespectively. We’ll demonstrate how to build advanced NLP applications such as\nhigh-quality machine translation and spell-checkers, using those technologies.\n By the time you finish reading this part, you’ll feel confident that you can\nnow solve a wide range of NLP tasks with what you have learned so far.\n \n \n",
      "page_number": 152
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 160-168)",
      "start_page": 160,
      "end_page": 168,
      "detection_method": "topic_boundary",
      "content": "140\nCHAPTER \n\n\n141\nSequence-to-sequence models\nIn this chapter, we are going to discuss sequence-to-sequence (Seq2Seq) models,\nwhich are some of the most important complex NLP models and are used for a wide\nrange of applications, including machine translation. Seq2Seq models and their\nvariations are already used as the fundamental building blocks in many real-world\napplications, including Google Translate and speech recognition. We are going to\nbuild a simple neural machine translation system using a powerful framework to\nThis chapter covers\nBuilding a machine translation system using Fairseq\nTransforming one sentence to another using a \nSeq2Seq model\nUsing a beam search decoder to generate better \noutput\nEvaluating the quality of machine translation \nsystems\nBuilding a dialogue system (chatbot) using a \nSeq2Seq model\n\n\n142\nCHAPTER 6\nSequence-to-sequence models\nlearn how the models work and how to generate the output using greedy and beam\nsearch algorithms. At the end of this chapter, we will build a chatbot—an NLP applica-\ntion with which you can have a conversation. We’ll also discuss the challenges and lim-\nitations of simple Seq2Seq models.\n6.1\nIntroducing sequence-to-sequence models\nIn the previous chapter, we discussed two types of powerful NLP models, namely,\nsequential labeling and language models. To recap, a sequence-labeling model takes a\nsequence of some units (e.g., words) and assigns a label (e.g., a part-of-speech (POS)\ntag) to each unit, whereas a language model takes a sequence of some units (e.g.,\nwords) and estimates how probable the given sequence is in the domain in which the\nmodel is trained. You can also use a language model to generate realistic-looking text\nfrom scratch. See figure 6.1 for the overview of these two models.\nFigure 6.1\nSequential labeling and language models\nAlthough these two models are useful for a number of NLP tasks, for some, you may\nwant the best of both worlds—you may want your model to take some input (e.g., a\nsentence) and generate something else (e.g., another sentence) in response. For\nexample, if you wish to translate some text written in one language into another, you\nneed your model to take a sentence and produce another. Can you do this with\nsequential-labeling models? No, because they can produce only the same number of\noutput labels as there are tokens in the input sentence. This is obviously too limiting\nfor translation—one expression in a language (say, “Enchanté” in French) can have\nan arbitrarily large or small number of words in another (say, “Nice to meet you” in\nEnglish). Can you do this with language models? Again, not really. Although you can\ngenerate realistic-looking text using language models, you have almost no control\nover the text they generate. In fact, language models do not take any input.\n But if you look at figure 6.1 more carefully, you might notice something. The\nmodel on the left (the sequential-labeling model) takes a sentence as its input and\nproduces some form of representations, whereas the model on the right produces a\ntime\nflies\nlike\narrow\n.\nNOUN\nVERB\n...\nADP\nNOUN\nPUNKT\n<START>\nThe\nquick\ndog\n.\nThe\nquick\n...\nbrown\n.\n<END>\nSequential labeling\nLanguage model\n\n\n143\nIntroducing sequence-to-sequence models\nsentence with variable length that looks like natural language text. We already have\nthe components needed to build what we want, that is, a model that takes a sentence\nand transforms it into another. The only missing part is a way to connect these two so\nthat we can control what the language model generates.\n In fact, by the time the model on the left finishes processing the input sentence,\nthe RNN has already produced its abstract representation, which is encoded in the\nRNN’s hidden states. If you can simply connect these two so that the sentence repre-\nsentation is passed from left to right and the language model can generate another\nsentence based on the representation, it seems like you can achieve what you wanted\nto do in the first place!\n Sequence-to-sequence models—or Seq2Seq models, in short—are built on this\ninsight. A Seq2Seq model consists of two subcomponents—an encoder and a decoder.\nSee figure 6.2 for an illustration. An encoder takes a sequence of some units (e.g., a\nsentence) and converts it into some internal representation. A decoder, on the other\nhand, generates a sequence of some units (e.g., a sentence) from the internal repre-\nsentation. As a whole, a Seq2Seq model takes a sequence and generates another\nsequence. As with the language model, the generation stops when the decoder pro-\nduces a special token, <END>, which enables a Seq2Seq model to generate an output\nthat can be longer or shorter than the input sequence.\nFigure 6.2\nSequence-to-sequence model\nMany variants of Seq2Seq models exist, depending on what architecture you use for\nthe encoder, what architecture you use for the decoder, and how information flows\nbetween the two. This chapter covers the most basic type of Seq2Seq model—simply\nconnecting two RNNs via the sentence representation. We’ll discuss more advanced\nvariants in chapter 8.\n Machine translation is the first, and by far the most popular, application of\nSeq2Seq models. However, the Seq2Seq architecture is a generic model applicable to\nnumerous NLP tasks. In one such task, summarization, an NLP system takes a long\ntext (e.g., a news article) and produces its summary (e.g., a news headline). A Seq2Seq\nMaria\nno\nbaba\nverde\n.\n...\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nEncoder\nDecoder\nSentence\nrepresentation\n \n\n\n144\nCHAPTER 6\nSequence-to-sequence models\nmodel can be used to “translate” the longer text into the shorter one. Another task is\na dialogue system, or a chatbot. If you think of a user’s utterance as the input and the\nsystem’s response as the output, the dialogue system’s job is to “translate” the former\ninto the latter. Later in this chapter, we will discuss a case study where we actually build\na chatbot using a Seq2Seq model. Yet another (somewhat surprising) application is\nparsing—if you think of the input text as one language and its syntax representation\nas another, you can parse natural language texts with a Seq2Seq model.1\n6.2\nMachine translation 101\nWe briefly touched upon machine translation in section 1.2.1. To recap, machine\ntranslation (MT) systems are NLP systems that translate a given text from one lan-\nguage to another. The language the input text is written in is called the source language,\nwhereas the one for the output is called the target language. The combination of the\nsource and target languages is called the language pair. \n First, let’s look at a couple of examples to see what it’s like and why it’s difficult to\ntranslate a foreign language to English (or any other language you understand). In\nthe first example, let’s translate a Spanish sentence, “Maria no daba una bofetada a la\nbruja verde.” to the English counterpart, “Mary did not slap the green witch.” A com-\nmon practice in illustrating the\nprocess of translation is to draw\nhow words or phrases of the same\nmeaning map between the two sen-\ntences. Correspondence of linguis-\ntic units between two instances is\ncalled alignment. Figure 6.3 shows\nthe alignment between the Spanish\nand English sentences.\n Some words (e.g., “Maria” and “Mary,” “bruja” and “witch,” and “verde” and\n“green”) match exactly one to one. However, some expressions (e.g., “daba una bofe-\ntada” and “slap”) differ in such a significant way that you can only align phrases\nbetween Spanish and English. Finally, even where there’s one-to-one correspondence\nbetween words, the way words are arranged, or word order, may differ between the two\nlanguages. For example, adjectives are added after nouns in Spanish (“la bruja\nverde”) whereas in English, they come before nouns (“the green witch”). Spanish and\nEnglish are linguistically similar in terms of grammar and vocabulary, especially when\ncompared to, say, Chinese and English, although this single example shows translating\nbetween the two may be a challenging task.\n1 See Oriol Vinyals et al., “Grammar as a Foreign Language,” (2015; https://arxiv.org/abs/1412.7449) for more\ndetails.\nMaria\nMary\ndid not\nslap\nthe\ngreen\nwitch\nno\ndaba una bofetada\na la\nbruja\nverde\nFigure 6.3\nTranslation and word alignment between \nSpanish and English\n\n\n145\nMachine translation 101\n Things start to look more complicated\nbetween Mandarin Chinese and English.\nFigure 6.4 illustrates the alignment between\na Chinese sentence (“Bushi yu Shalong jux-\ning le huitan.”) and its English translation\n(“Bush held a talk with Shalon.”). Although\nChinese uses ideographic characters of its\nown, we use romanized sentences here for\nsimplicity.\n You can now see more crossing arrows in the figure. Unlike English, Chinese prep-\nositional phrases such as “with Shalon” are usually attached to verbs from the left.\nAlso, the Chinese language doesn’t explicitly mark tense, and MT systems (and\nhuman translators alike) need to “guess” the correct tense to use for the English trans-\nlation. Finally, Chinese-to-English MT systems also need to infer the correct number\n(singular or plural) of each noun, because Chinese nouns are not explicitly marked\naccording to their number (e.g., “huitan” just means “talk” with no explicit mention\nof number). This is a good example showing how the difficulty of translation depends\non the language pair. Development of MT systems between linguistically different lan-\nguages (such as Chinese and English) is usually more challenging than those between\nlinguistically similar ones (such as Spanish and Portuguese).\n Let’s take a look at one more example—\ntranslating from Japanese to English, illus-\ntrated in figure 6.5. All the arrows in the fig-\nure are crossed, meaning that the word\norder is almost exactly opposite in these two\nsentences. In addition to the fact that Japa-\nnese prepositional phrases (“to music”) and\nrelative clauses attach from the left like Chi-\nnese, objects (such as “listening” in “I love\nlistening” in the example) come before the\nverb. In other words, Japanese is an SOV (subject-object-verb) language, whereas all\nthe other languages we mentioned so far (English, Spanish, and Chinese) are SVO\n(subject-verb-object) languages. Structural differences are a reason why direct, word-\nto-word translation doesn’t work very well.\nNOTE\nThis word-order classification system of language (such as SOV and\nSVO) is often used in linguistic typology. The vast majority of world lan-\nguages are either SOV (most common) or SVO (slightly less common),\nalthough a small number of languages follow other word-order systems,\nsuch as VSO (verb-subject-object), used by Arabic and Irish, for example.\nVery few languages (less than 3% of all languages) follow other types (VOS,\nOVS, and OSV).\nBushi\nyu\nShalong\njuxing le\nhuitan\nBush\nheld\na talk\nwith\nShalon\nFigure 6.4\nTranslation and word alignment \nbetween Mandarin Chinese and English\nOngaku\nI\nlove\nlistening\nto\nmusic\nwo\nkiku\nno ga\ndaisuki\ndesu\nFigure 6.5\nTranslation and word alignment \nbetween Japanese and English\n\n\n146\nCHAPTER 6\nSequence-to-sequence models\nBesides the structural differences shown in the previous figures, many other factors can\nmake MT a difficult task. One such factor is lexical difference. If you are translating, for\nexample, the Japanese word “ongaku” to the English “music,” there’s little ambiguity.\n“Ongaku” is almost always “music.” However, if you are translating, say, the English word\n“brother” to Chinese, you face ambiguity, because Chinese uses distinct words for\n“elder brother” and “younger brother.” In an even more extreme case, if you are trans-\nlating “cousin” to Chinese, you have eight different choices, because in the Chinese\nfamily system, you need to use distinct words depending on whether your cousin is\nmaternal or paternal, female or male, and older or younger than you.\n Another factor that makes MT challenging is omission. You can see that in figure\n6.5, there’s no Japanese word for “I.” In languages such as Chinese, Japanese, Spanish,\nand many others, you can omit the subject pronoun when it’s clear from the context\nand/or the verb form. This is called zero pronoun, and it can become a problem when\ntranslating from a pronoun-dropping language to a language where it happens less\noften (e.g., English).\n One of the earliest MT systems, developed during the Georgetown-IBM experi-\nment, was built to translate Russian sentences into English during the Cold War. But\nall it did was not much different from looking up each word in a bilingual dictionary\nand replacing it with its translation. The three examples shown above should be\nenough to convince you that simply replacing word by word is too limiting. Later sys-\ntems incorporated a larger set of lexicons and grammar rules, but these rules are writ-\nten manually by linguists and are not enough to capture the complexities of language\n(again, remember the poor software engineer from chapter 1). \n The main paradigm for MT that remained dominant both in academia and indus-\ntry before the advent of neural machine translation (NMT) is called statistical machine\ntranslation (SMT). The idea behind it is simple: learn how to translate from data, not\nby manually crafting rules. Specifically, SMT systems learn how to translate from data-\nsets that contain a collection of texts in the source language and their translation in\nthe target language. Such datasets are called parallel corpora (or parallel texts, or bitexts).\nBy looking at a collection of paired sentences in both languages, the algorithm seeks\npatterns of how words in one language should be translated to another. The resulting\nstatistical model is called a translation model. At the same time, by looking at a collec-\ntion of target sentences, the algorithm can learn what valid sentences in the target lan-\nguages should look like. Sounds familiar? This is exactly what a language model is all\nabout (see the previous chapter). The final SMT model combines these two models\nand produces output that is a plausible translation of the input and is a valid, fluent\nsentence in the target language on its own.\n Around 2015, the advent of powerful neural machine translation (NMT) models\nsubverted the dominance of SMT. SMT and NMT have two key differences. First, by\ndefinition, NMT is based on neural networks, which are well known for their power to\nmodel language accurately. As a result, target sentences generated by NMT tend to be\nmore fluent and natural than those generated by SMT. Second, NMT models are\n\n\n147\nBuilding your first translator\ntrained end-to-end, as I briefly touched on in chapter 1. This means that NMT models\nconsist of a single neural network that takes an input and directly produces an output,\ninstead of a patchwork of submodels and submodules that you need to train inde-\npendently. As a result, NMT models are simpler to train and smaller in code size than\nSMT models.\n MT is already used in many different industries and aspects of our lives. Translat-\ning foreign text into a language that you understand to grasp its meaning quickly is\ncalled gisting. If the text is deemed important enough after gisting, it may be sent to\nformal, manual translation. Professional translators also use MT for their work. Often-\ntimes, the source text is first translated to the target language using an MT system,\nthen the produced text is edited by human translators. Such editing is called postedit-\ning. The use of automated systems (called computer-aided translation, or CAT) can accel-\nerate the translation process and reduce the cost. \n6.3\nBuilding your first translator\nIn this section, we are going to build a working MT system. Instead of writing any\nPython code to do that, we’ll make the most of existing MT frameworks. A number of\nopen source frameworks make it easier to build MT systems, including Moses (http://\nwww.statmt.org/moses/) for SMT and OpenNMT (http://opennmt.net/) for NMT.\nIn this section, we will use Fairseq (https://github.com/pytorch/fairseq), an NMT\ntoolkit developed by Facebook that is becoming more and more popular among NLP\npractitioners these days. The following aspects make Fairseq a good choice for devel-\noping an NMT system quickly: 1) it is a modern framework that comes with a number\nof predefined state-of-the-art NMT models that you can use out of the box; 2) it is very\nextensible, meaning you can quickly implement your own model by following their\nAPI; and 3) it is very fast, supporting multi-GPU and distributed training by default.\nThanks to its powerful models, you can build a decent quality NMT system within a\ncouple of hours.\n Before you start, install Fairseq by running pip install fairseq in the root of\nyour project directory. Also, run the following commands in your shell to download\nand expand the dataset (you may need to install unzip if you are using Ubuntu by\nrunning sudo apt-get install unzip):2\n$ mkdir -p data/mt\n$ wget https://realworldnlpbook.s3.amazonaws.com/data/mt/tatoeba.eng_spa.zip\n$ unzip tatoeba.eng_spa.zip -d data/mt\nWe are going to use Spanish and English parallel sentences from the Tatoeba project,\nwhich we used previously in chapter 4, to train a Spanish-to-English MT system. The\ncorpus consists of approximately 200,000 English sentences and their Spanish transla-\ntions. I went ahead and already formatted the dataset so that you can use it without\n2  Note that $ at the beginning of every line is rendered by the shell, and you don’t need to type it.\n\n\n148\nCHAPTER 6\nSequence-to-sequence models\nworrying about obtaining the data, tokenizing the text, and so on. The dataset is\nalready split into train, validate, and test subsets.\n6.3.1\nPreparing the datasets\nAs mentioned previously, MT systems (both SMT and NMT) are machine learning\nmodels and thus are trained from data. The development process of MT systems looks\nsimilar to any other modern NLP systems, as shown in figure 6.6. First, the training\nportion of the parallel corpus is preprocessed and used to train a set of NMT model\ncandidates. Next, the validation portion is used to choose the best-performing model\nout of all the candidates. This process is called model selection (see chapter 2 for a\nreview). Finally, the best model is tested on the test portion of the dataset to obtain\nevaluation metrics, which reflect how good the model is.\nFigure 6.6\nPipeline for building an NMT system\nThe first step in MT development is preprocessing the dataset. But before preprocess-\ning, you need to convert the dataset into an easy-to-use format, which is usually plain\ntext in NLP. In practice, the raw data for training MT systems come in many different\nformats, for example, plain text files (if you are lucky), XML formats of proprietary\nsoftware, PDF files, and database records. Your first job is to format the raw files so\nthat source sentences and their target translations are aligned sentence by sentence.\nThe resulting file is often a TSV file where each line is a tab-separated sentence pair,\nwhich looks like the following:\nLet's try something.                   Permíteme intentarlo.\nMuiriel is 20 now.                     Ahora, Muiriel tiene 20 años.\nI just don't know what to say.         No sé qué decir.\nYou are in my way.                     Estás en mi camino.\nSometimes he can be a strange guy.     A veces él puede ser un chico raro.\n…\nTrain data\nSource\nTarget\nSource\nTarget\nValidation data\nNMT models\nTest data\nPreprocessing\nPreprocessing\nTraining\nSource\nTarget\nNMT model\nMetrics\nModel selection\nPreprocessing\nEvaluation\n",
      "page_number": 160
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 169-185)",
      "start_page": 169,
      "end_page": 185,
      "detection_method": "topic_boundary",
      "content": "149\nBuilding your first translator\nAfter the translations are aligned, the parallel corpus is fed into the preprocessing\npipeline. Specific operations applied in this process differ from application to applica-\ntion, and from language to language, but the following steps are most common:\n1\nFiltering\n2\nCleaning\n3\nTokenization\nIn the filtering step, any sentence pairs that are not suitable for training an MT system\nare removed from the dataset. What makes a sentence pair not suitable depends on\nmany factors, but, for example, any sentence pair where either text is too long (say,\nmore than 1,000 words) is not useful, because most MT models are not capable of\nmodeling such a long sentence. Also, any sentence pairs where one sentence is too\nlong but the other is too short are probably noise caused by a data processing or align-\nment error. For example, if a Spanish sentence is 10 words long, the length of its\nEnglish translation should fall within a 5- to 15-word range. Finally, if, for any reason,\nthe parallel corpus contains any languages other than the source and target lan-\nguages, you should remove such sentence pairs. This happens a lot more often than\nyou’d imagine—many documents are multilingual due to, for example, quotes, expla-\nnation, or code switching (mixing more than one language in a sentence). Language\ndetection (see chapter 4) can help detect such anomalies.\n After filtering, sentences in the dataset can be cleaned further. This process may\ninclude such things as removal of HTML tags and any special characters and normal-\nization of characters (e.g., traditional and simplified Chinese) and spelling (e.g.,\nAmerican and British English).\n If the target language uses scripts such as the Latin (a, b, c, …) or Cyrillic (а, б, в, …)\nalphabets, which distinguish upper- and lowercases, you may want to normalize case. By\ndoing so, your MT system will group “NLP” with “nlp” and “Nlp.” This step is usually a\ngood thing, because by having three different representations of a single concept, the\nMT model needs to learn that they are in fact a single concept purely from the data.\nNormalizing cases also reduces the number of distinct words, which makes training and\nprediction faster. However, this also groups “US” and “Us” and “us,” which might not be\na desirable behavior, depending on the type of data and the domain you are working\nwith. In practice, such decisions, including whether to normalize cases, are carefully\nmade by observing their effect on the validation data performance.\nData cleaning for machine translation and NLP\nNote that the cleaning techniques mentioned here are not specific to MT. Any NLP\napplications and tasks can benefit from a carefully crafted pipeline of filtering and\ncleaning operations. However, cleaning of the training data is particularly important\nfor MT, because the consistency of translation goes a long way in building a robust\nMT model. If your training data uses “NLP” in some cases and “nlp” in others, the\nmodel will have a difficulty figuring out the proper way to translate the word, whereas\nhumans would easily understand that the two words represent a single concept.\n\n\n150\nCHAPTER 6\nSequence-to-sequence models\nAt this point, the dataset is still a bunch of strings of characters. Most MT systems\noperate on words, so you need to tokenize the input (section 3.3) to identify words.\nDepending on the language, you may need to run a different pipeline (e.g., word seg-\nmentation is needed for Chinese and Japanese).\n The Tatoeba dataset you downloaded and expanded earlier has already gone\nthrough all this preprocessing pipeline. Now you are ready to hand the dataset over to\nFairseq. The first step is to tell Fairseq to convert the input files to the binary format so\nthat the training script can read them easily, as follows:\n$ fairseq-preprocess \\\n      --source-lang es \\\n      --target-lang en \\\n      --trainpref data/mt/tatoeba.eng_spa.train.tok \\\n      --validpref data/mt/tatoeba.eng_spa.valid.tok \\\n      --testpref data/mt/tatoeba.eng_spa.test.tok \\\n      --destdir data/mt-bin \\\n      --thresholdsrc 3 \\\n      --thresholdtgt 3\nWhen this succeeds, you should see a message Wrote preprocessed data to data/\nmt-bin on your terminal. You should also find the following group of files under the\ndata/mt-bin directory:\ndict.en.txt dict.es.txt  test.es-en.en.bin  test.es-en.en.idx  test.es-\nen.es.bin  test.es-en.es.idx  train.es-en.en.bin  train.es-en.en.idx  \ntrain.es-en.es.bin  train.es-en.es.idx  valid.es-en.en.bin  valid.es-\nen.en.idx  valid.es-en.es.bin  valid.es-en.es.idx\nOne of the key functionalities of this preprocessing step is to build the vocabulary\n(called the dictionary in Fairseq), which is a mapping from vocabulary items (usually\nwords) to their IDs. Notice the two dictionary files in the directory, dict.en.txt and\ndict.es.txt. MT deals with two languages, so the system needs to maintain two\nmappings, one for each language.\n6.3.2\nTraining the model\nNow that the train data is converted into the binary format, you are ready to train the\nMT model. Invoke the fairseq-train command with the directory where the\nbinary files are located, along with several hyperparameters, as shown next:\n$ fairseq-train \\\n    data/mt-bin \\\n    --arch lstm \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam \\\n    --lr 1.0e-3 \\\n    --max-tokens 4096 \\\n    --save-dir data/mt-ckpt\n\n\n151\nBuilding your first translator\nYou don’t have to worry about understanding what most of the parameters here mean\n(just yet). At this point, you need to know only that you are training a model using the\ndata stored in the directory specified by the first parameter (data/mt-bin) using an\nLSTM architecture (--arch lstm) with a bunch of other hyperparameters, and sav-\ning the results in data/mt-ckpt (short for “checkpoint”).\n When you run this command, your terminal will show two types of progress bars\nalternatively—one for training and another for validating, as shown here:\n| epoch 001:  16%|???▏                | 61/389 [00:13<01:23,  3.91it/s, \nloss=8.347, ppl=325.58, wps=17473, ups=4, wpb=3740.967, bsz=417.180, \nnum_updates=61, lr=0.001, gnorm=2.099, clip=0.000, oom=0.000, wall=17, \ntrain_wall=12]\n| epoch 001 | valid on 'valid' subset | loss 4.208 | ppl 18.48 | num_updates \n389\nThe lines corresponding to validation results are easily distinguishable by their\ncontents—they say “valid” subset. For each epoch, the training process alternates two\nstages: training and validation. An epoch, a concept used in machine learning, means\none pass through the entire train data. In the training stage, the loss is calculated\nusing the training data, then the model parameters are adjusted in such a way that the\nnew set of parameters lowers the loss. In the validation stage, the model parameters\nare fixed, and a separate dataset (validation set) is used to measure how well the\nmodel is performing against the dataset.\n I mentioned in chapter 1 that validation sets are used for model selection, a pro-\ncess where the best machine learning model is chosen among all the possible models\ntrained from a single training set. Here, by alternating between training and valida-\ntion stages, we use the validation set to check the performance of all the intermediary\nmodels (i.e., the model after the first epoch, the one after two epochs, and so on). In\nother words, we use the validation stage to monitor the progress of the training. \n Why is this a good idea? We gain many benefits by inserting the validation stage\nafter every epoch, but the most important one is to avoid overfitting—the very reason\nwhy a validation data is important in the first place. To illustrate this further, let’s look\nat how the loss changes over the course of the training of our Spanish-to-English MT\nmodel, for both the train and the validation sets, as shown in figure 6.7.\n As the training continues, the train loss becomes smaller and smaller and gradually\napproaches zero, because this is exactly what we told the optimizer to do: decrease the\nloss as much as possible. Checking whether the train loss is decreasing steadily epoch\nafter epoch is a good “sanity check” that your model and the training pipeline are\nworking as expected.\n On the other hand, if you look at the validation loss, it goes down at first for several\nepochs, but after a certain point, it gradually goes back up, forming a U-shaped\ncurve—a typical sign of overfitting. After several epochs of training, your model fits\nthe train set so well that it begins to lose its generalizability on the validation set.\n\n\n152\nCHAPTER 6\nSequence-to-sequence models\nFigure 6.7\nTrain and validation loss\nLet’s use a concrete example in MT to illustrate what’s really going on when a model is\noverfitted. For example, if your training data contains the English sentence “It is rain-\ning hard” and its Spanish translation “Esta lloviendo fuerte,” with no other sentences\nhaving the word “hard” in them, the overfitted model may believe that “fuerte” is the\nonly possible translation of “hard.” A properly fitted model might leave some wiggle\nroom for other Spanish words to appear as a translation for “hard,” but an overfitted\nMT system would always translate “hard” to “fuerte,” which is the “correct” thing to do\naccording to the train set but obviously not ideal if you’d like to build a robust MT sys-\ntem. For example, the best way to translate “hard” in “She is trying hard” is not “fuerte.”\n If you see your validation loss starting to creep up, there’s little point keeping the\ntraining process running, because chances are, your model has already overfitted to\nthe data to some extent. A common practice in such a situation, called early stopping, is\nto terminate the training. Specifically, if your validation loss is not improving for a cer-\ntain number of epochs, you stop the training and use the model at the point when the\nvalidation loss was the lowest. The number of epochs you wait until the training is ter-\nminated is called patience. In practice, the metric you care about the most (such as\nBLEU; see section 6.5.2) is used for early stopping instead of the validation loss.\n OK, that was enough about training and validating for now. The graph in figure\n6.7 indicates that the validation loss is lowest around epoch 8, so you can stop (by\npressing Ctrl + C) the fairseq-train command after around 10 epochs; otherwise,\n0\n2\n4\n6\n0\n10\n20\n30\nepoch\nloss\nstage\ntrain\nvalid\n\n\n153\nBuilding your first translator\nthe command would keep running indefinitely. Fairseq will automatically save the best\nmodel parameters (in terms of the validation loss) to the checkpoint_best.pt file.\nWARNING\nNote that the training may take a long time if you are just using a\nCPU. Chapter 11 explains how to use GPUs to accelerate the training.\n6.3.3\nRunning the translator\nAfter the model is trained, you can invoke the fairseq-interactive command to\nrun your MT model on any input in an interactive way. You can run the command by\nspecifying the binary file location and the model parameter file as follows:\n$ fairseq-interactive \\\n    data/mt-bin \\\n    --path data/mt-ckpt/checkpoint_best.pt \\\n    --beam 5 \\\n    --source-lang es \\\n    --target-lang en\nAfter you see the prompt Type the input sentence and press return, try typing\n(or copying and pasting) the following Spanish sentences one by one:\n¡ Buenos días !\n¡ Hola !\n¿ Dónde está el baño ?\n¿ Hay habitaciones libres ?\n¿ Acepta tarjeta de crédito ?\nLa cuenta , por favor .\nNote the punctuation and the whitespace in these sentences—Fairseq assumes that\nthe input is already tokenized. Your results may vary slightly, depending on many fac-\ntors (the training of deep learning models usually involves some randomness), but\nyou get something along the line of the following (I added boldface for emphasis):\n¡ Buenos días !\nS-0     ¡ Buenos días !\nH-0     -0.20546913146972656    Good morning !\nP-0     -0.3342 -0.3968 -0.0901 -0.0007\n¡ Hola !\nS-1     ¡ Hola !\nH-1     -0.12050756067037582    Hi !\nP-1     -0.3437 -0.0119 -0.0059\n¿ Dónde está el baño ?\nS-2     ¿ Dónde está el baño ?\nH-2     -0.24064254760742188    Where &apos;s the restroom ?\nP-2     -0.0036 -0.4080 -0.0012 -1.0285 -0.0024 -0.0002\n¿ Hay habitaciones libres ?\nS-3     ¿ Hay habitaciones libres ?\nH-3     -0.25766071677207947    Is there free rooms ?\nP-3     -0.8187 -0.0018 -0.5702 -0.1484 -0.0064 -0.0004\n¿ Acepta tarjeta de crédito ?\nS-4     ¿ Acepta tarjeta de crédito ?\nH-4     -0.10596384853124619    Do you accept credit card ?\n\n\n154\nCHAPTER 6\nSequence-to-sequence models\nP-4     -0.1347 -0.0297 -0.3110 -0.1826 -0.0675 -0.0161 -0.0001\nLa cuenta , por favor .\nS-5     La cuenta , por favor .\nH-5     -0.4411449432373047     Check , please .\nP-5     -1.9730 -0.1928 -0.0071 -0.0328 -0.0001\nMost of the output sentences here are almost perfect, except the fourth one (I would\ntranslate to “Are there free rooms?”). Even considering the fact that these sentences\nare all simple examples you can find in any travel Spanish phrasebook, this is not a\nbad start for a system built within an hour!\n6.4\nHow Seq2Seq models work\nIn this section, we will dive deep into the individual components that constitute a\nSeq2Seq model, which include the encoder and the decoder. We’ll also cover the algo-\nrithms used for decoding the target sentence—greedy decoding and beam search\ndecoding.\n6.4.1\nEncoder\nAs we saw in the beginning of this chapter, the encoder of a Seq2Seq model is not\nmuch different from the sequential-labeling models we covered in chapter 5. Its main\njob is to take the input sequence (usually a sentence) and convert it into a vector rep-\nresentation of a fixed length. You can use an LSTM-RNN as shown in figure 6.8.\nFigure 6.8\nEncoder of a Seq2Seq model\nUnlike sequential-labeling models, we need only the final hidden state of an RNN,\nwhich is then passed to the decoder to generate the target sentence. You can also use\na multilayer RNN as an encoder, in which case the sentence representation is the con-\ncatenation of the output of each layer, as illustrated in figure 6.9.\n  \nMaria\nno\nbaba\nverde\n.\n...\nSentence\nrepresentation\nRNN\ncell\nRNN\ncell\nRNN\ncell\nRNN\ncell\nRNN\ncell\n\n\n155\nHow Seq2Seq models work\nFigure 6.9\nUsing a multilayer RNN as an encoder\nSimilarly, you can use a bidirectional (or even a bidirectional multilayer) RNN as an\nencoder. The final sentence representation is a concatenation of the output of the for-\nward and the backward layers, as shown in figure 6.10.\nFigure 6.10\nUsing a bidirectional RNN as an encoder\nLayer 1\nLayer 2\n...\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\nSentence\nrepresentation\nMaria\nno\nbaba\nverde\n.\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nForward\nlayer\nBackward\nlayer\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\nForward\nlayer\nMaria\nno\nbaba\n.\nSentence\nrepresentation\n\n\n156\nCHAPTER 6\nSequence-to-sequence models\nNOTE\nThis is a small detail, but remember that an LSTM cell produces two\ntypes of output: the cell state and the hidden state (see section 4.2.2 for\nreview). When using LSTM for encoding a sequence, we usually just use the\nfinal hidden state while discarding the cell state. Think of the cell state as\nsomething like a temporary loop variable used for computing the final out-\ncome (the hidden state). See figure 6.11 for an illustration.\nFigure 6.11\nAn encoder using LSTM cells\n6.4.2\nDecoder\nLikewise, the decoder of a Seq2Seq model is similar to the language model we cov-\nered in chapter 5. In fact, they are identical except for one crucial difference—a\ndecoder takes an input from the encoder. The language models we covered in chapter\n5 are called unconditional language models because they generate language without any\ninput or precondition. On the other hand, language models that generate language\nbased on some input (condition) are called conditional language models. A Seq2Seq\ndecoder is one type of conditional language model, where the condition is the sen-\ntence representation produced by the encoder. See figure 6.12 for an illustration of\nhow a Seq2Seq decoder works.\nMaria\ninit\nstate\ncell\nstate\ncell\nstate\ncell\nstate\ncell\nstate\ncell\nstate\nhidden\nstate\nhidden\nstate\nhidden\nstate\nhidden\nstate\nhidden\nstate\nno\nbaba\nverde\n.\n...\nSentence\nrepresentation\nx\nLSTM\ncell\nLSTM\ncell\nLSTM\ncell\nLSTM\ncell\nLSTM\ncell\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nDecoder\nSentence\nrepresentation\nFigure 6.12\nA decoder \nof a Seq2Seq model\n\n\n157\nHow Seq2Seq models work\nJust as with language models, Seq2Seq decoders generate text from left to right. Like\nthe encoder, you can use an RNN to do this. A decoder can also be a multilayer RNN.\nHowever, a decoder cannot be bidirectional—you cannot generate a sentence from\nboth sides. As was mentioned in chapter 5, models that operate on the past sequence\nthey produced are called autoregressive models. \nHow the decoder behaves is a bit different between the training and the prediction\nstages. Let’s see how it is trained first. At the training stage, we know exactly how the\nsource sentence should be translated into the target sentence. In other words, we\nknow exactly what the decoder should produce, word by word. Because of this, decod-\ners are trained in a similar way to how sequential-labeling models are trained (see\nchapter 5).\n First, the decoder is fed the sentence representation produced by the encoder and\na special token <START>, which indicates the start of a sentence. The first RNN cell\nprocesses these two inputs and produces the first hidden state. The hidden state vec-\ntor is fed to a linear layer that shrinks or expands this vector to match the size of the\nvocabulary. The resulting vector then goes through softmax, which converts it to a\nprobability distribution. This distribution dictates how likely each word in the vocabu-\nlary is to come next. \n Then, this is where the training happens. If the input is “Maria no daba una bofetada\na la bruja verde,” then we would like the decoder to produce its English equivalent:\n“Mary did not slap the green witch.” This means that we would like to maximize the\nprobability that the first RNN cell generates “Mary” given the input sentence. This is a\nmulticlass classification problem we have seen many times so far in this book—word\nembeddings (chapter 3), sentence classification (chapter 4), and sequential labeling\n(chapter 5). You use the cross-entropy loss to measure how far apart the desired out-\ncome is from the actual output of your network. If the probability for “Mary” is large,\nthen good—the network incurs a small loss. On the other hand, if the probability for\n“Mary” is small, then the network incurs a large loss, which encourages the optimization\nalgorithm to change the parameters (magic constants) by a large amount.\nNon-autoregressive models\nIf you think simply generating text from left to right is too limiting, you have a good\npoint. Humans also do not always write language linearly—we often revise, add, and\ndelete words and phrases afterward. Also, generating text in a linear fashion is not\nvery efficient. The latter half of a sentence needs to wait until its first half is com-\npleted, which makes it very difficult to parallelize the generation process. As of this\nwriting, researchers are putting a lot of effort into developing non-autoregressive MT\nmodels that do not generate the target sentence in a linear fashion (see, for example,\nthis paper from Salesforce Research: https://arxiv.org/abs/1711.02281). However,\nthey haven’t exceeded autoregressive models in terms of translation quality, and\nmost research and production MT systems still adopt autoregressive models.\n\n\n158\nCHAPTER 6\nSequence-to-sequence models\n Then, we move on to the next\ncell. The next cell receives the hid-\nden state computed by the first cell\nand the word “Mary,” regardless of\nwhat the first cell generated. Instead of\nfeeding the token generated by the\nprevious cell, as we did when gener-\nating text using a language model,\nwe constrain the input to the\ndecoder so that it won’t “go astray.”\nThe second cell produces the hid-\nden state based on these two\ninputs, which is then used to com-\npute the probability distribution for\nthe second word. We compute the\ncross-entropy loss by comparing the\ndistribution against the desired out-\nput “did” and move on to the next\ncell. We keep doing this until we\nreach the final token, which is\n<END>. The total loss for the sen-\ntence is the average of all the losses\nincurred for all the words in the\nsentence, as shown in figure 6.13.\nFinally, the loss computed this\nway is used to adjust the model\nparameters of the decoder, so that\nit can generate the desired output\nthe next time around. Note that the parameters of the encoder are also adjusted in\nthis process, because the loss propagates all the way back to the encoder through the\nsentence representation. If the sentence representation produced by the encoder is\nnot good, the decoder won’t be able to produce high-quality target sentences no mat-\nter how hard it tries.\n6.4.3\nGreedy decoding\nNow let’s look at how the decoder behaves at the prediction stage, where a source sen-\ntence is given to the network, but we don’t know what the correct translation should\nbe. At this stage, a decoder behaves a lot like the language models we discussed in\nchapter 5. It is fed the sentence representation produced by the encoder, as well as a\nspecial token <START>, which indicates the start of a sentence. The first RNN cell pro-\ncesses these two inputs and produces the first hidden state, which is then fed to the lin-\near layer and the softmax layer to produce the probability distribution over the target\nvocabulary. Here comes the key part—unlike the training phase, you don’t know the\nstate\nstate\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\n \nSoftmax\nDistribution\nover \nvocabulary\n \nCross\nentropy\n \nRNN\n \nMary\nCross\nentropy\n \ndid\nCross\nentropy\n \nnot\n+\n+\n=\n<START>\nMary\ndid\nSentence\nrepresentation\n \nLinear\nlayer\n \nSoftmax\nLinear\nlayer\n \nSoftmax\nTotal\nloss\nFigure 6.13\nTraining a Seq2Seq decoder\n\n\n159\nHow Seq2Seq models work\ncorrect word to come next, so\nyou have multiple options. You\ncan choose any random word\nthat has a reasonably high prob-\nability (say, “dog”), but proba-\nbly the best you can do is pick\nthe word whose probability is\nthe highest (you are lucky if it’s\n“Mary”). The MT system pro-\nduces the word that was just\npicked and then feeds it to the\nnext RNN cell. This is repeated\nuntil the special token <END> is\nencountered. Figure 6.14 illus-\ntrates this process.\n OK, so are we all good, then?\nCan we move on to evaluating\nour MT system, because it is\ndoing everything it can to pro-\nduce the best possible transla-\ntion? Not so fast—many things\ncould go wrong by decoding the\ntarget sentence in this manner. \n First of all, the goal of MT\ndecoding is to maximize the\nprobability of the target sentence as a whole, not just individual words. This is exactly\nwhat you trained the network to do—to produce the largest probability for correct sen-\ntences. However, the way words are picked at each step described earlier is to maximize\nthe probability of that word only. In other words, this decoding process guarantees only\nthe locally maximum probability. This type of myopic, locally optimal algorithm is\ncalled greedy in computer science, and the decoding algorithm I just explained is called\ngreedy decoding. However, just because you are maximizing the probability of individual\nwords at each step doesn’t mean you are maximizing the probability of the whole sen-\ntence. Greedy algorithms, in general, are not guaranteed to produce the globally opti-\nmal solution, and using greedy decoding can leave you stuck with suboptimal\ntranslations. This is not very intuitive to understand, so let me use a simple example to\nillustrate this.\n When you are picking words at each timestep, you have multiple words to pick\nfrom. You pick one of them and move on to the next RNN cell, which produces\nanother set of possible words to pick from, depending on the word you picked previ-\nously. This can be represented using a tree structure like the one shown in figure 6.15.\nThe diagram shows how the word you pick at one timestep (e.g., “did”) branches out\nto another set of possible words (“you” and “not”) to pick from at the next timestep.\nstate\nstate\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\nSoftmax\nDistribution\nover\nvocabulary\nRNN\n<START>\nMary\nMary\ndid\nnot\ndid\nSentence\nrepresentation\nSoftmax\nSoftmax\nLinear\nlayer\nLinear\nlayer\nFigure 6.14\nA prediction using a Seq2Seq decoder\n\n\n160\nCHAPTER 6\nSequence-to-sequence models\nFigure 6.15\nA decoding decision tree\nEach transition from word to word is labeled with a score, which corresponds to how\nlarge the probability of choosing that transition is. Your goal here is to maximize the\ntotal sum of the scores when you traverse one path from timestep 1 to 4. Mathemati-\ncally, probabilities are real numbers between 0 to 1, and you should multiply (instead\nof add) each probability to get the total, but I’m simplifying things here. For example,\nif you go from “Mary” to “did,” then on to “you” and “do,” you just generated a sen-\ntence “Mary did you do” and the total score is 1 + 5 + 1 = 7.\n The greedy decoder we saw earlier will face two choices after it generates “did” at\ntimestep 2: either generate “you” with a score of 5 or “not” with a score of 3. Because\nall it does is pick the one with the highest score, it will pick “you” and move on. Then\nit will face another branch after timestep 3—generating “do” with a score of 1 or gen-\nerating “know” with a score of 2. Again, it will pick the largest score, and you will end\nup with the translation “Mary did you know” whose score is 1+ 5 + 1 = 8.\n This is not a bad result. At least, it is not as bad as the first path, which sums up to a\nscore of 7. By picking the maximum score at each branch, you are making sure that\nyour final result is at least decent. However, what if you picked “not” at timestep 3? At\nfirst glance, this doesn’t seem like a good idea, because the score you get is only 3,\nwhich is smaller than you’d get by taking the other path, 5. But at the next timestep,\nby generating “slap,” you get a score of 5. In retrospect, this was the right thing to\ndo—in total, you get 1 + 3 + 5 = 9, which is larger than any scores you’d get by taking\nthe other “you” path. By sacrificing short-term rewards, you are able to gain even\nlarger rewards in the long run. But due to the myopic nature of the greedy decoder, it\nwill never choose this path—it can’t backtrack and change its mind once it’s taken one\nbranch over another.\n Choosing which way to go to maximize the total score seems easy if you look at the\ntoy example in figure 6.15, but in reality, you can’t “foresee” the future—if you are at\ntimestep t, you can’t predict what will happen at timestep t + 1 and onward, until you\nMary\n1\n1\n2\n3\n4\nTimestep\nTotal score\ndid\nnot\nslap\nyou\ndo\nknow\n5\n3\n5\n1\n1 + 5 + 1 = 7\n1 + 5 + 2 = 8\n1 + 3 + 5 = 9\n2\n\n\n161\nHow Seq2Seq models work\nactually choose one word and feed it to the RNN. But the path that maximizes the\nindividual probability is not necessarily the optimal solution. You just can’t try every\npossible path and see what score you’d get, either, because the vocabulary usually con-\ntains tens of thousands of unique words, meaning the number of possible paths is\nexponentially large.\n The sad truth is that you can’t realistically expect to find the optimal path that\nmaximizes the probability for the entire sentence within a reasonable amount of time.\nBut you can avoid being stuck (or at least, make it less likely to be stuck) with a sub-\noptimal solution, which is what the beam search decoder does.\n6.4.4\nBeam search decoding\nLet’s think what you would do if you were in the same situation. Let’s use an analogy\nand say you are a college sophomore and need to decide which major to pursue by the\nend of the school year. Your goal is to maximize the total amount of income (or happi-\nness or whatever thing you care about) over the course of your lifetime, but you don’t\nknow which major is the best for this. You can’t simply try every possible major and see\nwhat happens after a couple of years—there are too many majors and you can’t go\nback in time. Also, just because some particular majors look appealing in the short\nrun (e.g., choosing an economics major may lead to some good internship opportuni-\nties at large investment banks) doesn’t mean that path is the best in the long run (see\nwhat happened in 2008). \n In such a situation, one thing you could do is to hedge your bet by pursuing more\nthan one major (as a double major or a minor) at the same time instead of commit-\nting 100% to one particular major. After a couple of years, if the situation is more dif-\nferent than you had imagined, you can still change your mind and pursue another\noption, which is not possible if you choose your major greedily (i.e., based only on the\nshort-term prospects).\n The main idea of beam search decoding is similar to this—instead of committing\nto one path, it purses multiple paths (called hypotheses) at the same time. In this way,\nyou leave some room for “dark horses,” that is, hypotheses that had low scores at first\nbut may prove promising later. Let’s see this in action using the example in figure\n6.16, a slightly modified version of figure 6.15.\n The key idea of beam search decoding is to use a beam (figure 6.16 bottom), which\nyou can think of as some sort of buffer that can retain multiple hypotheses at the same\ntime. The size of the beam, that is, the number of hypotheses it can retain, is called the\nbeam width. Let’s use a beam of size 2 and see what happens. Initially, your first hypoth-\nesis consists of only one word, “Mary,” and a score of 0. When you move on to the next\nword, the word you chose is appended to the hypothesis, and the score is incremented\nby the score of the path you have just taken. For example, when you move on to “did,”\nit will make a new hypothesis consisting of “Mary did” and a score of 1.\n If you have multiple words to choose from at any particular timestep, a hypothesis\ncan spawn multiple child hypotheses. At timestep 2, you have three different\n\n\n162\nCHAPTER 6\nSequence-to-sequence models\nchoices—“you,” “not,” and “n’t”—which generate three new child hypotheses: [Mary\ndid you] (6), [Mary did not] (4), and [Mary did n’t] (3). And here’s the key part of\nbeam search decoding: because there’s only so much room in the beam, any hypothe-\nses that are not good enough fall off of the beam after sorting them by their scores.\nBecause the beam can hold up to only two hypotheses in this example, anything\nexcept the top two gets kicked out of the beam, which leaves [Mary did you] (6) and\n[Mary did not] (4).\n At timestep 3, each remaining hypothesis can spawn up to two child hypotheses.\nThe first one ([Mary did you] (6)) will generate [Mary did you know] (8) and [Mary\ndid you do] (7), whereas the second one ([Mary did not] (4)) turns into [Mary did\nnot slap] (9). These three hypotheses are sorted by their scores, and the best two will\nbe returned as the result of the beam search decoding.\n Congratulations—now your algorithm was able to find the path that maximizes the\ntotal sum of the scores. By considering multiple hypotheses at the same time, beam\nsearch decoding can increase the chance that you will find better solutions. However,\nit is never perfect—notice that an equally good path [Mary did n’t do] with a score of\n9 fell out of the beam as early as timestep 3. To “rescue” it, you’d need to increase the\nbeam width to 3 or larger. In general, the larger the beam width, the higher the\nexpected quality of the translation results will be. However, there’s a tradeoff: because\nMary\n1\n1\n2\n3\n4\nTimestep\ndid\nnot\nn’t\nslap\nyou\ndo\ndo\nknow\n5\n3\n2\n5\n6\n1\n2\n[Mary] 0\n[Mary did] 1\n[Mary did you] 6\n[Mary did you do] 7\n[Mary did you know] 8\n[Mary did not slap] 9\n[Mary did not] 4\n[Mary did n’t] 3\nBeam\n(width = 2)\n \nFigure 6.16\nBeam search decoding\n\n\n163\nEvaluating translation systems\nthe computer needs to consider multiple hypotheses, it will be linearly slower as the\nbeam width increases.\n In Fairseq, you can use the --beam option to change the beam size. In the exam-\nple in section 6.3.3, I used --beam 5 to use a beam width of 5. You were already using\nbeam search without noticing. If you invoke the same command with --beam 1,\nwhich means you are using greedy decoding instead of a beam search, you may get\nslightly different results. When I tried this, I got almost the same results except the last\none: “counts, please,” which is not a great translation for “La cuenta, por favor.” This\nmeans using a beam search indeed helps improve the translation quality!\n6.5\nEvaluating translation systems\nIn this section, I’d like to briefly touch on the topic of evaluating machine translation\nsystems. Accurately evaluating MT systems is an important topic, both in theory and\npractice.\n6.5.1\nHuman evaluation\nThe simplest and the most accurate way to evaluate MT systems’ output is to use\nhuman evaluation. After all, language is translated for humans. Translations that are\ndeemed good by humans should be good.\n As mentioned previously, we have a few considerations for what makes a translation\ngood. There are two most important and commonly used concepts for this—adequacy\n(also called fidelity) and fluency (also closely related to intelligibility). Adequacy is the\ndegree to which the information in the source sentence is reflected in the translation.\nIf you can reconstruct a lot of information expressed by the source sentence by reading\nits translation, then the translation has high adequacy. Fluency is, on the other hand,\nhow natural the translation is in the target language. If you are translating into English,\nfor example, “Mary did not slap the green witch” is a fluent translation, whereas “Mary\nno had a hit with witch, green” is not, although both translations are almost equally\nadequate. Note that these two aspects are somewhat independent—you can think of a\ntranslation that is fluent but not adequate (e.g., “Mary saw a witch in the forest” is a per-\nfectly fluent but inadequate translation) and vice versa, like the earlier example. MT\nsystems that produce output that is both adequate and fluent are desirable.\n An MT system is usually evaluated by presenting its translations to human annota-\ntors and having them judge its output on a 5- or 7-point scale for each aspect. Fluency\nis easier to judge because it requires only monolingual speakers of the target sentence,\nwhereas adequacy requires bilingual speakers of both the source and target languages.\n6.5.2\nAutomatic evaluation\nAlthough human evaluation gives the most accurate assessment to MT systems’ qual-\nity, it’s not always feasible. In most cases, you cannot afford to hire human evaluators\nto assess an MT system’s output every time you need it. If you are dealing with lan-\nguage pairs that are not common, you might not be able to find bilingual speakers for\nevaluating adequacy at all.\n\n\n164\nCHAPTER 6\nSequence-to-sequence models\n But more importantly, you need to constantly evaluate and monitor an MT sys-\ntem’s quality when you are developing one. For example, if you use a Seq2Seq model\nto train an NMT system, you need to reevaluate its performance every time you adjust\none of the hyperparameters. Otherwise, you wouldn’t know whether your change has\na good or bad effect on its final performance. Even worse, if you were to do something\nlike early stopping (see section 6.3.2) to determine when to stop the training process,\nyou would need to evaluate its performance after every epoch. You can’t possibly hire\nsomebody and have them evaluate your intermediate models at each epoch—that\nwould be a terribly slow way to develop an MT system. It’s also a huge waste of time,\nbecause the output of initial models is largely garbage and does not warrant human\nevaluation. A large amount of correlation exists between the outputs of intermediate\nmodels, and human evaluators would be spending a lot of time evaluating very similar,\nif not identical, sentences.\n For this reason, it’d be desirable if we could use some automatic way to assess trans-\nlation quality. The way this works is similar to some automatic metrics for other NLP\ntasks that we saw earlier, such as accuracy, precision, recall, and F1-measure for classifi-\ncation. The idea is to create the desirable output for each input instance in advance\nand compare a system’s output against it. This is usually done by preparing a set of\nhuman-created translations called reference for each source sentence and calculating\nsome sort of similarity between the reference and a system’s output. Once you create\nthe reference and define the metric, you can automatically evaluate translation quality\nas many times as you want.\n One of the simplest ways to compute the similarity between the reference and a\nsystem output is to use the word error rate (WER). WER reflects how many errors the\nsystem made compared to the reference, measured by the relative number of inser-\ntions, deletions, and substitutions. The concept is similar to the edit distance, except\nthat WER is counted for words, not characters. For example, when the reference sen-\ntence is “Mary did not slap the green witch” and a system translation is “Mary did hit\nthe green wicked witch,” you need three “edits” to match the latter to the former—\ninsert “not,” replace “hit” with “slap,” and delete “wicked.” If you divide three by the\nlength of the reference (= 7), it’s your WER (= 3/7, or 0.43). The lower the WER, the\nbetter the quality of your translation.\n Although WER is simple and easy to compute, it is not widely used for evaluating\nMT systems nowadays. One reason is related to multiple references. There may be\nmultiple, equally valid translations for a single source sentence, but it is not clear how\nto apply WER when there are multiple references. A slightly more advanced and by far\nthe most commonly used metric for automatic evaluation in MT is BLEU (bilingual\nevaluation understudy). BLEU solves the problem of multiple references by using\nmodified precision. I’ll illustrate this next using a simple example.\n In the following table, we are evaluating a candidate (a system’s output) “the the\nthe the the the the” (which is, by the way, a terrible translation) against two refer-\nences: “the cat is on the mat” and “there is a cat on the mat.” The basic idea of BLEU\n\n\n165\nCase study: Building a chatbot\nis to calculate the precision of all unique words in the candidate. Because there’s only\none unique word in the candidate, “the,” if you calculate its precision, it will automati-\ncally become the candidate’s score, which is 1, or 100%. But there seems to be some-\nthing wrong about this.\nBecause only two “thes” exist in the references, the spurious “thes” generated by the\nsystem shouldn’t count toward the precision. In other words, we should treat them as\nfalse positives. We can do this by capping the denominator of precision by the maxi-\nmum number of occurrences of that word in any of the references. Because it’s 2 in\nthis case (in reference 1), its modified precision will be 2/7, or about 29%. In prac-\ntice, BLEU uses not only unique words (i.e., unigrams) but also all unique sequences\nof words (n-grams) up to a length of 4 in the candidate and the references. \n However, we can game this metric in another way—because it’s based on precision,\nnot on recall, an MT system can easily obtain high scores by producing very few words\nthat the system is confident about. In the previous example, you can simply produce\n“cat” (or even more simply, “the”), and the BLEU score will be 100%, which is obvi-\nously not a good translation. BLEU solves this issue by introducing the brevity penalty,\nwhich discounts the score if the candidate is shorter than the references.\n Development of accurate automatic metrics has been an active research area.\nMany new metrics are proposed and used to address the shortcomings of BLEU. We\nbarely scratched the surface in this section. Although new metrics show higher cor-\nrelations with human evaluations and are claimed to be better, BLEU is still by far the\nmost widely used metric, mainly due to its simplicity and long tradition.\n6.6\nCase study: Building a chatbot\nIn this section, I’m going to go over another application of a Seq2Seq model—a chat-\nbot, which is an NLP application with which you can have a conversation. We are\ngoing to build a very simple yet functional chatbot using a Seq2Seq model and discuss\ntechniques and challenges in building intelligent agents.\n6.6.1\nIntroducing dialogue systems\nI briefly touched upon dialogue systems in section 1.2.1. To recap, two main types of\ndialogue systems exist: task-oriented and chatbots. Although task-oriented dialogue\nsystems are used to achieve some specific goals, such as making a reservation at a\nrestaurant and obtaining some information, chatbots are used to have conversations\nwith humans. Conversational technologies are currently a hot topic among NLP prac-\ntitioners, due to the success and proliferation of commercial conversational AI sys-\ntems such as Amazon Alexa, Apple Siri, and Google Assistant. \nCandidate\nthe\nthe\nthe\nthe\nthe\nthe\nthe\nReference 1\nthe\ncat\nis\non\nthe\nmat\nReference 2\nthere\nis\na\ncat\non\nthe\nmat\n",
      "page_number": 169
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 186-193)",
      "start_page": 186,
      "end_page": 193,
      "detection_method": "topic_boundary",
      "content": "166\nCHAPTER 6\nSequence-to-sequence models\n You may not have a clue as to how we can get started with building an NLP applica-\ntion that can have conversations. How can we build something “intelligent” that\n“thinks” so that it can generate meaningful responses to human input? This seems far-\nfetched and difficult. But if you step back and look at a typical conversation we have\nwith other people, how much of it is actually “intelligent?” If you are like most of us, a\nlarge fraction of the conversation you are having is autopilot: “How are you?” “I’m\ndoing good, thanks” “Have a good day” “You, too!” and so on. You may also have a set\nof “template” responses to a lot of everyday questions such as “What do you do?” and\n“Where are you from?” These questions can be answered just by looking at the input.\nEven more complex questions like “What’s your favorite restaurant in X?” (where X is\nthe name of a neighborhood in your city) and “Did you see any Y movies lately?”\n(where Y is a genre) can be answered just by “pattern matching” and retrieving rele-\nvant information from your memory. \n If you think of a conversation as a set of “turns” where the response is generated by\npattern matching against the previous utterance, this starts to look a lot like a typical\nNLP problem. In particular, if you regard dialogues as a problem where an NLP sys-\ntem is simply converting your question to its response, this is exactly where we can\napply the Seq2Seq models we covered in this chapter so far. We can treat the previous\n(human’s) utterance as a foreign sentence and have the chatbot “translate” it into\nanother language. Even though these two languages are both English in this case, it is\na common practice in NLP to treat the input and the output as two different lan-\nguages and apply a Seq2Seq model to them, including summarization (longer text to\na shorter one) and grammatical error correction (text with errors to one without).\n6.6.2\nPreparing a dataset\nIn this case study, we are going to use The Self-dialogue Corpus (https://github.com/\njfainberg/self_dialogue_corpus), a collection of 24,165 conversations. What’s special\nabout this dataset is that these conversations are not actual ones between two people,\nbut fictitious ones written by one person who plays both sides. You could use several\nconversation datasets for text-based chatbots (e.g., the OpenSubtitles dataset, http://\nopus.nlpl.eu/OpenSubtitles-v2018.php), but these datasets are often noisy and often\ncontain obscenities. By collecting made-up conversations instead, the Self-dialogue\nCorpus improves the quality for half the original cost (because you need only one per-\nson versus two people!).\n The same as earlier, I tokenized and converted the corpus into a format that is\ninterpretable by Fairseq. You can obtain the converted dataset as follows:\n$ mkdir -p data/chatbot\n$ wget https://realworldnlpbook.s3.amazonaws.com/data/chatbot/selfdialog.zip\n$ unzip selfdialog.zip -d data/chatbot\nYou can use the following combination of the paste command (to stitch files hori-\nzontally) and the head command to peek at the beginning of the training portion.\n\n\n167\nCase study: Building a chatbot\nNote that we are using fr (for “foreign,” not “French”) to denote the “language” we\nare translating from:\n$ paste data/chatbot/selfdialog.train.tok.fr data/chatbot/\nselfdialog.train.tok.en | head \n...\nHave you played in a band ?    What type of band ?\nWhat type of band ?    A rock and roll band .\nA rock and roll band .    Sure , I played in one for years .\nSure , I played in one for years .    No kidding ?\nNo kidding ?    I played in rock love love .\nI played in rock love love .    You played local ?\nYou played local ?    Yes\nYes    Would you play again ?\nWould you play again ?    Why ?\n...\nAs you can see, each line consists of an utterance (on the left) and a response to it (on\nthe right). Notice that this dataset has the same structure as the Spanish-English paral-\nlel corpus we used in section 6.3.1. The next step is to run the fairseq-preprocess\ncommand to convert it to a binary format as follows:\n$ fairseq-preprocess \\\n    --source-lang fr \\\n    --target-lang en \\\n    --trainpref data/chatbot/selfdialog.train.tok \\\n    --validpref data/chatbot/selfdialog.valid.tok \\\n    --destdir data/chatbot-bin \\\n    --thresholdsrc 3 \\\n    --thresholdtgt 3\nAgain, this is similar to what we ran for the Spanish translator example. Just pay atten-\ntion to what you specify as the source language—we are using fr instead of es here.\n6.6.3\nTraining and running a chatbot\nNow that the training data for the chatbot is ready, let’s train a Seq2Seq model from\nthis data. You can invoke the fairseq-train command with almost identical param-\neters to the last time, as shown next:\n$ fairseq-train \\\n    data/chatbot-bin \\\n    --arch lstm \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam \\\n    --lr 1.0e-3 \\\n    --max-tokens 4096 \\\n    --save-dir data/chatbot-ckpt\nAs previously, pay attention to how the validation loss changes every epoch. When I\ntried this, the validation loss decreased for about five epochs but then started to slowly\ncreep back up. Feel free to stop the training command by pressing Ctrl + C after you\n\n\n168\nCHAPTER 6\nSequence-to-sequence models\nobserve the validation loss leveling out. Fairseq will automatically save the best model\n(measured by the validation loss) to checkpoint_best.pt.\n Finally, you can run the chatbot model by invoking the fairseq-interactive\ncommand, as shown here:\n$ fairseq-interactive \\\n    data/chatbot-bin \\\n    --path data/chatbot-ckpt/checkpoint_best.pt \\\n    --beam 5 \\\n    --source-lang fr \\\n    --target-lang en\nAs with the previous example, you can type your source sentences and have a conver-\nsion with your chatbot by having them “translate” to another language! Here’s part of\na conversation that I had with the model that I trained (I added boldface for clarity).\nAgain, your results might be different:\nHi !\nS-0     Hi !\nH-0     -1.011711835861206      Hey .\nP-0     -0.6860 -2.1352 -0.2140\nHow are you ?\nS-1     How are you ?\nH-1     -0.6552960276603699     Doing good , you ?\nP-1     -0.6472 -0.8651 -1.0960 -0.9257 -0.3337 -0.0640\nGood .\nS-2     Good .\nH-2     -0.8540937900543213     Do you have a favorite team ?\nP-2     -2.3027 -0.0550 -1.7088 -0.4403 -0.1763 -2.0262 -0.1150 -0.0085\nYeah I like the Seahawks .\nS-3     Yeah I like the Seahawks .\nH-3     -0.6036218404769897     Do you have a favorite team ?\nP-3     -2.7614 -0.0525 -1.0905 -0.3178 -0.3213 -0.1049 -0.1701 -0.0106\nHow about you ?\nS-4     How about you ?\nH-4     -1.2329542636871338     I 'm a big fan of them .\nP-4     -0.7648 -1.8435 -1.6808 -1.2868 -0.2102 -0.1362 -3.5273 -1.0031 -0.6438\nWhat 's your favorite movie ?\nS-5     What 's your favorite movie ?\nH-5     -0.8238655924797058     Oh , that 's a tough one . I 'd have to say \nGhost Busters .\nP-5     -3.2622 -0.3213 -1.4815 -0.1012 -0.3417 -0.3069 -0.2432 -0.6954 -\n1.0858 -2.3246 -0.4296 -0.0039 -0.0905 -2.6429 -0.1676 -0.3853 -0.1221\nOh yeah , I like that movie too .\nS-6     Oh yeah , I like that movie too .\nH-6     -1.0659444332122803     Yeah , that 's a good one .\nP-6     -2.0782 -0.1724 -2.5089 -0.9560 -1.5034 -0.9040 -0.4471 -0.5435 -0.4801\nIn this example, the conversation looks natural. Because the Self-dialogue Corpus is\nbuilt by restricting the set of possible conversation topics, the conversation is more\nlikely to go smoothly if you stay on such topics (movie, sports, music, and so on).\n\n\n169\nCase study: Building a chatbot\n However, as soon as you start talking about unfamiliar topics, the chatbot loses its\nconfidence in its answers, as shown next:\nWhat 's your name ?\nS-0     What 's your name ?\nH-0     -0.9807574152946472     I do n't know , but I do n't think I 've\n                           heard of them .\nP-0     -1.4680 -2.2938 -0.0635 -1.0242 -1.2039 -0.5201 -0.3381 -2.2612 -\n0.1665 -1.6033 -0.6112 -1.5515 -0.8739 -0.8453 -1.0143 -0.4702 -0.3637\nWhat do you do ?\nS-1     What do you do ?\nH-1     -1.170507788658142      I do n't know .\nP-1     -0.9807 -2.1846 -0.3276 -0.9733 -1.3650 -1.1919\nAre you a student ?\nS-2     Are you a student ?\nH-2     -0.9505285024642944     I 'm not sure .\nP-2     -1.5676 -1.5270 -0.6944 -0.2493 -0.8445 -0.8204\nThis is a well-known phenomenon—a simple Seq2Seq-based chatbot quickly regresses\nto producing cookie-cutter answers such as “I don’t know” and “I’m not sure” when-\never asked about something it’s not familiar with. This has to do with the way we\ntrained this chatbot. Because we trained the model so that it minimizes the loss in the\ntraining data, the best strategy it can take to reduce the loss is to produce something\napplicable to as many input sentences as possible. Very generic phrases such as “I\ndon’t know” can be an answer for many questions, so it’s a great way to play it safe and\nreduce the loss!\n6.6.4\nNext steps\nAlthough our chatbot can produce realistic-looking responses for many inputs, it’s far\nfrom perfect. One issue that it’s not great at dealing with is proper nouns. You can see\nthis when you ask questions that solicit specific answers, like the following:\nWhat 's your favorite show ?\nS-0     What 's your favorite show ?\nH-0     -0.9829921722412109     I would have to say <unk> .\nP-0     -0.8807 -2.2181 -0.4752 -0.0093 -0.0673 -2.9091 -0.9338 -0.3705\nHere <unk> is the catch-all special symbol for unknown words. The chatbot is trying to\nanswer something, but that something occurs too infrequently in the training data to\nbe treated as an independent word. This is an issue seen in simple NMT systems in gen-\neral. Because the models need to cram everything about a word in a 200-something-\ndimensional vector of numbers, many details and distinctions between similar words\nare sacrificed. Imagine compressing all the information about all the restaurants in\nyour city into a 200-dimensional vector!\n Also, the chatbot we trained doesn’t have any “memory” or any notion of context\nwhatsoever. You can test this by asking a series of related questions as follows:\nDo you like Mexican food ?\nS-0     Do you like Mexican food ?\n\n\n170\nCHAPTER 6\nSequence-to-sequence models\nH-0     -0.805641770362854      Yes I do .\nP-0     -1.0476 -1.1101 -0.6642 -0.6651 -0.5411\nWhy do you like it ?\nS-1     Why do you like it ?\nH-1     -1.2453081607818604     I think it 's a great movie .\nP-1     -0.7999 -2.1023 -0.7766 -0.7130 -1.4816 -2.2745 -1.5750 -1.0524 -0.4324\nIn the second question, the chatbot is having difficulties understanding the context\nand produces a completely irrelevant response. To answer such questions correctly,\nthe model needs to understand that the pronoun “it” refers to a previous noun,\nnamely, “Mexican food” in this case. The task where NLP systems resolve which men-\ntions refer to which entities in the real world is called coreference resolution. The system\nalso needs to maintain some type of memory to keep track of what was discussed so far\nin the dialogue. \n Finally, the simple Seq2Seq models we discussed in this chapter are not great at\ndealing with long sentences. If you look back at figure 6.2, you’ll understand why—the\nmodel reads the input sentence using an RNN and represents everything about the\nsentence using a fixed-length sentence representation vector and then generates the\ntarget sentence from that vector. It doesn’t matter whether the input is “Hi!” or “The\nquick brown fox jumped over the lazy dog.” The sentence representation becomes a\nbottleneck, especially for longer input. Because of this, neural MT models couldn’t\nbeat traditional phrase-based statistical MT models until around 2015, when a mecha-\nnism called attention was invented to tackle this very problem. We’ll discuss attention\nin detail in chapter 8. \nSummary\nSequence-to-sequence (Seq2Seq) models transform one sequence into another\nusing an encoder and a decoder.\nYou can use the fairseq framework to build a working MT system within an\nhour.\nA Seq2Seq model uses a decoding algorithm to generate the target sequence.\nGreedy decoding maximizes the probability at each step, whereas beam search\ntries to find better solutions by considering multiple hypotheses at once.\nA metric called BLEU is commonly used for automatically evaluating MT systems.\nA simple chatbot can be built by using a Seq2Seq model and a conversation\ndataset.\n\n\n171\nConvolutional\nneural networks\nIn previous chapters, we covered linear layers and RNNs, two main neural network\narchitectures commonly used in NLP. In this chapter, we introduce another import-\nant class of neural networks called convolutional neural networks (CNNs). CNNs have\ndifferent characteristics than RNNs that make them suitable for NLP tasks where\ndetecting linguistic patterns is important, such as text classification.\nThis chapter covers\nSolving text classification by detecting patterns\nUsing convolutional layers to detect patterns \nand produce scores\nUsing pooling layers to aggregate the scores \nproduced by convolution\nBuilding a convolutional neural network (CNN) \nby combining convolution and pooling\nBuilding a CNN-based text classifier using \nAllenNLP\n\n\n172\nCHAPTER 7\nConvolutional neural networks\n7.1\nIntroducing convolutional neural networks (CNNs)\nThis section introduces convolutional neural networks (CNNs), another type of neu-\nral network architecture that operates in a different way from how RNNs work. CNNs\nare particularly good at pattern-matching tasks and are increasingly popular in the\nNLP community.\n7.1.1\nRNNs and their shortcomings\nIn chapter 4, we covered sentence classification, which is an NLP task that receives\nsome text as the input and produces a label for it. We also discussed how to use recur-\nrent neural networks (RNNs) for that task. As a refresher, an RNN is a type of neural\nnetwork that has a “loop” in it, which processes the input sequence one element at a\ntime from the beginning until the end. The internal loop variable, which is updated at\nevery step, is called the hidden state. When the RNN finishes processing the entire\nsequence, the hidden state at the final timestep represents the compressed content of\nthe input sequence, which can be used for NLP tasks including sentence classification.\nAlternatively, you can take out the hidden state after every step and use it to assign\nlabels (such as PoS and named entity tags) to individual words. The structure that is\napplied repeatedly in the loop is called a cell. An RNN with a simple multiplication\nand nonlinearity is called a vanilla or an Elman RNN. On the other hand, LSTM and\nGRU-based RNNs use more complicated cells that employ memory and gating.\n RNNs are a powerful tool in modern NLP with a wide range of applications; how-\never, they are not without shortcomings. First, RNNs are slow—they need to scan the\ninput sequence element by element no matter what. Their computational complexity\nis proportional to the length of the input sequences. Second, due to their sequential\nnature, RNNs are hard to parallelize. Think of a multilayer RNN where multiple RNN\nlayers are stacked on top of each other (as shown in figure 7.1). In a naive implemen-\ntation, each layer needs to wait until all the layers below it finish processing the input.\n Third, the RNN structure is simply overkill and inefficient for some tasks. For\nexample, recall the task of detecting grammatical English sentences that we covered\nin chapter 4. In its simplest form, the task is to recognize valid and invalid subject-verb\nagreement in a two-word sentence. If a sentence contains phrases such as “I am” and\n“you are,” it’s grammatical. If it contains “I are” or “you am,” it’s not. In chapter 4, we\nbuilt a simple LSTM-RNN with a nonlinearity to recognize the grammaticality of two-\nword sentences with a vocabulary of four words. But what if you need to classify\nwhether an arbitrary long sentence with a very large vocabulary is grammatical? Sud-\ndenly, this process starts to sound very complex. Your LSTM needs to learn to pick up\nthe signal (subject-verb agreement) from a large amount of noise (all other words and\nphrases that have nothing to do with agreement), while learning to do all this using\nthe update operation that gets repeated for every single element of the input.\n But if you think about it, no matter how long the sentence is or how large the\nvocabulary is, your network’s job should still be quite simple—if the sentence contains\nvalid collocations (such as “I am” and “you are”), it’s grammatical. Otherwise, it’s not.\n\n\n173\nIntroducing convolutional neural networks (CNNs)\nThe task is actually not very far from the “if-then” sentiment analyzer that we saw in\nchapter 1. It is obvious that the structure of LSTM RNNs is overkill for this task, where\nsimple pattern matching over words and phrases would suffice.\n7.1.2\nPattern matching for sentence classification\nIf you look at text classification in general, many tasks can be effectively solved by this\n“pattern matching.” Take spam filtering, for example—if you want to detect spam\nemails, simply look for words and phrases such as “v1agra” and “business opportunity”\nwithout even reading the entire email; it doesn’t matter where these patterns appear. If\nyou want to detect sentiment from movie reviews, detecting positive and negative words\nsuch as “amazing” and “awful” would go a long way. In other words, learning and detect-\ning such local linguistic patterns, regardless of their location, is an effective and effi-\ncient strategy for text-classification tasks, and possibly for other NLP tasks as well.\n In chapter 3, we learned the concept of n-grams—contiguous sequences of one or\nmore words. They are often used in NLP as proxies for more formally defined linguis-\ntic units such as phrases and clauses. If there’s some tool that can wade through a\nlarge amount of noise in text and detect n-grams that serve as signals, it would be a\ngreat fit for text classification.\nLayer 1\nLayer 2\ninit_state()\n...\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\ninit_state()\nstate\nstate\nstate\nstate\nstate\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\"arrow\")\nv(\".\")\nFigure 7.1\nMultilayer RNN\n",
      "page_number": 186
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 194-201)",
      "start_page": 194,
      "end_page": 201,
      "detection_method": "topic_boundary",
      "content": "174\nCHAPTER 7\nConvolutional neural networks\n7.1.3\nConvolutional neural networks (CNNs)\nConvolutional neural networks, or CNNs, do exactly this. A CNN is a type of neural\nnetwork that involves a mathematical operation called convolution, which, put simply,\ndetects local patterns that are useful for the task at hand. A CNN usually consists of\none or more convolutional layers, which do convolution, and pooling layers, which\nare responsible for aggregating the result of convolution. See figure 7.2 for a diagram.\nSections 7.2 and 7.3 provide some detail of convolutional layers and pooling layers,\nrespectively.\nCNNs, which are inspired by the visual system in the human brain, have been widely\nused for computer vision tasks such as image classification and object detection. In\nrecent years, the use of CNNs has been increasingly popular in NLP, especially for\ntasks such as text classification, sequential labeling, and machine translation.\n7.2\nConvolutional layers\nIn this section, we’ll discuss convolutional layers, the essential part of the CNN archi-\ntecture. The term convolution may sound a bit scary, but at its essence, it’s just pattern\nmatching. We’ll use diagrams and intuitive examples to illustrate how it really works.\nConvolutional neural network (CNN)\nConvolutional\nlayer\n \nPooling\nlayer\n \nv(\"i\")\nv(\"am\")\nv(\"a\")\nv(\"student\")\nv(\".\")\nLinear\nlayer\n \nSoftmax\nDistribution\nover\nlabels \nFigure 7.2\nConvolutional \nneural network\n\n\n175\nConvolutional layers\n7.2.1\nPattern matching using filters\nConvolutional layers are the most important component in CNNs. As mentioned ear-\nlier, convolutional layers apply a mathematical operation called convolution to input\nvectors and produce output. But what is convolution? Understanding the strict defini-\ntion of convolution requires knowing linear algebra, so we’ll use some analogy and\nconcrete examples to understand it. Imagine holding a rectangular-shaped patch of\ncolored glass with complex patterns (like the stained glass you see in a church) and\nsliding it over the input sequence while looking through it. If the input pattern\nmatches that of the patch, more light goes through the glass and you get larger output\nvalues. If the input pattern does not look like that of the patch or looks the opposite,\nyou get smaller output values. In other words, you are looking for particular patterns\nin the input sequence using a patch of colored glass.\n This analogy is a little bit too vague, so let’s revisit the grammaticality-detection\nexample we used in chapter 4 and see how we’d apply a convolutional layer to the\ntask. To recap, our neural network receives a two-word sentence as an input and needs\nto distinguish grammatical sequences from ungrammatical ones. There are only four\nwords in the vocabulary—“I,” “you,” “am,” and “are,” which are represented by word\nembeddings. Similarly, there are only four possibilities for the input sentence—“I\nam,” “I are,” “you am,” and “you are.” You want the network to produce 1s for the first\nand the last cases and 0s for others. See figure 7.3 for an illustration.\nFigure 7.3\nRecognizing grammatical English sentences\nNow, let’s represent word embeddings as patterns. We’ll draw a black circle for value\n–1 and a white one for 1. Then you can represent each word vector as a pair of two circles\n(see the table on the left in figure 7.3). Similarly, you can represent each two-word sen-\ntence as a small “patch” of two vectors, or four circles (see the table on the right in figure\n7.3). Our task is beginning to look more like a pattern-recognition task, where the net-\nwork needs to learn black-and-white patterns that correspond to grammatical sentences.\n Then, let’s think of a “filter” of the same size (two circles × two circles) that acts as\nthe colored glass we talked about earlier. Each circle of this filter is also either black or\nwhite, corresponding to values –1 and 1. You are going to look at a pattern through\nWord\nI\nyou\nam\nEmbeddings\n[-1, 1]\n[1, -1]\n[-1, -1]\n[1, 1]\nare\nWord embeddings\nPatterns and desired output\nWord 1\nI\nI\nyou\nx1\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n1\n0\n0\n1\nyou\nWord 2\nDesired\nPattern\nPattern\nam\nare\nam\nare\n\n\n176\nCHAPTER 7\nConvolutional neural networks\nthis filter and determine whether the pattern is the one you are looking for. You do this\nby putting the filter over a pattern and counting the number of color matches between\nthe two. For each one of four positions, you get a score of +1 if the colors match (black-\nblack or white-white) and a score of –1 if they don’t (black-white or white-black). Your\nfinal score is the sum of four scores, which varies from –4 (no matches) to +4 (four\nmatches). See figure 7.4 for some examples.\n The score you get varies depending on the pattern and the filter, but as you can see\nin the figure, the score becomes larger when the filter looks similar to the pattern and\nbecomes smaller when the two are not similar. You get the largest score (4) when the\ntwo match exactly and the smallest score (–4) when the two are exactly opposite. The\nfilter acts as a pattern detector against the input. Although this is a very simplified\nexample, it basically shows what a convolutional layer is doing. In convolutional neu-\nral networks, such filters are called kernels.\n In a more general setting, you have an input sentence of arbitrary length, and you\nslide a kernel over the sentence from left to right. See figure 7.5 for an illustration of\nthis. The kernel is repeatedly applied to two consecutive words to produce a sequence\nof scores. Because the kernel we are using here covers two words, it is said to have a\nsize of 2. Also, because there are two dimensions in the input embeddings (which are\ncalled channels), the number of the kernel’s input channels is 2.\n \n \nNOTE\nThe reason embedding dimensions are called channels is because\nCNNs are most commonly applied to computer vision tasks where the input is\noften a 2-D image of different channels that correspond to intensities of dif-\nferent colors (such as red, green, and blue). In computer vision, kernels are\ntwo dimensional and move over the input 2-D images, which is also called 2-D\nconvolution. In NLP, however, kernels are usually one-dimensional (1-D convo-\nlution) and have only one size.\n7.2.2\nRectified linear unit (ReLU)\nAs the next step, let’s think about how we can get the desired output (the Desired col-\numn in figure 7.3) using kernels. How about if we use the filter shown in the second col-\numn of figure 7.4? The kernel, which we’ll call kernel 1 from now on, matches the first\npattern exactly and gives it a high score, while giving zero or negative scores to others.\nFigure 7.6 shows the score (called score 1) when kernel 1 is applied to each pattern.\n×\n=\n2\n×\n= –2\n×\n= –2\n×\n=\n2\n×\n=\n4\n×\n=\n0\n×\n×\n=\n0\n= –4\nPattern Filter Score\nPattern Filter Score\nFigure 7.4\nExamples of convolutional \nfilters\nInput\nKernel\nOutput\n0\n4\n0\n–4\nWidth\nChannels\nFigure 7.5\nSliding a kernel \nover the input sentence\n\n\n177\nConvolutional layers\n \nLet’s forget the magnitude of the scores for now and focus on their signs (positive and\nnegative). The signs for the first three patterns match between Score 1 and Desired,\nbut not for the last pattern. To score it correctly—that is, to give it a positive score—\nyou need to use another filter that matches the last pattern exactly. Let’s call this\nkernel 2. Figure 7.7 shows the score (called score 2) when kernel 2 is applied to each\npattern.\n Kernel 2 can give correct scores that match the signs of the desired ones for the\nlast three patterns, but not for the first one. But if you observe figures 7.6 and 7.7 care-\nfully, it looks like you could get closer to the desired scores if there was a way to some-\nhow disregard the output when a kernel gives negative scores and then combine the\nscores from multiple kernels. \n Let’s think of a function that clamps any negative input to zero while passing any\npositive values through unchanged. In Python, this function can be written as follows:\ndef f(x):\n    if x >= 0:\n        return x\n    else:\n        return 0\nor even simpler \ndef f(x):\n    return max(0, x)\nYou can disregard negative values by applying this function to score 1 and score 2, as\nshown in figures 7.8 and 7.9.\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n4\n0\n0\n-4\nScore 1\nPattern\nKernel 1\nam\nare\nam\nare\nFigure 7.6\nApplying kernel 1 to patterns\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n-4\n0\n0\n4\nScore 2\nPattern\nKernel 2\nam\nare\nam\nare\nFigure 7.7\nApplying kernel 2 to patterns\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n4\n0\n0\n-4\nScore 1\n4\n0\n0\n0\nf(Score 1)\nPattern\nKernel 1\nam\nare\nam\nare\nFigure 7.8\nApplying \nReLU to score 1\n\n\n178\nCHAPTER 7\nConvolutional neural networks\nThis function, which is called a rectified linear unit, or ReLU (pronounced “rel-you”), is\none of the simplest yet most commonly used activation functions in deep learning. It\nis often used with a convolutional layer, and although it is very simple (all it does is just\nclamp negative values to zero), it is still an activation function that enables neural net-\nworks to learn complex nonlinear functions (see chapter 4 for why nonlinear activa-\ntion functions are important). It also has favorable mathematical properties that make\nit easier to optimize the network, although the theoretical details are beyond the\nscope of this book.\n7.2.3\nCombining scores\nIf you look at both figures 7.8 and 7.9, the “clamped” scores—shown in the f(Score 1)\nand f(Score 2) columns—capture the desired scores at least partially. All you need to\ndo is combine them together (by summing) and adjust the range (by dividing by 4).\nFigure 7.10 shows the result of this.\nFigure 7.10\nCombining the results from two kernels\nAfter combining, the scores match the desired outcomes exactly. All we did so far was\ndesign kernels that match the patterns we want to detect and then simply combine the\nscores. Compare this to the RNN example we worked on in section 4.1.3, where we\nneeded to use some complicated numeric computation to derive the parameters.\nHopefully this example is enough to show you how simple and powerful CNNs can be\nfor text classification!\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n-4\n0\n0\n4\nScore 2\n0\n0\n0\n4\nf(Score 2)\nPattern\nKernel 2\nam\nare\nam\nare\nFigure 7.9\nApplying \nReLU to score 2\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n1\n0\n0\n1\nCombined\n4\n0\n0\n0\nf(Score 1)\n0\n(4 + 0) / 4 = 1\n0\n0\n4\nf(Score 2)\nKernel 2\nPattern\nKernel 1\nam\nare\nam\nare\n\n\n179\nPooling layers\n The example we worked on in this section is simply for introducing the basic con-\ncepts of CNNs, so we cut many corners. First, in practice, patterns and kernels are not\njust black and white but contain real-valued numbers. The score after applying a ker-\nnel to a pattern is obtained not by counting color matches but through a mathemati-\ncal operation called inner product, which captures the similarity between the two.\nSecond, the scores produced by kernels aren’t combined by some arbitrary operation\n(like we did in this section) but usually by a linear layer (see section 3.4.3), which can\nlearn a linear transformation against the input to produce the output. Finally, kernels\nand the weights (magic constants w and b) in the final linear layer are all trainable\nparameters of a CNN, meaning that their values are adjusted so that the CNN can pro-\nduce the desired scores.\n7.3\nPooling layers\nIn the previous section, we assumed that the input is just a\ncombination of two words—subjects and verbs—although\nin practice, the input to a CNN can be of arbitrary length.\nYour CNN needs to not only detect patterns but also find\nthem in a potentially large amount of noise in the input. As\nwe saw in section 7.2, you slide a kernel over the sentence\nfrom left to right, and the kernel is repeatedly applied to\ntwo consecutive words to produce a sequence of scores.\nThe remaining question is what to do with these produced\nscores. Specifically, what operation should we use in the “?”\nposition in figure 7.11 to derive the desired score? This\noperation needs to have some properties—it must be\nsomething that can be applied to an arbitrarily large num-\nber of scores, because the sentence can be very long. It also\nneeds to aggregate the scores in a way that is agnostic to\nwhere the target pattern (word embeddings for “I am”) is\nin the input sentence. Can you figure out the answer?\n The simplest thing you can do to aggregate the scores is to take their maximum.\nBecause the largest score in figure 7.11 is 4, it will become the output of this layer.\nThis aggregation operation is called pooling, and the neural network substructure that\ndoes pooling is called a pooling layer. You can also do other types of mathematical oper-\nations that do aggregation, such as taking the average, although taking the maximum\n(called max pooling) is most commonly used.\n The pooled score will be fed to a linear layer, optionally combined with scores\nfrom other kernels, and used as a predicted score. This entire process is illustrated in\nfigure 7.12. Now we have a fully functional CNN!\n As with other neural networks we’ve seen so far, the output from the linear layer is\nfed to softmax to produce a probability distribution over labels. These predicted val-\nues are then compared with the true labels to produce the loss and used for optimiz-\ning the network.\nInput\nKernel\nf(Score)\n0\n4\n0\n?\n1\n0\nv(\"i\")\nv(\"am\")\nDesired\nReLU\nFigure 7.11\nAggregating \nscores to derive the \ndesired score\n\n\n180\nCHAPTER 7\nConvolutional neural networks\n   Before we wrap up, a few more words on\nCNNs: notice that the CNN in figure 7.12\nproduces the same prediction value no mat-\nter where the search pattern (“I am”) is in\nthe input sentence. This is due to the kernel\nlocality as well as the property of the max\npooling layer we just added. In general,\nCNNs produce the same prediction, even if\nthe input sentence is modified by shifting by\na few words. In a technical term, the CNN is\ncalled transformation invariant, which is an\nimportant property of CNNs. This property\nis perhaps more intuitive if you use an\nimage recognition example. An image of a\ncat is still an image of a cat, no matter where\nthe cat is in the image. Similarly, a grammat-\nical English sentence (e.g., “I am a student”)\nis still grammatical, even if the sentence is\ntransformed by adding a few words (e.g.,\n“that’s right”) to the beginning, making it\n“That’s right, I am a student.”\n Because the kernels in a CNN do not depend on each other (unlike RNNs, where\none cell needs to wait until all the preceding cells finish processing the input), CNNs\nare computationally efficient. GPUs can process these kernels in parallel without wait-\ning on other kernels’ output. Due to this property, CNNs are usually faster than RNNs\nof similar size.\n7.4\nCase study: Text classification\nNow that we know the basics of CNNs, in this section we are going to build an NLP\napplication using a CNN and see how it works in practice. As mentioned previously,\none of the most popular and straightforward applications of CNNs in NLP is text clas-\nsification. CNNs are good at detecting patterns (such as salient words and phrases in\ntext), which is also the key to accurate text classification. \n7.4.1\nReview: Text classification\nWe already covered text classification in chapters 2 and 4, but to recap, text classifica-\ntion is a task where an NLP system assigns a label to a given piece of text. If the text is\nan email and the label is whether the email is spam, it’s spam filtering. If the text is a\ndocument (such as a news article) and the label is its topic (such as politics, business,\ntechnology, or sports), it’s called document classification. Many other variants of text\nclassification exist, depending on what the input and the output are. But the task we’ll\nbe working on in this section is again sentiment analysis, where the input is some text\nInput\nv(\"i\")\nv(\"am\")\nKernels\nf(Scores)\nmax(f(Scores))\n0\n0\n0\n4\n0\n4\n0\n0\n4\n4\nReLU\nMax\nLinear\nPooling\nActivation\nSoftmax\nDistribution\nover\nlabels\n \n \nFigure 7.12\nA full CNN with multiple kernels\n\n\n181\nCase study: Text classification\nin which the writer’s subjective opinions are expressed (such as movie and product\nreviews) and the output is the label for the opinion (such as positive or negative, or\neven the number of stars), also called polarity.\n In chapters 2 and 4, we built an NLP system that detected the polarity given a\nmovie review using the Stanford Sentiment Treebank, a dataset containing movie\nreviews and their polarity (strongly positive, positive, neutral, negative, or strongly\nnegative). In this section, we will build the same text classifier but with a CNN instead\nof an RNN. The good news is that we can reuse most of the code we wrote in chapter 2\nin this section—in fact, we need to modify only a few lines of code to swap the RNN\nwith a CNN. This is largely thanks to AllenNLP’s powerful, well-designed abstractions,\nwhich let you work with many modules with different architectures through the com-\nmon interfaces. Let’s see this in action next.\n7.4.2\nUsing CnnEncoder\nRemember that back in section 4.4, we defined our LstmClassifier for text classifi-\ncation as follows:\nclass LstmClassifier(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n    ...\nWe hadn’t put much thought into what this definition meant, but from this construc-\ntor we can see that the model is built on top of two subcomponents: a TextField-\nEmbedder called embedder and a Seq2VecEncoder called encoder, in addition to\nthe vocabulary and the string for the positive label, which are not relevant to our dis-\ncussion here. We discussed word embeddings in chapter 3 at length, although we only\nbriefly touched on the encoder. What does this Seq2VecEncoder actually mean?\n In AllenNLP, Seq2VecEncoder is a class of neural network architectures that take\na sequence of vectors (or tensors in general) and return a single vector. An RNN, one\nexample of this, takes a variable-length input consisting of multiple vectors and con-\nverts it into a single vector at the last cell. We created an instance of Seq2VecEncoder\nbased on an LSTM-RNN using the following code:\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nBut as long as your component has the same input and output specifications, you can\nuse any neural network architecture as a Seq2VecEncoder. In programming lan-\nguage, Seq2VecEncoder is analogous to an interface in Java (and in many other\nlanguages)—interfaces define what your class looks like and what it does, but they do\nnot care about how your class does it. In fact, your model can do something as simple\n",
      "page_number": 194
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 202-210)",
      "start_page": 202,
      "end_page": 210,
      "detection_method": "topic_boundary",
      "content": "182\nCHAPTER 7\nConvolutional neural networks\nas just summing up all the input vectors to produce the output, without any complex\ntransformations such as nonlinearities. This is, in fact, what BagOfEmbeddings-\nEncoder—one of the Seq2VecEncoders implemented in AllenNLP—does.\n Next, we use a CNN to “squash” a sequence of vectors into a single vector. A CNN-\nbased Seq2VecEncoder is implemented as CnnEncoder in AllenNLP, which can be\ninstantiated as follows:\nencoder = CnnEncoder(\n    embedding_dim=EMBEDDING_DIM,\n    num_filters=8,\n    ngram_filter_sizes=(2, 3, 4, 5))\nIn this example, embedding_dim specifies the dimensionality of the input embed-\ndings. The second argument, num_filters, tells how many filters (or kernels, as\nexplained in section 7.2.1) will be used per n-gram. The final argument, ngram_\nfilter_sizes, specifies the list of n-gram sizes, which are the sizes of these kernels.\nHere, we are using n-gram sizes of 2, 3, 4, and 5, meaning there are 8 kernels for\nbigrams, 8 kernels for trigrams, and so on, up to 5-grams. In total, this CNN can learn\n32 different kernels to detect patterns. CnnEncoder runs these results from the ker-\nnels through a max pooling layer and comes up with a single vector that summarizes\nthe input.\n The rest of the training pipeline looks almost identical to the LSTM version we\nsaw in chapter 2. The entire code is available on Google Colab (http://www.realworld\nnlpbook.com/ch7.html#cnn-nb). There is one caveat: because some n-gram filters\nhave a wide shape (e.g., 4- and 5-grams), you need to make sure that each text field is\nat least that long, even when the original text is short (e.g., just one or two words).\nYou need to know how batching and padding work in AllenNLP (which we’ll cover in\nchapter 10) to fully understand how to deal with this, but in a nutshell, you need to\nspecify the token_min_padding_length parameter when initializing the token\nindexer as follows:\ntoken_indexer = SingleIdTokenIndexer(token_min_padding_length=5)\nreader = StanfordSentimentTreeBankDatasetReader(\n    token_indexers={'tokens': token_indexer})\n7.4.3\nTraining and running the classifier\nWhen you run the script, you’ll see something like the following log output at the end\nof the training: \n{'best_epoch': 1,\n 'best_validation_accuracy': 0.40236148955495005,\n 'best_validation_f1_measure': 0.37362638115882874,\n 'best_validation_loss': 1.346440097263881,\n 'best_validation_precision': 0.4722222089767456,\n 'best_validation_recall': 0.30909091234207153,\n 'epoch': 10,\n 'peak_cpu_memory_MB': 601.656,\n 'training_accuracy': 0.993562734082397,\n\n\n183\nSummary\n 'training_cpu_memory_MB': 601.656,\n 'training_duration': '0:01:10.138277',\n 'training_epochs': 10,\n 'training_f1_measure': 0.994552493095398,\n 'training_loss': 0.03471498479299275,\n 'training_precision': 0.9968798756599426,\n 'training_recall': 0.9922360181808472,\n 'training_start_epoch': 0,\n 'validation_accuracy': 0.35149863760217986,\n 'validation_f1_measure': 0.376996785402298,\n 'validation_loss': 3.045241366113935,\n 'validation_precision': 0.3986486494541168,\n 'validation_recall': 0.35757574439048767}\nThis means that the training accuracy reaches ~99%, whereas the validation accuracy\ntops around 40%. Again, this is a typical symptom of overfitting, where your model is\nso powerful that it fits the training data well, but it doesn’t generalize to the validation\nand test datasets as well. Our CNN has many filters that can remember salient patterns\nin the training data, but these patterns are not necessarily the ones that help predict\nthe labels for the validation instances. We are not worried too much about overfitting\nin this chapter. See chapter 10 for common techniques for avoiding overfitting.\n If you want to make predictions for new instances, you can use the same Predic-\ntor as we did in chapter 2. Predictors in AllenNLP are a thin wrapper around your\ntrained model, which take care of formatting the input and output in a JSON format\nand feeding the instance to the model. You can use the following snippet to make pre-\ndictions using your trained CNN model:\npredictor = SentenceClassifierPredictor(model, dataset_reader=reader)\nlogits = predictor.predict('This is the best movie ever!')['logits']\nlabel_id = np.argmax(logits)\nprint(model.vocab.get_token_from_index(label_id, 'labels'))\nSummary\nCNNs use filters called kernels and an operation called convolution to detect\nlocal linguistic patterns in the input.\nAn activation function called ReLU, which clamps negative values to zero, is\nused with convolution layers.\nCNNs then use pooling layers to aggregate the result from the convolutional\nlayer.\nCNN prediction is transformation invariant, meaning it remains unchanged\neven after linear modification of the input.\nYou can use a CNN-based encoder as a Seq2VecEncoder in AllenNLP by mod-\nifying a few lines of code of your text classifier.\n\n\n184\nAttention and Transformer\nOur focus so far in this book has been recurrent neural networks (RNNs), which are\na powerful model that can be applied to various NLP tasks such as sentiment analysis,\nnamed entity recognition, and machine translation. In this chapter, we will intro-\nduce an even more powerful model—the Transformer1—a new type of encoder-\ndecoder neural network architecture based on the concept of self-attention. It is\nwithout a doubt the most important NLP model since it appeared in 2017. Not only\nThis chapter covers\nUsing attention to produce summaries of the \ninput and improve the quality of Seq2Seq models\nReplacing RNN-style loops with self-attention, a \nmechanism for the input to summarize itself \nImproving machine translation systems with the \nTransformer model\nBuilding a high-quality spell-checker using the \nTransformer model and publicly available datasets\n1 Vaswani et al., “Attention Is All You Need,” (2017). https://arxiv.org/abs/1706.03762.\n\n\n185\nWhat is attention?\nis it a powerful model itself (for machine translation and various Seq2Seq tasks, for\nexample), but it is also used as the underlying architecture that powers numerous mod-\nern NLP pretrained models, including GPT-2 (section 8.4.3) and BERT (section 9.2).\nThe developments in modern NLP since 2017 can be best summarized as “the era of the\nTransformer.”\n In this chapter, we start with attention, a mechanism that made a breakthrough in\nmachine translation, then move on to introducing self-attention, the concept that\nforms the foundation of the Transformer model. We will build two NLP applications—\na Spanish-to-English machine translator and a high-quality spell-checker—and learn\nhow to apply the Transformer model to your everyday applications. As we’ll see later,\nthe Transformer models can improve the quality of NLP systems over RNNs by a large\nmargin and achieve almost human-level performance in some tasks, such as translation\nand generation. \n8.1\nWhat is attention?\nIn chapter 6, we covered Seq2Seq models—NLP models that transform one sequence\nto another using an encoder and a decoder. Seq2Seq is a versatile and powerful para-\ndigm with many applications, although the “vanilla” Seq2Seq models are not without\nlimitation. In this section, we discuss the Seq2Seq models’ bottleneck and motivate\nthe use of an attention mechanism.\n8.1.1\nLimitation of vanilla Seq2Seq models\nLet’s remind ourselves how Seq2Seq models work. Seq2Seq models consist of an\nencoder and a decoder. The decoder takes a sequence of tokens in the source lan-\nguage and runs it through an RNN, which produces a fixed-length vector at the end.\nThis fixed-length vector is a representation of the input sentence. The decoder, which\nis another RNN, takes this vector and produces a sequence in the target language,\ntoken by token. Figure 8.1 illustrates how Spanish sentences are translated into\nEnglish with a vanilla Seq2Seq model.\nFigure 8.1\nA bottleneck in a vanilla Seq2Seq model\nMaria\nno\nbaba\nverde\n.\n...\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nEncoder\nDecoder\nSentence\nrepresentation\nBottleneck!\n\n\n186\nCHAPTER 8\nAttention and Transformer\nThis Seq2Seq architecture is quite simple and powerful, but it is known that its vanilla\nversion (shown in figure 8.1) does not translate sentences as well as other traditional\nmachine translation algorithms (such as phrase-based statistical machine translation\nmodels). You may be able to guess why this is the case if you look at its structure\ncarefully—its encoder is trying to “compress” all the information in the source sen-\ntence into the sentence representation, which is a vector of some fixed length (e.g.,\n256 floating-point numbers), and the decoder is trying to restore the entire target sen-\ntence just from that vector. The size of the vector is fixed no matter how long (or how\nshort) the source sentence is. The intermediate vector is a huge bottleneck. If you\nthink of how humans actually translate between languages, this sounds quite difficult\nand somewhat unusual. Professional translators do not just read the source sentence\nand write down its translation in one breath. They refer to the source sentences as\nmany times as necessary to translate the relevant parts in the target sentence.\n Compressing all the information into one vector may (and does) work for short\nsentences, as we’ll see later in section 8.2.2, but it becomes increasingly difficult as the\nsentences get longer and longer. Studies have shown that the translation quality of a\nvanilla Seq2Seq model gets worse as the sentence gets longer.2 \n8.1.2\nAttention mechanism\nInstead of relying on a single, fixed-length vector to represent all the information in a\nsentence, the decoder would have a much easier time if there was a mechanism where\nit can refer to some specific part of the encoder as it generates the target tokens. This\nis similar to how human translators (the decoder) reference the source sentence (the\nencoder) as needed. \n This can be achieved by using attention, which is a mechanism in neural networks\nthat focuses on a specific part of the input and computes its context-dependent sum-\nmary. It is like having some sort of key-value store that contains all of the input’s infor-\nmation and then looking it up with a query (the current context). The stored values\nare not just a single vector but usually a list of vectors, one for each token, associated\nwith corresponding keys. This effectively increases the size of the “memory” the\ndecoder can refer to when it’s making a prediction.\n Before we discuss how the attention mechanism works for Seq2Seq models, let’s\nsee it in action in a general form. Figure 8.2 illustrates a generic attention mechanism\nwith the following features: \n1\nThe inputs to an attention mechanism are the values and their associated keys.\nThe input values can take many different forms, but in NLP, they are almost\nalways lists of vectors. For Seq2Seq models, the keys and values here are the hid-\nden states of the encoder, which represent token-by-token encoding of the\ninput sentence. \n2 Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate,” (2014). https://\narxiv.org/abs/1409.0473.\n\n\n187\nSequence-to-sequence with attention\nFigure 8.2\nUsing an attention mechanism to summarize the input\n2\nEach key associated with a value is compared against the query using an atten-\ntion function f. By applying f to the query and each one of the keys, you get a\nset of scores, one per key-value pair, which are then normalized to obtain a set\nof attention weights. The specific function f depends on the architecture\n(more on this later). For Seq2Seq models, this gives you a distribution over the\ninput tokens. The more relevant an input token is, the larger the weight it gets.\n3\nThe input values are weighted by their corresponding weights obtained in step 2\nand summed up to compute the final summary vector. For Seq2Seq models, this\nsummary vector is appended to the decoder hidden states to aid the translation\nprocess.\nBecause of step 3, the output of an attention mechanism is always a weighted sum of\nthe input vectors, but how they are weighted is determined by the attention weights,\nwhich are in turn are calculated from the keys and the query. In other words, what an\nattention mechanism computes is a context (query)-dependent summary of the input. Down-\nstream components of a neural network (e.g., the decoder of an RNN-based Seq2Seq\nmodel, or the upper layers of a Transformer model) use this summary to further pro-\ncess the input.\n In the following sections, we will learn the two most commonly used types of atten-\ntion mechanisms in NLP—encoder-decoder attention (also called cross-attention; used\nin both RNN-based Seq2Seq models and the Transformer) and self-attention (used in\nthe Transformer).\n8.2\nSequence-to-sequence with attention\nIn this section, we’ll learn how the attention mechanism is applied to an RNN-based\nSeq2Seq model for which the attention mechanism was first invented. We’ll study how\nit works with specific examples, and then we’ll experiment with Seq2Seq models with\nAttention\nweights\nQuery\n50%\n10%\n10%\n30%\nKeys\nValues\nValues\n× weights\nSum\nf\nf\nf\nf\n\n\n188\nCHAPTER 8\nAttention and Transformer\nand without the attention mechanism using fairseq to observe how it affects the\ntranslation quality.\n8.2.1\nEncoder-decoder attention\nAs we saw earlier, attention is a mechanism for creating a summary of the input under\na specific context. We used a key-value store and a query as an analogy for how it\nworks. Let’s see how an attention mechanism is used with RNN-based Seq2Seq models\nusing the concrete examples that follow.  \n Figure 8.3 illustrates a Seq2Seq model with attention. It looks complex at first, but\nit is just an RNN-based Seq2Seq model with some extra “things” added on top of the\nencoder (the lightly shaded box in top left corner of the figure). If you ignore what’s\ninside and see it as a black box, all it does is simply take a query and return some sort\nof summary created from the input. The way it computes this summary is just a variant\nof the generic form of attention we covered in section 8.1.2. It proceeds as follows:\n1\nThe input to the attention mechanism is the list of hidden states computed by\nthe encoder. These hidden states are used as both keys and values (i.e., the keys\nand the values are identical). The encoder hidden state at a certain token (e.g.,\nat token “no”) reflects the information about that token and all the tokens lead-\ning up to it (if the RNN is unidirectional) or the entire sentence (if the RNN is\nbidirectional).\n2\nLet’s say you finished decoding up to “Mary did.” The hidden states of the\ndecoder at that point are used as the query, which is compared against every key\nusing function f. This produces a list of attention scores, one per each key-value\nMaria\nno\ndaba\nverde\n.\n...\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nContext\nvector\nKeys\nValues\nAttention\nweights\nAttention mechanism\nf\nf\nf\nf\nf\nSoftmax\nWeighted\nsum\nQuery\nFigure 8.3\nAdding an attention mechanism to an RNN-based Seq2Seq model (the lightly shaded box)\n\n\n189\nSequence-to-sequence with attention\npair. These scores determine which part of the input the decoder should attend\nto when it’s trying to generate a word that follows “Mary did.”\n3\nThese scores are converted to a probability distribution (a set of positive values\nthat sum to one), which is used to determine which vectors should get the most\nattention. The return value from this attention mechanism is the sum of all val-\nues, weighted by the attention scores after normalizing with softmax.\nYou may be wondering what the attention function f looks like. A couple of variants of\nf are possible, depending on how it computes the attention scores between the key\nand the query, but these details do not matter much here. One thing to note is that in\nthe original paper proposing the attention mechanism,3 the authors used a “mini”\nneural network to calculate attention scores from the key and the query.\n This “mini” network-based attention function is not something you just plug in to\nan RNN model post hoc and expect it to work. It is optimized as part of the entire\nnetwork—that is, as the entire network gets optimized by minimizing the loss func-\ntion, the attention mechanism also gets better at generating summaries because doing\nso well also helps the decoder generate better translation and lower the loss function.\nIn other words, the entire network, including the attention mechanism, is trained end\nto end. This usually means that, as the network is optimized, the attention mechanism\nstarts to learn to focus only on the relevant part of the input, which is usually where\nthe target tokens are aligned with the source tokens. In other words, attention is calcu-\nlating some sort of “soft” word alignment between the source and the target tokens.\n8.2.2\nBuilding a Seq2Seq machine translation with attention\nIn section 6.3, we built our first machine translation (MT) system using fairseq, an\nNMT toolkit developed by Facebook. Using the parallel dataset from Tatoeba, we built\nan LSTM-based Seq2Seq model to translate Spanish sentences into English.\n In this section, we are going to experiment with a Seq2Seq machine translation sys-\ntem and see how attention affects the translation quality. We assume that you’ve already\ngone through the steps we took when we built the MT system by downloading the data-\nset and running the fairseq-preprocess and fairseq-train commands (section\n6.3). After that, you ran the fairseq-interactive command to interactively trans-\nlate Spanish sentences into English. You might have noticed that the translation you get\nfrom this MT system that took you just 30 minutes to build was actually decent. In fact,\nthe model architecture we used (--arch lstm) has an attention mechanism built in by\ndefault. Notice when you ran the following fairseq-train command\n$ fairseq-train \\\n    data/mt-bin \\\n    --arch lstm \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam \\\n3 Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate,” (2014). https://\narxiv.org/abs/1409.0473.\n\n\n190\nCHAPTER 8\nAttention and Transformer\n    --lr 1.0e-3 \\\n    --max-tokens 4096 \\\n    --save-dir data/mt-ckpt\nyou should have seen the dump of what your model looks like in your terminal as\nfollows:\n...\nLSTMModel(\n  (encoder): LSTMEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (lstm): LSTM(512, 512)\n  )\n  (decoder): LSTMDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n    (layers): ModuleList(\n      (0): LSTMCell(1024, 512)\n    )\n    (attention): AttentionLayer(\n      (input_proj): Linear(in_features=512, out_features=512, bias=False)\n      (output_proj): Linear(in_features=1024, out_features=512, bias=False)\n    )\n  )\n)\n...\nThis tells you that your model has an encoder and a decoder, but the decoder also has\na component called attention (which is of type AttentionLayer), shown in bold in\nthe code snippet. This is exactly the “mini-network” that we covered in section 8.2.1. \n Now let’s train the same model, but without attention. You can add --decoder-\nattention 0 to fairseq-train to disable the attention mechanism, while keeping\neverything else the same, as shown here:\n$ fairseq-train \\\n      data/mt-bin \\\n      --arch lstm \\\n      --decoder-attention 0 \\\n      --share-decoder-input-output-embed \\\n      --optimizer adam \\\n      --lr 1.0e-3 \\\n      --max-tokens 4096 \\\n      --save-dir data/mt-ckpt-no-attn\nWhen you run this, you’ll see a similar dump, shown next, that shows the architecture\nof the model but without attention:\nLSTMModel(\n  (encoder): LSTMEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (lstm): LSTM(512, 512)\n  )\n  (decoder): LSTMDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n",
      "page_number": 202
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 211-219)",
      "start_page": 211,
      "end_page": 219,
      "detection_method": "topic_boundary",
      "content": "191\nSequence-to-sequence with attention\n    (layers): ModuleList(\n      (0): LSTMCell(1024, 512)\n    )\n  )\n)\nAs we saw in section 6.3.2, the training process alternates between training and valida-\ntion. In the training phase, the parameters of the neural network are optimized by the\noptimizer. In the validation phase, these parameters are fixed, and the model is run on\na held-out portion of the dataset called the validation set. In addition to making sure the\ntraining loss decreases, you should be looking at the validation loss during training,\nbecause it better represents how well the model generalizes outside the training data. \n During this experiment, you should observe that the lowest validation loss\nachieved by the attention model is around 1.727, whereas that for the attention-less\nmodel is around 2.243. Lower loss values mean the model is fitting the dataset better,\nso this indicates the attention is helping improve the translation. Let’s see if this is\nactually the case. As we’ve done in section 6.3.2, you can generate translations interac-\ntively by running the following fairseq-interactive command:\n$ fairseq-interactive \\\n    data/mt-bin \\\n    --path data/mt-ckpt/checkpoint_best.pt \\\n    --beam 5 \\\n    --source-lang es \\\n    --target-lang en\nIn table 8.1, we compare the translations generated by the model with and without\nattention. The translations you get from the attention-based model are the same as\nthe ones we saw in section 6.3.3. Notice that the translations you get from the\nattention-less model are a lot worse than those from the attention model. If you look\nat the translations for “¿Hay habitaciones libres?” and “Maria no daba una bofetada a\nla bruja verde,” you see unfamiliar tokens “<unk>” (for “unknown”) in them. What’s\nhappening here?\nTable 8.1\nTranslation generated by the model with and without attention\nSpanish (input)\nWith attention\nWithout attention\n¡Buenos días!\nGood morning!\nGood morning!\n¡Hola!\nHi!\nHi!\n¿Dónde está el baño?\nWhere’s the restroom?\nWhere’s the toilet?\n¿Hay habitaciones libres?\nIs there free rooms?\nAre there <unk> rooms?\n¿Acepta tarjeta de crédito?\nDo you accept credit card?\nDo you accept credit card?\nLa cuenta, por favor.\nThe bill, please.\nCheck, please.\nMaria no daba una bofetada a la \nbruja verde.\nMaria didn’t give the green witch.\nMary wasn’t a <unk> of the \npants.\n\n\n192\nCHAPTER 8\nAttention and Transformer\nThese are special tokens that are assigned to out-of-vocabulary (OOV) words. We\ntouched upon OOV words in section 3.6.1 (when we introduced the concept of sub-\nwords used for FastText). Most NLP applications operate within a fixed vocabulary,\nand whenever they encounter or try to produce words that are outside that pre-\ndefined set, the words are replaced with a special token, <unk>. This is akin to a spe-\ncial value (such as None in Python) returned when a method doesn’t know what to do\nwith the input. Because these sentences contain certain words (I suspect they are\n“libres” and “bofetada”), the Seq2Seq model without attention, whose memory is lim-\nited, didn’t know what to do with them and simply fell back on a safest thing to do,\nwhich is to produce a generic, catch-all symbol, <unk>. On the other hand, you can\nsee that attention prevents the system from producing these symbols and helps\nimprove the overall quality of the produced translations. \n8.3\nTransformer and self-attention\nIn this section, we are going to learn how the Transformer model works and, specifi-\ncally, how it generates high-quality translations by using a new mechanism called self-\nattention. Self-attention creates a summary of the entire input, but it does this for each\ntoken using the token as the context. \n8.3.1\nSelf-attention\nAs we’ve seen before, attention is a mechanism that creates a context-dependent sum-\nmary of the input. For RNN-based Seq2Seq models, the input is the encoder hidden\nstates, whereas the context is the decoder hidden states. The core idea of the Trans-\nformer, self-attention, also creates a summary of the input, except for one key\ndifference—the context in which the summary is created is also the input itself. See\nfigure 8.4 for a simplified illustration of a self-attention mechanism.\n Why is this a good thing? Why does it\neven work? As we discussed in chapter 4,\nRNNs can also create a summary of the\ninput by looping over the input tokens\nwhile updating an internal variable (hid-\nden states). This works—we previously saw\nthat RNNs can generate good translations\nwhen combined with attention, but they\nhave one critical issue: because RNNs pro-\ncess the input sequentially, it becomes pro-\ngressively more difficult to deal with long-range dependencies between tokens as the\nsentence gets longer. \n Let’s look at a concrete example. If the input sentence is “The Law will never be per-\nfect, but its application should be just,” understanding what the pronoun “its” refers to\n(“The Law”) is important for understanding what the sentence means and for any sub-\nsequent tasks (such as translating the sentence accurately). However, if you use an RNN\nMaria\nno\ndaba\nEmbeddings\n(input)\nSummaries\n(output)\nFigure 8.4\nSelf-attention transforms the \ninput into summaries.\n\n\n193\nTransformer and self-attention\nto encode this sentence, to learn this coreference relationship, the RNN needs to learn\nto remember the noun “The Law” in the hidden states first, then wait until the loop\nencounters the target pronoun (“its”) while learning to ignore everything unrelated in\nbetween. This sounds like a complicated trick for a neural network to learn. \n But things shouldn’t be that complicated. Singular possessive pronouns like “its”\nusually refer to their nearest singular nouns that appear before them, regardless of\nthe words in between, so simple rules like “replace it with the nearest noun that\nappeared before” will suffice. In other words, such “random access” is better suited in\nthis situation than “sequential access” is. Self-attention is better at learning such long-\nrange dependencies, as we’ll see later.\n Let’s walk through how self-attention works with an example. Let’s assume we are\ntranslating Spanish into English and would like to encode the first few words, “Maria\nno daba,” in the input sentence. Let’s also focus on one specific token, “no,” and how\nits embeddings are computed from the entire input. The first step is to compare the\ntarget token against all tokens in the input. Self-attention does this by converting the\ntarget into a query by using projection WQ as well as converting all the tokens into keys\nusing projection WK and computing attention weights using function f. The attention\nweights computed by f are normalized and converted to a probability distribution by\nthe softmax function. Figure 8.5 illustrates these steps where attention weights are\ncomputed. As with the encoder-decoder attention mechanism we covered in section\n8.2.1, these weights determine how to “mix” values we obtained from the input\ntokens. For words like “its,” we expect that the weight will be higher for related words\nsuch as “Law” in the example shown earlier. \nIn the next step, the vector corresponding to each input token is converted to a value\nvector by projection WV. Each projected value is weighted by the corresponding atten-\ntion weight and is summed up to produce a summary vector. See figure 8.6 for an\nillustration.\n This would be it if this were the “regular” encoder-decoder attention mechanism.\nYou need only one summary vector per each token during decoding. However, one key\ndifference between encoder-decoder attention and self-attention is the latter repeats\nMaria\nno\ndaba\nMaria\ndaba\nf\nf\nf\nEmbeddings\nKeys\nQuery\nAttention weights\nEmbeddings\nno(target)\nWK\nWK\nWQ\nWK\nFigure 8.5\nComputing attention \nweights from keys and queries\n\n\n194\nCHAPTER 8\nAttention and Transformer\n \nthis process for every single token in the input. As shown in figure 8.7, this produces a\nnew set of embeddings for the input, one for each token.\n Each summary produced by self-attention takes all the tokens in the input\nsequence into consideration, but with different weights. It is, therefore, straightfor-\nward for words like “its” to incorporate some information from related words, such as\n“The Law,” no matter how far apart these two words are. Using an analogy, self-atten-\ntion produces summaries through random access over the input. This is in contrast to\nRNNs, which allow only sequential access over the input, and is one of the key reasons\nwhy the Transformer is such a powerful model for encoding and decoding natural\nlanguage text.\n We need to cover one final piece of detail to fully understand self-attention. As it is,\nthe self-attention mechanism illustrated previously can use only one aspect of the input\nsequence to generate summaries. For example, if you want self-attention to learn which\nword each pronoun refers to, it can do that—but you may also want to “mix in” infor-\nmation from other words based on some other linguistic aspects. For example, you may\nwant to refer to some other words that the pronoun modifies (“applications,” in this\ncase). The solution is to have multiple sets of keys, values, and queries per token and\ncompute multiple sets of attention weights to “mix” values that focus on different\naspects of the input. The final embeddings are a combination of summaries generated\nthis way. This mechanism is called multihead self-attention (figure 8.8).\n You would need to learn some additional details if you were to fully understand how\na Transformer layer works, but this section has covered the most important concepts.\nIf you are interested in more details, check out The Illustrated Transformer (http://\njalammar.github.io/illustrated-transformer/), a well-written guide for understanding\nMaria\nno\ndaba\nMaria\nno\ndaba\nf\nEmbeddings\nKeys\nQuery\nAttention weights\nEmbeddings\nValues\nWeighted values\nNew embeddings\nWV\nWV\nWQ\nWV\nWK\nWK\nWK\nf\nf\nFigure 8.6\nCalculating the sum of all values \nweighted by attention weights\nMaria\nno\ndaba\nMaria\nno\ndaba\nEmbeddings\nKeys\nQueries\nEmbeddings\nNew embeddings\nFigure 8.7\nProducing summaries for the entire \ninput sequence (details are omitted)\n\n\n195\nTransformer and self-attention\nthe Transformer model with easy-to-understand illustrations. Also, if you are interested\nin implementing the Transformer model from scratch in Python, check out “The Anno-\ntated Transformer” (http://nlp.seas.harvard.edu/2018/04/03/attention.html).\n8.3.2\nTransformer\nThe Transformer model doesn’t just use a sin-\ngle step of self-attention to encode or decode\nnatural language text. It applies self-attention\nrepeatedly to the inputs to gradually trans-\nform them. As with multilayer RNNs, the\nTransformer also groups a series of transfor-\nmation operations into a layer and applies it\nrepeatedly. Figure 8.9 shows one layer of the\nTransformer encoder.\n A lot is going on within each layer, and it’s\nnot our goal to explain every bit of its detail—\nyou need to understand only that the multi-\nhead self-attention is at its core, followed by\ntransformation by a feed-forward neural net-\nwork (“FF” in figure 8.9). Residual connec-\ntions and normalization layers are introduced\nf\nf\nf\nEmbeddings\nKeys\nQueries\nAttention weights\nEmbeddings\nValues\nWeighted values\nNew embeddings\nMaria\nno\ndaba\nMaria\nno\ndaba\nf\nf\nf\nFigure 8.8\nMultihead self-attention produces summaries with multiple \nkeys, values, and queries.\nMaria\nno\ndaba\nMultihead self-attention\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nFigure 8.9\nA Transformer encoder \nlayer with self-attention and a feed-\nforward layer\n\n\n196\nCHAPTER 8\nAttention and Transformer\nto make it easier to train the model, although the details of these operations are out-\nside the scope of this book. The Transformer model applies this layer repeatedly to\ntransform the input from something literal (raw word embeddings) to something\nmore abstract (the “meaning” of the sentence). In the original Transformer paper,\nVaswani et al. used six layers for machine translation, although it is not uncommon for\nlarger models to use 10–20 layers these days.\n At this point, you may have noticed that the self-attention operation is completely\nindependent of positions. In other words, the embedded results of self-attention\nwould be completely identical even if, for example, we flipped the word order\nbetween “Maria” and “daba,” because the operation looks only at the word itself and\nthe aggregated embeddings from other words, regardless of where they are. This is\nobviously very limiting—what a natural language sentence means depends a lot on\nhow its words are ordered. How does the Transformer encode word order, then?\n    The Transformer model solves this prob-\nlem by generating some artificial embed-\ndings that differ from position to position\nand adding them to word embeddings\nbefore they are fed to the layers. These\nembeddings, called positional encoding and\nshown in figure 8.10, are either generated\nby some mathematical function (such as\nsine curves) or learned during training\nper position. This way, the Transformer\ncan distinguish between “Maria” at the\nfirst position and “Maria” at the third posi-\ntion, because they have different posi-\ntional encoding.\n  Figure 8.11 shows the Transformer\ndecoder. Although a lot is going on, make\nsure to notice two important things. First,\nyou’ll notice one extra mechanism called\ncross-attention inserted between the self-\nattention and feed-forward networks. This\ncross-attention mechanism is similar to the encoder-decoder attention mechanism we\ncovered in section 8.2. This works exactly the same as self-attention, except that the\nvalues for the attention come from the encoder, not the decoder, summarizing the\ninformation extracted from the encoder.\n Finally, the Transformer model generates the target sentence in exactly the same\nway as RNN-based Seq2Seq models we’ve previously learned in section 6.4. The\ndecoder is initialized by a special token <START> and produces a probability distribu-\ntion over possible next tokens. From here, you can proceed by choosing the token with\nthe maximum probability (greedy decoding, as shown in section 6.4.3) or keeping a few\ntokens with the highest probability while searching for the path that maximizes the\nMaria\nno\ndaba\nMultihead self-attention\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nFigure 8.10\nAdding positional encoding to the \ninput to represent word order\n\n\n197\nTransformer and self-attention\ntotal score (beam search, as shown in section 6.4.4). In fact, if you look at the Trans-\nformer decoder as a black box, the way it produces the target sequence is exactly the\nsame as RNNs, and you can use the same set of decoding algorithms. In other words, the\ndecoding algorithms covered in section 6.4 are generic ones that are agnostic of the\nunderlying decoder architecture.\n8.3.3\nExperiments\nNow that we know how the Transformer model works, let’s build a machine transla-\ntion system with it. The good news is the sequence-to-sequence toolkit, Fairseq,\nalready supports the Transformer-based models (along with other powerful models),\nwhich can be specified by the --arch transformer option when you train the\nmodel. Assuming that you have already preprocessed the dataset we used to build the\nSpanish-to-English machine translation, you need to tweak only the parameters you\ngive to fairseq-train, as shown next:\nfairseq-train \\\n  data/mt-bin \\\n<START>\nMary\ndid\nMary\ndid\nMasked multihead self-attention\nCross-attention\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nAdd & norm\nFrom\nencoder\nnot\nFigure 8.11\nA Transformer decoder \nlayer with self- and cross-attention\n\n\n198\nCHAPTER 8\nAttention and Transformer\n  --arch transformer \\\n  --share-decoder-input-output-embed \\\n  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n  --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n  --dropout 0.3 --weight-decay 0.0 \\\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n  --max-tokens 4096 \\\n  --save-dir data/mt-ckpt-transformer\nNote that this might not even run on your laptop. You really need GPUs to train the\nTransformer models. Also note that training can take hours even with GPUs. See sec-\ntion 11.5 for more information on using GPUs.\n A number of cryptic parameters appear here, but you don’t need to worry about\nthem. You can see the model structure when you run this command. The entire\nmodel dump is quite long, so we are omitting some intermediate layers in listing 8.1.\nIf you look carefully, you’ll see that the structure of the layers corresponds to the fig-\nures we showed earlier.\nTransformerModel(\n  (encoder): TransformerEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(     \n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)   \n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n      ...\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\nListing 8.1\nTransformer model dump from Fairseq\nSelf-attention of \nthe encoder\nFeed-forward\nnetwork of\nthe encoder\n\n\n199\nTransformer and self-attention\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(   \n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n        (encoder_attn): MultiheadAttention(    \n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)    \n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n      ...\n      (5): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n        (encoder_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n    )\n  )\n)\nWhen I ran this, the validation loss converges after around epoch 30, at which point\nyou can stop the training. The result I got by translating the same set of Spanish sen-\ntences into English follows:\n¡ Buenos días !\nS-0     ¡ Buenos días !\nH-0     -0.0753164291381836     Good morning !\nP-0     -0.0532 -0.0063 -0.1782 -0.0635\n¡ Hola !\nS-1     ¡ Hola !\nH-1     -0.17134985327720642    Hi !\nP-1     -0.2101 -0.2405 -0.0635\n¿ Dónde está el baño ?\nS-2     ¿ Dónde está el baño ?\nH-2     -0.2670585513114929     Where &apos;s the toilet ?\nP-2     -0.0163 -0.4116 -0.0853 -0.9763 -0.0530 -0.0598\nSelf-attention \nof the decoder\nEncoder-decoder \nof the decoder\nFeed-forward\nnetwork of\nthe decoder\n",
      "page_number": 211
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 220-227)",
      "start_page": 220,
      "end_page": 227,
      "detection_method": "topic_boundary",
      "content": "200\nCHAPTER 8\nAttention and Transformer\n¿ Hay habitaciones libres ?\nS-3     ¿ Hay habitaciones libres ?\nH-3     -0.26301929354667664    Are there any rooms available ?\nP-3     -0.1617 -0.0503 -0.2078 -1.2516 -0.0567 -0.0532 -0.0598\n¿ Acepta tarjeta de crédito ?\nS-4     ¿ Acepta tarjeta de crédito ?\nH-4     -0.06886537373065948    Do you accept credit card ?\nP-4     -0.0140 -0.0560 -0.0107 -0.0224 -0.2592 -0.0606 -0.0594\nLa cuenta , por favor .\nS-5     La cuenta , por favor .\nH-5     -0.08584468066692352    The bill , please .\nP-5     -0.2542 -0.0057 -0.1013 -0.0335 -0.0617 -0.0587\nMaria no daba una bofetada a la bruja verde .\nS-6     Maria no daba una bofetada a la bruja verde .\nH-6     -0.3688890039920807     Mary didn &apos;t slapped the green witch .\nP-6     -0.2005 -0.5588 -0.0487 -2.0105 -0.2672 -0.0139 -0.0099 -0.1503 -\n0.0602\nYou can see most of these English translations here are almost perfect. It is quite sur-\nprising that the model translated the most difficult sentence (“Maria no daba . . .”)\nalmost perfectly. This is probably enough to convince us that the Transformer is a\npowerful translation model. After its advent, this model became the de facto standard\nin research and commercial machine translation.\n8.4\nTransformer-based language models\nIn section 5.5, we introduced language models, which are statistical models that give a\nprobability to a piece of text. By decomposing text into a sequence of tokens, lan-\nguage models can estimate how “probable” the given text is. In section 5.6, we demon-\nstrated that by leveraging this property, language models can also be used to generate\nnew texts out of thin air!\n The Transformer is a powerful model that achieves impressive results in Seq2Seq\ntasks (such as machine translation), although its architecture can also be used for\nmodeling and generating language. In this section, we learn how to use the Trans-\nformer for modeling language and generating realistic texts.\n8.4.1\nTransformer as a language model\nIn section 5.6, we built a language-generation model based on a character LSTM-\nRNN. To recap, given a prefix (a partial sentence generated so far), the model uses an\nLSTM-based RNN (a neural network with a loop) to produce a probability distribu-\ntion over possible next tokens, as shown in figure 8.12. \n We noted earlier that, by regarding the Transformer decoder as a black box, you can\nuse the same set of decoding algorithms (greedy, beam search, and so on) as we intro-\nduced earlier for RNNs. This is also the case for language generation—by thinking of\nthe neural network as a black box that produces some sort of score given a prefix, you\ncan use the same logic to generate texts, no matter the underlying model. Figure 8.13\nshows how an architecture similar to the Transformer can be used for language gener-\n\n\n201\nTransformer-based language models\nation. Except for a few minor differences (such as lack of cross-attention), the structure\nis almost identical to the Transformer decoder.\n The following snippet shows Python-like pseudocode for generating text with the\nTransformer model. Here, model() is the main function where the model computa-\ntion happens—it takes the tokens, converts them to embeddings, adds positional\n<START>\nT\nT\nCharacter\nembeddings\n \nh\ng\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\nh\ne\n.\n<END>\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nT h e _ q u i c k _ b r … _ d o g .\nFigure 8.12\nGenerating text using an RNN\n<START>\nMary\ndid\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nMasked multihead self-attention\nMary\ndid\nnot\nFigure 8.13\nUsing the Transformer \nfor language generation\n\n\n202\nCHAPTER 8\nAttention and Transformer\nencoding, and passes them through all the Transformer layers, returning the final hid-\nden states back to the caller. The caller then passes them through a linear layer to con-\nvert them to logits, which in turn get converted to a probability distribution by\nsoftmax:\ndef generate():\n    token = <START>\n    tokens = [<START>]\n    while token != <END>:\n        hidden = model(tokens)\n        probs = softmax(linear(hidden))\n        token = sample(probs)\n        tokens.append(token)\n    return tokens\nIn fact, decoding for Seq2Seq models and language generation with language models\nare very similar tasks, where the output sequence is produced token by token, feeding\nitself back to the network, as shown in the previous code snippet. The only difference\nis that the former has some form of input (the source sentence) whereas the latter\ndoes not (the model feeds itself). These two tasks are also called unconditional and con-\nditional generation, respectively. Figure 8.14 illustrates these three components (net-\nwork, task, and decoding) and how they can be combined to solve a specific problem.\nFigure 8.14\nThree components of language generation and Seq2Seq tasks\nIn the rest of this section, we are going to experiment with some Transformer-based\nlanguage models and generate natural language texts using them. We’ll be using the\ntransformers library (https://huggingface.co/transformers/) developed by Hug-\nging Face, which has become a standard, go-to library for NLP researchers and engi-\nneers working with Transformer models in the past few years. It comes with a number\nof state-of-the-art model implementations including GPT-2 (this section) and BERT\nNetwork\nRNN (LSTM)\nTransformer\nTask\nLanguage model\n(unconditional generatoin)\nSeq2Seq\n(conditional generation)\nDecoding\nSampling\nGreedy decoding\nBeam search\n×\n×\nLanguage generation (section 5.6): \nRNN × Language model × Sampling\nSpanish translator (section 6.3): \nRNN × Seq2Seq × Beam search\nSpanish translator (section 8.3.3): \nTransformer × Seq2Seq × Beam search\nGPT-2 etc. (section 8.4): \nTransformer × Language model × Sampling\n\n\n203\nTransformer-based language models\n(next chapter), along with pretrained model parameters that you can load and use\nright away. It also provides a simple, consistent interface through which you can inter-\nact with powerful NLP models.\n8.4.2\nTransformer-XL\nIn many cases, you want to load and use pretrained models provided by third parties\n(most often the developer of the model), instead of training them from scratch.\nRecent Transformer models are fairly complex (usually with hundreds of millions of\nparameters) and are trained with huge datasets (tens of gigabytes of text). This would\nrequire GPU resources that only large institutions and tech giants can afford. It is not\ncompletely uncommon that some of these models take days to train, even with more\nthan a dozen GPUs! The good news is the implementation and pretrained model\nparameters for these huge Transformer models are usually made publicly available by\ntheir creators so that anyone can integrate them into their NLP applications.\n In this section, we’ll first check out Transformer-XL, a variant of the Transformer\ndeveloped by the researchers at Google Brain. Because there is no inherent “loop” in\nthe original Transformer model, unlike RNNs, the original Transformer is not good at\ndealing with super-long context. In training language models with the Transformer, you\nfirst split long texts into shorter chunks of, say, 512 words, and feed them to the model\nseparately. This means the model is unable to capture dependencies longer than 512\nwords. Transformer-XL4 addresses this issue by making a few improvements over the\nvanilla Transformer model (“XL” means extra-long). Although the details of these\nchanges are outside the scope of this book, in a nutshell, the model reuses its hidden\nstates from the previous segment, effectively creating a loop that passes information\nbetween different segments of texts. It also improves the positional encoding scheme\nwe touched on earlier to make it easier for the model to deal with longer texts.\n You can install the transformers library just by running pip install trans-\nformers from the command line. The main abstractions you’ll be interacting with are\ntokenizers and models. The tokenizers split a raw string into a sequence of tokens,\nwhereas the model defines the architecture and implements the main logic. The model\nand the pretrained weights usually depend on a specific tokenization scheme, so you\nneed to make sure you are using the tokenizer that is compatible with the model.\n The easiest way to initialize a tokenizer and a model with some specified pre-\ntrained weights is use the AutoTokenizer and AutoModelWithLMHead classes and\ncall their from_pretrained() methods as follows:\nimport torch\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\nmodel = AutoModelWithLMHead.from_pretrained('transfo-xl-wt103')\n4 Dai et al., “Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,” (2019). https://\narxiv.org/abs/1901.02860.\n\n\n204\nCHAPTER 8\nAttention and Transformer\nThe \nparameter \nto \nfrom_pre-\ntrained() is the name of the\nmodel/pretrained weights. This is a\nTransformer-XL model trained on a\ndataset called wt103 (WikiText103).\n    You may be wondering what this\n“LMHead” part in AutoModelWith-\nLMHead means. An LM (language\nmodel) head is a specific layer added\nto a neural network that converts its\nhidden states to a set of scores that\ndetermine which tokens to generate\nnext. These scores (also called logits)\nare then fed to a softmax layer to\nobtain a probability distribution over\npossible next tokens (figure 8.15).\nWe would like a model with an LM\nhead because we are interested in\ngenerating text by using the Trans-\nformer as a language model. How-\never, depending on the task, you may\nalso want a Transformer model with-\nout an LM head and just want to use\nits hidden states. That’s what we’ll do\nin the next chapter.\n    The next step is to initialize the\nprefix for which you would like your\nlanguage model to write the rest of\nthe story. You can use tokenizer.encode() to convert a string into a list of token\nIDs, which are then converted to a tensor. We’ll also initialize a variable past for cach-\ning the internal states and making the inference faster, as shown next:\ngenerated = tokenizer.encode(\"On our way to the beach\")\ncontext = torch.tensor([generated])\npast = None\nNow you are ready to generate the rest of the text. Notice the next code is similar to\nthe pseudocode we showed earlier. The idea is simple: get the output from the model,\nsample a token using the output, and feed it back to the model. Rinse and repeat.\nfor i in range(100):\n    output = model(context, mems=past)\n    token = sample_token(output.prediction_scores)\n    generated.append(token.item())\n    context = token.view(1, -1)\n    past = output.mems\nLinear\nLinear\nLinear\nSoftmax\n<START>\nMary\ndid\nAdd & Norm\nAdd & Norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nMasked multihead self-attention\nLM head\nFigure 8.15\nUsing a language model head with the \nTransformer\n\n\n205\nTransformer-based language models\nYou need to do some housekeeping to make the shape of the tensors compatible with\nthe model, which we can ignore for now. The sample_token() method here takes\nthe output of the model, converts it to a probability distribution, and samples a single\ntoken from it. I’m not showing the entire code for the method, but you can check the\nGoogle Colab notebook (http://realworldnlpbook.com/ch8.html#xformer-nb) for\nmore details. Also, here we wrote the generation algorithm from scratch, but if you\nneed more full-fledged generation (such as beam search), check out the official\nexample script from the developers of the library: http://mng.bz/wQ6q.\n After finishing the generation, you can convert the token IDs back into a raw string\nby calling tokenizer.decode()as follows:\nprint(tokenizer.decode(generated))\nThe following “story” is what I got when I ran this:\nOn our way to the beach, she finds, she finds the men who are in the group to \nbe \" in the group \". This has led to the perception that the \" group \" \nin the group is \" a group of people in the group with whom we share a \ndeep friendship, and which is a common cause to the contrary. \" <eos> \n<eos> = = Background = = <eos> <eos> The origins of the concept of \" \ngroup \" were in early colonial years with the English Civil War. The \nterm was coined by English abolitionist John\nThis is not a bad start. I like the way the story is trying to be consistent by sticking with\nthe concept of “group.” However, because the model is trained on Wikipedia text\nonly, its generation is not realistic and looks a little bit too formal.\n8.4.3\nGPT-2\nGPT-2 (which stands for generative pretraining), developed by OpenAI, is probably\nthe most famous language model to date. You may have heard the story about a lan-\nguage model generating natural language texts that are so realistic that you cannot\ntell them from those written by humans. Technically, GPT-2 is just a huge Transformer\nmodel, just like the one we introduced earlier. The main difference is its size (the larg-\nest model has 48 layers!) and the fact that the model is trained on a huge amount of\nnatural language text collected from the web. The OpenAI team publicly released the\nimplementation and the pretrained weights, so we can easily try out the model.\n Initialize the tokenizer and the model for GPT-2 as you have done for Transformer-\nXL, as shown next:\ntokenizer = AutoTokenizer.from_pretrained('gpt2-large')\nmodel = AutoModelWithLMHead.from_pretrained('gpt2-large')\nThen generate text using the next code snippet:\ngenerated = tokenizer.encode(\"On our way to the beach\")\ncontext = torch.tensor([generated])\npast = None\n\n\n206\nCHAPTER 8\nAttention and Transformer\nfor i in range(100):\n    output = model(context, past_key_values=past)\n    token = sample_token(output.logits)\n    generated.append(token.item())\n    context = token.unsqueeze(0)\n    past = output.past_key_values\nprint(tokenizer.decode(generated))\nYou may have noticed how little this code snippet changed from the one for\nTransformer-XL. In many cases, you don’t need to make any modifications when\nswitching between different models. This is why the transformers library is so\npowerful—you can try out and integrate a variety of state-of-the-art Transformer-based\nmodels into your application with a simple, consistent interface. As we’ll see in the\nnext chapter, this library is also integrated into AllenNLP, which makes it easy to build\npowerful NLP applications with state-of-the-art models.\n When I tried this, the GPT-2 generated the following beautifully written passage:\nOn our way to the beach, there was a small island that we visited for the \nfirst time. The island was called 'A' and it is a place that was used by \nthe French military during the Napoleonic wars and it is located in the \nsouth-central area of the island.\nA is an island of only a few hundred meters wide and has no other features to \ndistinguish its nature. On the island there were numerous small beaches \non which we could walk. The beach of 'A' was located in the...\nNotice how naturally it reads. Also, the GPT-2 model is good at staying consistent—\nyou can see the name of the island, “A,” is consistently used throughout the passage.\nAs far as I checked, there is no real island named A in the world, meaning that this is\nsomething the model simply made up. It is a great feat that the model remembered\nthe name it just coined and successfully wrote a story around it!\n Here’s another passage that GPT-2 generated with a prompt: 'Real World\nNatural Language Processing' is the name of the book:\n'Real World Natural Language Processing' is the name of the book. It has all \nthe tools you need to write and program natural language processing \nprograms on your computer. It is an ideal introductory resource for \nanyone wanting to learn more about natural language processing. You can \nbuy it as a paperback (US$12), as a PDF (US$15) or as an e-book \n(US$9.99).\nThe author's blog has more information and reviews.\nThe free 'Real World Natural Language Processing' ebook has all the necessary \ntools to get started with natural language processing. It includes a \nnumber of exercises to help you get your feet wet with writing and \nprogramming your own natural language processing programs, and it \nincludes a few example programs. The book's author, Michael Karp has \nalso written an online course about Natural Language Processing.\n\n\n207\nTransformer-based language models\n'Real World Natural Language Processing: Practical Applications' is a free \ne-book that explains how to use natural language processing to solve \nproblems of everyday life (such as writing an email, creating and\nAs of February 2019, when GPT-2 was released, I had barely begun writing this book,\nso I doubt GPT-2 knew anything about it. For a language model that doesn’t have any\nprior knowledge about the book, this is an amazing job, although I have to note that it\ngot the price and the name of the author wrong.\n8.4.4\nXLM\nFinally, as an interesting example, we will experiment with multilingual language gen-\neration. XLM (cross-lingual language model), proposed by researchers at Facebook\nAI Research, is a Transformer-based cross-lingual language model that can generate\nand encode texts in multiple languages.5 By learning how to encode multilingual\ntexts, the model can be used for transfer learning between different languages. We’ll\ncover transfer learning in chapter 9.\n You can start by initializing the tokenizer and the model and initialize it with the\npretrained weights as follows:\ntokenizer = AutoTokenizer.from_pretrained('xlm-clm-enfr-1024')\nmodel = AutoModelWithLMHead.from_pretrained('xlm-clm-enfr-1024')\nHere, we are loading an XLM model (xlm), trained with a causal language modeling\n(CLM) objective (clm) in English and French (enfr). CLM is just a fancier way to\ndescribe what we’ve been doing in this chapter—predicting the next token based on a\nprefix. XLM is usually used for encoding multilingual texts for some downstream tasks\nsuch as text classification and machine translation, but we are simply using it as a lan-\nguage model to generate texts. See listing 8.2 for the code snippet for generating mul-\ntilingual text with XLM. You can again reuse most of the earlier code snippet,\nalthough you also need to specify what language you are working in (note the lang =\n0 line). Also, here we are generating text from scratch by supplying just the BOS\ntoken (whose index is zero).\ngenerated = [0] # start with just <s>\ncontext = torch.tensor([generated])\nlang = 0 # English\nfor i in range(100):\n    langs = torch.zeros_like(context).fill_(lang)\n    output = model(context, langs=langs)\n    token = sample_token(output)\n5 Lample and Conneau, “Cross-Lingual Language Model Pretraining,” (2019). https://arxiv.org/abs/1901\n.07291.\nListing 8.2\nGenerating multilingual text with XLM\n",
      "page_number": 220
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 228-235)",
      "start_page": 228,
      "end_page": 235,
      "detection_method": "topic_boundary",
      "content": "208\nCHAPTER 8\nAttention and Transformer\n    generated.append(token.item())\n    context = torch.tensor([generated])\nprint(tokenizer.decode(generated))\nWhen I ran this, I got the following:\n<s>and its ability to make decisions on the basis of its own. \" </s>The \ngovernment has taken no decisions on that matter, \" Mr Hockey said. </\ns>A lot of the information is very sensitive. </s>The new research and \ninformation on the Australian economy, which is what we're going to get \nfrom people, and the information that we are going to be looking at, \nwe're going to be able to provide and we 'll take it forward. </s>I'm \nnot trying to make sure we're not\nThen, let’s change lang to 1 (which means French) and run the same snippet again,\nwhich gives you the next bit of text:\n<s></s>En revanche, les prix des maisons individuelles n' ont guère augmenté \n( - 0,1 % ). </s>En mars dernier, le taux de la taxe foncière, en légère \naugmentation à la hausse par rapport à février 2008. </s>\" Je n' ai \njamais eu une augmentation \" précise \". </s>\" Je me suis toujours dit \nque ce n' était pas parce que c' était une blague. </s>En effet, j' \nétais un gars de la rue \" </s>Les jeunes sont des gens qui avaient beau-\ncoup d' humour... \"\nAlthough the quality of generation is not as great as GPT-2, which we experimented\nwith earlier, it is refreshing to see that a single model can produce texts both in\nEnglish and French. These days, it is increasingly common to build multilingual\nTransformer-based NLP models to solve NLP problems and tasks in multiple lan-\nguages at the same time. This also became possible thanks to the Transformer’s pow-\nerful capacity to model the complexity of language.\n8.5\nCase study: Spell-checker\nIn the final section of this chapter, we will build a practical NLP application—a spell-\nchecker—with the Transformer. In the modern world, spell-checkers are everywhere.\nChances are your web browser is equipped with a spell-checker that tells you when you\nmake a spelling mistake by underlining misspelled words. Many word processors and\neditors also run spell-checkers by default. Some applications (including Google Docs\nand Microsoft Word) even point out simple grammatical errors, too. Ever wondered\nhow they work? We’ll learn how to formulate this as an NLP problem, prepare the\ndataset, train, and improve the model next. \n8.5.1\nSpell correction as machine translation\nSpell-checkers receive a piece of text such as “tisimptant too spll chck ths dcment,”\ndetect spelling and grammatical errors, if any, and fix all errors: “It’s important to\nspell-check this document.” How can you solve this task with NLP technologies? How\ncan such systems be implemented? \n\n\n209\nCase study: Spell-checker\n The simplest thing you could do is tokenize the input text into words and check if\neach word is in a dictionary. If it’s not, you look for the closest valid word in the dictio-\nnary according to some measure such as the edit distance and replace with that word.\nYou repeat this until there are no words to fix. This word-by-word fixing algorithm is\nwidely used by many spell-checkers due to its simplicity.\n However, this type of spell-checker has several issues. First, just like the first word in\nthe example, “tisimptant,” how do you know which part of the sentence is actually a\nword? The default spell-checker for my copy of Microsoft Word indicates it’s a mis-\nspelling of “disputant,” although it would be obvious to any English speakers that it is\nactually a misspelling of two (or more) words. The fact that users can also misspell\npunctuation (including whitespace) makes everything complicated. Second, just\nbecause some word is in a dictionary doesn’t mean it’s not an error. For example, the\nsecond word in the example, “too” is a misspelling of “to,” but both are valid words\nthat are in any English dictionary. How can you tell if the former is wrong in this con-\ntext? Third, all these decisions are made out of context. One of the spell-checkers I\ntried shows “thus” as a candidate to replace “ths” in this example. However, from this\ncontext (before a noun), it is obvious that “this” is a more appropriate candidate,\nalthough both “this” and “thus” are one edit distance away from “ths,” meaning they\nare equally valid options according to the edit distance. \n You would be able to solve some of these issues by adding some heuristic rules. For\nexample, “too” is more likely a misspelling of “to” before a verb, and “this” is more\nlikely before a noun than “thus.” But this method is obviously not scalable. Remember\nthe poor junior developer from section 1.1.2? Language is vast and full of exceptions.\nYou cannot just keep writing such rules to deal with the full complexity of language.\nEven if you are able to write rules for such simple words, how would you tell that\n“tisimptant” is actually two words? Would you try to split this word at every possible\nposition to see if split words resemble existing words? What if the input was in a lan-\nguage that is written without whitespace, like Chinese and Japanese?\n At this point, you may realize this “split and fix” approach is going nowhere. In\ngeneral, when designing an NLP application, you should think in terms of the follow-\ning three aspects:\nTask—What is the task being solved? Is it a classification, sequential-labeling, or\nsequence-to-sequence problem?\nModel—What model are you going to use? Is it a feed-forward network, an RNN,\nor the Transformer?\nDataset—Where are you obtaining the dataset to train and validate your model? \nBased on my experience, a vast majority of NLP applications nowadays can be solved by\ncombining these aspects. How about spell-checkers? Because they take a piece of text\nas the input and produce the fixed string, it’d be most straightforward if we solve this\nas a Seq2Seq task using the Transformer model. In other words, we will be building a\nmachine translation system that translates noisy inputs with spelling/grammatical\n\n\n210\nCHAPTER 8\nAttention and Transformer\nerrors into clean, errorless outputs as shown in figure 8.16. You can regard these two\nsides as two different “languages” (or “dialects” of English).\n At this point, you may be wondering where we are obtaining the dataset. This is\noften the most important (and the most difficult) part in solving real-world NLP prob-\nlems. Fortunately, we can use a public dataset for this task. Let’s dive in and start build-\ning a spell-checker.\n8.5.2\nTraining a spell-checker\nWe will be using GitHub Typo Corpus (https://github.com/mhagiwara/github-typo\n-corpus) as the dataset to train a spell-checker. The dataset, created by my collaborator\nand me, consists of hundreds of thousands of “typo” edits automatically harvested\nfrom GitHub. It is the largest dataset of spelling mistakes and their corrections to\ndate, which makes it a perfect choice for training a spell-checker.\n One decision we need to make before preparing the dataset and training a model\nis what to use as the atomic linguistic unit on which the model operates. Many NLP mod-\nels use tokens as the smallest unit (i.e., RNN/Transformer is fed a sequence of tokens),\nbut a growing number of NLP models use word or sentence pieces as the basic units (section\n10.4). What should we use as the smallest unit for spelling correction? As with many\nother NLP models, using words as the input sounds like a good “default” thing to do at\nfirst. However, as we saw earlier, the concept of tokens is not well suited for spelling\ncorrection—users can mess up with punctuation, which makes everything overly com-\nplex if you are dealing with tokens. More importantly, because NLP models need to\noperate on a fixed vocabulary, the spell-corrector vocabulary would need to include\nevery single misspelling of every single word it encountered during the training. This\nwould make it unnecessarily expensive to train and maintain such an NLP model.\n For these reasons, we will be using characters as the basic unit for our spell-checker,\nas we did in section 5.6. Using characters has several advantages—it can keep the size\nof the vocabulary quite small (usually less than one hundred for a language with a small\nfairseq\nNoisy text\nClean text\nSpell-checker\ntisimptant too spll chck\nths dcment\n \nIt's important to spell check\nthis document\n \nTarget source\nFigure 8.16\nTraining a spell-checker as an MT system that translates “noisy” sentences \ninto “clean” ones\n\n\n211\nCase study: Spell-checker\nset of alphabets such as English). You don’t need to worry about bloating your vocabu-\nlary, even with a noisy dataset full of typos, because typos are just different arrangements\nof characters. You can also treat punctuation marks (even whitespace) as one of the\ncharacters in the vocabulary. This makes the preprocessing step extremely easy because\nyou don’t need any linguistic toolkits (such as tokenizers) for doing this.\nNOTE\nUsing characters is not without disadvantages. One main issue is using\nthem will increase the length of sequences, because you need to break every-\nthing up into characters. This makes the model large and slower to train.\nFirst, let’s prepare the dataset for training a spell-checker. All the necessary data and\ncode for building a spell-checker are included in this repository: https://github.com/\nmhagiwara/xfspell. The tokenized and split datasets are located under data/gtc (as\ntrain.tok.fr, train.tok.en, dev.tok.fr, dev.tok.en). The suffixes en and fr\nare a commonly used convention in machine translation—“fr” means “foreign lan-\nguage” and “en” means English, because many MT research projects were originally\nmotivated by people wanting to translate some foreign language into English. Here,\nwe are using “fr” and “en” to mean just “noisy text before spelling correction” and\n“clean text after spelling correction.”\n Figure 8.17 shows an excerpt from the dataset for spelling correction created from\nGitHub Typo Corpus. Notice that text is segmented into individual characters, even\nwhitespaces (replaced by “_”). Any characters outside common alphabets (upper- and\nlowercase letters, numbers, and some common punctuation marks) are replaced with\n“#.” You can see that the dataset contains diverse corrections, including simple typos\n(pubilc -> public on line 670, HYML -> HTML on line 672), trickier errors (mxnet\nas not -> mxnet is not on line 681, 22th -> 22nd on line 682), and even lines without\nany corrections (line 676). This looks like a good resource to use for training a\nspell-checker.\nFigure 8.17\nTraining data for spelling correction\n\n\n212\nCHAPTER 8\nAttention and Transformer\nThe first step for training a spell-checker (or any other Seq2Seq model) is to prepro-\ncess the datasets. Because the dataset is already split and formatted, all you need to do\nis run fairseq-preprocess to convert the datasets into a binary format as follows:\nfairseq-preprocess --source-lang fr --target-lang en \\\n    --trainpref data/gtc/train.tok \\\n    --validpref data/gtc/dev.tok \\\n    --destdir bin/gtc\nThen you can start training your model right away using the following code.\nfairseq-train \\\n    bin/gtc \\\n    --fp16 \\\n    --arch transformer \\\n    --encoder-layers 6 --decoder-layers 6 \\\n    --encoder-embed-dim 1024 --decoder-embed-dim 1024 \\\n    --encoder-ffn-embed-dim 4096 --decoder-ffn-embed-dim 4096 \\\n    --encoder-attention-heads 16 --decoder-attention-heads 16 \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-09 --clip-norm \n25.0 \\\n    --lr 1e-4 --lr-scheduler inverse_sqrt --warmup-updates 16000 \\\n    --dropout 0.1 --attention-dropout 0.1 --activation-dropout 0.1 \\\n    --weight-decay 0.00025 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\n    --max-tokens 4096 \\\n    --save-dir models/gtc01 \\\n    --max-epoch 40\nYou don’t need to worry about most of the hyperparameters here—this set of parame-\nters worked fairly well for me, although some other combinations of parameters may\nwork better. However, you may want to pay attention to some of the parameters\nrelated to the size of the model, namely:\nNumber of layers (--[encoder|decoder]-layers)\nEmbedding dimension of self-attention (--[encoder|decoder]-embed-dim)\nEmbedding dimension of feed-forward layers (--[encoder/decoder]-ffn-\nembed-dim)\nNumber of attention heads (--[encoder|decoder]-attention-heads)\nThese parameters determine the capacity of the model. In general, the larger these\nparameters are, the larger capacity the model would have, although as a result the\nmodel would also require more data, time, and GPU resources to train. Another\nimportant parameter is --max-tokens, which specifies the number of tokens loaded\nonto a single batch. If you are experiencing out-of-memory errors on a GPU, try\nadjusting this parameter.\nListing 8.3\nTraining a spell-checker\n\n\n213\nCase study: Spell-checker\n After the training is finished, you can run the following command to make predic-\ntions using the trained model:\necho \"tisimptant too spll chck ths dcment.\" \\\n    | python src/tokenize.py \\\n    | fairseq-interactive bin/gtc \\\n    --path models/gtc01/checkpoint_best.pt \\\n    --source-lang fr --target-lang en --beam 10 \\\n    | python src/format_fairseq_output.py\nBecause the fairseq-interactive interface can also take source text from the\nstandard input, we are directly providing the text using the echo command. The\nPython script src/format_fairseq_output.py, as its name suggests, formats the\noutput from fairseq-interactive and shows the predicted target text. When I ran\nthis, I got the following:\ntisimplement too spll chck ths dcment.\nThis is rather disappointing. The spell-checker learned to somehow fix “imptant” to\n“implement,” although it failed to correct any other words. I suspect a couple of rea-\nsons for this. The training data used, GitHub Typo Corpus, is heavily biased toward\nsoftware-related language and corrections, which might have led to the wrong correc-\ntion (imptant -> implement). Also, the training data might have just been too small\nfor the Transformer to be effective. How could we improve the model so that it can fix\nspellings more accurately? \n8.5.3\nImproving a spell-checker\nAs we discussed earlier, one main reason the spell-checker is not working as expected\nmight be because the model wasn’t exposed to a more diverse, larger amount of mis-\nspellings during training. But as far as I know, no such large datasets of diverse mis-\nspellings are publicly available for training a general-domain spell-checker. How could\nwe obtain more data for training a better spell-checker?\n This is where we need to be creative. One idea is to artificially generate noisy text\nfrom clean text. If you think of it, it is very difficult (especially for a machine learning\nmodel) to fix misspellings, whereas it is very easy to “corrupt” clean text to simulate\nhow people make typos, even for a computer. For example, we can take some clean\ntext (which is available from, for example, scraped web text almost indefinitely) and\nreplace some letters at random. If you pair artificially generated noisy text created this\nway with the original, clean text, this will effectively create a new, larger dataset on\nwhich you can train an even better spell-checker!\n The remaining issue we need to address is how to “corrupt” clean text to generate\nrealistic spelling errors that look like the ones made by humans. You can write a\nPython script that, for example, replaces, deletes, and/or swaps letters at random,\nalthough there is no guarantee that typos made this way are similar to those made by\nhumans and that the resulting artificial dataset will provide useful insights for the\n\n\n214\nCHAPTER 8\nAttention and Transformer\nTransformer model. How can we model the fact that, for example, humans are more\nlikely to type “too” in place of “to” than they do “two”? \n This is starting to sound familiar again. We can use the data to simulate the typos!\nBut how? This is where we need to be creative again—if you “flip” the direction of the\noriginal dataset we used to train the spell-checker, you can observe how humans make\ntypos. If you treat the clean text as the source language and the noisy text as the target\nand train a Seq2Seq model for that direction, you are effectively training a “spell-\ncorruptor”—a Seq2Seq model that inserts realistic-looking spelling errors into clean\ntext. See Figure 8.18 for an illustration.\nFigure 8.18\nUsing back-translation to generate artificial noisy data\nThis technique of using the “inverse” of the original training data to artificially gener-\nate a large amount of data in the source language from a real corpus in the target lan-\nguage is called back-translation in the machine learning literature. It is a popular\ntechnique to improve the quality of machine translation systems. As we’ll show next, it\nis also effective for improving the quality of spell-checkers.\n  You can easily train a spell corruptor just by swapping the source and the target lan-\nguages. You can do this by supplying “en” (clean text) as the source language and “fr”\n(noisy text) as the target language when you run fairseq-preprocess as follows:\nfairseq-preprocess --source-lang en --target-lang fr \\\n    --trainpref data/gtc/train.tok \\\n    --validpref data/gtc/dev.tok \\\n    --destdir bin/gtc-en2fr\nFairseq\nFairseq\nClean text\nArtificial\nnoisy text\nSpell corruptor\nSpell-checker\nClean text\nNoisy text\nTarget source\nSource\nTarget\ntisimptant too spll chck\nths dcment\nIt's important to spell check\nthis document\n\n\n215\nCase study: Spell-checker\nWe are not going over the training process again—you can use almost the same\nfairseq-train command to start the training. Just don’t forget to specify a different\ndirectory for --save-dir. After you finish training, you can check whether the spell-\ning corrupter can indeed corrupt the input text as expected:\n$ echo 'The quick brown fox jumps over the lazy dog.' | python src/\ntokenize.py \\ \n    | fairseq-interactive \\\n    bin/gtc-en2fr \\\n    --path models/gtc-en2fr/checkpoint_best.pt \\\n    --source-lang en --target-lang fr \\\n    --beam 1 --sampling --sampling-topk 10 \\\n    | python src/format_fairseq_output.py\nThe quink brown fox jumps ove-rthe lazy dog.\nNote the extra options that I added earlier, which are shown in bold. It means that the\nfairseq-interactive command uses sampling (from top 10 tokens with largest\nprobabilities) instead of beam search. When corrupting clean text, it is often better to\nuse sampling instead of beam search. To recap, sampling picks the next token ran-\ndomly according to the probability distribution after the softmax layer, whereas beam\nsearch tries to find the “best path” that maximizes the score of the output sequence.\nAlthough beam search can find better solutions when translating some text, we want\nnoisy, more diverse output when corrupting clean text. Past research6 has also shown\nthat sampling (instead of beam search) works better for augmenting data via back-\ntranslation.\n From here, the sky’s the limit. You can collect as much clean text as you want, gen-\nerate noisy text from it using the corruptor you just trained, and increase the size of\nthe training data. There is no guarantee that the artificial errors look like the real\nones made by humans, but this is not a big deal because 1) the source (noisy) side is\nused only for encoding, and 2) the target (clean) side data is always “real” data written\nby humans, from which the Transformer can learn how to generate real text. The\nmore text data you collect, the more confident the model will get about what error-\nfree, real text looks like.\n I won’t go over every step I took to increase the size of the data, but here’s the sum-\nmary of what I did and what you can also do. Collect as much clean and diverse text\ndata from publicly available datasets, such as Tatoeba and Wikipedia dumps. My favor-\nite way to do this is to use OpenWebTextCorpus (https://skylion007.github.io/Open-\nWebTextCorpus/), an open source project to replicate the dataset on which GPT-2 was\noriginally trained. It consists of a huge amount (40 GB) of high-quality web text\ncrawled from all outbound links from Reddit. Because the entire dataset would take\ndays, if not weeks, just to preprocess and run the corruptor on, you can take a subset\n(say, 1/1000th) and add it to the dataset. I took 1/100th of the dataset, preprocessed it,\nand ran the corruptor to obtain the noisy-clean parallel dataset. This 1/100th subset\n6 Edunov et al.,”Understanding Back-Translation at Scale,” (2018). https://arxiv.org/abs/1808.09381.\n",
      "page_number": 228
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 236-243)",
      "start_page": 236,
      "end_page": 243,
      "detection_method": "topic_boundary",
      "content": "216\nCHAPTER 8\nAttention and Transformer\nalone added more than five million pairs (in comparison, the original training set con-\ntains only ~240k pairs). Instead of training from scratch, you can download the pre-\ntrained weights and try the spell-checker from the repository.\n The training took several days, even on multiple GPUs, but when it was done, the\nresult was very encouraging. Not only can it accurately fix spelling errors, as shown here\n$ echo \"tisimptant too spll chck ths dcment.\" \\\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n    bin/gtc-bt512-owt1k-upper \\\n    --path models/bt05/checkpoint_best.pt \\\n    --source-lang fr --target-lang en --beam 10 \\\n    | python src/format_fairseq_output.py\n    It's important to spell check this document.\nbut the spell-checker also appears to understand the grammar of English to some\ndegree, as shown here:\n$ echo \"The book wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe book was about NLP.\n$ echo \"The books wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe books were about NLP.\nThis example alone may not prove that the model really understands the grammar\n(namely, using the correct verb depending on the number of the subject). It might\njust be learning some association between consecutive words, which can be achieved\nby any statistical NLP model, such as n-gram language models. However, even after\nyou make the sentences more complicated, the spell-checker shows amazing resil-\nience, as shown in the next code snippet:\n$ echo \"The book Tom and Jerry put on the yellow desk yesterday wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe book Tom and Jerry put on the yellow desk yesterday was about NLP.\n$ echo \"The books Tom and Jerry put on the yellow desk yesterday wer about \nNLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe books Tom and Jerry put on the yellow desk yesterday were about NLP.\nFrom these examples, it is clear that the model learned how to ignore irrelevant noun\nphrases (such as “Tom and Jerry” and “yellow desk”) and focus on the noun\n\n\n217\nSummary\n(“book(s)”) that determines the form of the verb (“was” versus “were”). We are more\nconfident that it understands the basic sentence structure. All we did was collect a\nlarge amount of clean text and trained the Transformer model on it, combined with\nthe original training data and the corruptor. Hopefully through these experiments,\nyou were able to feel how powerful the Transformer model can be!\nSummary\nAttention is a mechanism in neural networks that focuses on a specific part of\nthe input and computes its context-dependent summary. It works like a “soft”\nversion of a key-value store.\nEncoder-decoder attention can be added to Seq2Seq models to improve their\ntranslation quality.\nSelf-attention is an attention mechanism that produces the summary of the\ninput by summarizing itself.\nThe Transformer model applies self-attention repeatedly to gradually transform\nthe input.\nHigh-quality spell-checkers can be built using the Transformer and a technique\ncalled back-translation.\n\n\n218\nTransfer learning with\npretrained language models\nThe year 2018 is often called “an inflection point” in the history of NLP. A promi-\nnent NLP researcher, Sebastian Ruder (https://ruder.io/nlp-imagenet/), dubbed\nthis change “NLP’s ImageNet moment,” where he used the name of a popular com-\nputer vision dataset and powerful models pretrained on it, pointing out that similar\nchanges were underway in the NLP community as well. Powerful pretrained lan-\nguage models such as ELMo, BERT, and GPT-2 achieved state-of-the-art perfor-\nmance in many NLP tasks and completely changed how we build NLP models\nwithin months. \nThis chapter covers\nUsing transfer learning to leverage knowledge \nfrom unlabeled textual data\nUsing self-supervised learning to pretrain large \nlanguage models such as BERT\nBuilding a sentiment analyzer with BERT and \nthe Hugging Face Transformers library\nBuilding a natural language inference model \nwith BERT and AllenNLP\n\n\n219\nTransfer learning\n One important concept underlying these powerful pretrained language models is\ntransfer learning, a technique for improving the performance of one task using a model\ntrained on another task. In this chapter, we’ll first introduce the concept, then move\non to introducing BERT, the most popular pretrained language model proposed for\nNLP. We’ll cover how BERT is designed and pretrained, as well as how to use the\nmodel for downstream NLP tasks including sentiment analysis and natural language\ninference. We’ll also touch on other popular pretrained models including ELMo and\nRoBERTa.\n9.1\nTransfer learning\nWe start this chapter by introducing transfer learning, a powerful machine learning\nconcept fundamental to many pretrained language models (PLMs) in this chapter. \n9.1.1\nTraditional machine learning\nIn traditional machine learning, before the advent of pretrained language models,\nNLP models were trained on a per-task basis, and they were useful only for the type of\nthe task they were trained for (figure 9.1). For example, if you wanted a sentiment\nanalysis model, you needed to use a dataset annotated with the desired output (e.g.,\nnegative, neutral, and positive labels), and the trained model was useful only for senti-\nment analysis. If you needed to build another model for part-of-speech (POS) tagging\n(an NLP task to identify the part of speech of words; see section 5.2 for a review), you\nneeded to do this all over again by collecting training data and training a POS tagging\nmodel from scratch. You could not “reuse” your sentiment analysis model for POS tag-\nging, no matter how good your model was, because these two were trained for two fun-\ndamentally different tasks. However, these tasks both operated on the same language\nand all this seemed wasteful. For example, knowing that “wonderful,” “awesome,” and\n“great” are all adjectives that have positive meaning would help both sentiment analysis\nTraining\nModel 1\nTraining data 1\nTask 1\nTraining\nModel 2\nTraining data 2\nTask 2\nTraining\nModel 3\nTraining data 3\nTask 3\nFigure 9.1\nIn traditional \nmachine learning, each \ntrained model was used \nfor just one task.\n\n\n220\nCHAPTER 9\nTransfer learning with pretrained language models\nand part-of-speech tagging. Under the traditional machine learning paradigm, not\nonly did we need to prepare training data large enough to teach “common sense” like\nthis to the model, but individual NLP models also needed to learn such facts about the\nlanguage from scratch, solely from the given data.\n9.1.2\nWord embeddings\nAt this point, you may realize this sounds somewhat familiar. Recall our discussion in\nsection 3.1 on word embeddings and why they are important. To recap, word embed-\ndings are vector representations of words that are learned so that semantically similar\nwords share similar representations. As a result, vectors for “dog” and “cat,” for exam-\nple, end up being located in a close proximity in a high-dimensional space. These rep-\nresentations are trained on an independent, large textual corpus without any training\nsignals, using algorithms such as Skip-gram and CBOW, often collectively called\nWord2vec (section 3.4). \n After these word embeddings are trained, downstream NLP tasks can use them as\nthe input to their models (which are often neural networks, but not necessarily).\nBecause these embeddings already capture semantic relationship between words (e.g.,\ndogs and cats are both animals), these tasks no longer need to learn how the language\nworks from scratch, which gives them the upper hand in the task they are trying to\nsolve. The model can now focus on learning higher-level concepts that cannot be cap-\ntured by word embeddings (e.g., phrases, syntax, and semantics) and the task-specific\npatterns learned from the given annotated data. This is why using word embeddings\ngives a performance boost to many NLP models.\n In chapter 3, we likened this to teaching a baby (= an NLP model) how to dance.\nBy letting babies learn how to walk steadily first (= training word embeddings), dance\nteachers (= task-specific datasets and training objectives) can focus on teaching spe-\ncific dance moves without worrying whether babies can even stand and walk properly.\nThis “phased training” approach makes everything easier if you want to teach another\nskill to the baby (e.g., teaching martial arts) because they already have a good grasp of\nthe fundamental skill (walking).\n The beauty of all this is that word embeddings can be learned independently of\nthe downstream tasks. These word embeddings are pretrained, meaning their training\nhappens before the training of downstream NLP tasks. Using the dancing baby anal-\nogy, dance teachers can safely assume that all the incoming dance students have\nalready learned how to stand and walk properly. Pretrained word embeddings created\nby the developers of the algorithm are often freely available, and anyone can down-\nload and integrate them into their NLP applications. This process is illustrated in\nfigure 9.2.\n9.1.3\nWhat is transfer learning?\nIf you generalize what you did with word embeddings earlier, you took the outcome of\none task (i.e., predicting word cooccurrence with embeddings) and transferred the\nknowledge gleaned from it to another one (i.e., sentiment analysis, or any other NLP\n\n\n221\nTransfer learning\ntasks). In machine learning, this process is called transfer learning, which is a collection\nof related techniques to improve the performance of a machine learning model in a\ntask using data and/or models trained in a different task. Transfer learning always\nconsists of two or more steps—a machine learning model is first trained for one task\n(called pretraining), which is then adjusted and used in another (called adaptation). If\nthe same model is used for both tasks, the second step is called fine-tuning, because\nyou are tuning the same model slightly but for a different task. See figure 9.3 for an\nillustration of transfer learning in NLP.\nFigure 9.3\nLeveraging transfer learning helps build a better NLP model.\nTransfer learning has become the dominant way for building high-quality NLP models\nin the past few years for two main reasons. Firstly, thanks to powerful neural network\nmodels such as the Transformer and self-supervised learning (see section 9.2.2), it\nWord2vec\nTraining\nModel\nLarge \ntext corpus\nAnnotated data\nTask\nWord \nembeddings\nFigure 9.2\nLeveraging word embeddings helps build a better NLP model.\nPretraining\nAdaptation\nfine-tuning\n \nLarge\n \ntext corpus\nSmall\n \nannotated data\nTask\nPretrained\nlanguage model\nFine-tuned\nmodel\n\n\n222\nCHAPTER 9\nTransfer learning with pretrained language models\nbecame possible to bootstrap high-quality embeddings from an almost unlimited\namount of natural language text. These embeddings take into account the structure,\ncontext, and semantics of natural language text to a great extent. Secondly, thanks to\ntransfer learning, anyone can incorporate these powerful pretrained language models\ninto their NLP applications, even without access to a lot of textual resources, such as\nweb-scale corpora, or compute resources, such as powerful GPUs. The advent of these\nnew technologies (the Transformer, self-supervised learning, pretrained language\nmodels, and transfer learning) moved the field of NLP to a completely new stage and\npushed the performance of many NLP tasks to a near-human level. In the following\nsubsections, we’ll see transfer learning in action by actually building NLP models\nwhile leveraging PLMs such as BERT.\n Note that the concept called domain adaptation is closely related to transfer learn-\ning. Domain adaptation is a technique where you train a machine learning model in\none domain (e.g., news) and adapt it to another domain (e.g., social media), but\nthese are for the same task (e.g., text classification). On the other hand, the transfer\nlearning we cover in this chapter is applied to different tasks (e.g., language modeling\nversus text classification). You can achieve the same effect using the transfer learning\nparadigm covered in this chapter, and we do not specifically cover domain adaptation\nas a separate topic. Interested readers can learn more about domain adaptation from\na recent review paper.1\n9.2\nBERT\nIn this section, we will cover BERT in detail. BERT (Bidirectional Encoder Represen-\ntations from Transformers)2 is by far the most popular and most influential pretrained\nlanguage model to date that revolutionized how people train and build NLP models.\nWe will first introduce contextualized embeddings and why they are important, then move\non to explaining self-supervised learning, which is an important concept in pretrain-\ning language models. We’ll cover two self-supervised tasks used for pretraining BERT,\nnamely, masked language models and next-sentence prediction, and cover ways to\nadapt BERT for your applications. \n9.2.1\nLimitations of word embeddings\nWord embeddings are a powerful concept that can give your application a boost in the\nperformance, although they are not without limitation. One obvious issue is that they\ncannot take context into account. Words you see in natural language are often poly-\nsemous, meaning they may have more than one meaning, depending on their con-\ntext. However, because word embeddings are trained per token type, all the different\nmeanings are compressed into a single vector. For example, training a single vector\n1 Ramponi and Plank, “Neural Unsupervised Domain Adaptation in NLP—A Survey,” (2020). https://arxiv.org/\nabs/2006.00632.\n2 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-Training of Deep Bidirec-\ntional Transformers for Language Understanding,” (2018). https://arxiv.org/abs/1810.04805.\n\n\n223\nBERT\nfor “dog” or “apple” cannot deal with the fact that “hot dog” or “Big Apple” are not a\ntype of animal or fruit, respectively. As another example, consider what “play” means\nin these sentences: “They played games,” “I play Chopin,” “We play baseball,” and\n“Hamlet is a play by Shakespeare” (these sentences are all from Tatoeba.org). These\noccurrences of “play” have different meanings, and assigning a single vector wouldn’t\nhelp much in downstream NLP tasks (e.g., in classifying the topic into sports, music,\nand art). \n Due to this limitation, NLP researchers started exploring ways to transform the\nentire sentence into a series of vectors that consider the context, called contextualized\nembeddings or simply contextualization. With these representations, all the occurrences\nof “play” in the previous examples would have different vectors assigned, helping\ndownstream tasks disambiguate different uses of the word. Notable milestones in con-\ntextualized embeddings include CoVe3 and ELMo (section 9.3.1), although the big-\ngest breakthrough was achieved by BERT, a Transformer-based pretrained language\nmodel, which is the focus of this section.\n We learned the Transformer\nuses a mechanism called self-\nattention to gradually transform\nthe input sequence by summariz-\ning it. The core idea of BERT is\nsimple: it uses the Transformer\n(the Transformer encoder, to be\nprecise) to transform the input\ninto contextualized embeddings.\nThe Transformer transforms the\ninput through a series of layers\nby gradually summarizing the\ninput. Similarly, BERT contextu-\nalizes the input through a series\nof Transformer encoder layers.\nThis is illustrated in figure 9.4.\n Because BERT is based on the Transformer architecture, it inherits all the\nstrengths of the Transformer. Its self-attention mechanism enables it to “random\naccess” over the input and capture long-term dependencies among input tokens.\nUnlike traditional language models (such as the one based on LSTM that we covered\nin section 5.5) that can make predictions in only one direction, the Transformer can\ntake into account the context in both directions. Using the sentence “Hamlet is a play\nby Shakespeare” as an example, the contextualized embedding for “play” can incorpo-\nrate the information from both “Hamlet” and “Shakespeare,” which makes it easier to\ncapture its “dramatic work for the stage” meaning of “play.”\n3 Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher, “Learned in Translation: Contextual-\nized Word Vectors,” in NIPS 2017.\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\nBERT\nContextualized embeddings\nFigure 9.4\nBERT processes input through attention layers \nto produce contextualized embeddings.\n",
      "page_number": 236
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 244-251)",
      "start_page": 244,
      "end_page": 251,
      "detection_method": "topic_boundary",
      "content": "224\nCHAPTER 9\nTransfer learning with pretrained language models\n If this concept is as simple as “BERT is just a Transformer encoder,” why does it\ndeserve an entire section here? Because we haven’t answered two important practical\nquestions yet: how to train and adapt the model. Neural network models, no matter\nhow powerful, are useless without specific strategies for training and where to get the\ntraining data. Also, transfer learning is useless without specific strategies for adapting\nthe pretrained model. We will discuss these questions in the following subsections.\n9.2.2\nSelf-supervised learning\nThe Transformer, which was originally proposed for machine translation, is trained\nusing parallel text. Its encoder and decoder are optimized to minimize the loss func-\ntion, which is the cross entropy defined by the difference between the decoder output\nand the expected, correct translation. However, the purpose of pretraining BERT is to\nderive high-quality contextualized embeddings, and BERT has only an encoder. How\ncan we “train” BERT so that it is useful for downstream NLP tasks?\n If you think of BERT just as another way of deriving embeddings, you can draw\ninspiration from how word embeddings are trained. Recall that in section 3.4, to train\nword embeddings, we make up a “fake” task where surrounding words are predicted\nwith word embeddings. We are not interested in the prediction per se but rather the\n“by-product” of the training, which is the word embeddings derived as the parameters\nof the model. This type of training paradigm where the data itself provides training\nsignals is called self-supervised learning, or simply self-supervision, in modern machine\nlearning. Self-supervised learning is still one type of supervised learning from the\nmodel’s point of view—the model is trained in such a way that it minimizes the loss\nfunction defined by the training signal. It is where the training signal comes from that\nis different. In supervised learning, training signals usually come from human annota-\ntions. In self-supervised learning, training signals come from the data itself with no\nhuman intervention.\n With increasingly larger datasets and more powerful models, self-supervised learn-\ning has become a popular way to pretrain NLP models in the past several years. But\nwhy does it work so well? Two factors contribute to this—one is that the type of self-\nsupervision here is trivially simple to create (just extracting surrounding words for\nWord2vec), but it requires deep understanding of the language to solve it. For exam-\nple, reusing the example from the language model we discussed in chapter 5, to\nanswer “My trip to the beach was ruined by bad ___,” not only does the system need to\nunderstand the sentence but it also needs to be equipped with some sort of “common\nsense” for what type of things could ruin a trip to a beach (e.g., bad weather, heavy\ntraffic). The knowledge required to predict the surrounding words ranges from sim-\nple collocation/association (e.g., The Statue of ____ in New ____), syntactic and\ngrammatical (e.g., “My birthday is ___ May”), and semantic (the previous example).\nSecond, there is virtually no limit on the amount of data used for self-supervision,\nbecause all you need is clean, plain text. You can download large datasets (e.g., Wiki-\npedia dump) or crawl and filter web pages, which is a popular way to train many pre-\ntrained language models.\n\n\n225\nBERT\n9.2.3\nPretraining BERT\nNow that we all understand how useful self-supervised learning can be for pretraining\nlanguage models, let’s see how we can use it for pretraining BERT. As mentioned ear-\nlier, BERT is just a Transformer encoder that transforms the input into a series of\nembeddings that take context into account. For pretraining word embeddings, you\ncould simply predict surrounding words based on the embeddings of the target word.\nFor pretraining unidirectional language models, you could simply predict the next\ntoken based on the tokens that come before the target. But for bidirectional language\nmodels such as BERT, you cannot use these strategies, because the input for the pre-\ndiction (contextualized embeddings) also depends on what comes before and after\nthe input. This sounds like a chicken-and-egg problem.\n The inventors of BERT solved this with a brilliant idea called masked language model\n(MLM), where they drop (mask) words randomly in a given sentence and let the\nmodel predict what the dropped word is. Specifically, after replacing a small percent-\nage of words in a sentence with a\nspecial placeholder, BERT uses\nthe Transformer to encode the\ninput and then uses a feed-\nforward layer and a softmax lay-\ners to derive a probability distri-\nbution over possible words that\ncan fill in that blank. Because\nyou already know the answer\n(because you dropped the words\nin the first place), you can use the\nregular cross entropy to train the\nmodel, as illustrated in figure 9.5. \n \nMasking \nand \npredicting\nwords is not a completely new\nidea—it’s closely related to cloze\ntests, where the test-taker is asked\nto replace the removed words in\na sentence. This test form is\noften used to assess how well stu-\ndents can understand the language. As we saw earlier, completing missing words in a\nnatural language text requires deep understanding of the language, ranging from\nsimple associations to semantic relationships. As a result, by telling the model to solve\nthis fill-in-the-blank type of task over a huge amount of textual data, the neural net-\nwork model is trained so that it can produce contextualized embeddings that incorpo-\nrate deep linguistic knowledge.\n You may be wondering what this input [MASK] is and what you actually need to do\nif you want to implement pretraining BERT yourself. In training neural networks,\nStatue\nof\n[MASK]\nLiberty\nin\nTransformer layer\nTransformer layer\nSoftmax\nBERT\nFigure 9.5\nPretraining BERT with a masked language \nmodel\n\n\n226\nCHAPTER 9\nTransfer learning with pretrained language models\npeople often use special tokens such as [MASK] that we mentioned here. These spe-\ncial tokens are just like other (naturally occurring) tokens such as the words “dog”\nand “cat,” except they don’t occur in text naturally (you can’t find any [MASK] in nat-\nural language corpora, no matter how hard you look) and the designers of the neural\nnetworks define what they mean. The model will learn to give representations to these\ntokens so that it can solve the task at hand. Other special tokens include BOS (begin-\nning of sentence), EOS (end of sentence), and UNK (unknown word), which we\nalready encountered in earlier chapters.\n Finally, BERT is pretrained not just with the masked language model but also with\nanother type of task called next-sentence prediction (NSP), where two sentences are given\nto BERT and the model is asked to predict whether the second sentence is the “real”\nnext sentence of the first. This is another type of self-supervised learning (“fake” task)\nfor which the training data can be created in an unlimited manner without much\nhuman intervention, because you can extract two consecutive sentences (or just stitch\ntogether two sentences at random) from any corpus and make the training data for\nthis task. The rationale behind this task is that by training with this objective, the\nmodel will learn how to infer the relationship of two sentences. However, the effective-\nness of this task has been actively debated (e.g., RoBERTa dropped this task whereas\nALBERT replaced it with another task called sentence-order prediction), and we will not\ngo into the details of this task here.\n All this pretraining sounds somewhat complex, but good news is that you rarely\nneed to implement this step yourself. Similar to word embeddings, developers and\nresearchers of these language models pretrain their models on a huge amount of nat-\nural language text (usually 10 GB-plus or even 100 GB-plus of uncompressed text)\nwith many GPUs and make the pretrained models publicly available so that anyone\ncan use them.\n9.2.4\nAdapting BERT\nAt the second (and final) stage of transfer learning, a pretrained model is adapted to\nthe target task so that the latter can leverage signals learned by the former. There are\ntwo main ways to adapt BERT to individual downstream tasks: fine-tuning and feature\nextraction. In fine-tuning, the neural network architecture is slightly modified so that it\ncan produce the type of predictions for the task in question, and the entire network is\ncontinuously trained on the training data for the task so that the loss function is mini-\nmized. This is exactly the way you train a neural network for NLP tasks, such as senti-\nment analysis, with one important difference—BERT “inherits” the model weights\nlearned through pretraining, instead of being initialized randomly and trained from\nscratch. In this way, the downstream task can leverage the powerful representations\nlearned by BERT through pretraining on a large amount of data.\n The exact way the BERT architecture is modified varies, depending on the final\ntask, but here I’m going to describe the simplest case where the task is to predict some\nsort of label for a given sentence. This is also called a sentence-prediction task, which\n\n\n227\nBERT\nincludes sentiment analysis, which we covered in chapter 2. For downstream tasks to\nbe able to extract representations for a sentence, BERT prepends a special token\n[CLS] (for classification) to every sentence at the pretraining phase. You can extract\nthe hidden states of BERT with this token and use them as the representation of the\nsentence. As with other classification tasks, a linear layer can compress this representa-\ntion into a set of “scores” that correspond to how likely each label is the correct\nanswer. You can then use softmax to derive a probability distribution. For example, if\nyou are working on a sentiment analysis dataset with five labels (strongly negative to\nstrongly positive), you’ll use a linear layer to reduce the dimensionality to 5. This type\nof linear layer combined with softmax, which is plugged into a larger pretrained\nmodel such as BERT, is often called a head. In other words, we are attaching a classifica-\ntion head to BERT to solve a sentence-prediction task. The weights for the entire net-\nwork (the head and BERT) are adjusted so that the loss function is minimized. This\nmeans that the BERT weights initialized with pretrained ones also are adjusted (fine-\ntuned) through backpropagation. See figure 9.6 for an illustration.\nFigure 9.6\nPretraining and fine-tuning BERT with an attached classification head\nAnother variation in fine-tuning BERT uses all the embeddings, averaged over the\ninput tokens. In this method, called mean over time or bag of embeddings, all the embed-\ndings produced by BERT are summed up and divided by the length of input, just like\nthe bag-of-words model, to produce a single vector. This method is less popular than\nusing the CLS special token but may work better depending on the task. Figure 9.7\nillustrates this.\nClassification\nhead\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\nSoftmax\n[CLS]\nStatue\nof\nLiberty\n[MASK]\nTransformer layer\nTransformer layer\nSoftmax\nPretraining\nFine-tuning\nBackpropagation\nNegative Neutral Positive\n\n\n228\nCHAPTER 9\nTransfer learning with pretrained language models\nFigure 9.7\nPretraining and fine-tuning BERT using mean over time and a classification head\nAnother way to adapt BERT for downstream NLP tasks is feature extraction. Here BERT\nis used to extract features, which are simply a sequence of contextualized embeddings\nproduced by the final layer of BERT. You can simply feed these vectors to another\nmachine learning model as features and make predictions, as shown in figure 9.8.\nFigure 9.8\nPretraining and using BERT for feature extraction\nClassification\nhead\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\nSoftmax\n[CLS]\nStatue\nof\nLiberty\n[MASK]\nTransformer layer\nTransformer layer\nSoftmax\nPretraining\nFine-tuning\nBackpropagation\nMean over time\nNegative Neutral Positive\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\n[CLS]\nStatue\nof\nLiberty\n[MASK]\nTransformer layer\nTransformer layer\nSoftmax\nPretraining\nFeature extraction\nFeatures\nML Model\nNegative Neutral Positive\n\n\n229\nCase study 1: Sentiment analysis with BERT\nGraphically, this approach looks similar to fine-tuning. After all, you are feeding the\noutput from BERT to another ML model. However, there are two subtle but import-\nant differences: first, because you are no longer optimizing the neural network, the\nsecond ML model doesn’t have to be a neural network. Some machine learning tasks\n(e.g., unsupervised clustering) are not what neural networks are good at solving, and\nfeature extraction offers a perfect solution in these situations. Also, you are free to use\nmore “traditional” ML algorithms, such as SVMs (support vector machines), decision\ntrees, and gradient-boosted methods (such as GBDT, or gradient-boosted decision\ntrees), which may offer a better tradeoff in terms of computational cost and perfor-\nmance. Second, because BERT is used only as a feature extractor, there is no back-\npropagation and its internal parameters won’t be updated during the adaptation\nphase. In many cases, you get better accuracy in the downstream task if you fine-tune\nthe BERT parameters, because by doing so, you are also teaching BERT to get better\nat the task at hand. \n Finally, note that these two are not the only ways to adapt BERT. Transfer learning\nis an actively researched topic, not just in NLP but also in many fields of artificial intel-\nligence, and we have many other ways to use pretrained language models to make the\nbest of them. If you are interested in learning more, I recommend checking out the\ntutorial given at NAACL 2019 (one of the top NLP conferences) titled “Transfer\nLearning in Natural Language Processing” (http://mng.bz/o8qp).\n9.3\nCase study 1: Sentiment analysis with BERT\nIn this section, we will build a sentiment analyzer (again), but this time with BERT.\nInstead of AllenNLP, we will use the Transformers library developed by Hugging Face,\nwhich we used for making predictions with language models in the previous chapter.\nAll the code here is accessible on a Google Colab notebook (http://www.realworldnlp-\nbook.com/ch9.html#sst). The code snippets you see in this section all assume that\nyou import related modules, classes, and methods as follows:\nimport torch\nfrom torch import nn, optim\nfrom transformers import AutoTokenizer, AutoModel, AdamW, \nget_cosine_schedule_with_warmup\nIn the Transformers library, you specify the pretrained models by their names. We’ll\nuse the cased BERT-base model ('bert-base-cased') throughout this section, so\nlet’s define a constant first as follows: \nBERT_MODEL = 'bert-base-cased'\nThe Transformers library also supports other pretrained BERT models, which you\ncan see in their documentation (https://huggingface.co/transformers/pretrained_\nmodels.html). If you want to use other models, you can simply replace this constant\nwith the name of the model you want to use, and the rest of the code works as-is in\nmany cases (but not always).\n\n\n230\nCHAPTER 9\nTransfer learning with pretrained language models\n9.3.1\nTokenizing input\nThe first step we took for building an NLP model is to build a dataset reader.\nAlthough AllenNLP (or more precisely speaking, the allennlp-modules package)\nis shipped with a dataset reader for the Stanford Sentiment Treebank, the dataset\nreader’s output is compatible only with AllenNLP. In this section, we are going to write\na simple method that reads the dataset and returns a sequence of batched input\ninstances.\n Tokenization is one of the most important steps in processing natural language\ninput. As we saw in the previous chapter, tokenizers in the Transformers library can be\ninitialized with the AutoTokenizer.from_pretrained() class method as follows:\ntokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\nBecause different pretrained models use different tokenizers, it is important to initial-\nize the one that matches the pretrained model you are going to use by supplying the\nsame model name.\n You can use the tokenizer to convert between a string and a list of token IDs back\nand forth, as shown next:\n>>> token_ids = tokenizer.encode('The best movie ever!')\n[101, 1109, 1436, 2523, 1518, 106, 102]\n>>> tokenizer.decode(token_ids)\n'[CLS] The best movie ever! [SEP]'\nNotice that BERT’s tokenizer added two special tokens—[CLS] and [SEP]—to your\nsentence. As discussed earlier, CLS is a special token used to extract the embedding\nfor the entire input, whereas SEP is used to separate two sentences if your task involves\nmaking predictions on a pair of sentences. Because we are making predictions for sin-\ngle sentences here, there’s no need to pay much attention to this token. We’ll discuss\nsentence-pair classification tasks later in section 9.5.\n Deep neural networks rarely operate on single instances. They usually are trained\non and make predictions for batches of instances for stability and performance rea-\nsons. The tokenizer also supports converting the given input in batches by invoking\nthe __call__ method (i.e., just use the object as a method) as follows:\n>>> result = tokenizer(\n>>>    ['The best movie ever!', 'Aweful movie'],\n>>>    max_length=10,\n>>>    pad_to_max_length=True,\n>>>    truncation=True,\n>>>    return_tensors='pt')\nWhen you run this, each string in the input list is tokenized and then resulting tensors\nare padded with 0s to have the same lengths. Padding here means adding 0s at the end\n\n\n231\nCase study 1: Sentiment analysis with BERT\nof each sequence so that individual instances have the same length and can be bundled\nas a single tensor, which is needed for more efficient computation (we’ll cover padding\nin more detail in chapter 10). The method call contains several other parameters that\ncontrol the maximum length (max_length=10, meaning to pad everything to the\nlength of 10), whether to pad to the maximum length, whether to truncate sequences\nthat are too long, and the type of the returned tensors (return_tensors='pt',\nmeaning it returns PyTorch tensors). The result of this tokenizer() call is a dictio-\nnary that contains the following three keys and three different types of packed tensors:\n>>> result['input_ids']\ntensor([[ 101, 1109, 1436, 2523, 1518,  106,  102,    0,    0,    0],\n        [ 101,  138, 7921, 2365, 2523,  102,    0,    0,    0,    0]])\n>>> result['token_type_ids']\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> result['attention_mask']\ntensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\nThe input_ids tensor is a packed version of token IDs converted from the texts.\nNotice that each row is a vectorized token ID padded with 0s so that its length is always\n10. The token_type_ids tensor specifies which sentence each token comes from. As\nwith the SEP special token earlier, this is relevant only if you are working with sen-\ntence pairs, which is why the tensor is simply filled with just 0s. The attention_mask\ntensor specifies which tokens the Transformer should attend to. Because there are no\ntokens at the padded elements (0s in input_ids), the corresponding elements in\nattention_mask are all 0s, and attention to these tokens is simply ignored. Masking\nis a common technique often used in neural networks to ignore irrelevant elements in\nbatched tensors like the ones shown here. Chapter 10 covers masking in more detail.\n As you see here, the Transformers library’s tokenizers do more than just\ntokenizing—they take a list of strings and create batched tensors for you, including the\nauxiliary tensors (token_type_ids and attention_mask). You just need to create\nlists of strings from your dataset and pass them to tokenizer()to create batches to\npass on to the model. This logic for reading datasets is rather boring and a bit lengthy,\nso I packaged it in a method named read_dataset, which is not shown here. If you\nare interested, you can check the Google Colab notebook mentioned earlier. Using\nthis method, you can read a dataset and convert it to a list of batches as follows:\ntrain_data = read_dataset('train.txt', batch_size=32, tokenizer=tokenizer, \nmax_length=128)\ndev_data = read_dataset('dev.txt', batch_size=32, tokenizer=tokenizer, \nmax_length=128)\n",
      "page_number": 244
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 252-262)",
      "start_page": 252,
      "end_page": 262,
      "detection_method": "topic_boundary",
      "content": "232\nCHAPTER 9\nTransfer learning with pretrained language models\n9.3.2\nBuilding the model\nIn the next step, we’ll build the model to classify texts into their sentiment labels. The\nmodel we build here is nothing but a thin wrapper around BERT. All it does is pass the\ninput through BERT, take out its embedding at CLS, pass it through a linear layer to\nconvert to a set of scores (logits), and compute the loss.\n Note that we are building a PyTorch Module, not an AllenNLP Model, so make\nsure to inherit from nn.Module, although the structure of these two types of models\nare usually very similar (because AllenNLP’s Models inherit from PyTorch Modules).\nYou need to implement __init__(), where you define and initialize submodules of\nthe model, and forward(), where the main computation (“forward pass”) happens.\nThe entire code snippet is shown next.\nclass BertClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BertClassifier, self).__init__()\n        self.bert_model = AutoModel.from_pretrained(model_name)    \n        self.linear = nn.Linear(self.bert_model.config.hidden_size, \nnum_labels)   \n        self.loss_function = nn.CrossEntropyLoss()\n    def forward(self, input_ids, attention_mask, token_type_ids, label=None):\n        bert_out = self.bert_model(     \n          input_ids=input_ids,\n          attention_mask=attention_mask,\n          token_type_ids=token_type_ids)\n        \n        logits = self.linear(bert_out.pooler_output)    \n        loss = None\n        if label is not None:\n            loss = self.loss_function(logits, label)    \n        return loss, logits\nThe module first defines the BERT model (via the AutoModel.from_pretrained()\nclass method), a linear layer (nn.Linear), and the loss function (nn.CrossEntropy-\nLoss) in __init__(). Note that the module has no way of knowing the number of\nlabels it needs to classify into, so we are passing it as a parameter (num_labels).\n In the forward() method, it first calls the BERT model. You can simply pass the\nthree types of tensors (input_ids, attention_mask, and token_type_ids) to the\nmodel. The model returns a data structure that contains last_hidden_state\nand pooler_output among other things, where last_hidden_state is a sequence\nof hidden states of the last layer, whereas pooler_output is a pooled output, which\nis basically the embedding at CLS transformed with a linear layer. Because we are\nListing 9.1\nSentiment analysis model with BERT\nInitializes \nBERT\nDefines a \nlinear layer\nApplies \nBERT\nApplies the \nlinear layer\nComputes \nthe loss\n\n\n233\nCase study 1: Sentiment analysis with BERT\ninterested only in the pooled output that represents the entire input, we’ll pass the lat-\nter to the linear layer. Finally, the method computes the loss (if the label is supplied)\nand returns it, along with the logits, which are used for making predictions and mea-\nsuring the accuracy.\n Pay attention to the way we designed the method signature—it takes the three ten-\nsors we inspected earlier with their exact names. This lets us simply destruct a batch\nand pass it to the forward method, as shown here:\n>>> model(**train_data[0])\n(tensor(1.8050, grad_fn=<NllLossBackward>),\n tensor([[-0.5088,  0.0806, -0.2924, -0.6536, -0.2627],\n         [-0.3816,  0.3512, -0.1223, -0.5136, -0.4421],\n         ...\n         [-0.4220,  0.3026, -0.1723, -0.4913, -0.4106],\n         [-0.3354,  0.3871, -0.0787, -0.4673, -0.4169]],\n        grad_fn=<AddmmBackward>))\nNotice that the return value of the forward pass is a tuple of the loss and the logits.\nNow you are ready to train your model!\n9.3.3\nTraining the model\nIn the third and the final step of this case study, we will train and validate the model.\nAlthough AllenNLP took care of the training process in the previous chapters, in this\nsection we’ll write our own training loop from scratch so we can better understand\nwhat it takes to train a model yourself. Note that you can also choose to use the library’s\nown Trainer class (https://huggingface.co/transformers/main_classes/trainer.html),\nwhich works similarly to AllenNLP’s Trainer, to run the training loop by specifying its\nparameters.\n We covered the basics of training loops in section 2.5, but to recap, in modern\nmachine learning, every training loop looks somewhat similar. If you write it in\npseudocode, it would look like the one shown as follows. \nMAX_EPOCHS = 100\nmodel = Model()\nfor epoch in range(MAX_EPOCHS):\n    for batch in train_set:\n        loss, prediction = model.forward(**batch)\n        new_model = optimizer(model, loss)\n        model = new_model\nThis training loop is almost identical to listing 2.2, except it operates on batches instead\nof single instances. The dataset yields a series of batches, which are then passed to\nthe forward method of the model. The method returns the loss, which is then used to\nListing 9.2\nPseudocode for the neural network training loop\n\n\n234\nCHAPTER 9\nTransfer learning with pretrained language models\noptimize the model. It is also common for the model to return the predictions so that\nthe caller can use the result to compute some metrics, such as accuracy.\n Before we move on to writing our own training loop, we need to note two things—\nit is customary to alternate between training and validation during each epoch. In the\ntraining phase, the model is optimized (the “magic constants” are changed) based on\nthe loss function and the optimizer. The training data is used during this phase. In the\nvalidation phase, the model’s parameters are fixed, and its accuracy of prediction is\nmeasured against validation data. Although the loss is not used for optimization\nduring validation, it is common to compute it to monitor how the loss changes during\nthe course of the training, as we did in section 6.3.\n Another thing to note is that when training Transformer-based models such as\nBERT, we usually use warm-up, a process where the learning rate (how much to change\nthe magic constants) is gradually increased for the first few thousand steps. A step\nhere is just another name for one execution of backpropagation, which corresponds\nto the inner loop of listing 9.2. This is useful for stabilizing training. We are not going\ninto the mathematical details of warm-up and controlling the learning rate here—we\njust note that a learning rate scheduler is usually used for controlling the learning rate\nover the course of the training. With the Transformers library, you can define an opti-\nmizer (AdamW) and a learning controller as follows:\noptimizer = AdamW(model.parameters(), lr=1e-5)\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=1000)\nThe controller we are using here (get_cosine_schedule_with_warmup) increases\nthe learning rate from zero to the maximum during the first 100 steps, then gradually\ndecreases it afterward (based on the cosine function, which is where it got its name).\nIf you plot how the learning rate changes over time, it’ll look like the graph in\nfigure 9.9.\nFigure 9.9\nWith a cosine learning rate schedule with warm-up, the learning rate ramps up \nfirst, then declines following a cosine function.\nNow we are ready to train our BERT-based sentiment analyzer. The next listing shows\nour training loop.\n\n\n235\nCase study 1: Sentiment analysis with BERT\n \nfor epoch in range(epochs):\n    print(f'epoch = {epoch}')\n    \n    model.train()    \n    losses = []\n    total_instances = 0\n    correct_instances = 0\n    for batch in train_data:\n        batch_size = batch['input_ids'].size(0)\n        move_to(batch, device)     \n        optimizer.zero_grad()     \n        \n        loss, logits = model(**batch)   \n        loss.backward()    \n        optimizer.step()\n        scheduler.step()\n    \n        losses.append(loss)\n        \n        total_instances += batch_size\n        correct_instances += torch.sum(torch.argmax(logits, dim=-1) \n   == batch['label']).item()   \n    \n    avr_loss = sum(losses) / len(losses)\n    accuracy = correct_instances / total_instances\n    print(f'train loss = {avr_loss}, accuracy = {accuracy}')\nWhen you train a model using PyTorch (and, consequently, AllenNLP and Transform-\ners, two libraries that are built on top of it), remember to call model.train() to turn\non the “training mode” of the model. This is important because some layers such as\nBatchNorm and dropout behave differently between training and evaluation (we’ll\ncover dropout in chapter 10). On the other hand, when you validate or test your\nmodel, be sure to call model.eval().\n The code in listing 9.3 does not show the validation phase, but the code for valida-\ntion would look almost the same as that for training. When you validate/test your\nmodel, pay attention to the following:\nAs mentioned previously, make sure to call model.eval() before validating/\ntesting your model.\nOptimization calls (loss.backward(), optimizer.step(), and sched-\nuler.step()) are not necessary because you are not updating the model.\nLosses are still recorded and reported for monitoring. Be sure to wrap your for-\nward pass call with with torch.no_grad()—this will disable gradient com-\nputation and save memory.\nAccuracy is computed in exactly the same way (this is the point of validation!).\nListing 9.3\nTraining loop for the BERT-based sentiment analyzer\nTurns on the \ntraining mode\nMoves the batch to \nGPU (if available)\nRemember to reset the gradients\n(in PyTorch gradients accumulate).\nForward\npass\nBackpropagation\nComputes the accuracy by counting \nthe number of correct instances\n\n\n236\nCHAPTER 9\nTransfer learning with pretrained language models\nWhen I ran this, I got the following output to stdout (with intermediate epochs\nomitted):\nepoch = 0\ntrain loss = 1.5403757095336914, accuracy = 0.31624531835205993\ndev loss = 1.7507736682891846, accuracy = 0.2652134423251589\nepoch = 1\n...\nepoch = 8\ntrain loss = 0.4508829712867737, accuracy = 0.8470271535580525\ndev loss = 1.687158465385437, accuracy = 0.48319709355131696\nepoch = 9\n...\nThe dev accuracy peaked around 0.483 at epoch 8 and didn’t improve after that. Com-\npared to the result we got from LSTM (dev accuracy ~0.35, in chapter 2) and CNN\n(dev accuracy ~0.40, in chapter 7), this is the best result we have achieved on this data-\nset. We’ve done very little hyperparameter tuning, so it’s too early to conclude that\nBERT is the best model of the three we compared, but we at least know that it is a\nstrong baseline to start from!\n9.4\nOther pretrained language models\nBERT is neither the first nor the last of popular pretrained language models (PLMs)\ncommonly used in the NLP community nowadays. In this section, we’ll learn several\nother popular PLMs and how they are different from BERT. Most of these models are\nalready implemented and publicly available from the Transformers library, so you\ncan integrate them with your NLP application by changing just a couple of lines of\nyour code. \n9.4.1\nELMo\nELMo (Embeddings from Language Models), proposed4 in early 2018, is one of the\nearliest PLMs for deriving contextualized embeddings using unlabeled texts. Its core\nidea is simple—train an LSTM-based language model (similar to the one we trained\nback in chapter 5) and use its hidden states as additional “features” for downstream\nNLP tasks. Because the language model is trained to predict the next token given the\nprevious context, the hidden states can encode the information needed to “under-\nstand the language.” ELMo does the same with another, backward LM and combines\nthe embeddings from both directions so that it can also encode the information in\nboth directions. See figure 9.10 for an illustration.\n After pretraining LMs in both directions, downstream NLP tasks can simply use the\nELMo embeddings as features. Note that ELMo uses multilayer LSTM, so the features\nare the sum of hidden states taken from different layers, weighted in a task-specific way.\nThe inventors of ELMo showed that adding these features improves the performance\n4 Peters et al., “Deep Contextualized Word Representations,” (2018). https://arxiv.org/abs/1802.05365.\n\n\n237\nOther pretrained language models\nof a wide range of NLP tasks, including sentiment analysis, named entity recognition,\nand question answering. Although ELMo is not implemented in Hugging Face’s Trans-\nformers library, you can use it with AllenNLP fairly easily.5\n ELMo is a historically important PLM, although it is not often used in research or\nproduction anymore today—it predates BERT (and the advent of the Transformer)\nand there are other PLMs (including BERT) that outperform ELMo and are widely\navailable today. \n9.4.2\nXLNet\nXLNet, proposed in 2019, is an important successor of BERT and often referenced as\none of the most powerful PLMs as of today. XLNet addresses two main issues of how\nBERT is trained: train-test skew and the independence of masks. The first issue has to\n5 See here for the detailed documentation on how to use ELMo with AllenNLP: https://allennlp.org/elmo.\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nELMo\nis\nLSTM\nLSTM\nForward\nBackward\nLayer\n2\nForward\nBackward\nLayer\n1\n<s>\nThis\n</s>\nFigure 9.10\nELMo computes contextualized embeddings by combining forward and backward LSTMs.\n\n\n238\nCHAPTER 9\nTransfer learning with pretrained language models\ndo with how BERT is pretrained using the masked language model (MLM) objective.\nDuring training time, BERT is trained so that it can accurately predict masked tokens,\nwhereas during prediction, it just sees the input sentence, which does not contain any\nmasks. This means that there’s a discrepancy of information to which BERT is\nexposed to between training and testing, and that creates the train-test skew problem.\n The second issue has to do with how BERT makes predictions for masked tokens.\nIf there is more than one [MASK] token in the input, BERT makes predictions for\nthem in parallel. There doesn’t seem to be anything wrong with this approach at first\nglance—for example, if the input was “The Statue of [MASK] in New [MASK],” the\nmodel wouldn’t have difficulties answering this as “Liberty” and “York.” If the input\nwas “The Statue of [MASK] in Washington, [MASK],” most of you (and probably a\nlanguage model) would predict “Lincoln” and “DC.” However, what if the input was\nthe following:\nThe Statue of [MASK] in [MASK] [MASK]\nThen there is no information to bias your prediction one way or the other. BERT\nwon’t learn the fact that “The Statue of Liberty in Washington, DC” or “The Statue of\nLincoln in New York” don’t make much sense during the training from this example,\nbecause these predictions are all made in parallel. This is a good example showing\nthat you cannot simply make independent predictions on tokens and combine them\nto create a sentence that makes sense.\nNOTE\nThis issue is related to the multimodality of natural language, which\nmeans there are multiple modes in the joint probability distribution, and\ncombinations of best decisions made independently do not necessarily lead to\nglobally best decisions. Multimodality is a big challenge in natural language\ngeneration.\nTo address this issue, instead of making predictions in parallel, you can make predic-\ntions sequentially. In fact, this is exactly what typical language models do—generate\ntokens from left to right, one by one. However, here we have a sentence interspersed\nwith masked tokens, and predictions depend not only on the tokens on the left (e.g.,\n“The Statue of” in the previous example) but also on the right (“in”). XLNet solves\nthis by generating missing tokens in a random order, as shown in figure 9.11. For\nexample, you can choose to generate “New” first, which gives a strong clue for the\nnext words, “York” and “Liberty,” and so on. Note that prediction is still made based\non all the tokens generated previously. If the model chose to generate “Washington”\nfirst, then the model would proceed to generate “DC” and “Lincoln” and would never\nmix up these two.\n XLNet is already implemented in the Transformers library, and you can use the\nmodel with only a few lines of code change.6 \n6  See https://huggingface.co/transformers/model_doc/xlnet.html for the documentation.\n\n\n239\nOther pretrained language models\nFigure 9.11\nXLNet generates tokens in an arbitrary order.\n9.4.3\nRoBERTa\nRoBERTa (from “robustly optimized BERT”)7 is another important PLM that is com-\nmonly used in research and industry. RoBERTa revisits and modifies many training\ndecisions of BERT, which makes it match or even exceed the performance of post-\nBERT PLMs, including XLNet, which we covered earlier. My personal impression is\nthat RoBERTa is the second most referenced PLM after BERT as of this writing (mid-\n2020), and it shows robust performance in many downstream NLP tasks in English. \n RoBERTa makes several improvements over BERT, but the most important (and\nthe most straightforward) is the amount of its training data. The developers of\nRoBERTa collected five English corpora of varying sizes and domains, which total over\n160 GB of text (versus 16 GB used for training BERT). Simply by using a lot more data\nfor training, RoBERTa overperforms some of the other powerful PLMs, including\nXLNet, in downstream tasks after fine-tuning. The second modification has to do with\nthe next-sentence prediction (NSP) objective we touched on in section 9.2.3, where\nBERT is pretrained to classify whether the second sentence is the “true” sentence that\nfollows the first one in a corpus. The developers of RoBERTa found that, by removing\nNSP (and training with the MLM objective only), the performance of downstream\n7 Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach,” (2019). https://arxiv.org/abs/\n1907.11692.\nStatue\nof\n[MASK]\nin\nBERT\nXLNet\n[MASK][MASK]\nStatue\nof\nLiberty\nin\nNew\nYork\nNew\nXLNet\nNew\nYork\nXLNet\nNew\nLiberty\nYork\n\n\n240\nCHAPTER 9\nTransfer learning with pretrained language models\ntasks stays about the same or slightly improves. In addition to these, they also revisited\nthe batch size and the way masking is done for MLM. Combined, the new pretrained\nlanguage model achieved the state-of-the-art results on downstream tasks such as ques-\ntion answering and reading comprehension.\n Because RoBERTa uses the identical architecture to BERT and both are imple-\nmented in Transformers, switching to RoBERTa is extremely easy if your application\nalready uses BERT.\nNOTE\nSimilar to BERT versus RoBERTa, the cross-lingual language model\nXLM (covered in section 8.4.4) has its “robustly optimized” sibling called\nXLM-R (short for XML-RoBERTa).8 XLM-R pretrains on 100 languages and\nshows competitive performance on many cross-lingual NLP tasks.\n9.4.4\nDistilBERT\nAlthough pretrained models such as BERT and RoBERTa are powerful, they are com-\nputationally expensive, not just for pretraining but also for tuning and making predic-\ntions. For example, BERT-base (the regular-sized BERT) and BERT-large (the larger\ncounterpart) have 110 million and 340 million parameters, respectively, and virtually\nevery input has to go through this huge network to get predictions. If you were to fine-\ntune and make predictions with a BERT-based model (such as the one we built in sec-\ntion 9.3), you’d most certainly need a GPU, which is not always available, depending\non your computational environment. For example, if you’d like to run some real-time\ntext analytics on a mobile phone, BERT wouldn’t be a great choice (and it might not\neven fit in the memory).\n To reduce the computational requirement of modern large neural networks,\nknowledge distillation (or simply distillation) is often used. This is a machine learning\ntechnique where, given a large pretrained model (called the teacher model), a smaller\nmodel (called the student model) is trained to mimic the behavior of the larger model.\nSee figure 9.12 for more details. The student model is trained with the masked lan-\nguage model (MLM) loss (same as BERT), as well as the cross-entropy loss between\nthe teacher and the student. This pushes the student model to produce the probabil-\nity distribution over predicted tokens that are as similar to the teacher as possible.\n Researchers at Hugging Face developed a distilled version of BERT called Distil-\nBERT,9 which is 40% smaller and 60% faster while retraining 97% of task performance\ncompared to BERT. You can use DistilBERT by simply replacing the model name you\npass to AutoModel.from_pretrained() for BERT (e.g., bert-base-cased) with\ndistilled versions (e.g., distilbert-base-cased), while keeping the rest of your\ncode the same.\n8 Conneau et al., “Unsupervised Cross-lingual Representation Learning at Scale,” (2019). https://arxiv.org/\nabs/1911.02116.\n9 Sanh et al., “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,” (2019). https://\narxiv.org/abs/1910.01108.\n\n\n241\nOther pretrained language models\n9.4.5\nALBERT\nAnother pretrained language model that addresses the computational complexity\nproblem of BERT is ALBERT,10 short for “A Lite BERT.” Instead of resorting to know-\nledge distillation, ALBERT makes a few changes to its model and the training procedure. \n One design change ALBERT makes to its model is how it handles word embed-\ndings. In most deep NLP models, word embeddings are represented by and stored in\na big lookup table that contains one word embedding vector per word. This way of\nmanaging embeddings is usually fine for smaller models such as RNNs and CNNs.\nHowever, for Transformer-based models such as BERT, the dimensionality (i.e., the\nlength) of input needs to match that of the hidden states, which is usually as big as 768\ndimensions. This means that the model needs to maintain a big lookup table of size V\ntimes 768, where V is the number of unique vocabulary items. Because in many NLP\nmodels V is also large (e.g., 30,000), the resulting lookup table becomes huge and\ntakes up a lot of memory and computation.\n ALBERT addresses this issue by decomposing word embedding lookup into two\nstages, as shown in figure 9.13. The first stage is similar to how word embeddings are\nretrieved from a mapping table, except that the output dimensionality of word\nembedding vectors is smaller (say, 128 dimensions). In the next stage, these shorter\n10 Lan et al., “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations,” (2020).\nhttps://arxiv.org/abs/1909.11942.\nBERT\n(teacher)\nDistilBERT\n(student)\nTraining data\nSoftmax\nSoftmax\nSoftmax\nCross\nentropy\nLiberty\nMasked\nLM\nDistillation\nFigure 9.12\nKnowledge \ndistillation combines \ncross entropy and the \nmasked LM objectives.\n\n\n242\nCHAPTER 9\nTransfer learning with pretrained language models\nvectors are expanded using a linear layer so that they match the desired input dimen-\nsionality of the model (say, 768). This is similar to how we expanded word embed-\ndings with the Skip-gram model (section 3.4). Thanks to this decomposition, ALBERT\nneeds to store only two smaller lookup tables (V × 128, plus 128 × 768) instead of one\nbig look-up table (V × 768).\n Another design change that ALBERT implements is parameter sharing between\nTransformer layers. Transformer models use a series of self-attention layers to trans-\nform the input vector. The way these layers transform the input is usually different\nfrom layer to layer—the first layer may transform the input one way (e.g., capture\nbasic phrases), and the second one may do so another way (e.g., capture some syntac-\ntic information). However, this means that the model needs to retain all the necessary\nparameters (projections for keys, queries, and values) per each layer, which is expen-\nsive and takes up a lot of memory. Instead, ALBERT’s layers all share the same set of\nparameters, meaning that the model applies the same transformation repeatedly to\nthe input. These parameters are adjusted in such a way that the series of transforma-\ntions are effective for predicting the objective, even though they are identical.\n Finally, ALBERT uses a training objective called sentence-order prediction (SOP) for\npretraining, instead of the next-sentence prediction (NSP) adopted by BERT. As men-\ntioned earlier, the developers of RoBERTa and some others found out that the NSP\nobjective is basically useless and decided to eliminate it. ALBERT replaces NSP with\nsentence-order prediction (SOP), a task where the model is asked to predict the\nordering of two consecutive segments of text. For example:11\n(A) She and her boyfriend decided to go for a long walk. (B) After walking for\nover a mile, something happened.\n(C) However, one of the teachers around the area helped me get up. (D) At\nfirst, no one was willing to help me up.\n11 These examples are taken from ROCStories: https://cs.rochester.edu/nlp/rocstories/.\ndog\nchocolate\ncat\ndog\nchocolate\ncat\nBERT\nLinear\nlayer\n \nALBERT\nchocolate\ndog\nchocolate\ncat\nFigure 9.13\nALBERT (right) decomposes word embeddings into two smaller \nprojections.\n",
      "page_number": 252
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 263-270)",
      "start_page": 263,
      "end_page": 270,
      "detection_method": "topic_boundary",
      "content": "243\nCase study 2: Natural language inference with BERT\nIn the first example, you can tell that A happens before B. In the second, the order is\nflipped, and D should come before C. This is an easy feat for humans, but a difficult\ntask for machines—an NLP model needs to learn to ignore superficial topical signals\n(e.g., “go for a long walk,” “walking for over a mile,” “helped me get up,” and “help me\nup”) and focus on discourse-level coherence. Training with this objective makes the\nmodel more robust and effective for deeper natural language understanding tasks.\n As a result, ALBERT was able to scale up its training and outperform BERT-large\nwith fewer parameters. As with DistilBERT, the model architecture of ALBERT is\nalmost identical to that of BERT, and you can use it by simply supplying the model\nname when you call AutoModel.from_pretrained() (e.g., albert-base-v1).\n9.5\nCase study 2: Natural language inference with BERT\nIn this  final section of this chapter, we will build an NLP model for natural language\ninference, a task where the system predicts logical relationship between sentences.\nWe’ll use AllenNLP for building the model while demonstrating how to integrate\nBERT (or any other Transformer-based pretrained models) into your pipeline. \n9.5.1\nWhat is natural language inference?\nNatural language inference (or NLI, for short) is the task of determining the logical rela-\ntionship between a pair of sentences. Specifically, given one sentence (called premise)\nand another sentence (called hypothesis), you need to determine whether the hypo-\nthesis is logically inferred from the premise. This is easier to see in the following\nexamples.12\nIn the first example, the hypothesis (“The man is sleeping”) clearly contradicts the\npremise (“A man inspects . . .”) because someone cannot be inspecting something\nwhile asleep. In the second example, you cannot tell if the hypothesis contradicts or is\nentailed by the premise (especially the “laughing at the cats” part), which makes the\nrelationship “neutral.” In the third example, you can logically infer the hypothesis\nfrom the premise—in other words, the hypothesis is entailed by the premise.\n As you can guess, NLI can be tricky even for humans. The task requires not only\nlexical knowledge (e.g., plural of “man” is “men,” soccer is one type of sport) but also\nPremise\nHypothesis\nLabel\nA man inspects the uniform of a figure in \nsome East Asian country.\nThe man is sleeping.\ncontradiction\nAn older and younger man smiling.\nTwo men are smiling and laughing at \nthe cats playing on the floor.\nneutral\nA soccer game with multiple males playing.\nSome men are playing a sport.\nentailment\n12 These examples are taken from http://nlpprogress.com/english/natural_language_inference.html.\n\n\n244\nCHAPTER 9\nTransfer learning with pretrained language models\nsome “common sense” (e.g., you cannot inspect while sleeping). NLI is one of the\nmost typical natural language understanding (NLU) tasks. How can you build an NLP\nmodel to solve this task?\n Fortunately, NLI is a well-studied field in NLP. The most popular dataset for NLI,\nthe Standard Natural Language Inference (SNLI) corpus (https://nlp.stanford.edu/\nprojects/snli/), has been used in numerous NLP studies as a benchmark. In what fol-\nlows, we’ll build a neural NLI model with AllenNLP and learn how to use BERT for\nthis particular task. \n Before moving on, make sure that you have AllenNLP (we use version 2.5.0) and\nthe AllenNLP model’s modules installed. You can install them by running the follow-\ning code:\npip install allennlp==2.5.0\npip install allennlp-models==2.5.0\nThis also installs the Transformers library as a dependency.\n9.5.2\nUsing BERT for sentence-pair classification\nBefore we start building the model, notice that every input to the NLI task consists of\ntwo pieces: a premise and a hypothesis. Most of the NLP tasks we covered in this book\nhad just one part—usually a single sentence—as the input to the model. How can we\nbuild a model that makes predictions for instances that are pairs of sentences?\n We have multiple ways to deal with multipart input for NLP models. We can\nencode each sentence with an encoder and apply some mathematical operations\n(e.g., concatenation, subtractions) to the result to derive an embedding for the pair\n(which, by the way, is the basic idea of Siamese networks13). Researchers have also\ncome up with more complex neural network models with attention (such as BiDAF14).\n However, there’s inherently nothing preventing BERT from accepting more than\none sentence. Because the Transformer accepts a sequence of any tokens, you can\nsimply concatenate the two sentences and feed them to the model. If you are worried\nabout the model mixing up the two sentences, you can separate them with a special\ntoken, [SEP]. You can also add different values to each sentence as an extra signal to\nthe model. BERT uses these two techniques to solve sentence-pair classification tasks\nsuch as NLI with little modification to the model.\n The rest of the pipeline proceeds in a similar way to other classification tasks. A\nspecial token [CLS] is appended to every sentence pair, from which the final embed-\nding of the input is extracted. Finally, you can use a classification head to convert the\nembedding into a set of values (called logits) corresponding to the classes. This is illus-\ntrated in figure 9.14.\n13 Reimers and Gurevych, “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks,” (2019).\nhttps://arxiv.org/abs/1908.10084.\n14 Seo et al., “Bidirectional Attention Flow for Machine Comprehension,” (2018). https://arxiv.org/abs/1611\n.01603.\n\n\n245\nCase study 2: Natural language inference with BERT\nIn practice, concatenating and inserting special tokens are both taken care of by\nSnliReader, an AllenNLP dataset reader specifically built for dealing with the SNLI\ndataset. You can initialize the dataset and observe how it turns the data into AllenNLP\ninstances with the following snippet:\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\nfrom allennlp_models.pair_classification.dataset_readers import SnliReader\nBERT_MODEL = 'bert-base-cased'\ntokenizer = PretrainedTransformerTokenizer(model_name=BERT_MODEL, \nadd_special_tokens=False)\nreader = SnliReader(tokenizer=tokenizer)\ndataset_url = 'https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_dev.jsonl'\nfor instance in reader.read():\n    print(instance)\nThe dataset reader takes a JSONL (JSON line) file from the Stanford NLI corpus and\nturns it into a series of AllenNLP instances. We specify the URL of a dataset file that I\nput online (S3). Note that you need to specify add_special_tokens=False when\ninitializing the tokenizer. This sounds a little bit strange—aren’t the special tokens the\nvery things we need to add here? This is necessary because the dataset reader (Snli-\nReader), not the tokenizer, will take care of the special tokens. If you were to use the\nTransformer library only (without AllenNLP), you wouldn’t need this option.\n The previous snippet produces the following dump of generated instances:\nInstance with fields:\n         tokens: TextField of length 29 with text:\nTransformer layer\nTransformer layer\nSoftmax\nContradict Neutral Entail\nSentence 1\nSentence 2\nBERT\n[CLS]A soccer ...[SEP]Some men ...\nFigure 9.14\nFeeding \nand classifying a pair of \nsentences with BERT\n\n\n246\nCHAPTER 9\nTransfer learning with pretrained language models\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, \ngo, packages,\n                ., [SEP], The, sisters, are, hugging, goodbye, while, hold-\ning, to, go, \n                packages, after, just, eating, lunch, ., [SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: neutral in namespace: 'labels'.'\nInstance with fields:\n         tokens: TextField of length 20 with text:\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, \ngo, packages,\n                ., [SEP], Two, woman, are, holding, packages, ., [SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: entailment in namespace: 'labels'.'\nInstance with fields:\n         tokens: TextField of length 23 with text:\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, \ngo, packages,\n                ., [SEP], The, men, are, fighting, outside, a, del, ##i, ., \n[SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: contradiction in namespace: 'labels'.'\n...\nNotice that every sentence is tokenized, and the sentences are concatenated and sepa-\nrated by [SEP] special tokens. Each instance also has a label field containing the gold\nlabel. \nNOTE\nYou may have noticed some weird characters in the tokenized results,\nsuch as ##bracing and ##i. These are the results of byte-pair encoding (BPE),\na tokenization algorithm for splitting words into what’s called subword units.\nWe’ll cover BPE in detail in chapter 10.\n9.5.3\nUsing Transformers with AllenNLP\nNow we are ready to build our model with AllenNLP. The good news is you don’t need\nto write any Python code to build an NLI model thanks to AllenNLP’s built-in\nmodules—all you need to do is write a Jsonnet config file (as we did in chapter 4).\nAllenNLP also integrates Hugging Face’s Transformer library seamlessly, so you usu-\nally need to make little change, even if you want to integrate Transformer-based mod-\nels such as BERT into your existing models.\n You need to make changes to the following four components when integrating\nBERT into your model and pipeline:\nTokenizer—As you did in section 9.3 earlier, you need to use a tokenizer that\nmatches the pretrained model you are using.\nToken indexer—Token indexers turn tokens into integer indices. Because pre-\ntrained models come with their own predefined vocabularies, it is important\nthat you use a matching token indexer.\n\n\n247\nCase study 2: Natural language inference with BERT\nToken embedder—Token embedders turn tokens into embeddings. This is where\nthe main computation of BERT happens.\nSeq2Vec encoder—The raw output from BERT is a sequence of embeddings. You\nneed a Seq2Vec encoder to turn it into a single embedding vector.\nDon’t worry if this sounds intimidating—in most cases, all you need to do is remember\nto initialize the right modules with the name of the model you want. I’ll walk you\nthrough these steps next.\n First, let’s define the dataset we use for reading and converting the SNLI dataset.\nWe already did this with Python code earlier, but here we will write the corresponding\ninitialization in Jsonnet. First, let’s define the model name we’ll use throughout the\npipeline using the following code. One of the cool features of Jsonnet over vanilla\nJSON is you can define and use variables:\nlocal bert_model = \"bert-base-cased\";\nThe first section of the config file where the dataset is initialized looks like the following:\n\"dataset_reader\": {\n    \"type\": \"snli\",\n    \"tokenizer\": {\n        \"type\": \"pretrained_transformer\",\n        \"model_name\": bert_model,\n        \"add_special_tokens\": false\n    },\n    \"token_indexers\": {\n        \"bert\": {\n            \"type\": \"pretrained_transformer\",\n            \"model_name\": bert_model,\n        }\n    }\n},\nAt the top level, this is initializing a dataset reader specified by the type snli, which is\nthe SnliReader we experimented with previously. The dataset reader takes two\nparameters—tokenizer and token_indexers. For the tokenizer, we initialize a\nPretrainedTransformerTokenizer (type: pretrained_transformer) with a\nmodel name. Again, this is the tokenizer we initialized and used earlier in the Python\ncode. Notice how the Python code and the Jsonnet config file correspond to each\nother nicely. Most of AllenNLP modules are designed in such a way that there’s nice\ncorrespondence between these two, as shown in the following table.\nPython code\nJsonnet config\ntokenizer = \nPretrainedTransformerTokenizer(\n    model_name=BERT_MODEL, \n    add_special_tokens=False)\n\"tokenizer\": {\n    \"type\": \"pretrained_transformer\",\n    \"model_name\": bert_model,\n    \"add_special_tokens\": false\n}\n\n\n248\nCHAPTER 9\nTransfer learning with pretrained language models\nThe section for initializing a token indexer may look a bit confusing. It is initializing\na PretrainedTransformerIndexer (type: pretrained_transformer) with a\nmodel name. The indexer will store the indexed result to a section named bert (the\nkey corresponding to the token indexer). Fortunately, this code is a boilerplate that\nchanges little from model to model, and chances are you can simply copy and paste\nthis section when you work on a new Transformer-based model.\n As for the training/validation data, we can use the ones in this book’s S3 reposi-\ntory, shown here:\n\"train_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_train.jsonl\",\n\"validation_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_dev.jsonl\",\nNow we are ready to move on to defining our model:\n\"model\": {\n    \"type\": \"basic_classifier\",\n    \"text_field_embedder\": {\n        \"token_embedders\": {\n            \"bert\": {\n                \"type\": \"pretrained_transformer\",\n                \"model_name\": bert_model\n            }\n        }\n    },\n    \"seq2vec_encoder\": {\n        \"type\": \"bert_pooler\",\n        \"pretrained_model\": bert_model\n    }\n},\nAt the top level, this section is defining a BasicClassifier model (type: basic_\nclassifier). It is a generic text classification model that embeds the input, encodes\nit with a Seq2Vec encoder, and classifies it with a classification head (with a softmax\nlayer). You can “plug in” embedders and encoders of your choice as the subcompo-\nnents of the model. For example, you can embed the tokens via word embeddings and\nencode the sequence with an RNN (this is what we did in chapter 4). Alternatively, you\ncan encode the sequence with a CNN, as we did in chapter 7. This is where the design\nof AllenNLP excels—the generic model specifies only what (e.g., a TextField-\nEmbedder and a Seq2VecEncoder) but not exactly how (e.g., word embeddings,\nRNNs, BERT). You can use any submodules for embedding/encoding input, as long\nas those submodules conform to the specified interfaces (i.e., they are subclasses of\nthe required classes).\n In this case study, we will use BERT to embed the input sequence first. This is\nachieved by a special token embedder, PretrainedTransformerEmbedder (type:\npretrained_transformer), which takes the result of a Transformer tokenizer, puts\nit through a pretrained BERT model, and produces the embedded input. You need to\n\n\n249\nCase study 2: Natural language inference with BERT\npass this embedder as the value for the bert key (the one you specified for\ntoken_indexers earlier) of the token_embedders parameter.\n The raw output from BERT, however, is a sequence of embeddings. Because we are\ninterested in classifying the given pair of sentences, we need to extract the embed-\ndings for the entire sequence, which can be done by taking out the embeddings corre-\nsponding to the CLS special token. AllenNLP implements a type of Seq2VecEncoder\ncalled BertPooler (type: bert_pooler) that does exactly this.\n After embedding and encoding the input, the basic classifier model takes care of\nthe rest—the embeddings go through a linear layer that converts them into a set of\nlogits, and the entire network is trained with a cross-entropy loss, just like other classi-\nfication models. The entire config file is shown here.\nlocal bert_model = \"bert-base-cased\";\n{\n    \"dataset_reader\": {\n        \"type\": \"snli\",\n        \"tokenizer\": {\n            \"type\": \"pretrained_transformer\",\n            \"model_name\": bert_model,\n            \"add_special_tokens\": false\n        },\n        \"token_indexers\": {\n            \"bert\": {\n                \"type\": \"pretrained_transformer\",\n                \"model_name\": bert_model,\n            }\n        }\n    },\n    \"train_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_train.jsonl\",\n    \"validation_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/\nsnli/snli_1.0_dev.jsonl\",\n    \"model\": {\n        \"type\": \"basic_classifier\",\n        \"text_field_embedder\": {\n            \"token_embedders\": {\n                \"bert\": {\n                    \"type\": \"pretrained_transformer\",\n                    \"model_name\": bert_model\n                }\n            }\n        },\n        \"seq2vec_encoder\": {\n            \"type\": \"bert_pooler\",\n            \"pretrained_model\": bert_model,\n        }\n    },\nListing 9.4\nConfig file for training an NLI model with BERT\n\n\n250\nCHAPTER 9\nTransfer learning with pretrained language models\n    \"data_loader\": {\n        \"batch_sampler\": {\n            \"type\": \"bucket\",\n            \"sorting_keys\": [\"tokens\"],\n            \"padding_noise\": 0.1,\n            \"batch_size\" : 32\n        }\n    },\n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 5.0e-6\n        },\n        \"validation_metric\": \"+accuracy\",\n        \"num_epochs\": 30,\n        \"patience\": 10,\n        \"cuda_device\": 0\n    }\n}\nIt’s OK if you are not familiar with what’s going on in the data_loader and trainer\nsections. We’ll discuss these topics (batching, padding, optimizing, hyperparameter\ntuning) in chapter 10. After saving this config file under examples/nli/snli_\ntransformers.jsonnnet, you can start the training process by running the follow-\ning code:\nallennlp train examples/nli/snli_transformers.jsonnet --serialization-dir \nmodels/snli\nThis will run for a while (even on a fast GPU such as Nvidia V100) and produce a large\namount of log messages on stdout. The following is a snippet of log messages I got\nafter four epochs:\n...\nallennlp.training.trainer - Epoch 4/29\nallennlp.training.trainer - Worker 0 memory usage MB: 6644.208\nallennlp.training.trainer - GPU 0 memory usage MB: 8708\nallennlp.training.trainer - Training\nallennlp.training.trainer - Validating\nallennlp.training.tensorboard_writer -                        Training |  Validation\nallennlp.training.tensorboard_writer - accuracy           |     0.933  |     0.908\nallennlp.training.tensorboard_writer - gpu_0_memory_MB    |  8708.000  |       N/A\nallennlp.training.tensorboard_writer - loss               |     0.190  |     0.293\nallennlp.training.tensorboard_writer - reg_loss           |     0.000  |     0.000\nallennlp.training.tensorboard_writer - worker_0_memory_MB |  6644.208  |       N/A\nallennlp.training.checkpointer - Best validation performance so far. Copying weights \nto 'models/snli/best.th'.\nallennlp.training.trainer - Epoch duration: 0:21:39.687226\nallennlp.training.trainer - Estimated training time remaining: 9:04:56\n...\nPay attention to the validation accuracy (0.908). This looks very good considering that\nthis is a three-class classification and the random baseline would be just 0.3. In\n",
      "page_number": 263
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 271-278)",
      "start_page": 271,
      "end_page": 278,
      "detection_method": "topic_boundary",
      "content": "251\nSummary\ncomparison, when I replaced BERT with an LSTM-based RNN, the best validation\naccuracy I got was around ~0.68. We need to run experiments more carefully to make\na fair comparison between different models, but this result seems to suggest that\nBERT is a powerful model for solving natural language understanding problems.\nSummary\nTransfer learning is a machine learning concept where a model learned for one\ntask is applied to another by transferring knowledge between them. It is an\nunderlying concept for many modern, powerful, pretrained models.\nBERT is a Transformer encoder pretrained with masked language modeling\nand next-sentence prediction objectives to produce contextualized embed-\ndings, a series of word embeddings that take context into account.\nELMo, XLNet, RoBERTa, DistilBERT, and ALBERT are other popular pre-\ntrained models commonly used in modern deep NLP.\nYou can build BERT-based NLP applications by using Hugging Face’s Trans-\nformers library directly, or by using AllenNLP, which integrates the Transform-\ners library seamlessly.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n252\nCHAPTER 9\nTransfer learning with pretrained language models\n \n \n \n \n\n\nPart 3\nPutting into production\nIn parts 1 and 2, we learned a lot about the “modeling” part of the modern\nNLP, including word embeddings, RNNs, CNNs, and the Transformer. However,\nyou still need to learn how to effectively train, serve, deploy, and interpret those\nmodels for building robust and practical NLP applications. \n Chapter 10 touches upon important machine learning techniques and best\npractices when developing NLP applications, including batching and padding,\nregularization, and hyperparameter optimization.\n Finally, if chapters 1 to 10 are about building NLP models, chapter 11 covers\neverything that happens outside NLP models. The chapter covers how to deploy,\nserve, explain, and interpret NLP models.\n \n \n\n\n254\nCHAPTER \n\n\n255\nBest practices in developing\nNLP applications\nWe’ve covered a lot of ground so far, including deep neural network models such as\nRNNs, CNNs, and the Transformer, and modern NLP frameworks such as AllenNLP\nand Hugging Face Transformers. However, we’ve paid little attention to the details\nof training and inference. For example, how do you train and make predictions effi-\nciently? How do you avoid having your model overfit? How do you optimize hyper-\nparameters? These factors could make a huge impact on the final performance and\ngeneralizability of your model. This chapter covers these important topics that you\nThis chapter covers\nMaking neural network inference more efficient \nby sorting, padding, and masking tokens\nApplying character-based and BPE tokenization\nfor splitting text into tokens\nAvoiding overfitting via regularization\nDealing with imbalanced datasets by using \nupsampling, downsampling, and loss weighting\nOptimizing hyperparameters\n\n\n256\nCHAPTER 10\nBest practices in developing NLP applications\nneed to consider to build robust and accurate NLP applications that perform well in\nthe real world.\n10.1\nBatching instances\nIn chapter 2, we briefly mentioned batching, a machine learning technique where\ninstances are grouped together to form batches and sent to the processor (CPU or,\nmore often, GPU). Batching is almost always necessary when training large neural\nnetworks—it is critical for efficient and stable training. In this section, we’ll dive into\nsome more techniques and considerations related to batching.\n10.1.1 Padding\nTraining large neural networks requires a number of linear algebra operations such as\nmatrix addition and multiplication, which involve executing basic mathematical oper-\nations on many, many numbers at once. This is why it requires specialized hardware\nsuch as GPUs, processors designed to execute such operations in a highly parallelized\nmanner. Data is sent to the GPU as tensors, which are just high-dimensional arrays of\nnumbers, along with some instructions as to what types of mathematical operations it\nneeds to execute. The result is sent back as another tensor.\n In chapter 2, we likened GPUs to factories overseas that are highly specialized and\noptimized for manufacturing the same type of products in a large quantity. Because\nthere is considerable overhead in communicating and shipping products, it is more\nefficient if you make a small number of orders for manufacturing a large quantity of\nproducts by shipping all the required materials in batches, rather than shipping mate-\nrials on demand.\n Materials and products are usually shipped back and forth in standardized contain-\ners. If you have ever loaded a moving pod or trailer yourself (or observed someone\nelse do it), you may know that there are many considerations that are important for\nsafe and reliable shipping. You need to put furniture and boxes in tightly so that they\ndon’t shift around in transition. You need to wrap them with blankets and fix them\nwith ropes to prevent them from being damaged. You need to put heavy stuff at the\nbottom so that lighter stuff won’t get crushed, and so on.\n Batches in machine learning are similar to containers for shipping stuff in the real\nworld. Just like shipping containers are all the same size and rectangular, batches in\nmachine learning are just rectangular tensors packed with numbers of the same type.\nIf you want to “ship” multiple instances of different shapes in a single batch to the\nGPU, you need to pack them so that the packed numbers form a rectangular tensor. \n In NLP, we often deal with sequences of text in different lengths. Because batches\nhave to be rectangular, we need to do padding, (i.e., append special tokens, <PAD>, to\neach sequence so that each row of the tensor has the same length. You need as many\npadding tokens as necessary to make the sequences the same length, which means\nthat you need to pad short sequences until they are all as long as the longest sequence\nin the same batch. This is illustrated in figure 10.1.\n\n\n257\nBatching instances\nFigure 10.1\nPadding and batching. Black squares are tokens, gray ones are EOS tokens, and white ones are \npadding.\nIn reality, each token in natural language text is often represented as a vector of\nlength D, generated by the word embeddings method. This means that each batched\ntensor is a three-dimensional tensor that has a “depth” of D. In many NLP models,\nsequences are represented as batches of size N × L × D (see figure 10.2), where N, L, D\nare the number of instances per batch, the maximum length of the sequences, and\nthe dimension of word embeddings, respectively.\nFigure 10.2\nPadding and batching of embedded sequences create rectangular, three-dimensional tensors.\nThis is starting to look more like real containers!\n10.1.2 Sorting\nBecause each batch has to be rectangular, if one batch happens to include both short\nsequences and long sequences, you need to add a lot of padding to short sequences so\nthat they are as long as the longest sequence in the same batch. This often leads to\nPadding\n& batching\n \nBatch 1\nBatch 2\nBatch 3\nPadding \n& batching\nBatch 1\nBatch 2\nBatch 3\nD\nD\nN\nL\n\n\n258\nCHAPTER 10\nBest practices in developing NLP applications\nsome wasted space in the batch—see “batch 1” in figure 10.3 for an illustration. The\nshortest sequence (six tokens) needs to be padded with eight more tokens to be\nequally long as the longest sequence (14 tokens). Wasted space in a tensor means\nwasted memory and computation, so it is best avoided, but how?\nFigure 10.3\nSorting instances before batching (right) reduces the total number of tensors.\nYou can reduce the amount of padding by putting instances of similar size in the same\nbatch. If shorter instances are batched only with other equally shorter ones, they don’t\nneed to get padded with many padding tokens. Similarly, if longer instances are\nbatched only with other longer ones, they don’t need a lot of padding either, because\nthey are already long. One idea is to sort instances by their length and batch accord-\ningly. Figure 10.3 compares two situations—one in which the instances are batched in\ntheir original order, and the other where instances are first sorted before batching.\nThe numbers below each batch indicate how many tokens are required to represent\nthe batch, including the padding tokens. Notice that the number of total tokens is\nreduced from 144 to 120 by sorting. Because the number of tokens in the original sen-\ntences doesn’t change, this is purely because sorting reduced the number of padding\nBatch 1 \n(4 × 14 = 56)\nBatch 2 \n(4 × 11 = 44)\nBatch 3 \n(4 × 11 = 44)\nBatch 1 \n(4 × 7 = 28)\nBatch 2 \n(4 × 9 = 36)\nBatch 3 \n(4 × 14 = 56)\nPad & batch\nPad & batch\nSort\nTotal \n(56 + 44 + 44 = 144)\nTotal \n(28 + 36 + 56 = 120)\n",
      "page_number": 271
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 279-286)",
      "start_page": 279,
      "end_page": 286,
      "detection_method": "topic_boundary",
      "content": "259\nBatching instances\ntokens. Smaller batches require less memory to store and less computation to process,\nso sorting instances before batching improves the efficiency of training.\n All these techniques sound somewhat complicated, but the good news is, you\nrarely need to write code for sorting, padding, and batching instances yourself as long\nas you use high-level frameworks such as AllenNLP. Recall that we used a combination\nof DataLoader and BucketBatchSampler for building our sentiment analysis\nmodel back in chapter 2 as follows:\ntrain_data_loader = DataLoader(train_dataset,\n                               batch_sampler=BucketBatchSampler(\n                                   train_dataset,\n                                   batch_size=32,\n                                   sorting_keys=[\"tokens\"]))\nThe sorting_keys given to BucketBatchSampler specifies which field to use for\nsorting. As you can guess from its name, by specifying “tokens” you are telling the data\nloader to sort the instances by the number of tokens (which is what you want in most\ncases). The pipeline will take care of padding and batching automatically, and the\ndata loader will give you a series of batches you can feed into your model.\n10.1.3 Masking\nOne final detail that you need\nto pay attention to is masking.\nMasking is an operation where\nyou ignore some part of the net-\nwork that corresponds to pad-\nding. This becomes relevant\nespecially when you are dealing\nwith a sequential-labeling or a\nlanguage-generation model. To\nrecap, sequential labeling is a\ntask where the system assigns a\nlabel per token in the input\nsequence. We built a POS tagger\nwith a sequential labeling model\n(RNN) in chapter 5. \n As shown in figure 10.4,\nsequential-labeling models are\ntrained by minimizing the per-\ntoken loss aggregated across all\ntokens in a given sentence. We\ndo this because we’d like to min-\nimize the number of “errors”\nthe network makes per token.\nThis is fine as long as we are\ndealing \nwith \n“real” \ntokens\nstate\nstate\nupdate\nupdate\nupdate\nstate\n \nLinear\nlayer\nLinear\nlayer\n \n \n \nRNN\nTotal \nloss\nlabel\n \nlabel\nlabel\nlogits\n+\n+\n=\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nCross\nentropy\nCross\nentropy\nCross\nentropy\nLinear\nlayer\nFigure 10.4\nLoss for a sequence is the sum of per-\ntoken cross entropy.\n\n\n260\nCHAPTER 10\nBest practices in developing NLP applications\n(“time,” “flies,” and “like” in the figure), although it becomes an issue when the input\nbatch includes padded tokens. Because they exist just to pad the batch, they should be\nignored when computing the total loss.\n We usually do this by creating an extra vector for masking the loss. The vector for\nmasking has the same length as the input, whose elements are 1s for “real” tokens and\n0s for padding. When computing the total loss, you can simply take an element-wise\nproduct between the per-token loss and the mask and then sum up the result.\n Fortunately, as long as you are building standard sequential-labeling models with\nAllenNLP, you rarely need to implement masking yourself. Remember, in chapter 5,\nwe wrote the forward pass of the POS tagger model as shown in listing 10.1. Here, we\nget the mask vector from the get_text_field_mask() helper function and com-\npute the final loss with sequence_cross_entropy_with_logits().\n    def forward(self,\n                words: Dict[str, torch.Tensor],\n                pos_tags: torch.Tensor = None,\n                **args) -> Dict[str, torch.Tensor]:\n        mask = get_text_field_mask(words)\n        embeddings = self.embedder(words)\n        encoder_out = self.encoder(embeddings, mask)\n        tag_logits = self.linear(encoder_out)\n        output = {\"tag_logits\": tag_logits}\n        if pos_tags is not None:\n            self.accuracy(tag_logits, pos_tags, mask)\n            output[\"loss\"] = sequence_cross_entropy_with_logits(\n                tag_logits, pos_tags, mask)\n        return output\nIf you take a peek at what’s inside mask (e.g., by inserting a print statement in this\nforward method), you’ll see the following tensor made of binary (True or False)\nvalues:\ntensor([[ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\nListing 10.1\nForward pass of the POS tagger\n\n\n261\nTokenization for neural models\nEach row of this tensor corresponds to one sequence of tokens, and locations with\nFalse are where padding occurred. The loss function (sequence_cross_entropy\n_with_logits) receives the prediction, the ground truth (the correct labels), and\nthe mask and computes the final loss while ignoring all the elements marked as\nFalse.\n10.2\nTokenization for neural models\nIn chapter 3, we covered the basic linguistic units (words, characters, and n-grams)\nand how to compute their embeddings. In this section, we will go deeper and focus on\nhow to analyze texts and obtain these units—a process called tokenization. Neural net-\nwork models pose a set of unique challenges on how to deal with tokens, and we’ll\ncover some of the modern models to deal with these challenges.\n10.2.1 Unknown words\nA vocabulary is a set of tokens that an NLP model deals with. Many neural NLP models\noperate within a fixed, finite set of tokens. For example, when we built a sentiment ana-\nlyzer in chapter 2, the AllenNLP pipeline first tokenized the training dataset and con-\nstructed a Vocabulary object that consists of all unique tokens that appeared more\nthan, say, three times. The model then uses an embedding layer to convert tokens into\nword embeddings, which are some abstract representation of the input tokens.\n So far, so good, right? But the number of all words in the world is not finite. We\nconstantly make up new words that didn’t exist before (I don’t think people talked\nabout “NLP” a hundred years ago). What if the model receives a word that it has never\nseen during training? Because the word is not part of the vocabulary, the model can-\nnot even convert it to an index, let alone look up its embeddings. Such words are\ncalled out-of-vocabulary (OOV) words, and they are one of the biggest problems when\nbuilding NLP applications. \n By far the most common (but not the best) way to deal with this problem is to rep-\nresent all the OOV tokens as a special token, which is conventionally named UNK (for\n“unknown”). The idea is that every time the model sees a token that is not part of the\nvocabulary, it pretends it saw a special token UNK instead and proceeds as usual. This\nmeans that the vocabulary and the embedding table both have a designated “slot” for\nUNK so that the model can deal with words that it has never seen. The embeddings (and\nany other parameters) for UNK are trained in the same manner as other regular tokens.\n Do you see any problems with this approach? Treating all OOV tokens with a single\nUNK token means that they are collapsed into a single embedding vector. It doesn’t\nmatter if the word is “NLP” or “doggy”—as long as it’s something unseen, it always gets\ntreated as a UNK token and assigned the same vector, which becomes a generic, catch-\nall representation of various words. Because of this, the model cannot tell the differ-\nences among OOV words, no matter what the identity of the words is.\n This may be fine if you are building, for example, a sentiment analyzer. OOV\nwords are, by definition, very rare and might not affect the prediction of most of the\n\n\n262\nCHAPTER 10\nBest practices in developing NLP applications\ninput sentences. However, this becomes a huge problem if you are building a machine\ntranslation system or a conversational engine. It wouldn’t be a usable MT system or a\nchatbot if it produces “I don’t know” every time it sees new words! In general, the\nOOV problem is more serious for language-generation systems (including machine\ntranslation and conversational AI) compared to NLP systems for prediction (senti-\nment analysis, POS tagging, and so on).\n How can we do better? OOV tokens are such a big problem in NLP that there has\nbeen a lot of research work on how to deal with them. In the following subsections,\nwe’ll cover character-based and subword-based models, two techniques commonly\nused for building robust neural NLP models.\n10.2.2 Character models\nThe simplest yet effective solution for dealing with the OOV problem is to treat char-\nacters as tokens. Specifically, we break the input text into individual characters, even\nincluding punctuation and whitespace, and treat them as if they are regular tokens.\nThe rest of the application is unchanged—“word” embeddings are assigned to charac-\nters, which are further processed by the model. If the model produces text, it does so\ncharacter-by-character.\n In fact, we used a character-level model in chapter 5 when we built a language gen-\nerator. Instead of generating text word-by-word, the RNN produces text one character\nat a time, as illustrated in figure 10.5. Thanks to this strategy, the model was able to\nproduce words that look like English but actually aren’t. Notice a number of peculiar\nwords (despoit, studented, redusention, distaples) that resemble English words in the out-\nput shown in listing 10.2. If the model operated on words, it produces only known\nwords (or UNKs when unsure), and this wouldn’t have been possible. \nFigure 10.5\nA language-generation model that generates text character-by-\ncharacter (including whitespace)\n<START>\nT\nT\nCharacter\nembeddings\n \nh\ng\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\nh\ne\n.\n<END>\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nT h e _ q u i c k _ b r … _ d o g .\n\n\n263\nTokenization for neural models\nYou can say that you don't know it, and why decided of yourself.\nPike of your value is to talk of hubies.\nThe meeting despoit from a police?\nThat's a problem, but us?\nThe sky as going to send nire into better.\nWe'll be look of the best ever studented.\nThere's you seen anything every's redusention day.\nHow a fail is to go there.\nIt sad not distaples with money.\nWhat you see him go as famous to eat!\nCharacter-based models are versatile and put few assumptions on the structure of the\nlanguage. For languages with small sets of alphabets (like English), it effectively eradi-\ncates unknown words, because almost any words, no matter how rare they are, can be\nbroken down into characters. Tokenizing into characters is also an effective strategy\nfor languages with large alphabets (like Chinese), although you need to watch out for\n“unknown character” problems.\n However, this strategy is not without drawbacks. The biggest issue is its inefficiency.\nTo encode a sentence, the network (be it an RNN or the Transformer) needs to go\nover all the characters in it. For example, a character-based model needs to process\n“t,” “h,” “e,” and “_” (whitespace) to process a single word “the,” whereas a word-based\nmodel can finish this in a single step. This inefficiency takes its biggest toll on the\nTransformers, where the attention computation increases quadratically when the\ninput sequence gets longer.\n10.2.3 Subword models\nSo far, we studied two extremes—the word-based approach is efficient but not great at\ndealing with unknown words. The character-based approach is great at dealing with\nunknown words but is inefficient. Is there something in between? Can we use some\ntokenization that is both efficient and robust to unknown words?\n Subword models are a recent invention that addresses this problem for neural net-\nworks. In subword models, the input text is segmented into a unit called subwords,\nwhich simply means something smaller than words. There is no formal linguistic defi-\nnition as to what subwords actually are, but they roughly correspond to part of words\nthat appear frequently. For example, one way to segment “dishwasher” is “dish + wash\n+ er,” although some other segmentation is possible.\n Some varieties of algorithms (such as WordPiece1 and SentencePiece2) tokenize\ninput into subwords, but by far the most widely used is byte-pair encoding (BPE).3 BPE was\nListing 10.2\nGenerated sentences by a character-based language model\n1 Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine\nTranslation,” (2016). https://arxiv.org/abs/1609.08144.\n2 Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Can-\ndidates,” (2018). https://arxiv.org/abs/1804.10959.\n3 Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units,” (2016). https://arxiv.org/\nabs/1508.07909.\n\n\n264\nCHAPTER 10\nBest practices in developing NLP applications\noriginally invented as a compression algorithm,4 but since 2016, it’s been widely used as\na tokenization method for neural models, particularly in machine translation.\n The basic concept of BPE is to keep frequent words (such as “the” and “you”) and\nn-grams (such as “-able” and “anti-”) unsegmented, while breaking up rarer words\n(such as “dishwasher”) into subwords (“dish + wash + er”). Keeping frequent words\nand n-grams together helps the model process those tokens efficiently, whereas break-\ning up rare words ensures there are no UNK tokens, because everything can be ulti-\nmately broken up into individual characters, if necessary. By flexibly choosing where\nto tokenize based on the frequency, BPE achieves the best of two worlds—being effi-\ncient while addressing the unknown word problem.\n Let’s see how BPE determines where to tokenize with real examples. BPE is a\npurely statistical algorithm (it doesn’t use any language-dependent information) and\noperates by merging the most frequently occurring pair of consecutive tokens, one at\na time. First, BPE tokenizes all the input texts into individual characters. For example,\nif your input is four words low, lowest, newer, and wider, it will tokenize them into\nl o w _, l o w e s t _, n e w e r _, w i d e r _. Here, “_” is a special sym-\nbol that indicates the end of each word. Then, the algorithm identifies any two con-\nsecutive elements that appear most often. In this example, the pair l o appears most\noften (two times), so these two characters are merged, yielding lo w _, lo w e s t\n_, n e w e r _, w i d e r _. Then, lo w will be merged into low, e r into er, er\n_ into er_, at which time you have low _, low e s t _, n e w er_, w i d er_.\nThis process is illustrated in figure 10.6.\nFigure 10.6\nBPE learns subword units by iteratively merging consecutive units that cooccur frequently.\n4 See https://www.derczynski.com/papers/archive/BPE_Gage.pdf.\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\n\n\n265\nAvoiding overfitting\nNotice that, after four merge operations, lowest is segmented into low e s t where\nfrequent substrings such as low are merged together whereas infrequent ones such as\nest are broken apart. To segment a new input (e.g., lower), the same sequence of\nmerge operations is applied in order, yielding low e r _. If you start from 52 unique\nletters (26 upper- and lowercase letters), you will end up with 52 + N unique tokens in\nyour vocabulary, where N is the number of merge operations executed. In this way,\nyou have complete control over the size of the vocabulary.\n In practice, you rarely need to implement BPE (or any other subword tokenization\nalgorithms) yourself. These algorithms are implemented in many open source libraries\nand platforms. Two popular options are Subword-NMT (https://github.com/rsenn-\nrich/subword-nmt) and SentencePiece (https://github.com/google/sentencepiece)\n(which also supports a variant of subword tokenization using a unigram language\nmodel). Many of the default tokenizers shipped with NLP frameworks, such as the one\nimplemented in Hugging Face Transformers, support subword tokenization.\n10.3\nAvoiding overfitting\nOverfitting is one of the most common and important issues you need to address when\nbuilding any machine learning applications. An ML model is said to overfit when it\nfits the given data so well that it loses its generalization ability to unseen data. In other\nwords, the model may capture the training data very well and show good performance\non it, but it may not be able to capture its inherent patterns well and shows poor per-\nformance on data that the model has never seen before. \n Because overfitting is so prevalent in machine learning, researchers and practi-\ntioners have come up with a number of algorithms and techniques to combat overfit-\nting in the past. In this section, we’ll learn two such techniques—regularization and\nearly stopping. These are popular in any ML applications (not just NLP) and worth\ngetting under your belt.\n10.3.1 Regularization\nRegularization in machine learning refers to techniques that encourage the simplicity\nand the generalization of the model. You can think of it as one form of penalty you\nimpose on your ML model to ensure that it is as generic\nas possible. What does it mean? Say you are building an\n“animal classifier” by training word embeddings from a\ncorpus and by drawing a line between animals and\nother stuff in this embedding space (i.e., you represent\neach word as a multidimensional vector and classify\nwhether the word describes an animal based on the\ncoordinates of the vector). Let’s simplify this problem a\nlot and assume that each word is a two-dimensional vec-\ntor, and you end up with the plot shown in figure 10.7.\nYou can now visualize how a machine learning model\nmakes a classification decision by drawing lines where\nbat\nhot\ndog\ncat\nchocolate\npizza\nFigure 10.7\nAnimal vs. non-\nanimal classification plot\n\n\n266\nCHAPTER 10\nBest practices in developing NLP applications\nthe decision flips between different classes (animals and non-animals), which is called\nthe classification boundary. How would you draw a classification boundary so that ani-\nmals (blue circles) are separated from everything else (triangles)?\n One simple way to separate animals is to draw one straight line, as in the first plot\nin figure 10.8. This simple classifier makes several mistakes (in classifying words like\n“hot” and “bat”), but it correctly classifies the majority of data points. This sounds like\na good start.\nFigure 10.8\nClassification boundaries with increasing complexity\nWhat if you are told that the decision boundary doesn’t have to be a straight line? You\nmay want to draw something like the one shown in the middle in figure 10.8. This one\nlooks better—it makes fewer mistakes than the first one, although it is still not perfect.\nIt appears tractable for a machine learning model because the shape is simple. \n But there’s nothing that stops you here. If you want to make as few errors as possi-\nble, you can also draw something wiggly like the one shown in the third plot. That\ndecision boundary doesn’t even make any classification errors, which means that we\nachieved 100% classification accuracy!\n Not so fast—remember that up until here, we’ve been thinking only about the\ntraining time, but the main purpose of machine learning models is to achieve good\nclassification performance at the test time (i.e., they need to classify unobserved, new\ninstances as correctly as possible). Now let’s think about how the three decision bound-\naries described earlier fare at test time. If we assume the test instances are distributed\nsimilarly to the training instances we saw in figure 10.8, the new “animal” points are\nmost likely to fall in the upper-right region of the plot. The first two decision boundar-\nies will achieve decent accuracy by classifying the majority of new instances correctly.\nBut how about the third one? Training instances such as “hot” shown in the plot are\nmost likely exceptions rather than the norm, so the curved sections of the decision\nboundary that tried to accommodate as many training instances as possible may do\nmore harm than good at the test time by inadvertently misclassifying test instances.\nThis is exactly what overfitting looks like—the model fits the training data so well that\nit sacrifices its generalization ability, which is what’s happening here.\nbat\nhot\ndog\ncat\nchocolate\npizza\nbat\nhot\ndog\ncat\nchocolate\npizza\nbat\nhot\ndog\ncat\nchocolate\npizza\nUnderfitting\nJust right\nOverfitting\n",
      "page_number": 279
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 287-294)",
      "start_page": 287,
      "end_page": 294,
      "detection_method": "topic_boundary",
      "content": "267\nAvoiding overfitting\n Then, the question is, how can we avoid having your model look like the third\ndecision boundary? After all, it is doing a very good job correctly classifying the train-\ning data. If you looked only at the training accuracy and/or the loss, there would be\nnothing to stop you from choosing it. One way to avoid overfitting is to use a separate,\nheld-out dataset (called a validation set; see section 2.2.3) to validate the performance\nof your model. But can we do this even without using a separate dataset?\n The third decision boundary just doesn’t look right—it’s overly complex. With all\nother things being equal, we should prefer simpler models, because in general, sim-\npler models generalize better. This is also in line with Occam’s razor, which states that\na simpler solution is preferable to a more complex one. How can we balance between\nthe training fit and the simplicity of the model?\n This is where regularization comes into play. Think of regularization as additional\nconstraints imposed on the model so that simpler and/or more general models are\npreferred. The model is optimized so that it achieves the best training fit while being\nas generic as possible.\n Numerous regularization techniques have been proposed in machine learning\nbecause overfitting is such an important topic. We are going to introduce only a few\nof the most important ones—L2 regularization (weight decay), dropout, and early\nstopping.\nL2 REGULARIZATION\nL2 regularization, also called weight decay, is one of the most common regularization\nmethods not just for NLP or deep learning but for a wide range of ML models. We are\nnot going into its mathematical details, but in short, L2 regularization adds a penalty\nfor the complexity of a model measured by how large its parameters are. To represent\na complex classification boundary, an ML model needs to adjust a large number of\nparameters (the “magic constants”) to extreme values, measured by the L2 loss, which\ncaptures how far away they are from zero. Such models incur a larger L2 penalty, which\nis why L2 encourages simpler models. If you are interested in learning more about L2\nregularization (and other related topics about NLP in general), check out textbooks\nsuch as Speech and Language Processing by Jurafsky and Martin (https://web.stanford\n.edu/~jurafsky/slp3/5.pdf) or Goodfellow et al.’s Deep Learning (https://www.deep\nlearningbook.org/contents/regularization.html).\nDROPOUT\nDropout is another popular regularization technique commonly used with neural net-\nworks. Dropout works by randomly “dropping” neurons during training, where a “neu-\nron” is basically a dimension of an intermediate layer and “dropping” means to mask it\nwith zeros. You can think of dropout as a penalty to the model’s structural complexity\nand its reliance on particular features and values. As a result, the network tries to make\nthe best guess with the remaining smaller number of values, which forces it to general-\nize well. Dropout is easy to implement and effective in practice and is used as a default\nregularization method in many deep learning models. For more information on drop-\nout, the regularization chapter of the Goodfellow book mentioned earlier provides a\ngood introduction and mathematical details of regularization techniques.\n\n\n268\nCHAPTER 10\nBest practices in developing NLP applications\n10.3.2 Early stopping\nAnother popular approach for combatting overfitting in machine learning is early stop-\nping. Early stopping is a relatively simple technique where you stop training your\nmodel when the model performance stops improving, usually measured by the valida-\ntion set loss. In chapter 6, we plotted learning curves when we built an English-\nSpanish machine translation model (shown again in figure 10.9). Notice that the vali-\ndation loss curve flattens out around the eighth epoch and starts to creep up after\nthat, which is a sign of overfitting. Early stopping would detect this, stop the training,\nand use the result from the best epoch when the loss is lowest. In general, early stop-\nping has a “patience” parameter, which is the number of nonimproving epochs for\nearly stopping to kick in. When patience is 10 epochs, for example, the training pipe-\nline will wait 10 epochs after the loss stops improving to stop the training.\nFigure 10.9\nThe validation loss curve flattens out around the eighth epoch and creeps back up.\nWhy does early stopping help mitigate overfitting? What does it have to do with model\ncomplexity? Without getting into mathematical details, it takes some time (training\nepochs) for the model to learn complex, overfitted decision boundaries. Most models\nstart from something simple (e.g., straight decision lines) and gradually increase their\ncomplexity over the course of training. By stopping the training early, early stopping\ncan prevent the model from becoming overly complex.\n Many machine learning frameworks have built-in support for early stopping. For\nexample, AllenNLP’s trainer supports early stopping by default. Recall that we used\nthe following configuration in section 9.5.3 when we trained a BERT-based natural\n0\n2\n4\n6\n0\n10\n20\n30\nepoch\nloss\nstage\ntrain\nvalid\n\n\n269\nAvoiding overfitting\nlanguage inference model, where we used early stopping (with patience = 10) without\npaying much attention. This allows the trainer to stop if the validation metric doesn’t\nimprove for 10 epochs: \n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 1.0e-5\n        },\n        \"num_epochs\": 20,\n        \"patience\": 10,\n        \"cuda_device\": 0\n    }\n10.3.3 Cross-validation\nCross-validation is not exactly a regularization method, but it is one of the techniques\ncommonly used in machine learning. A common situation in building and validating\na machine learning model is this—you have only a couple of hundred instances avail-\nable for training. As we’ve seen so far in this book, you can’t train a reliable ML model\njust on the training set—you need a separate set for validation, and preferably another\nseparate set for testing. How much you use for validation/testing depends on the task\nand the data size, but in general, it is advised that you set aside 5–20% of your training\ninstances for validation and testing. This means that if your training data is small, your\nmodel is validated and tested on just a few dozen instances, which can make the esti-\nmated metrics unstable. Also, how you choose these instances has a large impact on\nthe evaluation metrics, which is not ideal.\n The basic idea of cross-validation is to iterate this phase (splitting the dataset into\ntraining and validation portions) multiple times with different splits to improve the\nstability of the result. Specifically, in a typical setting called k-fold cross validation, you\nfirst split the dataset into k different portions of equal size called folds. You use one of\nthe folds for validation while training the model on the rest (k – 1 folds), and repeat\nthis process k times, using a different fold for validation every time. See figure 10.10\nfor an illustration.\nFigure 10.10\nIn k-fold cross validation, the dataset is split into k equally sized \nfolds and one is used for validation.\nFold 1\nFold 2\nFold 3\nFold 4\nFold 5\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\nValidation\nValidation\nValidation\nValidation\nValidation\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\n\n\n270\nCHAPTER 10\nBest practices in developing NLP applications\nThe validation metrics are computed for every fold, and the final metrics are averaged\nover all iterations. This way, you can obtain a more stable estimate of the evaluation\nmetrics that are not impacted heavily by the way the dataset is split. \n The use of cross-validation in deep learning models is not common, because these\nmodels require a large amount of data, and you don’t need cross-validation if you\nhave a large dataset, although its use is more common for more traditional and indus-\ntrial settings where the amount of training data is limited.\n10.4\nDealing with imbalanced datasets\nIn this section, we’ll focus on one of the most common problems you may encounter in\nbuilding NLP and ML models—the class imbalance problem. The goal of a classifica-\ntion task is to assign one of the classes (e.g., spam or nonspam) to each instance (e.g.,\nan email), but these classes are rarely distributed evenly. For example, in spam filtering,\nthe number of nonspam emails is usually larger than the number of spam emails. In\ndocument classification, some topics (such as poli-\ntics or sports) are usually more popular than other\ntopics. Classes are said to be imbalanced when some\nclasses have way more instances than others (see fig-\nure 10.11 for an example). \n   Many classification datasets have imbalanced\nclasses, which poses some additional challenges\nwhen you train your classifier. The signals your\nmodel gets from smaller classes are overwhelmed by\nlarger classes, which causes your model to perform\npoorly on minority classes. In the following subsec-\ntions, I’m going to discuss some techniques you can\nconsider when faced with an imbalanced dataset.\n10.4.1 Using appropriate evaluation metrics\nBefore you even begin tweaking your dataset or your model, make sure you are vali-\ndating your model with an appropriate metric. In section 4.3, we discussed why it is a\nbad idea to use accuracy as your evaluation metric when the dataset is imbalanced. In\none extreme case, if 90% of your instances belong to class A and the other 10%\nbelong to class B, even a stupid classifier that assigns class A to everything can achieve\n90% accuracy. This is called a majority class baseline. A slightly more clever (but still stu-\npid) classifier that randomly assigns label A 90% of the time and label B 10% of the\ntime without even looking at the instance will achieve 0.9 * 0.9 + 0.1 * 0.1 = 82% accu-\nracy. This is called a random baseline, and the more imbalanced your dataset is, the\nhigher the accuracy of these baseline models will become.\n But this kind of random baseline is rarely a good model for minority classes. Imag-\nine what would happen to class B if you used the random baseline. Because it will\nassign class A 90% of the time no matter what, 90% of the instances belonging to class\nB will be assigned class A. In other words, the accuracy of this random baseline for\nClass A\nClass B\nInstances\nFigure 10.11\nImbalanced \ndataset\n\n\n271\nDealing with imbalanced datasets\nclass B is only 10%. If this was a spam filter, it would let 90% of spam emails go\nthrough, no matter what the content is, just because 90% of emails you receive are not\nspam! This would make a terrible spam filter.\n If your dataset is imbalanced and you care about the classification performance on\nthe minority class, you should consider using metrics that are more appropriate for\nsuch settings. For example, if your task is a “needle in a haystack” type of setting, where\nthe goal is to find a very small number of instances among others, you may want to use\nthe F1-measure instead of accuracy. As we saw in chapter 4, the F-measure is some sort\nof average between precision (how hay-free your prediction is) and recall (how much\nof the needle you actually found). Because the F1-measure is calculated per class,\nit does not underrepresent minority classes. If you’d like to measure the model’s over-\nall performance including majority classes, you can compute the macro-averaged\nF-measure, which is simply an arithmetic average of F-measures computed per class.\n10.4.2 Upsampling and downsampling\nNow let’s look at concrete techniques that can mitigate the class imbalance problem.\nFirst of all, if you can collect more labeled training data, you should seriously consider\ndoing that first. Unlike academic and ML competition settings where the dataset is\nfixed while you tweak your model, in a real-world setting you are free to do whatever is\nnecessary to improve your model (of course, as long as it’s lawful and practical).\nOften, the best thing you can do to improve a model’s generalization is expose it to\nmore data.\n If your dataset is imbalanced and the model is making biased predictions, you\ncan either upsample or downsample your data so that classes have roughly equal\nrepresentations. \n In upsampling (see the second figure in figure 10.12), you artificially increase the\nsize of the minority class by copying the instances multiple times. Take the scenario we\ndiscussed earlier for example—if you duplicate the instances of class B and add eight\nextra copies of each instance to the dataset, they have an equal number of instances.\nClass A\nClass B\nInstances\nClass A\nClass B\nInstances\nClass A\nClass B\nInstances\nUpsampling\nDownsampling\nFigure 10.12\nUpsampling and downsampling\n\n\n272\nCHAPTER 10\nBest practices in developing NLP applications\nThis can mitigate the biased prediction issue. More sophisticated data augmentation\nalgorithms such as SMOTE5 are available, although they are not widely used in NLP,\ndue to the inherent difficulty in generating linguistic examples artificially.\n If your model is biased not because the minority class is too small but because the\nmajority class is too large, you can instead choose to downsample (the third figure in\nfigure 10.12). In downsampling, you artificially decrease the size of the majority class by\nchoosing a subset of the instances belonging to that class. For example, if you sample\none out of nine instances from class A, you’ll end up with the equal number of instances\nin classes A and B. You can downsample in multiple ways—the easiest is to randomly\nchoose the subset. If you would like to make sure that the downsampled dataset still pre-\nserves the diversity in the original data, you can try stratified sampling, where you sample\nsome number of instances per group defined by some attributes. For example, if you\nhave too many nonspam emails and want to downsample, you can group them by the\nsender’s domain first, then sample a fixed number of emails per domain. This will\nensure that your sampled dataset will contain a diverse set of domains.\n Note that neither upsampling nor downsampling is a magic bullet. If you “correct”\nthe distribution of classes too aggressively, you risk making unfair predictions for the\nmajority class, if that’s what you care about. Always make sure to check your model\nwith a held-out validation set with appropriate evaluation metrics. \n10.4.3 Weighting losses\nAnother approach for mitigating the class imbalance problem is to use weighting\nwhen computing the loss, instead of making modification to your training data.\nRemember that the loss function is used to measure how “off” the model’s prediction\nfor an instance is compared against the ground truth. When you measure how bad the\nmodel’s prediction is, you can tweak the loss so that it penalizes more when the\nground truth belongs to the minority class.\n    Let’s take a look at a concrete example. The\nbinary cross-entropy loss, a common loss func-\ntion used for training a binary classifier, looks\nlike the curve shown in figure 10.13, when the\ncorrect label is 1. The x-axis is the predicted\nprobability of the target class, and the y-axis is the\namount of loss the prediction will incur. When\nthe prediction is perfectly correct (probability\n= 1), there’s no penalty, whereas as the predic-\ntion gets worse (probability < 1), the loss goes up.\n   If you care more about the model’s perfor-\nmance on the minority class, you can tweak this\nloss. Specifically, you can change the shape of\nthis loss (by simply multiplying it by a constant\n5 Chawla et al., “SMOTE: Synthetic Minority Over-Sampling Technique,” (2002). https://arxiv.org/abs/\n1106.1813.\nPrediction\n0\nLoss\n1\nFigure 10.13\nBinary cross-entropy loss \n(when the correct label is 1)\n\n\n273\nHyperparameter tuning\nnumber) just for that class so that the model\nincurs a larger loss when it makes mistakes on the\nminority class. One such tweaked loss curve is\nshown in the figure 10.14 as the top curve. This\nweighting has the same effect as upsampling the\nminority class, although modifying the loss is\ncomputationally cheaper because you don’t need\nto actually increase the amount of training data.\n It is easy to implement loss weighting in\nPyTorch and AllenNLP. PyTorch’s binary cross-\nentropy implementation BCEWithLogitLoss\nalready supports different weights for different\nclasses. You simply need to pass the weight as the\npos_weight parameter as follows:\n>>> import torch\n>>> import torch.nn as nn\n>>> input = torch.randn(3)\n>>> input\ntensor([-0.5565,  1.5350, -1.3066])\n>>> target = torch.empty(3).random_(2)\n>>> target\ntensor([0., 0., 1.])\n>>> loss = nn.BCEWithLogitsLoss(reduction='none')\n>>> loss(input, target)\ntensor([0.4531, 1.7302, 1.5462])\n>>> loss = nn.BCEWithLogitsLoss(reduction='none', \npos_weight=torch.tensor(2.))\n>>> loss(input, target)\ntensor([0.4531, 1.7302, 3.0923])\nIn this code snippet, we randomly generate prediction (input) and the ground truth\n(target). There are three instances in total, two of which are of class 0 (majority)\nand one belongs to class 1 (minority). We first compute the loss without weighting by\ncalling the BCEWithLogitsLoss object, which returns the three loss values, one for\neach instance. We then compute the loss with weighting by passing the weight 2—this\nmeans that the wrong prediction will be penalized twice as much if the target class is\npositive (class 1). Notice that the third element corresponding to class 1 is twice as\nlarge as the one returned by the unweighted loss function. \n10.5\nHyperparameter tuning\nIn this final section of this chapter, we’ll discuss hyperparameter tuning. Hyperparame-\nters are parameters about the model and the training algorithm. This term is used in\ncontrast with parameters, which are numbers that are used by the model to make\nPrediction\n0\nLoss\n1\nFigure 10.14\nWeighted binary cross \nentropy loss\n\n\n274\nCHAPTER 10\nBest practices in developing NLP applications\npredictions from the input. This is what we’ve been calling “magic constants” through-\nout this book—they work like constants in programming languages, although their\nexact values are automatically adjusted by optimization so that the prediction matches\nthe desired output as closely as possible.\n Correctly tuning hyperparameters is critical for many machine learning models to\nwork properly and achieve their highest potential, and ML practitioners spend a lot of\ntime tuning hyperparameters. Knowing how to tune hyperparameters effectively has a\nhuge impact on your productivity in building NLP and ML systems.\n10.5.1 Examples of hyperparameters\nHyperparameters are “meta”-level parameters—unlike model parameters, they are\nused not to make predictions but for controlling the structure of the model and how\nthe model is trained. For example, if you are working on word embeddings or an\nRNN, how many hidden units (dimensions) to use for representing words is one\nimportant hyperparameter. The number of RNN layers to use is another hyperparam-\neter. In addition to these two hyperparameters (the number of hidden units and lay-\ners), the Transformer model we covered in chapter 9 has a number of other\nparameters, such as the number of attention heads and the dimension of the feed-\nforward network. Even the type of architecture you use, such as RNN versus Trans-\nformer, can be thought of as one hyperparameter.\n Besides, the optimization algorithm you use may have hyperparameters, too. For\nexample, the learning rate (section 9.3.3), one of the most important hyperparame-\nters in many ML settings, determines how much to tweak the model parameters per\noptimization step. The number of epochs (iterations through the training dataset) is\nalso an important hyperparameter, too.\n So far, we have been paying little attention to those hyperparameters, let alone\noptimizing them. However, hyperparameters can have a huge impact on the perfor-\nmance of machine learning models. In fact, many ML models have a “sweet spot” of\nhyperparameters that makes them most effective, whereas using a set of hyperparame-\nters outside of this spot may make the model perform poorly. \n Many ML practitioners tune hyperparameters by hand. This means that you start\nfrom a set of hyperparameters that look reasonable and measure the model’s perfor-\nmance on a validation set. Then you change one or more of the hyperparameters\nslightly and measure the performance again. You repeat this process several times\nuntil you hit the “plateau,” where any change of hyperparameters provides only a mar-\nginal improvement.\n One issue with this manual tuning approach is that it is slow and arbitrary. Let’s say\nyou start from one set of hyperparameters. How do you know which ones to adjust\nnext, and how much? How do you know when to stop? If you have experience tuning\na wide range of ML models, you might have some “hunch” about how these models\nrespond to certain hyperparameter changes, but if not, it’s like shooting in the dark.\nHyperparameter tuning is such an important topic that ML researchers have been\nworking on better and more organized ways to optimize them.\n",
      "page_number": 287
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 295-302)",
      "start_page": 295,
      "end_page": 302,
      "detection_method": "topic_boundary",
      "content": "275\nHyperparameter tuning\n10.5.2 Grid search vs. random search\nWe understand that manual optimization of hyperparameters is inefficient, but how\nshould we go about optimizing them, then? We have two more-organized ways of tun-\ning hyperparameters—grid search and random search. \n In grid search, you simply try every possible combination of the hyperparameter val-\nues you want to optimize. For example, let’s assume your model has just two hyperpa-\nrameters—the number of RNN layers and the embedding dimension. You first define\nreasonable ranges for these two hyperparameters, for example, [1, 2, 3] for the num-\nber of layers and [128, 256, 512] for the dimensionality. Then grid search measures\nthe model’s validation performance for every combination—(1, 128), (1, 256),\n(1, 512), (2, 128), . . . , (3, 512)—and simply picks the best-performing combination.\nIf you plot these combinations on a 2-D plot, it looks like a grid (see the illustration in\nfigure 10.15), which is why this is called grid search. \n Grid search is a simple and intuitive way to optimize the hyperparameters. How-\never, if you have many hyperparameters and/or their ranges are large, this method\ngets out of hand. The number of possible combinations is exponential, which makes it\nimpossible to explore all of them in a reasonable amount of time.\nFigure 10.15\nGrid search vs. random search for hyperparameter tuning. \n(Adapted from Bergstra and Bengio, 2012; https://www.jmlr.org/papers/\nvolume13/bergstra12a/bergstra12a.pdf.)\nA better alternative to grid search is random search. In random search, instead of trying\nevery possible combination of hyperparameter values, you randomly sample the val-\nues and measure the model’s performance on a specified number of combinations\n(which are called trials). For example, in the previous example, random search may\nchoose (2, 87), (1, 339), (2, 101), (3, 254), and so on until it hits the specified number\nof trials. See the illustration in figure 10.15 (right).\nGrid search\nUnimportant parameter\nImportant parameter\nUnimportant parameter\nImportant parameter\nRandom search\n\n\n276\nCHAPTER 10\nBest practices in developing NLP applications\n Unless your hyperparameter search space is very small (like the first example), ran-\ndom search is usually recommended over grid search if you want to optimize hyper-\nparameters efficiently. Why? In many machine learning settings, not every hyper-\nparameter is made equal—there are usually only a small number of hyperparameters\nthat actually matter for the performance, whereas many others do not. Grid search\nwill waste a lot of computation searching for the best combination of hyperparameters\nthat do not really matter, while being unable to explore the few hyperparameters that\ndo matter in detail (figure 10.15, left). On the other hand, random search can\nexplore many possible points on the axis that matters for the performance (figure\n10.15, right). Notice that random search can find a better model by exploring more\npoints on the x-axis with the same number of trials (total of nine trials).\n10.5.3 Hyperparameter tuning with Optuna\nOK, we’ve covered some ways to tune hyperparameters including manual, grid, and\nrandom search, but how should you go about implementing it in practice? You can\nalways write your own for-loop (or “for-loops,” in the case of grid search), although it\nwould quickly get tiring if you need to write this type of boilerplate code for every\nmodel and task you work on. \n Hyperparameter optimization is such a universal topic that many ML researchers\nand engineers have been working on better algorithms and software libraries. For exam-\nple, AllenNLP has its own library called Allentune (https://github.com/allenai/allen-\ntune) that you can easily integrate with your AllenNLP training pipeline. In the\nremainder of this section, however, I’m going to introduce another hyperparameter\ntuning library called Optuna (https://optuna.org/) and show how to use it with Allen-\nNLP to optimize your hyperparameters. Optuna implements state-of-the-art algorithms\nthat search for optimal hyperparameters efficiently and provides integration with a wide\nrange of machine learning frameworks, including TensorFlow, PyTorch, and AllenNLP.\n First, we assume that you have installed AllenNLP (1.0.0+) and the Optuna plugin\nfor AllenNLP. These can be installed by running the following:\npip install allennlp\npip install allennlp_optuna\nAlso, as instructed by the official documentation (https://github.com/himkt/allennlp\n-optuna), you need to register the plugin with AllenNLP by running the next code:\necho 'allennlp_optuna' >> .allennlp_plugins\nWe are going to use the LSTM-based classifier for the Stanford Sentiment Treebank\ndataset we built in chapter 2. You can find the AllenNLP config file in the book repos-\nitory (http://www.realworldnlpbook.com/ch10.html#config). Note that you need to\nreference the variables (std.extVar) for Optuna to have control over the parameters.\nSpecifically, you need to define them at the beginning of the config file:\nlocal embedding_dim = std.parseJson(std.extVar('embedding_dim'));\nlocal hidden_dim = std.parseJson(std.extVar('hidden_dim'));\nlocal lr = std.parseJson(std.extVar('lr'));\n\n\n277\nHyperparameter tuning\nThen, you need to tell Optuna which parameters to optimize. You can do this by writ-\ning a JSON file (hparams.json (http://www.realworldnlpbook.com/ch10.html#\nhparams)). You need to specify every hyperparameter you want Optuna to optimize\nwith its types and ranges as follows:\n[\n    {\n        \"type\": \"int\",\n        \"attributes\": {\n            \"name\": \"embedding_dim\",\n            \"low\": 64,\n            \"high\": 256\n        }\n    },\n    {\n        \"type\": \"int\",\n        \"attributes\": {\n            \"name\": \"hidden_dim\",\n            \"low\": 64,\n            \"high\": 256\n        }\n    },\n    {\n        \"type\": \"float\",\n        \"attributes\": {\n            \"name\": \"lr\",\n            \"low\": 1e-4,\n            \"high\": 1e-1,\n            \"log\": true\n        }\n    }\n]\nNext, invoke this command to start the optimization:\nallennlp tune \\\n    examples/tuning/sst_classifier.jsonnet \\\n    examples/tuning/hparams.json \\\n    --include-package examples \\\n    --serialization-dir result \\\n    --study-name sst-lstm \\\n    --n-trials 20 \\\n    --metrics best_validation_accuracy \\\n    --direction maximize\nNote that we are running 20 trials (--n-trials) with validation accuracy (--metrics\nbest_validation_accuracy) as the metric to maximize (--direction maxi-\nmize). If you do not specify the metric and the direction, by default Optuna tries to\nminimize the validation loss.\n This will take a while, but after all the trials are finished you will see the following\none-line summary of the optimization:\nTrial 19 finished with value: 0.3469573115349682 and parameters: \n{'embedding_dim': 120, 'hidden_dim': 82, 'lr': 0.00011044322486693224}. \nBest is trial 14 with value: 0.3869209809264305.\n\n\n278\nCHAPTER 10\nBest practices in developing NLP applications\nFinally, Optuna supports a wide range of visualization of the optimization result,\nincluding very nice contour plots (http://www.realworldnlpbook.com/ch10.html#\ncontour), but here we'll simply use its web-based dashboard to quickly inspect the\noptimization process. All you need to do is invoke its dashboard from the command\nline as follows:\noptuna dashboard --study-name sst-lstm --storage sqlite:///allennlp_optuna.db\nNow you can access http:/./localhost:5006/dashboard to see the dashboard,\nshown in figure 10.16.\nFigure 10.16\nThe Optuna dashboard shows the evaluation metrics of the parameters for each trial.\nFrom this dashboard you can quickly see not only that your optimal trial was trial #14\nbut also the optimal hyperparameters at each trial.\nSummary\nInstances are sorted, padded, and batched together for more efficient\ncomputation.\nSubword tokenization algorithms such as BPE split words into units smaller than\nwords to mitigate the out-of-vocabulary problem in neural network models.\n\n\n279\nSummary\nRegularization (such as L2 and dropout) is a technique to encourage model\nsimplicity and generalizability in machine learning.\nYou can use data upsampling, downsampling, or loss weights for addressing the\ndata imbalance issue.\nHyperparameters are parameters about the model or the training algorithm.\nThey can be optimized using manual, grid, or random search. Even better, use\nhyperparameter optimization libraries such as Optuna, which integrates easily\nwith AllenNLP.\n\n\n280\nDeploying and serving\nNLP applications\nWhere chapters 1 through 10 of this book are about building NLP models, this chap-\nter covers everything that happens outside NLP models. Why is this important? Isn’t\nNLP all about building high-quality ML models? It may come as a surprise if you\ndon’t have much experience with production NLP systems, but a large portion of an\nNLP system has very little to do with NLP at all. As shown in figure 11.1, only a tiny\nfraction of a typical real-world ML system is the ML code, but the “ML code” part is\nsupported by numerous components that provide various functionalities, including\ndata collection, feature extraction, and serving. Let’s use a nuclear power plant as an\nanalogy. In operating a nuclear power plant, only a tiny fraction concerns nuclear\nThis chapter covers\nChoosing the right architecture for your NLP \napplication\nVersion-controlling your code, data, and model\nDeploying and serving your NLP model\nInterpreting and analyzing model predictions \nwith LIT (Language Interpretability Tool)\n\n\n281\nArchitecting your NLP application\nreaction. Everything else is a vast and complex infrastructure that supports safe and effi-\ncient generation and transportation of materials and electricity—how to use the gen-\nerated heat to turn the turbine to make electricity, how to cool and circulate water\nsafely, how to transmit the electricity efficiently, and so on. All those supporting infra-\nstructures have little to do with nuclear physics.\n Partly due to the “AI hype” in popular media, I personally think people pay too\nmuch attention to the ML modeling part and too little attention to how to serve the\nmodel in a useful way. After all, the goal of your product is to deliver values to the\nusers, not provide them with the raw predictions of the model. Even if your model is\n99% accurate, it’s not useful if you cannot make the most of the prediction so that\nusers can benefit from them. Using the previous analogy, users want to power their\nappliances and light their houses with electricity and do not care much how exactly\nthe electricity is generated in the first place.\n In the rest of this chapter, we’ll discuss how to architect your NLP applications—we\nfocus on some of the best practices when it comes to designing and developing NLP\napplications in a reliable and effective manner. Then we talk about deploying your\nNLP models—this is how we bring the NLP models to production and serve their\npredictions.\n11.1\nArchitecting your NLP application\nMachine learning engineering is still software engineering. All the best practices (decou-\npled software architectures, well-designed abstractions, clean and readable code, ver-\nsion control, continuous integration, etc.) apply to ML engineering as well. In this\nsection, we’ll discuss some best practices specific to designing and building NLP/ML\napplications. \nML model\nFeature pipeline\n…\nServing infrastructure\nTraining infrastructure\nTesting\nMonitoring\nDeployment infrastructure\nData \npipeline\nVisualization \n& \ninterpretation\nFigure 11.1\nA typical ML system consists of many different components, and the ML code is \nonly a tiny fraction of it. We cover the highlighted components in this chapter.\n\n\n282\nCHAPTER 11\nDeploying and serving NLP applications\n11.1.1 Before machine learning\nI understand this is a book about NLP and ML, but you should seriously think about\nwhether you need ML at all for your product before you start working on your NLP\napplication. Building an ML system is no easy feat—it requires a lot of money and\ntime to collect data, train models, and serve predictions. If you can solve your prob-\nlem by writing some rules, by all means do so. As a rule of thumb, if a deep learning\nmodel can achieve an accuracy of 80%, a simpler, rule-based model can take you at\nleast halfway there.\n Also, you should consider using existing solutions, if any. Many open source NLP\nlibraries (including AllenNLP and Transformers, the two libraries that we’ve been\nusing extensively throughout the book) exist that come with a wide range of pretrained\nmodels. Cloud service providers (such as AWS AI services (https://aws.amazon.com/\nmachine-learning/ai-services/), Google Cloud AutoML (https://cloud.google.com/\nautoml), and Microsoft Azure Cognitive Services (https://azure.microsoft.com/en-us/\nservices/cognitive-services/)) offer a wide range of ML-related APIs for many domains,\nincluding NLP. If your task is something that can be solved using their offerings with no\nor little modification, that’d usually be a cost-efficient way to build your NLP applica-\ntion. After all, the most expensive component of any NLP application is usually highly\nskilled talent (i.e., your salary), and you should think twice before you go all-in and\nbuild in-house NLP solutions.\n In addition, you shouldn’t rule out “traditional” machine learning approaches.\nWe’ve paid little attention to traditional ML models in this book, but you can find rich\nliterature of statistical NLP models that were mainstream before the advent of deep\nNLP methods. Quickly building a prototype with statistical features (such as n-grams)\nand ML models (such as SVM) is often a great start. Non-deep algorithms such as\ngradient-boosted decision trees (GBDTs) often work almost as well as, if not better than,\ndeep learning methods at a fraction of the cost.\n Finally, I always recommend that practitioners start by developing the validation\nset and choosing the right evaluation metric first, even before starting to choose the\nright ML approach. A validation set doesn’t need to be big, and most people can\nafford to sit down for a couple of hours and manually annotate a couple of hundred\ninstances. Doing this offers many benefits—first, by solving the task manually, you get\na feel of what’s important when it comes to solving the problem and whether it’s\nsomething that a machine can really solve automatically. Second, by putting yourself\nin the machine’s shoes, you gain a lot of insights into the task (what the data looks\nlike, how the input and output data are distributed, and how they are related), which\nbecome valuable when it comes to actually designing an ML system to solve it. \n11.1.2 Choosing the right architecture\nExcept for rare occasions where the output of an ML system is the end product itself\n(such as machine translation), NLP modules usually interact with a larger system that\ncollectively provide some values to the end users. For example, a spam filter is usually\nimplemented as a module or a microservice embedded in a larger application (email\n",
      "page_number": 295
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 303-311)",
      "start_page": 303,
      "end_page": 311,
      "detection_method": "topic_boundary",
      "content": "283\nArchitecting your NLP application\nservice). Voice assistant systems are usually large, complex combinations of many ML/\nNLP subcomponents, including voice recognition, sentence-intent classification, ques-\ntion answering, and speech generation, that interact with each other. Even machine\ntranslation models can be one tiny component in a larger complex system if you\ninclude data pipelines, the backend, and the translation interface that the end users\ninteract with.\n An NLP application can take many forms. Surprisingly, many NLP components\ncan be structured as a one-off task that takes some static data as its input and produces\ntransformed data as its output. For example, if you have a static database of some doc-\numents and you’d like to classify them by their topics, your NLP classifier can be a sim-\nple one-off Python script that runs this classification task. If you’d like to extract\ncommon entities (e.g., company names) from the same database, you can write a\nPython script that runs a named entity recognition (NER) model to do it. Even a text-\nbased recommender engine that finds objects based on textual similarity can be a\ndaily task that reads from and writes data to the database. You don’t need to architect\na complex software system with many services talking to each other.\n Many other NLP components can be structured as a (micro)service that runs pre-\ndiction in batches, which is the architecture that I recommend for many scenarios.\nFor example, a spam filter doesn’t need to classify every single email as soon as they\narrive—the system can queue a certain number of emails that arrive at the system and\npass the batched emails to the classifier service. The NLP application usually commu-\nnicates with the rest of the system via some intermediary (e.g., a RESTful API or a\nqueuing system). This configuration is great for applications that require some fresh-\nness for their prediction (after all, users do not want to wait for hours until their\nemails arrive to their inbox), but the requirement is not that strict.\n Finally, NLP components can also be designed so that they serve real-time predic-\ntion. This is necessary when, for example, an audience needs real-time subtitles for a\nspeech. Another example is when the system wants to show ads based on the user’s\nreal-time behavior. For these cases, the NLP service needs to receive a stream of input\ndata (such as audio or user events) and produce another stream of data (such as tran-\nscribed text or ad-click probabilities). Real-time streaming frameworks such as Apache\nFlink (https://flink.apache.org/) are often used for processing such stream data.\nAlso, if your application is based on a server-client architecture, as with typical mobile\nand web apps, and you want to show some real-time prediction to the users, you can\nchoose to run ML/NLP models on the client side, such as the web browser or the\nsmartphones. Client-side ML frameworks such as TensorFlow.js (https://www.tensor-\nflow.org/js), Core ML (https://developer.apple.com/documentation/coreml), and\nML Kit (https://developers.google.com/ml-kit) can be used for such purposes.\n11.1.3 Project structure\nMany NLP applications follow somewhat similar project structures. A typical NLP proj-\nect may need to manage datasets to train a model from, intermediate files generated\nby preprocessing data, model files produced as a result of training, source code for\n\n\n284\nCHAPTER 11\nDeploying and serving NLP applications\ntraining and inference, and log files that store additional information about the train-\ning and inference.\n Because typical NLP applications have many components and directories in com-\nmon, it’d be useful if you simply follow best practices as your default choice when\nstarting a new project. Here are my recommendations for structuring your NLP\nprojects:\nData management—Make a directory called data and put all the data in it. It\nmay also be helpful to subdivide this into raw, interim, and result directories.\nThe raw directory contains the unprocessed dataset files you obtained exter-\nnally (such as the Stanford Sentiment Treebank we’ve been using throughout\nthis book) or built internally. It is very critical that you do not modify any files in\nthis raw directory by hand. If you need to make changes, write a script that runs\nsome processing against the raw files and then writes the result to the interim\ndirectory, which serves as a place for intermediate results. Or make a patch file\nthat manages the “diff” you made to the raw file, and version-control the patch\nfiles instead. The final results such as predictions and metrics should be stored\nin the result directory.\nVirtual environment—It is strongly recommended that you work in a virtual envi-\nronment so that your dependencies are separated and reproducible. You can\nuse tools like Conda (https://docs.conda.io/en/latest/) (my recommenda-\ntion) and venv (https://docs.python.org/3/library/venv.html) to set up a sepa-\nrate environment for your project and use pip to install individual packages.\nConda can export the environment configuration into an environment.yml\nfile, which you can use to recover the exact Conda environment. You can also\nkeep track of pip packages for the project in a requirements.txt file. Even\nbetter, you can use Docker containers to manage and package the entire ML\nenvironment. This greatly reduces dependency-related issues and simplifies\ndeployment and serving.\nExperiment management—Training and inference pipelines for an NLP applica-\ntion usually consist of several steps, such as preprocessing and joining the data,\nconverting them into features, training and running the model, and converting\nthe results back to a human-readable format. These steps can easily get out of\nhand if you try to remember to manage them manually. A good practice is to\nkeep track of the steps for the pipeline in a shell script file so that the experi-\nments are reproducible with a single command, or use dependency manage-\nment software such as GNU Make, Luigi (https://github.com/spotify/luigi),\nand Apache Airflow (https://airflow.apache.org/).\nSource code—Python source code is usually put in a directory of the same name\nas the project, which is further subdivided into directories such as data (for\ndata-processing code), model (for model code), and scripts (for putting\nscripts for training and other one-off tasks).\n\n\n285\nArchitecting your NLP application\n11.1.4 Version control\nYou probably don’t need to be convinced that version-controlling your source code is\nimportant. Tools like Git help you keep track of the changes and manage different\nversions of the source code. Development of NLP/ML applications is usually an itera-\ntive process where you (often with other people) make many changes to the source\ncode and experiment with many different models. You can easily end up with a num-\nber of slightly different versions of the same code. \n In addition to version-controlling your source code, it is also important to version-\ncontrol your data and models. This means that you should version-control your training\ndata, source code, and models separately, as shown in the dotted-line boxes in figure\n11.2. This is one of the major differences between regular software projects and ML\napplications. Machine learning is about improving computer algorithms through\ndata. By definition, the behavior of any ML system depends on data it is fed. This\ncould lead to a situation where the behavior of the system is different even if you use\nthe same code.\n Tools like Git Large File Storage (https://git-lfs.github.com/) and DVC (https://\ndvc.org) can version-control your data and models in a seamless way. Even if you are\nnot using these tools, you should at least manage different versions as separate files\nthat are named clearly.\nFigure 11.2\nMachine learning components to version-control: training data, source code, and models\nTraining infrastructure\nFeature pipeline\n…\nDataset\nreader\nTrainer\nOptimizer\nModel\nBatching\nFeature pipeline\n…\nServing infrastructure\nDataset\nreader\nPredictor \nModel\nNew instance\nTraining data\nPrediction\n\n\n286\nCHAPTER 11\nDeploying and serving NLP applications\nIn a larger and more complex ML project, you may want to version-control your\nmodel and your feature pipeline separately, because the behavior of an ML model can\nbe different depending on how you preprocess the input, even with the same model\nand input data. This will also mitigate the train-serve skew problem we’ll discuss later\nin section 11.3.2.\n Finally, when working on ML applications, you will experiment with a lot of different\nsettings—different combinations of training datasets, feature pipelines, models, and\nhyperparameters—which can easily get out of control. I recommend keeping track of\nthe training settings using some experiment management system, such as Weights &\nBiases (https://wandb.ai/), but you can also use something as simple as a spreadsheet\nin which you enter experiment information manually. When keeping track of experi-\nments, be sure to record the following information for each experiment:\nVersions of the model code, feature pipeline, and the training data used\nHyperparameters used to train the model\nEvaluation metrics for the training and the validation data\nPlatforms like AllenNLP support experiment configuration by default, which makes\nthe first two items easy. Tools like TensorBoard, which is supported by AllenNLP and\nHugging Face out of the box, make it trivial to keep track of various metrics.\n11.2\nDeploying your NLP model\nIn this section, we’ll move on to the deployment stage, where your NLP application is\nput on a server and becomes available for use. We’ll discuss practical considerations\nwhen deploying NLP/ML applications.\n11.2.1 Testing\nAs with software engineering, testing is an important part of building reliable NLP/\nML applications. The most fundamental and important tests are unit tests, which\nautomatically check whether small units of software (such as methods and classes) are\nworking as expected. In NLP/ML applications, it is important to unit-test your feature\npipeline. For example, if you write a method that converts raw text into a tensor repre-\nsentation, make sure that it works for typical and corner cases with unit tests. In my\nexperience, this is where bugs often sneak in. Reading a dataset, building a vocabulary\nfrom a corpus, tokenizing, converting tokens into integer IDs—these are all essential\nyet error-prone steps in preprocessing. Fortunately, frameworks such as AllenNLP\noffer standardized, well-tested components for these steps, which makes building NLP\napplications easier and bug-free.\n In addition to unit tests, you need to make sure that your model learns what it’s told\nto learn. This corresponds to testing logic errors in regular software engineering—\ntypes of errors where the software runs without crashing yet produces incorrect results.\nThis type of error is more difficult to catch and fix in NLP/ML, because you need more\ninsight into how the learning algorithm works mathematically. Moreover, many ML\n\n\n287\nDeploying your NLP model\nalgorithms involve some randomness, such as random initialization and sampling,\nwhich makes testing even more difficult.\n One recommended technique for testing NLP/ML models is sanity checks against\nthe model output. You can start with a small and simple model and just a few toy\ninstances with obvious labels. If you are testing a sentiment analysis model, for exam-\nple, this goes as follows:\nCreate a small and simple model for debugging, such as a toy encoder that sim-\nply averages the input word embeddings with a softmax layer on top.\nPrepare a few toy instances, such as “The best movie ever!” (positive) and “This\nis an awful movie!” (negative).\nFeed these instances to the model, and train it until convergence. Because we\nare using a very small dataset without a validation set, the model will heavily\noverfit to the instances, and that’s totally fine. Check whether the training loss\ngoes down as expected.\nFeed the same instances to the trained model, and check whether the predicted\nlabels match the expected ones.\nTry the steps above with more toy instances and a larger model.\nAs a related technique, I always recommend you start with a smaller dataset, especially\nif the original dataset is large. Because training NLP/ML models takes a long time\n(hours or even days), you often find that your code has some errors only after your\ntraining is finished. You can subsample your training data, for example, by simply tak-\ning one out of every 10 instances, so that your entire training finishes quickly. Once\nyou are sure that your model works as expected, you can gradually ramp up the\namount of data you use for training. This technique is also great for quickly iterating\nand experimenting with many different architectures and hyperparameter settings.\nWhen you have just started building your model, you don’t usually have clear under-\nstanding of the best models for your task. With a smaller dataset, you can quickly vali-\ndate a large number of different options (RNN versus Transformers, different\ntokenizers, etc.) and narrow down the set of candidate models that work best. One\ncaveat to this approach is that the best model architectures and hyperparameters may\ndepend on the size of the training data. Because of this, don’t forget to run the valida-\ntion against the full dataset, too.\n Finally, you can use integration tests to verify whether the individual components\nof your application work in combination. For NLP, this usually means running the\nwhole pipeline to see if the prediction is correct. Similar to the unit tests, you can pre-\npare a small number of instances where the expected prediction is clear and run them\nagainst the trained model. Note that these instances are not for measuring how good\nthe model is, but rather to serve as a sanity check whether your model can produce\ncorrect predictions for “obvious” cases. It is a good practice to run integration tests\nevery time a new model or code is deployed. This is usually part of continuous integra-\ntion (CI) used for regular software engineering.\n\n\n288\nCHAPTER 11\nDeploying and serving NLP applications\n11.2.2 Train-serve skew\nOne common source of errors in ML applications is called train-serve skew, a situation\nwhere there’s a discrepancy between how instances are processed at the training and\nthe inference times. This could occur in various situations, but let’s discuss a concrete\nexample. Say you are building a sentiment-analyzer system with AllenNLP and would\nlike to convert texts into instances. You usually start writing a data loader first, which\nreads the dataset and produces instances. Then you write a Python script or a config\nfile that tells AllenNLP how the model should be trained. You train and validate your\nmodel. So far, so good. However, when it comes to using the model for prediction,\nthings look slightly different. You need to write a predictor, which, given an input text,\nconverts it into an instance and passes it to the model’s forward method. Notice that\nnow you have two independent pipelines that preprocess the input—one for the train-\ning in the dataset reader, and another for the inference in the predictor.\n What happens if you want to modify the way the input text is processed? For exam-\nple, let’s say you find something you want to improve in your tokenization process,\nand you make changes to how input text is tokenized in your data loader. You update\nyour data loader code, retrain the model, and deploy the model. However, you forgot\nto update the corresponding tokenization code in your predictor, effectively creating\na discrepancy in how input is tokenized between training and serving. This is illus-\ntrated in figure 11.3.\nFigure 11.3\nTrain-serve skew is caused by discrepancies in how input is processed between training and serving.\nTraining infrastructure\nFeature pipeline\n…\nDataset\nreader\nTrainer\nOptimizer \nModel\nBatching\nFeature pipeline\n…\nServing infrastructure\nTrain-serve skew\nPredictor\nPredictor \nModel\nNew instance\nTraining data\nPrediction\n\n\n289\nDeploying your NLP model\nThe best way to fix this—or even better, to prevent this from happening in the first\nplace—is to share as much of the feature pipeline as possible between the training and\nthe serving infrastructure. A common practice in AllenNLP is to implement a method\ncalled _text_to_instance() in the dataset reader, which takes an input and returns\nan instance. By making sure that both the dataset reader and the predictor refer to the\nsame method, you can minimize the discrepancy between the pipelines.\n In NLP, the fact that input text is tokenized and converted to numerical values\nmakes debugging your model even more difficult. For example, an obvious bug in\ntokenization that you can spot easily with your naked eyes can be quite difficult to\nidentify if everything is numerical values. A good practice is to log some intermediate\nresults into a log file that you can inspect later. \n Finally, note that some behaviors of neural networks do differ between training\nand serving. One notable example is dropout, a regularization method we briefly\ncovered in section 10.3.1. To recap, dropout regularizes the model by randomly mask-\ning activation values in a neural network. This makes sense in training, because by\nremoving activations, the model learns to make robust predictions based on available\nvalues. However, remember to turn it off at the serving time, because you don’t want\nyour model to randomly drop neurons. PyTorch models implement methods—\ntrain() and eval()—that switch between the training and prediction modes,\naffecting how layers like dropout behave. If you are implementing a training loop\nmanually, remember to call model.eval()to disable dropout. The good news is that\nframeworks such as AllenNLP can handle this automatically as long as you are using\ntheir default trainer.\n11.2.3 Monitoring\nAs with other software services, deployed ML systems should be monitored continu-\nously. In addition to the usual server metrics (e.g., CPU and memory usage), you\nshould also monitor metrics related to the input and the output of the model. Specifi-\ncally, you can monitor some higher-level statistics such as the distribution of input val-\nues and output labels. As mentioned earlier, logic errors, which are a type of error that\ncauses the model to produce wrong results without crashing it, are the most common\nand hardest to find in ML systems. Monitoring those high-level statistics makes it eas-\nier to find them. Libraries and platforms like PyTorch Serve and Amazon SageMaker\n(discussed in section 11.3) support monitoring by default.\n11.2.4 Using GPUs\nTraining large modern ML models almost always requires hardware accelerators such\nas GPUs. Recall that back in chapter 2, we used overseas factories as an analogy for\nGPUs, which are designed to execute a huge number of arithmetic operations such as\nvector and matrix addition and multiplications in parallel. In this subsection, we’ll\ncover how to use GPUs to accelerate the training and prediction of ML models.\n\n\n290\nCHAPTER 11\nDeploying and serving NLP applications\n If you don’t own GPUs or have never used cloud-based GPU solutions before, the\neasiest way to “try” GPUs for free is to use Google Colab. Go to its URL (https://\ncolab.research.google.com/), create a new notebook, go to the Runtime menu, and\nchoose “Change runtime type.” This will bring up the dialog box shown in figure 11.4.\nChoose GPU as the type of the hardware accelerator, and type !nvidia-smi in a\ncode block and execute it. Some detailed information about your GPU is displayed, as\nshown next:\n \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nThe nvidia-smi command (short for Nvidia System Management Interface) is a\nhandy tool for checking information about Nvidia GPUs on the machine. From the\nprevious snippet, you can see the version of the driver and CUDA (an API and a\nFigure 11.4\nGoogle Colab \nallows you to choose the type \nof hardware accelerator.\n\n\n291\nDeploying your NLP model\nlibrary for interacting with GPUs), type of GPUs (Tesla T4), available and used mem-\nory (15109 MiB and 3 MiB), and the list of processes that currently use GPUs (there\naren’t any). The most typical use of this command is to check how much memory your\ncurrent process(es) use, because in GPU programming, you can easily get an out-of-\nmemory error if your program uses more memory than is available.\n If you use cloud infrastructures such as AWS (Amazon Web Services) and GCP\n(Google Cloud Platform), you’ll find a wide array of virtual machine templates that\nyou can use to quickly create cloud instances that support GPUs. For example, GCP\nhas Nvidia’s official GPU-optimized images for PyTorch and TensorFlow, which you\ncan use as templates to launch your GPU instances. AWS offers Deep Learning AMIs\n(Amazon Machine Images), which preinstall basic GPU libraries such as CUDA, as\nwell as deep learning libraries such as PyTorch. With these templates, you don’t need\nto install necessary drivers and libraries manually—you can start building your ML\napplications right away. Note that although these templates are free, you do need to\npay for the infrastructure. The price for GPU-enabled virtual machines is usually sig-\nnificantly higher than CPU machines. Make sure to check their price before you keep\nthem running for an extended period of time.\n If you are setting up GPU instances from scratch, you can find detailed instruc-\ntions1 for how to set up necessary drivers and libraries. To build NLP applications\nwith the libraries that we covered in this book (namely, AllenNLP and Transformers),\nyou need to install CUDA drivers and toolkits, as well as a PyTorch version that sup-\nports GPU.\n If your machine has GPU(s), you can enable GPU acceleration by specifying\ncuda_device in an AllenNLP config file as follows:\n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 1.0e-5\n        },\n        \"num_epochs\": 20,\n        \"patience\": 10,\n        \"cuda_device\": 0\n}\nThis tells the trainer to use the first GPU for training and validating the AllenNLP\nmodel. \n If you are writing PyTorch code from scratch, you need to manually transfer your\nmodel and tensors to the GPU. Using an analogy, this is when your materials get\nshipped to an overseas factory in container ships. First, you can specify the device\n(GPU ID) to use, and invoke the to() method of tensors and models to move them\n1 GCP: https://cloud.google.com/compute/docs/gpus/install-drivers-gpu; AWS: https://docs.aws.amazon\n.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html.\n",
      "page_number": 303
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 312-319)",
      "start_page": 312,
      "end_page": 319,
      "detection_method": "topic_boundary",
      "content": "292\nCHAPTER 11\nDeploying and serving NLP applications\nbetween devices. For example, you can use the following code snippet to run text gen-\neration on a GPU with Hugging Face Transformers:\ndevice = torch.device('cuda:0')\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\nmodel = AutoModelWithLMHead.from_pretrained(\"gpt2-large\")\ngenerated = tokenizer.encode(\"On our way to the beach \")\ncontext = torch.tensor([generated])\nmodel = model.to(device)\ncontext = context.to(device)\nThe rest is identical to the code we used in section 8.4.\n11.3\nCase study: Serving and deploying NLP applications\nIn this section, we will go over a case study where we serve and deploy an NLP model\nbuilt with Hugging Face. Specifically, we’ll take a pretrained language generation\nmodel (DistilGPT2), serve it with TorchServe, and deploy it to a cloud server using\nAmazon SageMaker. \n11.3.1 Serving models with TorchServe\nAs you have seen, deploying an NLP application is more than just writing an API for\nyour ML model. You need to take care of a number of production-related consider-\nations, including how to deal with high traffic by parallelizing model inference with\nmultiple workers, how to store and manage different versions of multiple ML models,\nhow to consistently handle pre- and postprocessing of the data, and how to monitor\nthe health of the server as well as various metrics about the data. \n Because these problems are so common, ML practitioners have been working on\ngeneral-purpose platforms for serving and deploying ML models. In this section, we’ll\nuse TorchServe (https://github.com/pytorch/serve), an easy-to-use framework for\nserving PyTorch models jointly developed by Facebook and Amazon. TorchServe is\nshipped with many functionalities that can address the issues mentioned earlier.\n TorchServe can be installed by running the following:\npip install torchserve torch-model-archiver\nIn this case study, we’ll use a pretrained language model called DistilGPT2. DistilGPT2\nis a smaller version of GPT-2 built using a technique called knowledge distillation.\nKnowledge distillation (or simply distillation) is a machine learning technique where a\nsmaller model (called a student) is trained in such a way that it mimics the predictions\nproduced by a larger model (called a teacher). It is a great way to train a smaller model\nthat produces high quality output, and it often produces a better model than training\na smaller model from scratch.\n First, let’s download the pretrained DistilGPT2 model from the Hugging Face\nrepository by running the following commands. Note that you need to install Git\n\n\n293\nCase study: Serving and deploying NLP applications\nLarge File Storage (https://git-lfs.github.com/), a Git extension for handling large\nfiles under Git:\ngit lfs install\ngit clone https://huggingface.co/distilgpt2\nThis creates a subdirectory called distilgpt2, which contains files such as config.json\nand pytorch_model.bin.\n As the next step, you need write a handler for TorchServe, a lightweight wrapper\nclass that specifies how to initialize your model, preprocess and postprocess the input,\nand run the inference on the input. Listing 11.1 shows the handler code for serving\nthe DistilGPT2 model. In fact, nothing in the handler is specific to the particular\nmodel we use (DistilGPT2). You can use the same code for other GPT-2–like models,\nincluding the original GPT-2 models, as long as you use the Transformers library.\nfrom abc import ABC\nimport logging\nimport torch\nfrom ts.torch_handler.base_handler import BaseHandler\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nlogger = logging.getLogger(__name__)\nclass TransformersLanguageModelHandler(BaseHandler, ABC):\n    def __init__(self):\n        super(TransformersLanguageModelHandler, self).__init__()\n        self.initialized = False\n        self.length = 256\n        self.top_k = 0\n        self.top_p = .9\n        self.temperature = 1.\n        self.repetition_penalty = 1.\n    def initialize(self, ctx):           \n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        model_dir = properties.get(\"model_dir\")\n        self.device = torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\"))\n            if torch.cuda.is_available()\n            else \"cpu\"\n        )\n        self.model = GPT2LMHeadModel.from_pretrained(model_dir)\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n        self.model.to(self.device)\n        self.model.eval()\nListing 11.1\nHandler for TorchServe\nInitializes \nthe model\n\n\n294\nCHAPTER 11\nDeploying and serving NLP applications\n        logger.info('Transformer model from path {0} loaded \nsuccessfully'.format(model_dir))\n        self.initialized = True\n    def preprocess(self, data):             \n        text = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n        text = text.decode('utf-8')\n        logger.info(\"Received text: '%s'\", text)\n        encoded_text = self.tokenizer.encode(\n            text,\n            add_special_tokens=False,\n            return_tensors=\"pt\")\n        return encoded_text\n    def inference(self, inputs):            \n        output_sequences = self.model.generate(\n            input_ids=inputs.to(self.device),\n            max_length=self.length + len(inputs[0]),\n            temperature=self.temperature,\n            top_k=self.top_k,\n            top_p=self.top_p,\n            repetition_penalty=self.repetition_penalty,\n            do_sample=True,\n            num_return_sequences=1,\n        )\n        text = self.tokenizer.decode(\n            output_sequences[0],\n            clean_up_tokenization_spaces=True)\n        return [text]\n    def postprocess(self, inference_output):   \n        return inference_output\n_service = TransformersLanguageModelHandler()\ndef handle(data, context):                    \n    try:\n        if not _service.initialized:\n            _service.initialize(context)\n        if data is None:\n            return None\n        data = _service.preprocess(data)\n        data = _service.inference(data)\n        data = _service.postprocess(data)\nPreprocesses and tokenizes\nthe incoming data\nRuns inference \non the data\nPostprocesses \nthe prediction\nThe handler method \ncalled by TorchServe\n\n\n295\nCase study: Serving and deploying NLP applications\n        return data\n    except Exception as e:\n        raise e\nYour handler needs to inherit from BaseHandler and override a few methods includ-\ning initialize() and inference(). Your handler script also includes handle(),\na top-level method where the handler is initialized and called.\n The next step is to run torch-model-archiver, which is a command-line tool\nthat packages your model and your handler, as follows:\ntorch-model-archiver \\\n    --model-name distilgpt2 \\\n    --version 1.0 \\\n    --serialized-file distilgpt2/pytorch_model.bin \\\n    --extra-files \"distilgpt2/config.json,distilgpt2/vocab.json,distilgpt2/\ntokenizer.json,distilgpt2/merges.txt\" \\\n    --handler ./torchserve_handler.py\nThe first two options specify the name and the version of the model. The next option,\nserialized-file, specifies the main weight file of the PyTorch model you want to\npackage (which usually ends with .bin or .pt). You can also add any extra files (speci-\nfied by extra-files) that are needed for the model to run. Finally, you need to pass\nthe handler file you just wrote to the handler option.\n When finished, this creates a file named distilgpt2.mar (.mar stands for\n“model archive”) in the same directory. Let’s create a new directory named model_\nstore and move the .mar file there as follows. This directory serves as a model store,\na place where all the model files are stored and served from:\nmkdir model_store\nmv distilgpt2.mar model_store\nNow you are ready to spin up TorchServe and start serving your model! All you need is\nto run the following command:\ntorchserve --start --model-store model_store --models distilgpt2=distilgpt2.mar\nWhen the server is fully up, you can start making the HTTP requests to the server. It\nexposes a couple of endpoints, but if you just want to run inference, you need to\ninvoke http://127.0.0.1:8080/predictions/ with the model name as follows:\ncurl -d \"data=In a shocking finding, scientist discovered a herd of unicorns \nliving in a remote, previously unexplored valley, in the Andes \nMountains. Even more surprising to the researchers was the fact that the \nunicorns spoke perfect English.\" -X POST http://127.0.0.1:8080/\npredictions/distilgpt2\nHere, we are using a prompt from OpenAI’s original post about GPT-2 (https://\nopenai.com/blog/better-language-models/). This returns the generated sentences,\nshown next. The generated text is of decent quality, considering that the model is a\ndistilled, smaller version:\n\n\n296\nCHAPTER 11\nDeploying and serving NLP applications\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote,\npreviously unexplored valley, in the Andes Mountains. Even more surprising to the\nresearchers was the fact that the unicorns spoke perfect English. They used to speak\nthe Catalan language while working there, and so the unicorns were not just part of\nthe local herd, they were also part of a population that wasn't much less diverse than\ntheir former national-ethnic neighbors, who agreed with them.\n“In a sense they learned even better than they otherwise might have been,” says\nAndrea Rodriguez, associate professor of language at the University of California,\nIrvine. “They told me that everyone else was even worse off than they thought.”\nThe findings, like most of the research, will only support the new species that their\nnative language came from. But it underscores the incredible social connections\nbetween unicorns and foreigners, especially as they were presented with a hard new\nplatform for studying and creating their own language.\n“Finding these people means finding out the nuances of each other, and dealing with\ntheir disabilities better,” Rodriguez says.\n …\nWhen you are finished, you can run the following command to stop serving:\ntorchserve --stop\n11.3.2 Deploying models with SageMaker\nAmazon SageMaker is a managed platform for training and deploying machine learn-\ning models. It enables you to spin up a GPU server, run a Jupyter Notebook inside it,\nbuild and train ML models there, and directly deploy them in a hosted environment.\nOur next step is to deploy the machine learning model as a cloud SageMaker end-\npoint so that production systems can make requests to it. The concrete steps for\ndeploying an ML model with SageMaker consist of the following:\n1\nUpload your model to S3.\n2\nRegister and upload your inference code to Amazon Elastic Container Registry\n(ECR).\n3\nCreate a SageMaker model and an endpoint.\n4\nMake requests to the endpoint.\nWe are going to follow the official tutorial (http://mng.bz/p9qK) with a slight modifi-\ncation. First, let’s go to the SageMaker console (https://console.aws.amazon.com/\nsagemaker/home) and start a notebook instance. When you open the notebook, run\nthe following code to install the necessary packages and start a SageMaker session:\n!git clone https://github.com/shashankprasanna/torchserve-examples.git\n!cd torchserve-examples\n!git clone https://github.com/pytorch/serve.git\n!pip install serve/model-archiver/\n\n\n297\nCase study: Serving and deploying NLP applications\nimport boto3, time, json\nsess    = boto3.Session()\nsm      = sess.client('sagemaker')\nregion  = sess.region_name\naccount = boto3.client('sts').get_caller_identity().get('Account')\nimport sagemaker\nrole = sagemaker.get_execution_role()\nsagemaker_session = sagemaker.Session(boto_session=sess)\nbucket_name = sagemaker_session.default_bucket()\nThe variable bucket_name contains a string like sagemaker-xxx-yyy where xxx is\nthe region name (like us-east-1). Take note of this name—you need it to upload\nyour model to S3 in the next step.\n Next, you need to upload your model to an S3 bucket by running the following\ncommands from the machine where you just created the .mar file (not from the Sage-\nMaker notebook instance). Before uploading, you first need to compress your .mar\nfile into a tar.gz file, a format supported by SageMaker. Remember to replace sage-\nmaker-xxx-yyy with the actual bucket name specified by bucket_name:\ncd model_store\ntar cvfz distilgpt2.tar.gz distilgpt2.mar\naws s3 cp distilgpt2.tar.gz s3://sagemaker-xxx-yyy/torchserve/models/\nThe next step is to register and push the TorchServe inference code to ECR. Before\nyou start, in your SageMaker notebook instance, open torchserve-examples/\nDockerfile and modify the following line (add --no-cache-dir transformers):\nRUN pip install --no-cache-dir psutil \\\n                --no-cache-dir torch \\\n                --no-cache-dir torchvision \\\n                --no-cache-dir transformers\nNow you can build a Docker container and push it to ECR as follows:\nregistry_name = 'torchserve'\n!aws ecr create-repository --repository-name torchserve\nimage_label = 'v1'\nimage = f'{account}.dkr.ecr.{region}.amazonaws.com/\n{registry_name}:{image_label}'\n!docker build -t {registry_name}:{image_label} .\n!$(aws ecr get-login --no-include-email --region {region})\n!docker tag {registry_name}:{image_label} {image}\n!docker push {image}\nNow you are ready to create a SageMaker model and create an endpoint for it, as\nshown next:\nimport sagemaker\nfrom sagemaker.model import Model\n\n\n298\nCHAPTER 11\nDeploying and serving NLP applications\nfrom sagemaker.predictor import RealTimePredictor\nrole = sagemaker.get_execution_role()\nmodel_file_name = 'distilgpt2'\nmodel_data = f's3://{bucket_name}/torchserve/models/{model_file_name}.tar.gz'\nsm_model_name = 'torchserve-distilgpt2'\ntorchserve_model = Model(model_data = model_data, \n                         image_uri = image,\n                         role = role,\n                         predictor_cls=RealTimePredictor,\n                         name = sm_model_name)\nendpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", \ntime.gmtime())\npredictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n                                    initial_instance_count=1,\n                                    endpoint_name = endpoint_name)\nThe predictor object is something you can call directly to run the inference as follows:\nresponse = predictor.predict(data=\"In a shocking finding, scientist \ndiscovered a herd of unicorns living in a remote, previously unexplored \nvalley, in the Andes Mountains. Even more surprising to the researchers \nwas the fact that the unicorns spoke perfect English.\")\nThe content of the response should look something like this:\nb'In a shocking finding, scientist discovered a herd of unicorns living in a \nremote, previously unexplored valley, in the Andes Mountains. Even more \nsurprising to the researchers was the fact that the unicorns spoke \nperfect English. The unicorns said they would take a stroll in the \ndirection of scientists over the next month or so.\\n\\n\\n\\n\\nWhen \ncontacted by Animal Life and Crop.com, author Enrique Martinez explained \nhow he was discovered and how the unicorns\\' journey has surprised him. \nAccording to Martinez, the experience makes him more interested in \nresearch and game development.\\n\"This is really what I want to see this \nyear, and in terms of medical research, I want to see our population \nincrease.\"<|endoftext|>'\nCongratulations! We just completed our journey—we started building an ML model\nin chapter 2 and came all the way to deploying it to a cloud platform in this chapter.\n11.4\nInterpreting and visualizing model predictions\nPeople often talk about the metrics and leaderboard performance on standardized\ndatasets, but analyzing and visualizing model predictions and internal states is import-\nant for NLP applications in the real world. Although deep learning models can be\nreally good at what they do, often reaching human-level performance on some NLP\ntasks, those deep models are black boxes, and it is difficult to know why they make cer-\ntain predictions. \n Because of this (somewhat troubling) property of deep learning models, a growing\nfield in AI called explainable AI (XAI) is working to develop methods to explain the\n\n\n299\nInterpreting and visualizing model predictions\npredictions and behavior of ML models. Interpreting ML models is useful for\ndebugging—it gives you a lot of clues if you know why it made certain predictions. In\nsome domains such as medical applications and self-driving cars, making ML models\nexplainable is critical for legal and practical reasons. In this final section of the chap-\nter, we’ll go over a case study where we use the Language Interpretability Tool (LIT)\n(https://pair-code.github.io/lit/) for visualizing and interpreting the predictions and\nbehavior of NLP models.\n LIT is an open source toolkit developed by Google and offers a browser-based\ninterface for interpreting and visualizing ML predictions. Note that it is framework\nagnostic, meaning that it works with any Python-based ML frameworks of choice,\nincluding AllenNLP and Hugging Face Transformers.2 LIT offers a wide range of fea-\ntures, including the following:\nSaliency map—Visualizing in color which part of the input played an important\nrole to reach the current prediction\nAggregate statistics—Showing aggregate statistics such as dataset metrics and con-\nfusion matrices\nCounterfactuals—Observing how model predictions change for generated new\nexamples\nIn the remainder of this section, let’s take one of the AllenNLP models we trained\n(the BERT-based sentiment analysis model in chapter 9) and analyze it via LIT. LIT\noffers a set of extensible abstractions such as datasets and models to make it easier to\nwork with any Python-based ML models.\n First, let’s install LIT. It can be installed with a single call of pip as follows:\npip install lit-nlp\nNext, you need to wrap your dataset and model with the abstract classes defined by\nLIT. Let’s create a new script called run_lit.py, and import the necessary modules\nand classes, as shown here:\nimport numpy as np\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors.predictor import Predictor\nfrom lit_nlp import dev_server\nfrom lit_nlp import server_flags\nfrom lit_nlp.api import dataset as lit_dataset\nfrom lit_nlp.api import model as lit_model\nfrom lit_nlp.api import types as lit_types\nfrom examples.sentiment.sst_classifier import LstmClassifier\nfrom examples.sentiment.sst_reader import \nStanfordSentimentTreeBankDatasetReaderWithTokenizer\n2 There is another toolkit called AllenNLP Interpret (https://allennlp.org/interpret) that offers a similar set of\nfeatures for understanding NLP models, although it is specifically designed to interact with AllenNLP models.\n",
      "page_number": 312
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 320-327)",
      "start_page": 320,
      "end_page": 327,
      "detection_method": "topic_boundary",
      "content": "300\nCHAPTER 11\nDeploying and serving NLP applications\nThe next code shows how to define a dataset for LIT. Here, we are creating a toy data-\nset that consists of just four hardcoded examples, but in practice, you may want to\nread a real dataset that you want to explore. Remember to define the spec() method\nthat returns the type specification of the dataset:\nclass SSTData(lit_dataset.Dataset):\n    def __init__(self, labels):\n        self._labels = labels\n        self._examples = [\n            {'sentence': 'This is the best movie ever!!!', 'label': '4'},\n            {'sentence': 'A good movie.', 'label': '3'},\n            {'sentence': 'A mediocre movie.', 'label': '1'},\n            {'sentence': 'It was such an awful movie...', 'label': '0'}\n        ]\n    def spec(self):\n        return {\n            'sentence': lit_types.TextSegment(),\n            'label': lit_types.CategoryLabel(vocab=self._labels)\n        }\nNow, we are ready to define the main model, as shown next.\nclass SentimentClassifierModel(lit_model.Model):\n    def __init__(self):\n        cuda_device = 0\n        archive_file = 'model/model.tar.gz'\n        predictor_name = 'sentence_classifier_predictor'\n        archive = load_archive(                         \n            archive_file=archive_file,\n            cuda_device=cuda_device\n        )\n        predictor = Predictor.from_archive(archive, \npredictor_name=predictor_name)\n        self.predictor = predictor                         \n        label_map = \narchive.model.vocab.get_index_to_token_vocabulary('labels')\n        self.labels = [label for _, label in sorted(label_map.items())]\n    def predict_minibatch(self, inputs):\n        for inst in inputs:\n            pred = self.predictor.predict(inst['sentence'])     \n            tokens = self.predictor._tokenizer.tokenize(inst['sentence'])\n            yield {\n                'tokens': tokens,\n                'probas': np.array(pred['probs']),\n                'cls_emb': np.array(pred['cls_emb'])\n            }\nListing 11.2\nDefining the main model for LIT\nLoads the \nAllenNLP archive\nExtracts and sets \nthe predictor\nRuns the predict\nmethod of\nthe predictor\n\n\n301\nInterpreting and visualizing model predictions\n    def input_spec(self):\n        return {\n            \"sentence\": lit_types.TextSegment(),\n            \"label\": lit_types.CategoryLabel(vocab=self.labels, \nrequired=False)\n        }\n    def output_spec(self):\n        return {\n            \"tokens\": lit_types.Tokens(),\n            \"probas\": lit_types.MulticlassPreds(parent=\"label\", \nvocab=self.labels),\n            \"cls_emb\": lit_types.Embeddings()\n        }\nIn the constructor (__init__), we are loading an AllenNLP model from an archive\nfile and creating a predictor from it. We are assuming that your model is put under\nmodel/model.tar.gz and hard-coding its path, but feel free to modify this, depending\non where your model is located.\n The model prediction is computed in predict_minibatch(). Given the input\n(which is simply an array of dataset instances), it runs the model via the predictor and\nreturns the result. Note that the predictions are made instance-by-instance, although\nin practice, you should consider making predictions in batches because it will improve\nthroughput for larger input data. The method also returns the embeddings for pre-\ndicted classes (as cls_emb), which will be used for visualizing embeddings (figure\n11.5).\n Finally, here’s the code for running the LIT server:\nmodel = SentimentClassifierModel()\nmodels = {\"sst\": model}\ndatasets = {\"sst\": SSTData(labels=model.labels)}\nlit_demo = dev_server.Server(models, datasets, **server_flags.get_flags())\nlit_demo.serve()\nAfter running the script above, go to http:/./localhost:5432/ on your browser.\nYou should see a screen similar to the one shown in figure 11.5. You can see an array\nof panels corresponding to various information about the data and predictions,\nincluding embeddings, the dataset table and editor, classification results, and saliency\nmaps (which shows contributions of tokens computed via an automated method\ncalled LIME 3).\n Visualizing and interacting with model predictions are a great way to get insights\ninto how the model works and how you should improve it. \n \n3 Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” (2016). https://\narxiv.org/abs/1602.04938.\n\n\n302\nCHAPTER 11\nDeploying and serving NLP applications\nFigure 11.5\nLIT can show saliency maps, aggregate statistics, and embeddings for analyzing your \nmodel and predictions.\n11.5\nWhere to go from here\nIn this book, we’ve only scratched the surface of this vast, long-historied field of natu-\nral language processing. If you are interested in learning the practical aspects of NLP\nfurther, Natural Language Processing in Action by Hobson Lane and others (Manning\nPublications, 2019) and Practical Natural Language Processing by Sowmya Vajjala and\nothers (O’Reilly, 2020) can be a good next step. Machine Learning Engineering by\nAndriy Burkov (True Positive Inc., 2020) is also a good book to learn engineering top-\nics for machine learning in general.\n If you are interested in learning more mathematical and theoretical aspects of\nNLP, I’d recommend giving some popular textbooks a try, such as Speech and Language\nProcessing by Dan Jurafsky and James H. Martin (Prentice Hall, 2008)4 and Introduction\nto Natural Language Processing by Jacob Eisenstein (MIT Press, 2019). Foundations of Sta-\ntistical Natural Language Processing by Christopher D. Manning and Hinrich Schütze\n(Cambridge, 1999), though a bit outdated, is also a classic textbook that can give you\na solid foundation for a wide variety of NLP methods and models.\n4 You can read the draft of the third edition (2021) for free at https://web.stanford.edu/~jurafsky/slp3/.\n\n\n303\nSummary\n Also remember that you can often find great resources online for free. A free Allen-\nNLP course, “A Guide to Natural Language Processing with AllenNLP” (https://guide\n.allennlp.org/), and the documentation for Hugging Face Transformers (https://\nhuggingface.co/transformers/index.html) are great places to go to if you want to learn\nthose libraries in depth.\n Finally, the most effective way to learn NLP is actually doing it yourself. If you have\nproblems for your hobby, work, or anything that involves dealing with natural lan-\nguage text, think whether any of the techniques you learned in this book are applica-\nble. Is it a classification, tagging, or sequence-to-sequence problem? Which models do\nyou use? How do you get the training data? How do you evaluate your model? If you\ndon’t have NLP problems laying around, don’t worry—head over to Kaggle, where\nyou can find a number of NLP-related competitions in which you can “get your hands\ndirty” and gain NLP experience while working on real-world problems. NLP confer-\nences and workshops often host shared tasks, where participants can compete on a\ncommon task, datasets, and evaluation metrics, which are also a great way to learn fur-\nther if you want to deep dive into a particular field of NLP.\nSummary\nMachine learning code is usually a small portion in real-world NLP/ML systems,\nsupported by a complex infrastructure for data collection, feature extraction,\nand model serving and monitoring.\nNLP modules can be developed as a one-off script, a batch prediction service,\nor a real-time prediction service. \nIt is important to version-control your model and data, in addition to the source\ncode. Beware of train-serve skew that causes discrepancies between the training\nand the testing times.\nYou can easily serve PyTorch models with TorchServe and deploy them to Ama-\nzon SageMaker.\nExplainable AI is a new field for explaining and interpreting ML models and\ntheir predictions. You can use LIT (Language Interpretability Tool) to visualize\nand interpret model predictions.\n\n\n304\nCHAPTER 11\nDeploying and serving NLP applications\n\n\n305\nindex\nNumerics\n2-D convolution 176\nA\nabstraction 82–84\naccuracy 93–94\nactivation functions 84\nAdam optimizer 102\nadaptation 221\nadapting BERT 226–229\nadd_special_tokens 245\nadequacy 13, 163\nAI (artificial intelligence) 8–10\nALBERT 241–243\nalignment 144\nAllenNLP\nimplementing Skip-gram on 62–66\nloading SST datasets 33\nSeq2Seq encoder in 117–118\ntraining pipelines 96–102\nbuilding model 100–101\nconfiguring 102–105\ninstances and fields 97–98\nputting all together 101–102\ntoken embedders and RNNs 99–100\nvocabulary and token indexers 98–99\nTransformers with 246–251\nallennlp command 104\nallennlp-models package 33\nallennlp-modules package 230\nallennlp-server plugin 47\nAllentune library 276\nambiguity 5, 127\nAMIs (Amazon Machine Images) 291\nanalysis 22\napplications 13–15, 21–48, 255–303\narchitecting 281–286\nbefore machine learning 282\nchoosing right architecture 282–283\nproject structure 283–284\nversion control 285–286\navoiding overfitting 265–270\ncross-validation 269–270\nearly stopping 268–269\nregularization 265–267\nbatching instances 256–261\nmasking 259–261\npadding 256–257\nsorting 257–259\ncase study 292–298\ndeploying models with SageMaker 296–298\nserving models with TorchServe 292–296\ndealing with imbalanced dataset 270–273\nupsampling and downsampling 271–272\nuse appropriate evaluation metrics 270–271\nweighting losses 272–273\ndeploying 46–48, 286–292\nmaking predictions 46\nmonitoring 289\nserving predictions 46–48\ntesting 286–287\ntrain-serve skew 288–289\nusing GPUs (graphics processing units)\n289–292\ndevelopment of 21–24\nanalysis and experimenting 22\n\n\nINDEX\n306\napplications (continued)\ndata collection 22\ndeploying 23\nimplementation 23\nmonitoring 23–24\ntraining 23\ndialog systems 15\nevaluating classifier 45–46\ngrammatical and spelling error correction\n13–14\nhyperparameters 273–278\nexamples of 274\ngrid search vs. random search 275–276\ntuning with Optuna 276–278\ninterpreting and visualizing model \npredictions 298–301\nloss functions and optimization 41–43\nmachine translation (MT) 13\nneural networks 37–41\narchitecture for sentiment analysis 39–41\ndefined 37–38\nRNNs and linear layers 38–39\nsearch engine 14\nsentiment analysis 27\nstructure of 24–25\ntokenization for neural models 261–265\ncharacter models 262–263\nsubword models 263–265\nunknown words 261–262\ntraining classifier 43–44\nbatching 43–44\nputting everything together 44\nword embeddings 34–37\ndefined 34–36\nfor sentiment analysis 36–37\nworking with NLP datasets 28–33\ndefined 28–29\nloading Stanford Sentiment Treebank (SST) \ndatasets using AllenNLP 33\nStanford Sentiment Treebank (SST) 29–30\ntrain, validation, and test sets 30–32\n- -arch lstm 151, 189\n- -arch transformer option 197\narchitecting applications\nbefore machine learning 282\nchoosing right architecture 282–283\nproject structure 283–284\nversion control 285–286\narchitecture 34\nartificial intelligence (AI) 8–10\nartificial neural networks 37\nassociation 58\nattention 184, 186, 190–217\nlimitation of vanilla Seq2Seq models 185–186\nmechanism 186–187\nself-attention 192–200\nsequence-to-sequence with 187–192\nbuilding Seq2Seq machine translation (MT) \nwith attention 189–192\nencoder-decoder attention 188–189\nspell-checker case study 208–217\nimproving spell-checker 213–217\nspell correction as machine translation\n208–210\ntraining spell-checker 210–213\nTransformer-based language models 200–208\nGPT-2 (generative pretraining) 205–207\nTransformer as language model 200–203\nTransformer-XL 203–205\nXLM 207–208\nattention_mask tensor 231–232\nAutoModel.from_pretrained() class method 232\nAutoModelWithLMHead class 203–204\nautoregressive models 137, 157\nAutoTokenizer class 203\nAutoTokenizer.from_pretrained() class \nmethod 230\nAWS (Amazon Web Services) 291\nB\nb constant 60, 87, 179\nb parameter 59, 85, 89\nbackpropagation 42, 90\nbag of embeddings 227\nBagOfEmbeddingsEncoder 182\nBaseHandler 295\nbasic_classifier 248\nBasicTextFieldEmbedder 36\nbatch_size parameter 44\nbatching\nclassifiers 43–44\ninstances 256–261\nmasking 259–261\npadding 256–257\nsorting 257–259\nBCEWithLogitsLoss 273\n- -beam option 163\nbeam search decoding 161–163\nbeginning of sentence (BOS) token 226\nBERT (Bidirectional Encoder Representations \nfrom Transformers) 222–229\nadapting 226–229\nlimitations of word embeddings 222–224\nnatural language inference with BERT case \nstudy 243–251\n\n\nINDEX\n307\nnatural language inference (NLI) \ndefined 243–244\nusing BERT for sentence-pair \nclassification 244–246\nusing Transformers with AllenNLP 246–251\npretraining 225–226\nself-supervised learning 224\nsentiment analysis with BERT case study\n229–236\nbuilding model 232–233\ntokenizing input 230–231\ntraining model 233–236\nbert key 249\nbert_pooler 249\nbert-base-cased 229, 240\nBertPooler 249\nbias 59\nBidirectional Encoder Representations from \nTransformers. See BERT\nbidirectional RNN (recurrent neural \nnetwork) 124–125\nbigrams 54\nbinary classification 27\nbitexts 146\nBLEU (bilingual evaluation understudy) 152, 164\nBOS (beginning of sentence) token 226\nBPE (byte-pair encoding) 55, 263\nbucket_name variable 297\nBucketBatchSampler 44, 98, 259\nbyte-pair encoding (BPE) 55, 263\nC\n__call__ method 230\nCAT (computer-aided translation) 147\ncausal language modeling (CLM) 207\nCBOW (continuous bag of words) model 67–68\ncells 83, 172\nchain rule 132\nchannels 176\ncharacters 52, 134, 210\ncharacter models 262–263\nusing as input 106\nCharacterTokenizer 108, 134\nchatbot case study 165–170\ndialogue systems 165–166\npreparing dataset 166–167\ntraining and running chatbot 167–169\nCI (continuous integration) 23, 287\nCL (computational linguistics) 9\nclassification 27\nclassification boundary 266\nclassification head 227\nclassifiers\nevaluating 45–46\ntraining 43–44\nbatching 43–44\nputting everything together 44\nCLM (causal language modeling) 207\ncloze tests 225\nCLS token 230, 244, 249\nCnnEncoder 181–182\nCNNs (convolutional neural networks) 171–183\nconvolutional layers 175–179\ncombining scores 178–179\npattern matching using filters 175–176\nrectified linear unit (ReLU) 176–178\noverview 174\npattern matching for sentence classification 173\npooling layers 179–180\nrecurrent neural networks (RNNs) \nshortcomings 172–173\ntext classification case study 180–183\ntext classification defined 180–181\ntraining and running classifier 182–183\nusing CnnEncoder 181–182\ncomputer-aided translation (CAT) 147\nconditional generation task 202\nconditional language models 131, 156\nCoNLL-U format 119\ncontextualization 223\ncontextualized embeddings 222–223\ncontinuous bag of words (CBOW) model 67–68\nconvolution 174\nconvolutional layers 175–179\ncombining scores 178–179\npattern matching using filters 175–176\nrectified linear unit (ReLU) 176–178\ncoreference resolution 170\ncosine similarity 65\ncost 41\ncross entropy 62, 120\ncross-attention 187, 196\ncross-entropy loss 41\ncross-validation 269–270\ncuda_device parameter 44, 291\nD\ndata collection 22\ndata-to-text generation 20\nDataLoader 44, 259\ndataset_reader key 103\nDatasetReader 33, 97, 103, 107\n",
      "page_number": 320
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 328-335)",
      "start_page": 328,
      "end_page": 335,
      "detection_method": "topic_boundary",
      "content": "INDEX\n308\ndatasets 28–33\nchatbot case study 166–167\ncreating dataset reader 106–108\ndefined 28–29\nimbalanced datasets 270–273\nupsampling and downsampling 271–272\nuse appropriate evaluation metrics 270–271\nweighting losses 272–273\nloading Stanford Sentiment Treebank (SST) \ndatasets using AllenNLP 33\nPOS (part-of-speech) tagging 118–119\nStanford Sentiment Treebank (SST) 29–30\ntrain, validation, and test sets 30–32\ntranslators 148–150\ndecoder\nencoder-decoder attention 188–189\noverview 156–158\ndeep learning (DL) 8–10\ndependency parsing 19\ndependency trees 19\ndeploying\napplications 23\nNLP model 286–292\nmonitoring 289\ntesting 286–287\ntrain-serve skew 288–289\nusing GPUs 289–292\ndetectors 110–111\ndev or development sets 30\ndialogue systems 15, 165–166\ndict 45, 59, 65, 70, 104\n- -direction maximize 277\nDistilBERT 240\nDistilGPT2 292\ndistillation 240, 292\ndistributed memory model of paragraph vectors \n(PV-DM) 74\ndistributed representations 58\ndistributional hypothesis 58\nDL (deep learning) 8–10\nDoc2Vec class 76\nDoc2Vec model 74\ndocument classification 180\ndocument-level embeddings 74–76\ndomain adaptation 222\ndownsampling 271–272\ndropout 267\nE\nearly stopping 152, 268–269\necho command 213\nElman RNN 84, 172\nELMo (Embeddings from Language \nModels) 236–237\nembedder 181\nEmbedding class 99\nEMBEDDING_DIM 36, 39, 64\nembedding_dim 100, 182\nEMNLP (Empirical Methods in Natural Language \nProcessing) 10\nencoder\nencoder-decoder attention 188–189\noverview 154–155\n- -[encoder|decoder]-embed-dim parameter 212\n- -[encoder|decoder]-attention-head \nparameter 212\n- -[encoder|decoder]-layers parameter 212\nencoder parameter 104\n- -[encoder/decoder]-ffn-embed-dim \nparameter 212\nentity linking 127\nEOS (end of sentence) token 226\nepochs 31\neval() method 289\nevaluation metrics 270–271\nEWT (English Web Treebank) 119\nF\nf function 187–189, 193\nF-measure 96\nf() function 84, 86, 90\nF1-measure 96, 129, 164\nfairseq-interactive command 153, 168, 189, 191, \n215\nfairseq-preprocess command 167, 189, 212, 214\nfairseq-train command 150, 152, 167, 189, 197, \n215\nfalse negatives (FN) 95\nfalse positives (FP) 95\nfastText 72–74\nsubword information 72–73\nusing toolkit 73–74\nfeature extraction 226, 228\nfidelity 163\nfields 97–98\nfilters 175–176\nfine-tuning 221, 226\nfluency 13, 163\nFN (false negatives) 95\nfolds 269\nforget() function 91–92\nform_instances() function 99\nforward method 260\nforward pass 42\n\n\nINDEX\n309\nforward_on_instances() method 110\nforward() function 40\nforward() method 100, 232\nFP (False positives) 95\nfrom_pretrained() method 203–204\nfully connected layers 39\nG\nGANs (generative adversarial networks) 21\nGated Recurrent Units (GRUs) 92–93\ngating 92\ngazetteers 127\nGBDTs (gradient-boosted decision trees) 282\ngenerative adversarial networks (GANs) 21\ngenerative pretraining (GPT-2) 205–207\nGeorgetown–IBM Experiment, The 10\nget_metrics() method 45\nget_text_field_mask() helper function 260\ngisting 147\nGloVe 68–71\nlearning word embeddings 68\nusing pretrained vectors 69–71\nGPT-2 (generative pretraining) 205–207\nGPUs (graphics processing units) 43, 289–292\ngradient-boosted decision trees (GBDTs) 282\ngradients 90\ngrammatical error correction 13–14\ngraphemes 52\ngreedy decoding 158–161\ngrid search 275–276\nGRUs (Gated Recurrent Units) 92–93\nH\nhandle() method 295\nhandler option 295\nhead command 166\nhidden state 172\nHIDDEN_DIM 39\nhidden_state 92\nHMMs (hidden Markov models) 16\nhyperbolic tangent function 86\nhyperparameters 31, 273–278\nexamples of 274\ngrid search vs. random search 275–276\ntuning with Optuna 276–278\nhypotheses 161, 243\nI\nICA (independent component analysis) 77\nIllustrated Transformer, The 194\nimage captioning 20\nimbalanced datasets 93, 270–273\nupsampling and downsampling 271–272\nuse appropriate evaluation metrics 270–271\nweighting losses 272–273\nimplementation phase 23\nin_features 40\n- -include-package 105\nindependent component analysis (ICA) 77\ninference() method 295\n__init__() method 232\n(__init__) constructor 301\ninit_state() function 136\ninitialize() method 295\ninner product 179\ninput_ids tensor 231–232\ninstances 28, 97–98\nbatching 256–261\nmasking 259–261\npadding 256–257\nsorting 257–259\nrunning detector on unseen 110–111\ninterpreting model predictions 298–301\nIOB2 tagging 128\nis_grammatical() function 89–91\nJ\nJsonnet format 103\nK\nk-fold cross validation 269\nkernels 176\nknowledge distillation 240, 292\nL\nL2 regularization 267\nlabels 29\nlanguage building blocks 52–54\ncharacters 52\nn-grams 53–54\nwords, tokens, morphemes, and phrases 53\nlanguage detection case study 105–111\nbuilding training pipeline 108–110\ncreating dataset reader 106–108\nrunning detector on unseen instances\n110–111\nusing characters as input 106\nLanguage Interpretability Tool (LIT) 280, 299\nlanguage model (LM) head 204\n\n\nINDEX\n310\nlanguage modeling 130–133\nbased on Transformer 200–208\nGPT-2 (generative pretraining) 205–207\nTransformer as language model 200–203\nTransformer-XL 203–205\nXLM 207–208\ndefined 130–131\nimportance of 131–132\ntext generation using RNNs 133–138\nevaluating text using language model 134–136\nfeeding characters to an recurrent neural net-\nwork (RNN) 134\ngenerating text using language model\n136–138\ntraining RNN language model 132–133\nlanguage models 130–131, 146\nlanguage pair 144\nlast_hidden_state 232\nlayers 9, 39\nlemmatization 56–57\nlinear layers 38–39, 59–60\nlinear() function 137\nlinear2() function 84\nLIT (Language Interpretability Tool) 280, 299\nLM (language model) head 204\nlocally sensitive hashing (LSH) 70\nlogits 41, 204\nlong short-term memory (LSTM) 39, 90–92\nloss 41\nloss functions 41–43\nPOS (part-of-speech) tagging 119–121\nweighting losses 272–273\n'loss' key 65\nloss.backward() function 235\nLSH (locally sensitive hashing) 70\nLSTM (long short-term memory) 39, 90–92\nLstmClassifier class 104, 108\nM\nmacro average 129\nmajority class baseline 270\n[MASK] 225–226, 238\nmasked language model (MLM) 225, 238\nmasking 259–261\nmax pooling 179\n- -max-tokens parameter 212\nmean over time 227\n- -metrics best_validation_accuracy 277\nmicro average 129\nmin_count 64, 99\nML (machine learning) 8–10\narchitecting applications before 282\ntraditional 219–220\nMLM (masked language model) 225, 238\nModel class 100\nmodel selection 30, 148\nmodel_store 295\nmodel.eval() function 235, 289\nmodel() function 201\nmodified precision 164\nModule class 101\nmonitoring\napplications 23–24\ndeployed NLP model 289\nmorphemes 53\nmorphological analysis 53\nMT (machine translation) 13, 144–147\nbuilding Seq2Seq MT with attention 189–192\nspell correction as 208–210\nmulticlass classification 27\nmultihead self-attention 194\nmultilayer perceptron 60\nmultilayer RNNs (recurrent neural \nnetworks) 122–124\nN\nn-grams 53–54\nnamespaces 98\nnatural language generation (NLG) 20\nnegative classes 94\nNER (named entity recognition) 126–130\ndefined 127–128\nimplementing named entity recognizer\n128–130\ntagging spans 128\nneural machine translation (NMT) 123, 146\nneural network models 37–41\narchitecture for sentiment analysis 39–41\ndefined 37–38\nRNNs and linear layers 38–39\ntokenization for 261–265\ncharacter models 262–263\nsubword models 263–265\nunknown words 261–262\nneural networks, defined 37\nnext-sentence prediction (NSP) 226, 239, 242\nngram_filter_sizes argument 182\nNLG (natural language generation) 20\nNLI (natural language inference) 243–251\ndefined 243–244\nusing BERT for sentence-pair classification\n244–246\nusing Transformers with AllenNLP 246–251\nNLP (natural language processing) 3–25\napplications 13–15, 21–48, 255–303\n\n\nINDEX\n311\narchitecting 281–286\navoiding overfitting 265–270\nbatching instances 256–261\ncase study 292–298\ndealing with imbalanced dataset 270–273\ndeploying 46–48, 286–292\ndevelopment of 21–24\ndialog systems 15\nevaluating classifier 45–46\ngrammatical and spelling error \ncorrection 13–14\nhyperparameters 273–278\ninterpreting and visualizing model \npredictions 298–301\nloss functions and optimization 41–43\nmachine translation (MT) 13\nneural networks 37–41\nsearch engine 14\nsentiment analysis 27\nstructure of 24–25\ntokenization for neural models 261–265\ntraining classifier 43–44\nword embeddings 34–37\nworking with NLP datasets 28–33\nartificial intelligence, machine learning, deep \nlearning 8–10\ndefined 4–6\nreasons for 10–12\ntasks 15–21\nparsing 17–20\npart of speech (POS) tagging 16–17\ntext classification 15\ntext generation 20–21\nwhat is not 6–8\nNMT (neural machine translation) 123, 146\nnn.CrossEntropyLoss function 232\n- -no-cache-dir transformers 297\nnonlinearity 84–88\nNP (noun phrase) 17\nnp.argmax 110\nNSP (next-sentence prediction) 226, 239, 242\nnum_embeddings 100\nnum_filters argument 182\nnum_labels parameter 232\n—num_layers flag 125\nnum_layers parameter 124\nnvidia-smi command 290\nO\none-hot vectors 36\nOOV (out-of-vocabulary) 56, 72, 134, 192, 261\noptimization 41–43, 60\noptimizer.step() function 235\noptimizers 42\nOptuna 276–278\nout_feature 40\noverfitting 31, 265–270\ncross-validation 269–270\nearly stopping 268–269\nregularization 265–267\ndropout 267\nL2 regularization 267\nP\npadding 256–257\nparagraph vectors 74\nparallel corpora 146\nparameters 37, 60, 273\nparse trees 18\nparsing 17–20\npart-of-speech. See POS (part-of-speech)\npast variables 204\npaste command 166\npatience 152\npattern matching\nfor sentence classification 173\nusing filters 175–176\nPCA (principal component analysis) 77\nPenn Treebank (PTB) 28\nperceptrons 60\nperplexity 136\nphrases 53\npip install annoy 70\npip install fairseq 147\npip install gensim 75\npip install nltk 54\npip install scikit-learn 77\npip install spacy 54\npip install transformers 203\nPLMs (pretrained language models) 218–251\nALBERT 241–243\nBERT (Bidirectional Encoder Representations \nfrom Transformers) 222–229\nadapting 226–229\nlimitations of word embeddings 222–224\nnatural language inference with BERT case \nstudy 243–251\npretraining 225–226\nself-supervised learning 224\nsentiment analysis with BERT case study\n229–236\nDistilBERT 240\nELMo (Embeddings from Language \nModels) 236–237\n\n\nINDEX\n312\nPLMs (pretrained language models) (continued)\nRoBERTa 239–240\ntransfer learning 219–222\noverview 220–222\ntraditional machine learning 219–220\nword embeddings 220\nXLNet 237–238\npolarity 27, 181\npooler_output 232\npooling layers 179–180\nPorter stemming algorithm 56\nPorter, Martin 56\nPorterStemmer class 56\nPOS (part-of-speech) 16, 112–113, 132, 142, 219\nPOS (part-of-speech) tagging 16–17, 118–122\nbuilding training pipeline 121–122\ndefining model and loss 119–121\nreading dataset 118–119\npos_weight parameter 273\npositional encoding 196\npositive classes 94\npositive_label 104\nPOST requests 47\npostediting 147\nPP (prepositional phrase) 19\nPP-attachment problem 6\nprecision 94–95\npredict_minibatch() function 301\npredict() method 130\npredictions\nmaking 46\nserving 46–48\npredictors 46\npremises 243\npretrained GloVe vectors 69–71\npretrained_transformer 247–248\nPretrainedTransformerEmbedder 248\nPretrainedTransformerIndexer 248\nPretrainedTransformerTokenizer 247\npretraining 221\npretraining BERT 225–226\nprincipal component analysis (PCA) 77\nprint statement 260\nprocess_adverb() 89\nprocess_main_subject() 89–90\nprocess_main_verb 89\nproject structure 283–284\nPTB (Penn Treebank) 28\nPV-DM (distributed memory model of paragraph \nvectors) 74\npython -m spacy download en 54\nPytorchSeq2SeqWrapper 117\nPytorchSeq2VecWrapper 39, 100, 117, 119\nR\nrandom baseline 270\nrandom search 275–276\nread_corpus() method 75\nread_dataset 231\nread() method 97, 107\nreal-time streaming frameworks 283\nrecall 94–95\nrectified linear units 178\nrecurrent neural networks. See RNNs (recurrent \nneural networks)\nregularization 265–267\ndropout 267\nL2 regularization 267\nrelation extraction 127\nReLU (rectified linear unit) 176–178\nrnn_vec() function 115–116\nrnn() function 88\nRNNs (recurrent neural networks) 17, 38–39, 112, \n184\nencoding sequences 115–117\nsentence classification 81–88\nabstraction 82–84\nhandling variable-length input 81–82\nsimple and nonlinearity 84–88\nshortcomings of 172–173\ntext generation using 133–138\nevaluating text using language model\n134–136\nfeeding characters to RNN 134\ngenerating text using language model\n136–138\ntoken embedders 99–100\ntraining RNN language model 132–133\nRoBERTa (robustly optimized BERT) 239–240\nS\nSageMaker 296–298\nsample_token() method 205\nsample() function 137\nscheduler.step() function 235\nscores, combining 178–179\nsearch engine 14\nself-attention 192–200\nself-supervised learning 61, 224\nsentence classification 80–111\naccuracy 93–94\nAllenNLP training pipelines 96–102\nbuilding model 100–101\nconfiguring 102–105\ninstances and fields 97–98\n\n\nINDEX\n313\nputting all together 101–102\ntoken embedders and RNNs 99–100\nvocabulary and token indexers 98–99\nF-measure 96\nGRUs (Gated Recurrent Units) 92–93\nlanguage detection case study 105–111\nbuilding training pipeline 108–110\ncreating dataset reader 106–108\nrunning detector on unseen instances\n110–111\nusing characters as input 106\nLSTM (long short-term memory) 90–92\npattern matching for 173\nprecision and recall 94–95\nRNNs (recurrent neural networks) 81–88\nabstraction 82–84\nhandling variable-length input 81–82\nsimple and nonlinearity 84–88\nsentence-pair classification 244–246\nvanishing gradients problem 88–90\nsentence pairs 148\nsentence pieces 210\nsentence-order prediction (SOP) 226, 242\nsentence-prediction task 226\nSentenceClassifierPredictor 46\nsentiment analysis 15\nneural networks for 39–41\noverview 27\nwith BERT case study 229–236\nbuilding model 232–233\ntokenizing input 230–231\ntraining model 233–236\nword embeddings for 36–37\n[SEP] token 230, 244, 246\nSeq2Seq (sequence-to-sequence) models\n141–170\nbuilding translator 147–154\npreparing datasets 148–150\nrunning translator 153–154\ntraining model 150–153\nchatbot case study 165–170\ndialogue systems 165–166\npreparing dataset 166–167\ntraining and running chatbot 167–169\ncomponents of 154–163\nbeam search decoding 161–163\ndecoder 156–158\nencoder 154–155\ngreedy decoding 158–161\nencoder in AllenNLP 117–118\nevaluating translation systems 163–165\nautomatic evaluation 163–165\nhuman evaluation 163\nlimitation of vanilla Seq2Seq models 185–186\nmachine translation (MT) 144–147\noverview 142–144\nwith attention 187–192\nbuilding Seq2Seq machine translation (MT) \nwith attention 189–192\nencoder-decoder attention 188–189\nSeq2SeqEncoder 117\nSeq2VecEncoder 181–182, 248–249\nsequence_cross_entropy_with_logits() \nfunction 260–261\nsequential labeling 112–138\nbidirectional RNN 124–125\nbuilding part-of-speech tagger 118–122\nbuilding training pipeline 121–122\ndefining model and loss 119–121\nreading dataset 118–119\ndefined 113–114\nimplementing Seq2Seq encoder in \nAllenNLP 117–118\nmultilayer RNNs 122–124\nnamed entity recognition 126–130\ndefined 127–128\nimplementing named entity recognizer\n128–130\ntagging spans 128\nusing RNNs to encode sequences 115–117\n- -serialization-dir 105\nserialized-file 295\nSGD (stochastic gradient descent) 42\nsimple RNNs (recurrent neural networks) 84–88\nSingleIdTokenIndexer 135\nSkip-gram 57–68\nCBOW (continuous bag of words) model\n67–68\nimplementing on AllenNLP 62–66\nlinear layers 59–60\norigin of word embeddings 57–58\nSoftmax 61–62\nword associations 58–59\nSMT (statistical machine translation) 146\nSNLI (Standard Natural Language Inference) 244\nsnli type 247\nSnliReader 245, 247\nSoftmax 61–62\nsoftmax() function 137\nSOP (sentence-order prediction) 242\nsorting 257–259\nsorting_keys 259\nsource language 13, 144\nSpanBasedF1Measure 129\nspans, tagging 128\nspec() method 300\n\n\nINDEX\n314\nspell-checker case study 208–217\nimproving spell-checker 213–217\nspell correction as machine translation 208–210\ntraining spell-checker 210–213\nspelling error correction 13–14\nSST (Stanford Sentiment Treebank) datasets\nloading using AllenNLP 33\noverview 29–30\nsst_tokens 103\nStanfordSentimentTreeBankDatasetReader 33, \n97, 103, 107\nstate 82–83, 85, 89, 115–117\nstdout 236, 250\nstemming 55–56\nstochastic gradient descent (SGD) 42\nstratified sampling 272\nstudent model 240, 292\nsubword information 72–73\nsubword models 263–265\nsubword units 246\nsupervised machine learning paradigm 27\nswitch 92\nsyntactic ambiguity 6\nT\nt-SNE (t-distributed Stochastic Neighbor \nEmbedding) 77\nTaggedDocument 75\ntagging\nPOS (part-of-speech) 16–17\nspans 128\ntanh 86\ntarget language 13, 144\ntasks 15–21\nparsing 17–20\nPOS (part-of-speech) tagging 16–17\ntext classification 15\ntext generation 20–21\nteacher model 240, 292\ntest instances 46\ntest sets 30–32\ntesting, NLP model 286–287\ntext classification\ncase study 180–183\ntext classification defined 180–181\ntraining and running classifier 182–183\nusing CnnEncoder 181–182\noverview 15\ntext generation 20–21, 133–138\nevaluating text using language model 134–136\nfeeding characters to RNN 134\nusing language model 136–138\ntext mining 10\ntext_to_instance() method 107\ntext-to-text generation 20\nTextFieldEmbedder 181, 248\nTN (true negatives) 95\nto() method 291\ntoken_embedders parameter 249\ntoken_in 63, 65\ntoken_indexers 135, 247, 249\ntoken_min_padding_length parameter 182\ntoken_out 63, 65\ntoken_type_ids tensor 231–232\nTokenEmbedder 99\ntokenization 54–55\nBERT case study 230–231\nfor neural models 261–265\ncharacter models 262–263\nsubword models 263–265\nunknown words 261–262\ntokens\ndefined 53\ntoken embedders 99–100\ntoken indexers 98–99\ntokenize() method 108\n—tokenizer parameter 247\ntokenizer.decode() method 204–205\ntokenizer() function 231\ntokens 41, 53, 68–69, 97, 135\ntoolkit, fastText 73–74\ntorch-model-archiver 295\ntorch.multinomial() function 137\ntorch.nn.LSTM 100, 117\nTorchServe 292–296\nTP (true positives) 94\ntrain (or training) sets 30\ntrain method 44\ntrain sets 30–32\ntrain_data_path key 103\ntrain-serve skew 288–289\ntrain() function 102\ntrain() method 289\nTrainer class 44\ntraining\napplications 23\nchatbot case study 167–169\nclassifiers 43–44\nbatching 43–44\nputting everything together 44\nRNN language model 132–133\nspell-checker 210–213\ntranslators 150–153\ntraining pipelines\nAllenNLP 96–102\n\n\nINDEX\n315\nbuilding model 100–101\nconfiguring 102–105\ninstances and fields 97–98\nputting all together 101–102\ntoken embedders and recurrent neural net-\nworks (RNNs) 99–100\nvocabulary and token indexers 98–99\nlanguage detection case study 108–110\nPOS (part-of-speech) tagging 121–122\ntraining-serving skew 46\ntransfer learning 219–222\noverview 220–222\ntraditional machine learning 219–220\nword embeddings 220\ntransformation invariant CNNs 180\nTransformer\nlanguage models based on 200–208\nGPT-2 (generative pretraining) 205–207\nTransformer as language model 200–203\nTransformer-XL 203–205\nXLM 207–208\nself-attention and 192–200\nwith AllenNLP 246–251\ntransformers library 202–203, 206\ntranslation models 146\ntranslation systems 163–165\nautomatic evaluation 163–165\nhuman evaluation 163\ntranslators 147–154\npreparing datasets 148–150\nrunning translator 153–154\ntraining model 150–153\ntreebanks 28\ntrials 275\ntrigram 54\ntype key 104\ntypes 68–69\nU\nUD (Universal Dependencies) 20, 118\nunconditional generation 202\nunconditional language models 131, 156\nunigram 54\nuniversal part-of-speech tagsets 16, 118\nUniversalDependenciesDatasetReader 119\nUniversalPOSPredictor 122\n<UNK> 99, 134\nUNK tokens 226, 261, 264\nunknown words 261–262\nunrolling 88\nupdate_hidden() function 92\nupdate_simple() function 84, 87\nupdate() function 83–84, 88, 137\nupsampling 271–272\nV\nv() function 116\nvalidation sets 30–32, 191, 267, 282\nvalidation_data_path key 103\nvanilla 172\nvanilla Seq2Seq models 185–186\nvanishing gradients problem 88, 90\nvariable-length input 81–82\nvectors 34\nversion control 285–286\nvisualizing\nmodel predictions 298–301\nword embeddings 76–79\nvocabulary 34, 98–99\nVocabulary class 98–99\nvocabulary items 98\nVocabulary object 98, 261\nVP (verb phrase) 17\nVSO (verb-subject-object) 145\nW\nw constant 60, 179\nw parameter 59\nwarm-up 234\nweight decay 267\nweighting losses 272–273\nweights 37\nWER (word error rate) 164\nWikification 127\nwith torch.no_grad() function 235\nWK projection 193\nword associations 58–59\nword embeddings 34–37, 49–79\nbuilding blocks of language 52–54\ncharacters 52\nn-grams 53–54\nwords, tokens, morphemes, and phrases 53\ndefined 34–36, 50\ndocument-level embeddings 74–76\nfastText 72–74\nsubword information 72–73\nusing toolkit 73–74\nfor sentiment analysis 36–37\nGloVe 68–71\nlearning word embeddings 68\nusing pretrained vectors 69–71\nimportance of 50–52\nlemmatization 56–57\n",
      "page_number": 328
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 336-337)",
      "start_page": 336,
      "end_page": 337,
      "detection_method": "topic_boundary",
      "content": "INDEX\n316\nword embeddings (continued)\nlimitations of 222–224\nSkip-gram 57–68\nCBOW (continuous bag of words) model\n67–68\nimplementing on AllenNLP 62–66\nlinear layers 59–60\norigin of word embeddings 57–58\nSoftmax 61–62\nword associations 58–59\nstemming 55–56\ntokenization 54–55\ntransfer learning 220\nvisualizing 76–79\nword order 144\nword stems 55\nword_embeddings parameter 104\nWordNetLemmatizer 57\nwords 53, 116\nX\nxlim() function 78\nXLM 207–208\nXLM-R (XML-RoBERTa) 240\nXLNet 237–238\nXOR (or exclusive-or) 87\nY\nylim() function 78\nZ\nzero pronoun 146\n\n\nMasato Hagiwara\nISBN: 978-1-61729-642-0\nT\nraining computers to interpret and generate speech and \ntext is a monumental challenge, and the payoff  for reduc-\ning labor and improving human/computer interaction \nis huge! Th e fi eld of Natural Language Processing (NLP) is \nadvancing rapidly, with countless new tools and practices. Th is \nunique book off ers an innovative collection of NLP techniques \nwith applications in machine translation, voice assistants, text \ngeneration, and more.\nReal-world Natural Language Processing shows you how to \nbuild the practical NLP applications that are transforming \nthe way humans and computers work together. Guided by \nclear explanations of each core NLP topic, you’ll create many \ninteresting applications including a sentiment analyzer and a \nchatbot. Along the way, you’ll use Python and open source \nlibraries like AllenNLP and HuggingFace Transformers to \nspeed up your development process. \nWhat’s Inside\n● Design, develop, and deploy useful NLP applications\n● Create named entity taggers\n● Build machine translation systems\n● Construct language generation systems and chatbots\nFor Python programmers. No prior machine learning know-\nledge assumed.\nMasato Hagiwara received his computer science PhD from \nNagoya University in 2009. He has interned at Google and \nMicrosoft Research, and worked at Duolingo as a Senior \nMachine Learning Engineer. He now runs his own research \nand consulting company.\nRegister this print book to get free access to all ebook formats. \nVisit https://www.manning.com/freebook\n$59.99 / Can $79.99  [INCLUDING eBOOK]\nReal-World Natural Language Processing\nDATA SCIENCE/NATURAL LANGUAGE PROCESSING\nM A N N I N G\n“\nTh e defi nitive reference for \nthose of us trying to understand \nNLP and its applications.”\n \n—Richard Vaughan\nPurple Monkey Collective\n“\nVery practical book about \nNLP and how to use it \nsuccessfully in real-world \n applications.”\n \n—Salvatore Campagna, King\n“\nIf you need to step up your \ngame but were turned off  by \na diffi  cult learning curve, \n then this book is for you!”\n \n—Alain Lompo, ISO-Gruppe\n“\nAn excellent and \napproachable fi rst step in \nlearning NLP. Well written \n  and easy to follow.”\n—Marc-Anthony Taylor\nBlackshark.ai\nSee first page\n",
      "page_number": 336
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "M A N N I N G\nMasato Hagiwara\n",
      "content_length": 30,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "302\nThe development cycle of NLP applications\nR\nE\nS\nE\nA\nR\nC\nH\nO\nP\nE\nR\nA\nT\nI\nO\nN\nS\nD\nE\nV\nE\nL\nO\nP\nM\nE\nN\nT\nImplementation\nDeploying\nNLP system \ndevelopment\nprocess\nTraining\nMonitoring\nData\ncollection\nAnalysis &\nexperimenting\n",
      "content_length": 222,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Real-World Natural\nLanguage Processing\nMASATO HAGIWARA\nM A N N I N G\nSHELTER ISLAND\n",
      "content_length": 84,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2021 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nThe author and publisher have made every effort to ensure that the information in this book \nwas correct at press time. The author and publisher do not assume and hereby disclaim any \nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \nof the information herein.\nManning Publications Co.\nDevelopment editor: Karen Miller\n20 Baldwin Road\nTechnical development editor: Mike Shepard\nPO Box 761\nReview editor: Adriana Sabo\nShelter Island, NY 11964\nProduction editor: Deirdre S. Hiam\nCopy editor: Pamela Hunt\nProofreader: Keri Hales\nTechnical proofreader: Mayur Patil\nTypesetter and cover designer: Marija Tudor\nISBN 9781617296420\nPrinted in the United States of America\n",
      "content_length": 2100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "To Daphne, Laurel, and Lynn\n",
      "content_length": 28,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "iv\n \n",
      "content_length": 5,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "v\ncontents\npreface\nxi\nacknowledgments\nxiii\nabout this book\nxiv\nabout the author\nxvii\nabout the cover illustration\nxviii\nPART 1\nBASICS ............................................................ 1\n1 Introduction to natural language processing\n3\n1.1\nWhat is natural language processing (NLP)?\n4\nWhat is NLP?\n4\n■What is not NLP?\n6\n■AI, ML, DL, and \nNLP\n8\n■Why NLP?\n10\n1.2\nHow NLP is used\n12\nNLP applications\n13\n■NLP tasks\n15\n1.3\nBuilding NLP applications\n21\nDevelopment of NLP applications\n21\n■Structure of NLP \napplications\n24\n2 Your first NLP application\n26\n2.1\nIntroducing sentiment analysis\n27\n2.2\nWorking with NLP datasets\n28\nWhat is a dataset?\n28\n■Stanford Sentiment Treebank\n29\nTrain, validation, and test sets\n30\n■Loading SST datasets using \nAllenNLP\n33\n",
      "content_length": 760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "CONTENTS\nvi\n2.3\nUsing word embeddings\n34\nWhat are word embeddings?\n34\n■Using word embeddings \nfor sentiment analysis\n36\n2.4\nNeural networks\n37\nWhat are neural networks?\n37\n■Recurrent neural networks \n(RNNs) and linear layers\n38\n■Architecture for sentiment \nanalysis\n39\n2.5\nLoss functions and optimization\n41\n2.6\nTraining your own classifier\n43\nBatching\n43\n■Putting everything together\n44\n2.7\nEvaluating your classifier\n45\n2.8\nDeploying your application\n46\nMaking predictions\n46\n■Serving predictions\n46\n3 Word and document embeddings\n49\n3.1\nIntroducing embeddings\n50\nWhat are embeddings?\n50\n■Why are embeddings \nimportant?\n50\n3.2\nBuilding blocks of language: Characters, words, \nand phrases\n52\nCharacters\n52\n■Words, tokens, morphemes, and phrases\n53\nN-grams\n53\n3.3\nTokenization, stemming, and lemmatization\n54\nTokenization\n54\n■Stemming\n55\n■Lemmatization\n56\n3.4\nSkip-gram and continuous bag of words (CBOW)\n57\nWhere word embeddings come from\n57\n■Using word \nassociations\n58\n■Linear layers\n59\n■Softmax\n61\nImplementing Skip-gram on AllenNLP\n62\n■Continuous \nbag of words (CBOW) model\n67\n3.5\nGloVe\n68\nHow GloVe learns word embeddings\n68\n■Using pretrained \nGloVe vectors\n69\n3.6\nfastText\n72\nMaking use of subword information\n72\n■Using the \nfastText toolkit\n73\n3.7\nDocument-level embeddings\n74\n3.8\nVisualizing embeddings\n76\n",
      "content_length": 1315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "CONTENTS\nvii\n4 Sentence classification\n80\n4.1\nRecurrent neural networks (RNNs)\n81\nHandling variable-length input\n81\n■RNN abstraction\n82\nSimple RNNs and nonlinearity\n84\n4.2\nLong short-term memory units (LSTMs) and gated \nrecurrent units (GRUs)\n88\nVanishing gradients problem\n88\n■Long short-term memory \n(LSTM)\n90\n■Gated recurrent units (GRUs)\n92\n4.3\nAccuracy, precision, recall, and F-measure\n93\nAccuracy\n93\n■Precision and recall\n94\n■F-measure\n96\n4.4\nBuilding AllenNLP training pipelines\n96\nInstances and fields\n97\n■Vocabulary and token \nindexers\n98\n■Token embedders and RNNs\n99\nBuilding your own model\n100\n■Putting it all \ntogether\n101\n4.5\nConfiguring AllenNLP training pipelines\n102\n4.6\nCase study: Language detection\n105\nUsing characters as input\n106\n■Creating a dataset \nreader\n106\n■Building the training pipeline\n108\nRunning the detector on unseen instances\n110\n5 Sequential labeling and language modeling\n112\n5.1\nIntroducing sequential labeling\n113\nWhat is sequential labeling?\n113\n■Using RNNs \nto encode sequences\n115\n■Implementing a Seq2Seq \nencoder in AllenNLP\n117\n5.2\nBuilding a part-of-speech tagger\n118\nReading a dataset\n118\n■Defining the model and the \nloss\n119\n■Building the training pipeline\n121\n5.3\nMultilayer and bidirectional RNNs\n122\nMultilayer RNNs\n122\n■Bidirectional RNNs\n124\n5.4\nNamed entity recognition\n126\nWhat is named entity recognition?\n127\n■Tagging \nspans\n128\n■Implementing a named entity \nrecognizer\n128\n5.5\nModeling a language\n130\nWhat is a language model?\n130\n■Why are language models \nuseful?\n131\n■Training an RNN language model\n132\n",
      "content_length": 1564,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "CONTENTS\nviii\n5.6\nText generation using RNNs\n133\nFeeding characters to an RNN\n134\n■Evaluating text using \na language model\n134\n■Generating text using a language \nmodel\n136\nPART 2\nADVANCED MODELS ..................................... 139\n6 Sequence-to-sequence models\n141\n6.1\nIntroducing sequence-to-sequence models\n142\n6.2\nMachine translation 101\n144\n6.3\nBuilding your first translator\n147\nPreparing the datasets\n148\n■Training the model\n150\nRunning the translator\n153\n6.4\nHow Seq2Seq models work\n154\nEncoder\n154\n■Decoder\n156\n■Greedy decoding\n158\nBeam search decoding\n161\n6.5\nEvaluating translation systems\n163\nHuman evaluation\n163\n■Automatic evaluation\n163\n6.6\nCase study: Building a chatbot\n165\nIntroducing dialogue systems\n165\n■Preparing a dataset\n166\nTraining and running a chatbot\n167\n■Next steps\n169\n7 Convolutional neural networks\n171\n7.1\nIntroducing convolutional neural networks (CNNs)\n172\nRNNs and their shortcomings\n172\n■Pattern matching \nfor sentence classification\n173\n■Convolutional neural \nnetworks (CNNs)\n174\n7.2\nConvolutional layers\n174\nPattern matching using filters\n175\n■Rectified linear unit \n(ReLU)\n176\n■Combining scores\n178\n7.3\nPooling layers\n179\n7.4\nCase study: Text classification\n180\nReview: Text classification\n180\n■Using CnnEncoder\n181\nTraining and running the classifier\n182\n8 Attention and Transformer\n184\n8.1\nWhat is attention?\n185\nLimitation of vanilla Seq2Seq models\n185\n■Attention \nmechanism\n186\n",
      "content_length": 1428,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "CONTENTS\nix\n8.2\nSequence-to-sequence with attention\n187\nEncoder-decoder attention\n188\n■Building a Seq2Seq \nmachine translation with attention\n189\n8.3\nTransformer and self-attention\n192\nSelf-attention\n192\n■Transformer\n195\n■Experiments\n197\n8.4\nTransformer-based language models\n200\nTransformer as a language model\n200\n■Transformer-XL\n203\nGPT-2\n205\n■XLM\n207\n8.5\nCase study: Spell-checker\n208\nSpell correction as machine translation\n208\n■Training a spell-\nchecker\n210\n■Improving a spell-checker\n213\n9 Transfer learning with pretrained language models\n218\n9.1\nTransfer learning\n219\nTraditional machine learning\n219\n■Word embeddings\n220\nWhat is transfer learning?\n220\n9.2\nBERT\n222\nLimitations of word embeddings\n222\n■Self-supervised learning\n224\nPretraining BERT\n225\n■Adapting BERT\n226\n9.3\nCase study 1: Sentiment analysis with BERT\n229\nTokenizing input\n230\n■Building the model\n232\nTraining the model\n233\n9.4\nOther pretrained language models\n236\nELMo\n236\n■XLNet\n237\n■RoBERTa\n239\nDistilBERT\n240\n■ALBERT\n241\n9.5\nCase study 2: Natural language inference with BERT\n243\nWhat is natural language inference?\n243\n■Using BERT \nfor sentence-pair classification\n244\n■Using Transformers \nwith AllenNLP\n246\nPART 3\nPUTTING INTO PRODUCTION ....................... 253\n10 Best practices in developing NLP applications\n255\n10.1\nBatching instances\n256\nPadding\n256\n■Sorting\n257\n■Masking\n259\n10.2\nTokenization for neural models\n261\nUnknown words\n261\n■Character models\n262\n■Subword \nmodels\n263\n",
      "content_length": 1467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "CONTENTS\nx\n10.3\nAvoiding overfitting\n265\nRegularization\n265\n■Early stopping\n268\n■Cross-\nvalidation\n269\n10.4\nDealing with imbalanced datasets\n270\nUsing appropriate evaluation metrics\n270\n■Upsampling and \ndownsampling\n271\n■Weighting losses\n272\n10.5\nHyperparameter tuning\n273\nExamples of hyperparameters\n274\n■Grid search vs. random \nsearch\n275\n■Hyperparameter tuning with Optuna\n276\n11 Deploying and serving NLP applications\n280\n11.1\nArchitecting your NLP application\n281\nBefore machine learning\n282\n■Choosing the right \narchitecture\n282\n■Project structure\n283\n■Version \ncontrol\n285\n11.2\nDeploying your NLP model\n286\nTesting\n286\n■Train-serve skew\n288\n■Monitoring\n289\nUsing GPUs\n289\n11.3\nCase study: Serving and deploying NLP applications\n292\nServing models with TorchServe\n292\n■Deploying models with \nSageMaker\n296\n11.4\nInterpreting and visualizing model predictions\n298\n11.5\nWhere to go from here\n302\nindex\n305\n",
      "content_length": 909,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "xi\npreface\nHaving worked at the intersection of machine learning (ML), natural language pro-\ncessing (NLP), and education for the last two decades, I have always been passionate\nabout education and helping people learn new technologies. That’s why I didn’t think\ntwice when I heard about the opportunity of publishing a book on NLP.\n The field of artificial intelligence (AI) went through a lot of changes over the past\nseveral years, including the explosive popularization of neural network–based meth-\nods and the advent of large, pretrained language models. This change made advanced\nlanguage technologies possible, many of which you interact with daily—voice-based\nvirtual assistants, speech recognition, and machine translation, to name a few. How-\never, the “technology stack” of NLP, characterized by the use of pretrained models\nand transfer learning, has finally stabilized in the last few years and is expected to\nremain so, at least for the next couple of years. This is why I think now is a good time\nto start learning about NLP.\n Developing a book on AI is never easy. It feels like you are chasing a moving target\nthat doesn’t slow down and wait for you. When I started writing this book, the Trans-\nformer had just been published, and BERT did not yet exist. Over the course of writ-\ning, AllenNLP, the main NLP framework we use in this book, went through two major\nupdates. Few people were using Hugging Face Transformer, a widely popular deep\nNLP library currently used by many practitioners all over the world. Within two years,\nthe landscape of the NLP field changed completely, due to the advent of the Trans-\nformer and pretrained language models such as BERT. The good news is that the\nbasics of modern machine learning, including word and sentence embeddings, RNNs,\nand CNNs, have not become obsolete and remain important. This book intends to cap-\nture this “core” of ideas and concepts that help you build real-world NLP applications. \n",
      "content_length": 1961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "PREFACE\nxii\n Many great books about ML and deep learning in general are on the market, but\nsome of them focus heavily on math and theories. There’s a gap between what’s\ntaught in books and what the industry needs. I hope this book will serve to bridge\nthis gap.\n \n \n",
      "content_length": 266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "xiii\nacknowledgments\nThis book would not be possible without the help of many people. I must start by thank-\ning Karen Miller, the development editor at Manning Publications. Thank you for your\nsupport and patience during the development of this book. I’m also grateful for the rest\nof the Manning team: technical development editor Mike Shepard, review editor Adri-\nana Sabo, production editor Deirdre Hiam, copy editor Pamela Hunt, proofreader Keri\nHales, and technical proofreader Mayur Patil. Denny (http://www.designsonline.id/)\nalso created some of the high-quality illustrations you see in this book.\n I’d also like to thank the reviewers who gave valuable feedback after reading the\nmanuscript of this book: Al Krinker, Alain Lompo, Anutosh Ghosh, Brian S. Cole, Cass\nPetrus, Charles Soetan, Dan Sheikh, Emmanuel Medina Lopez, Frédéric Flayol,\nGeorge L. Gaines, James Black, Justin Coulston, Lin Chen, Linda Ristevski, Luis\nMoux, Marc-Anthony Taylor, Mike Rosencrantz, Nikos Kanakaris, Ninoslav Čerkez,\nRichard Vaughan, Robert Diana, Roger Meli, Salvatore Campagna, Shanker Janakira-\nman, Stuart Perks, Taylor Delehanty, and Tom Heiman.\n I’d like to acknowledge the AllenNLP team at the Allen Institute for Artificial Intel-\nligence. I’ve had great discussions with the team, namely, Matt Gardner, Mark Neu-\nmann, and Michael Schmitz. I always look up to their great work that makes deep NLP\ntechnologies easy and accessible to the world.\n Last but not least, I’d like to thank my awesome wife, Lynn. She not only helped\nme choose the right cover image for this book but has also been understanding and\nsupportive of my work throughout the development of this book. \n",
      "content_length": 1675,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "xiv\nabout this book\nReal-World Natural Language Processing is not a typical NLP textbook. We focus on build-\ning real-world NLP applications. Real-world’s meaning here is twofold: first, we pay\nattention to what it takes to build real-world NLP applications. As a reader, you will\nlearn not just how to train NLP models but also how to design, develop, deploy, and\nmonitor them. Along the way, you will also learn the basic building blocks of modern\nNLP models, as well as recent developments in the NLP field that are useful for build-\ning NLP applications. Second, unlike most introductory books, we take a top-down\napproach to teaching. Instead of a bottom-up approach, spending page after page\nshowing neural network theories and mathematical formulae, we focus on quickly\nbuilding NLP applications that “just work.” We then dive deeper into individual con-\ncepts and models that make up NLP applications. You’ll also learn how to build\nend-to-end custom NLP applications tailored to your needs using these basic building\nblocks.\nWho should read this book\nThis book is written mainly for software engineers and programmers who are looking\nto learn the basics of NLP and how to build NLP applications. We assume that you, the\nreader, have basic programming and software engineering skills in Python. This book\nalso comes in handy if you are already working on machine learning but would like to\nmove into the NLP field. Either way, you don’t need any prior knowledge of ML or\nNLP. You don’t need any math knowledge to read this book, although basic under-\nstanding of linear algebra might be helpful. There is not a single mathematical for-\nmula in this book.\n",
      "content_length": 1663,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "ABOUT THIS BOOK\nxv\nHow this book is organized: A roadmap\nThis book consists of three parts that span a total of 11 chapters. Part 1 covers the\nbasics of NLP, where we learn how to quickly build an NLP application with AllenNLP\nfor basic tasks such as sentiment analysis and sequence labeling.\nChapter 1 begins by introducing the “what” and “why” of NLP—what is NLP,\nwhat is not NLP, how NLP technologies are used, and how NLP is related to other\nfields of AI.\nChapter 2 demonstrates how to build your very first NLP application, a sentiment\nanalyzer, and introduces the basics of modern NLP models—word embeddings\nand recurrent neural networks (RNNs)—along the way.\nChapter 3 introduces two important building blocks of NLP applications, word\nand sentence embeddings, and demonstrates how to use and train them.\nChapter 4 discusses one of the simplest but most important NLP tasks, sentence\nclassification, and how to use RNNs for this task.\nChapter 5 covers sequence labeling tasks such as part-of-speech tagging and\nnamed entity extraction. It also touches upon a related technique, language\nmodeling.\nPart 2 covers advanced NLP topics including sequence-to-sequence models, the Trans-\nformer, and how to leverage transfer learning and pretrained language models to\nbuild powerful NLP applications.\nChapter 6 introduces sequence-to-sequence models, which transform one\nsequence into another. We build a simple machine translation system and a\nchatbot within an hour.\nChapter 7 discusses another type of popular neural network architecture, con-\nvolutional neural networks (CNNs).\nChapter 8 provides a deep dive into the Transformer, one of the most import-\nant NLP models today. We’ll demonstrate how to build an improved machine\ntranslation system and a spell-checker using the Transformer.\nChapter 9 builds upon the previous chapter and discusses transfer learning, a pop-\nular technique in modern NLP, with pretrained language models such as BERT.\nPart 3 covers topics that become relevant when you develop NLP applications that are\nrobust to real-world data, and deploy and serve them.\nChapter 10 details best practices when developing NLP applications, including\nbatching and padding, regularization, and hyperparameter optimization.\nChapter 11 concludes the book by covering how to deploy and serve NLP mod-\nels. It also covers how to explain and interpret ML models.\nAbout the code \nThis book contains many examples of source code both in numbered listings and in\nline with normal text. In both cases, source code is formatted in a fixed-width font\n",
      "content_length": 2569,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "ABOUT THIS BOOK\nxvi\nlike this to separate it from ordinary text. Sometimes code is also in bold to high-\nlight code that has changed from previous steps in the chapter, such as when a new\nfeature adds to an existing line of code.\n In many cases, the original source code has been reformatted; we’ve added line\nbreaks and reworked indentation to accommodate the available page space in the\nbook. In rare cases, even this was not enough, and listings include line-continuation\nmarkers (➥). Additionally, comments in the source code have often been removed\nfrom the listings when the code is described in the text. Code annotations accompany\nmany of the listings, highlighting important concepts.\n The code for the examples in this book is available for download from the Manning\nwebsite at https://www.manning.com/books/real-world-natural-language-processing\nand from GitHub at https://github.com/mhagiwara/realworldnlp.\n Most of the code can also be run on Google Colab, which is a free web-based plat-\nform where you can run your machine learning code on hardware accelerators,\nincluding GPUs. \nliveBook discussion forum\nPurchase of Real-World Natural Language Processing includes free access to a private web\nforum run by Manning Publications where you can make comments about the book,\nask technical questions, and receive help from the author and from other users.\nTo access the forum, go to https://livebook.manning.com/book/real-world-natural\n-language-processing/discussion. You can also learn more about Manning’s forums\nand the rules of conduct at https://livebook.manning.com/#!/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking the author some challenging questions lest his interest stray! The\nforum and the archives of previous discussions will be accessible from the publisher’s\nwebsite as long as the book is in print.\nOther online resources\nThe two NLP frameworks we use heavily in this book, AllenNLP and Hugging Face\nTransformers, both have great online courses (https://guide.allennlp.org/ and\nhttps://huggingface.co/course) where you can learn the basics of NLP and how to\nuse the libraries to solve a variety of NLP tasks.\n",
      "content_length": 2452,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "xvii\nabout the author\nMASATO HAGIWARA received a PhD in computer science from\nNagoya University in 2009, focusing on natural language processing\nand machine learning. He has interned at Google and Microsoft\nResearch and worked at Baidu, Rakuten Institute of Technology,\nand Duolingo, as an engineer and a researcher. He now runs his\nown research and consultancy company, Octanove Labs, focusing\non educational applications of NLP.\n",
      "content_length": 431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "xviii\nabout the cover illustration\nThe figure on the cover of Real-World Natural Language Processing is captioned “Bulgare,”\nor a man from Bulgaria. The illustration is taken from a collection of dress costumes\nfrom various countries by Jacques Grasset de Saint-Sauveur (1757–1810), titled Costumes\nde Différents Pays, published in France in 1797. Each illustration is finely drawn and col-\nored by hand. The rich variety of Grasset de Saint-Sauveur’s collection reminds us viv-\nidly of how culturally apart the world’s towns and regions were just 200 years ago.\nIsolated from each other, people spoke different dialects and languages. In the streets\nor in the countryside, it was easy to identify where they lived and what their trade or sta-\ntion in life was just by their dress. \n The way we dress has changed since then and the diversity by region, so rich at the\ntime, has faded away. It is now hard to tell apart the inhabitants of different conti-\nnents, let alone different towns, regions, or countries. Perhaps we have traded cultural\ndiversity for a more varied personal life—certainly for a more varied and fast-paced\ntechnological life. \n At a time when it is hard to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nGrasset de Saint-Sauveur’s pictures.\n",
      "content_length": 1441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "Part 1\nBasics\nWelcome to the beautiful and exciting world of natural language process-\ning (NLP)! NLP is a subfield of artificial intelligence (AI) that concerns compu-\ntational approaches to processing, understanding, and generating human\nlanguages. NLP is used in many technologies you interact with in your daily life—\nspam filtering, conversational assistants, search engines, and machine translation.\nThis first part of the book is intended to give you a gentle introduction to the field\nand bring you up to speed with how to build practical NLP applications.\n In chapter 1, we’ll begin by introducing the “what” and “why” of NLP—what\nis NLP, what is not NLP, how NLP technologies are used, and how it’s related to\nother fields of AI.\n In chapter 2, you’ll build a complete, working NLP application—a sentiment\nanalyzer—within an hour with the help of a powerful NLP framework, Allen-\nNLP. You’ll also learn to use basic machine learning (ML) concepts, including\nword embeddings and recurrent neural networks (RNNs). Don’t worry if this sounds\nintimidating—we’ll introduce you to the concepts gradually and provide an\nintuitive explanation.\n Chapter 3 provides a deep dive into the one of the most important concepts\nfor deep learning approaches to NLP—word and sentence embeddings. The\nchapter demonstrates how to use and even train them using your own data.\n Chapters 4 and 5 cover fundamental NLP tasks, sentence classification and\nsequence labeling. Though simple, these tasks have a wide range of applica-\ntions, including sentiment analysis, part-of-speech tagging, and named entity\nrecognition. \n This part familiarizes you with some basic concepts of modern NLP and we’ll\nbuild useful NLP applications along the way.\n",
      "content_length": 1730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "2\nCHAPTER \n",
      "content_length": 11,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "3\nIntroduction to natural\nlanguage processing\nThis is not an introductory book to machine learning or deep learning. You won’t\nlearn how to write neural networks in mathematical terms or how to compute gra-\ndients, for example. But don’t worry, even if you don’t have any idea what they are.\nI’ll explain those concepts as needed, not mathematically but conceptually. In fact,\nthis book contains no mathematical formulae—not a single one. Also, thanks to\nmodern deep learning libraries, you don’t really need to understand the math to\nbuild practical NLP applications. If you are interested in learning the theories and\nthe math behind machine learning and deep learning, you can find a number of\ngreat resources out there.\nThis chapter covers\nWhat natural language processing (NLP) is, what it is not, \nand why it’s such an interesting, yet challenging, field\nHow NLP relates to other fields, including artificial \nintelligence (AI) and machine learning (ML)\nWhat typical NLP applications and tasks are\nHow a typical NLP application is developed and structured\n",
      "content_length": 1066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "4\nCHAPTER 1\nIntroduction to natural language processing\n But you do need to be at least comfortable enough to write in Python and know its\necosystems. However, you don’t need to be an expert in software engineering topics.\nIn fact, this book’s purpose is to introduce software engineering best practices for\ndeveloping NLP applications. You also don’t need to know NLP in advance. Again,\nthis book is designed to be a gentle introduction to the field. \n You need Python version 3.6.1 or higher and AllenNLP 2.5.0 or higher to run the\ncode examples in this book. Note that we do not support Python 2, mainly because\nAllenNLP (https://allennlp.org/), the deep natural language processing framework\nI’m going to heavily use in this book, supports only Python 3. If you haven’t done so, I\nstrongly recommend upgrading to Python 3 and familiarizing yourself with the latest\nlanguage features such as type hints and new string-formatting syntax. This will be\nhelpful, even if you are developing non-NLP applications.\n Don’t worry if you don’t have a Python development environment ready. Most of\nthe examples in this book can be run via the Google Colab platform (https://\ncolab.research.google.com). You need only a web browser to build and experiment\nwith NLP models!\n This book will use PyTorch (https://pytorch.org/) as its main choice of deep learn-\ning framework. This was a difficult decision for me, because several deep learning\nframeworks are equally great choices for building NLP applications, namely, Tensor-\nFlow, Keras, and Chainer. A few factors make PyTorch stand out among those\nframeworks—it’s a flexible and dynamic framework that makes it easier to prototype\nand debug NLP models; it’s becoming increasingly popular within the research com-\nmunity, so it’s easy to find open source implementations of major models; and the\ndeep NLP framework AllenNLP mentioned earlier is built on top of PyTorch.\n1.1\nWhat is natural language processing (NLP)?\nNLP is a principled approach to processing human language. Formally, it is a subfield\nof artificial intelligence (AI) that refers to computational approaches to process,\nunderstand, and generate human language. The reason it is part of AI is because lan-\nguage processing is considered a huge part of human intelligence. The use of lan-\nguage is arguably the most salient skill that separates humans from other animals.\n1.1.1\nWhat is NLP?\nNLP includes a range of algorithms, tasks, and problems that take human-produced\ntext as an input and produce some useful information, such as labels, semantic repre-\nsentations, and so on, as an output. Other tasks, such as translation, summarization,\nand text generation, directly produce text as output. In any case, the focus is on pro-\nducing some output that is useful per se (e.g., a translation) or as input to other down-\nstream tasks (e.g., parsing). I’ll touch upon some popular NLP applications and tasks\nin section 1.3.\n You might wonder why NLP explicitly has “natural” in its name. What does it mean\nfor a language to be natural? Are there any unnatural languages? Is English natural?\nWhich is more natural: Spanish or French?\n",
      "content_length": 3140,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "5\nWhat is natural language processing (NLP)?\n The word “natural” here is used to contrast natural languages with formal lan-\nguages. In this sense, all the languages humans speak are natural. Many experts\nbelieve that language emerged naturally tens of thousands of years ago and has\nevolved organically ever since. Formal languages, on the other hand, are types of lan-\nguages that are invented by humans and have strictly and explicitly defined syntax\n(i.e., what is grammatical) and semantics (i.e., what it means).\n Programming languages such as C and Python are good examples of formal lan-\nguages. These languages are defined in such a strict way that it is always clear what is\ngrammatical and ungrammatical. When you run a compiler or an interpreter on the\ncode you write in those languages, you either get a syntax error or not. The compiler\nwon’t say something like, “Hmm, this code is maybe 50% grammatical.” Also, the behav-\nior of your program is always the same if it’s run on the same code, assuming external\nfactors such as the random seed and the system states remain constant. Your interpreter\nwon’t show one result 50% of the time and another the other 50% of the time.\n This is not the case for human languages. You can write a sentence that is maybe\ngrammatical. For example, do you consider the phrase “The person I spoke to”\nungrammatical? There are some grammar topics where even experts disagree with\neach other. This is what makes human languages interesting but challenging, and why\nthe entire field of NLP even exists. Human languages are ambiguous, meaning that\ntheir interpretation is often not unique. Both structures (how sentences are formed)\nand semantics (what sentences mean) can have ambiguities in human language. As an\nexample, let’s take a close look at the next sentence:\n He saw a girl with a telescope.\nWhen you read this sentence, who do you think has a telescope? Is it the boy, who’s\nusing a telescope to see a girl (from somewhere far), or the girl, who has a telescope\nand is seen by the boy? There seem to be at least two interpretations of this sentence\nas shown in figure 1.1.\nFigure 1.1\nTwo interpretations of “He saw a girl with a telescope.”\n",
      "content_length": 2196,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "6\nCHAPTER 1\nIntroduction to natural language processing\nThe reason you are confused upon reading this sentence is because you don’t know\nwhat the phrase “with a telescope” is about. More technically, you don’t know what\nthis prepositional phrase (PP) modifies. This is called a PP-attachment problem and is a\nclassic example of syntactic ambiguity. A syntactically ambiguous sentence has more\nthan one interpretation of how the sentence is structured. You can interpret the sen-\ntence in multiple ways, depending on which structure of the sentence you believe.\n Another type of ambiguity that may arise in natural language is semantic ambiguity.\nThis is when the meaning of a word or a sentence, not its structure, is ambiguous. For\nexample, let’s look at the following sentence:\n I saw a bat.\nThere is no question how this sentence is structured. The subject of the sentence is “I”\nand the object is “a bat,” connected by the verb “saw.” In other words, there is no syn-\ntactical ambiguity in it. But how about its meaning? “Saw” has at least two meanings.\nOne is the past tense of the verb “to see.” The other is to cut some object with a saw. Sim-\nilarly, “a bat” can mean two very different things: is it a nocturnal flying mammal or a\npiece of wood used to hit a ball? All in all, does this sentence mean that I observed a noc-\nturnal flying mammal or that I cut a baseball or cricket bat? Or even (cruelly) that I cut\na nocturnal animal with a saw? You never know, at least from this sentence alone.\n Ambiguity is what makes natural languages rich but also challenging to process.\nWe can’t simply run a compiler or an interpreter on a piece of text and just “get it.”\nWe need to face the complexities and subtleties of human languages. We need a scien-\ntific, principled approach to deal with them. That’s what NLP is all about.\n Welcome to the beautiful world of natural languages.\n1.1.2\nWhat is not NLP?\nNow let’s consider the following scenario and think how you’d approach this prob-\nlem: you are working as a junior developer at a midsized company that has a\nconsumer-facing product line. It’s 3 p.m. on a Friday. The rest of the team is becoming\nmore and more restless as the weekend approaches. That’s when your boss drops by at\nyour cubicle.\n “Hey, got a minute? I’ve got something interesting to show you. I just sent it to\nyou.”\n Your boss just sent you an email with a huge zip file attached to it.\n “OK, so this is a giant TSV file. It contains all the responses to the survey questions\nabout our product. I just got this data from the marketing team.”\n Obviously, the marketing team has been collecting user opinions about one of the\nproducts through a series of survey questions online. \n “The survey questions include standard ones like ‘How did you know about our\nproduct?’ and ‘How do you like our product?’ There is also a free-response question,\nwhere our customers can write whatever they feel about our product. The thing is, the\nmarketing team realized there was a bug in the online system and the answers to the\nsecond question were not recorded in the database at all.”\n",
      "content_length": 3100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "7\nWhat is natural language processing (NLP)?\n “Wait, so there’s no way to tell how the customers are feeling about our product?”\nThis sounds weirdly familiar. This must be a copy-and-paste error. When you first cre-\nated an online data-collection interface, you copied and pasted the backend code\nwithout modifying the ID parameters, resulting in a loss of some data fields. \n “So,” your boss continues. “I was wondering if we can recover the lost data some-\nhow. The marketing team is a little desperate now because they need to report the\nresults to the VP early next week.” \n At this point, your bad feeling has been confirmed. Unless you come up with a way\nto get this done as quickly as possible, your weekend plans will be ruined. \n “Didn’t you say you were interested in some machine learning? I think this is a per-\nfect project for you. Anyway, it’d be great if you can give it a stab and let me know what\nyou find. Do you think you can have some results by Monday?”\n “Well, I’ll give it a try.” \n You know “no” is not an acceptable answer here. Satisfied with your answer, your\nboss smiles and walks off. \n You start by skimming the TSV file. To your relief, its structure is fairly standard—it\nhas several fields such as timestamps and submission IDs. At the end of each line is a\nlengthy field for the free-response question. Here they are, you think. At least you\nknow where to look for some clues.\n After a quick glance over the field, you find responses such as “A very good prod-\nuct!” and “Very bad. It crashes all the time!” Not too bad, you think. At least you can\ncapture these simple cases. You start by writing the following method that captures\nthose two cases:\ndef get_sentiment(text):\n    \"\"\"Return 1 if text is positive, -1 if negative.\n       Otherwise, return 0.\"\"\"\n    if 'good' in text:\n        return 1\n    elif 'bad' in text:\n        return -1\n    return 0\nThen you run this method on the responses in the file and log the results, along with\nthe original input. As intended, this method seems to be able to capture a dozen or so\nof the responses that contains “good” or “bad.” \n But then you start to see something alarming, as shown next:\n “I can’t think of a single good reason to use this product”: positive\n “It’s not bad.”: negative\nOops, you think. Negation. Yeah, of course. But this is pretty easy to deal with. You\nmodify the method as follows:\ndef get_sentiment(text):\n    \"\"\"Return 1 if text is positive, -1 if negative.\n       Otherwise, return 0.\"\"\"\n    sentiment = 0\n",
      "content_length": 2514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "8\nCHAPTER 1\nIntroduction to natural language processing\n    if 'good' in text:\n        sentiment = 1\n    elif 'bad' in text:\n        sentiment = -1\n    if 'not' in text or \"n't\" in text:\n        sentiment *= -1\n    return sentiment\nYou run the script again. This time, it seems to be behaving as intended, until you see\nan even more complicated example:\n “The product is not only cheap but also very good!”: negative\nHmm, you think. This is probably not as straightforward as I initially thought after all.\nMaybe the negation has to be somewhere near “good” or “bad” for it to be effective.\nWondering what steps you could take next, you scroll down to see more examples,\nwhich is when you see responses like these:\n “I always wanted this feature badly!”: negative\n “It’s very badly made.”: negative\nYou silently curse to yourself. How could a single word in a language have two com-\npletely opposite meanings? At this point, your little hope for enjoying the weekend\nhas already disappeared. You are already wondering what excuses you use with your\nboss next Monday.\n \n As a reader of this book, you’ll know better. You’ll know that NLP is not about\nthrowing a bunch of ifs and thens at natural language text. It is a more principled\napproach to processing natural language. In the following chapters, you’ll learn how\nyou should approach this problem before writing a single line of code and how to\nbuild a custom-made NLP application just for your task at hand.\n1.1.3\nAI, ML, DL, and NLP\nBefore delving into the details of NLP, it’d be useful to clarify how it relates to other,\nsimilar fields. Most of you have at least heard about artificial intelligence (AI) and\nmachine learning (ML). You may also have heard of deep learning (DL), because it’s\ngenerating a lot of buzz in popular media these days. Figure 1.2 illustrates how those\ndifferent fields overlap with each other.\n Artificial intelligence (AI) is a broad umbrella field that is concerned with achiev-\ning human-like intelligence using machines. It encompasses a wide range of subfields,\nincluding machine learning, natural language processing, computer vision, and\nspeech recognition. The field also includes subfields such as reasoning, planning, and\nsearch, which do not fall under either machine learning or natural language process-\ning and are not in the scope of this book.\n Machine learning (ML) is usually considered a subfield of artificial intelligence that\nis about improving computer algorithms through experience and data. This includes\nlearning a general function that maps inputs to outputs based on past experience\n",
      "content_length": 2597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "9\nWhat is natural language processing (NLP)?\n(supervised learning), drawing hidden patterns and structures from data (unsuper-\nvised learning), and learning how to act in a dynamic environment based on indirect\nrewards (reinforcement learning). Throughout this book, we’ll make a heavy use of\nsupervised machine learning, which is the main paradigm for training NLP models.\n Deep learning (DL) is a subfield of machine learning that usually uses deep neural\nnetworks. These neural network models are called “deep” because they consist of a\nnumber of layers. A layer is just a fancy word for a substructure of neural networks. By\nhaving many stacked layers, deep neural networks can learn complex representations\nof data and can capture highly complicated relationships between the input and the\noutput.\n As the amount of available data and computational resources increases, modern\nNLP makes a heavier and heavier use of machine learning and deep learning. Modern\nNLP applications and tasks are usually built on top of machine learning pipelines and\ntrained from data. But also notice in figure 1.2 that a part of NLP does not overlap with\nmachine learning. Traditional methods such as counting words and measuring similar-\nities between text are usually not considered to be machine learning techniques per se,\nalthough they can be important building blocks for ML-based models.\n I’d also like to mention some other fields that are related to NLP. One such field is\ncomputational linguistics (CL). As its name suggests, computational linguistics is a sub-\nfield of linguistics that uses computational approaches to study human language. The\nmain distinction between CL and NLP is that the former encompasses scientific\napproaches to study language, whereas the latter is concerned with engineering\nArtificial\nintelligence (AI)\nMachine\nlearning (ML)\nNatural\nlanguage\nprocessing\n (NLP) \nDeep\nlearning (DL)\nFigure 1.2\nThe relationship \namong different fields: AI, \nML, DL, and NLP\n",
      "content_length": 1979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "10\nCHAPTER 1\nIntroduction to natural language processing\napproaches for making computers perform something useful related to language.\nPeople often use those terms interchangeably, partly due to some historical reasons.\nFor example, the most prestigious conference in the field of NLP is called ACL, which\nactually stands for “the Association for Computational Linguistics!”\n Another related field is text mining. Text mining is a type of data mining targeted at\ntextual data. Its focus is on drawing useful insights from unstructured textual data,\nwhich is a type of text data that is not formatted in a form that is easily interpretable by\ncomputers. Such data is usually collected from various sources, such as crawling the\nweb and social media. Although its purpose is slightly different from that of NLP,\nthese two fields are similar, and we can use the same tools and algorithms for both. \n1.1.4\nWhy NLP?\nIf you are reading this, you have at least some interest in NLP. Why is NLP exciting?\nWhy is it worth learning more about NLP and, specifically, real-world NLP?\n The first reason is that NLP is booming. Even without the recent AI and ML boom,\nNLP is more important than ever. We are witnessing the advent of practical NLP appli-\ncations in our daily lives, such as conversational agents (think Apple Siri, Amazon\nAlexa, and Google Assistant) and near human-level machine translation (think Google\nTranslate). A number of NLP applications are already an integral part of our day-to-day\nactivities, such as spam filtering, search engines, and spelling correction, as we’ll discuss\nlater. The number of Stanford students enrolled in NLP classes grew fivefold from\n2008 to 2018 (http://realworldnlpbook.com/ch1.html#tweet1). Similarly, the number\nof attendees for EMNLP (Empirical Methods in Natural Language Processing), one\nof the top NLP conferences, doubled within just one year (http://realworldnlpbook\n.com/ch1.html#tweet2). Other major NLP conferences are also experiencing similar\nincreases in participants and paper submissions (http://realworldnlpbook.com/ch1\n.html#nivre17).\n The second reason is that NLP is an evolving field. The field of NLP itself has a\nlong history. The first experiment to build a machine translation system, called The\nGeorgetown–IBM Experiment, was attempted back in 1954. For more than 30 years since\nthis experiment, most NLP systems relied on handwritten rules. Yes, it was not much\ndifferent from what you just saw in section 1.1.1. The first milestone, which came in\nthe late 1980s, was the use of statistical methods and machine learning for NLP. Many\nNLP systems started leveraging statistical models trained from data. This led to some\nrecent successes in NLP, including, most notably, IBM Watson. The second milestone\nwas more drastic. Starting around the late 2000s, the use of so-called deep learning,\nthat is, deep neural network models, took the field by storm. By the mid-2010s, deep\nneural network models became the new standard in the field. \n This second milestone was so drastic and fast that it’s worth noting here. New neural\nnetwork–based NLP models are not only more effective but also a lot simpler. For exam-\nple, it used to take a lot of expertise and effort to replicate even a simple, baseline\nmachine translation model. One of the most popular open source software packages\nfor statistical machine translation, called Moses (http://www.statmt.org/moses/), is a\n",
      "content_length": 3432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "11\nWhat is natural language processing (NLP)?\nbehemoth, consisting of 100,000s of lines of code and dozens of supporting modules\nand tools. Experts spent hours just installing the software and making it work. On the\nother hand, as of 2018, anyone with some prior programming experience could run a\nneural machine translation system more powerful than traditional statistical models\nwith a fraction of the code size—less than a few thousand lines of code (e.g., see Tensor-\nFlow’s neural machine translation tutorial at https://github.com/tensorflow/nmt).\nAlso, the new neural network models are trained “end-to-end,” which means that those\nbig, monolithic networks take the input and directly produce the output. Entire models\nare trained to match the desired output. On the other hand, traditional machine learn-\ning models consist of (at least) several submodules. These submodules are trained sep-\narately using different machine learning algorithms. In this book, I’ll mainly discuss\nmodern neural network–based approaches to NLP but also touch upon some tradi-\ntional concepts as well.\n The third and final reason is that NLP is challenging. Understanding and produc-\ning language is the central problem of artificial intelligence, as we saw in the previous\nsection. The accuracy and performance in major NLP tasks such as speech recogni-\ntion and machine translation got drastically better in the past decade or so. But\nhuman-level understanding of language is far from being solved.\n To verify this quickly, open up your favorite machine translation service (or simply\nGoogle Translate), and type this sentence: “I saw her duck.” Try to translate it to Span-\nish or some other language you understand. You should see words like “pato,” which\nmeans “a duck” in Spanish. But did you notice another interpretation of this sen-\ntence? See figure 1.3 for the two interpretations. The word “duck” here could be a\nverb meaning “to crouch down.” Try adding another sentence after this, such as “She\ntried to avoid a flying ball.” Did the machine translation change the first translation in\nany way? The answer is probably no. You should still see the same “pato” in the transla-\ntion. As you can see, most (if not all) commercial machine translation systems that are\navailable as of today do not understand the context outside of the sentence that is\nbeing translated. A lot of research effort is spent on this problem in academia, but this\nis still one of many problems in NLP that is considered unsolved.\nFigure 1.3\nTwo interpretations \nof “I saw her duck.”\n",
      "content_length": 2559,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "12\nCHAPTER 1\nIntroduction to natural language processing\nCompared to other AI fields such as robotics and computer vision, language has its\nown quirks. Unlike images, utterances and sentences have variable length. You can say\na very short sentence (“Hello.”) or a very long one (“A quick brown fox . . .”). Most\nmachine learning algorithms are not good at dealing with something of variable\nlength, and you need to come up with ways to represent languages with something\nmore fixed. If you look back at the history of the field, NLP is largely concerned with\nthe problem of how to represent language mathematically. Vector space models and\nword embeddings (discussed in chapter 3) are some examples of this. \n Another characteristic of language is that it is discrete. What this means is that\nthings in languages are separate as concepts. For example, if you take a word “rat” and\nchange its first letter to the next one, you’ll have “sat.” In computer memory, the dif-\nference is just a single bit. However, there is no relationship between those two words\nexcept they both end with “at,” and maybe a rat can sit. There is no such thing as\nsomething that is in between “rat” and “sat.” These two are totally discrete, separate\nconcepts that happen to have similar spelling. On the other hand, if you take an\nimage of a car and change the value of a pixel by a single bit, you still have a car that is\nalmost identical to the one before this change. Maybe it has a slightly different color.\nIn other words, images and sounds are continuous, meaning that you can make small\nmodifications without greatly affecting what they are. Many mathematical toolkits,\nsuch as vectors, matrices, and functions, are good at dealing with something continu-\nous. The history of NLP is actually a history of challenging this discreteness of lan-\nguage, and only recently have we begun to see some successes on this front, for\nexample, with word embeddings.\n1.2\nHow NLP is used\nAs I mentioned previously, NLP is already an integral part of our daily life. In modern\nlife, a larger and larger portion of our daily communication is done online, and our\nonline communication is still largely conducted in natural language text. Think of\nyour favorite social networking services, such as Facebook and Twitter. Although you\ncan post photos and videos, a large portion of communication is still in text. As long\nas you are dealing with text, there is a need for NLP. For example, how do you know if\na particular post is spam? How do you know which posts are the ones you are most\nlikely to “like?” How do you know which ads you are most likely to click?\n Because many large internet companies need to deal with text in one way or\nanother, chances are many of them are already using NLP. You can also confirm this\nfrom their “careers” page—you’ll see that they are always hiring NLP engineers and\ndata scientists. NLP is also used to a varying extent in many other industries and prod-\nucts including, but not limited to, customer service, e-commerce, education, enter-\ntainment, finance, and health care, which all involve text in some ways.\n Many NLP systems and services can be classified into or built by combining some\nmajor types of NLP applications and tasks. In this section, I’m going to introduce\nsome of the most popular applications of NLP as well as common NLP tasks.\n",
      "content_length": 3356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "13\nHow NLP is used\n1.2.1\nNLP applications\nAn NLP application is a software application whose main purpose is to process natural\nlanguage text and draw some useful information from it. Similar to general software\napplications, it can be implemented in various ways, such as an offline data-processing\nscript, an offline standalone application, a backend service, or a full-stack service with\na frontend, depending on its scope and use cases. It can be built for end users to use\ndirectly, for other backend services to consume its output, or for other businesses to\nuse as a SaaS (software as a service). \n You can use many NLP applications out of the box, such as machine translation\nsoftware and major SaaS products (e.g., Google Cloud API), if your requirement is\ngeneric and doesn’t require a high level of customization. You can also build your own\nNLP applications if you need customizations and/or you need to deal with a specific\ntarget domain. This is exactly what you’ll learn throughout this book!\nMACHINE TRANSLATION\nMachine translation is probably one of the most popular and easy-to-understand NLP\napplications. Machine translation (MT) systems translate a given text from one lan-\nguage to another language. An MT system can be implemented as a full-stack service\n(e.g., Google Translate), as well as a pure backend service (e.g., NLP SaaS products).\nThe language the input text is written in is called the source language, whereas the one\nfor the output is called the target language. MT encompasses a wide range of NLP prob-\nlems, including language understanding and generation, because MT systems need\nto understand the input and then generate the output. MT is one of the most well-\nstudied areas in NLP, and it was one of the earliest applications of NLP as well.\n One challenge in MT is the tradeoff between fluency and adequacy. Translation\nneeds to be fluent, meaning that the output has to sound natural in the target lan-\nguage. Translation also needs to be adequate, meaning that the output has to reflect\nthe meaning expressed by the input as closely as possible. These two are often in con-\nflict, especially when the source and the target languages are not very similar (e.g.,\nEnglish and Mandarin Chinese). You can write a sentence that is a precise, verbatim\ntranslation of the input, but doing so often leads to something that doesn’t sound nat-\nural in the target language. On the other hand, you can make up something that\nsounds natural but might not reflect the precise meaning. Good human translators\naddress this tradeoff in a creative way. It’s their job to come up with translations that\nare natural in the target language while reflecting the meaning of the original.\nGRAMMATICAL AND SPELLING ERROR CORRECTION\nMost major web browsers nowadays support spelling correction. Even if you forget\nhow to spell “Mississippi,” you can do your best and type what you remember, and the\nbrowser highlights it with a correction. Some word-processing software applications,\nincluding recent versions of Microsoft Word, do more than just correct spelling. They\npoint out grammatical errors such as uses of “it’s” instead of “its.” This is not an easy\nfeat, because both words are, in a sense, “correct” (no mistakes in spelling), and the\nsystem needs to infer whether they are used correctly from the context. Some\n",
      "content_length": 3342,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "14\nCHAPTER 1\nIntroduction to natural language processing\ncommercial products (most notably, Grammarly, https://www.grammarly.com/) spe-\ncialize in grammatical error correction. Some products go a long way and point out\nincorrect usage of punctuation and even writing styles. These products are popular\namong both native and non-native speakers of the language.\n Research into grammatical error correction has been active due to the increasing\nnumber of non-native English speakers. Traditionally, grammatical error correction\nsystems for non-native English speakers dealt with individual types of mistakes one by\none. For example, you could think of a subcomponent of the system that detects and\ncorrects only incorrect uses of articles (a, an, the, etc.), which is very common among\nnon-native English speakers. More recent approaches to grammatical error correction\nare similar to the ones for machine translation. You can think of the (potentially\nincorrect) input as one language and the corrected output as another. Then your job\nis to “translate” between these two languages! \nSEARCH ENGINE\nAnother application of NLP that is already an integral part of our daily lives is search\nengines. Few people would think of search engines as an NLP application, yet NLP\nplays such an important role in making search engines useful that they are worth men-\ntioning here.\n Page analysis is one area where NLP is heavily used for search engines. Ever won-\nder why you don’t see any “hot dog” pages when you search for “dogs?” If you have any\nexperience building your own full-text search engines using open source software\nsuch as Solr and Elasticsearch, and if you simply used a word-based index, your search\nresult pages would be littered with “hot dogs,” even when you want just “dogs.” Major\ncommercial search engines solve this problem by running the page content being\nindexed through NLP pipelines that recognize that “hot dogs” are not a type of\n“dogs.” But the extent and types of NLP pipelines that go into page analysis is confi-\ndential information for search engines and is difficult to know.\n Query analysis is another NLP application in search engines. If you have noticed\nGoogle showing a box with pictures and bios when you search for a celebrity or a box\nwith the latest news stories when you search for certain current events, that’s query anal-\nysis in action. Query analysis identifies the intent (what the user wants) of the query and\nshows relevant information accordingly. A common way to implement query analysis is\nto make it a classification problem, where an NLP pipeline classifies queries into classes\nof intent (e.g., celebrity, news, weather, videos), although again, the details of how com-\nmercial search engines run query analysis are usually highly confidential.\n Finally, search engines are not only about analyzing pages and classifying queries.\nThey have many other functionalities that make your searches easier, one of which is\nquery correction. This comes into play when you make a spelling or a grammatical mis-\ntake when formulating the query, and Google and other major search engines show cor-\nrections with labels such as “showing results for:” and “Did you mean.” How this works\nis somewhat similar to grammatical error correction that I mentioned earlier, except it\nis optimized for the types of mistakes and queries that search engine users use.\n",
      "content_length": 3390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "15\nHow NLP is used\nDIALOG SYSTEMS\nDialog systems are machines that humans can have conversations with. The field of\ndialog systems has a long history. One of the earliest dialog systems, ELIZA, was devel-\noped in 1966. \n But it’s only recently that dialog systems have found their ways into our daily lives.\nWe have seen an almost exponential increase in their popularity in recent years, mainly\ndriven by the availability of consumer-facing “conversational AI” products such as Ama-\nzon Alexa and Google Assistant. In fact, according to a survey in 2018, 20% of US homes\nalready own a smart speaker. You may also remember being mind-blown watching the\nkeynote at Google IO in 2018, where Google’s conversational AI called Google Duplex\nwas shown making a phone call to a hair salon and a restaurant, having natural conver-\nsations with the staff at the business, and making an appointment on behalf of its user. \n The two main types of dialog systems are task-oriented and chatbots. Task-oriented\ndialog systems are used to achieve specific goals (for example, reserving a plane\nticket), obtaining some information, and, as we saw, making a reservation at a restau-\nrant. Task-oriented dialog systems are usually built as an NLP pipeline consisting of\nseveral components, including speech recognition, language understanding, dialog\nmanagement, response generation, and speech synthesis, which are usually trained\nseparately. Similar to machine translation, though, there are new deep learning\napproaches where dialog systems (or their subsystems) are trained end-to-end.\n The other type of dialog system is chatbots, whose main purpose is to have conver-\nsations with humans. Traditional chatbots are usually managed by a set of handwritten\nrules (e.g., when the human says this, say that). Recently, the use of deep neural net-\nworks, particularly sequence-to-sequence models and reinforcement learning, has\nbecome increasingly popular. However, because the chatbots do not serve particular\npurposes, the evaluation of chatbots, that is, assessing how good a particular chatbot\nis, remains an open question.\n1.2.2\nNLP tasks\nBehind the scenes, many NLP applications are built by combining multiple NLP com-\nponents that solve different NLP problems. In this section, I introduce some notable\nNLP tasks that are commonly used in NLP applications.\nTEXT CLASSIFICATION\nText classification is the process of classifying pieces of text into different categories.\nThis NLP task is one of the simplest yet most widely used. You might not have heard of\nthe term “text classification” before, but I bet most of you benefit from this NLP task\nevery day. For example, spam filtering is one type of text classification. It classifies\nemails (or other types of text, such as web pages) into two categories—spam or not\nspam. This is why you get very few spam emails when you use Gmail and you see so few\nspammy (low-quality) web pages when you use Google. \n Another type of text classification is called sentiment analysis, which is what we saw\nin section 1.1. Sentiment analysis is used to automatically identify subjective informa-\ntion, such as opinions, emotions, and feelings, within text. \n",
      "content_length": 3184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "16\nCHAPTER 1\nIntroduction to natural language processing\nPART-OF-SPEECH TAGGING\nA part of speech (POS) is a category of words that share the similar grammatical proper-\nties. In English, for example, nouns describe the names of things like objects, animals,\npeople, and concepts, among many other things. A noun can be used as a subject of a\nverb, an object of a verb, and an object of a preposition. Verbs, in contrast, describe\nactions, states, and occurrences. Other English parts of speech include adjectives\n(green, furious), adverbs (cheerfully, almost), determiners (a, the, this, that), prepositions\n(in, from, with), conjunctions (and, yet, because), and many others. Almost all languages\nhave nouns and verbs, but other parts of speech differ from language to language. For\nexample, many languages, such as Hungarian, Turkish, and Japanese, have postposi-\ntions instead of prepositions, which are placed after words to add some extra meaning\nto them. A group of NLP researchers came up with a set of tags that cover frequent\nparts of speech that exist in most languages, called a universal part-of-speech tagset\n(http://realworldnlpbook.com/ch1.html#universal-pos). This tagset is widely used for\nlanguage-independent tasks.\n Part-of-speech tagging is the process of tagging each word in a sentence with a cor-\nresponding part-of-speech tag. Some of you may have done this at school. As an exam-\nple, let’s take the sentence “I saw a girl with a telescope.” The POS tags for this\nsentence are shown in figure 1.4.\nThese tags come from the Penn Treebank POS tagset, which is the most popular stan-\ndard corpus for training and evaluating various NLP tasks such as POS tagging and\nparsing. Traditionally, POS tagging was solved by sequential labeling algorithms such\nas hidden Markov models (HMMs) and conditional random fields (CRFs). Recently,\nInput sentence\nPOS tags\nPRP\nI\nsaw\na\nVBD\nDT\ngirl\nNN\nwith\nIN\na\ntelescope\n.\nDT\nNN\n.\nPOS tag\nDT\nIN\nDescription\nDeterminer\nPreposition\nNN\nNoun (singular or mass)\nPRP\nPronoun\nVBD\nVerb (past tense)\nFigure 1.4\nPart-of-\nspeech (POS) tagging\n",
      "content_length": 2086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "17\nHow NLP is used\nrecurrent neural networks (RNNs) have become a popular and practical choice for\ntraining a POS tagger with high accuracy. The results of POS tagging are often used as\nthe input to other downstream NLP tasks, such as machine translation and parsing.\nI’ll cover part-of-speech tagging in more detail in chapter 5.\nPARSING\nParsing is the task of analyzing the structure of a sentence. Broadly speaking, there are\ntwo main types of parsing, constituency parsing and dependency parsing, which we’ll dis-\ncuss in detail next.\n Constituency parsing uses context-free grammars to represent natural language sen-\ntences. (See http://mng.bz/GO5q for a brief introduction to context-free grammars).\nA context-free grammar is a way to specify how smaller building blocks of a language\n(e.g., words) are combined to form larger building blocks (e.g., phrases and clauses)\nand eventually sentences. To put it another way, it specifies how the largest unit (a sen-\ntence) is broken down to phrases and clauses and all the way down to words. The ways\nthe linguistic units interact with each other are specified by a set of production rules as\nfollows:\nS -> NP VP\nNP -> DT NN | PRN | NP PP\nVP -> VBD NP | VBD PN PP\nPP -> IN NP\nDT -> a\nIN -> with\nNN -> girl | telescope\nPRN -> I\nVBD -> saw\nA production rule describes a transformation from the symbol on the left-hand side\n(e.g., “S”) to the symbols on the right-hand side (e.g., “NP VP”). The first rule means\nthat a sentence is a noun phrase (NP) followed by a verb phrase (VP). Some of the\nsymbols (e.g., DT, NN, VBD) may look familiar to you—yes, they are the POS tags we\njust saw in the POS tagging section. In fact, you can consider POS tags as the smallest\ngrammatical categories that behave in similar ways (because they are!).\n Now the parser’s job is to figure out how to reach the final sym-\nbol (in this case, “S”) starting from the raw words in the sentence.\nYou can think of those rules as transformation rules from the sym-\nbols on the right to the ones on the left by traversing the arrow\nbackward. For example, using the rule “DT  a” and “NN  girl,”\nyou can convert “a girl” to “DT NN.” Then, if you use “NP  DT\nNN,” you can reduce the entire phrase to “NP.” If you illustrate this\nprocess in a tree-like diagram, you get something like the one\nshown in figure 1.5.\nNP\nDT\nNN\na\ngirl\nFigure 1.5\nSubtree for \n“a girl”\n",
      "content_length": 2386,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "18\nCHAPTER 1\nIntroduction to natural language processing\nTree structures that are created in the process of parsing are called parse trees, or sim-\nply parses. The figure is a subtree because it doesn’t cover the entirety of the tree (i.e.,\nit doesn’t show all the way from “S” to words). Using the sentence “I saw a girl with a\ntelescope” that we discussed earlier and see if you can parse it by hand. If you keep\nbreaking down the sentence using the production rules until you get the final “S” sym-\nbol, you get the tree-like structure shown in figure 1.6.\nFigure 1.6\nParse tree for “I saw a girl with a telescope.”\nDon’t worry if the tree in figure 1.6 is different from what you got. Actually, there’s\nanother parse tree that is a valid parse of this sentence, shown in figure 1.7.\nFigure 1.7\nAnother parse tree for “I saw a girl with a telescope.”\nNP\nVP\nNP\nPRN\nVP\nVBD\nDT\nNN\na\ngirl\nsaw\nI\nNP\nPP\nDT\nNN\na\nwith\ntelescope\nIN\nS\nNP\nNP\nVP\nNP\nPRN\nVP\nVBD\nDT\nNN\na\ngirl\nsaw\nI\nNP\nPP\nDT\nNN\na\nwith\ntelescope\nIN\nS\n",
      "content_length": 1003,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "19\nHow NLP is used\nIf you look at those two trees carefully, you’ll notice a difference where the “PP” (prep-\nositional phrase) is located, or attached. In fact, these two parse trees correspond to\nthe two different interpretations of this sentence we discussed in section 1.1. The first\ntree (figure 1.6), where the PP attaches the verb “saw,” corresponds to the interpreta-\ntion where the boy is using a telescope to see the girl. In the second tree (figure 1.7),\nwhere the PP attaches to the noun “a girl,” the boy saw the girl who has a telescope.\nParsing is a great step forward to reveal the structure and the semantics of a sentence,\nbut in cases like this one, parsing alone cannot uniquely decide what is the single most\nlikely interpretation of a sentence.\n The other type of parsing is called dependency parsing. Dependency parsing uses\ndependency grammars to describe the structure of sentences, not in terms of phrases\nbut in terms of words and the binary relations between them. For example, the result\nof dependency parsing of the earlier sentence is shown in figure 1.8.\nFigure 1.8\nDependency parse for “I saw a girl with a telescope.”\nNotice that each relation is directional and labeled. A relation specifies which word\ndepends on which word and the type of relationship between the two. For example,\nthe relation connecting “a” to “girl” is labeled “det,” meaning the first word is the\ndeterminer of the second. If you take the most central word, “saw,” and pull it upward,\nyou’ll notice that these words and relations form a tree. Such trees are called depen-\ndency trees.\n One advantage of dependency grammars is that they are agnostic regarding some\nword-order changes, meaning that the order of certain words in the sentence will not\nchange the dependency tree. For example, in English, there is some freedom as to\nwhere to put an adverb in a sentence, especially when the adverb describes the man-\nner in which the action referred to by the verb is done. For example, “I carefully\npainted the house” and “I painted the house carefully” are both acceptable and mean\nthe same thing. If you represent these sentences by a dependency grammar, the word\n“carefully” always modifies the verb “painted,” and the two sentences have completely\nidentical dependency trees. Dependency grammars capture more than just phrasal\nstructures of sentences—they capture something more fundamental about the rela-\ntionship of the words. Therefore, dependency parsing is considered an important step\ntoward semantic analysis of natural language. A group of researchers is working on a\nI\nsaw\na\ngirl\nwith\na\ntelescope\nnsubj\ndet\ndet\ncase\ndobj\nnmod\n",
      "content_length": 2646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "20\nCHAPTER 1\nIntroduction to natural language processing\nformal language-independent dependency grammar, called Universal Dependencies,\nthat is linguistically motivated and applicable to many languages, similar to the univer-\nsal POS tagset.\nTEXT GENERATION\nText generation, also called natural language generation (NLG), is the process of gener-\nating natural language text from something else. In a broader sense, machine transla-\ntion, which we discussed previously, involves a text-generation problem, because MT\nsystems need to generate text in the target language. Similarly, summarization, text\nsimplification, and grammatical error correction all produce natural language text as\noutput and are instances of text-generation tasks. Because all of these tasks take natu-\nral language text as their input, they are called text-to-text generation. \n Another class of text-generation task is called data-to-text generation. For those\ntasks, the input is data that is not text. For example, a dialog system needs to generate\nnatural utterances based on the current state of the conversation. A publisher may\nwish to generate news text based on events such as sports game outcomes and weather.\nThere is also a growing interest in generating natural language text that best describes\na given image, called image captioning.\n Finally, a third class of text classification is unconditional text generation, where\nnatural language text is generated randomly from a model. You can train models so\nthat they can generate random academic papers, Linux source code, or even poems\nand play scripts. For example, Andrej Karpathy trained an RNN model from all of\nShakespeare’s works and succeeded in generating pieces of text that look exactly like\nhis work (http://realworldnlpbook.com/ch1.html#karpathy15), as shown next:\nPANDARUS:\nAlas, I think he shall be come approached and the day\nWhen little srain would be attain'd into being never fed,\nAnd who is but a chain and subjects of his death,\nI should not sleep.\nSecond Senator:\nThey are away this miseries, produced upon my soul,\nBreaking and strongly should be buried, when I perish\nThe earth and thoughts of many states.\nDUKE VINCENTIO:\nWell, your wit is in the care of side and that.\nSecond Lord:\nThey would be ruled after this chamber, and\nmy fair nues begun out of the fact, to be conveyed,\nWhose noble souls I'll have the heart of the wars.\nClown:\nCome, sir, I will make did behold your worship.\nVIOLA:\nI'll drink it.\n",
      "content_length": 2468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "21\nBuilding NLP applications\nTraditionally, text generation has been solved by handcrafted templates and rules for\ngenerating text from some information. You can think of this as the reverse of parsing,\nwhere rules are used to infer information about natural language text, as we discussed\nearlier. In recent years, neural network models are an increasingly popular choice for\nnatural language generation, be it text-to-text generation (sequence-to-sequence\nmodels), data-to-text generation (encoder-decoder models), and unconditional text\ngeneration (neural language models and generative adversarial networks, or GANs).\nWe’ll discuss text generation more in chapter 5.\n1.3\nBuilding NLP applications\nIn this section, I’m going to show you how NLP applications are typically developed\nand structured. Although details may vary on a case-by-case basis, understanding the\ntypical process helps you plan and budget before you start developing an application.\nIt also goes a long way if you know best practices in developing NLP applications\nbeforehand.\n1.3.1\nDevelopment of NLP applications\nThe development of NLP applications is a highly iterative process, consisting of many\nphases of research, development, and operations (figure 1.9). Most learning materials\nsuch as books and online tutorials focus mainly on the training phase, although all the\nother phases of application development are equally important for real-world NLP\nR\nE\nS\nE\nA\nR\nC\nH\nO\nP\nE\nR\nA\nT\nI\nO\nN\nS\nD\nE\nV\nE\nL\nO\nP\nM\nE\nN\nT\nImplementation\nDeploying\nNLP system \ndevelopment\nprocess\nTraining\nMonitoring\nData\ncollection\nAnalysis &\nexperimenting\nFigure 1.9\nThe development \ncycle of NLP applications\n",
      "content_length": 1659,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "22\nCHAPTER 1\nIntroduction to natural language processing\napplications. In this section, I briefly introduce what each stage involves. Note that no\nclear boundary exists between these phases. It is not uncommon that application\ndevelopers (researchers, engineers, managers, and other stakeholders) go back and\nforth between some of these phases through trial and error.\nDATA COLLECTION\nMost modern NLP applications are based on machine learning. Machine learning, by\ndefinition, requires data on which NLP models are trained (remember the definition\nof ML we talked about previously—it’s about improving algorithms through data). In\nthis phase, NLP application developers discuss how to formulate the application as an\nNLP/ML problem and what kind of data should be collected. Data can be collected\nfrom humans (e.g., by hiring in-house annotators and having them go through a bunch\nof text instances), crowdsourcing (e.g., using platforms such as Amazon Mechanical\nTurk), or automated mechanisms (e.g., from application logs or clickstreams). \n You may choose not to use machine learning approaches for your NLP application\nat first, which could totally be the right choice depending on various factors, such as\ntime, budgets, the complexity of the task, and the expected amount of data you might\nbe able to collect. Even in that case, it may be a good idea to collect a small amount of\ndata for validation purposes. I’ll talk more about training, validation, and testing of\nNLP applications in chapter 11.\nANALYSIS AND EXPERIMENTING\nAfter collecting the data, you move on to the next phase where you analyze and run\nsome experiments. For analyses, you usually look for signals such as: What are the\ncharacteristics of the text instances? How are the training labels distributed? Can you\ncome up with signals that are correlated with the training labels? Can you come up\nwith some simple rules that can predict the training labels with reasonable accuracy?\nShould we even use ML? This list goes on and on. This analysis phase includes aspects\nof data science, where various statistical techniques may come in handy. \n You run experiments to try a number of prototypes quickly. The goal in this phase\nis to narrow down the possible set of approaches to a couple of promising ones, before\nyou go all-in and start training a gigantic model. By running experiments, you wish to\nanswer questions including: What types of NLP tasks and approaches are appropriate\nfor this NLP application? Is this a classification, parsing, sequence labeling, regression,\ntext generation, or some other problem? What is the performance of the baseline\napproach? What is the performance of the rule-based approach? Should we even use\nML? What is the estimate of training and serving time for the promising approaches?\n I call these first two phases the “research” phase. The existence of this phase is\narguably the biggest difference between NLP applications and other generic software\nsystems. Due to its nature, it is difficult to predict the performance and the behavior\nof a machine learning system, or an NLP system, for that matter. At this point, you\nmight not have written a single line of production code, and that’s totally fine. The\npoint of this research phase is to prevent you from wasting your effort writing produc-\ntion code that turns out to be useless at a later stage. \n",
      "content_length": 3366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "23\nBuilding NLP applications\nTRAINING\nAt this point you have pretty clear ideas what the approaches will be for your NLP\napplication. This is when you start adding more data and computational resources\n(e.g., GPUs) for training your model. It is not uncommon for modern NLP models to\ntake days if not weeks to train, especially if they are based on neural network models.\nIt is always a good practice to gradually ramp up the amount of the data and the size\nof the model you train. You don’t want to spend weeks training a gigantic neural net-\nwork model only to find that a smaller and simpler model performs just as well, or\neven worse, that you introduced a bug in the model and that the model you spent\nweeks training is simply useless!\n It is critical at this phase that you keep your training pipeline reproducible.\nChances are, you will need to run this several times with different sets of hyperparam-\neters, which are tuning values set before starting the model’s learning process. It is\nalso likely that you will need to run this pipeline several months later, if not years. I’ll\ntouch upon some best practices when training NLP/ML models in chapter 10.\nIMPLEMENTATION\nWhen you have a model that is working with acceptable performance, you move on to\nthe implementation phase. This is when you start making your application “produc-\ntion ready.” This process basically follows software engineering best practices, includ-\ning: writing unit and integration tests for your NLP modules, refactoring your code,\nhaving your code reviewed by other developers, improving the performance of your\nNLP modules, and dockerizing your application. I’ll talk more about this process in\nchapter 11.\nDEPLOYING\nYour NLP application is finally ready to deploy. You can deploy your NLP application\nin many ways—it can be an online service, a recurring batch job, an offline applica-\ntion, or an offline one-off task. If this is an online service that needs to serve its predic-\ntions in real time, it is a good idea to make this a microservice to make it loosely coupled\nwith other services. In any case, it is a good practice to use continuous integration\n(CI) for your application, where you run tests and verify that your code and model are\nworking as intended every time you make changes to your application.\nMONITORING\nAn important final step for developing NLP applications is monitoring. This not only\nincludes monitoring the infrastructure such as server CPU, memory, and request\nlatency, but also higher-level ML statistics such as the distributions of the input and the\npredicted labels. Some of the important questions to ask at this stage are: What do the\ninput instances look like? Are they what you expected when you built your model?\nWhat do the predicted labels look like? Does the predicted label distribution match\nthe one in the training data? The purpose of the monitoring is to check that the model\nyou built is behaving as intended. If the incoming text or data instances or the pre-\ndicted labels do not match your expectation, you may have an out-of-domain problem,\nmeaning that the domain of the natural language data you are receiving is different\n",
      "content_length": 3165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "24\nCHAPTER 1\nIntroduction to natural language processing\nfrom the one in which your model is trained. Machine learning models are usually not\ngood at dealing with out-of-domain data, and the prediction accuracy may suffer. If this\nissue becomes obvious, it may be a good idea to repeat the whole process again, start-\ning from collecting more in-domain data.\n1.3.2\nStructure of NLP applications\nThe structures of modern, machine learning–based NLP applications are becoming\nsurprisingly similar for two main reasons—one is that most modern NLP applications\nrely on machine learning to some degree, and they should follow best practices for\nmachine learning applications. The other is that, due to the advent of neural network\nmodels, a number of NLP tasks, including text classification, machine translation, dia-\nlog systems, and speech recognition, can now be trained end-to-end, as I mentioned\nbefore. Some of these tasks used to be hairy, enormous monsters with dozens of com-\nponents with complex plumbing. Now, however, some of these tasks can be solved by\nless than 1,000 lines of Python code, provided that there’s enough data to train the\nmodel end-to-end.\n Figure 1.10 illustrates the typical structure of a modern NLP application. There are\ntwo main infrastructures: the training and the serving infrastructure. The training\ninfrastructure is usually offline and serves the purpose of training the machine learning\nmodel necessary for the application. It takes the training data, converts it to some data\nstructure that can be handled by the pipeline, and further processes it by transforming\nthe data and extracting the features. This part varies greatly from task to task. Finally,\nTraining data \nTraining infrastructure\nDataset\nreader\nTransformer\nTrainer\nOptimizer\nModel\nBatching\nServing infrastructure\nDataset\nreader\nTransformer\nPredictor \nModel\nPrediction \nNew instance\nFigure 1.10\nStructure of typical NLP applications\n",
      "content_length": 1936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "25\nSummary\nif the model is a neural network, data instances are batched and fed to the model, which\nis optimized to minimize the loss. Don’t worry if you don’t understand what I’m talking\nabout in that last sentence—we’ll talk about those technical terms used with neural net-\nworks in chapter 2. The trained model is usually serialized and stored to be passed to\nthe serving infrastructure.\n The serving infrastructure’s job is to, given a new instance, produce the prediction,\nsuch as classes, tags, or translations. The first part of this infrastructure, which reads the\ninstance and transforms it into some numbers, is similar to the one for training. In fact,\nyou must keep the dataset reader and the transformer identical. Otherwise, discrepan-\ncies will arise in the way those two process the data, also known as training-serving skew.\nAfter the instance is processed, it’s fed to the pretrained model to produce the predic-\ntion. I’ll talk more about designing your NLP applications in chapter 11.\nSummary\nNatural language processing (NLP) is a subfield of artificial intelligence (AI)\nthat refers to computational approaches to process, understand, and generate\nhuman language.\nOne of the challenges for NLP is ambiguity in natural languages. There is syn-\ntactic and semantic ambiguity.\nWhere there is text, there is NLP. Many tech companies use NLP to draw infor-\nmation from a large amount of text. Typical NLP applications include machine\ntranslation, grammatical error correction, search engines, and dialog systems.\nNLP applications are developed in an iterative way, with more emphasis on the\nresearch phase.\nMany modern NLP applications rely heavily on machine learning (ML) and are\nstructurally similar to ML systems.\n",
      "content_length": 1741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "26\nYour first NLP application\nIn section 1.1.2, we saw how not to do NLP. In this chapter, we are going to discuss\nhow to do NLP in a more principled, modern way. Specifically, we’d like to build a\nsentiment analyzer using a neural network. Even though the sentiment analyzer we\nare going to build is a simple application and the library (AllenNLP) takes care of\nmost heavy lifting, it is a full-fledged NLP application that covers a lot of basic com-\nponents of modern NLP and machine learning. I’ll introduce important terms and\nconcepts along the way. Don’t worry if you don’t understand some concepts at first.\nWe will revisit most of the concepts introduced here in later chapters.\nThis chapter covers\nBuilding a sentiment analyzer using AllenNLP\nApplying basic machine learning concepts \n(datasets, classification, and regression)\nEmploying neural network concepts (word \nembeddings, recurrent neural networks, linear \nlayers)\nTraining the model through reducing loss\nEvaluating and deploying your model\n",
      "content_length": 1015,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "27\nIntroducing sentiment analysis\n2.1\nIntroducing sentiment analysis\nIn the scenario described in section 1.1.2, you wanted to extract users’ subjective opin-\nions from online survey results. You have a collection of textual data in response to a\nfree-response question, but you are missing the answers to the “How do you like our\nproduct?” question, which you’d like to recover from the text. This task is called senti-\nment analysis, which is a text analytic technique used in the automatic identification\nand categorization of subjective information within text. The technique is widely used\nin quantifying opinions, emotions, and so on that are written in an unstructured way\nand, thus, hard to quantify otherwise. Sentiment analysis is applied to a wide variety of\ntextual resources such as survey, reviews, and social media posts.\n In machine learning, classification means categorizing something into a set of pre-\ndefined, discrete categories. One of the most basic tasks in sentiment analysis is the\nclassification of polarity, that is, to classify whether the expressed opinion is positive,\nnegative, or neutral. You could use more than three classes, for example, strongly pos-\nitive, positive, neutral, negative, or strongly negative. This may sound familiar to you if\nyou have used a website (such as Amazon) where people can review things using a five-\npoint scale expressed by the number of stars.\n Classification of polarity is one type of sentence classification task. Another type of\nsentence classification task is spam filtering, where each sentence is categorized into\ntwo classes—spam or not spam. It’s called binary classification if there are only two\nclasses. If there are more than two classes (the five-star classification system mentioned\nearlier, for example), it’s called multiclass classification. \n In contrast, when the prediction is a continuous value instead of discrete catego-\nries, it’s called regression. If you’d like to predict the price of a house based on its prop-\nerties, such as its neighborhood, numbers of bedrooms and bathrooms, and square\nfootage, it’s a regression problem. If you attempt to predict stock prices based on the\ninformation collected from news articles and social media posts, it’s also a regression\nproblem. (Disclaimer: I’m not suggesting this is an appropriate approach to stock\nprice prediction. I’m not even sure if it works.) As I mentioned earlier, most linguistic\nunits such as characters, words, and part-of-speech tags are discrete. For this reason,\nmost uses of machine learning in NLP are classification, not regression. \nNOTE \nLogistic regression, a widely used statistical model, is usually used for clas-\nsification, even though it has “regression” in its name. Yes, I know it’s confusing!\nMany modern NLP applications, including the sentiment analyzer we are going to\nbuild in this chapter (shown in figure 2.1), are built based on the supervised machine\nlearning paradigm. Supervised machine learning is one type of machine learning\nwhere the algorithm is trained with data that has supervision signals—the desired out-\ncome for individual input. The algorithm is trained in such a way that it reproduces\nthe signals as closely as possible. For sentiment analysis, this means that the system is\ntrained on data that contains the desired labels for each input sentence.\n \n",
      "content_length": 3353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "28\nCHAPTER 2\nYour first NLP application\nFigure 2.1\nSentiment analysis pipeline\n2.2\nWorking with NLP datasets\nAs we discussed in the previous section, many modern NLP applications are devel-\noped using supervised machine learning, where algorithms are trained from data\nannotated with desired outcomes, instead of using handwritten rules. Almost by defi-\nnition, data is a critical part for machine learning, and it is important to understand\nhow it is structured and used with machine learning algorithms. \n2.2.1\nWhat is a dataset?\nA dataset simply means a collection of data. If you are familiar with relational data-\nbases, you can think of a dataset as a dump of one table. It consists of pieces of data\nthat follow the same format. In database terms, each piece of the data corresponds to\na record, or a row in a table. A record can have any number of fields, which corre-\nspond to columns in a database.\n In NLP, records in a dataset are usually some type of linguistic units, such as words,\nsentences, or documents. A dataset of natural language texts is called a corpus (plural:\ncorpora). As an example, let’s think of a (hypothetical) dataset for spam filtering. Each\nrecord in this dataset is a pair of a piece of text and a label, where the text is a sen-\ntence or a paragraph (e.g., from an email) and the label specifies whether the text is\nspam. Both the text and the label are the fields of a record.\n Some NLP datasets and corpora have more complex structures. For example, a\ndataset may contain a collection of sentences, where each sentence is annotated with\ndetailed linguistic information, such as part-of-speech tags, parse trees, dependency\nstructures, and semantic roles. If a dataset contains a collection of sentences anno-\ntated with their parse trees, the dataset is called a treebank. The most famous example\nof this is Penn Treebank (PTB) (http://realworldnlpbook.com/ch2.html#ptb), which\nhas been serving as the de facto standard dataset for training and evaluating NLP tasks\nsuch as part-of-speech (POS) tagging and parsing.\n A closely related term to a record is an instance. In machine learning, an instance is\na basic unit for which the prediction is made. For example, in the spam-filtering task\nmentioned earlier, an instance is one piece of text, because predictions (spam or not\nLinear\nlayer \nStrongly \npositive \nStrongly \nnegative \nPositive\nNegative \nNeutral\nRecurrent neural network (RNN) \nThis\nis\nthe\never\nWord\nembeddings\n!\n",
      "content_length": 2464,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "29\nWorking with NLP datasets\nspam) are made for individual texts. An instance is usually created from a record in a\ndataset, as is the case for the spam-filtering task, but this is not always the case—for\nexample, if you take a treebank and use it to train an NLP task that detects all nouns\nin a sentence, then each word, not a sentence, becomes an instance, because predic-\ntion (noun or not noun) is made for each word. Finally, a label is a piece of informa-\ntion attached to some linguistic unit in a dataset. A spam-filtering dataset has labels\nthat correspond to whether each text is a spam. A treebank may have one label per\nword for its part of speech. Labels are usually used as training signals (i.e., answers for\nthe training algorithm) in a supervised machine learning setting. See figure 2.2 for a\ndepiction of these parts of a dataset.\nFigure 2.2\nDatasets, records, fields, instances, and labels\n2.2.2\nStanford Sentiment Treebank\nTo build a sentiment analyzer, we are going to use the Stanford Sentiment Treebank\n(SST; https://nlp.stanford.edu/sentiment/), one of the most widely used sentiment\nanalysis datasets as of today. Go ahead and download the dataset from the Train,\nDev, Test Splits in PTB Tree Format link. One feature that differentiates SST from\nother datasets is the fact that sentiment labels are assigned not only to sentences but\nalso to every word and phrase in sentences. For example, some excerpts from the\ndataset follow:\n(4\n  (2 (2 Steven) (2 Spielberg))\n    (4\n      (2 (2 brings) (3 us))\n      (4 (2 another) (4 masterpiece))))\nField\nField\nField\nField\nRecord\nField\nField\nField\nField\nRecord\nDataset\nDataset\nLabel\nInstance\nLabel\nInstance\nLabel\nInstance\nField\nField\nField\nField\nRecord\n",
      "content_length": 1721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "30\nCHAPTER 2\nYour first NLP application\n(1\n  (2 It)\n  (1\n    (1 (2 (2 's) (1 not))\n      (4 (2 a) (4 (4 great) (2 (2 monster) (2 movie)))))\n    (2 .)))\nDon’t worry about the details for now—these trees are written in S-expressions that\nare painfully hard to read for humans (unless you are a Lisp programmer). Notice the\nfollowing:\nEach sentence is annotated with sentiment labels (4 and 1).\nEach word is also annotated, for example, (4 masterpiece) and (1 not).\nEvery single phrase is also annotated, for example, (4 (2 another) \n(4 masterpiece)).\nThis property of the dataset enables us to study the complex semantic interactions\nbetween words and phrases. For example, let’s consider the polarity of the following\nsentence as a whole:\n The movie was actually neither that funny, nor super witty.\nThe above statement would definitely be a negative, although, if you focus on the indi-\nvidual words (such as funny, witty), you might be fooled into thinking it’s a positive. If\nyou built a simple classifier that takes “votes” from individual words (e.g., the sentence\nis positive if a majority of its words are positive), such classifiers would have difficulties\nclassifying this example correctly. To correctly classify the polarity of this sentence, you\nneed to understand the semantic impact of the negation “neither . . . nor.” For this\nproperty, SST has been used as the standard benchmark for neural network models\nthat can capture the syntactic structures of sentences (http://realworldnlpbook.com/\nch2.html#socher13). However, in this chapter, we are going to ignore all the labels\nassigned to internal phrases and use only labels for sentences.\n2.2.3\nTrain, validation, and test sets\nBefore we move on to show how to use SST datasets and start building our own senti-\nment analyzer, I’d like to touch upon some important concepts in machine learning.\nIn NLP and ML, it is common to use a couple of different types of datasets to develop\nand evaluate models. A widely used best practice is to use three different types of data-\nset splits—train, validation, and test sets.\n A train (or training) set is the main dataset used to train the NLP/ML models.\nInstances from the train set are usually fed to the ML training pipeline directly and\nused to learn parameters of the model. Train sets are usually the biggest among the\nthree types of splits discussed here.\n A validation set (also called a dev or development set) is used for model selection. Model\nselection is a process where appropriate NLP/ML models are selected among all pos-\nsible models that can be trained using the train set, and here’s why it’s necessary. Let’s\nthink of a situation where you have two machine learning algorithms, A and B, with\n",
      "content_length": 2722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "31\nWorking with NLP datasets\nwhich you want to train an NLP model. You use both algorithms and obtain models A\nand B. Now, how can you know which model is better?\n “That’s easy,” you might say. “Evaluate them both on the train set.” At first glance,\nthis may sound like a good idea. You run both models A and B on the train set and see\nhow they perform in terms of metrics such as accuracy. Why do people bother to use a\nseparate validation set for selecting models?\n The answer is overfitting—another important concept in NLP and ML. Overfitting is\na situation where a trained model fits the train set so well that it loses its generalizability.\nLet’s think of an extreme case to illustrate the point here. Assume algorithm B is a very,\nvery powerful one that remembers everything as-is. Think of it as a big associative array\n(or dict in Python) that can store all the pairs of instances and labels it has ever\nencountered. For the spam-filtering task, this means that the model stores the exact\ntexts and their labels as they are presented when the model is being trained. If the exact\nsame text is presented when the model is evaluated, it just returns what’s stored as its\nlabel. On the other hand, if the presented text is even slightly different from any other\ntexts it has in memory, the model has no clue, because it’s never seen it before.\n How do you think this model would perform if it was evaluated on the train set? The\nanswer is . . . yes, 100%! Because the model remembers all the instances from the train\nset, it can just “replay” the entire dataset and classify it perfectly. Now, would this algo-\nrithm make a good spam filter if you installed it on your email software? Absolutely not!\nBecause countless spam emails look very similar to existing ones but are slightly differ-\nent, or completely new, the model has no clue if the input email is even one character\ndifferent from what’s stored in the memory, and the model would be useless when\ndeployed in production. In other words, it has poor (in fact, zero) generalizability.\n How could you prevent choosing such a model? By using a validation set! A valida-\ntion set consists of separate instances that are collected in a similar way to the train set.\nBecause they are independent from the train set, if you run your trained model on the\nvalidation set, you’ll get a good idea how the model would perform outside the train\nset. In other words, the validation set gives a proxy for the model’s generalizability.\nImagine if the model trained by the earlier “remember all” algorithm was evaluated\non a validation set. Because the instances in the validation set are similar to but inde-\npendent from the ones in the train set, you’d get very low accuracy and know the\nmodel would perform poorly, even before deploying it.\n The validation set is also used for tuning hyperparameters. A hyperparameter is a\nparameter about a machine learning algorithm or about a model that is being trained.\nFor example, if you repeat the training loop (also called an epoch—see later for more\nexplanation) for N times, this N is a hyperparameter. If you increase the number of lay-\ners of the neural network, you just changed one hyperparameter about the model.\nMachine learning algorithms and models usually have a number of hyperparameters,\nand it is crucial to tune them for them to perform optimally. You can do this by training\nmultiple models with different hyperparameters and evaluating them on a validation\nset. In fact, you can think of models with different hyperparameters as separate models,\neven if they have the same structure, and hyperparameter tuning can be considered\none type of model selection.\n",
      "content_length": 3671,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "32\nCHAPTER 2\nYour first NLP application\n Finally, a test set is used to evaluate the model using a new, unseen set of data. It\nconsists of instances that are independent from the train and validation sets. It gives\nyou a good idea how the model would perform “in the wild.” \n You might wonder why yet another separate dataset is necessary for evaluating the\nmodel’s generalizability. Can’t you just use the validation set for this? Again, you\nshouldn’t rely solely on a train set and a validation set to measure the generalizability\nof your model, because your model could also overfit to the validation set in a subtle\nway. This point is less intuitive, but let me give you an example. Imagine you are fran-\ntically experimenting with a ton of different spam-filtering models. You wrote a script\nthat automatically trains a spam-filtering model. The script also automatically evalu-\nates the trained models on the validation set. If you run this script 1,000 times with dif-\nferent combinations of algorithms and hyperparameters and pick one model with the\nhighest validation set performance, would it also perform the best on the completely\nnew, unseen instances? Probably not. If you try a large number of models, some of\nthem happen to perform relatively well on the validation set purely by chance\n(because the predictions inherently have some noise, and/or because those models\nhappen to have some characteristics that make them perform better on the validation\nset), but this is no guarantee that those models perform well outside of the validation\nset. In other words, it could be possible to overfit the model to the validation set.\n In summary, when training NLP models, use a train set to train your model candi-\ndates, use a validation set to choose good ones, and use a test set to evaluate them.\nMany public datasets used for NLP and ML evaluation are already split into train/\nvalidation/test sets. If you just have a single dataset, you can split it into those three\ndatasets by yourself. An 80:10:10 split is commonly used. Figure 2.3 depicts the train/\nvalidation/test split as well as the entire training pipeline.\nTrain (80%) \nValidation  \n(10%)\nTest  \n(10%)\nDataset\nTraining\nalgorithm 1\nTraining\n algorithm 2\nTraining\n algorithm N\nModel\n selection\nEvaluation\nModel 1\nModel 2\nModel N\nMetrics\nBest \nmodel\nFigure 2.3\nTrain/validation/test \nsplit and the training pipeline\n",
      "content_length": 2390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "33\nWorking with NLP datasets\n2.2.4\nLoading SST datasets using AllenNLP\nFinally, let’s see how we can actually load datasets in code. In the remainder of this\nchapter, we assume that you have already installed AllenNLP (version 2.5.0) and the\ncorresponding version of the allennlp-models package by running the following:\npip install allennlp==2.5.0\npip install allennlp-models==2.5.0\nand imported necessary classes and modules as shown here:\nfrom itertools import chain\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder, \nPytorchSeq2VecWrapper\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, \nBasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training import GradientDescentTrainer\nfrom allennlp.training.metrics import CategoricalAccuracy, F1Measure\nfrom allennlp_models.classification.dataset_readers.stanford_sentiment_\ntree_bank import \\\n    StanfordSentimentTreeBankDatasetReader\nUnfortunately, as of this writing, AllenNLP does not officially support Windows. But\ndon’t worry—all the code in this chapter (and all the code in this book, for that mat-\nter) is available as Google Colab notebooks (http://www.realworldnlpbook.com/\nch2.html#sst-nb), where you can run and modify the code and see the results. \n You also need to define the following two constants used in the code snippets:\nEMBEDDING_DIM = 128\nHIDDEN_DIM = 128\nAllenNLP already supports an abstraction called DatasetReader, which takes care of\nreading a dataset from the original format (be it raw text or some exotic XML-based\nformat) and returns it as a collection of instances. We are going to use Stanford-\nSentimentTreeBankDatasetReader(), which is a type of DatasetReader that\nspecifically deals with SST datasets, as shown here:\nreader = StanfordSentimentTreeBankDatasetReader()\ntrain_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/train.txt'\ndev_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/dev.txt'\n",
      "content_length": 2368,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "34\nCHAPTER 2\nYour first NLP application\nThis snippet will create a dataset reader for SST datasets and define the paths for the\ntrain and dev text files.\n2.3\nUsing word embeddings\nFrom this section on, we’ll start building the neural network architecture for the senti-\nment analyzer. Architecture is just another word for the structure of neural networks.\nBuilding neural networks is a lot like building structures such as houses. The first step\nis to figure out how to feed the input (e.g., sentences for sentiment analysis) into the\nnetwork.\n As we have seen previously, everything in NLP is discrete, meaning there is no pre-\ndictable relationship between the forms and the meanings (remember “rat” and “sat”).\nOn the other hand, neural networks are best at dealing with something numerical and\ncontinuous, meaning everything in neural networks needs to be float numbers. How\ncan we “bridge” between these two worlds—discrete and continuous? The key is the use\nof word embeddings, which we are going to discuss in detail in this section.\n2.3.1\nWhat are word embeddings?\nWord embeddings are one of the most important concepts in modern NLP. Technically,\nan embedding is a continuous vector representation of something that is usually dis-\ncrete. A word embedding is a continuous vector representation of a word. If you are\nnot familiar with the concept of vectors, vector is a mathematical name for single-\ndimensional arrays of numbers. In simpler terms, word embeddings are a way to rep-\nresent each word with a 300-element array (or an array of any other size) filled with\nnonzero float numbers. It is conceptually very simple. Then, why has it been so\nimportant and prevalent in modern NLP?\n As I mentioned in chapter 1, the history of NLP is actually the history of continu-\nous battle against “discreteness” of language. In the eyes of computers, “cat” is no\ncloser to “dog” than it is to “pizza.” One way to deal with discrete words programmati-\ncally is to assign indices to individual words as follows (here we simply assume that\nthese indices are assigned alphabetically):\nindex(\"cat\") = 1\nindex(\"dog\") = 2\nindex(\"pizza\") = 3\n…\nThese assignments are usually managed by a lookup table. The entire, finite set of\nwords that one NLP application or task deals with is called vocabulary. But this method\nisn’t any better than dealing with raw words. Just because words are now represented\nby numbers doesn’t mean you can do arithmetic operations on them and conclude\nthat “cat” is equally similar to “dog” (difference between 1 and 2), as “dog” is to\n“pizza” (difference between 2 and 3). Those indices are still discrete and arbitrary.\n  “What if we can represent them on a numerical scale?” some NLP researchers\nwondered decades ago. Can we think of some sort of numerical scale where words are\n",
      "content_length": 2809,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "35\nUsing word embeddings\nrepresented as points, so that\nsemantically closer words (e.g.,\n“dog” and “cat,” which are both\nanimals) are also geometrically\ncloser? Conceptually, the numeri-\ncal scale would look like the one\nshown in figure 2.4.\n This is a step forward. Now we can represent the fact that “cat” and “dog” are more\nsimilar to each other than “pizza” is to those words. But still, “pizza” is slightly closer to\n“dog” than it is to “cat.” What if you wanted to place it somewhere that is equally far\nfrom “cat” and “dog?” Maybe only one dimension is too limiting. How about adding\nanother dimension to this, as shown in figure 2.5?\n Much better! Because computers are really good at dealing with multidimensional\nspaces (because you can just represent points by arrays), you can simply keep doing\nthis until you have a sufficient number of dimensions. Let’s have three dimensions. In\nthis 3-D space, you can represent those three words as follows:\nvec(\"cat\") = [0.7, 0.5, 0.1] \nvec(\"dog\") = [0.8, 0.3, 0.1]\nvec(\"pizza\") = [0.1, 0.2, 0.8]\nFigure 2.6 illustrates this three-dimensional space.\n The x-axis (the first element) here represents some concept of “animal-ness” and\nthe z-axis (the third dimension) corresponds to “food-ness.” (I’m making these num-\nbers up, but you get the point.) This is essentially what word embeddings are. You just\nembedded those words in a three-dimensional space. By using those vectors, you\nalready “know” how the basic building blocks of the language work. For example, if\nyou wanted to identify animal names, then you would just look at the first element of\n \nPizza\nDog\nx\nCat\nFigure 2.4\nWord embeddings in a 1-D space\nx\ny\nz\nCat\nPizza\nDog\nFigure 2.6\nWord embeddings in a \n3-D space\nx\ny\nCat\nPizza\nDog\nFigure 2.5\nWord embeddings in a \n2-D space\n",
      "content_length": 1790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "36\nCHAPTER 2\nYour first NLP application\neach word vector and see if the value is high enough. This is a great jump start com-\npared to the raw word indices!\n You may be wondering where those numbers come from in practice. These num-\nbers are actually “learned” using some machine learning algorithms and a large text\ndataset. We’ll discuss this further in chapter 3.\n By the way, we have a much simpler method to “embed” words into a multidimen-\nsional space. Think of a multidimensional space that has as many dimensions as there\nare words. Then, give to each word a vector that is filled with zeros but just one 1, as\nshown next:\nvec(\"cat\") = [1, 0, 0] \nvec(\"dog\") = [0, 1, 0]\nvec(\"pizza\") = [0, 0, 1]\nNotice that each vector has only one 1 at the position corresponding to the word’s\nindex. These special vectors are called one-hot vectors. These vectors are not very useful\nthemselves in representing semantic relationship between those words—the three\nwords are all at the equal distance from each other—but they are still (a very dumb\nkind of) embeddings. They are often used as the input to a machine learning algo-\nrithm when embeddings are not available.\n2.3.2\nUsing word embeddings for sentiment analysis\nFirst, we create dataset loaders that take care of loading data and passing it to the\ntraining pipeline, as shown next (more discussion on this data later in this chapter):\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(reader, train_path,\n                                           batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(reader, dev_path,\n                                           batch_sampler=sampler)\nAllenNLP provides a useful Vocabulary class that manages mappings from some lin-\nguistic units (such as characters, words, and labels) to their IDs. You can tell the class\nto create a Vocabulary instance from a set of instances as follows:\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(),\n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\nThen, you need to initialize an Embedding instance, which takes care of converting\nIDs to embeddings, as shown in the next code snippet. The size (dimension) of the\nembeddings is determined by EMBEDDING_DIM:\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_DIM)\nFinally, you need to specify which index names correspond to which embeddings and\npass it to BasicTextFieldEmbedder as follows:\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "37\nNeural networks\nNow you can use word_embeddings to convert words (or more precisely, tokens,\nwhich I’ll talk more about in chapter 3) to their embeddings. \n2.4\nNeural networks\nAn increasingly large number of modern NLP applications are built using neural net-\nworks. You may have seen many amazing things that modern neural network models\ncan achieve in the domain of computer vision and game playing (such as self-driving\ncars and Go-playing algorithms defeating human champions), and NLP is no excep-\ntion. We are going to use neural networks for most of the NLP examples and applica-\ntions we are going to build in this book. In this section, we discuss what neural\nnetworks are and why they are powerful.\n2.4.1\nWhat are neural networks?\nNeural networks are at the core of modern NLP (and many other related AI fields,\nsuch as computer vision). It is such an important, vast research topic that it’d take a\nbook (or maybe several books) to fully explain what it is and all the related models,\nalgorithms, and so on. In this section, I’ll briefly explain the gist of it and will go into\nmore details in later chapters as needed.\n In short, a neural network (also called an artificial neural network) is a generic mathe-\nmatical model that transforms a vector to another vector. That’s it. Contrary to what\nyou may have read and heard in popular media, its essence is simple. If you are famil-\niar with programming terms, think of it as a function that takes a vector, does some\ncomputation inside, and produces another vector as the return value. Then why is it\nsuch as big deal? How is it different from normal functions in programming?\n The first difference is that neural networks are trainable. Think of it not just as a\nfixed function but more as a “template” for a set of related functions. If you use a pro-\ngramming language and write a function that includes several mathematical equa-\ntions with some constants, you always get the same result if you feed the same input.\nOn the contrary, neural networks can receive “feedback” (how close the output is to\nyour desired output) and adjust their internal constants. Those “magic” constants are\ncalled weights or, more generally, parameters. Next time you run it, you expect that its\nanswer is closer to what you want. \n The second difference is its mathematical power. It’d be overly complicated if you\nwere to use your favorite programming language and write a function that does, for\nexample, sentiment analysis, if at all possible. (Remember the poor software engineer\nfrom chapter 1?) In theory, given enough model power and training data, neural net-\nworks are known to be able to approximate any continuous functions. This means\nthat, whatever your problem is, neural networks can solve it if there’s a relationship\nbetween the input and the output and if you provide the model with enough compu-\ntational power and training data.\n Neural networks achieve this by learning functions that are not linear. What does it\nmean for a function to be linear? A linear function is a function where, if you change\nthe input by x, the output will always change by c * x, where c is a constant number.\n",
      "content_length": 3158,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "38\nCHAPTER 2\nYour first NLP application\nFor example, 2.0 * x is linear, because the return value always increases by 2.0 if you\nchange x by 1.0. If you plot this on a graph, the relationship between the input and\nthe output forms a straight line, which is why it’s called linear. On the other hand,\n2.0 * x * x is not linear, because how much the return value changes depends not\nonly on how much you change x but also on the value of x.\n What this means is that a linear function cannot capture a more complex relation-\nship between the input and the output and between the input variables. On the con-\ntrary, natural phenomena such as language are highly nonlinear. If you change the\ninput by x (e.g., a word in a sentence), how much the output changes depends not\nonly on how much x is changed but also on many other factors such as the value of x\nitself (e.g., what word you changed x to) and what other variables (e.g., the context\nof x) are. Neural networks, which are nonlinear mathematic models, have the poten-\ntial to capture such complex interactions.\n2.4.2\nRecurrent neural networks (RNNs) and linear layers\nTwo special types of neural network components are important for sentiment\nanalysis—recurrent neural networks (RNNs) and linear layers. I’ll explain them in\ndetail in later chapters, but I’ll briefly describe what they are and their roles in senti-\nment analysis (or in general, sentence classification).\n A recurrent neural network (RNN) is a neural network with loops, as shown in figure\n2.7. It has an internal structure that is applied to the input again and again. Using\nthe programming analogy, it’s like writing a function that contains for word in\nsentence: that loops over each word in the input sentence. It can either output the\ninterim values of the internal variables of the loop, or the final values of the variables\nafter the loop is finished, or both. If you just take the final values, you can use an RNN\nas a function that transforms a sentence to a vector with a fixed length. In many NLP\ntasks, you can use an RNN to transform a sentence to an embedding of the sentence.\nRemember word embeddings? They were fixed-length representation of words. Simi-\nlarly, RNNs can produce fixed-length representation of sentences.\nFigure 2.7\nRecurrent neural network (RNN)\nRecurrent neural network (RNN) \nSentence \nembedding\nThis\nis\nthe\never\n!\n...\n",
      "content_length": 2374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "39\nNeural networks\n Another type of neural network component we’ll be using here is linear layers. A\nlinear layer, also called a fully connected layer, transforms a vector to another vector in a\nlinear fashion. As mentioned earlier, layer is just a fancier term for a substructure of\nneural networks, because you can stack them on top of each other to form a larger\nstructure.\n Remember, neural networks can learn nonlinear relationships between the input\nand the output. Why would we want something that is more constrained (linear) at\nall? Linear layers are used for compressing (or less often, expanding) vectors by\nreducing (or increasing) the dimensionality. For example, assume you receive a 64-\ndimensional vector (an array of 64 float numbers) from an RNN as a sentence embed-\nding, but all you care about is a smaller number of values that are essential for your\nprediction. In sentiment analysis, you may care about only five values that correspond\nto five different sentiment labels, namely, strongly positive, positive, neutral, negative,\nand strongly negative. But you have no idea how to extract those five values from the\nembedded 64 values. This is exactly where a linear layer comes in handy—you can add\na layer that transforms a 64-dimensional vector to a 5-dimensional one, and the neural\nnetworks figure out how to do that well, as shown in figure 2.8.\nFigure 2.8\nLinear layer\n2.4.3\nArchitecture for sentiment analysis\nNow you are ready to put the components together to build the neural network for\nthe sentiment analyzer. First, you need to create the RNN as follows: \nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nDon’t worry too much about PytorchSeq2VecWrapper and batch_first=True.\nHere, you are creating an RNN (or more specifically, one type of RNN called LSTM,\nwhich stands for long short-term memory). The size of the input vector is EMBEDDING_DIM,\nwhich we saw earlier, and that of the output vector is HIDDEN_DIM. \nLinear\nlayer \nStrongly positive \nStrongly negative \nPositive\nNegative \nNeutral\nSentence \nembedding\n",
      "content_length": 2097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "40\nCHAPTER 2\nYour first NLP application\n Next, you need to create a linear layer, as shown here:\nself.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n                              out_features=vocab.get_vocab_size('labels'))\nThe size of the input vector is defined by in_features, whereas out_features is\nthat of the output vector. Because we are transforming the sentence embedding to a\nvector whose elements correspond to five sentiment labels, we need to specify the size\nof the encoder output and obtain the total number of labels from vocab.\n Finally, we can connect those components and build a model as shown in the fol-\nlowing code.\nclass LstmClassifier(Model):\n    def __init__(self,\n                 word_embeddings: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n        super().__init__(vocab)\n        self.word_embeddings = word_embeddings\n        self.encoder = encoder\n        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n                                      \nout_features=vocab.get_vocab_size('labels'))\n        self.loss_function = torch.nn.CrossEntropyLoss()   \n    def forward(self,    \n                tokens: Dict[str, torch.Tensor],\n                label: torch.Tensor = None) -> torch.Tensor:\n        mask = get_text_field_mask(tokens)\n        embeddings = self.word_embeddings(tokens)\n        encoder_out = self.encoder(embeddings, mask)\n        logits = self.linear(encoder_out)\n        output = {\"logits\": logits}\n        if label is not None:\n            self.accuracy(logits, label)\n            self.f1_measure(logits, label)\n            output[\"loss\"] = self.loss_function(logits, label)  \n        return output\nI want you to focus on the forward() function which is the most important function\nthat every neural network model has. Its role is to take the input, pass it through sub-\ncomponents of the neural network, and produce the output. Although the function\nhas some unfamiliar logics that we haven’t covered yet (such as mask and loss),\nListing 2.1\nBuilding a sentiment analyzer model\nDefines the \nloss function \n(cross entropy)\nThe forward() function \nis where most of the \ncomputation happens \nin a model.\nComputes the loss and\nassigns it to the “loss” key\nof the returned dict\n",
      "content_length": 2358,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "41\nLoss functions and optimization\nwhat’s important here is the fact that you can chain the subcomponents of the model\n(word embeddings, RNN, and the linear layer) as if they were functions that trans-\nform the input (tokens), and you get something called logits at the end of the pipe-\nline. Logit is a term in statistics that has a specific meaning, but here, you can think of\nit as something like a score for a class. The higher the score is for a specific label, the\nmore confident that the label is the correct one.\n2.5\nLoss functions and optimization\nNeural networks are trained using supervised learning. As mentioned earlier, super-\nvised learning is a type of machine learning that learns a function that maps inputs to\noutputs based on a large amount of labeled data. So far, I covered only about how\nneural networks take an input and produce an output. How can we make it so that\nneural networks produce the output that we actually want?\n Neural networks are not just like regular functions that you usually write in pro-\ngramming languages. They are trainable, meaning that they can receive some feedback\nand change their internal parameters so that they can produce more accurate out-\nputs, even for the same inputs next time around. Notice there are two parts to this—\nreceiving feedback and adjusting parameters, which are done through loss functions\nand optimization, respectively, which I’ll explain next.\n A loss function is a function that measures how far an output of a machine learning\nmodel is from a desired one. The difference between an actual output and a desired\none is called the loss. Loss is also called cost in some contexts. Either way, the bigger\nthe loss, the worse it is, and you want it as close to zero as possible. Take sentiment\nanalysis, for example. If the model thinks a sentence is 100% negative, but the train-\ning data says it’s strongly positive, the loss will be big. On the other hand, if the model\nthinks a sentence is maybe 80% negative and the training label is indeed negative, the\nloss will be small. It will be zero if both match exactly.\n PyTorch provides a wide range of functions to compute losses. What we need here\nis called cross-entropy loss, which is often used for classification problems, as shown\nhere:\nself.loss_function = torch.nn.CrossEntropyLoss()\nIt can be used later by passing a prediction and labels from the training set as follows:\noutput[\"loss\"] = self.loss_function(logits, label)\nThen, this is where the magic happens. Neural networks, thanks to their mathematical\nproperties, know how to change their internal parameters to make the loss smaller.\nUpon receiving some large loss, the neural network goes, “Oops, sorry, that was my\nmistake, but I’ll do better next round!” and changes its parameters. Remember I\ntalked about a function that you write in a programming language that has some\nmagic constants in it? Neural networks act like that function but know exactly how to\nchange the magic constants to reduce the loss. They do this for each and every\n",
      "content_length": 3034,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "42\nCHAPTER 2\nYour first NLP application\ninstance in the training data, so that they can produce more correct answers for as\nmany instances as possible. Of course, they can’t reach the perfect answer after adjust-\ning the parameters only once. It requires multiple passes, called epochs, over the train-\ning data. Figure 2.9 shows the overall training procedure for neural networks.\nFigure 2.9\nOverall training procedure for neural networks\nThe process where a neural network computes an output from an input using the cur-\nrent set of parameters is called the forward pass. This is why the main function in listing\n2.1 is called forward(). The way the loss is fed back to the neural network is called\nbackpropagation. An algorithm called stochastic gradient descent (SGD) is often used to\nminimize the loss. The process where the loss is minimized is called optimization, and\nthe algorithm (such as SGD) used to achieve this is called the optimizer. You can initial-\nize an optimizer using PyTorch as follows:\noptimizer = optim.Adam(model.parameters())\nHere, we are using one type of optimizer called Adam. There are many types of opti-\nmizers proposed in the neural network community, but the consensus is that there is\nLoss\nLoss function\nBackpropagation\nOptimizer\nForward pass\nModel\nUpdated\nmodel\nModel\nPrediction\nLabel\nInstance\nLabel\nInstance\n× len(train set)\n=\n1 epoch\n",
      "content_length": 1373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "43\nTraining your own classifier\nno single optimization algorithm that works well for any problem, and you should be\nready to experiment with multiple ones for your own problem. \n OK, that was a lot of technical terms. You don’t need to know the details of those\nalgorithms for now, but it’d be helpful if you learn just the terms and what they\nroughly mean. If you write the entire training process in Python pseudocode, it will\nappear as shown in listing 2.2. Note that there are two nested loops, one over epochs\nand another over instances.\nMAX_EPOCHS = 100\nmodel = Model()\nfor epoch in range(MAX_EPOCHS):\n    for instance, label in train_set:\n        prediction = model.forward(instance)\n        loss = loss_function(prediction, label)\n        new_model = optimizer(model, loss)\n        model = new_model\n2.6\nTraining your own classifier\nIn this section, we are going to train our own classifier using AllenNLP’s training\nframework. I’ll also touch upon the concept of batching, an important practical con-\ncept that is used in training neural network models.\n2.6.1\nBatching\nSo far, I have left out one piece of detail—batching. We assumed that an optimization\nstep happens for each and every instance, as you saw in the earlier pseudocode. In\npractice, however, we usually group a number of instances together and feed them to\na neural network, updating model parameters per each group, not per each instance.\nWe call this group of instances a batch.\n Batching is a good idea for a couple of reasons. The first is stability. Any data is\ninherently noisy. Your dataset may contain sampling and labeling errors. If you update\nyour model parameters for every instance, and if some instances contain errors, the\nupdate is influenced too much by the noise. But if you group instances into batches\nand compute the loss for the entire batch, not for individual instances, you can “aver-\nage out” small errors and the feedback to your model stabilizes. \n The second reason is speed. Training neural networks involves a huge number of\narithmetic operations such as matrix additions and multiplications, and it is often\ndone on GPUs (graphics processing units). Because GPUs are designed so that they\ncan process a huge number of arithmetic operations in parallel, it is often efficient if\nyou pass a large amount of data and process it at once instead of passing instances one\nby one. Think of a GPU as a factory overseas that manufactures products based on\nyour specifications. Because factories are often optimized for manufacturing a small\nvariety of products at a large quantity, and there is overhead in communicating and\nListing 2.2\nPseudocode for the neural network training loop\n",
      "content_length": 2682,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "44\nCHAPTER 2\nYour first NLP application\nshipping products, it is more efficient if you make a small number of orders for manu-\nfacturing a large quantity of products instead of making a large number of orders for\nmanufacturing a small quantity of products, even if you want the same quantity of\nproducts in total in either way.\n It is easy to group instances into batches using AllenNLP. The framework uses\nPyTorch’s DataLoader abstraction, which takes care of receiving instances and\nreturning batches. We’ll use a BucketBatchSampler that groups instances into\nbuckets of similar lengths, as shown in the next code snippet. I’ll discuss why it’s\nimportant in later chapters:\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(reader, train_path, \nbatch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(reader, dev_path, \nbatch_sampler=sampler)\nThe parameter batch_size specifies the size of the batch (the number of instances\nin a batch). There is often a “sweet spot” in adjusting this parameter. It should be\nlarge enough to have any effect of the batching I mentioned earlier, but also small\nenough so that batches fit in the GPU memory, because factories have the maximum\ncapacity of products they can manufacture at once. \n2.6.2\nPutting everything together\nNow you are ready to train your sentiment analyzer. We assume that you already\ndefined and initialized your model as follows:\nmodel = LstmClassifier(word_embeddings, encoder, vocab)\nSee the full code listing (http://www.realworldnlpbook.com/ch2.html#sst-nb) for\nwhat the model looks like and how to use it.\n AllenNLP provides the Trainer class, which acts as a framework for putting all the\ncomponents together and managing the training pipeline, as shown here:\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\ntrainer.train()\nYou provide the model, optimizer, iterator, train set, dev set, and the number of\nepochs you want to the trainer and invoke the train method. The last parameter,\ncuda_device, tells the trainer which device (CPU or GPU) to use to use for training.\nHere, we are explicitly using the CPU. This will run the neural network training loop\ndescribed in listing 2.2 and display the progress, including the evaluation metrics. \n",
      "content_length": 2441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "45\nEvaluating your classifier\n2.7\nEvaluating your classifier\nWhen training an NLP/ML model, you should always monitor how the loss changes\nover time. If the training is working as expected, you should see the loss decrease over\ntime. It doesn’t always decrease each epoch, but it should decrease as a general trend,\nbecause this is exactly what you told the optimizer to do. If it’s increasing or showing\nweird values (such as NaN), it’s usually a sign that your model is too limiting or there’s\na bug in your code.\n In addition to the loss, it is important to monitor other evaluation metrics you care\nabout in your task. Loss is a purely mathematical concept that measures the closeness\nbetween your model and the answer, but smaller losses do not always guarantee better\nperformance in the NLP task. \n You can use a number of evaluation metrics, depending on the nature of your NLP\ntask, but some that you need to know no matter what task you are working on include\naccuracy, precision, recall, and F-measure. Roughly speaking, these metrics measure\nhow precisely your model’s predictions match the expected answers defined by the\ndataset. For now, it suffices to know that they are used to measure how good your clas-\nsifier is (more details coming in chapter 4). \n To monitor and report evaluation metrics during training using AllenNLP, you\nneed to implement the get_metrics() method in your model class, which returns a\ndict from metric names to their values, as shown next.\n   def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {'accuracy': self.accuracy.get_metric(reset),\n                **self.f1_measure.get_metric(reset)}\nself.accuracy and self.f1_measure are defined in __init__() as follows:\n    self.accuracy = CategoricalAccuracy()\n    self.f1_measure = F1Measure(positive_index)\nWhen you run trainer.train() with the metrics defined, you’ll see progress bars\nlike these after every epoch:\naccuracy: 0.7268, precision: 0.8206, recall: 0.8703, f1: 0.8448, batch_loss: \n0.7609, loss: 0.7194 ||: 100%|##########| 267/267 [00:13<00:00, 19.28it/s]\naccuracy: 0.3460, precision: 0.3476, recall: 0.3939, f1: 0.3693, batch_loss: \n1.5834, loss: 1.9942 ||: 100%|##########| 35/35 [00:00<00:00, 119.53it/s]\nYou can see that the training framework reports these metrics both for the train and the\nvalidation sets. This is useful not only for evaluating your model but also for monitoring\nthe progress of the training. If you see any unusual values, such as extremely low or high\nnumbers, you’ll know that something is wrong, even before the training completes.\n You may have noticed a large gap between the train and the validation metrics.\nSpecifically, the metrics for the train set are a lot higher than those for the validation\nListing 2.3\n Defining evaluation metrics\n",
      "content_length": 2805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "46\nCHAPTER 2\nYour first NLP application\nset. This is a common symptom of overfitting, which I mentioned earlier, where a\nmodel fits to a train set so well that it loses generalizability outside of it. This is why it’s\nimportant to monitor the metrics using a validation set as well, because you won’t\nknow if it’s just doing well or overfitting only by looking at the training set metrics!\n2.8\nDeploying your application\nThe final step in making your own NLP application is deploying it. Training your\nmodel is only half the story. You need to set it up so that it can make predictions for\nnew instances it has never seen. Making sure the model is serving predictions is criti-\ncal in real-world NLP applications, and a lot of development efforts may go into this\nstage. In this section, I’m going to show what it’s like to deploy the model we just\ntrained using AllenNLP. This topic is discussed in more detail in chapter 11.\n2.8.1\nMaking predictions\nTo make predictions for new instances your model has never seen (called test\ninstances), you need to pass them through the same neural network pipeline as you\ndid for training. It has to be exactly the same—otherwise, you’ll risk skewing the\nresult. This is called training-serving skew, which I’ll explain in chapter 11.\n AllenNLP provides a convenient abstraction called predictors, whose job it is to\nreceive an input in its raw form (e.g., raw string), pass it through the preprocessing\nand neural network pipeline, and give back the result. I wrote a specific predictor for\nSST called SentenceClassifierPredictor (http://realworldnlpbook.com/ch2\n.html#predictor), which you can call as follows:\npredictor = SentenceClassifierPredictor(model, dataset_reader=reader)\nlogits = predictor.predict('This is the best movie ever!')['logits']\nNote that the predictor returns the raw output from the model, which is logits in this\ncase. Remember, logits are some sort of scores corresponding to target labels, so if you\nwant the predicted label itself, you need to convert it to the label. You don’t need to\nunderstand all the details for now, but this can be done by first taking the argmax of\nthe logits, which returns the index of the logit with the maximum value, and then by\nlooking up the label by the ID, as follows:\nlabel_id = np.argmax(logits)\nprint(model.vocab.get_token_from_index(label_id, 'labels'))\nIf this prints out a “4,” congratulations! Label “4” corresponds to “very positive,” so\nyour sentiment analyzer just predicted that the sentence “This is the best movie ever!”\nis very positive, which is indeed correct.\n2.8.2\nServing predictions\nFinally, you can easily deploy the trained model using AllenNLP. If you use a JSON\nconfiguration file (which I’ll explain in chapter 4), you can save your trained model\nonto disk and then quickly fire up a web-based interface where you can make requests\n",
      "content_length": 2859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "47\nDeploying your application\nto your model. To do this, you need to install allennlp-server, a plugin for Allen-\nNLP that provides a web interface for prediction, as follows:\ngit clone https:/./github.com/allenai/allennlp-server\npip install --editable allennlp-server\nAssuming your model is saved under examples/sentiment/model, you can run a\nPython-based web application using the following AllenNLP command:\n$ allennlp serve \\ \n    --archive-path examples/sentiment/model/model.tar.gz \\\n    --include-package examples.sentiment.sst_classifier \\\n    --predictor sentence_classifier_predictor \\\n    --field-name sentence\nIf you open http:/./localhost:8000/ using your browser, you’ll see the interface shown\nin figure 2.10.\nFigure 2.10\nRunning the sentiment analyzer on a web browser\nTry typing some sentences in the sentence text box, and click Predict. You should see\nthe logits values on the right side of the screen. They are just a raw array of logits and\nhard to read, but you can see that the fourth value (which corresponds to the label\n“very positive”) is the largest and the model is working as expected.\n You can also directly make POST requests to the backend from the command line\nas follows:\ncurl -d '{\"sentence\": \"This is the best movie ever!\"}'\n    -H \"Content-Type: application/json\" \\\n    -X POST http:/./localhost:8000/predict\n",
      "content_length": 1347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "48\nCHAPTER 2\nYour first NLP application\nThis should return the same JSON as you saw above:\n{\"logits\":[-0.2549717128276825,-0.35388273000717163,\n-0.0826418399810791,0.7183976173400879,0.23161858320236206]}\nOK, that’s it for now. We covered a lot in this chapter, but don’t worry—I just wanted\nto show you that it is easy to build an NLP application that actually works. You may\nhave found some books or online tutorials about neural networks and deep learning\nintimidating, or you may have even given up on learning before creating anything\nthat works. Notice that I didn’t even mention any concepts such as neurons, activa-\ntions, gradient, and partial derivatives, which other learning materials teach at the\nvery beginning. These concepts are indeed important and helpful to know, but thanks\nto powerful frameworks such as AllenNLP, you are also able to build practical NLP\napplications without fully understanding their details. In later chapters, I’ll go into\nmore details and discuss these concepts as needed.\nSummary\nSentiment analysis is a text analytic technique to automatically identify subjec-\ntive information within text, such as its polarity (positive or negative).\nTrain, dev, and test sets are used to train, choose, and evaluate machine learn-\ning models.\nWord embeddings represent the meaning of words using vectors of real\nnumbers.\nRecurrent neural networks (RNNs) and linear layers are used to convert a vec-\ntor to another vector of different size.\nNeural networks are trained using an optimizer so that the loss (discrepancy\nbetween the actual and the desired output) is minimized.\nIt is important to monitor the metrics for the train and the dev sets during\ntraining to avoid overfitting.\n",
      "content_length": 1718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "49\nWord and document\nembeddings\nIn chapter 2, I pointed out that neural networks can deal only with numbers,\nwhereas almost everything in natural language is discrete (i.e., separate concepts).\nTo use neural networks in your NLP application, you need to convert linguistic\nunits to numbers, such as vectors. For example, if you wish to build a sentiment\nanalyzer, you need to convert the input sentence (sequence of words) into a\nsequence of vectors. In this chapter, we’ll discuss word embeddings, which are the\nkey to achieving this bridging. We’ll also touch upon a couple of fundamental\nThis chapter covers\nWhat word embeddings are and why they \nare important\nHow the Skip-gram model learns word \nembeddings and how to implement it\nWhat GloVe embeddings are and how to use \npretrained vectors\nHow to use Doc2Vec and fastText to train more \nadvanced embeddings\nHow to visualize word embeddings\n",
      "content_length": 902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "50\nCHAPTER 3\nWord and document embeddings\nlinguistic components that are important in understanding embeddings and neural\nnetworks in general.\n3.1\nIntroducing embeddings\nAs we discussed in chapter 2, an embedding is a real-valued vector representation of\nsomething that is usually discrete. In this section, we’ll revisit what embeddings are\nand discuss in detail what roles they play in NLP applications.\n3.1.1\nWhat are embeddings?\nA word embedding is a real-valued vector representation of a word. If you find the\nconcept of vectors intimidating, think of them as single-dimensional arrays of float\nnumbers, like the following:\nvec(\"cat\") = [0.7, 0.5, 0.1] \nvec(\"dog\") = [0.8, 0.3, 0.1]\nvec(\"pizza\") = [0.1, 0.2, 0.8]\nBecause each array contains three elements, you can plot them as points in a 3-D\nspace as in figure 3.1. Notice that semantically-related words (“cat” and “dog”) are\nplaced close to each other.\nNOTE\nIn fact, you can embed (i.e., represent by a list of numbers) not just\nwords but also almost anything—characters, sequences of characters, sen-\ntences, or categories. You can embed any categorical variables using the same\nmethod, although in this chapter, we’ll focus on two of the most important\nconcepts in NLP—words and sentences.\n3.1.2\nWhy are embeddings important?\nWhy are embeddings important? Well, word embeddings are not just important but\nessential for using neural networks to solve NLP tasks. Neural networks are pure math-\nematical computation models that can deal only with numbers. They can’t do symbolic\noperations, such as concatenating two strings or conjugating a verb to past tense, unless\nx\ny\nz\nCat\nPizza\nDog\nFigure 3.1\nWord embeddings on a 3-D space\n",
      "content_length": 1694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "51\nIntroducing embeddings\nthese items are all represented by numbers and arithmetic operations. On the other\nhand, almost everything in NLP, such as words and labels, is symbolic and discrete. This\nis why you need to bridge these two worlds, and using embeddings is a way to do it. See\nfigure 3.2 for an overview on how to use word embeddings for an NLP application.\nWord embeddings, just like any other neural network models, can be trained, because\nthey are simply a collection of parameters (or “magic constants,” which we talked\nabout in the previous chapter). Embeddings are used with your NLP model in the fol-\nlowing three scenarios:\nScenario 1: Train word embeddings and your model at the same time using the\ntrain set for your task.\nScenario 2: First, train word embeddings independently using a larger text data-\nset. Alternatively, obtain pretrained word embeddings from somewhere else.\nThen initialize your model using the pretrained word embeddings, and train\nthem and your model at the same time using the train set for your task.\nScenario 3: Same as scenario 2, except you fix word embeddings while you train\nyour model.\nIn the first scenario, word embeddings are initialized randomly and trained in con-\njunction with your NLP model using the same dataset. This is basically how we built the\nsentiment analyzer in chapter 2. Using an analogy, this is like having a dance teacher\nteach a baby to walk and dance at the same time. It is not an entirely impossible feat (in\nfact, some babies might end up being better, maybe more creative dancers by skipping\nthe walking part, but don’t try this at home), but rarely a good idea. Babies would\nNLP\nmodel\n \nPrediction\nTraining data\nLarge text data\nWord \nembeddings\nWords\ndog\nchocolate\nbark\ncat\nFigure 3.2\nUsing word \nembeddings with NLP models\n",
      "content_length": 1807,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "52\nCHAPTER 3\nWord and document embeddings\nprobably have a much better chance if they are taught how to stand and walk properly\nfirst, and then how to dance.\n Similarly, it’s not uncommon to train an NLP model and word embeddings as its\nsubcomponent at the same time. But many large-scale, high-performance NLP mod-\nels usually rely on external word embeddings that are pretrained using larger datasets\n(scenarios 2 and 3). Word embeddings can be learned from unlabeled large text\ndatasets—that is, a large amount of plain text data (e.g., Wikipedia dumps), which are\nusually more readily available than the train datasets for your task (e.g., the Stanford\nSentiment Treebank). By leveraging such large textual data, you can teach your model\na lot about how natural language works even before it sees a single instance from the\ndataset for your task. Training a machine learning model on one task and repurposing\nit for another task is called transfer learning, which is becoming increasingly popular in\nmany machine learning domains, NLP included. We’ll further discuss transfer learn-\ning in chapter 9.\n Using the dancing baby analogy again, most healthy babies figure out how to stand\nand walk themselves. They may get some help from adults, usually from their close\ncaregivers such as parents. This form of “help,” however, is usually a lot more abun-\ndant and cheaper than the “training signal” you get from a hired dance teacher, which\nis why it’s a lot more effective if they learn how to walk first, then move on to dancing.\nMany skills used for walking transfer to dancing.\n The difference between scenarios 2 and 3 is whether the word embeddings are\nadjusted, or fine-tuned, while your NLP model is trained. Whether or not this is effec-\ntive may depend on your task and the dataset. Teaching your toddler ballet may have a\ngood effect on how they walk (e.g., by improving their posture), which in turn could\nhave a positive effect on how they dance, but scenario 3 doesn’t allow this to happen.\n The only remaining question you might have is: Where do embeddings come\nfrom? I mentioned earlier that they can be trained from a large amount of plain text.\nThis chapter explains how this is possible and what models are used to achieve this.\n3.2\nBuilding blocks of language: Characters, words, and phrases\nBefore I explain word-embedding models, I’m going to touch upon some basic\nconcepts of language, such as characters, words, and phrases. It helps to understand\nthese concepts when you design the structure of your NLP application. Figure 3.3\nshows some examples of those concepts.\n3.2.1\nCharacters\nA character (also called a grapheme in linguistics) is the smallest unit of a writing system.\nIn written English, “a,” “b,” and “z” are characters. Characters do not necessarily carry\nmeaning by themselves or represent any fixed sound when spoken, although in some\nlanguages (e.g., Chinese), most do. A typical character in many languages can be rep-\nresented by a single Unicode codepoint (by string literals such as \"\\uXXXX\" in Python),\nbut this is not always the case. Many languages use a combination of more than one Uni-\ncode codepoint (e.g., accent marks) to represent a single character. Punctuation\nmarks, such as “.” (period), “,” (comma), and “?” (question mark), are also characters.\n",
      "content_length": 3306,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "53\nBuilding blocks of language: Characters, words, and phrases\n3.2.2\nWords, tokens, morphemes, and phrases\nA word is the smallest unit in a language that can be uttered independently and that\nusually carries some meaning. In English, “apple,” “banana,” and “zebra” are words.\nIn most written languages that use alphabetic scripts, words are usually separated by\nspaces or punctuation marks. In some languages, like Chinese, Japanese, and Thai,\nhowever, words are not explicitly delimited by spaces and require a preprocessing step\ncalled word segmentation to identify words in a sentence.\n A closely related concept to a word in NLP is a token. A token is a string of contigu-\nous characters that play a certain role in a written language. Most words (“apple,”\n“banana,” “zebra”) are also tokens when written. Punctuation marks such as the excla-\nmation mark (“!”) are tokens but not words, because you can’t utter them in isolation.\nWord and token are often used interchangeably in NLP. In fact, when you see “word”\nin NLP text (including this book), it often means “token,” because most NLP tasks\ndeal only with written text that is processed in an automatic way. Tokens are the out-\nput of a process called tokenization, which I’ll explain more below.\n Another closely related concept is morpheme. A morpheme is the smallest unit of\nmeaning in a language. A typical word consists of one or more morphemes. For exam-\nple, “apple” is a word and also a morpheme. “Apples” is a word comprised of two mor-\nphemes, “apple” and “-s,” which is used to signify the noun is plural. English contains\nmany other morphemes, including “-ing,” “-ly,” “-ness,” and “un-.” The process for iden-\ntifying morphemes in a word or a sentence is called morphological analysis, and it has a\nwide range of NLP/linguistics applications, but this is outside the scope of this book.\n A phrase is a group of words that play a certain grammatical role. For example, “the\nquick brown fox” is a noun phrase (a group of words that behaves like a noun),\nwhereas “jumps over the lazy dog” is a verb phrase. The concept of phrase may be\nused somewhat liberally in NLP to simply mean any group of words. For example, in\nmany NLP literatures and tasks, words like “Los Angeles” are treated as phrases,\nalthough, linguistically speaking, they are closer to a word.\n3.2.3\nN-grams\nFinally, you may encounter the concept of n-grams in NLP. An n-gram is a contiguous\nsequence of one or more occurrences of linguistic units, such as characters and words.\nFor example, a word n-gram is a contiguous sequence of words, such as “the” (one\nCharacters\nWords\nMorphemes\nTokens\nPhrases\nWord n-grams\nThe quick brown fox jumps over the lazy dog.\nA\nbrown\nThe quick brown fox\nthe lazy dog\nThe fox the .\nbrown\njump\n-s\ndog\nover\nb\nq\n.\nquick brown\nbrown fox jumps\nFigure 3.3\nBuilding blocks \nof language used in NLP \n",
      "content_length": 2860,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "54\nCHAPTER 3\nWord and document embeddings\nword), “quick brown” (two words), “brown fox jumps” (three words). Similarly, a char-\nacter n-gram is composed of characters, such as “b” (one character), “br” (two charac-\nters), “row” (three characters), and so on, which are all character n-grams made from\n“brown.” An n-gram of size 1 (when n = 1) is called a unigram. N-grams of size 2 and 3\nare called a bigram and a trigram, respectively.\n Word n-grams are often used as proxies for phrases in NLP, because if you enumer-\nate all the n-grams of a sentence, they often contain linguistically interesting units that\ncorrespond to phrases such as “Los Angeles” and “take off.” In a similar vein, we use\ncharacter n-grams when we want to capture subword units that roughly correspond to\nmorphemes. In NLP, when you see “n-grams” (without a qualifier), they are often word\nn-grams.\nNOTE\nInterestingly, in search and information retrieval, n-grams often mean\ncharacter n-grams used for indexing documents. Be mindful which type of\nn-grams are implied by the context when you read papers.\n3.3\nTokenization, stemming, and lemmatization\nWe covered some basic linguistic units often encountered in NLP. In this section, I\nintroduce some steps where linguistic units are processed in a typical NLP pipeline. \n3.3.1\nTokenization\nTokenization is a process where the input text is split into smaller units. There are two\ntypes of tokenization: word and sentence tokenization. Word tokenization splits a sen-\ntence into tokens (rough equivalent to words and punctuation), which I mentioned\nearlier. Sentence tokenization, on the other hand, splits a piece of text that may include\nmore than one sentence into individual sentences. If you say tokenization, it usually\nmeans word tokenization in NLP.\n Many NLP libraries and frameworks support tokenization out of the box, because\nit is one of the most fundamental and widely used preprocessing steps in NLP. In what\nfollows, I show you how to do tokenization using two popular NLP libraries—NLTK\n(https://www.nltk.org/) and spaCy (https://spacy.io/).\nNOTE\nBefore running the example code in this section, make sure the libraries\nare both installed. In a typical Python environment, they can be installed by run-\nning pip install nltk and pip install spacy. After installation, you need\nto download the necessary data and models by running python -c \"import\nnltk; nltk.download('punkt')\" for NLTK, and python -m spacy down-\nload en for spaCy from the command line. You can also run all the examples in\nthis section via Google Colab (http://realworldnlpbook.com/ch3.html#tokeni\nzation) without installing any Python environments or dependencies.\nTo use the default word and sentence tokenizers from NLTK, you can import them\nfrom nltk.tokenize package as follows:\n>>> import nltk\n>>> from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "content_length": 2869,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "55\nTokenization, stemming, and lemmatization\nYou can call these methods with a string, and they return a list of words or sentences as\nfollows:\n>>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of \nthem.\\n\\nThanks.'''\n>>> word_tokenize(s)\n['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please',\n 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n>>> sent_tokenize(s)\n['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.',\n 'Thanks.']\nNLTK implements a wide range of tokenizers in addition to the default one we used\nhere. Its documentation page (https://www.nltk.org/api/nltk.tokenize.html) is a\ngood starting point if you are interested in exploring more options.\n You can tokenize words and sentences as follows using spaCy:\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_sm')\n>>> doc = nlp(s)\n>>> [token.text for token in doc]\n['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'in', 'New', 'York', '.', ' ',\n 'Please', 'buy', 'me', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.']\n>>> [sent.string.strip() for sent in doc.sents]\n['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.', 'Thanks.']\nNote that the results from NLTK and spaCy are slightly different. For example, spaCy’s\nword tokenizer leaves newlines ('\\n') intact. The behavior of tokenizers differs from\none implementation to another, and there is no single standard solution that every\nNLP practitioner agrees upon. Although standard libraries such as NLTK and spaCy\ngive a good baseline, be ready to experiment depending on your task and data. Also, if\nyou are dealing with languages other than English, your options may vary (and might\nbe quite limited depending on the language). If you are familiar with the Java ecosys-\ntem, Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/) is another good\nNLP framework worth checking out. \n Finally, an increasingly popular and important tokenization method for neural\nnetwork-based NLP models is byte-pair encoding (BPE). Byte-pair encoding is a purely\nstatistical technique to split text into sequences of characters in any language, relying\nnot on heuristic rules (such as spaces and punctuations) but only on character statis-\ntics from the dataset. We’ll study byte-pair encoding more in depth in chapter 10.\n3.3.2\nStemming\nStemming is a process for identifying word stems. A word stem is the main part of a word\nafter stripping off its affixes (prefixes and suffixes). For example, the word stem of\n“apples” (plural) is “apple.” The stem of “meets” (with a third-person singular s) is\n",
      "content_length": 2586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "56\nCHAPTER 3\nWord and document embeddings\n“meet.” The word stem of “unbelievable” is “believe.” It is often a part that remains\nunchanged after inflection.\n Stemming—that is, normalizing words to something closer to their original\nforms—has great benefits in many NLP applications. In search, for example, you can\nimprove the chance of retrieving relevant documents if you index documents using\nword stems instead of words. In many feature-based NLP pipelines, you’d be able to\nalleviate the OOV (out-of-vocabulary) problem by dealing with word stems instead.\nFor example, even if your dictionary doesn’t have an entry for “apples,” you can\ninstead use its stem “apple” as a proxy. \n The most popular algorithm used for stemming English words is called the Porter\nstemming algorithm, originally written by Martin Porter. It consists of a number of rules\nfor rewriting affixes (e.g., if a word ends with “-ization,” change it to “-ize”). NLTK\nimplements a version of the algorithm as the PorterStemmer class, which can be\nused as follows:\n>>> from nltk.stem.porter import PorterStemmer\n>>> stemmer = PorterStemmer()\n>>> words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n...          'died', 'agreed', 'owned', 'humbled', 'sized',\n...          'meetings', 'stating', 'siezing', 'itemization',\n...          'sensational', 'traditional', 'reference', 'colonizer',\n...          'plotted']\n>>> [stemmer.stem(word) for word in words]\n['caress', 'fli', 'die', 'mule', 'deni',\n 'die', 'agre', 'own', 'humbl', 'size',\n 'meet', 'state', 'siez', 'item',\n 'sensat', 'tradit', 'refer', 'colon',\n 'plot']\nStemming is not without its limitations. In many cases, it can be too aggressive. For\nexample, as you can see from the previous example, the Porter stemming algorithm\nchanges both “colonizer” and “colonize” to just “colon.” I can’t imagine many applica-\ntions would be happy to treat those three words as an identical entry. Also, many stem-\nming algorithms do not consider the context or even the parts of speech. In the\nprevious example, “meetings” is changed to “meet,” but you could argue that “meet-\nings” as a plural noun should be stemmed to “meeting,” not “meet.” For those rea-\nsons, as of today, few NLP applications use stemming.\n3.3.3\nLemmatization\nA lemma is the original form of a word that you often find as a head word in a dictio-\nnary. It is also the base form of the word before inflection. For example, the lemma of\n“meetings” (as a plural noun) is “meeting.” The lemma of “met” (a verb past form) is\n“meet.” Notice that it differs from stemming, which simply strips off affixes from a\nword and cannot deal with such irregular verbs and nouns.\n It is straightforward to run lemmatization using NLTK, as shown here:\n>>> from nltk.stem import WordNetLemmatizer\n>>> lemmatizer = WordNetLemmatizer()\n",
      "content_length": 2813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "57\nSkip-gram and continuous bag of words (CBOW)\n>>> [lemmatizer.lemmatize(word) for word in words]\n['caress', 'fly', 'dy', 'mule', 'denied',\n 'died', 'agreed', 'owned', 'humbled', 'sized',\n 'meeting', 'stating', 'siezing', 'itemization',\n 'sensational', 'traditional', 'reference', 'colonizer',\n 'plotted']\nAnd the spaCy code looks like this:\n>>> doc = nlp(' '.join(words))\n>>> [token.lemma_ for token in doc]\n['caress', 'fly', 'die', 'mule', 'deny',\n 'die', 'agree', 'own', 'humble', 'sized',\n 'meeting', 'state', 'siezing', 'itemization',\n 'sensational', 'traditional', 'reference', 'colonizer',\n 'plot']\nNote that lemmatization inherently requires that you know the part of speech of the\ninput word, because the lemma depends on it. For example, “meeting” as a noun\nshould be lemmatized to “meeting,” whereas the result should be “meet” if it’s a verb.\nWordNetLemmatizer in NLTK treats everything as a noun by default, which is why\nyou see many unlemmatized words in the result (“agreed,” “owned,” etc.). On the\nother hand, spaCy automatically infers parts of speech from the word form and the\ncontext, which is why most of the lemmatized words are correct in its result. Lemmati-\nzation is more resource-intensive than stemming because it requires statistical analysis\nof the input and/or some form of linguistic resources such as dictionaries, but it has a\nwider range of applications in NLP due to its linguistic correctness.\n3.4\nSkip-gram and continuous bag of words (CBOW)\nIn previous sections, I explained what word embeddings are and how they are used in\nNLP applications. In this section, we’ll start exploring how to calculate word embed-\ndings from large textual data using two popular algorithms—Skip-gram and CBOW.\n3.4.1\nWhere word embeddings come from\nIn section 3.1, I explained that word embeddings represent each word in the vocabu-\nlary using a single-dimensional array of float numbers:\nvec(\"cat\") = [0.7, 0.5, 0.1] \nvec(\"dog\") = [0.8, 0.3, 0.1]\nvec(\"pizza\") = [0.1, 0.2, 0.8]\nNow, there’s one important piece of information missing from the discussion so far.\nWhere do those numbers come from? Do we hire a group of experts and have them\ncome up with those numbers? It would be virtually impossible to assign them by hand.\nHundreds of thousands of unique words exist in a typical large corpus, and the arrays\nshould be at least around 100-dimensional long to be effective, which means you need\nto tweak more than tens of millions of numbers.\n More importantly, what should those numbers look like? How do you determine\nwhether you should assign a 0.8 to the first element of the “dog” vector, or 0.7, or any\nother number?\n",
      "content_length": 2647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "58\nCHAPTER 3\nWord and document embeddings\n The answer is that those numbers are also trained using a training dataset and a\nmachine learning model like any other model in this book. In what follows, I’ll intro-\nduce and implement one of the most popular models to train word embeddings—the\nSkip-gram model.\n3.4.2\nUsing word associations\nFirst, let’s step back and think how humans learn concepts such as “a dog.” I don’t\nthink any of you have ever been explicitly taught what a dog is. You knew this thing\ncalled “dog” since you were a toddler without anyone else telling you, “Oh by the way,\nthere’s this thing called ‘dog’ in this world. It’s a four-legged animal that barks.” How\nis this possible? You acquire the concept through a large amount of physical (touch-\ning and smelling dogs), cognitive (seeing and hearing dogs), and linguistic (reading\nand hearing about dogs) interactions with the external world.\n Now let’s think about what it takes to teach the concept of “dog” to a computer.\nCan we get a computer to “experience” interactions with the external world related to\nthe concept of a dog? Although typical computers cannot move around and have\ninteractions with actual dogs (well, not yet, as of this writing), one possible way to do\nthis without teaching the computer what “dog” means is to use association relative to\nother words. For example, what words tend to appear together with the word “dog” if\nyou look at its appearances in a large text corpus? “Pet,” “tail,” “smell,” “bark,”\n“puppy”—there can be countless options. How about “cat”? Maybe “pet,” “tail,” “fur,”\n“meow,” “kitten,” and so on. Because “dog” and “cat” have a lot in common conceptu-\nally (they are both popular pet animals with a tail, etc.), these two sets of context\nwords also have large overlap. In other words, you can guess how close two words are\nto each other by looking at what other words appear in the same context. This is\ncalled the distributional hypothesis, and it has a long history in NLP.\nNOTE\nThere’s a related term used in artificial\nintelligence—distributed representations. Distributed\nrepresentations of words are simply another name\nfor word embeddings. Yes, it’s confusing, but both\nterms are commonly used in NLP.\nWe are now one step closer. If two words have a lot of\ncontext words in common, we can give similar vectors\nto those two words. You can think of a word vector as a\n“compressed” representation of its context words. Then\nthe question becomes: how can you “decompress” a\nword vector to obtain its context words? How can you\neven represent a set of context words mathematically?\nConceptually, we’d like to come up with a model that\ndoes something like the one in figure 3.4.\n One way to represent a set of words mathematically\nis to assign a score to each word in the vocabulary.\ndog\nDecompressor\nWord \nembeddings\nContext words\nbark\npet\nsmell\npuppy\nFigure 3.4\nDecompressing a \nword vector\n",
      "content_length": 2915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "59\nSkip-gram and continuous bag of words (CBOW)\nInstead of representing context words as a set, we can think of them as an associative\narray (dict in Python) from words to their “scores” that correspond to how related\neach word is to “dog,” as shown next:\n{\"bark\": 1.4,\n \"chocolate\": 0.1,\n ...,\n \"pet\": 1.2,\n ...,\n \"smell\": 0.6,\n ...}\nThe only remaining piece of the model is how to come up with those “scores.” If you\nsort this list by word IDs (which may be assigned alphabetically), the scores can be\nconveniently represented by an N-dimensional vector, where N is the size of the entire\nvocabulary (the number of unique context words we consider), as follows:\n[1.4, 0.1, ..., 1.2, ..., 0.6, ...]\nAll the “decompressor” needs to do is expand the word-embedding vector (which has\nthree dimensions) to another vector of N dimensions.\n This may sound very familiar to some of you—yes, it’s exactly what linear layers\n(aka fully connected layers) do. I briefly talked about linear layers in section 2.4.2, but\nthis is a perfect time to go deeper into what they really do.\n3.4.3\nLinear layers\nLinear layers transform a vector into another vector in a linear fashion, but how\nexactly do they do this? Before talking about vectors, let’s simplify and start with num-\nbers. How would you write a function (say, a method in Python) that transforms a\nnumber into another one in a linear fashion? Remember, being linear means the out-\nput always changes by a fixed amount (say, w) if you change the input by 1, no matter\nwhat the value of the input is. For example, 2.0 * x is a linear function, because the\nvalue always increases by 2.0 if you increase x by 1, no matter what value x is. You can\nwrite a general version of such a function as follows:\ndef linear(x):\n    return w * x + b\nLet’s now assume the parameters w and b are fixed and defined somewhere else. You\ncan confirm that the output (the return value) always changes by w if you increase or\ndecrease x by 1. b is the value of the output when x = 0. It is called a bias in machine\nlearning.\n Now, what if there are two input variables, say, x1 and x2? Can you still write a\nfunction that transforms two input variables into another number in a linear way? Yes,\nand there’s very little change required to do this, as shown next:\ndef linear2(x1, x2):\n    return w1 * x1 + w2 * x2 + b\n",
      "content_length": 2338,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "60\nCHAPTER 3\nWord and document embeddings\nYou can confirm its linearity by checking that the output changes by w1 if you\nchange x1 by 1, and the output changes by w2 if you change x2 by 1, regardless of\nthe value of the other variable. Bias b is still the value of the output when x1 and x2\nare both 0.\n For example, assume we have w1 = 2.0, w2 = -1.0, and b = 1. For an input (1, 1),\nthe function returns 2. If you increase x1 by 1 and give (2, 1) as the input, you’ll get\n4, which is w1 more than 2. If you increase x2 by 1 and give (1, 2) as the input, you’ll\nget 1, which is 1 less (or w2 more) than 2.\n At this point, we can start thinking about generalizing this to vectors. What if there\nare two output variables, say, y1 and y2? Can you still write a linear function with\nrespect to the two inputs? Yes, you can simply duplicate the linear transformation\ntwice, with different weights and biases, as follows:\ndef linear3(x1, x2):\n    y1 = w11 * x1 + w12 * x2 + b1\n    y2 = w21 * x1 + w22 * x2 + b2\n    return [y1, y2]\nOK, it’s getting a little bit complicated, but you effectively wrote a function for a linear\nlayer that transforms a two-dimensional vector into another two-dimensional vector! If\nyou increase the input dimension (the number of input variables), this method would\nget horizontally long (i.e., more additions per line), whereas if you increase the out-\nput dimension, this method would get vertically long (i.e., more lines). \n In practice, deep learning libraries and frameworks implement linear layers in a\nmore efficient, generic way, and often most of the computation happens on a GPU.\nHowever, knowing how linear layers—the most important, simplest form of neural\nnetworks—work conceptually should be essential for understanding more complex\nneural network models.\nNOTE\nIn AI literature, you may encounter the concept of perceptrons. A per-\nceptron is a linear layer with only one output variable, applied to a classifica-\ntion problem. If you stack multiple linear layers (= perceptrons), you get a\nmultilayer perceptron, which is basically another name for a feedforward neural\nnetwork with some specific structures.\nFinally, you may be wondering where the constants w and b you saw in this section\ncome from. These are exactly the “magic constants” that I talked about in section\n2.4.1. You adjust these constants so that the output of the linear layer (and the neural\nnetwork as a whole) gets closer to what you want through a process called optimization.\nThese magic constants are also called parameters of a machine learning model.\n Putting this all together, the structure we want for the Skip-gram model is shown in\nfigure 3.5. This network is very simple. It takes a word embedding as an input and\nexpands it via a linear layer to a set of scores, one for each context word. Hopefully\nthis is not as intimidating as many people think! \n",
      "content_length": 2875,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "61\nSkip-gram and continuous bag of words (CBOW)\n3.4.4\nSoftmax\nNow let’s talk about how to “train” the Skip-gram model and learn the word embed-\ndings we want. The key here is to turn this into a classification task, where the model\npredicts what words appear in the context. The “context” here simply means a window\nof a fixed size (e.g., 5 + 5 words on both sides) centered around the target word (e.g.,\n“dog”). See figure 3.6 for an illustration when the window size is 2. This is actually a\n“fake” task because we are not interested\nin the prediction of the model per se, but\nrather in the by-product (word embed-\ndings) produced while training the\nmodel. In machine learning and NLP, we\noften make up a fake task to train some-\nthing else as a by-product. \nNOTE\nThis setting of machine learning, where the training labels are auto-\nmatically created from a given dataset, can also be called self-supervised learn-\ning. Recent popular techniques, such as word embeddings and language\nmodeling, all use self-supervision. \nIt is relatively easy to make a neural network solve a classification task. You need to do\nthe following two things:\nModify the network so that it produces a probability distribution.\nUse cross entropy as the loss function (we’ll cover this in detail shortly).\nWord\nembeddings\nbark\nchocolate\npet\nsmell\n…\n…\nLinear\nlayer\n \nScores\ndog\nFigure 3.5\nSkip-gram model structure\nTarget word\nContext words\nContext words\nI heard a dog barking in the distance.\nFigure 3.6\nTarget word and context words \n(when window size = 2)\n",
      "content_length": 1539,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "62\nCHAPTER 3\nWord and document embeddings\n You can use something called softmax to do the first. Softmax is a function that\nturns a vector of K float numbers to a probability distribution, by first “squashing” the\nnumbers so that they fit a range between 0.0–1.0, and then normalizing them so that\nthe sum equals 1. If you are not familiar with the concept of probabilities, replace\nthem with confidence. A probability distribution is a set of confidence values that the\nnetwork places on individual predictions (in this case, context words). Softmax does\nall this while preserving the relative ordering of the input float numbers, so large\ninput numbers still have large probability values in the output distribution. Figure 3.7\nillustrates this conceptually.\nAnother component required to turn a neural network into a classifier is cross entropy.\nCross entropy is a loss function used to measure the distance between two probability\ndistributions. It returns zero if two distributions match exactly and higher values if the\ntwo diverge. For classification tasks, we use cross entropy to compare the following:\nPredicted probability distribution produced by the neural network (output of\nsoftmax) \nTarget probability distribution, where the probability of the correct class is 1.0\nand everything else is 0.0\nThe predictions made by the Skip-gram model get closer and closer to the actual con-\ntext words, and word embeddings are learned at the same time.\n3.4.5\nImplementing Skip-gram on AllenNLP\nIt is relatively straightforward to turn this model into working code using AllenNLP.\nNote that all the code listed in this section can be executed on the Google Colab note-\nbook (http://realworldnlpbook.com/ch3.html#word2vec-nb). First, you need to\nimplement a dataset reader that reads a plain text corpus and turns it into a set of\ninstances that can be consumed by the Skip-gram model. The details of the dataset\nreader are not critical to the discussion here, so I’m going to omit the full code listing.\nYou can clone the code repository of this book (https://github.com/mhagiwara/\nrealworldnlp) and import it as follows:\nfrom examples.embeddings.word2vec import SkipGramReader\nA\nB\nC\nD\n…\n…\nScores\nProbability \ndistribution\nSoftmax\n…\nA\nB\nC\nD\n…\nFigure 3.7\nSoftmax\n",
      "content_length": 2266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "63\nSkip-gram and continuous bag of words (CBOW)\nAlternatively, if you are interested, you can see the full code from http://realworldnlp-\nbook.com/ch3.html#word2vec. You can use the reader as follows:\nreader = SkipGramReader()\ntext8 = reader.read('https:/./realworldnlpbook.s3.amazonaws.com/data/text8/\ntext8')\nAlso, be sure to import all the necessary modules and define some constants in this\nexample, as shown next:\nfrom collections import Counter\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import SimpleDataLoader\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training.trainer import GradientDescentTrainer\nfrom torch.nn import CosineSimilarity\nfrom torch.nn import functional\nEMBEDDING_DIM = 256\nBATCH_SIZE = 256\nWe are going to use the text8 (http://mattmahoney.net/dc/textdata) dataset in this\nexample. The dataset is an excerpt from Wikipedia and is often used for training toy\nword embedding and language models. You can iterate over the instances in the data-\nset. token_in is the input token to the model, and token_out is the output (the\ncontext word): \n>>> for inst in text8:\n>>>     print(inst)\n...\nInstance with fields:\n  token_in: LabelField with label: ideas in namespace: 'token_in'.'\n  token_out: LabelField with label: us in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: ideas in namespace: 'token_in'.'\n  token_out: LabelField with label: published in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: ideas in namespace: 'token_in'.'\n  token_out: LabelField with label: journal in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: in in namespace: 'token_in'.'\n  token_out: LabelField with label: nature in namespace: 'token_out'.'\nInstance with fields:\n  token_in: LabelField with label: in in namespace: 'token_in'.'\n  token_out: LabelField with label: he in namespace: 'token_out'.'\n",
      "content_length": 2043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "64\nCHAPTER 3\nWord and document embeddings\nInstance with fields:\n  token_in: LabelField with label: in in namespace: 'token_in'.'\n  token_out: LabelField with label: announced in namespace: 'token_out'.'\n...\nThen, you can build the vocabulary, as we did in chapter 2, as shown next:\nvocab = Vocabulary.from_instances(\n    text8, min_count={'token_in': 5, 'token_out': 5})\nNote that we are using min_count, which sets the lower bound on the number of\noccurrences for each token. Also, let’s define the data loader we use for training as\nfollows:\ndata_loader = SimpleDataLoader(text8, batch_size=BATCH_SIZE)\ndata_loader.index_with(vocab)\nLet’s then define an Embedding object that holds all the word embeddings we’d like\nto learn:\nembedding_in = Embedding(num_embeddings=vocab.get_vocab_size('token_in'),\n                         embedding_dim=EMBEDDING_DIM)\nHere, EMBEDDING_DIM is the length of each word vector (number of float numbers).\nA typical NLP application uses word vectors of a couple hundred dimensions long (in\nthis example, 256), but this value depends greatly on the task and the datasets. It is\noften suggested that you use longer word vectors as your training data grows.\n Finally, you need to implement the body of the Skip-gram model, as shown next.\nclass SkipGramModel(Model):   \n    def __init__(self, vocab, embedding_in):\n        super().__init__(vocab)\n        self.embedding_in = embedding_in   \n        self.linear = torch.nn.Linear(\n            in_features=EMBEDDING_DIM,\n            out_features=vocab.get_vocab_size('token_out'),\n            bias=False)   \n    def forward(self, token_in, token_out):   \n        \n        embedded_in = self.embedding_in(token_in)   \n        \n        logits = self.linear(embedded_in)   \n        loss = functional.cross_entropy(logits, token_out)  \n        return {'loss': loss}\nListing 3.1\nSkip-gram model implemented in AllenNLP\nAllenNLP requires every model \nto be inherited from Model.\nThe embedding object is passed from \noutside rather than defined inside.\nThis creates a linear\nlayer (note that we\ndon’t need biases).\nThe body of neural network computation \nis implemented in forward().\nConverts input tensors \n(word IDs) to word \nembeddings\nApplies the\nlinear layer\nComputes \nthe loss\n",
      "content_length": 2251,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "65\nSkip-gram and continuous bag of words (CBOW)\nA few things to note:\nAllenNLP requires every model to be inherited from Model, which can be\nimported from allennlp.models.\nModel’s initializer (__init__) takes a Vocabulary instance and any other\nparameters or submodels defined externally. It also defines any internal param-\neters or models.\nThe main computation of the model is defined in forward(). It takes all the\nfields from instances (in this example, token_in and token_out) as tensors\n(multidimensional arrays) and returns a dict that contains the 'loss' key,\nwhich will be used by the optimizer to train the model.\nYou can train this model using the following code.\nreader = SkipGramReader()\ntext8 = reader.read(' https:/./realworldnlpbook.s3.amazonaws.com/data/text8/\ntext8')\nvocab = Vocabulary.from_instances(\n    text8, min_count={'token_in': 5, 'token_out': 5})\ndata_loader = SimpleDataLoader(text8, batch_size=BATCH_SIZE)\ndata_loader.index_with(vocab)\nembedding_in = Embedding(num_embeddings=vocab.get_vocab_size('token_in'),\n                         embedding_dim=EMBEDDING_DIM)\nmodel = SkipGramModel(vocab=vocab,\n                      embedding_in=embedding_in)\noptimizer = optim.Adam(model.parameters())\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=data_loader,\n    num_epochs=5,\n    cuda_device=CUDA_DEVICE)\ntrainer.train()\nTraining takes a while, so I recommend truncating the training data first, say, by using\nonly the first one million tokens. You can do this by inserting text8 = list(text8)\n[:1000000] after reader.read(). After the training is finished, you can get related\nwords (words with the same meanings) using the method shown in listing 3.3. This\nmethod first obtains the word vector for a given word (token), then computes how\nsimilar it is to every other word vector in the vocabulary. The similarity is calculated\nusing something called the cosine similarity. In simple terms, the cosine similarity is the\nopposite of the angle between two vectors. If two vectors are identical, the angle\nbetween them is zero, and the similarity will be 1, which is the largest possible value. If\nListing 3.2\nCode for training the Skip-gram model\n",
      "content_length": 2217,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "66\nCHAPTER 3\nWord and document embeddings\ntwo vectors are perpendicular, the angle is 90 degrees, and the cosine will be 0. If the\nvectors are in totally opposite directions, the cosine will be –1.\ndef get_related(token: str, embedding: Model, vocab: Vocabulary, \n                num_synonyms: int = 10):\n    token_id = vocab.get_token_index(token, 'token_in')\n    token_vec = embedding.weight[token_id]\n    cosine = CosineSimilarity(dim=0)\n    sims = Counter()\n    for index, token in \nvocab.get_index_to_token_vocabulary('token_in').items():\n        sim = cosine(token_vec, embedding.weight[index]).item()\n        sims[token] = sim\n    return sims.most_common(num_synonyms)\nIf you run this for words “one” and “december,” you get the lists of related words\nshown in table 3.1. Although you can see some words that are not related to the query\nword, overall, the results look good. \nOne final note: you need to implement a couple of techniques if you want to use Skip-\ngram to train high-quality word vectors in practice, namely, negative sampling and\nsubsampling of high-frequency words. Although they are important concepts, they\ncan be a distraction if you are just starting out and would like to learn the basics of\nNLP. If you are interested in learning more, check out this blog post that I wrote on\nthis topic: http://realworldnlpbook.com/ch3.html#word2vec-blog.\nListing 3.3\nMethod to obtain related words using word embeddings\nTable 3.1\nRelated words for “one” and “december\"\n“one”\n“december”\none\ndecember\nnine\njanuary\neight\nnixus\nsix\nlondini\nfive\nplantarum\nseven\njune\nthree\nsmissen\nfour\nfebruary\nd\nqanuni\nactress\noctober\n",
      "content_length": 1631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "67\nSkip-gram and continuous bag of words (CBOW)\n3.4.6\nContinuous bag of words (CBOW) model\nAnother word-embedding model that is often mentioned along with the Skip-gram\nmodel is the continuous bag of words (CBOW) model. As a close sibling of the Skip-\ngram model, proposed at the same time (http://realworldnlpbook.com/ch3.html#\nmikolov13), the architecture of the CBOW model looks similar to that of the Skip-\ngram model but flipped upside down. The “fake” task the model is trying to solve is\nto predict the target word from a set of its context words. This is also similar to fill-in-\nthe-blank type of questions. For example, if you see a sentence “I heard a ___ barking\nin the distance,” most of you can probably guess the answer “dog” instantly. Figure\n3.8 shows the structure of this model.\nFigure 3.8\nContinuous bag of words (CBOW) model\nI’m not going to implement the CBOW model from scratch here for a couple of rea-\nsons. It should be straightforward to implement if you understand the Skip-gram\nmodel. Also, the accuracy of the CBOW model measured on word semantic tasks is\na\nin\nWord\nembeddings\ndog\nchocolate\nwolf\ncat\n…\n…\nJoin\nLinear\nlayer\n \nScores\nbarking\nheard\n",
      "content_length": 1175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "68\nCHAPTER 3\nWord and document embeddings\nusually slightly lower than that of Skip-gram, and CBOW is less often used in NLP than\nSkip-gram. Both models are implemented in the original Word2vec (https://code\n.google.com/archive/p/word2vec/) toolkit, if you want to try them yourself, although\nthe vanilla Skip-gram and CBOW models are less and less often used nowadays because\nof the advent of more recent, powerful word-embedding models (such as GloVe and\nfastText) that are covered in the rest of this chapter. \n3.5\nGloVe\nIn the previous section, I implemented Skip-gram and showed how you can train\nyour word embeddings using large text data. But what if you wanted to build your\nown NLP applications leveraging high-quality word embeddings while skipping all\nthe hassle? What if you couldn’t afford the computation and data required to train\nword embeddings? \n Instead of training word embeddings, you can always download pretrained word\nembeddings published by somebody else, which many NLP practitioners do. In this\nsection, I’m going to introduce another popular word-embedding model—GloVe,\nnamed after Global Vectors. Pretrained word embeddings generated by GloVe are prob-\nably the most widely used embeddings in NLP applications today. \n3.5.1\nHow GloVe learns word embeddings\nThe main difference between the two models described earlier and GloVe is that the\nformer is local. To recap, Skip-gram uses a prediction task where a context word\n(“bark”) is predicted from the target word (“dog”). CBOW basically does the opposite\nof this. This process is repeated as many times as there are word tokens in the dataset.\nIt basically scans the entire dataset and asks the question, “Can this word be predicted\nfrom this other word?” for every single occurrence of words in the dataset.\n Let’s think how efficient this algorithm is. What if there were two or more identical\nsentences in the dataset? Or very similar sentences? In that case, Skip-gram would\nrepeat the exact same set of updates multiple times. “Can ‘bark’ be predicted from\n‘dog’?” you might ask. But chances are you already asked that exact same question a\ncouple of hundred sentences ago. If you know that the words “dog” and “bark” appear\ntogether in the context N times in the entire dataset, why repeat this N times? It’s as if\nyou were adding “1” N times to something else (x + 1 + 1 + 1 + ... + 1) when you could\nsimply add N to it (x + N). Could we somehow use this global information directly?\n The design of GloVe is motivated by this insight. Instead of using local word\nco-occurrences, it uses aggregated word co-occurrence statistics in the entire dataset.\nLet’s say “dog” and “bark” co-occur N times in the dataset. I’m not going into the\ndetails of the model, but roughly speaking, the GloVe model tries to predict this num-\nber N from the embeddings of both words. Figure 3.9 illustrates this prediction task. It\nstill makes some predictions about word relations, but notice that it makes one predic-\ntion per a combination of word types, but Skip-gram does so for every combination of\nword tokens!\n",
      "content_length": 3082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "69\nGloVe\nTOKEN AND TYPE\nAs mentioned in section 3.3.1, a token is an occurrence of a\nword in text. There may be multiple occurrences of the same word in a cor-\npus. A type, on the other hand, is a distinctive, unique word. For example, in\nthe sentence “A rose is a rose is a rose,” there are eight tokens but only three\ntypes (“a,” “rose,” and “is”). If you are familiar with object-oriented program-\nming, they are roughly equivalent to instance and class. There can be multi-\nple instances of a class, but there is only one class for a concept.\n3.5.2\nUsing pretrained GloVe vectors\nIn fact, not many NLP practitioners train GloVe vectors from scratch by themselves.\nMore often, we download and use word embeddings, which are pretrained using large\ntext corpora. This is not only quick but usually beneficial in making your NLP applica-\ntions more accurate, because those pretrained word embeddings (often made public\nby the inventor of word-embedding algorithms) are usually trained using larger data-\nsets and more computational power than most of us can afford. By using pretrained\nword embeddings, you can “stand on the shoulders of giants” and quickly leverage\nhigh-quality linguistic knowledge distilled from large text corpora.\n In the rest of this section, let’s see how we can download and search for similar\nwords using pretrained GloVe embeddings. First, you need to download the data file.\nThe official GloVe website (https://nlp.stanford.edu/projects/glove/) provides multi-\nple word-embedding files trained using different datasets and vector sizes. You can\npick any one you like (although the file size could be large, depending on which one\nyou choose) and unzip it. In what follows, we assume you save it under the relative\npath data/glove/.\n Most word-embedding files are formatted in a similar way. Each line contains a\nword, followed by a sequence of numbers that correspond to its word vector. There\nare as many numbers as there are dimensions (in the GloVe files distributed on the\nwebsite above, you can tell the dimensionality from the filename suffix in the form of\ndog\nbark\nWord\nembeddings\nCo-occurrence\nN(“dog”, “bark”)\nGloVe\nFigure 3.9\nGloVe\n",
      "content_length": 2171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "70\nCHAPTER 3\nWord and document embeddings\nxxxd). Each field is delimited by a space. Here is an excerpt from one of the GloVe\nword-embedding files:\n...\nif 0.15778 0.17928 -0.45811 -0.12817 0.367 0.18817 -4.5745 0.73647 ...\none 0.38661 0.33503 -0.25923 -0.19389 -0.037111 0.21012 -4.0948 0.68349 ...\nhas 0.08088 0.32472 0.12472 0.18509 0.49814 -0.27633 -3.6442 1.0011 ...\n...\nAs we did in section 3.4.5, what we’d like to do is to take a query word (say, “dog”) and\nfind its neighbors in the N-dimensional space. One way to do this is to calculate the\nsimilarity between the query word and every other word in the vocabulary and sort the\nwords by their similarities, as shown in listing 3.3. Depending on the size of the vocab-\nulary, this approach could be very slow. It’s like linearly scanning an array to find an\nelement instead of using binary search. \n Instead, we’ll use approximate nearest neighbor algorithms to quickly search for\nsimilar words. In a nutshell, these algorithms enable us to quickly retrieve nearest\nneighbors without computing the similarity between every word pair. In particular,\nwe’ll use Annoy (https://github.com/spotify/annoy), a library for approximate\nneighbor search released from Spotify. You can install it by running pip install\nannoy. It implements a popular approximate nearest neighbor algorithm called\nlocally sensitive hashing (LSH) using random projection.\n To use Annoy to search similar words, you first need to build an index, which can be\ndone as shown in listing 3.4. Note that we are also building a dict from word indices\nto words and saving it to a separate file to facilitate the word lookup later (listing 3.5).\nfrom annoy import AnnoyIndex\nimport pickle\nEMBEDDING_DIM = 300\nGLOVE_FILE_PREFIX = 'data/glove/glove.42B.300d{}'\ndef build_index():\n    num_trees = 10\n    idx = AnnoyIndex(EMBEDDING_DIM)\n    index_to_word = {}\n    with open(GLOVE_FILE_PREFIX.format('.txt')) as f:\n        for i, line in enumerate(f):\n            fields = line.rstrip().split(' ')\n            vec = [float(x) for x in fields[1:]]\n            idx.add_item(i, vec)\n            index_to_word[i] = fields[0]\n    idx.build(num_trees)\n    idx.save(GLOVE_FILE_PREFIX.format('.idx'))\n    pickle.dump(index_to_word,\n                open(GLOVE_FILE_PREFIX.format('.i2w'), mode='wb'))\nListing 3.4\nBuilding an Annoy index\n",
      "content_length": 2341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "71\nGloVe\nReading a GloVe embedding file and building an Annoy index can be quite slow, but\nonce it’s built, accessing it and retrieving similar words can be performed very quickly.\nThis configuration is similar to search engines, where an index is built to achieve near\nreal-time retrieval of documents. This is suitable for applications where retrieval of\nsimilar items in real time is required but update of the dataset happens less frequently.\nExamples include search engines and recommendation engines.\ndef search(query, top_n=10):\n    idx = AnnoyIndex(EMBEDDING_DIM)\n    idx.load(GLOVE_FILE_PREFIX.format('.idx'))\n    index_to_word = pickle.load(open(GLOVE_FILE_PREFIX.format('.i2w'),\n                                     mode='rb'))\n    word_to_index = {word: index for index, word in index_to_word.items()}\n    \n    query_id = word_to_index[query]\n    word_ids = idx.get_nns_by_item(query_id, top_n)\n    for word_id in word_ids:\n        print(index_to_word[word_id])\nIf you run this for the words “dog” and “december,” you’ll get the lists of the 10 most-\nrelated words shown in table 3.2.\nYou can see that each list contains many related words to the query word. You see the\nidentical words at the top of each list—this is because the cosine similarity of two iden-\ntical vectors is always 1, its maximum possible value. \nListing 3.5\nUsing an Annoy index to retrieve similar words\nTable 3.2\nRelated words for “dog” and “december\"\n“dog”\n“december”\ndog\ndecember\npuppy\njanuary\ncat\noctober\ncats\nnovember\nhorse\nseptember\nbaby\nfebruary\nbull\naugust\nkid\njuly\nkids\napril\nmonkey\nmarch\n",
      "content_length": 1583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "72\nCHAPTER 3\nWord and document embeddings\n3.6\nfastText\nIn the previous section, we saw how to download pretrained word embeddings and\nretrieve related words. In this section, I’ll explain how to train word embeddings using\nyour own text data using fastText, a popular word-embedding toolkit. This is handy\nwhen your textual data is not in a general domain (e.g., medical, financial, legal, and\nso on) and/or is not in English.\n3.6.1\nMaking use of subword information\nAll the word-embedding methods we’ve seen so far in this chapter assign a distinct\nword vector for each word. For example, word vectors for “dog” and “cat” are treated\ndistinctly and are independently trained at the training time. At first glance, there\nseems to be nothing wrong about this. After all, they are separate words. But what if\nthe words were, say, “dog” and “doggy?” Because “-y” is an English suffix that denotes\nsome familiarity and affection (other examples include “grandma” and “granny” and\n“kitten” and “kitty”), these pairs of words have some semantic connection. However,\nword-embedding algorithms that treat words as distinct cannot make this connection.\nIn the eyes of these algorithms, “dog” and “doggy” are nothing more than, say,\nword_823 and word_1719.\n This is obviously limiting. In most languages, there’s a strong connection between\nword orthography (how you write words) and word semantics (what they mean). For\nexample, words that share the same stems (e.g., “study” and “studied,” “repeat” and\n“repeatedly,” and “legal” and “illegal”) are often related. By treating them as separate\nwords, word-embedding algorithms are losing a lot of information. How can they\nleverage word structures and reflect the similarities in the learned word embeddings? \n fastText, an algorithm and a word-embedding library developed by Facebook, is\none such model. It uses subword information, which means information about linguistic\nunits that are smaller than words, to train higher-quality word embeddings. Specifi-\ncally, fastText breaks words down to character n-grams (section 3.2.3) and learns\nembeddings for them. For example, if the target word is “doggy,” it first adds special\nsymbols at the beginning and end of the word and learns embeddings for <do, dog,\nogg, ggy, gy>, when n = 3. The vector for “doggy” is simply the sum of all these vec-\ntors. The rest of the architecture is quite similar to that of Skip-gram. Figure 3.10\nshows the structure of the fastText model.\n Another benefit in leveraging subword information is that it can alleviate the out-of-\nvocabulary (OOV) problem. Many NLP applications and models assume a fixed vocab-\nulary. For example, a typical word-embedding algorithm such as Skip-gram learns word\nvectors only for the words that were encountered in the train set. However, if a test set\ncontains words that did not appear in the train set (which are called OOV words), the\nmodel would be unable to assign any vectors to them. For example, if you train Skip-\ngram word embeddings from books published in the 1980s and apply them to modern\nsocial media text, how would it know what vectors to assign to “Instagram”? It won’t.\nOn the other hand, because fastText uses subword information (character n-grams), it\ncan assign word vectors to any OOV words, as long as they contain character n-grams\n",
      "content_length": 3315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "73\nfastText\nseen in the training data (which is almost always the case). It can potentially guess it’s\nrelated to something quick (“Insta”) and pictures (“gram”).\n3.6.2\nUsing the fastText toolkit\nFacebook provides the open source for the fastText toolkit, a library for training the\nword-embedding model discussed in the previous section. In the remainder of this\nsection, let’s see how it feels like to use this library to train word embeddings.\n First, go to their official documentation (http://realworldnlpbook.com/ch3.html\n#fasttext) and follow the instruction to download and compile the library. It is just a\nmatter of cloning the GitHub repository and running make from the command line\nin most environments. After compilation is finished, you can run the following com-\nmand to train a Skip-gram-based fastText model:\n$ ./fasttext skipgram -input ../data/text8 -output model\nWe are assuming there’s a text data file under ../data/text8 that you’d like to use\nas the training data, but change this if necessary. This will create a model.bin file,\nwhich is a binary representation of the trained model. After training the model, you\nCharacter n-gram\nembeddings\nbark\nchocolate\npet\nsmell\n…\n…\nAdd\nLinear\nlayer\n \nScores\ndog\ngy>\nggy\n<do\nFigure 3.10\nArchitecture\nof fastText\n",
      "content_length": 1276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "74\nCHAPTER 3\nWord and document embeddings\ncan obtain word vectors for any words, even for the ones that you’ve never seen in the\ntraining data, as follows:\n$ echo \"supercalifragilisticexpialidocious\" \\\n| ./fasttext print-word-vectors model.bin\nsupercalifragilisticexpialidocious 0.032049 0.20626 -0.21628 -0.040391 -\n0.038995 0.088793 -0.0023854 0.41535 -0.17251 0.13115 ...\n3.7\nDocument-level embeddings\nAll the models I have described so far learn embeddings for individual words. If you are\nconcerned only with word-level tasks such as inferring word relationships, or if they are\ncombined with more powerful neural network models such as recurrent neural net-\nworks (RNNs), they can be very useful tools. However, if you wish to solve NLP tasks that\nare concerned with larger linguistic structures such as sentences and documents using\nword embeddings and traditional machine learning tools such as logistic regression\nand support vector machines (SVMs), word-level embedding methods are still limited.\nHow can you represent larger linguistic units such as sentences using vector represen-\ntations? How can you use word embeddings for sentiment analysis, for example?\n One way to achieve this is to simply use the average of all word vectors in a sen-\ntence. You can average vectors by taking the average of first elements, second ele-\nments, and so on and make a new vector by combining these averaged numbers. You\ncan use this new vector as an input to traditional machine learning models. Although\nthis method is simple and can be effective, it is also very limiting. The biggest issue is\nthat it cannot take word order into consideration. For example, both sentences “Mary\nloves John.” and “John loves Mary.” would have exactly the same vectors if you simply\naveraged word vectors for each word in the sentence.\n NLP researchers have proposed models and algorithms that can specifically address\nthis issue. One of the most popular is Doc2Vec, originally proposed by Le and Mikolov\nin 2014 (https://cs.stanford.edu/~quocle/paragraph_vector.pdf). This model, as its\nname suggests, learns vector representations for documents. In fact, “document” here\nsimply means any variable-length piece of text that contains multiple words. Similar\nmodels are also called under many similar names such as Sentence2Vec, Paragraph2Vec,\nparagraph vectors (this is what the authors of the original paper used), but in essence,\nthey all refer to the variations of the same model.\n In the rest of this section, I’m going to discuss one of the Doc2Vec models called\ndistributed memory model of paragraph vectors (PV-DM). The model looks very similar to\nCBOW, which we studied earlier in this chapter, but with one key difference—one\nadditional vector, called paragraph vector, is added as an input. The model predicts the\ntarget word from a set of context words and the paragraph vector. Each paragraph is\nassigned a distinct paragraph vector. Figure 3.11 shows the structure of this PV-DM\nmodel. Also, PV-DM uses only context words that come before the target word for pre-\ndiction, but this is a minor difference.\n What effect would this paragraph vector have on the prediction task? Now you\nhave some extra information from the paragraph vector for predicting the target\nword. As the model tries to maximize the prediction accuracy, you can expect that the\n",
      "content_length": 3346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "75\nDocument-level embeddings\nparagraph vector is updated so that it provides some useful “context” information in\nthe sentence that is not collectively captured by the context word vectors. As a by-\nproduct, the model learns something that reflects the overall meaning of each para-\ngraph, along with word vectors.\n Several open source libraries and packages support Doc2Vec models, but one of\nthe most widely used is Gensim (https://radimrehurek.com/gensim/), which can be\ninstalled by running pip install gensim. Gensim is a popular NLP toolkit that sup-\nports a wide range of vector and topic models such as TF-IDF (term frequency and\ninverse document frequency), LDA (latent semantic analysis), and word embeddings. \n To train a Doc2Vec model using Gensim, you first need to read a dataset and con-\nvert documents to TaggedDocument. This can be done using the read_corpus()\nmethod shown here:\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.doc2vec import TaggedDocument\ndef read_corpus(file_path):\n    with open(file_path) as f:\n        for i, line in enumerate(f):\n            yield TaggedDocument(simple_preprocess(line), [i])\nI\na\nWord\nembeddings\nParagraph\nembedding\n \ndog\nchocolate\nwolf\ncat\n…\n…\nJoin\nLinear\nlayer\n \nScores\nheard\nparagraph_id\nFigure 3.11\nDistributed memory \nmodel of paragraph \nvectors\n",
      "content_length": 1323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "76\nCHAPTER 3\nWord and document embeddings\nWe are going to use a small dataset consisting of the first 200,000 English sentences\ntaken from the Tatoeba project (https://tatoeba.org/). You can download the dataset\nfrom http://mng.bz/7l0y. Then you can use Gensim’s Doc2Vec class to train the\nDoc2Vec model and retrieve similar documents based on the trained paragraph vec-\ntors, as demonstrated next.\n    from gensim.models.doc2vec import Doc2Vec\n    \n    train_set = list(read_corpus('data/mt/sentences.eng.200k.txt'))\n    model = Doc2Vec(vector_size=256, min_count=3, epochs=30)\n    model.build_vocab(train_set)\n    model.train(train_set,\n                total_examples=model.corpus_count,\n                epochs=model.epochs)\n    query_vec = model.infer_vector(\n        ['i', 'heard', 'a', 'dog', 'barking', 'in', 'the', 'distance'])\n    sims = model.docvecs.most_similar([query_vec], topn=10)\n    for doc_id, sim in sims:\n        print('{:3.2f} {}'.format(sim, train_set[doc_id].words)) \nThis will show you a list of documents similar to the input document “I heard a dog\nbarking in the distance,” as follows:\n0.67 ['she', 'was', 'heard', 'playing', 'the', 'violin']\n0.65 ['heard', 'the', 'front', 'door', 'slam']\n0.61 ['we', 'heard', 'tigers', 'roaring', 'in', 'the', 'distance']\n0.61 ['heard', 'dog', 'barking', 'in', 'the', 'distance']\n0.60 ['heard', 'the', 'door', 'open']\n0.60 ['tom', 'heard', 'the', 'door', 'open']\n0.60 ['she', 'heard', 'dog', 'barking', 'in', 'the', 'distance']\n0.59 ['heard', 'the', 'door', 'close']\n0.59 ['when', 'he', 'heard', 'the', 'whistle', 'he', 'crossed', 'the', 'street']\n0.58 ['heard', 'the', 'telephone', 'ringing']\nNotice that most of the retrieved sentences here are related to hearing sound. In fact,\nan identical sentence is in the list, because I took the query sentence from Tatoeba in\nthe first place! Gensim’s Doc2Vec class has a number of hyperparameters that you can\nuse to tweak the model. You can read further about the class on their reference page\n(https://radimrehurek.com/gensim/models/doc2vec.html).\n3.8\nVisualizing embeddings\nIn the final section of this chapter, we are going to shift our focus on visualizing word\nembeddings. As we’ve done earlier, retrieving similar words given a query word is a\ngreat way to quickly check if word embeddings are trained correctly. But it gets tiring\nand time-consuming if you need to check a number of words to see if the word\nembeddings are capturing semantic relationships between words as a whole.\nListing 3.6\nTraining a Doc2Vec model and retrieving similar documents\n",
      "content_length": 2566,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "77\nVisualizing embeddings\n As mentioned earlier, word embeddings are simply N-dimensional vectors, which\nare also “points” in an N-dimensional space. We were able to see those points visually\nin a 3-D space in figure 3.1 because N was 3. But N is typically a couple of hundred in\nmost word embeddings, and we cannot simply plot them on an N-dimensional space.\n A solution is to reduce the dimension down to something that we can see (two or\nthree dimensions) while preserving relative distances between points. This technique\nis called dimensionality reduction. We have a number of ways to reduce dimensionality,\nincluding PCA (principal component analysis) and ICA (independent component\nanalysis), but by far the most widely used visualization technique for word embeddings\nis called t-SNE (t-distributed Stochastic Neighbor Embedding, pronounced “tee-snee).\nAlthough the details of t-SNE are outside the scope of this book, the algorithm tries to\nmap points to a lower-dimensional space by preserving the relative neighboring rela-\ntionship between points in the original high-dimensional space.\n The easiest way to use t-SNE is to use Scikit-Learn (https://scikit-learn.org/), a\npopular Python library for machine learning. After installing it (usually just a matter\nof running pip install scikit-learn), you can use it to visualize the GloVe vec-\ntors read from a file as shown next (we use Matplotlib to draw the plot).\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\ndef read_glove(file_path):\n    with open(file_path) as f:\n        for i, line in enumerate(f):\n            fields = line.rstrip().split(' ')\n            vec = [float(x) for x in fields[1:]]\n            word = fields[0]\n            yield (word, vec)\nwords = []\nvectors = []\nfor word, vec in read_glove('data/glove/glove.42B.300d.txt'):\n    words.append(word)\n    vectors.append(vec)\nmodel = TSNE(n_components=2, init='pca', random_state=0)\ncoordinates = model.fit_transform(vectors)\nplt.figure(figsize=(8, 8))\nfor word, xy in zip(words, coordinates):\n    plt.scatter(xy[0], xy[1])\n    plt.annotate(word,\n                 xy=(xy[0], xy[1]),\n                 xytext=(2, 2),\n                 textcoords='offset points')\nplt.xlim(25, 55)\nplt.ylim(-15, 15)\nplt.show()\nListing 3.7\nUsing t-SNE to visualize GloVe embeddings\n",
      "content_length": 2308,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "78\nCHAPTER 3\nWord and document embeddings\nIn listing 3.7, I used xlim() and ylim() to limit the plotted range to magnify some\nareas that are of interest to us. You may want to try different values to focus on other\nareas in the plot.\n The code in listing 3.7 generates the plot shown in figure 3.12. There’s a lot of\ninteresting stuff going on here, but at a quick glance, you will notice the following\nclusters of words that are semantically related:\nBottom-left: web-related words (posts, article, blog, comments, . . . ).\nUpper-left: time-related words (day, week, month, year, . . . ).\nMiddle: numbers (0, 1, 2, . . . ). Surprisingly, these numbers are lined up in an\nincreasing order toward the bottom. GloVe figured out which numbers are\nlarger purely from a large amount of textual data.\nBottom-right: months (january, february, . . . ) and years (2004, 2005, . . . ).\nAgain, the years seem to be lined up in an increasing order, almost in parallel\nwith the numbers (0, 1, 2, . . . ).\nFigure 3.12\nGloVe embeddings visualized by t-SNE\n",
      "content_length": 1045,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "79\nSummary\nIf you think about it, it’s an incredible feat for a purely mathematical model to figure\nout these relationships among words, all from a large amount of text data. Hopefully,\nnow you know how much of an advantage it is if the model knows “july” and “june” are\nclosely related compared to needing to figure everything out starting from word_823\nand word_1719.\nSummary\nWord embeddings are numeric representations of words, and they help convert\ndiscrete units (words and sentences) to continuous mathematical objects\n(vectors).\nThe Skip-gram model uses a neural network with a linear layer and softmax to\nlearn word embeddings as a by-product of the “fake” word-association task.\nGloVe makes use of global statistics of word co-occurrence to train word embed-\ndings efficiently.\nDoc2Vec and fastText learn document-level embeddings and word embeddings\nwith subword information, respectively.\nYou can use t-SNE to visualize word embeddings.\n",
      "content_length": 954,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "80\nSentence classification\nIn this chapter, we are going to study the task of sentence classification, where an\nNLP model receives a sentence and assigns some label to it. A spam filter is an\napplication of sentence classification. It receives an email message and assigns\nwhether or not it is spam. If you want to classify news articles into different topics\nThis chapter covers\nHandling variable-length input with recurrent neural \nnetworks (RNN)\nWorking with RNNs and their variants (LSTMs and \nGRUs)\nUsing common evaluation metrics for classification \nproblems\nDeveloping and configuring a training pipeline using \nAllenNLP\nBuilding a language detector as a sentence \nclassification task\n",
      "content_length": 697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "81\nRecurrent neural networks (RNNs)\n(business, politics, sports, and so on), it’s also a sentence-classification task. Sentence\nclassification is one of the simplest NLP tasks that has a wide range of applications,\nincluding document classification, spam filtering, and sentiment analysis. Specifically,\nwe are going to revisit the sentiment classifier we introduced in chapter 2 and discuss\nits components in detail. At the end of this section, we are going to study another\napplication of sentence classification—language detection.\n4.1\nRecurrent neural networks (RNNs)\nThe first step in sentence classification is to represent variable-length sentences using\nneural networks (RNNs). In this section, I’m going to present the concept of recur-\nrent neural networks, one of the most important concepts in deep NLP. Many modern\nNLP models use RNNs in some way. I’ll explain why they are important, what they do,\nand introduce their simplest variant.\n4.1.1\nHandling variable-length input\nThe Skip-gram network structure shown in the previous chapter was simple. It takes a\nword vector of a fixed size, runs it through a linear layer, and obtains a distribution of\nscores over all the context words. The structure and the size of the input, output, and\nthe network are all fixed throughout the training.\n However, many, if not most, of what we deal with in NLP are sequences of variable\nlengths. For example, words, which are sequences of characters, can be short (“a,”\n“in”) or long (“internationalization”). Sentences (sequences of words) and docu-\nments (sequences of sentences) can be of any lengths. Even characters, if you look at\nthem as sequences of strokes, can be simple (e.g., “O” and “L” in English) or more\ncomplex (e.g., “鬱” is a Chinese character meaning “depression” which, depressingly,\nhas 29 strokes). \n As we discussed in the previous chapter, neural networks can handle only numbers\nand arithmetic operations. That was why we needed to convert words and documents\nto numbers through embeddings. We used linear layers to convert a fixed-length vec-\ntor into another. But to do something similar with variable-length inputs, we need to\nfigure out how to structure the neural networks so that they can handle them.\n One idea is to first convert the input (e.g., a sequence of words) to embeddings,\nthat is, a sequence of vectors of floating-point numbers, then average them. Let’s\nassume the input sentence is sentence = [\"john\", \"loves\", \"mary\", \".\"], and\nyou already know word embeddings for each word in the sentence v(\"john\"),\nv(\"loves\"), and so on. The average can be obtained with the following code and\nillustrated in figure 4.1:\nresult = (v(\"john\") + v(\"loves\") + v(\"mary\") + v(\".\")) / 4\n",
      "content_length": 2712,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "82\nCHAPTER 4\nSentence classification\nThis method is quite simple and is actually used in many NLP applications. However,\nit has one critical issue, which is that it cannot take word order into account. Because\nthe order of input elements doesn’t affect the result of averaging, you’d get the same\nvector for both “Mary loves John” and “John loves Mary.” Although it’s up to the task\nin hand, it’s hard to imagine many NLP applications would want this kind of behavior.\n If we step back and reflect how we humans read language, this “averaging” is far\nfrom actuality. When we read a sentence, we don’t usually read individual words in iso-\nlation and remember them first, then move on to figuring out what the sentence\nmeans. We usually scan the sentence from the beginning, one word at a time, while\nholding what the “partial” sentence means up until the part you are reading in our\nshort-term memory. In other words, you maintain some sort of mental representation\nof the sentence while you read it. When you reach the end of the sentence, the mental\nrepresentation is its meaning. \n Can we design a neural network structure that simulates this incremental reading\nof the input? The answer is a resounding yes. That structure is called recurrent neural\nnetworks (RNNs), which I’ll explain in detail next.\n4.1.2\nRNN abstraction\nIf you break down the reading process mentioned earlier, its core is the repetition of\nthe following series of operations:\n1\nRead a word.\n2\nBased on what has been read so far (your “mental state”), figure out what the\nword means.\n3\nUpdate the mental state.\n4\nMove on to the next word.\nLet’s see how this works using a concrete example. If the input sentence is sentence\n= [\"john\", \"loves\", \"mary\", \".\"], and each word is already represented as a\nword-embedding vector. Also, let’s denote your “mental state” as state, which is ini-\ntialized by init_state(). Then, the reading process is represented by the following\nincremental operations:\nstate = init_state()\nstate = update(state, v(\"john\"))\nAverage\nresult\nsentence\nv(\"john\") v(\"loves\") v(\"mary\")\nv(\".\")\nFigure 4.1\nAveraging \nembedding vectors\n",
      "content_length": 2124,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "83\nRecurrent neural networks (RNNs)\nstate = update(state, v(\"loves\"))\nstate = update(state, v(\"mary\"))\nstate = update(state, v(\".\"))\nThe final value of state becomes the representation of the entire sentence from this\nprocess. Notice that if you change the order in which these words are processed (e.g.,\nby flipping “John” and “Mary”), the final value of state also changes, meaning that the\nstate also encodes some information about the word order.\n You can achieve something similar if you can design a network substructure that is\napplied to each element of the input while updating some internal states. RNNs are neu-\nral network structures that do exactly this. In a nutshell, an RNN is a neural network with\na loop. At its core is an operation that is applied to every element in the input as they\ncome in. If you wrote what RNNs do in Python pseudocode, it’d be like the following:\ndef rnn(words):\n    state = init_state()\n    for word in words:\n        state = update(state, word)\n    return state\nNotice that there is state that gets initialized first and passed around during the iter-\nation. For every input word, state is updated based on the previous state and the\ninput using the function update. The network substructure corresponding to this\nstep (the code block inside the loop) is called a cell. This stops when the input is\nexhausted, and the final value of state becomes the result of this RNN. See figure 4.2\nfor the illustration. \n You can see the parallelism here. When you are reading a sentence (sequence of\nwords), your internal mental representation of the sentence, state, is updated after\nreading each word. You can assume that the final state encodes the representation of\nthe entire sentence. \n The only remaining work is to design two functions—init_state() and\nupdate(). The state is usually initialized with zero (i.e., a vector filled with zeros), so\ninit_state()\nstate\nupdate\nupdate\nupdate\nupdate\nv(\"john\")\nv(\"loves\")\nv(\"mary\")\nv(\".\")\nRecurrent neural network (RNN)\nFigure 4.2\nRNN abstraction\n",
      "content_length": 2030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "84\nCHAPTER 4\nSentence classification\nyou usually don’t have to worry about how to go about defining the former. It’s more\nimportant how you design update(), which determines the characteristics of the RNN.\n4.1.3\nSimple RNNs and nonlinearity\nIn section 3.4.3, I talked about how to implement a linear layer with an arbitrary num-\nber of inputs and outputs. Can we do something similar and implement update(),\nwhich is basically a function that takes two input variables and produces one output\nvariable? After all, a cell is a neural network with its own input and output, right? The\nanswer is yes, and it looks like this:\ndef update_simple(state, word):\n    return f(w1 * state + w2 * word + b)\nNotice that this is strikingly similar to the linear2() function in section 3.4.3. In\nfact, if you ignore the difference in variable names, it’s exactly the same except for the\nf() function. An RNN defined by this type of the update function is called a simple\nRNN or Elman RNN, which is, as its name suggests, one of the simplest RNN structures.\n You may be wondering, then, what is this function f() doing here? What does it\nlook like? Do we need it here at all? The function, called an activation function or non-\nlinearity, takes a single input (or a vector) and transforms it (or every element of a vec-\ntor) in a nonlinear fashion. Many kinds of nonlinearities exist, and they play an\nindispensable role in making neural networks truly powerful. What they exactly do and\nwhy they are important requires some math to understand, which is outside the scope\nof this book, but I’ll attempt an intuitive explanation with a simple example next.\n Imagine you are building an RNN that recognizes “grammatical” English sentences.\nDifferentiating grammatical sentences from ungrammatical ones is itself a difficult NLP\nproblem, which is, in fact, a well-established research area (see section 1.2.1), but here,\nlet’s simplify it and consider agreement only between the subject and the verb. Let’s fur-\nther simplify it and assume that there are only four words in this “language”—“I,” “you,”\n“am,” and “are.” If the sentence is either “I am” or “you are,” it’s grammatically correct.\nThe other two combinations, “I are” and “you am,” are incorrect. What you want to\nbuild is an RNN that outputs 1 for the correct sentences while producing 0 for the\nincorrect ones. How would you go about building such a neural network?\n The first step in almost every modern NLP model is to represent words with\nembeddings. As mentioned in the previous chapter, they are usually learned from a\nlarge dataset of natural language text, but here, we are simply going to give them\nsome predefined values, as shown in figure 4.3.\n Now, let’s imagine there was no activation function. The previous update_\nsimple() function simplifies to the following:\ndef update_simple_linear(state, word):\n    return w1 * state + w2 * word + b\nWe will assume the initial value of state is simply [0, 0], because the specific initial val-\nues are not relevant to the discussion here. The RNN takes the first word embedding,\n",
      "content_length": 3078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "85\nRecurrent neural networks (RNNs)\nx1, updates state, takes the second word embedding, x2, then produces the final\nstate, which is a two-dimensional vector. Finally, the two elements in this vector are\nsummed and converted to result. If result is close to 1, the sentence is grammati-\ncal. Otherwise, it is not. If you apply the update_simple_linear() function twice\nand simplify it a little bit, you get the following function, which is all this RNN does,\nafter all:\nw1 * w2 * x1 + w2 * x2 + w1 * b + b\nRemember, w1, w2, and b are parameters of the model (aka “magic constants”) that\nneed to be trained (adjusted). Here, instead of adjusting these parameters using a\ntraining dataset, let’s assign some arbitrary values and see what happens. For example,\nwhen w1 = [1, 0], w2 = [0, 1], and b = [0, 0], the input and the output of this RNN\nwill be as shown in figure 4.4.\nFigure 4.4\nInput and output when w1 = [1, 0], w2 = [0, 1], and b = [0, 0] without \nan activation function\nWord\nI\nyou\nam\nEmbeddings\n[-1, 1]\n[1, -1]\n[-1, -1]\n[1, 1]\nare\nWord embeddings\ninit_state()\nstate\nresult\n+\nupdate\nupdate\nv(\"i\")\nv(\"you\")\nv(\"am\")\nv(\"are\")\nRecurrent neural network (RNN)\nFigure 4.3\nRecognizing grammatical English sentences using an RNN\nWord 1\nI\nI\nyou\nx1\nw1 = [1, 0], w2 = [0, 1], b = [0, 0]\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\nstate\nresult\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n[0, -1]\n[0, 1]\n[0, -1]\n[0, 1]\n-1\n1\n-1\n1\n1\n0\n0\n1\nyou\nWord 2\nDesired\nam\nare\nam\nare\n",
      "content_length": 1444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "86\nCHAPTER 4\nSentence classification\nIf you look at the values of result, this RNN groups ungrammatical sentences (e.g.,\n“I are”) with grammatical ones (e.g., “you are”), which is not the desired behavior.\nHow about we try another set of values for the parameters? Let’s use w1 = [1, 0], w2\n= [-1, 0], and b = [0, 0] and see what happens (see figure 4.5 for the result).\nFigure 4.5\nInput and output when w1 = [1, 0], w2 = [-1, 0], and b = [0, 0] \nwithout an activation function\nThis is much better, because the RNN is successful in grouping ungrammatical sen-\ntences by assigning 0 to both “I are” and “you am.” However, it also assigns completely\nopposite values (2 and –2) to grammatical sentences (“I am” and “you are”). \n I’m going to stop here, but as it turns out, you cannot use this neural network to\ndifferentiate grammatical sentences from ungrammatical ones, no matter how hard\nyou try. Despite the values you assign to the parameters, this RNN cannot produce\nresults that are close enough to the desired values and, thus, are able to group sen-\ntences by their grammaticality.\n Let’s step back and think why this is the case. If you look at the previous update\nfunction, all it does is multiply the input by some value and add them up. In more spe-\ncific terms, it only transforms the input in a linear fashion. The result of this neural net-\nwork always changes by some constant amount when you change the value of the\ninput by some amount. But this is obviously not desirable—you want the result to be 1\nonly when the input variables are some specific values. In other words, you don’t want\nthis RNN to be linear; you want it to be nonlinear.\n To use an analogy, imagine you can use only assignment (“=”), addition (“+”), and\nmultiplication (“*”) in your programming language. You can tweak the input values to\nsome degree to come up with the result, but you can’t write more complex logic in\nsuch a restricted setting. \n Now let’s put the activation function f() back and see what happens. The specific\nactivation function we are going to use is called the hyperbolic tangent function, or more\ncommonly, tanh, which is one of the most commonly used activation functions in neu-\nral networks. The details of this function are not important in this discussion, but in a\nWord 1\nI\nI\nyou\nx1\nw1 = [1, 0], w2 = [-1, 0], b = [0, 0]\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\nstate\nresult\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n[2, 0]\n[0, 0]\n[0, 0]\n[-2, 0]\n2\n0\n0\n-2\n1\n0\n0\n1\nyou\nWord 2\nDesired\nam\nare\nam\nare\n",
      "content_length": 2498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "87\nRecurrent neural networks (RNNs)\nnutshell, it behaves as follows: tanh doesn’t do much to the input when it is close to\nzero, for example, 0.3 or –0.2. In other words, the input passes through the function\nalmost unchanged. When the input is far from zero, tanh tries to squeeze it between\n–1 and 1. For example, when the input is large (say, 10.0), the output becomes very\nclose to 1.0, whereas when it is small (say, –10.0), the output becomes almost –1.0.\nThis creates an effect similar to the OR logical gate (or an AND gate, depending on\nthe weights) if two or more variables are fed into the activation function. The output\nof the gate becomes ON (~1) and OFF (~–1) depending on the input.\n When w1 = [-1, 2], w2 = [-1, 2], b = [0, 1], and the tanh activation func-\ntion is used, the result of the RNN becomes a lot closer to what we desire (see figure\n4.6). If you round them to the closest integers, the RNN successfully groups sentences\nby their grammaticality.\nFigure 4.6\nInput and output when w1 = [-1, 2], w2 = [-1, 2], and b = [0, 1] \nwith an activation function\nTo use the same analogy, using activation functions in your neural networks is like\nusing ANDs and ORs and IFs in your programming language, in addition to basic\nmath operations like addition and multiplication. In this way, you can write complex\nlogic and model complex interactions between input variables, like the example in\nthis section. \nNOTE\nThis example I use in this section is a slightly modified version of the\npopular XOR (or exclusive-or) example commonly seen in deep learning text-\nbooks. This is the most basic and simplest example that can be solved by neu-\nral networks but not by other linear models.\nSome final notes on RNNs—they are trained just like any other neural networks. The\nfinal outcome is compared with the desired outcome using the loss function, then the\ndifference between the two—the loss—is used for updating the “magic constants.”\nThe magic constants are, in this case, w1, w2, and b in the update_simple() func-\ntion. Note that the update function and its magic constants are identical across all the\nWord 1\nI\nI\nyou\nx1\nw1 = [-1, 2], w2 = [-1, 2], b = [0, 1]\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\nstate\nresult\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n[0.23, 0.75]\n[-0.94, 0.99]\n[0.94, -0.98]\n[-0.23, 0.90]\n0.99\n0.06\n-0.04\n0.67\n1\n0\n0\n1\nyou\nWord 2\nDesired\nam\nare\nam\nare\n",
      "content_length": 2372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "88\nCHAPTER 4\nSentence classification\ntimesteps in the loop. This means that what RNNs are learning is a general form of\nupdates that can be applied to any situation. \n4.2\nLong short-term memory units (LSTMs) \nand gated recurrent units (GRUs)\nIn fact, the simple RNNs that we discussed earlier are rarely used in real-world NLP\napplications due to one problem called the vanishing gradients problem. In this section,\nI’ll show the issue associated with simple RNNs and how more popular RNN architec-\ntures, namely LSTMs and GRUs, solve this particular problem.\n4.2.1\nVanishing gradients problem\nJust like any programming language, if you know the length of the input, you can\nrewrite a loop without using one. An RNN can also be rewritten without using a loop,\nwhich makes it look just like a regular neural network with many layers. For example,\nif you know that there are only six words in the input, the rnn() from earlier can be\nrewritten as follows:\ndef rnn(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n    \n    state = update(state, word1)\n    state = update(state, word2)\n    state = update(state, word3)\n    state = update(state, word4)\n    state = update(state, word5)\n    state = update(state, word6)\n    return state\nRepresenting RNNs without loops is called unrolling. Now we know what update()\nlooks like for a simple RNN (update_simple), so we can replace the function calls\nwith their bodies, as shown here:\ndef rnn_simple(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n    state = f(w1 * f(w1 * f(w1 * f(w1 * f(w1 * f(w1 * state + w2 * word1 + b) \n+ w2 * word2 + b) + w2 * word3 + b) + w2 * word4 + b) + w2 * word5 + b) \n+ w2 * word6 + b)\n    return state\nThis is getting a bit ugly, but I just want you to notice the very deeply nested function\ncalls and multiplications. Now, recall the task we wanted to accomplish in the previous\nsection—classifying grammatical English sentences by recognizing subject-verb agree-\nment. Let’s say the input is sentence = [\"The\", \"books\", \"I\", \"read\", \"yes-\nterday\", \"were\"]. In this case, the innermost function call processes the first word\n",
      "content_length": 2187,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "89\nLong short-term memory units (LSTMs) and gated recurrent units (GRUs)\n“The,” the next one processes the second word “books,” and so on, all the way to the\noutermost function call, which processes “were.” If we rewrite the previous pseudocode\nslightly, as shown in the next code snippet, you can understand it more intuitively:\ndef is_grammatical(sentence):\n    word1, word2, word3, word4, word5, word6 = sentence\n    state = init_state()\n    state = process_main_verb(w1 *\n        process_adverb(w1 *\n            process_relative_clause_verb(w1 *\n                process_relative_clause_subject(w1 *\n                    process_main_subject(w1 *\n                        process_article(w1 * state + w2 * word1 + b) +\n                    w2 * word2 + b) +\n                w2 * word3 + b) +\n            w2 * word4 + b) + \n        w2 * word5 + b) + \n    w2 * word6 + b)\n    return state\nTo recognize that the input is indeed a grammatical English sentence (or a prefix of a\nsentence), the RNN needs to retain the information about the subject (“the books”)\nin state until it sees the verb (“were”) without being distracted by anything in\nbetween (“I read yesterday”). In the previous pseudocode, the states are represented\nby the return values of function calls, so the information about the subject (return\nvalue of process_main_subject) needs to propagate all the way up in this chain\nuntil it reaches the outermost function (process_main_verb). This is starting to\nsound like a difficult task.\n Things don’t look any better when it comes to training this RNN. RNNs, as well as\nany other neural networks, are trained using an algorithm called backpropagation. Back-\npropagation is a process where the components of a neural network communicate with\nprevious components on how to adjust the parameters to minimize the loss. This is how\nit works for this particular case. First, you look at the outcome, that is, the return value\nof is_grammatical()and compare it with what you desire. The difference between\nthese two is called the loss. The outermost function, is_grammatical(), basically has\nfour ways to decrease the loss to make its output closer to what is desired—1) adjust w1\nwhile fixing the return value of the nested function process_adverb(), 2) adjust w2,\n3) adjust b, or 4) adjust the return value of process_adverb() while fixing the param-\neters. Adjusting the parameters (w1, w2, and b) is the easy part because the function\nknows the exact effect of adjusting each parameter to its return value. Adjusting the\nreturn value of the previous function, however, is not easy, because the caller has no\nidea about the inner workings of the function. Because of this, the caller tells the pre-\nvious function (callee) to adjust its return values to minimize the loss. See figure 4.7 for\nhow the loss is propagated back to the parameters and previous functions.\n",
      "content_length": 2872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "90\nCHAPTER 4\nSentence classification\n \nFigure 4.7\nBackpropagation of loss\nThe nested function calls repeat this process and plays the Telephone game until the\nmessage reaches the innermost function. By that time, because the message needs to\npass through many layers, it becomes so weak and obscure (or so strong and skewed\nbecause of some misunderstanding) that the inner functions have a difficult time fig-\nuring out what they did wrong.\n Technically, the deep learning literature calls this the vanishing gradients problem. A\ngradient is a mathematical term that corresponds to the message signal that each func-\ntion receives from the next one that states how exactly they should improve their pro-\ncess (how to change their magic constants). The reverse Telephone game, where\nmessages are passed backward from the final function (= loss function), is called back-\npropagation. I’m not going into the mathematical details of these terms, but it is use-\nful to understand them at least conceptually.\n Because of the vanishing gradients problem, simple RNNs are difficult to train and\nrarely used in practice nowadays. \n4.2.2\nLong short-term memory (LSTM)\nThe way the nested functions mentioned earlier process information about grammar\nseems too inefficient. After all, why doesn’t the outermost function (is_grammati-\ncal) tell the particular function in charge (e.g., process_main_subject) what\nwent wrong directly, instead of playing the Telephone game? It can’t, because the mes-\nsage can change its shape entirely after each function call because of w2 and f(). The\noutermost function cannot tell which function was responsible for which part of the\nmessage from only the final output. \n How could we address this inefficiency? Instead of passing the information\nthrough an activation function every time and changing its shape completely, how\nabout adding and subtracting information relevant to the part of the sentence being\nprocessed at each step? For example, if process_main_subject() can directly add\nv(\"were\")\nw1\nw2\nb\n*\n...\n+\n*\nstate\nstate\nis_grammatical()\nprocess_adverb()\nf()\nloss\n1)\n2)\n3)\n4)\n",
      "content_length": 2112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "91\nLong short-term memory units (LSTMs) and gated recurrent units (GRUs)\ninformation about the subject to some type of “memory,” and the network can make\nsure the memory passes through the intermediate functions intact, is_grammati-\ncal()will have a much easier time telling the previous functions what to do to adjust\nits output.\n Long short-term memory units (LSTMs) are a type of RNN cell that is proposed based\non this insight. Instead of passing around states, LSTM cells share a “memory” that\neach cell can remove old information from and/or add new information to, some-\nthing like an assembly line in a manufacturing factory. Specifically, LSTM RNNs use\nthe following function for updating states:\ndef update_lstm(state, word):\n    cell_state, hidden_state = state\n    cell_state *= forget(hidden_state, word)\n    cell_state += add(hidden_state, word)\n    hidden_state = update_hidden(hidden_state, cell_state, word)\n    return (cell_state, hidden_state)\nAlthough this looks relatively complicated compared to its simple version, if you break\nit down to subcomponents, it’s not that difficult to understand what is going on here,\nas described next and shown in figure 4.8:\nThe LSTM states comprise two halves—the cell state (the “memory” part) and\nthe hidden state (the “mental representation” part).\nThe function forget() returns a value between 0 and 1, so multiplying by this\nnumber means erasing old memory from cell_state. How much to erase is\nforget\n*\n+\ncell\nstate\nLSTM cell\nhidden\nstate\ncell\nstate\nhidden\nstate\nword\nadd\nupdate\nhidden\nFigure 4.8\nLSTM update function\n",
      "content_length": 1583,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "92\nCHAPTER 4\nSentence classification\ndetermined from hidden_state and word (input). Controlling the flow of\ninformation by multiplying by a value between 0 and 1 is called gating. LSTMs\nare the first RNN architecture that uses this gating mechanism.\nThe function add()returns a new value added to the memory. The value again\nis determined from hidden_state and word.\nFinally, hidden_state is updated using a function, whose value is computed\nfrom the previous hidden state, the updated memory, and the input word.\nI abstracted the update function by hiding some mathematical details in the functions\nforget(), add(), and update_hidden(), which are not important for the discus-\nsion here. If you are interested in understanding LSTMs more deeply, I refer you to a\nwonderful blog post Chris Olah wrote on this topic (http://colah.github.io/posts/\n2015-08-Understanding-LSTMs/). \n Because LSTMs have this cell state that stays constant across different timesteps\nunless explicitly modified, they are easier to train and relatively well behaved. Because\nyou have a shared “memory” and functions are adding and removing information\nrelated to different parts of the input sentence, it is easier to pinpoint which function\ndid what and what went wrong. The error signal from the outermost function can\nreach responsible functions more directly.\n A word on the terminology—LSTM refers to one particular type of architecture\nmentioned here, but people use “LSTMs” to mean RNNs with LSTM cells. Also, “RNN”\nis often used to mean “simple RNN,” introduced in section 4.1.3. When you see “RNNs”\nin the literature, you need to be aware of which exact architectures they are using.\n4.2.3\nGated recurrent units (GRUs)\nAnother RNN architecture, called Gated Recurrent Units (GRUs), uses the gating\nmechanism. The philosophy behind GRUs is similar to that of LSTMs, but GRUs\nuse only one set of states instead of two halves. The update function for GRUs is\nshown next:\ndef update_gru(state, word): \n    new_state = update_hidden(state, word)\n    switch = get_switch(state, word)\n    state = swtich * new_state + (1 – switch) * state\n    return state\nInstead of erasing or updating the memory, GRUs use a switching mechanism. The\ncell first computes the new state from the old state and the input. It then computes\nswitch, a value between 0 and 1. The state is chosen between the new state and the\nold one based on the value of switch. If it’s 0, the old state passes through intact. If\nit’s 1, it’s overwritten by the new state. If it’s somewhere in between, the state will be a\nmix of two. See figure 4.9 for an illustration of the GRU update function.\n",
      "content_length": 2640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "93\nAccuracy, precision, recall, and F-measure\nFigure 4.9\nGRU update function\nNotice that the update function for GRUs is much simpler than that for the LSTMs.\nIndeed, it has fewer parameters (magic constants) that need to be trained compared\nto LSTMs. Because of this, GRUs are faster to train than LSTMs.\n Finally, although we introduced two different types of RNN architecture, LSTM\nand GRU, there’s no consensus in the community on which type of architecture is the\nbest for all applications. You often need to treat them as a hyperparameter and exper-\niment with different configurations. Fortunately, it is easy to experiment with different\ntypes of RNN cells as long as you are using modern deep learning frameworks such as\nPyTorch and TensorFlow. \n4.3\nAccuracy, precision, recall, and F-measure\nIn section 2.7, I briefly talked about some metrics that we use for evaluating the per-\nformance of a classification task. Before we move on to actually building a sentence\nclassifier, I’d like to further discuss the evaluation metrics we are going to use—what\nthey mean and what they actually measure.\n4.3.1\nAccuracy\nAccuracy is probably the simplest of all the evaluation metrics that we talk about here.\nIn a classification setting, accuracy is the fraction of instances that your model got\nright. For example, if there are 10 emails and your spam-filtering model got 8 of them\ncorrect, the accuracy of your prediction is 0.8, or 80% (see figure 4.10).\n Though simple, accuracy is not without its limitations. Specifically, accuracy can be\nmisleading when the test set is imbalanced. An imbalanced dataset contains multiple\nclass labels that greatly differ in their numbers. For example, if a spam-filtering dataset\nis imbalanced, it may contain 90% nonspam emails and 10% spams. In such a case,\nupdate\nhidden\n \nstate\nnew_state\nGRU cell\nstate\nx\nswitch\n",
      "content_length": 1857,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "94\nCHAPTER 4\nSentence classification\neven a stupid classifier that labels everything as nonspam would be able to achieve an\naccuracy of 90%. As an example, if a “stupid” classifier classifies everything as “non-\nspam” in figure 4.10, it would still achieve an accuracy of 70% (7 out of 10 instances).\nIf you look at this number in isolation, you might be fooled into thinking the perfor-\nmance of the classifier is actually great. When you are using accuracy as a metric, it is\nalways a good idea to compare with the hypothetical, stupid classifier (majority vote) as\na baseline.\n4.3.2\nPrecision and recall\nThe rest of the metrics—precision, recall, and F-measure—are used in a binary classi-\nfication setting. The goal of a binary classification task is to identify one class (called a\npositive class) from the other (called a negative class). In the spam-filtering setting, the\npositive class is spam, whereas the negative class is nonspam.\n The Venn diagram in figure 4.11 contains four subregions: true positives, false pos-\nitives, false negatives, and true negatives. True positives (TP) are instances that are\nEmail 1\nEmail 2\nEmail 3\nEmail 4\nEmail 5\nEmail 6\nEmail 7\nEmail 8\nEmail 9\nEmail 10\nInstances\nLabels\nPredictions\nCorrect?\nSpam\nNonspam\nNonspam\nSpam\nNonspam\nNonspam\nNonspam\nNonspam\nNonspam\nSpam\nSpam\nNonspam\nNonspam\nNonspam\nNonspam\nNonspam\nNonspam\nAccuracy = 8/10 = 80%\nSpam\nNonspam\nSpam\nFigure 4.10\nCalculating accuracy\n",
      "content_length": 1433,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "95\nAccuracy, precision, recall, and F-measure\npredicted as positive (= spam) and are indeed in the positive class. False positives (FP)\nare instances that are predicted as positive (= spam) but are actually not in the positive\nclass. These are noises in the prediction, that is, innocent nonspam emails that are mis-\ntakenly caught by the spam filter and end up in the spam folder of your email client.\n On the other hand, false negatives (FN) are instances that are predicted as nega-\ntive but are actually in the positive class. These are spam emails that slip through the\nspam filter and end up in your inbox. Finally, true negatives (TN) are instances that\nare predicted as negative and are indeed in the negative class (nonspam emails in\nyour inbox).\n Precision is the fraction of instances that the model classifies as positive that are\nindeed correct. For example, if your spam filter identifies three emails as spam, and\ntwo of them are indeed spam, the precision will be 2/3, or about 66%. \n Recall is somewhat opposite of precision. It is the fraction of positive instances in\nyour dataset that are identified as positive by your model. Again, using spam filtering\nas an example, if your dataset contains three spam emails and your model identifies\ntwo of them as spam successfully, the recall will be 2/3, or about 66%.\n Figure 4.11 shows the relationship between predicted and true labels as well as\nrecall and precision. \nFigure 4.11\nPrecision and recall\nTrue\npositives\n(TP)\nFalse\npositives\n(FP)\nPredicted\npositives\nTrue\nlabels\nFalse\nnegatives\n(FN)\nTrue\nnegatives\n(TN)\nPrecision =\nTP\nTP + FP\nRecall =\nTP\nTP + FN\nAccuracy =\nTP + TN\nTP + FP + FN + TN\n",
      "content_length": 1662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "96\nCHAPTER 4\nSentence classification\n4.3.3\nF-measure\nYou may have noticed a tradeoff between precision and recall. Imagine there’s a spam\nfilter that is very, very careful in classifying emails. It outputs only one out of several\nthousand emails as spam, but when it does, it is always correct. This is not a difficult\ntask, because some spam emails are pretty obvious—if they contain a word “v1@gra”\nin the text and it’s sent from someone in the spam blacklist, it should be pretty safe to\nmark it as a spam. What would the precision of this spam filter be? 100%. Similarly,\nthere’s another spam filter that is very, very careless in classifying emails. It classifies\nevery single email as spam, including the ones from your colleagues and friends. Its\nrecall? 100%. Would any of these two spam filters be useful? Hardly!\n As you’ve seen, improving precision or recall alone while ignoring the other is not\na good practice, because of the tradeoff between them. It’s like you were looking only\nat your body weight when you are on a diet. You lost 10 pounds? Great! But what if you\nare seven feet tall? Not so much. You need to take into account both your height and\nweight—how much is too much depends on the other variable. That’s why there are\nmeasures like BMI (body mass index) that take both measures into account. Similarly,\nresearchers came up with this metric called F-measure, which is an average (or, more pre-\ncisely speaking, a harmonic mean) of precision and recall. Most often, a special case\ncalled F1-measure is used, which is the equally weighted version of F-measure. In a clas-\nsification setting, it is a good practice to measure and try to maximize the F-measure.\n4.4\nBuilding AllenNLP training pipelines\nIn this section, we are going to revisit the sentiment analyzer we built in chapter 2 and\ndiscuss how to build its training pipeline in more detail. Although I already showed\nthe important steps for building an NLP application using AllenNLP, in this section\nwe will dive deep into some important concepts and abstractions. Understanding\nthese concepts is important not just in using AllenNLP but also in designing NLP\napplications in general, because NLP applications are usually built using these abstrac-\ntions in some way or the other. \n To run the code in this section, you need to import the necessary classes and mod-\nules, as shown in the following code snippet (the code examples in this section can also\nbe accessed via Google Colab, http://www.realworldnlpbook.com/ch2.html#sst-nb):\nfrom itertools import chain\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2vec_encoders import Seq2VecEncoder, \nPytorchSeq2VecWrapper\n",
      "content_length": 2916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "97\nBuilding AllenNLP training pipelines\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, \nBasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training.metrics import CategoricalAccuracy, F1Measure\nfrom allennlp.training.trainer import GradientDescentTrainer\nfrom allennlp_models.classification.dataset_readers.stanford_sentiment_tree_\nbank import \\\n    StanfordSentimentTreeBankDatasetReader\n4.4.1\nInstances and fields\nAs mentioned in section 2.2.1, an instance is the atomic unit for which a prediction is\nmade by a machine learning algorithm. A dataset is a collection of instances of the\nsame form. The first step in most NLP applications is to read in or receive some data\n(e.g., from a file or via network requests) and convert them to instances so that the\nNLP/ML algorithm can consume them.\n AllenNLP supports an abstraction called DatasetReader whose job is to read in\nsome input (raw strings, CSV files, JSON data structures from network requests, and so\non) and convert it to instances. AllenNLP already provides a wide range of dataset read-\ners for major formats used in NLP, such as the CoNLL format (used in popular shared\ntasks for language analysis) and the Penn Treebank (a popular dataset for syntactic\nparsing). To read the Standard Sentiment Treebank, you can use the built-in\nStanfordSentimentTreeBankDatasetReader, which we used earlier in chapter 2.\nYou can also write your own dataset reader just by overriding some core methods from\nDatasetReader.\n The AllenNLP class Instance represents a single instance. An instance can have\none or more fields, which hold some type of data. For example, an instance for the\nsentiment analysis task has two fields—the text body and the label—which can be cre-\nated by passing a dictionary of fields to its constructor as follows:\nInstance({'tokens': TextField(tokens),\n          'label': LabelField(sentiment)})\nHere we assumed that you already created tokens, which is a list of tokens, and sen-\ntiment, a string label corresponding to the sentiment class, from reading the input\nfile. AllenNLP supports other types of fields, depending on the task. \n The read()method of DatasetReader returns an iterator over instances, which\nenables you to enumerate the generated instances and visually check them, as shown\nin the following snippet:\nreader = StanfordSentimentTreeBankDatasetReader()\ntrain_dataset = reader.read('path/to/sst/dataset/train.txt')\ndev_dataset = reader.read('path/to/sst/dataset/dev.txt')\nfor inst in train_dataset + dev_dataset:\n    print(inst)\n",
      "content_length": 2633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "98\nCHAPTER 4\nSentence classification\nIn many cases, you access your dataset readers through data loaders. A data loader is\nan AllenNLP abstraction (which is really a thin wrapper around PyTorch’s data load-\ners) that handles the data and iterates over batched instances. You can specify how\ninstances are sorted, grouped into batches, and fed to the training algorithm by sup-\nplying a batch sampler. Here, we are using a BucketBatchSampler, which does this\nby sorting instances by their length and grouping instances with similar lengths into a\nsingle batch, as shown next: \nreader = StanfordSentimentTreeBankDatasetReader()\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\n4.4.2\nVocabulary and token indexers\nThe second step in many NLP applications is to build the vocabulary. In computer sci-\nence, vocabulary is a theoretical concept that represents the set of all possible words in\na language. In NLP, though, it often means just the set of all unique tokens that\nappeared in a dataset. It is simply impossible to know all the possible words in a lan-\nguage, nor is it necessary for an NLP application. What is stored in a vocabulary is\ncalled a vocabulary item (or just an item). A vocabulary item is usually a word, although\ndepending on the task at hand, it can be any form of linguistic units, including charac-\nters, character n-grams, and labels for linguistic annotation.\n AllenNLP provides a class called Vocabulary. It not only takes care of storing\nvocabulary items that appeared in a dataset, but it also holds mappings between\nvocabulary items and their IDs. As mentioned earlier, neural networks and machine\nlearning models in general can deal only with numbers, and there needs to be a way\nto map discrete items such as words to some numerical representations such as word\nIDs. The vocabulary is also used to map the results of an NLP model back to the origi-\nnal words and labels so that humans can actually read them.\n You can create a Vocabulary object from instances as follows:\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), \n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\nA couple of things to note here: first, because we are dealing with iterators (returned\nby the data loaders’ iter_instances()method), we need to use the chain method\nfrom itertools to enumerate all the instances in both datasets. \n Second, AllenNLP’s Vocabulary class supports namespaces, which are a system to\nseparate different sets of items so that they don’t get mixed up. Here’s why they are\nuseful—say you are building a machine translation system, and you just read a dataset\nthat contains English and French translations. Without namespaces, you’d have just\none set that contains all words in English and French. This is usually not a big issue\n",
      "content_length": 3065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "99\nBuilding AllenNLP training pipelines\nbecause English words (“hi,” “thank you,” “language”) and French words (“bonjour,”\n“merci, “langue”) look quite different in most cases. However, a number of words look\nexactly the same in both languages. For example, “chat” means “talk” in English and\n“cat” in French, but it’s hard to imagine anybody wanting to mix those two words and\nassign the same ID (and embeddings). To avoid this conflict, Vocabulary imple-\nments namespaces and assigns separate sets of items of different types.\n You may have noticed the form_instances() function call has a min_count\nargument. For each namespace, this specifies the minimum number of occurrences in\nthe dataset that is necessary for an item to be included in the vocabulary. All the items\nthat appear less frequently than this threshold are treated as “unknown” items. Here’s\nwhy this is a good idea: in a typical language, a very small number of words appear a\nlot (in English: “the,” “a,” “of”) and a very large number of words appear very infre-\nquently. This usually exhibits a long tail distribution of word frequencies. But it is not\nlikely that these super infrequent words add anything useful to the model, and pre-\ncisely because they appear infrequently, it is difficult to learn any useful patterns from\nthem anyway. Also, because there are so many of them, they inflate the size of the\nvocabulary and the number of model parameters. In such a case, a common practice\nin NLP is to cut this long tail and collapse all the infrequent words to a single entity\n<UNK> (for “unknown” words).\n Finally, a token indexer is an AllenNLP abstraction that takes in a token and returns\nits index, or a list of indices that represent the token. In most cases, there’s a one-to-\none mapping between unique tokens and their indices, but depending on your\nmodel, you may need more advanced ways to index the tokens (such as using charac-\nter n-grams).\n After you create a vocabulary, you can tell the data loaders to index the tokens with\nthe specified vocabulary, as shown in the next code snippet. This means that the\ntokens that the data loaders read from the datasets are converted to integer IDs\naccording to the vocabulary’s mappings:\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n4.4.3\nToken embedders and RNNs\nAfter you index words using a vocabulary and token indexers, you need to convert them\nto embeddings. An AllenNLP abstraction called TokenEmbedder takes word indices as\nan input and produces word embedding vectors as an output. You can embed words\nusing continuous vectors in many ways, but if all you want is to map unique tokens to\nembedding vectors one-to-one, you can use the Embedding class as follows:\ntoken_embedding = Embedding(\n    num_embeddings=vocab.get_vocab_size('tokens'),\n    embedding_dim=EMBEDDING_DIM)\nThis will create an Embedding instance that takes word IDs and converts them to\nfixed-length vectors in a one-to-one fashion. The number of unique words this\n",
      "content_length": 2999,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "100\nCHAPTER 4\nSentence classification\ninstance can support is given by num_embeddings, which is equal to the size of the\ntokens vocabulary namespace. The dimensionality of embeddings (i.e., the length of\nembedded vectors) is given by embedding_dim.\n Next, let’s define our RNN and convert a variable-length input (a list of embedded\nwords) to a fixed-length vector representation of the input. As we discussed in section\n4.1, you can think of an RNN as a neural network structure that consumes a sequence\nof things (words) and returns a fixed-length vector. AllenNLP abstracts such models\ninto the Seq2VecEncoder class, and you can create an LSTM RNN by using\nPytorchSeq2VecWrapper as follows:\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nA lot is happening here, but essentially this wraps PyTorch’s LSTM implementation\n(torch.nn.LSTM) and makes it pluggable to the rest of the AllenNLP pipeline. The\nfirst argument to torch.nn.LSTM() is the dimensionality of the input vector, and the\nsecond one is that of LSTM’s internal state. The last one, batch_first, specifies the\nstructure of the input/output tensors for batching, but you usually don’t have to\nworry about its details as long as you are using AllenNLP.\nNOTE\nIn AllenNLP, everything is batch first, meaning that the first dimen-\nsion of any tensor is always equal to the number of instances in a batch.\n4.4.4\nBuilding your own model\nNow that we defined all the subcomponents, we are ready to build the model that exe-\ncutes the prediction. Thanks to AllenNLP’s well-designed abstractions, you can easily\nbuild your model by inheriting AllenNLP’s Model class and overriding the\nforward() method. You don’t usually need to be aware of details such as the shapes\nand dimensions of tensors. The following listing defines the LSTM RNN used for clas-\nsifying sentences.\n@Model.register(\"lstm_classifier\")\nclass LstmClassifier(Model):   \n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n        super().__init__(vocab)\n        self.embedder = embedder\n        self.encoder = encoder\n        self.linear = torch.nn.Linear(   \n            in_features=encoder.get_output_dim(),\n            out_features=vocab.get_vocab_size('labels'))\nListing 4.1\nLSTM sentence classifier\nAllenNLP models \ninherit Model.\nCreates a linear layer to convert the RNN \noutput to a vector of another length\n",
      "content_length": 2537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "101\nBuilding AllenNLP training pipelines\n        positive_index = vocab.get_token_index(\n            positive_label, namespace='labels')\n        self.accuracy = CategoricalAccuracy()\n        self.f1_measure = F1Measure(positive_index)    \n        self.loss_function = torch.nn.CrossEntropyLoss()  \n    def forward(self,    \n                tokens: Dict[str, torch.Tensor],\n                label: torch.Tensor = None) -> torch.Tensor:\n        mask = get_text_field_mask(tokens)\n        embeddings = self.embedder(tokens)\n        encoder_out = self.encoder(embeddings, mask)\n        logits = self.linear(encoder_out)\n        output = {\"logits\": logits}   \n        if label is not None:\n            self.accuracy(logits, label)\n            self.f1_measure(logits, label)\n            output[\"loss\"] = self.loss_function(logits, label)\n        return output\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {'accuracy': self.accuracy.get_metric(reset),    \n                **self.f1_measure.get_metric(reset)}\nEvery AllenNLP Model inherits from PyTorch’s Module class, meaning you can use\nPyTorch’s low-level operations if necessary. This gives you a lot of flexibility in defining\nyour model while leveraging AllenNLP’s high-level abstractions.  \n4.4.5\nPutting it all together\nFinally, we finish this section by implementing the entire pipeline to train the senti-\nment analyzer, as shown next. \nEMBEDDING_DIM = 128\nHIDDEN_DIM = 128\nreader = StanfordSentimentTreeBankDatasetReader()\ntrain_path = 'path/to/sst/dataset/train.txt'\ndev_path = 'path/to/sst/dataset/dev.txt'\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(  \n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\nListing 4.2\nTraining pipeline for the sentiment analyzer\nF1Measure() requires the label \nID for the positive class. '4' \nmeans “very positive.”\nCross-entropy loss \nis used for \nclassification tasks. \nCrossEntropyLoss \ndirectly takes logits \n(no softmax needed).\nInstances are \ndestructed to \nindividual fields \nand passed to \nforward().\nOutput of forward() is a dict, \nwhich contains a “loss” key.\nReturns accuracy, \nprecision, recall, \nand F1-measure \nas the metrics\nDefines how to construct \nthe data loaders\n",
      "content_length": 2359,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "102\nCHAPTER 4\nSentence classification\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), \n                                        dev_data_loader.iter_instances()),\n                                  min_count={'tokens': 3})\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\ntoken_embedding = Embedding(\n    num_embeddings=vocab.get_vocab_size('tokens'),\n    embedding_dim=EMBEDDING_DIM)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nmodel = LstmClassifier(word_embeddings, encoder, vocab)   \noptimizer = optim.Adam(model.parameters())   \ntrainer = GradientDescentTrainer(   \n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\ntrainer.train()\nThe training pipeline completes when the Trainer instance is created and invoked\nwith train(). You pass all the ingredients that you need for training—the model,\noptimizer, data loaders, datasets, and a bunch of hyperparameters.\n An optimizer implements an algorithm for adjusting the parameters of the model\nto minimize the loss. Here, we are using one type of optimizer called Adam, which is a\ngood “default” optimizer to use as your first option. However, as I mentioned in chap-\nter 2, you often need to experiment with many different optimizers that work best for\nyour model.\n4.5\nConfiguring AllenNLP training pipelines\nYou may have noticed that very little of listing 4.2 is actually specific to the sentence-\nclassification problem. Indeed, loading datasets, initializing a model, and plugging an\niterator and an optimizer into the trainer are all common steps across almost every\nNLP training pipeline. What if you want to reuse the same training pipeline for many\nrelated tasks without writing the training script from scratch? Also, what if you want to\nexperiment with different sets of configurations (e.g., different hyperparameters, neu-\nral network architectures) and save the exact configurations you tried?\n For those problems, AllenNLP provides a convenient framework where you can\nwrite configuration files in the JSON format. The idea is that you write the specifics of\nyour training pipeline—for example, which dataset reader to use, which models and\nInitializes \nthe model\nDefines the \noptimizer\nInitializes \nthe trainer\n",
      "content_length": 2476,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "103\nConfiguring AllenNLP training pipelines\ntheir subcomponents to use, and what hyper-parameters to use for training—in a\nJSON-formatted file (more precisely, AllenNLP uses a format called Jsonnet, which is a\nsuperset of JSON). Instead of rewriting your model file or the training script, you feed\nthe configuration file to the AllenNLP executable, and the framework takes care of\nrunning the training pipeline. If you want to try a different configuration for your\nmodel, you simply change the configuration file (or make a new one) and run the\npipeline again, without changing the Python code. This is a great practice for making\nyour experiment manageable and reproducible. You need to manage only the config-\nuration files and their results—the same configuration always yields the same result.\n A typical AllenNLP configuration file consists of three main parts—the dataset,\nyour model, and the training pipeline. The first part, shown next, specifies which\ndataset files to use and how:\n\"dataset_reader\": {\n    \"type\": \"sst_tokens\"\n  },\n  \"train_data_path\": \"https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/train.txt\",\n  \"validation_data_path\": \"https:/./s3.amazonaws.com/realworldnlpbook/data/\nstanfordSentimentTreebank/trees/dev.txt\"\nThree keys are in this part: dataset_reader, train_data_path, and valida-\ntion_data_path. The first key, dataset_reader, specifies which DatasetReader\nto use to read the files. Dataset readers, models, and predictors, as well as many other\ntypes of modules in AllenNLP, can be registered using the decorator syntax and be\nreferred to from configuration files. For example, if you peek at the following code\nwhere StanfordSentimentTreeBankDatasetReader is defined\n@DatasetReader.register(\"sst_tokens\")\nclass StanfordSentimentTreeBankDatasetReader(DatasetReader): \n    ...\nyou notice that it is decorated by @DatasetReader.register(\"sst_tokens\").\nThis registers StanfordSentimentTreeBankDatasetReader under the name\nsst_tokens, which allows you to refer it by \"type\": \"sst_tokens\" from the con-\nfiguration files.\n In the second part of the configuration file, you specify the main model to be\ntrained as follows:\n\"model\": {\n    \"type\": \"lstm_classifier\",\n    \"embedder\": {\n      \"token_embedders\": {\n        \"tokens\": {\n          \"type\": \"embedding\",\n          \"embedding_dim\": embedding_dim\n        }\n      }\n    },\n",
      "content_length": 2391,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "104\nCHAPTER 4\nSentence classification\n    \"encoder\": {\n      \"type\": \"lstm\",\n      \"input_size\": embedding_dim,\n      \"hidden_size\": hidden_dim\n    }\n}\nAs mentioned before, models in AllenNLP can be registered using the decorator syn-\ntax and be referred from the configuration files via the type key. For example, the\nLstmClassifier class referred here is defined as follows:\n@Model.register(\"lstm_classifier\")\nclass LstmClassifier(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\nOther keys in the model definition JSON dict correspond to the names of the\nparameters of the model constructor. In the previous definition, because Lstm-\nClassifier’s constructor takes two parameters, word_embeddings and encoder\n(in addition to vocab, which is passed by default and can be omitted, and\npositive_label, for which we are going to use the default value), the model defini-\ntion has two corresponding keys, the values of which are also model definitions and\nfollow the same convention.\n In the final part of the configuration file, the data loader and the trainer are speci-\nfied. The convention here is similar to the model definition—you specify the type of\nthe class along with other parameters passed to the constructor as follows:\n  \"data_loader\": {\n    \"batch_sampler\": {\n      \"type\": \"bucket\",\n      \"sorting_keys\": [\"tokens\"],\n      \"padding_noise\": 0.1,\n      \"batch_size\" : 32\n    }\n  },\n  \"trainer\": {\n    \"optimizer\": \"adam\",\n    \"num_epochs\": 20,\n    \"patience\": 10\n  }\nYou can see the full JSON configuration file in the code repository (http://realworld-\nnlpbook.com/ch4.html#sst-json). Once you define the JSON configuration file, you\ncan simply feed it to the allennlp command as follows:\nallennlp train examples/sentiment/sst_classifier.jsonnet \\\n    --serialization-dir sst-model \\\n    --include-package examples.sentiment.sst_classifier\n",
      "content_length": 2013,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "105\nCase study: Language detection\nThe --serialization-dir specifies where the trained model (along with addi-\ntional information such as serialized vocabulary data) is going to be stored. You also\nneed to specify the module path to LstmClassifier using --include-package so\nthat the configuration file can find the registered class.\n As we saw in chapter 2, when the training is finished, you can launch a simple web-\nbased demo interface using the following command:\n$ allennlp serve \\ \n    --archive-path sst-model/model.tar.gz \\\n    --include-package examples.sentiment.sst_classifier \\\n    --predictor sentence_classifier_predictor \\\n    --field-name sentence\n4.6\nCase study: Language detection\nIn this final section of the chapter, we are going to discuss another scenario—\nlanguage detection—which can also be formulated as a sentence-classification task. A\nlanguage-detection system, given a piece of text, detects the language the text is writ-\nten in. It has a wide range of uses in other NLP applications. For example, a web\nsearch engine may want to detect the language a web page is written in before process-\ning and indexing it. Google Translate also switches the source language automatically\nbased on what is typed in the input textbox.\n Let’s see what this actually looks like. Can you tell the language of each of the fol-\nlowing lines? These sentences are all taken from the Tatoeba project (https://\ntatoeba.org/).\n Contamos con tu ayuda.\n Bitte überleg es dir.\n Parti için planları tartıştılar.\n Je ne sais pas si je peux le faire.\n Você estava em casa ontem, não estava?\n Ĝi estas rapida kaj efika komunikilo.\n Ha parlato per un'ora.\n Szeretnék elmenni és meginni egy italt.\n Ttwaliɣ nezmer ad nili d imeddukal.\n \nThe answer is: Spanish, German, Turkish, French, Portuguese, Esperanto, Italian,\nHungarian, and Berber. I chose them from the top 10 most popular languages on\nTatoeba that are written in the roman alphabet. You may not be familiar with some of\nthe languages listed here. For those of you who are not, Esperanto is a constructed\nauxiliary language invented in the late 19th century. Berber is actually a group of\nrelated languages spoken in some parts of North Africa that are cousins of Semitic lan-\nguages such as Arabic.\n Maybe you were able to recognize some of these languages, even though you\ndon’t actually speak them. I’d like you to step back and think how you did it. It’s quite\n",
      "content_length": 2425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "106\nCHAPTER 4\nSentence classification\ninteresting that people can do this without actually being able to speak the language,\nbecause these languages are all written in the roman alphabet and could look quite\nsimilar to each other. You may have recognized some unique diacritic marks (accents)\nfor some of the languages—for example, “ü” for German and “ã” for Portuguese.\nThese are a strong clue for these languages. Or you just knew some words—for exam-\nple, “ayuda” for Spanish (meaning “help”) and “pas” in French (“ne . . . pas” is a\nFrench negation syntax). It appears that every language has its own characteristics—\nbe it some unique characters or words—that makes it easy to tell it apart from others.\nThis is starting to sound a lot like a kind of problem that machine learning is good at\nsolving. Can we build an NLP system that can do this automatically? How should we go\nabout building it?\n4.6.1\nUsing characters as input\nA language detector can also be built in a similar way to the sentiment analyzer. You\ncan use an RNN to read the input text and convert it to some internal representation\n(hidden states). You can then use a linear layer to convert them to a set of scores cor-\nresponding to how likely the text is written in each language. Finally, you can use\ncross-entropy loss to train the model.\n One major difference between the sentiment analyzer and the language detector\nis how you feed the input into an RNN. When building the sentiment analyzer, we\nused the Stanford Sentiment Treebank and were able to assume that the input text is\nalways English and already tokenized. But this is not the case for language detection.\nIn fact, you don’t even know whether the input text is written in a language that can\nbe tokenized easily—what if the sentence is written in Chinese? Or in Finnish, which is\ninfamous for its complex morphology? You could use a tokenizer that is specific to the\nlanguage if you know what language it is, but we are building the language detector\nbecause we don’t know what language it is in the first place. This sounds like a typical\nchicken-and-egg problem.\n To address this issue, we are going to use characters instead of tokens as the input\nto an RNN. The idea is to break down the input into individual characters, even includ-\ning whitespace and punctuation, and feed them to the RNN one at a time. Using char-\nacters is a common practice used when the input can be better represented as a\nsequence of characters (such as Chinese, or of an unknown origin), or when you’d like\nto make the best use of internal structures of words (such as the fastText model we men-\ntioned in chapter 3). The RNN’s powerful representational power can still capture inter-\nactions between characters and some common words and n-grams mentioned earlier.\n4.6.2\nCreating a dataset reader\nFor this language-detection task, I created the train and the validation datasets from\nthe Tatoeba project by taking the 10 most popular languages on Tatoeba that use the\nroman alphabet and by sampling 10,000 sentences for the train set and 1,000 for the\nvalidation set. An excerpt of this dataset follows:\n",
      "content_length": 3125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "107\nCase study: Language detection\npor\nDe entre os designers, ele escolheu um jovem ilustrador e deu-lhe a \ntarefa.\npor\nA apresentação me fez chorar.\ntur\nBunu denememize gerek var mı?\ntur\nO korkutucu bir parçaydı.\nber\nTebḍamt aɣrum-nni ɣef sin, naɣ?\nber\nAd teddud ad twalid taqbuct n umaḍal n tkurt n uḍar deg Brizil?\neng\nTom works at Harvard.\neng\nThey fixed the flat tire by themselves.\nhun\nAz arca hirtelen elpirult.\nhun\nMiért aggodalmaskodsz? Hiszen még csak egy óra van!\nepo\nSidiĝu sur la benko.\nepo\nTiu ĉi kutime funkcias.\nfra\nVu d'avion, cette île a l'air très belle.\nfra\nNous boirons à ta santé.\ndeu\nDas Abnehmen fällt ihm schwer.\ndeu\nTom war etwas besorgt um Maria.\nita\nSono rimasto a casa per potermi riposare.\nita\nLe due più grandi invenzioni dell'uomo sono il letto e la bomba atomica: \nil primo ti tiene lontano dalle noie, la seconda le elimina.\nspa\nHe visto la película.\nspa\nHas hecho los deberes.\nThe first field is a three-letter language code that describes which language the text is\nwritten in. The second field is the text itself. The fields are delimited by a tab character.\nYou can obtain the datasets from the code repository (https://github.com/mhagiwara/\nrealworldnlp/tree/master/data/tatoeba).\n The first step in building a language detector is to prepare a dataset reader that can\nread datasets in this format. In the previous example (the sentiment analyzer), because\nAllenNLP already provides StanfordSentimentTreeBankDatasetReader, you\njust needed to import and use it. In this scenario, however, you need to write your\nown. Fortunately, writing a dataset reader that can read this particular format is not\nthat difficult. To write a dataset reader, you just need to do the following three things:\nCreate your own dataset reader class by inheriting DatasetReader.\nOverride the text_to_instance() method that takes raw text and converts it\nto an instance object.\nOverride the_read() method that reads the content of a file and yields\ninstances, by calling text_to_instance() above.\nThe complete dataset reader for the language detector is shown in listing 4.3. We also\nassume that you already imported necessary modules and classes as follows:\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\n",
      "content_length": 2373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "108\nCHAPTER 4\nSentence classification\nfrom allennlp.data.dataset_readers import DatasetReader\nfrom allennlp.data.fields import LabelField, TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training import GradientDescentTrainer\nfrom overrides import overrides\nfrom examples.sentiment.sst_classifier import LstmClassifier\nclass TatoebaSentenceReader(DatasetReader):    \n    def __init__(self,\n                 token_indexers: Dict[str, TokenIndexer]=None):\n        super().__init__()\n        self.tokenizer = CharacterTokenizer()   \n        self.token_indexers = token_indexers or {'tokens': \nSingleIdTokenIndexer()}\n    @overrides\n    def text_to_instance(self, tokens, label=None):  \n        fields = {}\n        fields['tokens'] = TextField(tokens, self.token_indexers)\n        if label:\n            fields['label'] = LabelField(label)\n        return Instance(fields)\n    @overrides\n    def _read(self, file_path: str):\n        file_path = cached_path(file_path)   \n        with open(file_path, \"r\") as text_file:\n            for line in text_file:\n                lang_id, sent = line.rstrip().split('\\t')\n                tokens = self.tokenizer.tokenize(sent)\n                yield self.text_to_instance(tokens, lang_id)    \nNote that the dataset reader in listing 4.3 uses CharacterTokenizer() to tokenize\ntext into characters. Its tokenize() method returns a list of tokens, which are Allen-\nNLP objects that represent tokens but actually contain characters in this scenario. \n4.6.3\nBuilding the training pipeline\nOnce you build the dataset reader, the rest of the training pipeline looks similar to\nthat of the sentiment analyzer. In fact, we can reuse the LstmClassifier class we\nListing 4.3\nDataset reader for the language detector \nEvery new dataset \nreader inherits \nDatasetReader.\nUses CharacterTokenizer() \nto tokenize text into \ncharacters\nLabel will be None \nat test time.\nIf file_path is an URL, returns the \nactual path to a cached file on disk\nYields instances using\ntext_to_instance(),\ndefined earlier\n",
      "content_length": 2488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "109\nCase study: Language detection\ndefined previously without any modification. The entire training pipeline is shown in\nlisting 4.4. You can access the Google Colab notebook of the entire code from here:\nhttp://realworldnlpbook.com/ch4.html#langdetect.\nEMBEDDING_DIM = 16\nHIDDEN_DIM = 16\nreader = TatoebaSentenceReader()\ntrain_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/tatoeba/\nsentences.top10langs.train.tsv'\ndev_path = 'https:/./s3.amazonaws.com/realworldnlpbook/data/tatoeba/\nsentences.top10langs.dev.tsv'\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\nvocab = Vocabulary.from_instances(train_data_loader.iter_instances(),\n                                  min_count={'tokens': 3})\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_DIM)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nmodel = LstmClassifier(word_embeddings,\n                       encoder,\n                       vocab,\n                       positive_label='eng')\ntrain_dataset.index_with(vocab)\ndev_dataset.index_with(vocab)\noptimizer = optim.Adam(model.parameters())\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=20,\n    cuda_device=-1)\ntrainer.train()\nListing 4.4\nTraining pipeline for the language detector\n",
      "content_length": 1804,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "110\nCHAPTER 4\nSentence classification\nWhen you run this training pipeline, you’ll get the metrics on the dev set that are in\nthe ballpark of the following:\naccuracy: 0.9461, precision: 0.9481, recall: 0.9490, f1_measure: 0.9485, \nloss: 0.1560\nThis is not bad at all! This means that the trained detector makes only one mistake out\nof about 20 sentences. Precision of 0.9481 means there’s only one false positive (non-\nEnglish sentence) out of 20 instances that are classified as English. Recall of 0.9490\nmeans there’s only one false negative (English sentence that was missed by the detec-\ntor) out of 20 true English instances.\n4.6.4\nRunning the detector on unseen instances\nFinally, let’s try running the detector we just trained on a set of unseen instances\n(instances that didn’t appear either in the train or the validation sets). It is always a\ngood idea to try feeding a small number of instances to your model and observe how\nit behaves. \n The recommended way for feeding instances into a trained AllenNLP model is to\nuse a predictor, as we did in chapter 2. But here I’d like to do something simpler and\ninstead write a method that, given a piece of text and a model, runs the prediction\npipeline. To run a model on arbitrary instances, you can call the model’s forward_\non_instances() method, as shown in the following snippet:\ndef classify(text: str, model: LstmClassifier):\n    tokenizer = CharacterTokenizer()\n    token_indexers = {'tokens': SingleIdTokenIndexer()}\n    tokens = tokenizer.tokenize(text)\n    instance = Instance({'tokens': TextField(tokens, token_indexers)})\n    logits = model.forward_on_instance(instance)['logits']\n    label_id = np.argmax(logits)\n    label = model.vocab.get_token_from_index(label_id, 'labels')\n    print('text: {}, label: {}'.format(text, label))\nThis method first takes the input (text and model) and passes it through a tokenizer\nto create an instance object. Then it calls model’s forward_on_instance() method\nto retrieve the logits, the scores for target labels (languages). It gets the label ID that\ncorresponds to the maximum logit value by calling np.argmax and then converts it to\nthe label text by using the vocabulary object associated with the model. \n When I ran this method on some sentences that are not in the two datasets, I got\nthe following results. Note that the result you get may be different from mine due to\nsome randomness:\ntext: Take your raincoat in case it rains., label: fra\ntext: Tu me recuerdas a mi padre., label: spa\ntext: Wie organisierst du das Essen am Mittag?, label: deu\ntext: Il est des cas où cette règle ne s'applique pas., label: fra\ntext: Estou fazendo um passeio em um parque., label: por\n",
      "content_length": 2684,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "111\nSummary\ntext: Ve, postmorgaŭ jam estas la limdato., label: epo\ntext: Credevo che sarebbe venuto., label: ita\ntext: Nem tudja, hogy én egy macska vagyok., label: hun\ntext: Nella ur nli qrib acemma deg tenwalt., label: ber\ntext: Kurşun kalemin yok, deǧil mi?, label: tur\nThese predictions are almost perfect, except the very first sentence—it is English, not\nFrench. It is surprising that the model makes a mistake on such a seemingly easy sen-\ntence while it predicts more difficult languages (such as Hungarian) perfectly. But\nremember, how difficult the language is for English speakers has nothing to do with\nhow difficult it is for a computer to classify. In fact, some of the “difficult” languages\nsuch as Hungarian and Turkish here have very clear signals (accent marks and unique\nwords) that make it easy to detect them. On the other hand, lack of clear signals in the\nfirst sentence may have made it more difficult to classify it from other languages. \n As a next step, you could try a couple of things: for example, you can tweak some\nof the hyperparameters to see how the evaluation metrics and the final prediction\nresults change. You can also try a larger number of test instances to see how exactly\nthe mistakes are distributed (e.g., between which two languages). You can also zero in\non some of the instances and see why the model made such mistakes. These are all\nimportant practices when you are working on real-world NLP applications. I’ll discuss\nthese topics in detail in chapter 10.\nSummary\nA recurrent neural network (RNN) is a neural network with a loop. It can trans-\nform a variable-length input to a fixed-length vector.\nNonlinearity is a crucial component that makes neural networks truly powerful.\nLSTMs and GRUs are two variants of RNN cells and are easier to train than\nvanilla RNNs.\nYou use accuracy, precision, recall, and F-measure for classification problems.\nAllenNLP provides useful NLP abstractions such as dataset readers, instances,\nand vocabulary. It also provides a way to configure the training pipeline in the\nJSON format.\nYou can build a language detector as a sentence-classification application simi-\nlar to the sentiment analyzer.\n",
      "content_length": 2187,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "112\nSequential labeling\nand language modeling\nIn this chapter, we are going to discuss sequential labeling—an important NLP\nframework where systems tag individual words with corresponding labels. Many\nNLP applications, such as part-of-speech tagging and named entity recognition,\ncan be framed as sequential-labeling tasks. In the second half of the chapter, I’ll\nintroduce the concept of language models, one of the most fundamental yet excit-\ning topics in NLP. I’ll talk about why they are important and how to use them to\nevaluate and even generate some natural language text.\nThis chapter covers\nSolving part-of-speech (POS) tagging and named \nentity recognition (NER) using sequential labeling\nMaking RNNs more powerful—multilayer and \nbidirectional recurrent neural networks (RNNs)\nCapturing statistical properties of language using \nlanguage models\nUsing language models to evaluate and generate \nnatural language text\n",
      "content_length": 931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "113\nIntroducing sequential labeling\n5.1\nIntroducing sequential labeling\nIn the previous chapter, we discussed sentence classification, where the task is to\nassign some label to a given sentence. Spam filtering, sentiment analysis, and lan-\nguage detection are some concrete examples of sentence classification. Although\nmany real-world NLP problems can be formulated as a sentence-classification task,\nthis method can also be quite limited, because the model, by definition, allows us to\nassign only a single label to the whole sentence. But what if you wanted something\nmore granular? For example, what if you wanted to do something with individual\nwords, not just with the sentence? The most typical scenario you encounter is when\nyou want to extract something from the sentence, which cannot be easily solved by\nsentence classification. This is where sequential labeling comes into play.\n5.1.1\nWhat is sequential labeling?\nSequential labeling is an NLP task where, given a sequence such as a sentence, the NLP\nsystem assigns a label to each element (e.g., word) of the input sequence. This con-\ntrasts with sentence classification, where a label is assigned just to the input sentence.\nFigure 5.1 illustrates this contrast.\nFigure 5.1\nSentence classification vs. sequential labeling\nBut why is this even a good idea? When do we need a label per word? A typical scenario\nwhere sequential labeling comes in handy is when you want to analyze a sentence and\nproduce some linguistic information per word. For example, part-of-speech (POS) tag-\nging, which I mentioned in chapter 1, produces a POS tag such as nouns, verbs, and\nprepositions for each word in the input sentence and is a perfect match for sequential\nlabeling. See figure 5.2 for an illustration.\n POS tagging is one of the most fundamental, important NLP tasks. Many English\nwords (and words in many other languages as well) are ambiguous, meaning that they\nhave multiple possible interpretations. For example, the word “book” can be used to\ndescribe a physical or electronic object consisting of pages (“I read a book”) or an\nSentence classification\nSequential labeling\nInput sentence\nword1\nword2\nword3\nwordn\nlabel\n...\nInput sentence\nlabel1\nword1\nword2\nword3\nwordn\n...\nlabel2\nlabel3\nlabeln\n",
      "content_length": 2253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "114\nCHAPTER 5\nSequential labeling and language modeling\naction for reserving something (“I need to book a flight”). Downstream NLP tasks, such\nas parsing and classification, benefit greatly by knowing what each appearance of\n“book” actually means to process the input sentence. If you were to build a speech syn-\nthesis system, you must know the POS of certain words to pronounce them correctly—\n“lead” as a noun (a kind of metal) rhymes with “bed,” whereas “lead” as a verb (to direct,\nguide) rhymes with “bead.” POS tagging is an important first step toward solving this\nambiguity.\n Another scenario is when you want to extract some pieces of information from a\nsentence. For example, if you want to extract subsequences (phrases) such as noun\nphrases and verb phrases, this is also a sequential-labeling task. How can you achieve\nextraction using labeling? The idea is to mark the beginning and the end (or the\nbeginning and the continuation, depending on how you represent it) of the desired\npiece of information using labeling. An example of this is named entity recognition\n(NER), which is a task to identify mentions to real-world entities, such as proper\nnouns and numerical expressions, from a sentence (illustrated in figure 5.3.). \nFigure 5.3\nNamed entity recognition (NER) using sequential labeling\nNotice that all the words that are not part of any named entities are tagged as O (for\n“Outside”). For now, you can ignore some cryptic labels in figure 5.3 such as B-GPE\nand I-MONEY. I’ll talk more about how to formulate NER as a sequential-labeling\nproblem in section 5.4.\nPart-of-speech (POS) tagging\nInput sentence\nPOS tags\nNOUN\nTime\nflies\nlike\nVERB\nADP\nan\nDET\narrow\nNOUN\n.\nPUNKT\nFigure 5.2\nPart-of-speech (POS) \ntagging using sequential labeling\nNamed entity recognition (NER)\nApple\nis\nlooking\nto\nbuy\nUK\nstartup for\n$1\nbillion\nB-ORG\nO\nO\nO\nO\nB-GPE\nO\nO\nB-MONEYI-MONEY\nInput sentence\nNER tags\n",
      "content_length": 1906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "115\nIntroducing sequential labeling\n5.1.2\nUsing RNNs to encode sequences\nIn sentence classification, we used recurrent neural networks (RNNs) to convert an\ninput of variable length into a fixed-length vector. The fixed-length vector, which is\nconverted to a set of “scores” by a linear layer, captures the information about the\ninput sentence that is necessary for deriving the sentence label. As a reminder, what\nthis RNN does can be represented by the following pseudocode and the diagram\nshown in figure 5.4: \ndef rnn_vec(words):\n    state = init_state()\n    for word in words:\n        state = update(state, word)\n    return state\nFigure 5.4\nRecurrent neural network (RNN) for sentence classification\nWhat kind of neural network could be used for sequential tagging? We seem to need\nsome information for every input word in the sentence, not just at the end. If you look\nat the pseudocode for rnn_vec() carefully, you can notice that we already have infor-\nmation for every word in the input, which is captured by state. The function just\nhappens to return only the final value of state, but there is no reason we can’t store\nintermediate values of state and return them as a list instead, as in the following\nfunction:\ndef rnn_seq(words):\n    state = init_state()\n    states = []\n    for word in words:\n        state = update(state, word)\n        states.append(state)\n    return states\ninit_state()\nstate\nupdate\nupdate\nupdate\nupdate\nv(\"john\")\nv(\"loves\")\nv(\"mary\")\nv(\".\")\nRecurrent neural network (RNN)\n",
      "content_length": 1506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "116\nCHAPTER 5\nSequential labeling and language modeling\nIf you apply this function to the “time flies” example shown in figure 5.2 and unroll\nit—that is, write it without using a loop—it will look like the following:\nstate = init_state()\nstates = []\nstate = update(state, v(\"time\"))\nstates.append(state)\nstate = update(state, v(\"flies\"))\nstates.append(state)\nstate = update(state, v(\"like\"))\nstates.append(state)\nstate = update(state, v(\"an\"))\nstates.append(state)\nstate = update(state, v(\"arrow\"))\nstates.append(state)\nstate = update(state, v(\".\"))\nstates.append(state)\nNote that v() here is a function that returns the embedding for the given word. This\ncan be visualized as shown in figure 5.5. Notice that for each input word word, the net-\nwork produces the corresponding state that captures some information about word.\nThe length of the list states is the same as that of words. The final value of states,\nthat is, states[-1], is identical to the return value of rnn_vec() from earlier.\nFigure 5.5\nRecurrent neural network (RNN) for sequential labeling\nIf you think of this RNN as a black box, it takes a sequence of something (e.g., word\nembeddings) and converts it to a sequence of vectors that encode some information\nabout individual words in the input, so this architecture is called a Seq2Seq (for\n“sequence-to-sequence”) encoder in AllenNLP.\n The final step is to apply a linear layer to each state of this RNN to derive a set of\nscores that correspond to how likely each label is. If this is a part-of-speech tagger, we\nRecurrent \nneural \nnetwork \n(RNN)\ninit_state()\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\"arrow\")\nv(\".\")\nstate\nstate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\nstate\nstate\nstate\n",
      "content_length": 1700,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "117\nIntroducing sequential labeling\nneed one score for the label NOUN,\nanother for VERB, and so on for\neach and every word. This conver-\nsion is illustrated in figure 5.6. Note\nthat the same linear layer (with the\nsame set of parameters) is applied to\nevery state.\n To sum up, we can use almost the\nsame structure for sequential label-\ning as the one we used for sentence\nclassification. The only difference is\nthe former produces a hidden state\nper each word, not just per sentence.\nTo derive scores used for determin-\ning labels, a linear layer is applied to\nevery hidden state. \n5.1.3\nImplementing a Seq2Seq encoder \nin AllenNLP\nAllenNLP implements an abstract\nclass called Seq2SeqEncoder for\nabstracting all Seq2Seq encoders that\ntake in a sequence of vectors and\nreturn another sequence of modified\nvectors. In theory, you can inherit the\nclass and implement your own Seq2Seq encoder. In practice, however, you most likely\nwill use one of the off-the-shelf implementations that PyTorch/AllenNLP provide, such\nas LSTM and GRU. Remember, when we built the encoder for the sentiment analyzer,\nwe used PyTorch’s built-in torch.nn.LSTM and wrapped it with PytorchSeq2Vec-\nWrapper, as shown next, which makes it compatible with AllenNLP’s abstraction:\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nAllenNLP also implements PytorchSeq2SeqWrapper, which takes one of PyTorch’s\nbuilt-in RNN implementations and makes it compliant with AllenNLP’s Seq2Seq-\nEncoder, so there’s very little change you need to initialize a Seq2Seq encoder, as\nshown here:\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nThat’s it! There are a couple more things to note, but there’s surprisingly few changes\nyou need to make to the sentence classification code to make it work for sequential\nlabeling. This is thanks to the powerful abstraction of AllenNLP—most of the time\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nstate\nstate\nlogits\nRNN\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\n \n…\nNOUN VERB …\nFigure 5.6\nApplying a linear layer to RNN\n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "118\nCHAPTER 5\nSequential labeling and language modeling\nyou need to worry only about how individual components interact with each other,\nnot about how these components work internally. \n5.2\nBuilding a part-of-speech tagger\nIn this section, we are going to build our first sequential-labeling application—a part-\nof-speech (POS) tagger. You can see the entire code for this section on the Google\nColab notebook (http://realworldnlpbook.com/ch5.html#pos-nb). We assume that\nyou have already imported all necessary dependencies as follows:\nfrom itertools import chain\nfrom typing import Dict\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom allennlp.data.data_loaders import MultiProcessDataLoader\nfrom allennlp.data.samplers import BucketBatchSampler\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, \nPytorchSeq2SeqWrapper\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, \nBasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.nn.util import get_text_field_mask, \nsequence_cross_entropy_with_logits\nfrom allennlp.training.metrics import CategoricalAccuracy\nfrom allennlp.training import GradientDescentTrainer\nfrom \nallennlp_models.structured_prediction.dataset_readers.universal_dependen\ncies import UniversalDependenciesDatasetReader\nfrom realworldnlp.predictors import UniversalPOSPredictor\n5.2.1\nReading a dataset\nAs we saw in chapter 1, a part of speech (POS) is a category of words that share similar\ngrammatical properties. Part-of-speech tagging is the process of tagging each word in\na sentence with a corresponding part-of-speech tag. A training set for POS tagging fol-\nlows a tagset, which is a set of predefined POS tags for the language.\n To train a POS tagger, we need a dataset where every word in every sentence is\ntagged with its corresponding POS tag. In this experiment, we are going to use the\nEnglish Universal Dependencies (UD) dataset. Universal Dependencies is a language-\nindependent dependency grammar framework developed by a group of researchers.\nUD also defines a tagset called the Universal part-of-speech tagset (http://realworldnlp-\nbook.com/ch1.html#universal-pos). The use of UD and the Universal POS tagset has\nbeen very popular in the NLP community, especially for language-independent tasks\nand models such as POS tagging and parsing.\n",
      "content_length": 2440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "119\nBuilding a part-of-speech tagger\n We are going to use one subcorpus of UD called A Gold Standard Universal Depen-\ndencies Corpus for English, which is built on top of the English Web Treebank (EWT)\n(http://realworldnlpbook.com/ch5.html#ewt) and can be used under a Creative\nCommons license. You can download the entire dataset from the dataset page (http://\nrealworldnlpbook.com/ch5.html#ewt-data), if needed.\n Universal Dependencies datasets are distributed in a format called the CoNLL-U\nformat (http://universaldependencies.org/docs/format.html). The AllenNLP models\npackage already implements a dataset reader called UniversalDependencies-\nDatasetReader that reads datasets in this format and returns a collection of\ninstances that include information like word forms, POS tags, and dependency rela-\ntionship, so all you need to do is initialize and use it as follows:\nreader = UniversalDependenciesDatasetReader()\ntrain_path = ('https:/./s3.amazonaws.com/realworldnlpbook/data/'\n              'ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu')\ndev_path = ('https:/./s3.amazonaws.com/realworldnlpbook/'\n            'data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-dev.conllu')\nAlso, don’t forget to initialize data loaders and a Vocabulary instance, too, as shown\nnext:\nsampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"words\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, train_path, batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, dev_path, batch_sampler=sampler)\nvocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(),\n                                        dev_data_loader.iter_instances()))\ntrain_data_loader.index_with(vocab)\ndev_data_loader.index_with(vocab)\n5.2.2\nDefining the model and the loss\nThe next step for building a POS tagger is to define the model. In the previous sec-\ntion, we already saw that you can initialize a Seq2Seq encoder with very little modifica-\ntion using AllenNLP’s built-in PytorchSeq2VecWrapper. Let’s define other\ncomponents (word embeddings and LSTM) and some variables necessary for the\nmodel as follows:\nEMBEDDING_SIZE = 128\nHIDDEN_SIZE = 128\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                            embedding_dim=EMBEDDING_SIZE)\nword_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\nlstm = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))\n",
      "content_length": 2470,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "120\nCHAPTER 5\nSequential labeling and language modeling\nNow we are ready to define the body of the POS tagger model, as shown here.\nclass LstmTagger(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2SeqEncoder,\n                 vocab: Vocabulary) -> None:\n        super().__init__(vocab)\n        self.embedder = embedder\n        self.encoder = encoder\n        \n        self.linear = torch.nn.Linear(\n            in_features=encoder.get_output_dim(),\n            out_features=vocab.get_vocab_size('pos'))\n        \n        self.accuracy = CategoricalAccuracy()    \n    def forward(self,\n                words: Dict[str, torch.Tensor],\n                pos_tags: torch.Tensor = None,\n                **args) -> Dict[str, torch.Tensor]:   \n        mask = get_text_field_mask(words)\n        embeddings = self.embedder(words)\n        encoder_out = self.encoder(embeddings, mask)\n        tag_logits = self.linear(encoder_out)\n        output = {\"tag_logits\": tag_logits}\n        if pos_tags is not None:\n            self.accuracy(tag_logits, pos_tags, mask)\n            output[\"loss\"] = sequence_cross_entropy_with_logits(\n                tag_logits, pos_tags, mask)   \n        return output\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {\"accuracy\": self.accuracy.get_metric(reset)}\nNotice that the code shown in listing 5.1 is very similar to the code for LstmClassi-\nfier (listing 4.1), which we used for building a sentiment analyzer. In fact, except for\nsome naming differences, only one fundamental difference exists—the type of loss\nfunction.\n Recall that we used a loss function called cross entropy for sentence classification\ntasks, which basically measures how far apart two distributions are. If the model pro-\nduces a high probability for the true label, the loss will be low. Otherwise, it will be\nhigh. But this assumed that there is only one label per sentence. How can we measure\nhow far the prediction is from the true label when there is one label per word?\nListing 5.1\nPOS tagger model\nWe use accuracy to \nevaluate the POS tagger.\nWe need **args to capture \nunnecessary instance \nfields that AllenNLP \nautomatically destructures.\nThe Seq2Seq encoder is\ntrained using a sequence\ncross-entropy loss.\n",
      "content_length": 2303,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "121\nBuilding a part-of-speech tagger\n The answer is: still use the\ncross entropy, but average it\nover all the elements in the\ninput sequence. For POS tag-\nging, you compute the cross\nentropy per word as if it were\nan \nindividual \nclassification\ntask, sum it over all the words\nin the input sentence, and\ndivide by the length of the sen-\ntence. This will give you a num-\nber reflecting how well your\nmodel is predicting the POS\ntags for the input sentence on\naverage. See figure 5.7 for an\nillustration.\n As for the evaluation met-\nric, POS taggers are usually\nevaluated \nusing \naccuracy,\nwhich we are going to use here.\nAverage human performance\non POS tagging is around 97%,\nwhereas \nthe \nstate-of-the-art\nPOS taggers slightly outper-\nform this (http://realworldnlp\nbook.com/ch5.html#pos-sota).\nYou need to note that accuracy\nis not without a problem, however—assume there is a relatively rare POS tag (e.g.,\nSCONJ, which means subordinating conjugation), which accounts for only 2% of total\ntokens, and a POS tagger messes it up every time it appears. If the tagger gets the rest\nof the tokens all correct, it still achieves 98% accuracy. \n5.2.3\nBuilding the training pipeline\nNow we are ready to move on to building the training pipeline. As with the previous\ntasks, training pipelines in AllenNLP look very similar to each other. See the next list-\ning for the training code.\nmodel = LstmTagger(word_embeddings, encoder, vocab)\noptimizer = optim.Adam(model.parameters())\nListing 5.2\nTraining pipeline for POS tagger\nstate\nstate\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\n \nLinear\nlayer\n \nCross\nentropy\n \nLinear\nlayer\n \nRNN\nlabel\nCross\nentropy\n \nlabel\nCross\nentropy\n \nlabel\nlogits\n+\n+\n=\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nTotal\nloss\nFigure 5.7\nComputing loss for sequence\n",
      "content_length": 1768,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "122\nCHAPTER 5\nSequential labeling and language modeling\ntrainer = GradientDescentTrainer(\n    model=model,\n    optimizer=optimizer,\n    data_loader=train_data_loader,\n    validation_data_loader=dev_data_loader,\n    patience=10,\n    num_epochs=10,\n    cuda_device=-1)\ntrainer.train()\nWhen you run this code, AllenNLP alternates between two phases: 1) training the\nmodel using the train set, and 2) evaluating it using the validation set for each epoch,\nwhile monitoring the loss and accuracy on both sets. Validation set accuracy plateaus\naround 88% after several epochs. After the training is over, you can run the model for\nan unseen instance as shown next:\npredictor = UniversalPOSPredictor(model, reader)\ntokens = ['The', 'dog', 'ate', 'the', 'apple', '.']\nlogits = predictor.predict(tokens)['tag_logits']\ntag_ids = np.argmax(logits, axis=-1)\nprint([vocab.get_token_from_index(tag_id, 'pos') for tag_id in tag_ids])\nThis code uses UniversalPOSPredictor, a predictor that I wrote for this particular\nPOS tagger. Although its details are not important, you can look at its code if you are\ninterested (http://realworldnlpbook.com/ch5#upos-predictor). If successful, this will\nshow a list of POS tags: ['DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'PUNCT'],\nwhich is indeed a correct POS tag sequence for the input sentence.\n5.3\nMultilayer and bidirectional RNNs\nAs we’ve seen so far, RNNs are a powerful tool for building NLP applications. In this\nsection, I talk about their structural variants—multilayer and bidirectional\nRNNs—which are even more powerful components for building highly accurate NLP\napplications.\n5.3.1\nMultilayer RNNs\nIf you look at an RNN as a black box, it is a neural network structure that converts a\nsequence of vectors (word embeddings) into another sequence of vectors (hidden\nstates). The input and output sequences are of the same length, usually the number of\ninput tokens. This means that you can repeat this “encoding” process multiple times by\nstacking RNNs on top of each other. The output (hidden states) of one RNN becomes\nthe input of another RNN that is just above the previous one. A substructure (such as\na single RNN) of a bigger neural network is called a layer, because you can stack them\ntogether like layers. The structure of a two-layered RNN is shown in figure 5.8.\n",
      "content_length": 2307,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "123\nMultilayer and bidirectional RNNs\nWhy is this a good idea? If you think of a layer of RNN as a machine that takes in some-\nthing concrete (e.g., word embeddings) and extracts some abstract concepts (e.g.,\nscores for POS tags), you can expect that, by repeating this process, RNNs are able to\nextract increasingly more abstract concepts as the number of layers increases.\nAlthough not fully theoretically proven, many real-world NLP applications use multi-\nlayer RNNs. For example, Google’s Neural Machine Translation (NMT) system uses a\nstacked RNN consisting of eight layers for both the encoder and the decoder (http://\nrealworldnlpbook.com/ch5.html#nmt-paper).\n To use multilayer RNNs in your NLP application, all you need to do is change how\nthe encoder is initialized. Specifically, you need to specify only the number of layers\nLayer 1\nLayer 2\ninit_state()\n...\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\ninit_state()\nstate\nstate\nstate\nstate\nstate\nlogits\nLinear\nlayer\n \n…\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\"arrow\")\nv(\".\")\nNOUN VERB …\nFigure 5.8\nTwo-layered RNN\n",
      "content_length": 1100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "124\nCHAPTER 5\nSequential labeling and language modeling\nusing the num_layers parameter, as shown in the next code snippet, and AllenNLP\nmakes sure that the rest of the training pipeline works as-is:\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(\n        EMBEDDING_SIZE, HIDDEN_SIZE, num_layers=2, batch_first=True))\nIf you change this line and rerun the POS tagger training pipeline, you will notice that\naccuracy on the validation set is almost unchanged or slightly lower than the previous\nmodel with a single-layer RNN. This is not surprising—information required for POS\ntagging is mostly superficial, such as the identity of the word being tagged and neigh-\nboring words. Very rarely does it require deep understanding of the input sentence.\nOn the other hand, adding layers to an RNN is not without additional cost. It slows\ndown the training and inference and increases the number of parameters, which\nmakes it susceptible to overfitting. For this small experiment, adding layers to the\nRNN seems to do more harm than good. When you change the structure of the net-\nwork, always remember to verify its effect on a validation set.\n5.3.2\nBidirectional RNNs\nSo far, we’ve been feeding words to RNNs as they come in—from the beginning of the\nsentence to the end. This means that when an RNN is processing a word, it can lever-\nage only the information it has encountered so far, which is the word’s left context.\nTrue, you can get a lot of information from a word’s left context. For example, if a\nword is preceded by a modal verb (e.g., “can”), it is a strong signal that the next word\nis a verb. However, the right context holds a lot of information as well. For example, if\nyou know that the next word is a determiner (e.g., “a”), it is a strong signal that “book”\non its left is a verb, not a noun. \n Bidirectional RNNs (or simply biRNNs) solve this problem by combining two\nRNNs with opposite directions. A forward RNN is a forward-facing RNN that we’ve\nbeen using so far in this book—it scans the input sentence left to right and uses the\ninput word and all the information on its left to update the state. A backward RNN, on\nthe other hand, scans the input sentence right to left. It uses the input word and all\nthe information on its right to update the state. This is equivalent to flipping the\norder of the input sentence and feeding it to a forward RNN. The final hidden states\nproduced by biRNNs are concatenations of hidden states from the forward and back-\nward RNNs. See figure 5.9 for an illustration.\n Let’s use a concrete example to illustrate this. Assume the input sentence is “time\nflies like an arrow” and you’d like to know the POS tag for the word “like” in the mid-\ndle of this sentence. The forward RNN processes “time” and “flies,” and by the time it\nreaches “like,” its internal state (A in figure 5.9) encodes all the information about\n“time flies like.” Similarly, the backward RNN processes “arrow” and “an,” and by\nthe time it reaches “like,” the internal state (B in figure 5.9) has encoded all the infor-\nmation about “like an arrow.” The internal state from the biRNN for “like” is the\n",
      "content_length": 3129,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "125\nMultilayer and bidirectional RNNs\nconcatenation of these two states (A + B). You literally stitch together two vectors—no\nmathematical operations involved. As a result, the internal state for “like” encodes all\nthe information from the entire sentence. This is a great improvement over just know-\ning half the sentence!\n Implementing a biRNN is similarly easy—you just need to add the bidirec-\ntional=True flag when initializing the RNN as follows:\nencoder = PytorchSeq2SeqWrapper(\n    torch.nn.LSTM(\n        EMBEDDING_SIZE, HIDDEN_SIZE, bidirectional=True, batch_first=True))\nIf you train the POS tagger with this change, the validation set accuracy will jump\nfrom ~88% to 91%. This implies that incorporating the information on both sides of\nthe word is effective for POS tagging.\n Note that you can combine the two techniques introduced in this section by stack-\ning bidirectional RNNs. The output from one layer of biRNN (concatenation of a\nforward and a backward layer) becomes the input to another layer of biRNN (see\nfigure 5.10). You can implement this by specifying both flags—num_layers and\nbidirectional—when initializing the RNN in PyTorch/AllenNLP.\ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nstate\nstate\nstate\nstate\nForward\nlayer\n \n \ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\n \nBackward\nlayer\n \nlogits\nLinear\nlayer\n \nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\".\")\n…\nNOUN VERB …\n(A)\n(B)\nFigure 5.9\nBidirectional RNN\n",
      "content_length": 1488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "126\nCHAPTER 5\nSequential labeling and language modeling\nFigure 5.10\nTwo-layered bidirectional RNN\n5.4\nNamed entity recognition\nSequential labeling can be applied to many information extraction tasks, not just to\npart-of-speech tagging. In this section, I’ll introduce the task of named entity recogni-\ntion (NER) and demonstrate how to build an NER tagger using sequential labeling.\nThe code for this section can be viewed and executed via the Google Colab platform\n(http://realworldnlpbook.com/ch5#ner-nb).\ninit_state()\ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nstate\nstate\nstate\nstate\nForward\nlayer\n \n \ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\n \nBackward\nlayer\n \nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n \n \ninit_state()\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\nForward\nlayer\n \nBackward\nlayer\n \nLayer 1\nLayer 2\nlogits\nLinear\nlayer\n \nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\".\")\n…\nNOUN VERB …\n",
      "content_length": 992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "127\nNamed entity recognition\n5.4.1\nWhat is named entity recognition?\nAs mentioned earlier, named entities are mentions of real-world entities such as\nproper nouns. Common named entities that are usually covered by NER systems\ninclude the following:\nPersonal name (PER): Alan Turing, Lady Gaga, Elon Musk\nOrganization (ORG): Google, United Nations, Giants\nLocation (LOC): Mount Rainer, Bali Island, Nile\nGeopolitical entity (GPE): UK, San Francisco, Southeast Asia\nHowever, different NER systems deal with different sets of named entities. The con-\ncept of named entities is a bit overloaded in NLP to mean any mentions that are of\ninterest to the application’s user. For example, in the medical domain, you may want\nto extract mentions to names of drugs and chemical compounds. In the financial\ndomain, companies, products, and stock symbols may be of interest. In many\ndomains, numerical and temporal expressions are also considered.\n Identifying named entities is in itself important, because named entities (who,\nwhat, where, when, and so on) are often what most people are interested in. But NER\nis also an important first step for many other NLP applications. One such task is rela-\ntion extraction: extracting all relations between named entities from the given docu-\nment. For example, given a press release document, you may want to extract an event\ndescribed in the release, such as which company purchased which other company for\nwhat price. This often assumes that all the parties are already identified via NER.\nAnother task that is closely related to NER is entity linking, where mentions of named\nentities are linked to some knowledge base, such as Wikipedia. When Wikipedia is\nused as a knowledge base, entity linking is also called Wikification.\n But you may be wondering, what’s so difficult about simply extracting named enti-\nties? If they are just proper nouns, can you simply compile a dictionary of, say, all the\ncelebrities (or all the countries, or whatever you are interested in) and use it? The\nidea is, whenever the system encounters a noun, it would run the name through this\ndictionary and tag the mention if it appears in it. Such dictionaries are called gazetteers,\nand many NER systems do use them as a component. \n However, relying solely on such dictionaries has one major issue—ambiguity. Ear-\nlier we saw that a single word type could have multiple parts of speech (e.g., “book” as\na noun and a verb), and named entities are no exception. For example, “Georgia” can\nbe the name of a country, a US state, towns and communities across the United States\n(Georgia, Indiana; Georgia, Nebraska), a film, a number of songs, ships, and a per-\nsonal name. Simple words like “book” could also be named entities, including: Book\n(a community in Louisiana), Book/Books (a surname), The Books (an American\nband), and so on. Simply matching mentions against dictionaries would tell you noth-\ning about their identities if they are ambiguous. \n Fortunately, sentences often offer clues that can be used to disambiguate the men-\ntions. For example, if the sentence reads “I live in Georgia,” it’s usually a strong signal\nthat “Georgia” is a name of a place, not a film or a person’s name. NER systems use a\n",
      "content_length": 3231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "128\nCHAPTER 5\nSequential labeling and language modeling\ncombination of signals about the mentions themselves (e.g., whether they are in a pre-\ndefined dictionary) and about their context (whether they are preceded or followed\nby certain words) to determine their tags. \n5.4.2\nTagging spans\nUnlike POS tagging, where each word is assigned a POS tag, mentions to named enti-\nties can span over more than one word, for example, “the United States” and “World\nTrade Organization.” A span in NLP is simply a range over one or more contiguous\nwords. How can we use the same sequential tagging framework to model spans?\n A common practice in NLP is to use some form of encoding to convert spans into\nper-word tags. The most common encoding scheme used in NER is called IOB2 tag-\nging. It represents spans by a combination of the positional tag and the category tag.\nThree types of positional tags follow:\nB (Beginning): assigned to the first (or the only) token of a span\nI (Inside): assigned to all but the first token of a span\nO (Outside): assigned to all words outside of any spans\nNow, let’s take a look at the NER example we saw earlier and is shown in figure 5.11.\nThe token “Apple” is the first (and the only) token of ORG (for “organization”), and it\nis assigned a B-ORG tag. Similarly, “UK”, the first and the only token of GPE (for “geo-\npolitical entity”), is assigned B-GPE. For “$1” and “billion,” the first and the second\ntokens of a monetary expression (MONEY), B-MONEY and I-MONEY are assigned,\nrespectively. All the other tokens are given O. \nFigure 5.11\nNamed entity recognition (NER) using sequential labeling\nThe rest of the pipeline for solving NER is very similar to that of part-of-speech tag-\nging: both are concerned with assigning an appropriate tag for each word and can be\nsolved by RNNs. In the next section, we are going to build a simple NER system using\nneural networks. \n5.4.3\nImplementing a named entity recognizer\nTo build an NER system, we use the Annotated Corpus for Named Entity Recognition\nprepared by Abhinav Walia published on Kaggle (http://realworldnlpbook.com/\nNamed entity recognition (NER)\nApple\nis\nlooking\nto\nbuy\nUK\nstartup for\n$1\nbillion\nB-ORG\nO\nO\nO\nO\nB-GPE\nO\nO\nB-MONEYI-MONEY\nInput sentence\nNER tags\n",
      "content_length": 2247,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "129\nNamed entity recognition\nch5.html#ner-data). In what follows, I’m going to assume that you downloaded and\nexpanded the dataset under data/entity-annotated-corpus. Alternatively, you\ncan use the copy of the dataset I uploaded to S3 (http://realworldnlpbook.com/\nch5.html#ner-data-s3), which is what the following code does. I wrote a dataset reader\nfor this dataset (http://realworldnlpbook.com/ch5.html#ner-reader), so you can sim-\nply import (or copy and paste) it and use it:\nreader = NERDatasetReader('https:/./s3.amazonaws.com/realworldnlpbook/'\n                          'data/entity-annotated-corpus/ner_dataset.csv')\nBecause the dataset is not separated into train, validation, and test sets, the dataset\nreader will separate it into train and validation splits for you. All you need to do is\nspecify which split you want when you initialize data loaders, as shown here:\nsampler = BucketBatchSampler(batch_size=16, sorting_keys=[\"tokens\"])\ntrain_data_loader = MultiProcessDataLoader(\n    reader, 'train', batch_sampler=sampler)\ndev_data_loader = MultiProcessDataLoader(\n    reader, 'dev', batch_sampler=sampler)\nThe RNN-based sequential tagging model and the rest of the training pipeline look\nalmost the same as the previous example (POS tagger). The only difference is how we\nevaluate our NER model. Because most of the tags for a typical NER dataset are simply\n“O,” using tag accuracy is misleading—a stupid system that tags everything “O” would\nachieve very high accuracy. Instead, NER is usually evaluated as an information\nextraction task, where the goal is to extract named entities from texts, not just to tag\nthem. We’d like to evaluate NER systems based on the “cleanness” of retrieved named\nentities (how many of them are actual entities) and their “completeness” (how many of\nactual entities the system was able to retrieve). Does any of this sound familiar to you?\nYes, these are the definition of recall and precision we talked about in section 4.3.\nBecause there are usually multiple types of named entities, these metrics (precision,\nrecall, and F1-measure) are computed per entity type. \nNOTE\nIf these metrics are computed while ignoring entity types, it’s called a\nmicro average. For example, the micro-averaged precision is the total number\nof true positives of all types divided by the total number of retrieved named\nentities regardless of the type. On the other hand, if these metrics are com-\nputed per entity type and are then averaged, it’s called a macro average. For\nexample, if the precision for PER and GPE is 80% and 90%, respectively, its\nmacro average is 85%. What AllenNLP computes in the following is the micro\naverage.\nAllenNLP implements SpanBasedF1Measure, which computes per-type metrics\n(precision, recall, and F1-measure) as well as the average. You can define the metric in\n__init__() of your model as follows:\nself.f1 = SpanBasedF1Measure(vocab, tag_namespace='labels')\n",
      "content_length": 2923,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "130\nCHAPTER 5\nSequential labeling and language modeling\nAnd use it to get metrics during training and validation, as shown next:\ndef get_metrics(self, reset: bool = False) -> Dict[str, float]:\n    f1_metrics = self.f1.get_metric(reset)\n    return {'accuracy': self.accuracy.get_metric(reset),\n            'prec': f1_metrics['precision-overall'],\n            'rec': f1_metrics['recall-overall'],\n            'f1': f1_metrics['f1-measure-overall']}\nIf you run this training pipeline, you get an accuracy around 0.97, and precision,\nrecall, F1-measure will all hover around 0.83. You can also use the predict() method\nto obtain named entity tags for an unseen sentence as \ntokens = ['Apple', 'is', 'looking', 'to', 'buy', 'UK', 'startup',\n          'for', '$1', 'billion', '.']\nlabels = predict(tokens, model)\nprint(' '.join('{}/{}'.format(token, label)\n               for token, label in zip(tokens, labels)))\nwhich produces the following:\nApple/B-org is/O looking/O to/O buy/O UK/O startup/O for/O $1/O billion/O ./O\nThis is not perfect—the NER tagger got the first named entity (“Apple”) correct but\nmissed two others (“UK” and “$1 billion”). If you look at the training data, the men-\ntion “UK” never appears, and no monetary values are tagged. It is not surprising that\nthe system is struggling to tag entities that it has never seen before. In NLP (and also\nmachine learning in general), the characteristic of the test instances needs to match\nthat of the train data for the model to be fully effective. \n5.5\nModeling a language\nIn this section, I’ll switch gears a little bit and introduce language models, which is one\nof the most important concepts in NLP. We’ll discuss what they are, why they are\nimportant, and how to train them using the neural network components we’ve intro-\nduced so far.\n5.5.1\nWhat is a language model?\nImagine you are asked to predict what word comes next given a partial sentence: “My\ntrip to the beach was ruined by bad ___.” What words could come next? Many things\ncould ruin a trip to a beach, but most likely it’s bad weather. Maybe it’s bad-mannered\npeople at the beach, or maybe it’s bad food that the person had eaten before the trip,\nbut most would agree that “weather” is a likely word that comes after this partial sen-\ntence. Few other nouns (people, food, dogs) and words of other parts of speech (be, the,\nrun, green) are as appropriate as “weather” in this context.\n What you just did is to assign some belief (or probability) to an English sentence. You\njust compared several alternatives and judged how likely they are as English sentences.\nMost people would agree that the probability of “My trip to the beach was ruined by bad\nweather” is a lot higher than “My trip to the beach was ruined by bad dogs.”\n",
      "content_length": 2754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "131\nModeling a language\n Formally, a language model is a statistical model that gives a probability to a piece of\ntext. An English language model would assign higher probabilities to sentences that\nlook like English. For example, an English language model would give a higher prob-\nability to “My trip to the beach was ruined by bad weather” than it does to “My trip to\nthe beach was ruined by bad dogs” or even “by weather was trip my bad beach the\nruined to.” The more grammatical and the more “sense” the sentence makes, the\nhigher the probability is.\n5.5.2\nWhy are language models useful?\nYou may be wondering what use such a statistical model has. Although predicting the\nnext word might come in handy when you are answering fill-in-the-blank questions for\nan exam, what particular roles do language models play in NLP?\n The answer is, it is essential for any systems that generate natural language. For\nexample, machine translation systems, which generate a sentence in a language given\na sentence in another language, would benefit greatly from high-quality language mod-\nels. Why? Let’s say we’d like to translate a Spanish sentence “Está lloviendo fuerte” into\nEnglish (“It is raining hard”). The last word “fuerte” has several English equivalents—\nstrong, sharp, loud, heavy, and so on. How would you determine which English equivalent\nis the most appropriate in this context? There could be many approaches to solve this\nproblem, but one of the simplest is to use an English language model and rerank sev-\neral different translation candidates. Assuming you’ve finished translating up to “It is\nraining,” you would simply replace the word “fuerte” with all the equivalents you can\nfind in a Spanish–English dictionary, which generates “It is raining strong,” “It is raining\nsharp,” “It is raining loud,” “It is raining hard.” Then all you need to do is ask the lan-\nguage model which one of these candidates has the highest probability.\nNOTE\nIn fact, neural machine translation models can be thought of as a vari-\nation of a language model that generates sentences in the target language\nconditioned on its input (sentences in the source language). Such a language\nmodel is a called a conditional language model as opposed to an unconditional\nlanguage model, which we discuss here. We’ll discuss machine translation mod-\nels in chapter 6.\nA similar situation arises in speech recognition, too, which is another task that gener-\nates text given spoken audio input. For example, if somebody uttered “You’re right,”\nhow would a speech recognition system know it’s actually “you’re right?” Because\n“you’re” and “your” can have the same pronunciation, and so can “right” and “write”\nand even “Wright” and “rite,” the system output could be any one of “You’re write,”\n“You’re Wright,” “You’re rite,” “Your right,” “Your write,” “Your Wright,” and so on.\nAgain, the simplest approach to resolving this ambiguity is to use a language model.\nAn English language model would properly rerank these candidates and determine\n“you’re right” is the most likely transcription.\n In fact, humans do this type of disambiguation all the time, though unconsciously.\nWhen you are having a conversation with somebody else at a large party, the actual\n",
      "content_length": 3239,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "132\nCHAPTER 5\nSequential labeling and language modeling\naudio signal you receive is often very noisy. Most people can still understand each\nother without any issues because people’s language models help them “correct” what\nyou hear and interpolate any missing parts. You’ll notice this most if you try to con-\nverse in a less proficient, second language—you’d have a lot harder time understand-\ning the other person in a noisy environment, because your language model is not as\ngood as your first language’s.\n5.5.3\nTraining an RNN language model\nAt this point, you may be wondering what the connection is between predicting the\nnext word and assigning a probability to a sentence. These two are actually equivalent.\nInstead of explaining the theory behind it, which requires you to understand some\nmath (especially probability theory), I’ll attempt an intuitive example next without\ngoing into mathematical details.\n Imagine you want to estimate the chance of tomorrow’s weather being rainy and\nthe ground wet. Let’s simplify this and assume there are only two types of weather,\nsunny and rainy. There are only two outcomes for the ground: dry or wet. This is\nequivalent to estimating the probably of a sequence: [rain, wet].\n Further assume that there’s a 50-50 chance of rain on a given day. After raining,\nthe ground is wet with a 90% chance. Then, what is the probability of the rain and the\nground being wet? It’s simply 50% times 90%, which is 45%, or 0.45. If we know the\nprobability of one event happening after another, you can simply multiply two proba-\nbilities to get the total probability for the sequence. This is called the chain rule in\nprobability theory.\n Similarly, if you can correctly estimate the probability of one word occurring after a\npartial sentence, you can simply multiply it with the probability of the partial sen-\ntence. Starting from the first word, you can keep doing this until you reach the end of\nthe sentence. For example, if you’d like to compute the probability for “The trip to\nthe beach was . . . ,” you can multiply the following:\nThe probability of “The” occurring at the beginning of a sentence\nThe probability of “trip” occurring after “The”\nThe probability of “to” occurring after “The trip”\nThe probability of “the” occurring after “The trip to”\nAnd so on\nThis means that to build a language model, you need a model that predicts the proba-\nbility (or, more precisely, the probability distribution) of the next word given the con-\ntext. You may have noticed that this sounds a little familiar. Indeed, what’s done here is\nvery similar to the sequential-labeling models that we’ve been talking about in this\nchapter. For example, a part-of-speech (POS) tagging model predicts the probability\ndistribution over the possible POS tags given the context. A named entity recognition\n(NER) model does it for the possible named entity tags. The difference is that a lan-\nguage model does it for the possible next words, given what the model has encountered\nso far. Hopefully it’s starting to make some sense why I talk about language models in\nthis chapter!\n",
      "content_length": 3104,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "133\nText generation using RNNs\n In summary, to build a language model, you tweak an RNN-based sequence-labeling\nmodel a little bit so that it gives the estimates for the next word, instead of POS or NER\ntags. In chapter 3, I talked about the Skip-gram model, which predicts the words in a\ncontext given the target word. Notice the similarity here—both models predict the\nprobability over possible words. The input to the Skip-gram model is just a single word,\nwhereas the input to the language model is the partial sequence. You can use a similar\nmechanism for converting one vector to another using a linear layer, then converting\nit to a probability distribution using softmax, as we discussed in chapter 3. The archi-\ntecture is shown in figure 5.12.\nFigure 5.12\nArchitecture of RNN-based language model\nThe way RNN-based language models are trained is similar to other sequential-labeling\nmodels. The loss function we use is the sequential cross-entropy loss, which measures\nhow “off” the predicted words are from actual words. The cross-entropy loss is com-\nputed per word and averaged over all words in the sentence.\n5.6\nText generation using RNNs\nWe saw that language models give probabilities to natural language sentences. But the\nmore fun part is you can generate natural language sentences from scratch using a\nlanguage model! In the final section of this chapter, we are going to build a language\nmodel. You can use the trained model to evaluate and generate English sentences.\n<START>\nThe\nWord\nembeddings\n \ntrip\nweather\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\n<END>\n.\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nSoftmax\ntrip\ndog\nway\nkey\n…\n…\nProbabilities\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nThe\nto\n",
      "content_length": 1749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "134\nCHAPTER 5\nSequential labeling and language modeling\nYou can find the entire script for this subsection on a Google Colab notebook (http://\nrealworldnlpbook.com/ch5.html#lm-nb).\n5.6.1\nFeeding characters to an RNN\nIn the first half of this section, we are going to build an English language model and\ntrain it using a generic English corpus. Before we start, we note that the RNN lan-\nguage model we build in this chapter operates on characters, not on words or tokens.\nAll the RNN models we’ve seen so far operate on words, which means the input to the\nRNN was always sequences of words. On the other hand, the RNN we are going to use\nin this section takes sequences of characters as the input. \n In theory, RNNs can operate on sequences of anything, be it tokens or characters\nor something completely different (e.g., waveform for speech recognition), as long as\nthey are something that can be turned into vectors. In building language models,\nwe often feed characters, even including whitespace and punctuations as the input,\ntreating them as words of length 1. The rest of the model works exactly the same—\nindividual characters are first embedded (converted to vectors) and then fed into the\nRNN, which is in turn trained so that it can best predict the distribution over the char-\nacters that are likely to come next. \n You have a couple of considerations when you are deciding whether you should\nfeed words or characters to an RNN. Using characters will definitely make the RNN\nless efficient, meaning that it would need more computation to “figure out” the same\nconcept. For example, a word-based RNN can receive the word “dog” at a timestep\nand update its internal states, whereas a character-based RNN would not able to do it\nuntil it receives three elements d, o, and g, and probably “_” (whitespace). A character-\nbased RNN needs to “learn” that a sequence of these three characters means some-\nthing special (the concept of “dog”).\n On the other hand, by feeding characters to RNNs, you can bypass many issues\narising from dealing with tokens. One such issue is related to out-of-vocabulary (or\nOOV) words. When training a word-based RNN, you usually fix the entire set of vocab-\nulary, often by enumerating all words that appeared in the train set. But whenever it\nencounters an OOV word in the test set, it doesn’t know what to do with it. Often-\ntimes, it assigns a special token <UNK> to all OOV words and treats them in the same\nway, which is not ideal. A character-based RNN, on the contrary, can still operate on\nindividual characters, so it may be able to figure out what “doggy” means, for example,\nbased on the rules it has learned by observing “dog” in the train set, even though it\nhas never seen the exact word “doggy” before.\n5.6.2\nEvaluating text using a language model\nLet’s start building a character-based language model. The first step is to read a plain\ntext dataset file and generate instances for training the model. I’m going to show how\nto construct an instance without using a dataset reader for a demonstration purpose.\nSuppose you have a Python string object text that you’d like to turn into an instance\nfor training a language model. First you need to segment it into characters using\nCharacterTokenizer as follows:\n",
      "content_length": 3260,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "135\nText generation using RNNs\nfrom allennlp.data.tokenizers import CharacterTokenizer\ntokenizer = CharacterTokenizer()\ntokens = tokenizer.tokenize(text)\nNote that tokens here is a list of Token objects. Each Token object contains a single\ncharacter, instead of a single word. Then you insert the <START> and <END> symbols\nat the beginning and at the end of the list as shown next:\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL\ntokens.insert(0, Token(START_SYMBOL))\ntokens.append(Token(END_SYMBOL))\nInserting special symbols like these at the beginning and end of each sentence is a\ncommon practice in NLP. With these symbols, models can distinguish between occur-\nrences of a token in the middle of a sentence versus at the beginning/end of a sentence.\nFor example, a period is a lot more likely to occur at the end of a sentence (“. <END>”)\nthan the beginning (“<START> .”), to which a language model can give two very differ-\nent probabilities, which is impossible to do without the use of these symbols.\n Finally, you can construct an instance by specifying individual text fields. Notice\nthat the “output” of a language model is identical to the input, simply shifted by one\ntoken, as shown here:\nfrom allennlp.data.fields import TextField\nfrom allennlp.data.instance import Instance\ninput_field = TextField(tokens[:-1], token_indexers)\noutput_field = TextField(tokens[1:], token_indexers)\ninstance = Instance({'input_tokens': input_field,\n                     'output_tokens': output_field})\nHere token_indexers specifies how individual tokens are mapped into IDs. We sim-\nply use SingleIdTokenIndexer we’ve been using so far as follows:\nfrom allennlp.data.token_indexers import TokenIndexer \ntoken_indexers = {'tokens': SingleIdTokenIndexer()}\nFigure 5.13 shows an instance created from this process. \nFigure 5.13\nInstance for training a language model\ninstance\nT\nh\ne\n<ST>\n<ED>\n_\nq\nu\nT\nh\ne\n_\nq\nu\ni\n.\ng\n.\ng\no\ninput_tokens\noutput_tokens\n…\n",
      "content_length": 1956,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "136\nCHAPTER 5\nSequential labeling and language modeling\nThe rest of the training pipeline, as well as the model, is very similar to that for sequen-\ntial labeling mentioned earlier in this chapter. See the Colab notebook for more details.\nAs shown in the next code snippet, after the model is fully trained, you can construct\ninstances from new texts, turn them into instances, and compute the loss, which basi-\ncally measures how successful the model was in predicting what comes next:\npredict('The trip to the beach was ruined by bad weather.', model)\n{'loss': 1.3882852}\npredict('The trip to the beach was ruined by bad dogs.', model)\n{'loss': 1.5099115}\npredict('by weather was trip my bad beach the ruined to.', model)\n{'loss': 1.8084583}\nThe loss here is the cross-entropy loss between the predicted and the expected charac-\nters. The more “unexpected” the characters there are, the higher the values will be, so\nyou can use these values to measure how natural the input is as English text. As\nexpected, natural sentences (such as the first one) are given scores that are lower than\nunnatural sentences (such as the last one).\nNOTE\nIf you calculate 2 to the power of the cross entropy, the value is called\nperplexity. Given a fixed natural language text, perplexity becomes lower\nbecause the language model is better at predicting what comes next, so it is\ncommonly used for evaluating the quality of language models in the literature.\n5.6.3\nGenerating text using a language model\nThe most interesting aspect of (fully trained) language models is that they can predict\npossible characters that may appear next given some context. Specifically, they can give\nyou a probability distribution over possible characters that may come next, from which\nyou choose to determine the next character. For example, if the model has generated\n“t” and “h,” and the LM is trained on generic English text, it would probably assign a\nhigh probability on the letter “e,” generating common English words including the,\nthey, them, and so on. If you start this process from the <START> token and keep doing\nthis until you reach the end of the sentence (i.e., by generating <END>), you can gen-\nerate an English sentence from scratch. By the way, this is another reason why tokens\nsuch as <START> and <END> are useful—you need something to feed to the RNN to\nkick off the generation, and you also need to know when the sentence stops.\n Let’s look at this process in a Python-like pseudocode next:\ndef generate():\n    state = init_state()\n    token = <START>\n    tokens = [<START>]\n    while token != <END>:\n        state = update(state, token)\n        probs = softmax(linear(state))\n        token = sample(probs)\n",
      "content_length": 2697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "137\nText generation using RNNs\n        tokens.append(token)\n    return tokens\nThis loop looks very similar to the one for updating RNNs with one key difference:\nhere, we are not receiving any input but instead are generating characters and feed-\ning them as the input. In other words, the RNN operates on the sequence of charac-\nters that the RNN itself generated so far. Such models that operate on past sequences\nthey produced are called autoregressive models. See figure 5.14 for an illustration of this.\nFigure 5.14\nGenerating text using an RNN\nIn the previous code snippet, init_state() and update() functions are the ones\nthat initialize and update the hidden states of the RNN, as we’ve seen earlier. In gener-\nating text, we assume that the model and its parameters are already trained on a large\namount of natural language text. softmax() is a function to run Softmax on the\ngiven vector, and linear() is the linear layer to expand/shrink the size of the vector.\nThe function sample() returns a character according to the given probability distri-\nbution. For example, if the distribution is “a”: 0.6, “b”: 0.3, “c”: 0.1, it will choose “a”\n60% of the time, “b” 30% of the time, and “c” 10% of the time. This ensures that the\ngenerated string is different every time while every string is likely to look like a real\nEnglish sentence. \nNOTE\nYou can use PyTorch’s torch.multinomial() for sampling an ele-\nment from a probability distribution.\nIf you train this language model using the English sentences from Tatoeba and gener-\nate sentences according to this algorithm, the system will produce something similar\nto the following cherry-picked examples:\nYou can say that you don't know it, and why decided of yourself.\nPike of your value is to talk of hubies.\n<START>\nT\nT\nCharacter\nembeddings\n \nh\ng\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\nh\ne\n.\n<END>\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nT h e _ q u i c k _ b r … _ d o g .\n",
      "content_length": 1980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "138\nCHAPTER 5\nSequential labeling and language modeling\nThe meeting despoit from a police?\nThat's a problem, but us?\nThe sky as going to send nire into better.\nWe'll be look of the best ever studented.\nThere's you seen anything every's redusention day.\nHow a fail is to go there.\nIt sad not distaples with money.\nWhat you see him go as famous to eat!\nThis is not a bad start! If you look at these sentences, there are many words and\nphrases that make sense as valid English (You can say that, That’s a problem, to go there, see\nhim go, etc.). Even when the system generates peculiar words (despoit, studented, redusen-\ntion, distaples), they look almost like real English words because they all basically follow\nmorphological and phonological rules of English. This means that the language\nmodel was successful in learning the basic building blocks of English, such as how to\narrange letters (orthography), how to form words (morphology), and how to form\nbasic sentence structures (syntax).\n However, if you look at sentences as a whole, few of them make any sense (e.g.,\nWhat you see him go as famous to eat!). This means the language model we trained falls\nshort of modeling semantic consistency of sentences. This is potentially because our\nmodel is not powerful enough (our LSTM-RNN needs to compress everything about\nthe sentence into a 256-dimensional vector) or the training dataset is too small (just\n10,000 sentences), or both. But you can easily imagine that if we keep increasing the\nmodel capacity as well as the size of the train set, the model gets incredibly good at\nproducing realistic natural language text. In February 2019, OpenAI announced that\nit developed a huge language model based on the Transformer model (which we’ll\ncover in chapter 8) trained on 40 GB of internet text. The model shows that it can\nproduce realistic-looking text that shows near-perfect grammar and long-term topical\nconsistency given a prompt. In fact, the model was so good that OpenAI decided not\nto release the large model they had trained due to their concerns about malicious use\nof the technology. But it is important to keep in mind that, no matter how intelligent\nthe output looks, their model is trained on the same principle as our toy example in\nthis chapter—just trying to predict the next character!\nSummary\nSequential-labeling models tag each word in the input with a label, which can\nbe achieved by recurrent neural networks (RNNs).\nPart-of-speech (POS) tagging and named entity recognition (NER) are two\ninstances of sequential-labeling tasks.\nMultilayer RNNs stack multiple layers of RNNs, whereas bidirectional RNNs\ncombine forward and backward RNNs to encode the entire sentence.\nLanguage models assign probabilities to natural language text, which is\nachieved by predicting the next word.\nYou can use a trained language model to assess how “natural” a natural lan-\nguage sentence is or even to generate realistic-looking text from scratch.\n",
      "content_length": 2961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "Part 2\nAdvanced models\nThe field of NLP has seen rapid progress in the past few years. Specifically,\nthe advent of the Transformer and pretrained language models such as BERT\nhave completely changed the landscape of the field and how practitioners build\nNLP applications. This part of the book will help you catch up with these latest\ndevelopments.\n Chapter 6 introduces sequence-to-sequence models, an important class of\nmodels that will enable you to build more complex applications such as machine\ntranslation systems and chatbots. Chapter 7 discusses another type of popular\nneural network architecture, convolutional neural networks (CNNs).\n Chapters 8 and 9 are arguably the most important and exciting chapters of this\nbook. They cover the Transformer and transfer learning methods (such as BERT)\nrespectively. We’ll demonstrate how to build advanced NLP applications such as\nhigh-quality machine translation and spell-checkers, using those technologies.\n By the time you finish reading this part, you’ll feel confident that you can\nnow solve a wide range of NLP tasks with what you have learned so far.\n \n \n",
      "content_length": 1115,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "140\nCHAPTER \n",
      "content_length": 13,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "141\nSequence-to-sequence models\nIn this chapter, we are going to discuss sequence-to-sequence (Seq2Seq) models,\nwhich are some of the most important complex NLP models and are used for a wide\nrange of applications, including machine translation. Seq2Seq models and their\nvariations are already used as the fundamental building blocks in many real-world\napplications, including Google Translate and speech recognition. We are going to\nbuild a simple neural machine translation system using a powerful framework to\nThis chapter covers\nBuilding a machine translation system using Fairseq\nTransforming one sentence to another using a \nSeq2Seq model\nUsing a beam search decoder to generate better \noutput\nEvaluating the quality of machine translation \nsystems\nBuilding a dialogue system (chatbot) using a \nSeq2Seq model\n",
      "content_length": 820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "142\nCHAPTER 6\nSequence-to-sequence models\nlearn how the models work and how to generate the output using greedy and beam\nsearch algorithms. At the end of this chapter, we will build a chatbot—an NLP applica-\ntion with which you can have a conversation. We’ll also discuss the challenges and lim-\nitations of simple Seq2Seq models.\n6.1\nIntroducing sequence-to-sequence models\nIn the previous chapter, we discussed two types of powerful NLP models, namely,\nsequential labeling and language models. To recap, a sequence-labeling model takes a\nsequence of some units (e.g., words) and assigns a label (e.g., a part-of-speech (POS)\ntag) to each unit, whereas a language model takes a sequence of some units (e.g.,\nwords) and estimates how probable the given sequence is in the domain in which the\nmodel is trained. You can also use a language model to generate realistic-looking text\nfrom scratch. See figure 6.1 for the overview of these two models.\nFigure 6.1\nSequential labeling and language models\nAlthough these two models are useful for a number of NLP tasks, for some, you may\nwant the best of both worlds—you may want your model to take some input (e.g., a\nsentence) and generate something else (e.g., another sentence) in response. For\nexample, if you wish to translate some text written in one language into another, you\nneed your model to take a sentence and produce another. Can you do this with\nsequential-labeling models? No, because they can produce only the same number of\noutput labels as there are tokens in the input sentence. This is obviously too limiting\nfor translation—one expression in a language (say, “Enchanté” in French) can have\nan arbitrarily large or small number of words in another (say, “Nice to meet you” in\nEnglish). Can you do this with language models? Again, not really. Although you can\ngenerate realistic-looking text using language models, you have almost no control\nover the text they generate. In fact, language models do not take any input.\n But if you look at figure 6.1 more carefully, you might notice something. The\nmodel on the left (the sequential-labeling model) takes a sentence as its input and\nproduces some form of representations, whereas the model on the right produces a\ntime\nflies\nlike\narrow\n.\nNOUN\nVERB\n...\nADP\nNOUN\nPUNKT\n<START>\nThe\nquick\ndog\n.\nThe\nquick\n...\nbrown\n.\n<END>\nSequential labeling\nLanguage model\n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "143\nIntroducing sequence-to-sequence models\nsentence with variable length that looks like natural language text. We already have\nthe components needed to build what we want, that is, a model that takes a sentence\nand transforms it into another. The only missing part is a way to connect these two so\nthat we can control what the language model generates.\n In fact, by the time the model on the left finishes processing the input sentence,\nthe RNN has already produced its abstract representation, which is encoded in the\nRNN’s hidden states. If you can simply connect these two so that the sentence repre-\nsentation is passed from left to right and the language model can generate another\nsentence based on the representation, it seems like you can achieve what you wanted\nto do in the first place!\n Sequence-to-sequence models—or Seq2Seq models, in short—are built on this\ninsight. A Seq2Seq model consists of two subcomponents—an encoder and a decoder.\nSee figure 6.2 for an illustration. An encoder takes a sequence of some units (e.g., a\nsentence) and converts it into some internal representation. A decoder, on the other\nhand, generates a sequence of some units (e.g., a sentence) from the internal repre-\nsentation. As a whole, a Seq2Seq model takes a sequence and generates another\nsequence. As with the language model, the generation stops when the decoder pro-\nduces a special token, <END>, which enables a Seq2Seq model to generate an output\nthat can be longer or shorter than the input sequence.\nFigure 6.2\nSequence-to-sequence model\nMany variants of Seq2Seq models exist, depending on what architecture you use for\nthe encoder, what architecture you use for the decoder, and how information flows\nbetween the two. This chapter covers the most basic type of Seq2Seq model—simply\nconnecting two RNNs via the sentence representation. We’ll discuss more advanced\nvariants in chapter 8.\n Machine translation is the first, and by far the most popular, application of\nSeq2Seq models. However, the Seq2Seq architecture is a generic model applicable to\nnumerous NLP tasks. In one such task, summarization, an NLP system takes a long\ntext (e.g., a news article) and produces its summary (e.g., a news headline). A Seq2Seq\nMaria\nno\nbaba\nverde\n.\n...\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nEncoder\nDecoder\nSentence\nrepresentation\n \n",
      "content_length": 2343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "144\nCHAPTER 6\nSequence-to-sequence models\nmodel can be used to “translate” the longer text into the shorter one. Another task is\na dialogue system, or a chatbot. If you think of a user’s utterance as the input and the\nsystem’s response as the output, the dialogue system’s job is to “translate” the former\ninto the latter. Later in this chapter, we will discuss a case study where we actually build\na chatbot using a Seq2Seq model. Yet another (somewhat surprising) application is\nparsing—if you think of the input text as one language and its syntax representation\nas another, you can parse natural language texts with a Seq2Seq model.1\n6.2\nMachine translation 101\nWe briefly touched upon machine translation in section 1.2.1. To recap, machine\ntranslation (MT) systems are NLP systems that translate a given text from one lan-\nguage to another. The language the input text is written in is called the source language,\nwhereas the one for the output is called the target language. The combination of the\nsource and target languages is called the language pair. \n First, let’s look at a couple of examples to see what it’s like and why it’s difficult to\ntranslate a foreign language to English (or any other language you understand). In\nthe first example, let’s translate a Spanish sentence, “Maria no daba una bofetada a la\nbruja verde.” to the English counterpart, “Mary did not slap the green witch.” A com-\nmon practice in illustrating the\nprocess of translation is to draw\nhow words or phrases of the same\nmeaning map between the two sen-\ntences. Correspondence of linguis-\ntic units between two instances is\ncalled alignment. Figure 6.3 shows\nthe alignment between the Spanish\nand English sentences.\n Some words (e.g., “Maria” and “Mary,” “bruja” and “witch,” and “verde” and\n“green”) match exactly one to one. However, some expressions (e.g., “daba una bofe-\ntada” and “slap”) differ in such a significant way that you can only align phrases\nbetween Spanish and English. Finally, even where there’s one-to-one correspondence\nbetween words, the way words are arranged, or word order, may differ between the two\nlanguages. For example, adjectives are added after nouns in Spanish (“la bruja\nverde”) whereas in English, they come before nouns (“the green witch”). Spanish and\nEnglish are linguistically similar in terms of grammar and vocabulary, especially when\ncompared to, say, Chinese and English, although this single example shows translating\nbetween the two may be a challenging task.\n1 See Oriol Vinyals et al., “Grammar as a Foreign Language,” (2015; https://arxiv.org/abs/1412.7449) for more\ndetails.\nMaria\nMary\ndid not\nslap\nthe\ngreen\nwitch\nno\ndaba una bofetada\na la\nbruja\nverde\nFigure 6.3\nTranslation and word alignment between \nSpanish and English\n",
      "content_length": 2764,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "145\nMachine translation 101\n Things start to look more complicated\nbetween Mandarin Chinese and English.\nFigure 6.4 illustrates the alignment between\na Chinese sentence (“Bushi yu Shalong jux-\ning le huitan.”) and its English translation\n(“Bush held a talk with Shalon.”). Although\nChinese uses ideographic characters of its\nown, we use romanized sentences here for\nsimplicity.\n You can now see more crossing arrows in the figure. Unlike English, Chinese prep-\nositional phrases such as “with Shalon” are usually attached to verbs from the left.\nAlso, the Chinese language doesn’t explicitly mark tense, and MT systems (and\nhuman translators alike) need to “guess” the correct tense to use for the English trans-\nlation. Finally, Chinese-to-English MT systems also need to infer the correct number\n(singular or plural) of each noun, because Chinese nouns are not explicitly marked\naccording to their number (e.g., “huitan” just means “talk” with no explicit mention\nof number). This is a good example showing how the difficulty of translation depends\non the language pair. Development of MT systems between linguistically different lan-\nguages (such as Chinese and English) is usually more challenging than those between\nlinguistically similar ones (such as Spanish and Portuguese).\n Let’s take a look at one more example—\ntranslating from Japanese to English, illus-\ntrated in figure 6.5. All the arrows in the fig-\nure are crossed, meaning that the word\norder is almost exactly opposite in these two\nsentences. In addition to the fact that Japa-\nnese prepositional phrases (“to music”) and\nrelative clauses attach from the left like Chi-\nnese, objects (such as “listening” in “I love\nlistening” in the example) come before the\nverb. In other words, Japanese is an SOV (subject-object-verb) language, whereas all\nthe other languages we mentioned so far (English, Spanish, and Chinese) are SVO\n(subject-verb-object) languages. Structural differences are a reason why direct, word-\nto-word translation doesn’t work very well.\nNOTE\nThis word-order classification system of language (such as SOV and\nSVO) is often used in linguistic typology. The vast majority of world lan-\nguages are either SOV (most common) or SVO (slightly less common),\nalthough a small number of languages follow other word-order systems,\nsuch as VSO (verb-subject-object), used by Arabic and Irish, for example.\nVery few languages (less than 3% of all languages) follow other types (VOS,\nOVS, and OSV).\nBushi\nyu\nShalong\njuxing le\nhuitan\nBush\nheld\na talk\nwith\nShalon\nFigure 6.4\nTranslation and word alignment \nbetween Mandarin Chinese and English\nOngaku\nI\nlove\nlistening\nto\nmusic\nwo\nkiku\nno ga\ndaisuki\ndesu\nFigure 6.5\nTranslation and word alignment \nbetween Japanese and English\n",
      "content_length": 2749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "146\nCHAPTER 6\nSequence-to-sequence models\nBesides the structural differences shown in the previous figures, many other factors can\nmake MT a difficult task. One such factor is lexical difference. If you are translating, for\nexample, the Japanese word “ongaku” to the English “music,” there’s little ambiguity.\n“Ongaku” is almost always “music.” However, if you are translating, say, the English word\n“brother” to Chinese, you face ambiguity, because Chinese uses distinct words for\n“elder brother” and “younger brother.” In an even more extreme case, if you are trans-\nlating “cousin” to Chinese, you have eight different choices, because in the Chinese\nfamily system, you need to use distinct words depending on whether your cousin is\nmaternal or paternal, female or male, and older or younger than you.\n Another factor that makes MT challenging is omission. You can see that in figure\n6.5, there’s no Japanese word for “I.” In languages such as Chinese, Japanese, Spanish,\nand many others, you can omit the subject pronoun when it’s clear from the context\nand/or the verb form. This is called zero pronoun, and it can become a problem when\ntranslating from a pronoun-dropping language to a language where it happens less\noften (e.g., English).\n One of the earliest MT systems, developed during the Georgetown-IBM experi-\nment, was built to translate Russian sentences into English during the Cold War. But\nall it did was not much different from looking up each word in a bilingual dictionary\nand replacing it with its translation. The three examples shown above should be\nenough to convince you that simply replacing word by word is too limiting. Later sys-\ntems incorporated a larger set of lexicons and grammar rules, but these rules are writ-\nten manually by linguists and are not enough to capture the complexities of language\n(again, remember the poor software engineer from chapter 1). \n The main paradigm for MT that remained dominant both in academia and indus-\ntry before the advent of neural machine translation (NMT) is called statistical machine\ntranslation (SMT). The idea behind it is simple: learn how to translate from data, not\nby manually crafting rules. Specifically, SMT systems learn how to translate from data-\nsets that contain a collection of texts in the source language and their translation in\nthe target language. Such datasets are called parallel corpora (or parallel texts, or bitexts).\nBy looking at a collection of paired sentences in both languages, the algorithm seeks\npatterns of how words in one language should be translated to another. The resulting\nstatistical model is called a translation model. At the same time, by looking at a collec-\ntion of target sentences, the algorithm can learn what valid sentences in the target lan-\nguages should look like. Sounds familiar? This is exactly what a language model is all\nabout (see the previous chapter). The final SMT model combines these two models\nand produces output that is a plausible translation of the input and is a valid, fluent\nsentence in the target language on its own.\n Around 2015, the advent of powerful neural machine translation (NMT) models\nsubverted the dominance of SMT. SMT and NMT have two key differences. First, by\ndefinition, NMT is based on neural networks, which are well known for their power to\nmodel language accurately. As a result, target sentences generated by NMT tend to be\nmore fluent and natural than those generated by SMT. Second, NMT models are\n",
      "content_length": 3473,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "147\nBuilding your first translator\ntrained end-to-end, as I briefly touched on in chapter 1. This means that NMT models\nconsist of a single neural network that takes an input and directly produces an output,\ninstead of a patchwork of submodels and submodules that you need to train inde-\npendently. As a result, NMT models are simpler to train and smaller in code size than\nSMT models.\n MT is already used in many different industries and aspects of our lives. Translat-\ning foreign text into a language that you understand to grasp its meaning quickly is\ncalled gisting. If the text is deemed important enough after gisting, it may be sent to\nformal, manual translation. Professional translators also use MT for their work. Often-\ntimes, the source text is first translated to the target language using an MT system,\nthen the produced text is edited by human translators. Such editing is called postedit-\ning. The use of automated systems (called computer-aided translation, or CAT) can accel-\nerate the translation process and reduce the cost. \n6.3\nBuilding your first translator\nIn this section, we are going to build a working MT system. Instead of writing any\nPython code to do that, we’ll make the most of existing MT frameworks. A number of\nopen source frameworks make it easier to build MT systems, including Moses (http://\nwww.statmt.org/moses/) for SMT and OpenNMT (http://opennmt.net/) for NMT.\nIn this section, we will use Fairseq (https://github.com/pytorch/fairseq), an NMT\ntoolkit developed by Facebook that is becoming more and more popular among NLP\npractitioners these days. The following aspects make Fairseq a good choice for devel-\noping an NMT system quickly: 1) it is a modern framework that comes with a number\nof predefined state-of-the-art NMT models that you can use out of the box; 2) it is very\nextensible, meaning you can quickly implement your own model by following their\nAPI; and 3) it is very fast, supporting multi-GPU and distributed training by default.\nThanks to its powerful models, you can build a decent quality NMT system within a\ncouple of hours.\n Before you start, install Fairseq by running pip install fairseq in the root of\nyour project directory. Also, run the following commands in your shell to download\nand expand the dataset (you may need to install unzip if you are using Ubuntu by\nrunning sudo apt-get install unzip):2\n$ mkdir -p data/mt\n$ wget https://realworldnlpbook.s3.amazonaws.com/data/mt/tatoeba.eng_spa.zip\n$ unzip tatoeba.eng_spa.zip -d data/mt\nWe are going to use Spanish and English parallel sentences from the Tatoeba project,\nwhich we used previously in chapter 4, to train a Spanish-to-English MT system. The\ncorpus consists of approximately 200,000 English sentences and their Spanish transla-\ntions. I went ahead and already formatted the dataset so that you can use it without\n2  Note that $ at the beginning of every line is rendered by the shell, and you don’t need to type it.\n",
      "content_length": 2951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "148\nCHAPTER 6\nSequence-to-sequence models\nworrying about obtaining the data, tokenizing the text, and so on. The dataset is\nalready split into train, validate, and test subsets.\n6.3.1\nPreparing the datasets\nAs mentioned previously, MT systems (both SMT and NMT) are machine learning\nmodels and thus are trained from data. The development process of MT systems looks\nsimilar to any other modern NLP systems, as shown in figure 6.6. First, the training\nportion of the parallel corpus is preprocessed and used to train a set of NMT model\ncandidates. Next, the validation portion is used to choose the best-performing model\nout of all the candidates. This process is called model selection (see chapter 2 for a\nreview). Finally, the best model is tested on the test portion of the dataset to obtain\nevaluation metrics, which reflect how good the model is.\nFigure 6.6\nPipeline for building an NMT system\nThe first step in MT development is preprocessing the dataset. But before preprocess-\ning, you need to convert the dataset into an easy-to-use format, which is usually plain\ntext in NLP. In practice, the raw data for training MT systems come in many different\nformats, for example, plain text files (if you are lucky), XML formats of proprietary\nsoftware, PDF files, and database records. Your first job is to format the raw files so\nthat source sentences and their target translations are aligned sentence by sentence.\nThe resulting file is often a TSV file where each line is a tab-separated sentence pair,\nwhich looks like the following:\nLet's try something.                   Permíteme intentarlo.\nMuiriel is 20 now.                     Ahora, Muiriel tiene 20 años.\nI just don't know what to say.         No sé qué decir.\nYou are in my way.                     Estás en mi camino.\nSometimes he can be a strange guy.     A veces él puede ser un chico raro.\n…\nTrain data\nSource\nTarget\nSource\nTarget\nValidation data\nNMT models\nTest data\nPreprocessing\nPreprocessing\nTraining\nSource\nTarget\nNMT model\nMetrics\nModel selection\nPreprocessing\nEvaluation\n",
      "content_length": 2048,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "149\nBuilding your first translator\nAfter the translations are aligned, the parallel corpus is fed into the preprocessing\npipeline. Specific operations applied in this process differ from application to applica-\ntion, and from language to language, but the following steps are most common:\n1\nFiltering\n2\nCleaning\n3\nTokenization\nIn the filtering step, any sentence pairs that are not suitable for training an MT system\nare removed from the dataset. What makes a sentence pair not suitable depends on\nmany factors, but, for example, any sentence pair where either text is too long (say,\nmore than 1,000 words) is not useful, because most MT models are not capable of\nmodeling such a long sentence. Also, any sentence pairs where one sentence is too\nlong but the other is too short are probably noise caused by a data processing or align-\nment error. For example, if a Spanish sentence is 10 words long, the length of its\nEnglish translation should fall within a 5- to 15-word range. Finally, if, for any reason,\nthe parallel corpus contains any languages other than the source and target lan-\nguages, you should remove such sentence pairs. This happens a lot more often than\nyou’d imagine—many documents are multilingual due to, for example, quotes, expla-\nnation, or code switching (mixing more than one language in a sentence). Language\ndetection (see chapter 4) can help detect such anomalies.\n After filtering, sentences in the dataset can be cleaned further. This process may\ninclude such things as removal of HTML tags and any special characters and normal-\nization of characters (e.g., traditional and simplified Chinese) and spelling (e.g.,\nAmerican and British English).\n If the target language uses scripts such as the Latin (a, b, c, …) or Cyrillic (а, б, в, …)\nalphabets, which distinguish upper- and lowercases, you may want to normalize case. By\ndoing so, your MT system will group “NLP” with “nlp” and “Nlp.” This step is usually a\ngood thing, because by having three different representations of a single concept, the\nMT model needs to learn that they are in fact a single concept purely from the data.\nNormalizing cases also reduces the number of distinct words, which makes training and\nprediction faster. However, this also groups “US” and “Us” and “us,” which might not be\na desirable behavior, depending on the type of data and the domain you are working\nwith. In practice, such decisions, including whether to normalize cases, are carefully\nmade by observing their effect on the validation data performance.\nData cleaning for machine translation and NLP\nNote that the cleaning techniques mentioned here are not specific to MT. Any NLP\napplications and tasks can benefit from a carefully crafted pipeline of filtering and\ncleaning operations. However, cleaning of the training data is particularly important\nfor MT, because the consistency of translation goes a long way in building a robust\nMT model. If your training data uses “NLP” in some cases and “nlp” in others, the\nmodel will have a difficulty figuring out the proper way to translate the word, whereas\nhumans would easily understand that the two words represent a single concept.\n",
      "content_length": 3158,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "150\nCHAPTER 6\nSequence-to-sequence models\nAt this point, the dataset is still a bunch of strings of characters. Most MT systems\noperate on words, so you need to tokenize the input (section 3.3) to identify words.\nDepending on the language, you may need to run a different pipeline (e.g., word seg-\nmentation is needed for Chinese and Japanese).\n The Tatoeba dataset you downloaded and expanded earlier has already gone\nthrough all this preprocessing pipeline. Now you are ready to hand the dataset over to\nFairseq. The first step is to tell Fairseq to convert the input files to the binary format so\nthat the training script can read them easily, as follows:\n$ fairseq-preprocess \\\n      --source-lang es \\\n      --target-lang en \\\n      --trainpref data/mt/tatoeba.eng_spa.train.tok \\\n      --validpref data/mt/tatoeba.eng_spa.valid.tok \\\n      --testpref data/mt/tatoeba.eng_spa.test.tok \\\n      --destdir data/mt-bin \\\n      --thresholdsrc 3 \\\n      --thresholdtgt 3\nWhen this succeeds, you should see a message Wrote preprocessed data to data/\nmt-bin on your terminal. You should also find the following group of files under the\ndata/mt-bin directory:\ndict.en.txt dict.es.txt  test.es-en.en.bin  test.es-en.en.idx  test.es-\nen.es.bin  test.es-en.es.idx  train.es-en.en.bin  train.es-en.en.idx  \ntrain.es-en.es.bin  train.es-en.es.idx  valid.es-en.en.bin  valid.es-\nen.en.idx  valid.es-en.es.bin  valid.es-en.es.idx\nOne of the key functionalities of this preprocessing step is to build the vocabulary\n(called the dictionary in Fairseq), which is a mapping from vocabulary items (usually\nwords) to their IDs. Notice the two dictionary files in the directory, dict.en.txt and\ndict.es.txt. MT deals with two languages, so the system needs to maintain two\nmappings, one for each language.\n6.3.2\nTraining the model\nNow that the train data is converted into the binary format, you are ready to train the\nMT model. Invoke the fairseq-train command with the directory where the\nbinary files are located, along with several hyperparameters, as shown next:\n$ fairseq-train \\\n    data/mt-bin \\\n    --arch lstm \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam \\\n    --lr 1.0e-3 \\\n    --max-tokens 4096 \\\n    --save-dir data/mt-ckpt\n",
      "content_length": 2238,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "151\nBuilding your first translator\nYou don’t have to worry about understanding what most of the parameters here mean\n(just yet). At this point, you need to know only that you are training a model using the\ndata stored in the directory specified by the first parameter (data/mt-bin) using an\nLSTM architecture (--arch lstm) with a bunch of other hyperparameters, and sav-\ning the results in data/mt-ckpt (short for “checkpoint”).\n When you run this command, your terminal will show two types of progress bars\nalternatively—one for training and another for validating, as shown here:\n| epoch 001:  16%|???▏                | 61/389 [00:13<01:23,  3.91it/s, \nloss=8.347, ppl=325.58, wps=17473, ups=4, wpb=3740.967, bsz=417.180, \nnum_updates=61, lr=0.001, gnorm=2.099, clip=0.000, oom=0.000, wall=17, \ntrain_wall=12]\n| epoch 001 | valid on 'valid' subset | loss 4.208 | ppl 18.48 | num_updates \n389\nThe lines corresponding to validation results are easily distinguishable by their\ncontents—they say “valid” subset. For each epoch, the training process alternates two\nstages: training and validation. An epoch, a concept used in machine learning, means\none pass through the entire train data. In the training stage, the loss is calculated\nusing the training data, then the model parameters are adjusted in such a way that the\nnew set of parameters lowers the loss. In the validation stage, the model parameters\nare fixed, and a separate dataset (validation set) is used to measure how well the\nmodel is performing against the dataset.\n I mentioned in chapter 1 that validation sets are used for model selection, a pro-\ncess where the best machine learning model is chosen among all the possible models\ntrained from a single training set. Here, by alternating between training and valida-\ntion stages, we use the validation set to check the performance of all the intermediary\nmodels (i.e., the model after the first epoch, the one after two epochs, and so on). In\nother words, we use the validation stage to monitor the progress of the training. \n Why is this a good idea? We gain many benefits by inserting the validation stage\nafter every epoch, but the most important one is to avoid overfitting—the very reason\nwhy a validation data is important in the first place. To illustrate this further, let’s look\nat how the loss changes over the course of the training of our Spanish-to-English MT\nmodel, for both the train and the validation sets, as shown in figure 6.7.\n As the training continues, the train loss becomes smaller and smaller and gradually\napproaches zero, because this is exactly what we told the optimizer to do: decrease the\nloss as much as possible. Checking whether the train loss is decreasing steadily epoch\nafter epoch is a good “sanity check” that your model and the training pipeline are\nworking as expected.\n On the other hand, if you look at the validation loss, it goes down at first for several\nepochs, but after a certain point, it gradually goes back up, forming a U-shaped\ncurve—a typical sign of overfitting. After several epochs of training, your model fits\nthe train set so well that it begins to lose its generalizability on the validation set.\n",
      "content_length": 3174,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "152\nCHAPTER 6\nSequence-to-sequence models\nFigure 6.7\nTrain and validation loss\nLet’s use a concrete example in MT to illustrate what’s really going on when a model is\noverfitted. For example, if your training data contains the English sentence “It is rain-\ning hard” and its Spanish translation “Esta lloviendo fuerte,” with no other sentences\nhaving the word “hard” in them, the overfitted model may believe that “fuerte” is the\nonly possible translation of “hard.” A properly fitted model might leave some wiggle\nroom for other Spanish words to appear as a translation for “hard,” but an overfitted\nMT system would always translate “hard” to “fuerte,” which is the “correct” thing to do\naccording to the train set but obviously not ideal if you’d like to build a robust MT sys-\ntem. For example, the best way to translate “hard” in “She is trying hard” is not “fuerte.”\n If you see your validation loss starting to creep up, there’s little point keeping the\ntraining process running, because chances are, your model has already overfitted to\nthe data to some extent. A common practice in such a situation, called early stopping, is\nto terminate the training. Specifically, if your validation loss is not improving for a cer-\ntain number of epochs, you stop the training and use the model at the point when the\nvalidation loss was the lowest. The number of epochs you wait until the training is ter-\nminated is called patience. In practice, the metric you care about the most (such as\nBLEU; see section 6.5.2) is used for early stopping instead of the validation loss.\n OK, that was enough about training and validating for now. The graph in figure\n6.7 indicates that the validation loss is lowest around epoch 8, so you can stop (by\npressing Ctrl + C) the fairseq-train command after around 10 epochs; otherwise,\n0\n2\n4\n6\n0\n10\n20\n30\nepoch\nloss\nstage\ntrain\nvalid\n",
      "content_length": 1863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "153\nBuilding your first translator\nthe command would keep running indefinitely. Fairseq will automatically save the best\nmodel parameters (in terms of the validation loss) to the checkpoint_best.pt file.\nWARNING\nNote that the training may take a long time if you are just using a\nCPU. Chapter 11 explains how to use GPUs to accelerate the training.\n6.3.3\nRunning the translator\nAfter the model is trained, you can invoke the fairseq-interactive command to\nrun your MT model on any input in an interactive way. You can run the command by\nspecifying the binary file location and the model parameter file as follows:\n$ fairseq-interactive \\\n    data/mt-bin \\\n    --path data/mt-ckpt/checkpoint_best.pt \\\n    --beam 5 \\\n    --source-lang es \\\n    --target-lang en\nAfter you see the prompt Type the input sentence and press return, try typing\n(or copying and pasting) the following Spanish sentences one by one:\n¡ Buenos días !\n¡ Hola !\n¿ Dónde está el baño ?\n¿ Hay habitaciones libres ?\n¿ Acepta tarjeta de crédito ?\nLa cuenta , por favor .\nNote the punctuation and the whitespace in these sentences—Fairseq assumes that\nthe input is already tokenized. Your results may vary slightly, depending on many fac-\ntors (the training of deep learning models usually involves some randomness), but\nyou get something along the line of the following (I added boldface for emphasis):\n¡ Buenos días !\nS-0     ¡ Buenos días !\nH-0     -0.20546913146972656    Good morning !\nP-0     -0.3342 -0.3968 -0.0901 -0.0007\n¡ Hola !\nS-1     ¡ Hola !\nH-1     -0.12050756067037582    Hi !\nP-1     -0.3437 -0.0119 -0.0059\n¿ Dónde está el baño ?\nS-2     ¿ Dónde está el baño ?\nH-2     -0.24064254760742188    Where &apos;s the restroom ?\nP-2     -0.0036 -0.4080 -0.0012 -1.0285 -0.0024 -0.0002\n¿ Hay habitaciones libres ?\nS-3     ¿ Hay habitaciones libres ?\nH-3     -0.25766071677207947    Is there free rooms ?\nP-3     -0.8187 -0.0018 -0.5702 -0.1484 -0.0064 -0.0004\n¿ Acepta tarjeta de crédito ?\nS-4     ¿ Acepta tarjeta de crédito ?\nH-4     -0.10596384853124619    Do you accept credit card ?\n",
      "content_length": 2064,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "154\nCHAPTER 6\nSequence-to-sequence models\nP-4     -0.1347 -0.0297 -0.3110 -0.1826 -0.0675 -0.0161 -0.0001\nLa cuenta , por favor .\nS-5     La cuenta , por favor .\nH-5     -0.4411449432373047     Check , please .\nP-5     -1.9730 -0.1928 -0.0071 -0.0328 -0.0001\nMost of the output sentences here are almost perfect, except the fourth one (I would\ntranslate to “Are there free rooms?”). Even considering the fact that these sentences\nare all simple examples you can find in any travel Spanish phrasebook, this is not a\nbad start for a system built within an hour!\n6.4\nHow Seq2Seq models work\nIn this section, we will dive deep into the individual components that constitute a\nSeq2Seq model, which include the encoder and the decoder. We’ll also cover the algo-\nrithms used for decoding the target sentence—greedy decoding and beam search\ndecoding.\n6.4.1\nEncoder\nAs we saw in the beginning of this chapter, the encoder of a Seq2Seq model is not\nmuch different from the sequential-labeling models we covered in chapter 5. Its main\njob is to take the input sequence (usually a sentence) and convert it into a vector rep-\nresentation of a fixed length. You can use an LSTM-RNN as shown in figure 6.8.\nFigure 6.8\nEncoder of a Seq2Seq model\nUnlike sequential-labeling models, we need only the final hidden state of an RNN,\nwhich is then passed to the decoder to generate the target sentence. You can also use\na multilayer RNN as an encoder, in which case the sentence representation is the con-\ncatenation of the output of each layer, as illustrated in figure 6.9.\n  \nMaria\nno\nbaba\nverde\n.\n...\nSentence\nrepresentation\nRNN\ncell\nRNN\ncell\nRNN\ncell\nRNN\ncell\nRNN\ncell\n",
      "content_length": 1653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "155\nHow Seq2Seq models work\nFigure 6.9\nUsing a multilayer RNN as an encoder\nSimilarly, you can use a bidirectional (or even a bidirectional multilayer) RNN as an\nencoder. The final sentence representation is a concatenation of the output of the for-\nward and the backward layers, as shown in figure 6.10.\nFigure 6.10\nUsing a bidirectional RNN as an encoder\nLayer 1\nLayer 2\n...\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\nSentence\nrepresentation\nMaria\nno\nbaba\nverde\n.\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nForward\nlayer\nBackward\nlayer\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\nupdate\n...\n...\nForward\nlayer\nMaria\nno\nbaba\n.\nSentence\nrepresentation\n",
      "content_length": 704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "156\nCHAPTER 6\nSequence-to-sequence models\nNOTE\nThis is a small detail, but remember that an LSTM cell produces two\ntypes of output: the cell state and the hidden state (see section 4.2.2 for\nreview). When using LSTM for encoding a sequence, we usually just use the\nfinal hidden state while discarding the cell state. Think of the cell state as\nsomething like a temporary loop variable used for computing the final out-\ncome (the hidden state). See figure 6.11 for an illustration.\nFigure 6.11\nAn encoder using LSTM cells\n6.4.2\nDecoder\nLikewise, the decoder of a Seq2Seq model is similar to the language model we cov-\nered in chapter 5. In fact, they are identical except for one crucial difference—a\ndecoder takes an input from the encoder. The language models we covered in chapter\n5 are called unconditional language models because they generate language without any\ninput or precondition. On the other hand, language models that generate language\nbased on some input (condition) are called conditional language models. A Seq2Seq\ndecoder is one type of conditional language model, where the condition is the sen-\ntence representation produced by the encoder. See figure 6.12 for an illustration of\nhow a Seq2Seq decoder works.\nMaria\ninit\nstate\ncell\nstate\ncell\nstate\ncell\nstate\ncell\nstate\ncell\nstate\nhidden\nstate\nhidden\nstate\nhidden\nstate\nhidden\nstate\nhidden\nstate\nno\nbaba\nverde\n.\n...\nSentence\nrepresentation\nx\nLSTM\ncell\nLSTM\ncell\nLSTM\ncell\nLSTM\ncell\nLSTM\ncell\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nDecoder\nSentence\nrepresentation\nFigure 6.12\nA decoder \nof a Seq2Seq model\n",
      "content_length": 1586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "157\nHow Seq2Seq models work\nJust as with language models, Seq2Seq decoders generate text from left to right. Like\nthe encoder, you can use an RNN to do this. A decoder can also be a multilayer RNN.\nHowever, a decoder cannot be bidirectional—you cannot generate a sentence from\nboth sides. As was mentioned in chapter 5, models that operate on the past sequence\nthey produced are called autoregressive models. \nHow the decoder behaves is a bit different between the training and the prediction\nstages. Let’s see how it is trained first. At the training stage, we know exactly how the\nsource sentence should be translated into the target sentence. In other words, we\nknow exactly what the decoder should produce, word by word. Because of this, decod-\ners are trained in a similar way to how sequential-labeling models are trained (see\nchapter 5).\n First, the decoder is fed the sentence representation produced by the encoder and\na special token <START>, which indicates the start of a sentence. The first RNN cell\nprocesses these two inputs and produces the first hidden state. The hidden state vec-\ntor is fed to a linear layer that shrinks or expands this vector to match the size of the\nvocabulary. The resulting vector then goes through softmax, which converts it to a\nprobability distribution. This distribution dictates how likely each word in the vocabu-\nlary is to come next. \n Then, this is where the training happens. If the input is “Maria no daba una bofetada\na la bruja verde,” then we would like the decoder to produce its English equivalent:\n“Mary did not slap the green witch.” This means that we would like to maximize the\nprobability that the first RNN cell generates “Mary” given the input sentence. This is a\nmulticlass classification problem we have seen many times so far in this book—word\nembeddings (chapter 3), sentence classification (chapter 4), and sequential labeling\n(chapter 5). You use the cross-entropy loss to measure how far apart the desired out-\ncome is from the actual output of your network. If the probability for “Mary” is large,\nthen good—the network incurs a small loss. On the other hand, if the probability for\n“Mary” is small, then the network incurs a large loss, which encourages the optimization\nalgorithm to change the parameters (magic constants) by a large amount.\nNon-autoregressive models\nIf you think simply generating text from left to right is too limiting, you have a good\npoint. Humans also do not always write language linearly—we often revise, add, and\ndelete words and phrases afterward. Also, generating text in a linear fashion is not\nvery efficient. The latter half of a sentence needs to wait until its first half is com-\npleted, which makes it very difficult to parallelize the generation process. As of this\nwriting, researchers are putting a lot of effort into developing non-autoregressive MT\nmodels that do not generate the target sentence in a linear fashion (see, for example,\nthis paper from Salesforce Research: https://arxiv.org/abs/1711.02281). However,\nthey haven’t exceeded autoregressive models in terms of translation quality, and\nmost research and production MT systems still adopt autoregressive models.\n",
      "content_length": 3186,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "158\nCHAPTER 6\nSequence-to-sequence models\n Then, we move on to the next\ncell. The next cell receives the hid-\nden state computed by the first cell\nand the word “Mary,” regardless of\nwhat the first cell generated. Instead of\nfeeding the token generated by the\nprevious cell, as we did when gener-\nating text using a language model,\nwe constrain the input to the\ndecoder so that it won’t “go astray.”\nThe second cell produces the hid-\nden state based on these two\ninputs, which is then used to com-\npute the probability distribution for\nthe second word. We compute the\ncross-entropy loss by comparing the\ndistribution against the desired out-\nput “did” and move on to the next\ncell. We keep doing this until we\nreach the final token, which is\n<END>. The total loss for the sen-\ntence is the average of all the losses\nincurred for all the words in the\nsentence, as shown in figure 6.13.\nFinally, the loss computed this\nway is used to adjust the model\nparameters of the decoder, so that\nit can generate the desired output\nthe next time around. Note that the parameters of the encoder are also adjusted in\nthis process, because the loss propagates all the way back to the encoder through the\nsentence representation. If the sentence representation produced by the encoder is\nnot good, the decoder won’t be able to produce high-quality target sentences no mat-\nter how hard it tries.\n6.4.3\nGreedy decoding\nNow let’s look at how the decoder behaves at the prediction stage, where a source sen-\ntence is given to the network, but we don’t know what the correct translation should\nbe. At this stage, a decoder behaves a lot like the language models we discussed in\nchapter 5. It is fed the sentence representation produced by the encoder, as well as a\nspecial token <START>, which indicates the start of a sentence. The first RNN cell pro-\ncesses these two inputs and produces the first hidden state, which is then fed to the lin-\near layer and the softmax layer to produce the probability distribution over the target\nvocabulary. Here comes the key part—unlike the training phase, you don’t know the\nstate\nstate\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\n \nSoftmax\nDistribution\nover \nvocabulary\n \nCross\nentropy\n \nRNN\n \nMary\nCross\nentropy\n \ndid\nCross\nentropy\n \nnot\n+\n+\n=\n<START>\nMary\ndid\nSentence\nrepresentation\n \nLinear\nlayer\n \nSoftmax\nLinear\nlayer\n \nSoftmax\nTotal\nloss\nFigure 6.13\nTraining a Seq2Seq decoder\n",
      "content_length": 2398,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "159\nHow Seq2Seq models work\ncorrect word to come next, so\nyou have multiple options. You\ncan choose any random word\nthat has a reasonably high prob-\nability (say, “dog”), but proba-\nbly the best you can do is pick\nthe word whose probability is\nthe highest (you are lucky if it’s\n“Mary”). The MT system pro-\nduces the word that was just\npicked and then feeds it to the\nnext RNN cell. This is repeated\nuntil the special token <END> is\nencountered. Figure 6.14 illus-\ntrates this process.\n OK, so are we all good, then?\nCan we move on to evaluating\nour MT system, because it is\ndoing everything it can to pro-\nduce the best possible transla-\ntion? Not so fast—many things\ncould go wrong by decoding the\ntarget sentence in this manner. \n First of all, the goal of MT\ndecoding is to maximize the\nprobability of the target sentence as a whole, not just individual words. This is exactly\nwhat you trained the network to do—to produce the largest probability for correct sen-\ntences. However, the way words are picked at each step described earlier is to maximize\nthe probability of that word only. In other words, this decoding process guarantees only\nthe locally maximum probability. This type of myopic, locally optimal algorithm is\ncalled greedy in computer science, and the decoding algorithm I just explained is called\ngreedy decoding. However, just because you are maximizing the probability of individual\nwords at each step doesn’t mean you are maximizing the probability of the whole sen-\ntence. Greedy algorithms, in general, are not guaranteed to produce the globally opti-\nmal solution, and using greedy decoding can leave you stuck with suboptimal\ntranslations. This is not very intuitive to understand, so let me use a simple example to\nillustrate this.\n When you are picking words at each timestep, you have multiple words to pick\nfrom. You pick one of them and move on to the next RNN cell, which produces\nanother set of possible words to pick from, depending on the word you picked previ-\nously. This can be represented using a tree structure like the one shown in figure 6.15.\nThe diagram shows how the word you pick at one timestep (e.g., “did”) branches out\nto another set of possible words (“you” and “not”) to pick from at the next timestep.\nstate\nstate\nupdate\nupdate\nupdate\nstate\nLinear\nlayer\nSoftmax\nDistribution\nover\nvocabulary\nRNN\n<START>\nMary\nMary\ndid\nnot\ndid\nSentence\nrepresentation\nSoftmax\nSoftmax\nLinear\nlayer\nLinear\nlayer\nFigure 6.14\nA prediction using a Seq2Seq decoder\n",
      "content_length": 2494,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "160\nCHAPTER 6\nSequence-to-sequence models\nFigure 6.15\nA decoding decision tree\nEach transition from word to word is labeled with a score, which corresponds to how\nlarge the probability of choosing that transition is. Your goal here is to maximize the\ntotal sum of the scores when you traverse one path from timestep 1 to 4. Mathemati-\ncally, probabilities are real numbers between 0 to 1, and you should multiply (instead\nof add) each probability to get the total, but I’m simplifying things here. For example,\nif you go from “Mary” to “did,” then on to “you” and “do,” you just generated a sen-\ntence “Mary did you do” and the total score is 1 + 5 + 1 = 7.\n The greedy decoder we saw earlier will face two choices after it generates “did” at\ntimestep 2: either generate “you” with a score of 5 or “not” with a score of 3. Because\nall it does is pick the one with the highest score, it will pick “you” and move on. Then\nit will face another branch after timestep 3—generating “do” with a score of 1 or gen-\nerating “know” with a score of 2. Again, it will pick the largest score, and you will end\nup with the translation “Mary did you know” whose score is 1+ 5 + 1 = 8.\n This is not a bad result. At least, it is not as bad as the first path, which sums up to a\nscore of 7. By picking the maximum score at each branch, you are making sure that\nyour final result is at least decent. However, what if you picked “not” at timestep 3? At\nfirst glance, this doesn’t seem like a good idea, because the score you get is only 3,\nwhich is smaller than you’d get by taking the other path, 5. But at the next timestep,\nby generating “slap,” you get a score of 5. In retrospect, this was the right thing to\ndo—in total, you get 1 + 3 + 5 = 9, which is larger than any scores you’d get by taking\nthe other “you” path. By sacrificing short-term rewards, you are able to gain even\nlarger rewards in the long run. But due to the myopic nature of the greedy decoder, it\nwill never choose this path—it can’t backtrack and change its mind once it’s taken one\nbranch over another.\n Choosing which way to go to maximize the total score seems easy if you look at the\ntoy example in figure 6.15, but in reality, you can’t “foresee” the future—if you are at\ntimestep t, you can’t predict what will happen at timestep t + 1 and onward, until you\nMary\n1\n1\n2\n3\n4\nTimestep\nTotal score\ndid\nnot\nslap\nyou\ndo\nknow\n5\n3\n5\n1\n1 + 5 + 1 = 7\n1 + 5 + 2 = 8\n1 + 3 + 5 = 9\n2\n",
      "content_length": 2434,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "161\nHow Seq2Seq models work\nactually choose one word and feed it to the RNN. But the path that maximizes the\nindividual probability is not necessarily the optimal solution. You just can’t try every\npossible path and see what score you’d get, either, because the vocabulary usually con-\ntains tens of thousands of unique words, meaning the number of possible paths is\nexponentially large.\n The sad truth is that you can’t realistically expect to find the optimal path that\nmaximizes the probability for the entire sentence within a reasonable amount of time.\nBut you can avoid being stuck (or at least, make it less likely to be stuck) with a sub-\noptimal solution, which is what the beam search decoder does.\n6.4.4\nBeam search decoding\nLet’s think what you would do if you were in the same situation. Let’s use an analogy\nand say you are a college sophomore and need to decide which major to pursue by the\nend of the school year. Your goal is to maximize the total amount of income (or happi-\nness or whatever thing you care about) over the course of your lifetime, but you don’t\nknow which major is the best for this. You can’t simply try every possible major and see\nwhat happens after a couple of years—there are too many majors and you can’t go\nback in time. Also, just because some particular majors look appealing in the short\nrun (e.g., choosing an economics major may lead to some good internship opportuni-\nties at large investment banks) doesn’t mean that path is the best in the long run (see\nwhat happened in 2008). \n In such a situation, one thing you could do is to hedge your bet by pursuing more\nthan one major (as a double major or a minor) at the same time instead of commit-\nting 100% to one particular major. After a couple of years, if the situation is more dif-\nferent than you had imagined, you can still change your mind and pursue another\noption, which is not possible if you choose your major greedily (i.e., based only on the\nshort-term prospects).\n The main idea of beam search decoding is similar to this—instead of committing\nto one path, it purses multiple paths (called hypotheses) at the same time. In this way,\nyou leave some room for “dark horses,” that is, hypotheses that had low scores at first\nbut may prove promising later. Let’s see this in action using the example in figure\n6.16, a slightly modified version of figure 6.15.\n The key idea of beam search decoding is to use a beam (figure 6.16 bottom), which\nyou can think of as some sort of buffer that can retain multiple hypotheses at the same\ntime. The size of the beam, that is, the number of hypotheses it can retain, is called the\nbeam width. Let’s use a beam of size 2 and see what happens. Initially, your first hypoth-\nesis consists of only one word, “Mary,” and a score of 0. When you move on to the next\nword, the word you chose is appended to the hypothesis, and the score is incremented\nby the score of the path you have just taken. For example, when you move on to “did,”\nit will make a new hypothesis consisting of “Mary did” and a score of 1.\n If you have multiple words to choose from at any particular timestep, a hypothesis\ncan spawn multiple child hypotheses. At timestep 2, you have three different\n",
      "content_length": 3212,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "162\nCHAPTER 6\nSequence-to-sequence models\nchoices—“you,” “not,” and “n’t”—which generate three new child hypotheses: [Mary\ndid you] (6), [Mary did not] (4), and [Mary did n’t] (3). And here’s the key part of\nbeam search decoding: because there’s only so much room in the beam, any hypothe-\nses that are not good enough fall off of the beam after sorting them by their scores.\nBecause the beam can hold up to only two hypotheses in this example, anything\nexcept the top two gets kicked out of the beam, which leaves [Mary did you] (6) and\n[Mary did not] (4).\n At timestep 3, each remaining hypothesis can spawn up to two child hypotheses.\nThe first one ([Mary did you] (6)) will generate [Mary did you know] (8) and [Mary\ndid you do] (7), whereas the second one ([Mary did not] (4)) turns into [Mary did\nnot slap] (9). These three hypotheses are sorted by their scores, and the best two will\nbe returned as the result of the beam search decoding.\n Congratulations—now your algorithm was able to find the path that maximizes the\ntotal sum of the scores. By considering multiple hypotheses at the same time, beam\nsearch decoding can increase the chance that you will find better solutions. However,\nit is never perfect—notice that an equally good path [Mary did n’t do] with a score of\n9 fell out of the beam as early as timestep 3. To “rescue” it, you’d need to increase the\nbeam width to 3 or larger. In general, the larger the beam width, the higher the\nexpected quality of the translation results will be. However, there’s a tradeoff: because\nMary\n1\n1\n2\n3\n4\nTimestep\ndid\nnot\nn’t\nslap\nyou\ndo\ndo\nknow\n5\n3\n2\n5\n6\n1\n2\n[Mary] 0\n[Mary did] 1\n[Mary did you] 6\n[Mary did you do] 7\n[Mary did you know] 8\n[Mary did not slap] 9\n[Mary did not] 4\n[Mary did n’t] 3\nBeam\n(width = 2)\n \nFigure 6.16\nBeam search decoding\n",
      "content_length": 1803,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "163\nEvaluating translation systems\nthe computer needs to consider multiple hypotheses, it will be linearly slower as the\nbeam width increases.\n In Fairseq, you can use the --beam option to change the beam size. In the exam-\nple in section 6.3.3, I used --beam 5 to use a beam width of 5. You were already using\nbeam search without noticing. If you invoke the same command with --beam 1,\nwhich means you are using greedy decoding instead of a beam search, you may get\nslightly different results. When I tried this, I got almost the same results except the last\none: “counts, please,” which is not a great translation for “La cuenta, por favor.” This\nmeans using a beam search indeed helps improve the translation quality!\n6.5\nEvaluating translation systems\nIn this section, I’d like to briefly touch on the topic of evaluating machine translation\nsystems. Accurately evaluating MT systems is an important topic, both in theory and\npractice.\n6.5.1\nHuman evaluation\nThe simplest and the most accurate way to evaluate MT systems’ output is to use\nhuman evaluation. After all, language is translated for humans. Translations that are\ndeemed good by humans should be good.\n As mentioned previously, we have a few considerations for what makes a translation\ngood. There are two most important and commonly used concepts for this—adequacy\n(also called fidelity) and fluency (also closely related to intelligibility). Adequacy is the\ndegree to which the information in the source sentence is reflected in the translation.\nIf you can reconstruct a lot of information expressed by the source sentence by reading\nits translation, then the translation has high adequacy. Fluency is, on the other hand,\nhow natural the translation is in the target language. If you are translating into English,\nfor example, “Mary did not slap the green witch” is a fluent translation, whereas “Mary\nno had a hit with witch, green” is not, although both translations are almost equally\nadequate. Note that these two aspects are somewhat independent—you can think of a\ntranslation that is fluent but not adequate (e.g., “Mary saw a witch in the forest” is a per-\nfectly fluent but inadequate translation) and vice versa, like the earlier example. MT\nsystems that produce output that is both adequate and fluent are desirable.\n An MT system is usually evaluated by presenting its translations to human annota-\ntors and having them judge its output on a 5- or 7-point scale for each aspect. Fluency\nis easier to judge because it requires only monolingual speakers of the target sentence,\nwhereas adequacy requires bilingual speakers of both the source and target languages.\n6.5.2\nAutomatic evaluation\nAlthough human evaluation gives the most accurate assessment to MT systems’ qual-\nity, it’s not always feasible. In most cases, you cannot afford to hire human evaluators\nto assess an MT system’s output every time you need it. If you are dealing with lan-\nguage pairs that are not common, you might not be able to find bilingual speakers for\nevaluating adequacy at all.\n",
      "content_length": 3037,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "164\nCHAPTER 6\nSequence-to-sequence models\n But more importantly, you need to constantly evaluate and monitor an MT sys-\ntem’s quality when you are developing one. For example, if you use a Seq2Seq model\nto train an NMT system, you need to reevaluate its performance every time you adjust\none of the hyperparameters. Otherwise, you wouldn’t know whether your change has\na good or bad effect on its final performance. Even worse, if you were to do something\nlike early stopping (see section 6.3.2) to determine when to stop the training process,\nyou would need to evaluate its performance after every epoch. You can’t possibly hire\nsomebody and have them evaluate your intermediate models at each epoch—that\nwould be a terribly slow way to develop an MT system. It’s also a huge waste of time,\nbecause the output of initial models is largely garbage and does not warrant human\nevaluation. A large amount of correlation exists between the outputs of intermediate\nmodels, and human evaluators would be spending a lot of time evaluating very similar,\nif not identical, sentences.\n For this reason, it’d be desirable if we could use some automatic way to assess trans-\nlation quality. The way this works is similar to some automatic metrics for other NLP\ntasks that we saw earlier, such as accuracy, precision, recall, and F1-measure for classifi-\ncation. The idea is to create the desirable output for each input instance in advance\nand compare a system’s output against it. This is usually done by preparing a set of\nhuman-created translations called reference for each source sentence and calculating\nsome sort of similarity between the reference and a system’s output. Once you create\nthe reference and define the metric, you can automatically evaluate translation quality\nas many times as you want.\n One of the simplest ways to compute the similarity between the reference and a\nsystem output is to use the word error rate (WER). WER reflects how many errors the\nsystem made compared to the reference, measured by the relative number of inser-\ntions, deletions, and substitutions. The concept is similar to the edit distance, except\nthat WER is counted for words, not characters. For example, when the reference sen-\ntence is “Mary did not slap the green witch” and a system translation is “Mary did hit\nthe green wicked witch,” you need three “edits” to match the latter to the former—\ninsert “not,” replace “hit” with “slap,” and delete “wicked.” If you divide three by the\nlength of the reference (= 7), it’s your WER (= 3/7, or 0.43). The lower the WER, the\nbetter the quality of your translation.\n Although WER is simple and easy to compute, it is not widely used for evaluating\nMT systems nowadays. One reason is related to multiple references. There may be\nmultiple, equally valid translations for a single source sentence, but it is not clear how\nto apply WER when there are multiple references. A slightly more advanced and by far\nthe most commonly used metric for automatic evaluation in MT is BLEU (bilingual\nevaluation understudy). BLEU solves the problem of multiple references by using\nmodified precision. I’ll illustrate this next using a simple example.\n In the following table, we are evaluating a candidate (a system’s output) “the the\nthe the the the the” (which is, by the way, a terrible translation) against two refer-\nences: “the cat is on the mat” and “there is a cat on the mat.” The basic idea of BLEU\n",
      "content_length": 3427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "165\nCase study: Building a chatbot\nis to calculate the precision of all unique words in the candidate. Because there’s only\none unique word in the candidate, “the,” if you calculate its precision, it will automati-\ncally become the candidate’s score, which is 1, or 100%. But there seems to be some-\nthing wrong about this.\nBecause only two “thes” exist in the references, the spurious “thes” generated by the\nsystem shouldn’t count toward the precision. In other words, we should treat them as\nfalse positives. We can do this by capping the denominator of precision by the maxi-\nmum number of occurrences of that word in any of the references. Because it’s 2 in\nthis case (in reference 1), its modified precision will be 2/7, or about 29%. In prac-\ntice, BLEU uses not only unique words (i.e., unigrams) but also all unique sequences\nof words (n-grams) up to a length of 4 in the candidate and the references. \n However, we can game this metric in another way—because it’s based on precision,\nnot on recall, an MT system can easily obtain high scores by producing very few words\nthat the system is confident about. In the previous example, you can simply produce\n“cat” (or even more simply, “the”), and the BLEU score will be 100%, which is obvi-\nously not a good translation. BLEU solves this issue by introducing the brevity penalty,\nwhich discounts the score if the candidate is shorter than the references.\n Development of accurate automatic metrics has been an active research area.\nMany new metrics are proposed and used to address the shortcomings of BLEU. We\nbarely scratched the surface in this section. Although new metrics show higher cor-\nrelations with human evaluations and are claimed to be better, BLEU is still by far the\nmost widely used metric, mainly due to its simplicity and long tradition.\n6.6\nCase study: Building a chatbot\nIn this section, I’m going to go over another application of a Seq2Seq model—a chat-\nbot, which is an NLP application with which you can have a conversation. We are\ngoing to build a very simple yet functional chatbot using a Seq2Seq model and discuss\ntechniques and challenges in building intelligent agents.\n6.6.1\nIntroducing dialogue systems\nI briefly touched upon dialogue systems in section 1.2.1. To recap, two main types of\ndialogue systems exist: task-oriented and chatbots. Although task-oriented dialogue\nsystems are used to achieve some specific goals, such as making a reservation at a\nrestaurant and obtaining some information, chatbots are used to have conversations\nwith humans. Conversational technologies are currently a hot topic among NLP prac-\ntitioners, due to the success and proliferation of commercial conversational AI sys-\ntems such as Amazon Alexa, Apple Siri, and Google Assistant. \nCandidate\nthe\nthe\nthe\nthe\nthe\nthe\nthe\nReference 1\nthe\ncat\nis\non\nthe\nmat\nReference 2\nthere\nis\na\ncat\non\nthe\nmat\n",
      "content_length": 2869,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "166\nCHAPTER 6\nSequence-to-sequence models\n You may not have a clue as to how we can get started with building an NLP applica-\ntion that can have conversations. How can we build something “intelligent” that\n“thinks” so that it can generate meaningful responses to human input? This seems far-\nfetched and difficult. But if you step back and look at a typical conversation we have\nwith other people, how much of it is actually “intelligent?” If you are like most of us, a\nlarge fraction of the conversation you are having is autopilot: “How are you?” “I’m\ndoing good, thanks” “Have a good day” “You, too!” and so on. You may also have a set\nof “template” responses to a lot of everyday questions such as “What do you do?” and\n“Where are you from?” These questions can be answered just by looking at the input.\nEven more complex questions like “What’s your favorite restaurant in X?” (where X is\nthe name of a neighborhood in your city) and “Did you see any Y movies lately?”\n(where Y is a genre) can be answered just by “pattern matching” and retrieving rele-\nvant information from your memory. \n If you think of a conversation as a set of “turns” where the response is generated by\npattern matching against the previous utterance, this starts to look a lot like a typical\nNLP problem. In particular, if you regard dialogues as a problem where an NLP sys-\ntem is simply converting your question to its response, this is exactly where we can\napply the Seq2Seq models we covered in this chapter so far. We can treat the previous\n(human’s) utterance as a foreign sentence and have the chatbot “translate” it into\nanother language. Even though these two languages are both English in this case, it is\na common practice in NLP to treat the input and the output as two different lan-\nguages and apply a Seq2Seq model to them, including summarization (longer text to\na shorter one) and grammatical error correction (text with errors to one without).\n6.6.2\nPreparing a dataset\nIn this case study, we are going to use The Self-dialogue Corpus (https://github.com/\njfainberg/self_dialogue_corpus), a collection of 24,165 conversations. What’s special\nabout this dataset is that these conversations are not actual ones between two people,\nbut fictitious ones written by one person who plays both sides. You could use several\nconversation datasets for text-based chatbots (e.g., the OpenSubtitles dataset, http://\nopus.nlpl.eu/OpenSubtitles-v2018.php), but these datasets are often noisy and often\ncontain obscenities. By collecting made-up conversations instead, the Self-dialogue\nCorpus improves the quality for half the original cost (because you need only one per-\nson versus two people!).\n The same as earlier, I tokenized and converted the corpus into a format that is\ninterpretable by Fairseq. You can obtain the converted dataset as follows:\n$ mkdir -p data/chatbot\n$ wget https://realworldnlpbook.s3.amazonaws.com/data/chatbot/selfdialog.zip\n$ unzip selfdialog.zip -d data/chatbot\nYou can use the following combination of the paste command (to stitch files hori-\nzontally) and the head command to peek at the beginning of the training portion.\n",
      "content_length": 3139,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "167\nCase study: Building a chatbot\nNote that we are using fr (for “foreign,” not “French”) to denote the “language” we\nare translating from:\n$ paste data/chatbot/selfdialog.train.tok.fr data/chatbot/\nselfdialog.train.tok.en | head \n...\nHave you played in a band ?    What type of band ?\nWhat type of band ?    A rock and roll band .\nA rock and roll band .    Sure , I played in one for years .\nSure , I played in one for years .    No kidding ?\nNo kidding ?    I played in rock love love .\nI played in rock love love .    You played local ?\nYou played local ?    Yes\nYes    Would you play again ?\nWould you play again ?    Why ?\n...\nAs you can see, each line consists of an utterance (on the left) and a response to it (on\nthe right). Notice that this dataset has the same structure as the Spanish-English paral-\nlel corpus we used in section 6.3.1. The next step is to run the fairseq-preprocess\ncommand to convert it to a binary format as follows:\n$ fairseq-preprocess \\\n    --source-lang fr \\\n    --target-lang en \\\n    --trainpref data/chatbot/selfdialog.train.tok \\\n    --validpref data/chatbot/selfdialog.valid.tok \\\n    --destdir data/chatbot-bin \\\n    --thresholdsrc 3 \\\n    --thresholdtgt 3\nAgain, this is similar to what we ran for the Spanish translator example. Just pay atten-\ntion to what you specify as the source language—we are using fr instead of es here.\n6.6.3\nTraining and running a chatbot\nNow that the training data for the chatbot is ready, let’s train a Seq2Seq model from\nthis data. You can invoke the fairseq-train command with almost identical param-\neters to the last time, as shown next:\n$ fairseq-train \\\n    data/chatbot-bin \\\n    --arch lstm \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam \\\n    --lr 1.0e-3 \\\n    --max-tokens 4096 \\\n    --save-dir data/chatbot-ckpt\nAs previously, pay attention to how the validation loss changes every epoch. When I\ntried this, the validation loss decreased for about five epochs but then started to slowly\ncreep back up. Feel free to stop the training command by pressing Ctrl + C after you\n",
      "content_length": 2075,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "168\nCHAPTER 6\nSequence-to-sequence models\nobserve the validation loss leveling out. Fairseq will automatically save the best model\n(measured by the validation loss) to checkpoint_best.pt.\n Finally, you can run the chatbot model by invoking the fairseq-interactive\ncommand, as shown here:\n$ fairseq-interactive \\\n    data/chatbot-bin \\\n    --path data/chatbot-ckpt/checkpoint_best.pt \\\n    --beam 5 \\\n    --source-lang fr \\\n    --target-lang en\nAs with the previous example, you can type your source sentences and have a conver-\nsion with your chatbot by having them “translate” to another language! Here’s part of\na conversation that I had with the model that I trained (I added boldface for clarity).\nAgain, your results might be different:\nHi !\nS-0     Hi !\nH-0     -1.011711835861206      Hey .\nP-0     -0.6860 -2.1352 -0.2140\nHow are you ?\nS-1     How are you ?\nH-1     -0.6552960276603699     Doing good , you ?\nP-1     -0.6472 -0.8651 -1.0960 -0.9257 -0.3337 -0.0640\nGood .\nS-2     Good .\nH-2     -0.8540937900543213     Do you have a favorite team ?\nP-2     -2.3027 -0.0550 -1.7088 -0.4403 -0.1763 -2.0262 -0.1150 -0.0085\nYeah I like the Seahawks .\nS-3     Yeah I like the Seahawks .\nH-3     -0.6036218404769897     Do you have a favorite team ?\nP-3     -2.7614 -0.0525 -1.0905 -0.3178 -0.3213 -0.1049 -0.1701 -0.0106\nHow about you ?\nS-4     How about you ?\nH-4     -1.2329542636871338     I 'm a big fan of them .\nP-4     -0.7648 -1.8435 -1.6808 -1.2868 -0.2102 -0.1362 -3.5273 -1.0031 -0.6438\nWhat 's your favorite movie ?\nS-5     What 's your favorite movie ?\nH-5     -0.8238655924797058     Oh , that 's a tough one . I 'd have to say \nGhost Busters .\nP-5     -3.2622 -0.3213 -1.4815 -0.1012 -0.3417 -0.3069 -0.2432 -0.6954 -\n1.0858 -2.3246 -0.4296 -0.0039 -0.0905 -2.6429 -0.1676 -0.3853 -0.1221\nOh yeah , I like that movie too .\nS-6     Oh yeah , I like that movie too .\nH-6     -1.0659444332122803     Yeah , that 's a good one .\nP-6     -2.0782 -0.1724 -2.5089 -0.9560 -1.5034 -0.9040 -0.4471 -0.5435 -0.4801\nIn this example, the conversation looks natural. Because the Self-dialogue Corpus is\nbuilt by restricting the set of possible conversation topics, the conversation is more\nlikely to go smoothly if you stay on such topics (movie, sports, music, and so on).\n",
      "content_length": 2280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "169\nCase study: Building a chatbot\n However, as soon as you start talking about unfamiliar topics, the chatbot loses its\nconfidence in its answers, as shown next:\nWhat 's your name ?\nS-0     What 's your name ?\nH-0     -0.9807574152946472     I do n't know , but I do n't think I 've\n                           heard of them .\nP-0     -1.4680 -2.2938 -0.0635 -1.0242 -1.2039 -0.5201 -0.3381 -2.2612 -\n0.1665 -1.6033 -0.6112 -1.5515 -0.8739 -0.8453 -1.0143 -0.4702 -0.3637\nWhat do you do ?\nS-1     What do you do ?\nH-1     -1.170507788658142      I do n't know .\nP-1     -0.9807 -2.1846 -0.3276 -0.9733 -1.3650 -1.1919\nAre you a student ?\nS-2     Are you a student ?\nH-2     -0.9505285024642944     I 'm not sure .\nP-2     -1.5676 -1.5270 -0.6944 -0.2493 -0.8445 -0.8204\nThis is a well-known phenomenon—a simple Seq2Seq-based chatbot quickly regresses\nto producing cookie-cutter answers such as “I don’t know” and “I’m not sure” when-\never asked about something it’s not familiar with. This has to do with the way we\ntrained this chatbot. Because we trained the model so that it minimizes the loss in the\ntraining data, the best strategy it can take to reduce the loss is to produce something\napplicable to as many input sentences as possible. Very generic phrases such as “I\ndon’t know” can be an answer for many questions, so it’s a great way to play it safe and\nreduce the loss!\n6.6.4\nNext steps\nAlthough our chatbot can produce realistic-looking responses for many inputs, it’s far\nfrom perfect. One issue that it’s not great at dealing with is proper nouns. You can see\nthis when you ask questions that solicit specific answers, like the following:\nWhat 's your favorite show ?\nS-0     What 's your favorite show ?\nH-0     -0.9829921722412109     I would have to say <unk> .\nP-0     -0.8807 -2.2181 -0.4752 -0.0093 -0.0673 -2.9091 -0.9338 -0.3705\nHere <unk> is the catch-all special symbol for unknown words. The chatbot is trying to\nanswer something, but that something occurs too infrequently in the training data to\nbe treated as an independent word. This is an issue seen in simple NMT systems in gen-\neral. Because the models need to cram everything about a word in a 200-something-\ndimensional vector of numbers, many details and distinctions between similar words\nare sacrificed. Imagine compressing all the information about all the restaurants in\nyour city into a 200-dimensional vector!\n Also, the chatbot we trained doesn’t have any “memory” or any notion of context\nwhatsoever. You can test this by asking a series of related questions as follows:\nDo you like Mexican food ?\nS-0     Do you like Mexican food ?\n",
      "content_length": 2626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "170\nCHAPTER 6\nSequence-to-sequence models\nH-0     -0.805641770362854      Yes I do .\nP-0     -1.0476 -1.1101 -0.6642 -0.6651 -0.5411\nWhy do you like it ?\nS-1     Why do you like it ?\nH-1     -1.2453081607818604     I think it 's a great movie .\nP-1     -0.7999 -2.1023 -0.7766 -0.7130 -1.4816 -2.2745 -1.5750 -1.0524 -0.4324\nIn the second question, the chatbot is having difficulties understanding the context\nand produces a completely irrelevant response. To answer such questions correctly,\nthe model needs to understand that the pronoun “it” refers to a previous noun,\nnamely, “Mexican food” in this case. The task where NLP systems resolve which men-\ntions refer to which entities in the real world is called coreference resolution. The system\nalso needs to maintain some type of memory to keep track of what was discussed so far\nin the dialogue. \n Finally, the simple Seq2Seq models we discussed in this chapter are not great at\ndealing with long sentences. If you look back at figure 6.2, you’ll understand why—the\nmodel reads the input sentence using an RNN and represents everything about the\nsentence using a fixed-length sentence representation vector and then generates the\ntarget sentence from that vector. It doesn’t matter whether the input is “Hi!” or “The\nquick brown fox jumped over the lazy dog.” The sentence representation becomes a\nbottleneck, especially for longer input. Because of this, neural MT models couldn’t\nbeat traditional phrase-based statistical MT models until around 2015, when a mecha-\nnism called attention was invented to tackle this very problem. We’ll discuss attention\nin detail in chapter 8. \nSummary\nSequence-to-sequence (Seq2Seq) models transform one sequence into another\nusing an encoder and a decoder.\nYou can use the fairseq framework to build a working MT system within an\nhour.\nA Seq2Seq model uses a decoding algorithm to generate the target sequence.\nGreedy decoding maximizes the probability at each step, whereas beam search\ntries to find better solutions by considering multiple hypotheses at once.\nA metric called BLEU is commonly used for automatically evaluating MT systems.\nA simple chatbot can be built by using a Seq2Seq model and a conversation\ndataset.\n",
      "content_length": 2221,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "171\nConvolutional\nneural networks\nIn previous chapters, we covered linear layers and RNNs, two main neural network\narchitectures commonly used in NLP. In this chapter, we introduce another import-\nant class of neural networks called convolutional neural networks (CNNs). CNNs have\ndifferent characteristics than RNNs that make them suitable for NLP tasks where\ndetecting linguistic patterns is important, such as text classification.\nThis chapter covers\nSolving text classification by detecting patterns\nUsing convolutional layers to detect patterns \nand produce scores\nUsing pooling layers to aggregate the scores \nproduced by convolution\nBuilding a convolutional neural network (CNN) \nby combining convolution and pooling\nBuilding a CNN-based text classifier using \nAllenNLP\n",
      "content_length": 782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "172\nCHAPTER 7\nConvolutional neural networks\n7.1\nIntroducing convolutional neural networks (CNNs)\nThis section introduces convolutional neural networks (CNNs), another type of neu-\nral network architecture that operates in a different way from how RNNs work. CNNs\nare particularly good at pattern-matching tasks and are increasingly popular in the\nNLP community.\n7.1.1\nRNNs and their shortcomings\nIn chapter 4, we covered sentence classification, which is an NLP task that receives\nsome text as the input and produces a label for it. We also discussed how to use recur-\nrent neural networks (RNNs) for that task. As a refresher, an RNN is a type of neural\nnetwork that has a “loop” in it, which processes the input sequence one element at a\ntime from the beginning until the end. The internal loop variable, which is updated at\nevery step, is called the hidden state. When the RNN finishes processing the entire\nsequence, the hidden state at the final timestep represents the compressed content of\nthe input sequence, which can be used for NLP tasks including sentence classification.\nAlternatively, you can take out the hidden state after every step and use it to assign\nlabels (such as PoS and named entity tags) to individual words. The structure that is\napplied repeatedly in the loop is called a cell. An RNN with a simple multiplication\nand nonlinearity is called a vanilla or an Elman RNN. On the other hand, LSTM and\nGRU-based RNNs use more complicated cells that employ memory and gating.\n RNNs are a powerful tool in modern NLP with a wide range of applications; how-\never, they are not without shortcomings. First, RNNs are slow—they need to scan the\ninput sequence element by element no matter what. Their computational complexity\nis proportional to the length of the input sequences. Second, due to their sequential\nnature, RNNs are hard to parallelize. Think of a multilayer RNN where multiple RNN\nlayers are stacked on top of each other (as shown in figure 7.1). In a naive implemen-\ntation, each layer needs to wait until all the layers below it finish processing the input.\n Third, the RNN structure is simply overkill and inefficient for some tasks. For\nexample, recall the task of detecting grammatical English sentences that we covered\nin chapter 4. In its simplest form, the task is to recognize valid and invalid subject-verb\nagreement in a two-word sentence. If a sentence contains phrases such as “I am” and\n“you are,” it’s grammatical. If it contains “I are” or “you am,” it’s not. In chapter 4, we\nbuilt a simple LSTM-RNN with a nonlinearity to recognize the grammaticality of two-\nword sentences with a vocabulary of four words. But what if you need to classify\nwhether an arbitrary long sentence with a very large vocabulary is grammatical? Sud-\ndenly, this process starts to sound very complex. Your LSTM needs to learn to pick up\nthe signal (subject-verb agreement) from a large amount of noise (all other words and\nphrases that have nothing to do with agreement), while learning to do all this using\nthe update operation that gets repeated for every single element of the input.\n But if you think about it, no matter how long the sentence is or how large the\nvocabulary is, your network’s job should still be quite simple—if the sentence contains\nvalid collocations (such as “I am” and “you are”), it’s grammatical. Otherwise, it’s not.\n",
      "content_length": 3367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "173\nIntroducing convolutional neural networks (CNNs)\nThe task is actually not very far from the “if-then” sentiment analyzer that we saw in\nchapter 1. It is obvious that the structure of LSTM RNNs is overkill for this task, where\nsimple pattern matching over words and phrases would suffice.\n7.1.2\nPattern matching for sentence classification\nIf you look at text classification in general, many tasks can be effectively solved by this\n“pattern matching.” Take spam filtering, for example—if you want to detect spam\nemails, simply look for words and phrases such as “v1agra” and “business opportunity”\nwithout even reading the entire email; it doesn’t matter where these patterns appear. If\nyou want to detect sentiment from movie reviews, detecting positive and negative words\nsuch as “amazing” and “awful” would go a long way. In other words, learning and detect-\ning such local linguistic patterns, regardless of their location, is an effective and effi-\ncient strategy for text-classification tasks, and possibly for other NLP tasks as well.\n In chapter 3, we learned the concept of n-grams—contiguous sequences of one or\nmore words. They are often used in NLP as proxies for more formally defined linguis-\ntic units such as phrases and clauses. If there’s some tool that can wade through a\nlarge amount of noise in text and detect n-grams that serve as signals, it would be a\ngreat fit for text classification.\nLayer 1\nLayer 2\ninit_state()\n...\nupdate\nupdate\nupdate\nupdate\nupdate\n...\nupdate\nupdate\nupdate\nupdate\nupdate\ninit_state()\nstate\nstate\nstate\nstate\nstate\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nv(\"arrow\")\nv(\".\")\nFigure 7.1\nMultilayer RNN\n",
      "content_length": 1640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "174\nCHAPTER 7\nConvolutional neural networks\n7.1.3\nConvolutional neural networks (CNNs)\nConvolutional neural networks, or CNNs, do exactly this. A CNN is a type of neural\nnetwork that involves a mathematical operation called convolution, which, put simply,\ndetects local patterns that are useful for the task at hand. A CNN usually consists of\none or more convolutional layers, which do convolution, and pooling layers, which\nare responsible for aggregating the result of convolution. See figure 7.2 for a diagram.\nSections 7.2 and 7.3 provide some detail of convolutional layers and pooling layers,\nrespectively.\nCNNs, which are inspired by the visual system in the human brain, have been widely\nused for computer vision tasks such as image classification and object detection. In\nrecent years, the use of CNNs has been increasingly popular in NLP, especially for\ntasks such as text classification, sequential labeling, and machine translation.\n7.2\nConvolutional layers\nIn this section, we’ll discuss convolutional layers, the essential part of the CNN archi-\ntecture. The term convolution may sound a bit scary, but at its essence, it’s just pattern\nmatching. We’ll use diagrams and intuitive examples to illustrate how it really works.\nConvolutional neural network (CNN)\nConvolutional\nlayer\n \nPooling\nlayer\n \nv(\"i\")\nv(\"am\")\nv(\"a\")\nv(\"student\")\nv(\".\")\nLinear\nlayer\n \nSoftmax\nDistribution\nover\nlabels \nFigure 7.2\nConvolutional \nneural network\n",
      "content_length": 1443,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "175\nConvolutional layers\n7.2.1\nPattern matching using filters\nConvolutional layers are the most important component in CNNs. As mentioned ear-\nlier, convolutional layers apply a mathematical operation called convolution to input\nvectors and produce output. But what is convolution? Understanding the strict defini-\ntion of convolution requires knowing linear algebra, so we’ll use some analogy and\nconcrete examples to understand it. Imagine holding a rectangular-shaped patch of\ncolored glass with complex patterns (like the stained glass you see in a church) and\nsliding it over the input sequence while looking through it. If the input pattern\nmatches that of the patch, more light goes through the glass and you get larger output\nvalues. If the input pattern does not look like that of the patch or looks the opposite,\nyou get smaller output values. In other words, you are looking for particular patterns\nin the input sequence using a patch of colored glass.\n This analogy is a little bit too vague, so let’s revisit the grammaticality-detection\nexample we used in chapter 4 and see how we’d apply a convolutional layer to the\ntask. To recap, our neural network receives a two-word sentence as an input and needs\nto distinguish grammatical sequences from ungrammatical ones. There are only four\nwords in the vocabulary—“I,” “you,” “am,” and “are,” which are represented by word\nembeddings. Similarly, there are only four possibilities for the input sentence—“I\nam,” “I are,” “you am,” and “you are.” You want the network to produce 1s for the first\nand the last cases and 0s for others. See figure 7.3 for an illustration.\nFigure 7.3\nRecognizing grammatical English sentences\nNow, let’s represent word embeddings as patterns. We’ll draw a black circle for value\n–1 and a white one for 1. Then you can represent each word vector as a pair of two circles\n(see the table on the left in figure 7.3). Similarly, you can represent each two-word sen-\ntence as a small “patch” of two vectors, or four circles (see the table on the right in figure\n7.3). Our task is beginning to look more like a pattern-recognition task, where the net-\nwork needs to learn black-and-white patterns that correspond to grammatical sentences.\n Then, let’s think of a “filter” of the same size (two circles × two circles) that acts as\nthe colored glass we talked about earlier. Each circle of this filter is also either black or\nwhite, corresponding to values –1 and 1. You are going to look at a pattern through\nWord\nI\nyou\nam\nEmbeddings\n[-1, 1]\n[1, -1]\n[-1, -1]\n[1, 1]\nare\nWord embeddings\nPatterns and desired output\nWord 1\nI\nI\nyou\nx1\n[-1, 1]\n[-1, 1]\n[1, -1]\n[1, -1]\nx2\n[-1, -1]\n[1, 1]\n[-1, -1]\n[1, 1]\n1\n0\n0\n1\nyou\nWord 2\nDesired\nPattern\nPattern\nam\nare\nam\nare\n",
      "content_length": 2736,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "176\nCHAPTER 7\nConvolutional neural networks\nthis filter and determine whether the pattern is the one you are looking for. You do this\nby putting the filter over a pattern and counting the number of color matches between\nthe two. For each one of four positions, you get a score of +1 if the colors match (black-\nblack or white-white) and a score of –1 if they don’t (black-white or white-black). Your\nfinal score is the sum of four scores, which varies from –4 (no matches) to +4 (four\nmatches). See figure 7.4 for some examples.\n The score you get varies depending on the pattern and the filter, but as you can see\nin the figure, the score becomes larger when the filter looks similar to the pattern and\nbecomes smaller when the two are not similar. You get the largest score (4) when the\ntwo match exactly and the smallest score (–4) when the two are exactly opposite. The\nfilter acts as a pattern detector against the input. Although this is a very simplified\nexample, it basically shows what a convolutional layer is doing. In convolutional neu-\nral networks, such filters are called kernels.\n In a more general setting, you have an input sentence of arbitrary length, and you\nslide a kernel over the sentence from left to right. See figure 7.5 for an illustration of\nthis. The kernel is repeatedly applied to two consecutive words to produce a sequence\nof scores. Because the kernel we are using here covers two words, it is said to have a\nsize of 2. Also, because there are two dimensions in the input embeddings (which are\ncalled channels), the number of the kernel’s input channels is 2.\n \n \nNOTE\nThe reason embedding dimensions are called channels is because\nCNNs are most commonly applied to computer vision tasks where the input is\noften a 2-D image of different channels that correspond to intensities of dif-\nferent colors (such as red, green, and blue). In computer vision, kernels are\ntwo dimensional and move over the input 2-D images, which is also called 2-D\nconvolution. In NLP, however, kernels are usually one-dimensional (1-D convo-\nlution) and have only one size.\n7.2.2\nRectified linear unit (ReLU)\nAs the next step, let’s think about how we can get the desired output (the Desired col-\numn in figure 7.3) using kernels. How about if we use the filter shown in the second col-\numn of figure 7.4? The kernel, which we’ll call kernel 1 from now on, matches the first\npattern exactly and gives it a high score, while giving zero or negative scores to others.\nFigure 7.6 shows the score (called score 1) when kernel 1 is applied to each pattern.\n×\n=\n2\n×\n= –2\n×\n= –2\n×\n=\n2\n×\n=\n4\n×\n=\n0\n×\n×\n=\n0\n= –4\nPattern Filter Score\nPattern Filter Score\nFigure 7.4\nExamples of convolutional \nfilters\nInput\nKernel\nOutput\n0\n4\n0\n–4\nWidth\nChannels\nFigure 7.5\nSliding a kernel \nover the input sentence\n",
      "content_length": 2800,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "177\nConvolutional layers\n \nLet’s forget the magnitude of the scores for now and focus on their signs (positive and\nnegative). The signs for the first three patterns match between Score 1 and Desired,\nbut not for the last pattern. To score it correctly—that is, to give it a positive score—\nyou need to use another filter that matches the last pattern exactly. Let’s call this\nkernel 2. Figure 7.7 shows the score (called score 2) when kernel 2 is applied to each\npattern.\n Kernel 2 can give correct scores that match the signs of the desired ones for the\nlast three patterns, but not for the first one. But if you observe figures 7.6 and 7.7 care-\nfully, it looks like you could get closer to the desired scores if there was a way to some-\nhow disregard the output when a kernel gives negative scores and then combine the\nscores from multiple kernels. \n Let’s think of a function that clamps any negative input to zero while passing any\npositive values through unchanged. In Python, this function can be written as follows:\ndef f(x):\n    if x >= 0:\n        return x\n    else:\n        return 0\nor even simpler \ndef f(x):\n    return max(0, x)\nYou can disregard negative values by applying this function to score 1 and score 2, as\nshown in figures 7.8 and 7.9.\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n4\n0\n0\n-4\nScore 1\nPattern\nKernel 1\nam\nare\nam\nare\nFigure 7.6\nApplying kernel 1 to patterns\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n-4\n0\n0\n4\nScore 2\nPattern\nKernel 2\nam\nare\nam\nare\nFigure 7.7\nApplying kernel 2 to patterns\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n4\n0\n0\n-4\nScore 1\n4\n0\n0\n0\nf(Score 1)\nPattern\nKernel 1\nam\nare\nam\nare\nFigure 7.8\nApplying \nReLU to score 1\n",
      "content_length": 1666,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "178\nCHAPTER 7\nConvolutional neural networks\nThis function, which is called a rectified linear unit, or ReLU (pronounced “rel-you”), is\none of the simplest yet most commonly used activation functions in deep learning. It\nis often used with a convolutional layer, and although it is very simple (all it does is just\nclamp negative values to zero), it is still an activation function that enables neural net-\nworks to learn complex nonlinear functions (see chapter 4 for why nonlinear activa-\ntion functions are important). It also has favorable mathematical properties that make\nit easier to optimize the network, although the theoretical details are beyond the\nscope of this book.\n7.2.3\nCombining scores\nIf you look at both figures 7.8 and 7.9, the “clamped” scores—shown in the f(Score 1)\nand f(Score 2) columns—capture the desired scores at least partially. All you need to\ndo is combine them together (by summing) and adjust the range (by dividing by 4).\nFigure 7.10 shows the result of this.\nFigure 7.10\nCombining the results from two kernels\nAfter combining, the scores match the desired outcomes exactly. All we did so far was\ndesign kernels that match the patterns we want to detect and then simply combine the\nscores. Compare this to the RNN example we worked on in section 4.1.3, where we\nneeded to use some complicated numeric computation to derive the parameters.\nHopefully this example is enough to show you how simple and powerful CNNs can be\nfor text classification!\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n-4\n0\n0\n4\nScore 2\n0\n0\n0\n4\nf(Score 2)\nPattern\nKernel 2\nam\nare\nam\nare\nFigure 7.9\nApplying \nReLU to score 2\nWord 1\nI\nI\nyou\n1\n0\n0\n1\nyou\nWord 2\nDesired\n1\n0\n0\n1\nCombined\n4\n0\n0\n0\nf(Score 1)\n0\n(4 + 0) / 4 = 1\n0\n0\n4\nf(Score 2)\nKernel 2\nPattern\nKernel 1\nam\nare\nam\nare\n",
      "content_length": 1779,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "179\nPooling layers\n The example we worked on in this section is simply for introducing the basic con-\ncepts of CNNs, so we cut many corners. First, in practice, patterns and kernels are not\njust black and white but contain real-valued numbers. The score after applying a ker-\nnel to a pattern is obtained not by counting color matches but through a mathemati-\ncal operation called inner product, which captures the similarity between the two.\nSecond, the scores produced by kernels aren’t combined by some arbitrary operation\n(like we did in this section) but usually by a linear layer (see section 3.4.3), which can\nlearn a linear transformation against the input to produce the output. Finally, kernels\nand the weights (magic constants w and b) in the final linear layer are all trainable\nparameters of a CNN, meaning that their values are adjusted so that the CNN can pro-\nduce the desired scores.\n7.3\nPooling layers\nIn the previous section, we assumed that the input is just a\ncombination of two words—subjects and verbs—although\nin practice, the input to a CNN can be of arbitrary length.\nYour CNN needs to not only detect patterns but also find\nthem in a potentially large amount of noise in the input. As\nwe saw in section 7.2, you slide a kernel over the sentence\nfrom left to right, and the kernel is repeatedly applied to\ntwo consecutive words to produce a sequence of scores.\nThe remaining question is what to do with these produced\nscores. Specifically, what operation should we use in the “?”\nposition in figure 7.11 to derive the desired score? This\noperation needs to have some properties—it must be\nsomething that can be applied to an arbitrarily large num-\nber of scores, because the sentence can be very long. It also\nneeds to aggregate the scores in a way that is agnostic to\nwhere the target pattern (word embeddings for “I am”) is\nin the input sentence. Can you figure out the answer?\n The simplest thing you can do to aggregate the scores is to take their maximum.\nBecause the largest score in figure 7.11 is 4, it will become the output of this layer.\nThis aggregation operation is called pooling, and the neural network substructure that\ndoes pooling is called a pooling layer. You can also do other types of mathematical oper-\nations that do aggregation, such as taking the average, although taking the maximum\n(called max pooling) is most commonly used.\n The pooled score will be fed to a linear layer, optionally combined with scores\nfrom other kernels, and used as a predicted score. This entire process is illustrated in\nfigure 7.12. Now we have a fully functional CNN!\n As with other neural networks we’ve seen so far, the output from the linear layer is\nfed to softmax to produce a probability distribution over labels. These predicted val-\nues are then compared with the true labels to produce the loss and used for optimiz-\ning the network.\nInput\nKernel\nf(Score)\n0\n4\n0\n?\n1\n0\nv(\"i\")\nv(\"am\")\nDesired\nReLU\nFigure 7.11\nAggregating \nscores to derive the \ndesired score\n",
      "content_length": 2997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "180\nCHAPTER 7\nConvolutional neural networks\n   Before we wrap up, a few more words on\nCNNs: notice that the CNN in figure 7.12\nproduces the same prediction value no mat-\nter where the search pattern (“I am”) is in\nthe input sentence. This is due to the kernel\nlocality as well as the property of the max\npooling layer we just added. In general,\nCNNs produce the same prediction, even if\nthe input sentence is modified by shifting by\na few words. In a technical term, the CNN is\ncalled transformation invariant, which is an\nimportant property of CNNs. This property\nis perhaps more intuitive if you use an\nimage recognition example. An image of a\ncat is still an image of a cat, no matter where\nthe cat is in the image. Similarly, a grammat-\nical English sentence (e.g., “I am a student”)\nis still grammatical, even if the sentence is\ntransformed by adding a few words (e.g.,\n“that’s right”) to the beginning, making it\n“That’s right, I am a student.”\n Because the kernels in a CNN do not depend on each other (unlike RNNs, where\none cell needs to wait until all the preceding cells finish processing the input), CNNs\nare computationally efficient. GPUs can process these kernels in parallel without wait-\ning on other kernels’ output. Due to this property, CNNs are usually faster than RNNs\nof similar size.\n7.4\nCase study: Text classification\nNow that we know the basics of CNNs, in this section we are going to build an NLP\napplication using a CNN and see how it works in practice. As mentioned previously,\none of the most popular and straightforward applications of CNNs in NLP is text clas-\nsification. CNNs are good at detecting patterns (such as salient words and phrases in\ntext), which is also the key to accurate text classification. \n7.4.1\nReview: Text classification\nWe already covered text classification in chapters 2 and 4, but to recap, text classifica-\ntion is a task where an NLP system assigns a label to a given piece of text. If the text is\nan email and the label is whether the email is spam, it’s spam filtering. If the text is a\ndocument (such as a news article) and the label is its topic (such as politics, business,\ntechnology, or sports), it’s called document classification. Many other variants of text\nclassification exist, depending on what the input and the output are. But the task we’ll\nbe working on in this section is again sentiment analysis, where the input is some text\nInput\nv(\"i\")\nv(\"am\")\nKernels\nf(Scores)\nmax(f(Scores))\n0\n0\n0\n4\n0\n4\n0\n0\n4\n4\nReLU\nMax\nLinear\nPooling\nActivation\nSoftmax\nDistribution\nover\nlabels\n \n \nFigure 7.12\nA full CNN with multiple kernels\n",
      "content_length": 2599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "181\nCase study: Text classification\nin which the writer’s subjective opinions are expressed (such as movie and product\nreviews) and the output is the label for the opinion (such as positive or negative, or\neven the number of stars), also called polarity.\n In chapters 2 and 4, we built an NLP system that detected the polarity given a\nmovie review using the Stanford Sentiment Treebank, a dataset containing movie\nreviews and their polarity (strongly positive, positive, neutral, negative, or strongly\nnegative). In this section, we will build the same text classifier but with a CNN instead\nof an RNN. The good news is that we can reuse most of the code we wrote in chapter 2\nin this section—in fact, we need to modify only a few lines of code to swap the RNN\nwith a CNN. This is largely thanks to AllenNLP’s powerful, well-designed abstractions,\nwhich let you work with many modules with different architectures through the com-\nmon interfaces. Let’s see this in action next.\n7.4.2\nUsing CnnEncoder\nRemember that back in section 4.4, we defined our LstmClassifier for text classifi-\ncation as follows:\nclass LstmClassifier(Model):\n    def __init__(self,\n                 embedder: TextFieldEmbedder,\n                 encoder: Seq2VecEncoder,\n                 vocab: Vocabulary,\n                 positive_label: str = '4') -> None:\n    ...\nWe hadn’t put much thought into what this definition meant, but from this construc-\ntor we can see that the model is built on top of two subcomponents: a TextField-\nEmbedder called embedder and a Seq2VecEncoder called encoder, in addition to\nthe vocabulary and the string for the positive label, which are not relevant to our dis-\ncussion here. We discussed word embeddings in chapter 3 at length, although we only\nbriefly touched on the encoder. What does this Seq2VecEncoder actually mean?\n In AllenNLP, Seq2VecEncoder is a class of neural network architectures that take\na sequence of vectors (or tensors in general) and return a single vector. An RNN, one\nexample of this, takes a variable-length input consisting of multiple vectors and con-\nverts it into a single vector at the last cell. We created an instance of Seq2VecEncoder\nbased on an LSTM-RNN using the following code:\nencoder = PytorchSeq2VecWrapper(\n    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\nBut as long as your component has the same input and output specifications, you can\nuse any neural network architecture as a Seq2VecEncoder. In programming lan-\nguage, Seq2VecEncoder is analogous to an interface in Java (and in many other\nlanguages)—interfaces define what your class looks like and what it does, but they do\nnot care about how your class does it. In fact, your model can do something as simple\n",
      "content_length": 2732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "182\nCHAPTER 7\nConvolutional neural networks\nas just summing up all the input vectors to produce the output, without any complex\ntransformations such as nonlinearities. This is, in fact, what BagOfEmbeddings-\nEncoder—one of the Seq2VecEncoders implemented in AllenNLP—does.\n Next, we use a CNN to “squash” a sequence of vectors into a single vector. A CNN-\nbased Seq2VecEncoder is implemented as CnnEncoder in AllenNLP, which can be\ninstantiated as follows:\nencoder = CnnEncoder(\n    embedding_dim=EMBEDDING_DIM,\n    num_filters=8,\n    ngram_filter_sizes=(2, 3, 4, 5))\nIn this example, embedding_dim specifies the dimensionality of the input embed-\ndings. The second argument, num_filters, tells how many filters (or kernels, as\nexplained in section 7.2.1) will be used per n-gram. The final argument, ngram_\nfilter_sizes, specifies the list of n-gram sizes, which are the sizes of these kernels.\nHere, we are using n-gram sizes of 2, 3, 4, and 5, meaning there are 8 kernels for\nbigrams, 8 kernels for trigrams, and so on, up to 5-grams. In total, this CNN can learn\n32 different kernels to detect patterns. CnnEncoder runs these results from the ker-\nnels through a max pooling layer and comes up with a single vector that summarizes\nthe input.\n The rest of the training pipeline looks almost identical to the LSTM version we\nsaw in chapter 2. The entire code is available on Google Colab (http://www.realworld\nnlpbook.com/ch7.html#cnn-nb). There is one caveat: because some n-gram filters\nhave a wide shape (e.g., 4- and 5-grams), you need to make sure that each text field is\nat least that long, even when the original text is short (e.g., just one or two words).\nYou need to know how batching and padding work in AllenNLP (which we’ll cover in\nchapter 10) to fully understand how to deal with this, but in a nutshell, you need to\nspecify the token_min_padding_length parameter when initializing the token\nindexer as follows:\ntoken_indexer = SingleIdTokenIndexer(token_min_padding_length=5)\nreader = StanfordSentimentTreeBankDatasetReader(\n    token_indexers={'tokens': token_indexer})\n7.4.3\nTraining and running the classifier\nWhen you run the script, you’ll see something like the following log output at the end\nof the training: \n{'best_epoch': 1,\n 'best_validation_accuracy': 0.40236148955495005,\n 'best_validation_f1_measure': 0.37362638115882874,\n 'best_validation_loss': 1.346440097263881,\n 'best_validation_precision': 0.4722222089767456,\n 'best_validation_recall': 0.30909091234207153,\n 'epoch': 10,\n 'peak_cpu_memory_MB': 601.656,\n 'training_accuracy': 0.993562734082397,\n",
      "content_length": 2585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "183\nSummary\n 'training_cpu_memory_MB': 601.656,\n 'training_duration': '0:01:10.138277',\n 'training_epochs': 10,\n 'training_f1_measure': 0.994552493095398,\n 'training_loss': 0.03471498479299275,\n 'training_precision': 0.9968798756599426,\n 'training_recall': 0.9922360181808472,\n 'training_start_epoch': 0,\n 'validation_accuracy': 0.35149863760217986,\n 'validation_f1_measure': 0.376996785402298,\n 'validation_loss': 3.045241366113935,\n 'validation_precision': 0.3986486494541168,\n 'validation_recall': 0.35757574439048767}\nThis means that the training accuracy reaches ~99%, whereas the validation accuracy\ntops around 40%. Again, this is a typical symptom of overfitting, where your model is\nso powerful that it fits the training data well, but it doesn’t generalize to the validation\nand test datasets as well. Our CNN has many filters that can remember salient patterns\nin the training data, but these patterns are not necessarily the ones that help predict\nthe labels for the validation instances. We are not worried too much about overfitting\nin this chapter. See chapter 10 for common techniques for avoiding overfitting.\n If you want to make predictions for new instances, you can use the same Predic-\ntor as we did in chapter 2. Predictors in AllenNLP are a thin wrapper around your\ntrained model, which take care of formatting the input and output in a JSON format\nand feeding the instance to the model. You can use the following snippet to make pre-\ndictions using your trained CNN model:\npredictor = SentenceClassifierPredictor(model, dataset_reader=reader)\nlogits = predictor.predict('This is the best movie ever!')['logits']\nlabel_id = np.argmax(logits)\nprint(model.vocab.get_token_from_index(label_id, 'labels'))\nSummary\nCNNs use filters called kernels and an operation called convolution to detect\nlocal linguistic patterns in the input.\nAn activation function called ReLU, which clamps negative values to zero, is\nused with convolution layers.\nCNNs then use pooling layers to aggregate the result from the convolutional\nlayer.\nCNN prediction is transformation invariant, meaning it remains unchanged\neven after linear modification of the input.\nYou can use a CNN-based encoder as a Seq2VecEncoder in AllenNLP by mod-\nifying a few lines of code of your text classifier.\n",
      "content_length": 2289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "184\nAttention and Transformer\nOur focus so far in this book has been recurrent neural networks (RNNs), which are\na powerful model that can be applied to various NLP tasks such as sentiment analysis,\nnamed entity recognition, and machine translation. In this chapter, we will intro-\nduce an even more powerful model—the Transformer1—a new type of encoder-\ndecoder neural network architecture based on the concept of self-attention. It is\nwithout a doubt the most important NLP model since it appeared in 2017. Not only\nThis chapter covers\nUsing attention to produce summaries of the \ninput and improve the quality of Seq2Seq models\nReplacing RNN-style loops with self-attention, a \nmechanism for the input to summarize itself \nImproving machine translation systems with the \nTransformer model\nBuilding a high-quality spell-checker using the \nTransformer model and publicly available datasets\n1 Vaswani et al., “Attention Is All You Need,” (2017). https://arxiv.org/abs/1706.03762.\n",
      "content_length": 984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "185\nWhat is attention?\nis it a powerful model itself (for machine translation and various Seq2Seq tasks, for\nexample), but it is also used as the underlying architecture that powers numerous mod-\nern NLP pretrained models, including GPT-2 (section 8.4.3) and BERT (section 9.2).\nThe developments in modern NLP since 2017 can be best summarized as “the era of the\nTransformer.”\n In this chapter, we start with attention, a mechanism that made a breakthrough in\nmachine translation, then move on to introducing self-attention, the concept that\nforms the foundation of the Transformer model. We will build two NLP applications—\na Spanish-to-English machine translator and a high-quality spell-checker—and learn\nhow to apply the Transformer model to your everyday applications. As we’ll see later,\nthe Transformer models can improve the quality of NLP systems over RNNs by a large\nmargin and achieve almost human-level performance in some tasks, such as translation\nand generation. \n8.1\nWhat is attention?\nIn chapter 6, we covered Seq2Seq models—NLP models that transform one sequence\nto another using an encoder and a decoder. Seq2Seq is a versatile and powerful para-\ndigm with many applications, although the “vanilla” Seq2Seq models are not without\nlimitation. In this section, we discuss the Seq2Seq models’ bottleneck and motivate\nthe use of an attention mechanism.\n8.1.1\nLimitation of vanilla Seq2Seq models\nLet’s remind ourselves how Seq2Seq models work. Seq2Seq models consist of an\nencoder and a decoder. The decoder takes a sequence of tokens in the source lan-\nguage and runs it through an RNN, which produces a fixed-length vector at the end.\nThis fixed-length vector is a representation of the input sentence. The decoder, which\nis another RNN, takes this vector and produces a sequence in the target language,\ntoken by token. Figure 8.1 illustrates how Spanish sentences are translated into\nEnglish with a vanilla Seq2Seq model.\nFigure 8.1\nA bottleneck in a vanilla Seq2Seq model\nMaria\nno\nbaba\nverde\n.\n...\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nEncoder\nDecoder\nSentence\nrepresentation\nBottleneck!\n",
      "content_length": 2119,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "186\nCHAPTER 8\nAttention and Transformer\nThis Seq2Seq architecture is quite simple and powerful, but it is known that its vanilla\nversion (shown in figure 8.1) does not translate sentences as well as other traditional\nmachine translation algorithms (such as phrase-based statistical machine translation\nmodels). You may be able to guess why this is the case if you look at its structure\ncarefully—its encoder is trying to “compress” all the information in the source sen-\ntence into the sentence representation, which is a vector of some fixed length (e.g.,\n256 floating-point numbers), and the decoder is trying to restore the entire target sen-\ntence just from that vector. The size of the vector is fixed no matter how long (or how\nshort) the source sentence is. The intermediate vector is a huge bottleneck. If you\nthink of how humans actually translate between languages, this sounds quite difficult\nand somewhat unusual. Professional translators do not just read the source sentence\nand write down its translation in one breath. They refer to the source sentences as\nmany times as necessary to translate the relevant parts in the target sentence.\n Compressing all the information into one vector may (and does) work for short\nsentences, as we’ll see later in section 8.2.2, but it becomes increasingly difficult as the\nsentences get longer and longer. Studies have shown that the translation quality of a\nvanilla Seq2Seq model gets worse as the sentence gets longer.2 \n8.1.2\nAttention mechanism\nInstead of relying on a single, fixed-length vector to represent all the information in a\nsentence, the decoder would have a much easier time if there was a mechanism where\nit can refer to some specific part of the encoder as it generates the target tokens. This\nis similar to how human translators (the decoder) reference the source sentence (the\nencoder) as needed. \n This can be achieved by using attention, which is a mechanism in neural networks\nthat focuses on a specific part of the input and computes its context-dependent sum-\nmary. It is like having some sort of key-value store that contains all of the input’s infor-\nmation and then looking it up with a query (the current context). The stored values\nare not just a single vector but usually a list of vectors, one for each token, associated\nwith corresponding keys. This effectively increases the size of the “memory” the\ndecoder can refer to when it’s making a prediction.\n Before we discuss how the attention mechanism works for Seq2Seq models, let’s\nsee it in action in a general form. Figure 8.2 illustrates a generic attention mechanism\nwith the following features: \n1\nThe inputs to an attention mechanism are the values and their associated keys.\nThe input values can take many different forms, but in NLP, they are almost\nalways lists of vectors. For Seq2Seq models, the keys and values here are the hid-\nden states of the encoder, which represent token-by-token encoding of the\ninput sentence. \n2 Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate,” (2014). https://\narxiv.org/abs/1409.0473.\n",
      "content_length": 3100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "187\nSequence-to-sequence with attention\nFigure 8.2\nUsing an attention mechanism to summarize the input\n2\nEach key associated with a value is compared against the query using an atten-\ntion function f. By applying f to the query and each one of the keys, you get a\nset of scores, one per key-value pair, which are then normalized to obtain a set\nof attention weights. The specific function f depends on the architecture\n(more on this later). For Seq2Seq models, this gives you a distribution over the\ninput tokens. The more relevant an input token is, the larger the weight it gets.\n3\nThe input values are weighted by their corresponding weights obtained in step 2\nand summed up to compute the final summary vector. For Seq2Seq models, this\nsummary vector is appended to the decoder hidden states to aid the translation\nprocess.\nBecause of step 3, the output of an attention mechanism is always a weighted sum of\nthe input vectors, but how they are weighted is determined by the attention weights,\nwhich are in turn are calculated from the keys and the query. In other words, what an\nattention mechanism computes is a context (query)-dependent summary of the input. Down-\nstream components of a neural network (e.g., the decoder of an RNN-based Seq2Seq\nmodel, or the upper layers of a Transformer model) use this summary to further pro-\ncess the input.\n In the following sections, we will learn the two most commonly used types of atten-\ntion mechanisms in NLP—encoder-decoder attention (also called cross-attention; used\nin both RNN-based Seq2Seq models and the Transformer) and self-attention (used in\nthe Transformer).\n8.2\nSequence-to-sequence with attention\nIn this section, we’ll learn how the attention mechanism is applied to an RNN-based\nSeq2Seq model for which the attention mechanism was first invented. We’ll study how\nit works with specific examples, and then we’ll experiment with Seq2Seq models with\nAttention\nweights\nQuery\n50%\n10%\n10%\n30%\nKeys\nValues\nValues\n× weights\nSum\nf\nf\nf\nf\n",
      "content_length": 1994,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "188\nCHAPTER 8\nAttention and Transformer\nand without the attention mechanism using fairseq to observe how it affects the\ntranslation quality.\n8.2.1\nEncoder-decoder attention\nAs we saw earlier, attention is a mechanism for creating a summary of the input under\na specific context. We used a key-value store and a query as an analogy for how it\nworks. Let’s see how an attention mechanism is used with RNN-based Seq2Seq models\nusing the concrete examples that follow.  \n Figure 8.3 illustrates a Seq2Seq model with attention. It looks complex at first, but\nit is just an RNN-based Seq2Seq model with some extra “things” added on top of the\nencoder (the lightly shaded box in top left corner of the figure). If you ignore what’s\ninside and see it as a black box, all it does is simply take a query and return some sort\nof summary created from the input. The way it computes this summary is just a variant\nof the generic form of attention we covered in section 8.1.2. It proceeds as follows:\n1\nThe input to the attention mechanism is the list of hidden states computed by\nthe encoder. These hidden states are used as both keys and values (i.e., the keys\nand the values are identical). The encoder hidden state at a certain token (e.g.,\nat token “no”) reflects the information about that token and all the tokens lead-\ning up to it (if the RNN is unidirectional) or the entire sentence (if the RNN is\nbidirectional).\n2\nLet’s say you finished decoding up to “Mary did.” The hidden states of the\ndecoder at that point are used as the query, which is compared against every key\nusing function f. This produces a list of attention scores, one per each key-value\nMaria\nno\ndaba\nverde\n.\n...\n<START>\nMary\ndid\nwitch\n.\nMary\ndid\n...\nnot\n.\n<END>\nContext\nvector\nKeys\nValues\nAttention\nweights\nAttention mechanism\nf\nf\nf\nf\nf\nSoftmax\nWeighted\nsum\nQuery\nFigure 8.3\nAdding an attention mechanism to an RNN-based Seq2Seq model (the lightly shaded box)\n",
      "content_length": 1926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "189\nSequence-to-sequence with attention\npair. These scores determine which part of the input the decoder should attend\nto when it’s trying to generate a word that follows “Mary did.”\n3\nThese scores are converted to a probability distribution (a set of positive values\nthat sum to one), which is used to determine which vectors should get the most\nattention. The return value from this attention mechanism is the sum of all val-\nues, weighted by the attention scores after normalizing with softmax.\nYou may be wondering what the attention function f looks like. A couple of variants of\nf are possible, depending on how it computes the attention scores between the key\nand the query, but these details do not matter much here. One thing to note is that in\nthe original paper proposing the attention mechanism,3 the authors used a “mini”\nneural network to calculate attention scores from the key and the query.\n This “mini” network-based attention function is not something you just plug in to\nan RNN model post hoc and expect it to work. It is optimized as part of the entire\nnetwork—that is, as the entire network gets optimized by minimizing the loss func-\ntion, the attention mechanism also gets better at generating summaries because doing\nso well also helps the decoder generate better translation and lower the loss function.\nIn other words, the entire network, including the attention mechanism, is trained end\nto end. This usually means that, as the network is optimized, the attention mechanism\nstarts to learn to focus only on the relevant part of the input, which is usually where\nthe target tokens are aligned with the source tokens. In other words, attention is calcu-\nlating some sort of “soft” word alignment between the source and the target tokens.\n8.2.2\nBuilding a Seq2Seq machine translation with attention\nIn section 6.3, we built our first machine translation (MT) system using fairseq, an\nNMT toolkit developed by Facebook. Using the parallel dataset from Tatoeba, we built\nan LSTM-based Seq2Seq model to translate Spanish sentences into English.\n In this section, we are going to experiment with a Seq2Seq machine translation sys-\ntem and see how attention affects the translation quality. We assume that you’ve already\ngone through the steps we took when we built the MT system by downloading the data-\nset and running the fairseq-preprocess and fairseq-train commands (section\n6.3). After that, you ran the fairseq-interactive command to interactively trans-\nlate Spanish sentences into English. You might have noticed that the translation you get\nfrom this MT system that took you just 30 minutes to build was actually decent. In fact,\nthe model architecture we used (--arch lstm) has an attention mechanism built in by\ndefault. Notice when you ran the following fairseq-train command\n$ fairseq-train \\\n    data/mt-bin \\\n    --arch lstm \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam \\\n3 Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate,” (2014). https://\narxiv.org/abs/1409.0473.\n",
      "content_length": 3061,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "190\nCHAPTER 8\nAttention and Transformer\n    --lr 1.0e-3 \\\n    --max-tokens 4096 \\\n    --save-dir data/mt-ckpt\nyou should have seen the dump of what your model looks like in your terminal as\nfollows:\n...\nLSTMModel(\n  (encoder): LSTMEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (lstm): LSTM(512, 512)\n  )\n  (decoder): LSTMDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n    (layers): ModuleList(\n      (0): LSTMCell(1024, 512)\n    )\n    (attention): AttentionLayer(\n      (input_proj): Linear(in_features=512, out_features=512, bias=False)\n      (output_proj): Linear(in_features=1024, out_features=512, bias=False)\n    )\n  )\n)\n...\nThis tells you that your model has an encoder and a decoder, but the decoder also has\na component called attention (which is of type AttentionLayer), shown in bold in\nthe code snippet. This is exactly the “mini-network” that we covered in section 8.2.1. \n Now let’s train the same model, but without attention. You can add --decoder-\nattention 0 to fairseq-train to disable the attention mechanism, while keeping\neverything else the same, as shown here:\n$ fairseq-train \\\n      data/mt-bin \\\n      --arch lstm \\\n      --decoder-attention 0 \\\n      --share-decoder-input-output-embed \\\n      --optimizer adam \\\n      --lr 1.0e-3 \\\n      --max-tokens 4096 \\\n      --save-dir data/mt-ckpt-no-attn\nWhen you run this, you’ll see a similar dump, shown next, that shows the architecture\nof the model but without attention:\nLSTMModel(\n  (encoder): LSTMEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (lstm): LSTM(512, 512)\n  )\n  (decoder): LSTMDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n",
      "content_length": 1697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "191\nSequence-to-sequence with attention\n    (layers): ModuleList(\n      (0): LSTMCell(1024, 512)\n    )\n  )\n)\nAs we saw in section 6.3.2, the training process alternates between training and valida-\ntion. In the training phase, the parameters of the neural network are optimized by the\noptimizer. In the validation phase, these parameters are fixed, and the model is run on\na held-out portion of the dataset called the validation set. In addition to making sure the\ntraining loss decreases, you should be looking at the validation loss during training,\nbecause it better represents how well the model generalizes outside the training data. \n During this experiment, you should observe that the lowest validation loss\nachieved by the attention model is around 1.727, whereas that for the attention-less\nmodel is around 2.243. Lower loss values mean the model is fitting the dataset better,\nso this indicates the attention is helping improve the translation. Let’s see if this is\nactually the case. As we’ve done in section 6.3.2, you can generate translations interac-\ntively by running the following fairseq-interactive command:\n$ fairseq-interactive \\\n    data/mt-bin \\\n    --path data/mt-ckpt/checkpoint_best.pt \\\n    --beam 5 \\\n    --source-lang es \\\n    --target-lang en\nIn table 8.1, we compare the translations generated by the model with and without\nattention. The translations you get from the attention-based model are the same as\nthe ones we saw in section 6.3.3. Notice that the translations you get from the\nattention-less model are a lot worse than those from the attention model. If you look\nat the translations for “¿Hay habitaciones libres?” and “Maria no daba una bofetada a\nla bruja verde,” you see unfamiliar tokens “<unk>” (for “unknown”) in them. What’s\nhappening here?\nTable 8.1\nTranslation generated by the model with and without attention\nSpanish (input)\nWith attention\nWithout attention\n¡Buenos días!\nGood morning!\nGood morning!\n¡Hola!\nHi!\nHi!\n¿Dónde está el baño?\nWhere’s the restroom?\nWhere’s the toilet?\n¿Hay habitaciones libres?\nIs there free rooms?\nAre there <unk> rooms?\n¿Acepta tarjeta de crédito?\nDo you accept credit card?\nDo you accept credit card?\nLa cuenta, por favor.\nThe bill, please.\nCheck, please.\nMaria no daba una bofetada a la \nbruja verde.\nMaria didn’t give the green witch.\nMary wasn’t a <unk> of the \npants.\n",
      "content_length": 2354,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "192\nCHAPTER 8\nAttention and Transformer\nThese are special tokens that are assigned to out-of-vocabulary (OOV) words. We\ntouched upon OOV words in section 3.6.1 (when we introduced the concept of sub-\nwords used for FastText). Most NLP applications operate within a fixed vocabulary,\nand whenever they encounter or try to produce words that are outside that pre-\ndefined set, the words are replaced with a special token, <unk>. This is akin to a spe-\ncial value (such as None in Python) returned when a method doesn’t know what to do\nwith the input. Because these sentences contain certain words (I suspect they are\n“libres” and “bofetada”), the Seq2Seq model without attention, whose memory is lim-\nited, didn’t know what to do with them and simply fell back on a safest thing to do,\nwhich is to produce a generic, catch-all symbol, <unk>. On the other hand, you can\nsee that attention prevents the system from producing these symbols and helps\nimprove the overall quality of the produced translations. \n8.3\nTransformer and self-attention\nIn this section, we are going to learn how the Transformer model works and, specifi-\ncally, how it generates high-quality translations by using a new mechanism called self-\nattention. Self-attention creates a summary of the entire input, but it does this for each\ntoken using the token as the context. \n8.3.1\nSelf-attention\nAs we’ve seen before, attention is a mechanism that creates a context-dependent sum-\nmary of the input. For RNN-based Seq2Seq models, the input is the encoder hidden\nstates, whereas the context is the decoder hidden states. The core idea of the Trans-\nformer, self-attention, also creates a summary of the input, except for one key\ndifference—the context in which the summary is created is also the input itself. See\nfigure 8.4 for a simplified illustration of a self-attention mechanism.\n Why is this a good thing? Why does it\neven work? As we discussed in chapter 4,\nRNNs can also create a summary of the\ninput by looping over the input tokens\nwhile updating an internal variable (hid-\nden states). This works—we previously saw\nthat RNNs can generate good translations\nwhen combined with attention, but they\nhave one critical issue: because RNNs pro-\ncess the input sequentially, it becomes pro-\ngressively more difficult to deal with long-range dependencies between tokens as the\nsentence gets longer. \n Let’s look at a concrete example. If the input sentence is “The Law will never be per-\nfect, but its application should be just,” understanding what the pronoun “its” refers to\n(“The Law”) is important for understanding what the sentence means and for any sub-\nsequent tasks (such as translating the sentence accurately). However, if you use an RNN\nMaria\nno\ndaba\nEmbeddings\n(input)\nSummaries\n(output)\nFigure 8.4\nSelf-attention transforms the \ninput into summaries.\n",
      "content_length": 2835,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "193\nTransformer and self-attention\nto encode this sentence, to learn this coreference relationship, the RNN needs to learn\nto remember the noun “The Law” in the hidden states first, then wait until the loop\nencounters the target pronoun (“its”) while learning to ignore everything unrelated in\nbetween. This sounds like a complicated trick for a neural network to learn. \n But things shouldn’t be that complicated. Singular possessive pronouns like “its”\nusually refer to their nearest singular nouns that appear before them, regardless of\nthe words in between, so simple rules like “replace it with the nearest noun that\nappeared before” will suffice. In other words, such “random access” is better suited in\nthis situation than “sequential access” is. Self-attention is better at learning such long-\nrange dependencies, as we’ll see later.\n Let’s walk through how self-attention works with an example. Let’s assume we are\ntranslating Spanish into English and would like to encode the first few words, “Maria\nno daba,” in the input sentence. Let’s also focus on one specific token, “no,” and how\nits embeddings are computed from the entire input. The first step is to compare the\ntarget token against all tokens in the input. Self-attention does this by converting the\ntarget into a query by using projection WQ as well as converting all the tokens into keys\nusing projection WK and computing attention weights using function f. The attention\nweights computed by f are normalized and converted to a probability distribution by\nthe softmax function. Figure 8.5 illustrates these steps where attention weights are\ncomputed. As with the encoder-decoder attention mechanism we covered in section\n8.2.1, these weights determine how to “mix” values we obtained from the input\ntokens. For words like “its,” we expect that the weight will be higher for related words\nsuch as “Law” in the example shown earlier. \nIn the next step, the vector corresponding to each input token is converted to a value\nvector by projection WV. Each projected value is weighted by the corresponding atten-\ntion weight and is summed up to produce a summary vector. See figure 8.6 for an\nillustration.\n This would be it if this were the “regular” encoder-decoder attention mechanism.\nYou need only one summary vector per each token during decoding. However, one key\ndifference between encoder-decoder attention and self-attention is the latter repeats\nMaria\nno\ndaba\nMaria\ndaba\nf\nf\nf\nEmbeddings\nKeys\nQuery\nAttention weights\nEmbeddings\nno(target)\nWK\nWK\nWQ\nWK\nFigure 8.5\nComputing attention \nweights from keys and queries\n",
      "content_length": 2589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "194\nCHAPTER 8\nAttention and Transformer\n \nthis process for every single token in the input. As shown in figure 8.7, this produces a\nnew set of embeddings for the input, one for each token.\n Each summary produced by self-attention takes all the tokens in the input\nsequence into consideration, but with different weights. It is, therefore, straightfor-\nward for words like “its” to incorporate some information from related words, such as\n“The Law,” no matter how far apart these two words are. Using an analogy, self-atten-\ntion produces summaries through random access over the input. This is in contrast to\nRNNs, which allow only sequential access over the input, and is one of the key reasons\nwhy the Transformer is such a powerful model for encoding and decoding natural\nlanguage text.\n We need to cover one final piece of detail to fully understand self-attention. As it is,\nthe self-attention mechanism illustrated previously can use only one aspect of the input\nsequence to generate summaries. For example, if you want self-attention to learn which\nword each pronoun refers to, it can do that—but you may also want to “mix in” infor-\nmation from other words based on some other linguistic aspects. For example, you may\nwant to refer to some other words that the pronoun modifies (“applications,” in this\ncase). The solution is to have multiple sets of keys, values, and queries per token and\ncompute multiple sets of attention weights to “mix” values that focus on different\naspects of the input. The final embeddings are a combination of summaries generated\nthis way. This mechanism is called multihead self-attention (figure 8.8).\n You would need to learn some additional details if you were to fully understand how\na Transformer layer works, but this section has covered the most important concepts.\nIf you are interested in more details, check out The Illustrated Transformer (http://\njalammar.github.io/illustrated-transformer/), a well-written guide for understanding\nMaria\nno\ndaba\nMaria\nno\ndaba\nf\nEmbeddings\nKeys\nQuery\nAttention weights\nEmbeddings\nValues\nWeighted values\nNew embeddings\nWV\nWV\nWQ\nWV\nWK\nWK\nWK\nf\nf\nFigure 8.6\nCalculating the sum of all values \nweighted by attention weights\nMaria\nno\ndaba\nMaria\nno\ndaba\nEmbeddings\nKeys\nQueries\nEmbeddings\nNew embeddings\nFigure 8.7\nProducing summaries for the entire \ninput sequence (details are omitted)\n",
      "content_length": 2363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "195\nTransformer and self-attention\nthe Transformer model with easy-to-understand illustrations. Also, if you are interested\nin implementing the Transformer model from scratch in Python, check out “The Anno-\ntated Transformer” (http://nlp.seas.harvard.edu/2018/04/03/attention.html).\n8.3.2\nTransformer\nThe Transformer model doesn’t just use a sin-\ngle step of self-attention to encode or decode\nnatural language text. It applies self-attention\nrepeatedly to the inputs to gradually trans-\nform them. As with multilayer RNNs, the\nTransformer also groups a series of transfor-\nmation operations into a layer and applies it\nrepeatedly. Figure 8.9 shows one layer of the\nTransformer encoder.\n A lot is going on within each layer, and it’s\nnot our goal to explain every bit of its detail—\nyou need to understand only that the multi-\nhead self-attention is at its core, followed by\ntransformation by a feed-forward neural net-\nwork (“FF” in figure 8.9). Residual connec-\ntions and normalization layers are introduced\nf\nf\nf\nEmbeddings\nKeys\nQueries\nAttention weights\nEmbeddings\nValues\nWeighted values\nNew embeddings\nMaria\nno\ndaba\nMaria\nno\ndaba\nf\nf\nf\nFigure 8.8\nMultihead self-attention produces summaries with multiple \nkeys, values, and queries.\nMaria\nno\ndaba\nMultihead self-attention\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nFigure 8.9\nA Transformer encoder \nlayer with self-attention and a feed-\nforward layer\n",
      "content_length": 1434,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "196\nCHAPTER 8\nAttention and Transformer\nto make it easier to train the model, although the details of these operations are out-\nside the scope of this book. The Transformer model applies this layer repeatedly to\ntransform the input from something literal (raw word embeddings) to something\nmore abstract (the “meaning” of the sentence). In the original Transformer paper,\nVaswani et al. used six layers for machine translation, although it is not uncommon for\nlarger models to use 10–20 layers these days.\n At this point, you may have noticed that the self-attention operation is completely\nindependent of positions. In other words, the embedded results of self-attention\nwould be completely identical even if, for example, we flipped the word order\nbetween “Maria” and “daba,” because the operation looks only at the word itself and\nthe aggregated embeddings from other words, regardless of where they are. This is\nobviously very limiting—what a natural language sentence means depends a lot on\nhow its words are ordered. How does the Transformer encode word order, then?\n    The Transformer model solves this prob-\nlem by generating some artificial embed-\ndings that differ from position to position\nand adding them to word embeddings\nbefore they are fed to the layers. These\nembeddings, called positional encoding and\nshown in figure 8.10, are either generated\nby some mathematical function (such as\nsine curves) or learned during training\nper position. This way, the Transformer\ncan distinguish between “Maria” at the\nfirst position and “Maria” at the third posi-\ntion, because they have different posi-\ntional encoding.\n  Figure 8.11 shows the Transformer\ndecoder. Although a lot is going on, make\nsure to notice two important things. First,\nyou’ll notice one extra mechanism called\ncross-attention inserted between the self-\nattention and feed-forward networks. This\ncross-attention mechanism is similar to the encoder-decoder attention mechanism we\ncovered in section 8.2. This works exactly the same as self-attention, except that the\nvalues for the attention come from the encoder, not the decoder, summarizing the\ninformation extracted from the encoder.\n Finally, the Transformer model generates the target sentence in exactly the same\nway as RNN-based Seq2Seq models we’ve previously learned in section 6.4. The\ndecoder is initialized by a special token <START> and produces a probability distribu-\ntion over possible next tokens. From here, you can proceed by choosing the token with\nthe maximum probability (greedy decoding, as shown in section 6.4.3) or keeping a few\ntokens with the highest probability while searching for the path that maximizes the\nMaria\nno\ndaba\nMultihead self-attention\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nFigure 8.10\nAdding positional encoding to the \ninput to represent word order\n",
      "content_length": 2873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "197\nTransformer and self-attention\ntotal score (beam search, as shown in section 6.4.4). In fact, if you look at the Trans-\nformer decoder as a black box, the way it produces the target sequence is exactly the\nsame as RNNs, and you can use the same set of decoding algorithms. In other words, the\ndecoding algorithms covered in section 6.4 are generic ones that are agnostic of the\nunderlying decoder architecture.\n8.3.3\nExperiments\nNow that we know how the Transformer model works, let’s build a machine transla-\ntion system with it. The good news is the sequence-to-sequence toolkit, Fairseq,\nalready supports the Transformer-based models (along with other powerful models),\nwhich can be specified by the --arch transformer option when you train the\nmodel. Assuming that you have already preprocessed the dataset we used to build the\nSpanish-to-English machine translation, you need to tweak only the parameters you\ngive to fairseq-train, as shown next:\nfairseq-train \\\n  data/mt-bin \\\n<START>\nMary\ndid\nMary\ndid\nMasked multihead self-attention\nCross-attention\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nAdd & norm\nFrom\nencoder\nnot\nFigure 8.11\nA Transformer decoder \nlayer with self- and cross-attention\n",
      "content_length": 1273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "198\nCHAPTER 8\nAttention and Transformer\n  --arch transformer \\\n  --share-decoder-input-output-embed \\\n  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n  --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n  --dropout 0.3 --weight-decay 0.0 \\\n  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n  --max-tokens 4096 \\\n  --save-dir data/mt-ckpt-transformer\nNote that this might not even run on your laptop. You really need GPUs to train the\nTransformer models. Also note that training can take hours even with GPUs. See sec-\ntion 11.5 for more information on using GPUs.\n A number of cryptic parameters appear here, but you don’t need to worry about\nthem. You can see the model structure when you run this command. The entire\nmodel dump is quite long, so we are omitting some intermediate layers in listing 8.1.\nIf you look carefully, you’ll see that the structure of the layers corresponds to the fig-\nures we showed earlier.\nTransformerModel(\n  (encoder): TransformerEncoder(\n    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(     \n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)   \n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n      ...\n      (5): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (embed_tokens): Embedding(11416, 512, padding_idx=1)\nListing 8.1\nTransformer model dump from Fairseq\nSelf-attention of \nthe encoder\nFeed-forward\nnetwork of\nthe encoder\n",
      "content_length": 2351,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "199\nTransformer and self-attention\n    (embed_positions): SinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(   \n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n        (encoder_attn): MultiheadAttention(    \n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)    \n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n      ...\n      (5): TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n        (encoder_attn): MultiheadAttention(\n          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_\n         affine=True)\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (final_layer_norm): LayerNorm((512,), eps=1e-05, \nelementwise_affine=True)\n      )\n    )\n  )\n)\nWhen I ran this, the validation loss converges after around epoch 30, at which point\nyou can stop the training. The result I got by translating the same set of Spanish sen-\ntences into English follows:\n¡ Buenos días !\nS-0     ¡ Buenos días !\nH-0     -0.0753164291381836     Good morning !\nP-0     -0.0532 -0.0063 -0.1782 -0.0635\n¡ Hola !\nS-1     ¡ Hola !\nH-1     -0.17134985327720642    Hi !\nP-1     -0.2101 -0.2405 -0.0635\n¿ Dónde está el baño ?\nS-2     ¿ Dónde está el baño ?\nH-2     -0.2670585513114929     Where &apos;s the toilet ?\nP-2     -0.0163 -0.4116 -0.0853 -0.9763 -0.0530 -0.0598\nSelf-attention \nof the decoder\nEncoder-decoder \nof the decoder\nFeed-forward\nnetwork of\nthe decoder\n",
      "content_length": 2253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "200\nCHAPTER 8\nAttention and Transformer\n¿ Hay habitaciones libres ?\nS-3     ¿ Hay habitaciones libres ?\nH-3     -0.26301929354667664    Are there any rooms available ?\nP-3     -0.1617 -0.0503 -0.2078 -1.2516 -0.0567 -0.0532 -0.0598\n¿ Acepta tarjeta de crédito ?\nS-4     ¿ Acepta tarjeta de crédito ?\nH-4     -0.06886537373065948    Do you accept credit card ?\nP-4     -0.0140 -0.0560 -0.0107 -0.0224 -0.2592 -0.0606 -0.0594\nLa cuenta , por favor .\nS-5     La cuenta , por favor .\nH-5     -0.08584468066692352    The bill , please .\nP-5     -0.2542 -0.0057 -0.1013 -0.0335 -0.0617 -0.0587\nMaria no daba una bofetada a la bruja verde .\nS-6     Maria no daba una bofetada a la bruja verde .\nH-6     -0.3688890039920807     Mary didn &apos;t slapped the green witch .\nP-6     -0.2005 -0.5588 -0.0487 -2.0105 -0.2672 -0.0139 -0.0099 -0.1503 -\n0.0602\nYou can see most of these English translations here are almost perfect. It is quite sur-\nprising that the model translated the most difficult sentence (“Maria no daba . . .”)\nalmost perfectly. This is probably enough to convince us that the Transformer is a\npowerful translation model. After its advent, this model became the de facto standard\nin research and commercial machine translation.\n8.4\nTransformer-based language models\nIn section 5.5, we introduced language models, which are statistical models that give a\nprobability to a piece of text. By decomposing text into a sequence of tokens, lan-\nguage models can estimate how “probable” the given text is. In section 5.6, we demon-\nstrated that by leveraging this property, language models can also be used to generate\nnew texts out of thin air!\n The Transformer is a powerful model that achieves impressive results in Seq2Seq\ntasks (such as machine translation), although its architecture can also be used for\nmodeling and generating language. In this section, we learn how to use the Trans-\nformer for modeling language and generating realistic texts.\n8.4.1\nTransformer as a language model\nIn section 5.6, we built a language-generation model based on a character LSTM-\nRNN. To recap, given a prefix (a partial sentence generated so far), the model uses an\nLSTM-based RNN (a neural network with a loop) to produce a probability distribu-\ntion over possible next tokens, as shown in figure 8.12. \n We noted earlier that, by regarding the Transformer decoder as a black box, you can\nuse the same set of decoding algorithms (greedy, beam search, and so on) as we intro-\nduced earlier for RNNs. This is also the case for language generation—by thinking of\nthe neural network as a black box that produces some sort of score given a prefix, you\ncan use the same logic to generate texts, no matter the underlying model. Figure 8.13\nshows how an architecture similar to the Transformer can be used for language gener-\n",
      "content_length": 2813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "201\nTransformer-based language models\nation. Except for a few minor differences (such as lack of cross-attention), the structure\nis almost identical to the Transformer decoder.\n The following snippet shows Python-like pseudocode for generating text with the\nTransformer model. Here, model() is the main function where the model computa-\ntion happens—it takes the tokens, converts them to embeddings, adds positional\n<START>\nT\nT\nCharacter\nembeddings\n \nh\ng\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\nh\ne\n.\n<END>\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nT h e _ q u i c k _ b r … _ d o g .\nFigure 8.12\nGenerating text using an RNN\n<START>\nMary\ndid\nAdd & norm\nAdd & norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nMasked multihead self-attention\nMary\ndid\nnot\nFigure 8.13\nUsing the Transformer \nfor language generation\n",
      "content_length": 882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "202\nCHAPTER 8\nAttention and Transformer\nencoding, and passes them through all the Transformer layers, returning the final hid-\nden states back to the caller. The caller then passes them through a linear layer to con-\nvert them to logits, which in turn get converted to a probability distribution by\nsoftmax:\ndef generate():\n    token = <START>\n    tokens = [<START>]\n    while token != <END>:\n        hidden = model(tokens)\n        probs = softmax(linear(hidden))\n        token = sample(probs)\n        tokens.append(token)\n    return tokens\nIn fact, decoding for Seq2Seq models and language generation with language models\nare very similar tasks, where the output sequence is produced token by token, feeding\nitself back to the network, as shown in the previous code snippet. The only difference\nis that the former has some form of input (the source sentence) whereas the latter\ndoes not (the model feeds itself). These two tasks are also called unconditional and con-\nditional generation, respectively. Figure 8.14 illustrates these three components (net-\nwork, task, and decoding) and how they can be combined to solve a specific problem.\nFigure 8.14\nThree components of language generation and Seq2Seq tasks\nIn the rest of this section, we are going to experiment with some Transformer-based\nlanguage models and generate natural language texts using them. We’ll be using the\ntransformers library (https://huggingface.co/transformers/) developed by Hug-\nging Face, which has become a standard, go-to library for NLP researchers and engi-\nneers working with Transformer models in the past few years. It comes with a number\nof state-of-the-art model implementations including GPT-2 (this section) and BERT\nNetwork\nRNN (LSTM)\nTransformer\nTask\nLanguage model\n(unconditional generatoin)\nSeq2Seq\n(conditional generation)\nDecoding\nSampling\nGreedy decoding\nBeam search\n×\n×\nLanguage generation (section 5.6): \nRNN × Language model × Sampling\nSpanish translator (section 6.3): \nRNN × Seq2Seq × Beam search\nSpanish translator (section 8.3.3): \nTransformer × Seq2Seq × Beam search\nGPT-2 etc. (section 8.4): \nTransformer × Language model × Sampling\n",
      "content_length": 2138,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "203\nTransformer-based language models\n(next chapter), along with pretrained model parameters that you can load and use\nright away. It also provides a simple, consistent interface through which you can inter-\nact with powerful NLP models.\n8.4.2\nTransformer-XL\nIn many cases, you want to load and use pretrained models provided by third parties\n(most often the developer of the model), instead of training them from scratch.\nRecent Transformer models are fairly complex (usually with hundreds of millions of\nparameters) and are trained with huge datasets (tens of gigabytes of text). This would\nrequire GPU resources that only large institutions and tech giants can afford. It is not\ncompletely uncommon that some of these models take days to train, even with more\nthan a dozen GPUs! The good news is the implementation and pretrained model\nparameters for these huge Transformer models are usually made publicly available by\ntheir creators so that anyone can integrate them into their NLP applications.\n In this section, we’ll first check out Transformer-XL, a variant of the Transformer\ndeveloped by the researchers at Google Brain. Because there is no inherent “loop” in\nthe original Transformer model, unlike RNNs, the original Transformer is not good at\ndealing with super-long context. In training language models with the Transformer, you\nfirst split long texts into shorter chunks of, say, 512 words, and feed them to the model\nseparately. This means the model is unable to capture dependencies longer than 512\nwords. Transformer-XL4 addresses this issue by making a few improvements over the\nvanilla Transformer model (“XL” means extra-long). Although the details of these\nchanges are outside the scope of this book, in a nutshell, the model reuses its hidden\nstates from the previous segment, effectively creating a loop that passes information\nbetween different segments of texts. It also improves the positional encoding scheme\nwe touched on earlier to make it easier for the model to deal with longer texts.\n You can install the transformers library just by running pip install trans-\nformers from the command line. The main abstractions you’ll be interacting with are\ntokenizers and models. The tokenizers split a raw string into a sequence of tokens,\nwhereas the model defines the architecture and implements the main logic. The model\nand the pretrained weights usually depend on a specific tokenization scheme, so you\nneed to make sure you are using the tokenizer that is compatible with the model.\n The easiest way to initialize a tokenizer and a model with some specified pre-\ntrained weights is use the AutoTokenizer and AutoModelWithLMHead classes and\ncall their from_pretrained() methods as follows:\nimport torch\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\nmodel = AutoModelWithLMHead.from_pretrained('transfo-xl-wt103')\n4 Dai et al., “Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,” (2019). https://\narxiv.org/abs/1901.02860.\n",
      "content_length": 3049,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "204\nCHAPTER 8\nAttention and Transformer\nThe \nparameter \nto \nfrom_pre-\ntrained() is the name of the\nmodel/pretrained weights. This is a\nTransformer-XL model trained on a\ndataset called wt103 (WikiText103).\n    You may be wondering what this\n“LMHead” part in AutoModelWith-\nLMHead means. An LM (language\nmodel) head is a specific layer added\nto a neural network that converts its\nhidden states to a set of scores that\ndetermine which tokens to generate\nnext. These scores (also called logits)\nare then fed to a softmax layer to\nobtain a probability distribution over\npossible next tokens (figure 8.15).\nWe would like a model with an LM\nhead because we are interested in\ngenerating text by using the Trans-\nformer as a language model. How-\never, depending on the task, you may\nalso want a Transformer model with-\nout an LM head and just want to use\nits hidden states. That’s what we’ll do\nin the next chapter.\n    The next step is to initialize the\nprefix for which you would like your\nlanguage model to write the rest of\nthe story. You can use tokenizer.encode() to convert a string into a list of token\nIDs, which are then converted to a tensor. We’ll also initialize a variable past for cach-\ning the internal states and making the inference faster, as shown next:\ngenerated = tokenizer.encode(\"On our way to the beach\")\ncontext = torch.tensor([generated])\npast = None\nNow you are ready to generate the rest of the text. Notice the next code is similar to\nthe pseudocode we showed earlier. The idea is simple: get the output from the model,\nsample a token using the output, and feed it back to the model. Rinse and repeat.\nfor i in range(100):\n    output = model(context, mems=past)\n    token = sample_token(output.prediction_scores)\n    generated.append(token.item())\n    context = token.view(1, -1)\n    past = output.mems\nLinear\nLinear\nLinear\nSoftmax\n<START>\nMary\ndid\nAdd & Norm\nAdd & Norm\nFF\nFF\nFF\nResidual\nconnection\nResidual\nconnection\nPositional\nencoding\nMasked multihead self-attention\nLM head\nFigure 8.15\nUsing a language model head with the \nTransformer\n",
      "content_length": 2063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "205\nTransformer-based language models\nYou need to do some housekeeping to make the shape of the tensors compatible with\nthe model, which we can ignore for now. The sample_token() method here takes\nthe output of the model, converts it to a probability distribution, and samples a single\ntoken from it. I’m not showing the entire code for the method, but you can check the\nGoogle Colab notebook (http://realworldnlpbook.com/ch8.html#xformer-nb) for\nmore details. Also, here we wrote the generation algorithm from scratch, but if you\nneed more full-fledged generation (such as beam search), check out the official\nexample script from the developers of the library: http://mng.bz/wQ6q.\n After finishing the generation, you can convert the token IDs back into a raw string\nby calling tokenizer.decode()as follows:\nprint(tokenizer.decode(generated))\nThe following “story” is what I got when I ran this:\nOn our way to the beach, she finds, she finds the men who are in the group to \nbe \" in the group \". This has led to the perception that the \" group \" \nin the group is \" a group of people in the group with whom we share a \ndeep friendship, and which is a common cause to the contrary. \" <eos> \n<eos> = = Background = = <eos> <eos> The origins of the concept of \" \ngroup \" were in early colonial years with the English Civil War. The \nterm was coined by English abolitionist John\nThis is not a bad start. I like the way the story is trying to be consistent by sticking with\nthe concept of “group.” However, because the model is trained on Wikipedia text\nonly, its generation is not realistic and looks a little bit too formal.\n8.4.3\nGPT-2\nGPT-2 (which stands for generative pretraining), developed by OpenAI, is probably\nthe most famous language model to date. You may have heard the story about a lan-\nguage model generating natural language texts that are so realistic that you cannot\ntell them from those written by humans. Technically, GPT-2 is just a huge Transformer\nmodel, just like the one we introduced earlier. The main difference is its size (the larg-\nest model has 48 layers!) and the fact that the model is trained on a huge amount of\nnatural language text collected from the web. The OpenAI team publicly released the\nimplementation and the pretrained weights, so we can easily try out the model.\n Initialize the tokenizer and the model for GPT-2 as you have done for Transformer-\nXL, as shown next:\ntokenizer = AutoTokenizer.from_pretrained('gpt2-large')\nmodel = AutoModelWithLMHead.from_pretrained('gpt2-large')\nThen generate text using the next code snippet:\ngenerated = tokenizer.encode(\"On our way to the beach\")\ncontext = torch.tensor([generated])\npast = None\n",
      "content_length": 2676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "206\nCHAPTER 8\nAttention and Transformer\nfor i in range(100):\n    output = model(context, past_key_values=past)\n    token = sample_token(output.logits)\n    generated.append(token.item())\n    context = token.unsqueeze(0)\n    past = output.past_key_values\nprint(tokenizer.decode(generated))\nYou may have noticed how little this code snippet changed from the one for\nTransformer-XL. In many cases, you don’t need to make any modifications when\nswitching between different models. This is why the transformers library is so\npowerful—you can try out and integrate a variety of state-of-the-art Transformer-based\nmodels into your application with a simple, consistent interface. As we’ll see in the\nnext chapter, this library is also integrated into AllenNLP, which makes it easy to build\npowerful NLP applications with state-of-the-art models.\n When I tried this, the GPT-2 generated the following beautifully written passage:\nOn our way to the beach, there was a small island that we visited for the \nfirst time. The island was called 'A' and it is a place that was used by \nthe French military during the Napoleonic wars and it is located in the \nsouth-central area of the island.\nA is an island of only a few hundred meters wide and has no other features to \ndistinguish its nature. On the island there were numerous small beaches \non which we could walk. The beach of 'A' was located in the...\nNotice how naturally it reads. Also, the GPT-2 model is good at staying consistent—\nyou can see the name of the island, “A,” is consistently used throughout the passage.\nAs far as I checked, there is no real island named A in the world, meaning that this is\nsomething the model simply made up. It is a great feat that the model remembered\nthe name it just coined and successfully wrote a story around it!\n Here’s another passage that GPT-2 generated with a prompt: 'Real World\nNatural Language Processing' is the name of the book:\n'Real World Natural Language Processing' is the name of the book. It has all \nthe tools you need to write and program natural language processing \nprograms on your computer. It is an ideal introductory resource for \nanyone wanting to learn more about natural language processing. You can \nbuy it as a paperback (US$12), as a PDF (US$15) or as an e-book \n(US$9.99).\nThe author's blog has more information and reviews.\nThe free 'Real World Natural Language Processing' ebook has all the necessary \ntools to get started with natural language processing. It includes a \nnumber of exercises to help you get your feet wet with writing and \nprogramming your own natural language processing programs, and it \nincludes a few example programs. The book's author, Michael Karp has \nalso written an online course about Natural Language Processing.\n",
      "content_length": 2759,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "207\nTransformer-based language models\n'Real World Natural Language Processing: Practical Applications' is a free \ne-book that explains how to use natural language processing to solve \nproblems of everyday life (such as writing an email, creating and\nAs of February 2019, when GPT-2 was released, I had barely begun writing this book,\nso I doubt GPT-2 knew anything about it. For a language model that doesn’t have any\nprior knowledge about the book, this is an amazing job, although I have to note that it\ngot the price and the name of the author wrong.\n8.4.4\nXLM\nFinally, as an interesting example, we will experiment with multilingual language gen-\neration. XLM (cross-lingual language model), proposed by researchers at Facebook\nAI Research, is a Transformer-based cross-lingual language model that can generate\nand encode texts in multiple languages.5 By learning how to encode multilingual\ntexts, the model can be used for transfer learning between different languages. We’ll\ncover transfer learning in chapter 9.\n You can start by initializing the tokenizer and the model and initialize it with the\npretrained weights as follows:\ntokenizer = AutoTokenizer.from_pretrained('xlm-clm-enfr-1024')\nmodel = AutoModelWithLMHead.from_pretrained('xlm-clm-enfr-1024')\nHere, we are loading an XLM model (xlm), trained with a causal language modeling\n(CLM) objective (clm) in English and French (enfr). CLM is just a fancier way to\ndescribe what we’ve been doing in this chapter—predicting the next token based on a\nprefix. XLM is usually used for encoding multilingual texts for some downstream tasks\nsuch as text classification and machine translation, but we are simply using it as a lan-\nguage model to generate texts. See listing 8.2 for the code snippet for generating mul-\ntilingual text with XLM. You can again reuse most of the earlier code snippet,\nalthough you also need to specify what language you are working in (note the lang =\n0 line). Also, here we are generating text from scratch by supplying just the BOS\ntoken (whose index is zero).\ngenerated = [0] # start with just <s>\ncontext = torch.tensor([generated])\nlang = 0 # English\nfor i in range(100):\n    langs = torch.zeros_like(context).fill_(lang)\n    output = model(context, langs=langs)\n    token = sample_token(output)\n5 Lample and Conneau, “Cross-Lingual Language Model Pretraining,” (2019). https://arxiv.org/abs/1901\n.07291.\nListing 8.2\nGenerating multilingual text with XLM\n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "208\nCHAPTER 8\nAttention and Transformer\n    generated.append(token.item())\n    context = torch.tensor([generated])\nprint(tokenizer.decode(generated))\nWhen I ran this, I got the following:\n<s>and its ability to make decisions on the basis of its own. \" </s>The \ngovernment has taken no decisions on that matter, \" Mr Hockey said. </\ns>A lot of the information is very sensitive. </s>The new research and \ninformation on the Australian economy, which is what we're going to get \nfrom people, and the information that we are going to be looking at, \nwe're going to be able to provide and we 'll take it forward. </s>I'm \nnot trying to make sure we're not\nThen, let’s change lang to 1 (which means French) and run the same snippet again,\nwhich gives you the next bit of text:\n<s></s>En revanche, les prix des maisons individuelles n' ont guère augmenté \n( - 0,1 % ). </s>En mars dernier, le taux de la taxe foncière, en légère \naugmentation à la hausse par rapport à février 2008. </s>\" Je n' ai \njamais eu une augmentation \" précise \". </s>\" Je me suis toujours dit \nque ce n' était pas parce que c' était une blague. </s>En effet, j' \nétais un gars de la rue \" </s>Les jeunes sont des gens qui avaient beau-\ncoup d' humour... \"\nAlthough the quality of generation is not as great as GPT-2, which we experimented\nwith earlier, it is refreshing to see that a single model can produce texts both in\nEnglish and French. These days, it is increasingly common to build multilingual\nTransformer-based NLP models to solve NLP problems and tasks in multiple lan-\nguages at the same time. This also became possible thanks to the Transformer’s pow-\nerful capacity to model the complexity of language.\n8.5\nCase study: Spell-checker\nIn the final section of this chapter, we will build a practical NLP application—a spell-\nchecker—with the Transformer. In the modern world, spell-checkers are everywhere.\nChances are your web browser is equipped with a spell-checker that tells you when you\nmake a spelling mistake by underlining misspelled words. Many word processors and\neditors also run spell-checkers by default. Some applications (including Google Docs\nand Microsoft Word) even point out simple grammatical errors, too. Ever wondered\nhow they work? We’ll learn how to formulate this as an NLP problem, prepare the\ndataset, train, and improve the model next. \n8.5.1\nSpell correction as machine translation\nSpell-checkers receive a piece of text such as “tisimptant too spll chck ths dcment,”\ndetect spelling and grammatical errors, if any, and fix all errors: “It’s important to\nspell-check this document.” How can you solve this task with NLP technologies? How\ncan such systems be implemented? \n",
      "content_length": 2683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "209\nCase study: Spell-checker\n The simplest thing you could do is tokenize the input text into words and check if\neach word is in a dictionary. If it’s not, you look for the closest valid word in the dictio-\nnary according to some measure such as the edit distance and replace with that word.\nYou repeat this until there are no words to fix. This word-by-word fixing algorithm is\nwidely used by many spell-checkers due to its simplicity.\n However, this type of spell-checker has several issues. First, just like the first word in\nthe example, “tisimptant,” how do you know which part of the sentence is actually a\nword? The default spell-checker for my copy of Microsoft Word indicates it’s a mis-\nspelling of “disputant,” although it would be obvious to any English speakers that it is\nactually a misspelling of two (or more) words. The fact that users can also misspell\npunctuation (including whitespace) makes everything complicated. Second, just\nbecause some word is in a dictionary doesn’t mean it’s not an error. For example, the\nsecond word in the example, “too” is a misspelling of “to,” but both are valid words\nthat are in any English dictionary. How can you tell if the former is wrong in this con-\ntext? Third, all these decisions are made out of context. One of the spell-checkers I\ntried shows “thus” as a candidate to replace “ths” in this example. However, from this\ncontext (before a noun), it is obvious that “this” is a more appropriate candidate,\nalthough both “this” and “thus” are one edit distance away from “ths,” meaning they\nare equally valid options according to the edit distance. \n You would be able to solve some of these issues by adding some heuristic rules. For\nexample, “too” is more likely a misspelling of “to” before a verb, and “this” is more\nlikely before a noun than “thus.” But this method is obviously not scalable. Remember\nthe poor junior developer from section 1.1.2? Language is vast and full of exceptions.\nYou cannot just keep writing such rules to deal with the full complexity of language.\nEven if you are able to write rules for such simple words, how would you tell that\n“tisimptant” is actually two words? Would you try to split this word at every possible\nposition to see if split words resemble existing words? What if the input was in a lan-\nguage that is written without whitespace, like Chinese and Japanese?\n At this point, you may realize this “split and fix” approach is going nowhere. In\ngeneral, when designing an NLP application, you should think in terms of the follow-\ning three aspects:\nTask—What is the task being solved? Is it a classification, sequential-labeling, or\nsequence-to-sequence problem?\nModel—What model are you going to use? Is it a feed-forward network, an RNN,\nor the Transformer?\nDataset—Where are you obtaining the dataset to train and validate your model? \nBased on my experience, a vast majority of NLP applications nowadays can be solved by\ncombining these aspects. How about spell-checkers? Because they take a piece of text\nas the input and produce the fixed string, it’d be most straightforward if we solve this\nas a Seq2Seq task using the Transformer model. In other words, we will be building a\nmachine translation system that translates noisy inputs with spelling/grammatical\n",
      "content_length": 3274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "210\nCHAPTER 8\nAttention and Transformer\nerrors into clean, errorless outputs as shown in figure 8.16. You can regard these two\nsides as two different “languages” (or “dialects” of English).\n At this point, you may be wondering where we are obtaining the dataset. This is\noften the most important (and the most difficult) part in solving real-world NLP prob-\nlems. Fortunately, we can use a public dataset for this task. Let’s dive in and start build-\ning a spell-checker.\n8.5.2\nTraining a spell-checker\nWe will be using GitHub Typo Corpus (https://github.com/mhagiwara/github-typo\n-corpus) as the dataset to train a spell-checker. The dataset, created by my collaborator\nand me, consists of hundreds of thousands of “typo” edits automatically harvested\nfrom GitHub. It is the largest dataset of spelling mistakes and their corrections to\ndate, which makes it a perfect choice for training a spell-checker.\n One decision we need to make before preparing the dataset and training a model\nis what to use as the atomic linguistic unit on which the model operates. Many NLP mod-\nels use tokens as the smallest unit (i.e., RNN/Transformer is fed a sequence of tokens),\nbut a growing number of NLP models use word or sentence pieces as the basic units (section\n10.4). What should we use as the smallest unit for spelling correction? As with many\nother NLP models, using words as the input sounds like a good “default” thing to do at\nfirst. However, as we saw earlier, the concept of tokens is not well suited for spelling\ncorrection—users can mess up with punctuation, which makes everything overly com-\nplex if you are dealing with tokens. More importantly, because NLP models need to\noperate on a fixed vocabulary, the spell-corrector vocabulary would need to include\nevery single misspelling of every single word it encountered during the training. This\nwould make it unnecessarily expensive to train and maintain such an NLP model.\n For these reasons, we will be using characters as the basic unit for our spell-checker,\nas we did in section 5.6. Using characters has several advantages—it can keep the size\nof the vocabulary quite small (usually less than one hundred for a language with a small\nfairseq\nNoisy text\nClean text\nSpell-checker\ntisimptant too spll chck\nths dcment\n \nIt's important to spell check\nthis document\n \nTarget source\nFigure 8.16\nTraining a spell-checker as an MT system that translates “noisy” sentences \ninto “clean” ones\n",
      "content_length": 2442,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "211\nCase study: Spell-checker\nset of alphabets such as English). You don’t need to worry about bloating your vocabu-\nlary, even with a noisy dataset full of typos, because typos are just different arrangements\nof characters. You can also treat punctuation marks (even whitespace) as one of the\ncharacters in the vocabulary. This makes the preprocessing step extremely easy because\nyou don’t need any linguistic toolkits (such as tokenizers) for doing this.\nNOTE\nUsing characters is not without disadvantages. One main issue is using\nthem will increase the length of sequences, because you need to break every-\nthing up into characters. This makes the model large and slower to train.\nFirst, let’s prepare the dataset for training a spell-checker. All the necessary data and\ncode for building a spell-checker are included in this repository: https://github.com/\nmhagiwara/xfspell. The tokenized and split datasets are located under data/gtc (as\ntrain.tok.fr, train.tok.en, dev.tok.fr, dev.tok.en). The suffixes en and fr\nare a commonly used convention in machine translation—“fr” means “foreign lan-\nguage” and “en” means English, because many MT research projects were originally\nmotivated by people wanting to translate some foreign language into English. Here,\nwe are using “fr” and “en” to mean just “noisy text before spelling correction” and\n“clean text after spelling correction.”\n Figure 8.17 shows an excerpt from the dataset for spelling correction created from\nGitHub Typo Corpus. Notice that text is segmented into individual characters, even\nwhitespaces (replaced by “_”). Any characters outside common alphabets (upper- and\nlowercase letters, numbers, and some common punctuation marks) are replaced with\n“#.” You can see that the dataset contains diverse corrections, including simple typos\n(pubilc -> public on line 670, HYML -> HTML on line 672), trickier errors (mxnet\nas not -> mxnet is not on line 681, 22th -> 22nd on line 682), and even lines without\nany corrections (line 676). This looks like a good resource to use for training a\nspell-checker.\nFigure 8.17\nTraining data for spelling correction\n",
      "content_length": 2119,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "212\nCHAPTER 8\nAttention and Transformer\nThe first step for training a spell-checker (or any other Seq2Seq model) is to prepro-\ncess the datasets. Because the dataset is already split and formatted, all you need to do\nis run fairseq-preprocess to convert the datasets into a binary format as follows:\nfairseq-preprocess --source-lang fr --target-lang en \\\n    --trainpref data/gtc/train.tok \\\n    --validpref data/gtc/dev.tok \\\n    --destdir bin/gtc\nThen you can start training your model right away using the following code.\nfairseq-train \\\n    bin/gtc \\\n    --fp16 \\\n    --arch transformer \\\n    --encoder-layers 6 --decoder-layers 6 \\\n    --encoder-embed-dim 1024 --decoder-embed-dim 1024 \\\n    --encoder-ffn-embed-dim 4096 --decoder-ffn-embed-dim 4096 \\\n    --encoder-attention-heads 16 --decoder-attention-heads 16 \\\n    --share-decoder-input-output-embed \\\n    --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-09 --clip-norm \n25.0 \\\n    --lr 1e-4 --lr-scheduler inverse_sqrt --warmup-updates 16000 \\\n    --dropout 0.1 --attention-dropout 0.1 --activation-dropout 0.1 \\\n    --weight-decay 0.00025 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\n    --max-tokens 4096 \\\n    --save-dir models/gtc01 \\\n    --max-epoch 40\nYou don’t need to worry about most of the hyperparameters here—this set of parame-\nters worked fairly well for me, although some other combinations of parameters may\nwork better. However, you may want to pay attention to some of the parameters\nrelated to the size of the model, namely:\nNumber of layers (--[encoder|decoder]-layers)\nEmbedding dimension of self-attention (--[encoder|decoder]-embed-dim)\nEmbedding dimension of feed-forward layers (--[encoder/decoder]-ffn-\nembed-dim)\nNumber of attention heads (--[encoder|decoder]-attention-heads)\nThese parameters determine the capacity of the model. In general, the larger these\nparameters are, the larger capacity the model would have, although as a result the\nmodel would also require more data, time, and GPU resources to train. Another\nimportant parameter is --max-tokens, which specifies the number of tokens loaded\nonto a single batch. If you are experiencing out-of-memory errors on a GPU, try\nadjusting this parameter.\nListing 8.3\nTraining a spell-checker\n",
      "content_length": 2273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "213\nCase study: Spell-checker\n After the training is finished, you can run the following command to make predic-\ntions using the trained model:\necho \"tisimptant too spll chck ths dcment.\" \\\n    | python src/tokenize.py \\\n    | fairseq-interactive bin/gtc \\\n    --path models/gtc01/checkpoint_best.pt \\\n    --source-lang fr --target-lang en --beam 10 \\\n    | python src/format_fairseq_output.py\nBecause the fairseq-interactive interface can also take source text from the\nstandard input, we are directly providing the text using the echo command. The\nPython script src/format_fairseq_output.py, as its name suggests, formats the\noutput from fairseq-interactive and shows the predicted target text. When I ran\nthis, I got the following:\ntisimplement too spll chck ths dcment.\nThis is rather disappointing. The spell-checker learned to somehow fix “imptant” to\n“implement,” although it failed to correct any other words. I suspect a couple of rea-\nsons for this. The training data used, GitHub Typo Corpus, is heavily biased toward\nsoftware-related language and corrections, which might have led to the wrong correc-\ntion (imptant -> implement). Also, the training data might have just been too small\nfor the Transformer to be effective. How could we improve the model so that it can fix\nspellings more accurately? \n8.5.3\nImproving a spell-checker\nAs we discussed earlier, one main reason the spell-checker is not working as expected\nmight be because the model wasn’t exposed to a more diverse, larger amount of mis-\nspellings during training. But as far as I know, no such large datasets of diverse mis-\nspellings are publicly available for training a general-domain spell-checker. How could\nwe obtain more data for training a better spell-checker?\n This is where we need to be creative. One idea is to artificially generate noisy text\nfrom clean text. If you think of it, it is very difficult (especially for a machine learning\nmodel) to fix misspellings, whereas it is very easy to “corrupt” clean text to simulate\nhow people make typos, even for a computer. For example, we can take some clean\ntext (which is available from, for example, scraped web text almost indefinitely) and\nreplace some letters at random. If you pair artificially generated noisy text created this\nway with the original, clean text, this will effectively create a new, larger dataset on\nwhich you can train an even better spell-checker!\n The remaining issue we need to address is how to “corrupt” clean text to generate\nrealistic spelling errors that look like the ones made by humans. You can write a\nPython script that, for example, replaces, deletes, and/or swaps letters at random,\nalthough there is no guarantee that typos made this way are similar to those made by\nhumans and that the resulting artificial dataset will provide useful insights for the\n",
      "content_length": 2831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "214\nCHAPTER 8\nAttention and Transformer\nTransformer model. How can we model the fact that, for example, humans are more\nlikely to type “too” in place of “to” than they do “two”? \n This is starting to sound familiar again. We can use the data to simulate the typos!\nBut how? This is where we need to be creative again—if you “flip” the direction of the\noriginal dataset we used to train the spell-checker, you can observe how humans make\ntypos. If you treat the clean text as the source language and the noisy text as the target\nand train a Seq2Seq model for that direction, you are effectively training a “spell-\ncorruptor”—a Seq2Seq model that inserts realistic-looking spelling errors into clean\ntext. See Figure 8.18 for an illustration.\nFigure 8.18\nUsing back-translation to generate artificial noisy data\nThis technique of using the “inverse” of the original training data to artificially gener-\nate a large amount of data in the source language from a real corpus in the target lan-\nguage is called back-translation in the machine learning literature. It is a popular\ntechnique to improve the quality of machine translation systems. As we’ll show next, it\nis also effective for improving the quality of spell-checkers.\n  You can easily train a spell corruptor just by swapping the source and the target lan-\nguages. You can do this by supplying “en” (clean text) as the source language and “fr”\n(noisy text) as the target language when you run fairseq-preprocess as follows:\nfairseq-preprocess --source-lang en --target-lang fr \\\n    --trainpref data/gtc/train.tok \\\n    --validpref data/gtc/dev.tok \\\n    --destdir bin/gtc-en2fr\nFairseq\nFairseq\nClean text\nArtificial\nnoisy text\nSpell corruptor\nSpell-checker\nClean text\nNoisy text\nTarget source\nSource\nTarget\ntisimptant too spll chck\nths dcment\nIt's important to spell check\nthis document\n",
      "content_length": 1845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "215\nCase study: Spell-checker\nWe are not going over the training process again—you can use almost the same\nfairseq-train command to start the training. Just don’t forget to specify a different\ndirectory for --save-dir. After you finish training, you can check whether the spell-\ning corrupter can indeed corrupt the input text as expected:\n$ echo 'The quick brown fox jumps over the lazy dog.' | python src/\ntokenize.py \\ \n    | fairseq-interactive \\\n    bin/gtc-en2fr \\\n    --path models/gtc-en2fr/checkpoint_best.pt \\\n    --source-lang en --target-lang fr \\\n    --beam 1 --sampling --sampling-topk 10 \\\n    | python src/format_fairseq_output.py\nThe quink brown fox jumps ove-rthe lazy dog.\nNote the extra options that I added earlier, which are shown in bold. It means that the\nfairseq-interactive command uses sampling (from top 10 tokens with largest\nprobabilities) instead of beam search. When corrupting clean text, it is often better to\nuse sampling instead of beam search. To recap, sampling picks the next token ran-\ndomly according to the probability distribution after the softmax layer, whereas beam\nsearch tries to find the “best path” that maximizes the score of the output sequence.\nAlthough beam search can find better solutions when translating some text, we want\nnoisy, more diverse output when corrupting clean text. Past research6 has also shown\nthat sampling (instead of beam search) works better for augmenting data via back-\ntranslation.\n From here, the sky’s the limit. You can collect as much clean text as you want, gen-\nerate noisy text from it using the corruptor you just trained, and increase the size of\nthe training data. There is no guarantee that the artificial errors look like the real\nones made by humans, but this is not a big deal because 1) the source (noisy) side is\nused only for encoding, and 2) the target (clean) side data is always “real” data written\nby humans, from which the Transformer can learn how to generate real text. The\nmore text data you collect, the more confident the model will get about what error-\nfree, real text looks like.\n I won’t go over every step I took to increase the size of the data, but here’s the sum-\nmary of what I did and what you can also do. Collect as much clean and diverse text\ndata from publicly available datasets, such as Tatoeba and Wikipedia dumps. My favor-\nite way to do this is to use OpenWebTextCorpus (https://skylion007.github.io/Open-\nWebTextCorpus/), an open source project to replicate the dataset on which GPT-2 was\noriginally trained. It consists of a huge amount (40 GB) of high-quality web text\ncrawled from all outbound links from Reddit. Because the entire dataset would take\ndays, if not weeks, just to preprocess and run the corruptor on, you can take a subset\n(say, 1/1000th) and add it to the dataset. I took 1/100th of the dataset, preprocessed it,\nand ran the corruptor to obtain the noisy-clean parallel dataset. This 1/100th subset\n6 Edunov et al.,”Understanding Back-Translation at Scale,” (2018). https://arxiv.org/abs/1808.09381.\n",
      "content_length": 3045,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "216\nCHAPTER 8\nAttention and Transformer\nalone added more than five million pairs (in comparison, the original training set con-\ntains only ~240k pairs). Instead of training from scratch, you can download the pre-\ntrained weights and try the spell-checker from the repository.\n The training took several days, even on multiple GPUs, but when it was done, the\nresult was very encouraging. Not only can it accurately fix spelling errors, as shown here\n$ echo \"tisimptant too spll chck ths dcment.\" \\\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n    bin/gtc-bt512-owt1k-upper \\\n    --path models/bt05/checkpoint_best.pt \\\n    --source-lang fr --target-lang en --beam 10 \\\n    | python src/format_fairseq_output.py\n    It's important to spell check this document.\nbut the spell-checker also appears to understand the grammar of English to some\ndegree, as shown here:\n$ echo \"The book wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe book was about NLP.\n$ echo \"The books wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe books were about NLP.\nThis example alone may not prove that the model really understands the grammar\n(namely, using the correct verb depending on the number of the subject). It might\njust be learning some association between consecutive words, which can be achieved\nby any statistical NLP model, such as n-gram language models. However, even after\nyou make the sentences more complicated, the spell-checker shows amazing resil-\nience, as shown in the next code snippet:\n$ echo \"The book Tom and Jerry put on the yellow desk yesterday wer about NLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe book Tom and Jerry put on the yellow desk yesterday was about NLP.\n$ echo \"The books Tom and Jerry put on the yellow desk yesterday wer about \nNLP.\" |\n    | python src/tokenize.py \\\n    | fairseq-interactive \\\n   ...\nThe books Tom and Jerry put on the yellow desk yesterday were about NLP.\nFrom these examples, it is clear that the model learned how to ignore irrelevant noun\nphrases (such as “Tom and Jerry” and “yellow desk”) and focus on the noun\n",
      "content_length": 2179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "217\nSummary\n(“book(s)”) that determines the form of the verb (“was” versus “were”). We are more\nconfident that it understands the basic sentence structure. All we did was collect a\nlarge amount of clean text and trained the Transformer model on it, combined with\nthe original training data and the corruptor. Hopefully through these experiments,\nyou were able to feel how powerful the Transformer model can be!\nSummary\nAttention is a mechanism in neural networks that focuses on a specific part of\nthe input and computes its context-dependent summary. It works like a “soft”\nversion of a key-value store.\nEncoder-decoder attention can be added to Seq2Seq models to improve their\ntranslation quality.\nSelf-attention is an attention mechanism that produces the summary of the\ninput by summarizing itself.\nThe Transformer model applies self-attention repeatedly to gradually transform\nthe input.\nHigh-quality spell-checkers can be built using the Transformer and a technique\ncalled back-translation.\n",
      "content_length": 1002,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "218\nTransfer learning with\npretrained language models\nThe year 2018 is often called “an inflection point” in the history of NLP. A promi-\nnent NLP researcher, Sebastian Ruder (https://ruder.io/nlp-imagenet/), dubbed\nthis change “NLP’s ImageNet moment,” where he used the name of a popular com-\nputer vision dataset and powerful models pretrained on it, pointing out that similar\nchanges were underway in the NLP community as well. Powerful pretrained lan-\nguage models such as ELMo, BERT, and GPT-2 achieved state-of-the-art perfor-\nmance in many NLP tasks and completely changed how we build NLP models\nwithin months. \nThis chapter covers\nUsing transfer learning to leverage knowledge \nfrom unlabeled textual data\nUsing self-supervised learning to pretrain large \nlanguage models such as BERT\nBuilding a sentiment analyzer with BERT and \nthe Hugging Face Transformers library\nBuilding a natural language inference model \nwith BERT and AllenNLP\n",
      "content_length": 949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "219\nTransfer learning\n One important concept underlying these powerful pretrained language models is\ntransfer learning, a technique for improving the performance of one task using a model\ntrained on another task. In this chapter, we’ll first introduce the concept, then move\non to introducing BERT, the most popular pretrained language model proposed for\nNLP. We’ll cover how BERT is designed and pretrained, as well as how to use the\nmodel for downstream NLP tasks including sentiment analysis and natural language\ninference. We’ll also touch on other popular pretrained models including ELMo and\nRoBERTa.\n9.1\nTransfer learning\nWe start this chapter by introducing transfer learning, a powerful machine learning\nconcept fundamental to many pretrained language models (PLMs) in this chapter. \n9.1.1\nTraditional machine learning\nIn traditional machine learning, before the advent of pretrained language models,\nNLP models were trained on a per-task basis, and they were useful only for the type of\nthe task they were trained for (figure 9.1). For example, if you wanted a sentiment\nanalysis model, you needed to use a dataset annotated with the desired output (e.g.,\nnegative, neutral, and positive labels), and the trained model was useful only for senti-\nment analysis. If you needed to build another model for part-of-speech (POS) tagging\n(an NLP task to identify the part of speech of words; see section 5.2 for a review), you\nneeded to do this all over again by collecting training data and training a POS tagging\nmodel from scratch. You could not “reuse” your sentiment analysis model for POS tag-\nging, no matter how good your model was, because these two were trained for two fun-\ndamentally different tasks. However, these tasks both operated on the same language\nand all this seemed wasteful. For example, knowing that “wonderful,” “awesome,” and\n“great” are all adjectives that have positive meaning would help both sentiment analysis\nTraining\nModel 1\nTraining data 1\nTask 1\nTraining\nModel 2\nTraining data 2\nTask 2\nTraining\nModel 3\nTraining data 3\nTask 3\nFigure 9.1\nIn traditional \nmachine learning, each \ntrained model was used \nfor just one task.\n",
      "content_length": 2159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "220\nCHAPTER 9\nTransfer learning with pretrained language models\nand part-of-speech tagging. Under the traditional machine learning paradigm, not\nonly did we need to prepare training data large enough to teach “common sense” like\nthis to the model, but individual NLP models also needed to learn such facts about the\nlanguage from scratch, solely from the given data.\n9.1.2\nWord embeddings\nAt this point, you may realize this sounds somewhat familiar. Recall our discussion in\nsection 3.1 on word embeddings and why they are important. To recap, word embed-\ndings are vector representations of words that are learned so that semantically similar\nwords share similar representations. As a result, vectors for “dog” and “cat,” for exam-\nple, end up being located in a close proximity in a high-dimensional space. These rep-\nresentations are trained on an independent, large textual corpus without any training\nsignals, using algorithms such as Skip-gram and CBOW, often collectively called\nWord2vec (section 3.4). \n After these word embeddings are trained, downstream NLP tasks can use them as\nthe input to their models (which are often neural networks, but not necessarily).\nBecause these embeddings already capture semantic relationship between words (e.g.,\ndogs and cats are both animals), these tasks no longer need to learn how the language\nworks from scratch, which gives them the upper hand in the task they are trying to\nsolve. The model can now focus on learning higher-level concepts that cannot be cap-\ntured by word embeddings (e.g., phrases, syntax, and semantics) and the task-specific\npatterns learned from the given annotated data. This is why using word embeddings\ngives a performance boost to many NLP models.\n In chapter 3, we likened this to teaching a baby (= an NLP model) how to dance.\nBy letting babies learn how to walk steadily first (= training word embeddings), dance\nteachers (= task-specific datasets and training objectives) can focus on teaching spe-\ncific dance moves without worrying whether babies can even stand and walk properly.\nThis “phased training” approach makes everything easier if you want to teach another\nskill to the baby (e.g., teaching martial arts) because they already have a good grasp of\nthe fundamental skill (walking).\n The beauty of all this is that word embeddings can be learned independently of\nthe downstream tasks. These word embeddings are pretrained, meaning their training\nhappens before the training of downstream NLP tasks. Using the dancing baby anal-\nogy, dance teachers can safely assume that all the incoming dance students have\nalready learned how to stand and walk properly. Pretrained word embeddings created\nby the developers of the algorithm are often freely available, and anyone can down-\nload and integrate them into their NLP applications. This process is illustrated in\nfigure 9.2.\n9.1.3\nWhat is transfer learning?\nIf you generalize what you did with word embeddings earlier, you took the outcome of\none task (i.e., predicting word cooccurrence with embeddings) and transferred the\nknowledge gleaned from it to another one (i.e., sentiment analysis, or any other NLP\n",
      "content_length": 3145,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "221\nTransfer learning\ntasks). In machine learning, this process is called transfer learning, which is a collection\nof related techniques to improve the performance of a machine learning model in a\ntask using data and/or models trained in a different task. Transfer learning always\nconsists of two or more steps—a machine learning model is first trained for one task\n(called pretraining), which is then adjusted and used in another (called adaptation). If\nthe same model is used for both tasks, the second step is called fine-tuning, because\nyou are tuning the same model slightly but for a different task. See figure 9.3 for an\nillustration of transfer learning in NLP.\nFigure 9.3\nLeveraging transfer learning helps build a better NLP model.\nTransfer learning has become the dominant way for building high-quality NLP models\nin the past few years for two main reasons. Firstly, thanks to powerful neural network\nmodels such as the Transformer and self-supervised learning (see section 9.2.2), it\nWord2vec\nTraining\nModel\nLarge \ntext corpus\nAnnotated data\nTask\nWord \nembeddings\nFigure 9.2\nLeveraging word embeddings helps build a better NLP model.\nPretraining\nAdaptation\nfine-tuning\n \nLarge\n \ntext corpus\nSmall\n \nannotated data\nTask\nPretrained\nlanguage model\nFine-tuned\nmodel\n",
      "content_length": 1274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "222\nCHAPTER 9\nTransfer learning with pretrained language models\nbecame possible to bootstrap high-quality embeddings from an almost unlimited\namount of natural language text. These embeddings take into account the structure,\ncontext, and semantics of natural language text to a great extent. Secondly, thanks to\ntransfer learning, anyone can incorporate these powerful pretrained language models\ninto their NLP applications, even without access to a lot of textual resources, such as\nweb-scale corpora, or compute resources, such as powerful GPUs. The advent of these\nnew technologies (the Transformer, self-supervised learning, pretrained language\nmodels, and transfer learning) moved the field of NLP to a completely new stage and\npushed the performance of many NLP tasks to a near-human level. In the following\nsubsections, we’ll see transfer learning in action by actually building NLP models\nwhile leveraging PLMs such as BERT.\n Note that the concept called domain adaptation is closely related to transfer learn-\ning. Domain adaptation is a technique where you train a machine learning model in\none domain (e.g., news) and adapt it to another domain (e.g., social media), but\nthese are for the same task (e.g., text classification). On the other hand, the transfer\nlearning we cover in this chapter is applied to different tasks (e.g., language modeling\nversus text classification). You can achieve the same effect using the transfer learning\nparadigm covered in this chapter, and we do not specifically cover domain adaptation\nas a separate topic. Interested readers can learn more about domain adaptation from\na recent review paper.1\n9.2\nBERT\nIn this section, we will cover BERT in detail. BERT (Bidirectional Encoder Represen-\ntations from Transformers)2 is by far the most popular and most influential pretrained\nlanguage model to date that revolutionized how people train and build NLP models.\nWe will first introduce contextualized embeddings and why they are important, then move\non to explaining self-supervised learning, which is an important concept in pretrain-\ning language models. We’ll cover two self-supervised tasks used for pretraining BERT,\nnamely, masked language models and next-sentence prediction, and cover ways to\nadapt BERT for your applications. \n9.2.1\nLimitations of word embeddings\nWord embeddings are a powerful concept that can give your application a boost in the\nperformance, although they are not without limitation. One obvious issue is that they\ncannot take context into account. Words you see in natural language are often poly-\nsemous, meaning they may have more than one meaning, depending on their con-\ntext. However, because word embeddings are trained per token type, all the different\nmeanings are compressed into a single vector. For example, training a single vector\n1 Ramponi and Plank, “Neural Unsupervised Domain Adaptation in NLP—A Survey,” (2020). https://arxiv.org/\nabs/2006.00632.\n2 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-Training of Deep Bidirec-\ntional Transformers for Language Understanding,” (2018). https://arxiv.org/abs/1810.04805.\n",
      "content_length": 3134,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "223\nBERT\nfor “dog” or “apple” cannot deal with the fact that “hot dog” or “Big Apple” are not a\ntype of animal or fruit, respectively. As another example, consider what “play” means\nin these sentences: “They played games,” “I play Chopin,” “We play baseball,” and\n“Hamlet is a play by Shakespeare” (these sentences are all from Tatoeba.org). These\noccurrences of “play” have different meanings, and assigning a single vector wouldn’t\nhelp much in downstream NLP tasks (e.g., in classifying the topic into sports, music,\nand art). \n Due to this limitation, NLP researchers started exploring ways to transform the\nentire sentence into a series of vectors that consider the context, called contextualized\nembeddings or simply contextualization. With these representations, all the occurrences\nof “play” in the previous examples would have different vectors assigned, helping\ndownstream tasks disambiguate different uses of the word. Notable milestones in con-\ntextualized embeddings include CoVe3 and ELMo (section 9.3.1), although the big-\ngest breakthrough was achieved by BERT, a Transformer-based pretrained language\nmodel, which is the focus of this section.\n We learned the Transformer\nuses a mechanism called self-\nattention to gradually transform\nthe input sequence by summariz-\ning it. The core idea of BERT is\nsimple: it uses the Transformer\n(the Transformer encoder, to be\nprecise) to transform the input\ninto contextualized embeddings.\nThe Transformer transforms the\ninput through a series of layers\nby gradually summarizing the\ninput. Similarly, BERT contextu-\nalizes the input through a series\nof Transformer encoder layers.\nThis is illustrated in figure 9.4.\n Because BERT is based on the Transformer architecture, it inherits all the\nstrengths of the Transformer. Its self-attention mechanism enables it to “random\naccess” over the input and capture long-term dependencies among input tokens.\nUnlike traditional language models (such as the one based on LSTM that we covered\nin section 5.5) that can make predictions in only one direction, the Transformer can\ntake into account the context in both directions. Using the sentence “Hamlet is a play\nby Shakespeare” as an example, the contextualized embedding for “play” can incorpo-\nrate the information from both “Hamlet” and “Shakespeare,” which makes it easier to\ncapture its “dramatic work for the stage” meaning of “play.”\n3 Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher, “Learned in Translation: Contextual-\nized Word Vectors,” in NIPS 2017.\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\nBERT\nContextualized embeddings\nFigure 9.4\nBERT processes input through attention layers \nto produce contextualized embeddings.\n",
      "content_length": 2708,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "224\nCHAPTER 9\nTransfer learning with pretrained language models\n If this concept is as simple as “BERT is just a Transformer encoder,” why does it\ndeserve an entire section here? Because we haven’t answered two important practical\nquestions yet: how to train and adapt the model. Neural network models, no matter\nhow powerful, are useless without specific strategies for training and where to get the\ntraining data. Also, transfer learning is useless without specific strategies for adapting\nthe pretrained model. We will discuss these questions in the following subsections.\n9.2.2\nSelf-supervised learning\nThe Transformer, which was originally proposed for machine translation, is trained\nusing parallel text. Its encoder and decoder are optimized to minimize the loss func-\ntion, which is the cross entropy defined by the difference between the decoder output\nand the expected, correct translation. However, the purpose of pretraining BERT is to\nderive high-quality contextualized embeddings, and BERT has only an encoder. How\ncan we “train” BERT so that it is useful for downstream NLP tasks?\n If you think of BERT just as another way of deriving embeddings, you can draw\ninspiration from how word embeddings are trained. Recall that in section 3.4, to train\nword embeddings, we make up a “fake” task where surrounding words are predicted\nwith word embeddings. We are not interested in the prediction per se but rather the\n“by-product” of the training, which is the word embeddings derived as the parameters\nof the model. This type of training paradigm where the data itself provides training\nsignals is called self-supervised learning, or simply self-supervision, in modern machine\nlearning. Self-supervised learning is still one type of supervised learning from the\nmodel’s point of view—the model is trained in such a way that it minimizes the loss\nfunction defined by the training signal. It is where the training signal comes from that\nis different. In supervised learning, training signals usually come from human annota-\ntions. In self-supervised learning, training signals come from the data itself with no\nhuman intervention.\n With increasingly larger datasets and more powerful models, self-supervised learn-\ning has become a popular way to pretrain NLP models in the past several years. But\nwhy does it work so well? Two factors contribute to this—one is that the type of self-\nsupervision here is trivially simple to create (just extracting surrounding words for\nWord2vec), but it requires deep understanding of the language to solve it. For exam-\nple, reusing the example from the language model we discussed in chapter 5, to\nanswer “My trip to the beach was ruined by bad ___,” not only does the system need to\nunderstand the sentence but it also needs to be equipped with some sort of “common\nsense” for what type of things could ruin a trip to a beach (e.g., bad weather, heavy\ntraffic). The knowledge required to predict the surrounding words ranges from sim-\nple collocation/association (e.g., The Statue of ____ in New ____), syntactic and\ngrammatical (e.g., “My birthday is ___ May”), and semantic (the previous example).\nSecond, there is virtually no limit on the amount of data used for self-supervision,\nbecause all you need is clean, plain text. You can download large datasets (e.g., Wiki-\npedia dump) or crawl and filter web pages, which is a popular way to train many pre-\ntrained language models.\n",
      "content_length": 3428,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "225\nBERT\n9.2.3\nPretraining BERT\nNow that we all understand how useful self-supervised learning can be for pretraining\nlanguage models, let’s see how we can use it for pretraining BERT. As mentioned ear-\nlier, BERT is just a Transformer encoder that transforms the input into a series of\nembeddings that take context into account. For pretraining word embeddings, you\ncould simply predict surrounding words based on the embeddings of the target word.\nFor pretraining unidirectional language models, you could simply predict the next\ntoken based on the tokens that come before the target. But for bidirectional language\nmodels such as BERT, you cannot use these strategies, because the input for the pre-\ndiction (contextualized embeddings) also depends on what comes before and after\nthe input. This sounds like a chicken-and-egg problem.\n The inventors of BERT solved this with a brilliant idea called masked language model\n(MLM), where they drop (mask) words randomly in a given sentence and let the\nmodel predict what the dropped word is. Specifically, after replacing a small percent-\nage of words in a sentence with a\nspecial placeholder, BERT uses\nthe Transformer to encode the\ninput and then uses a feed-\nforward layer and a softmax lay-\ners to derive a probability distri-\nbution over possible words that\ncan fill in that blank. Because\nyou already know the answer\n(because you dropped the words\nin the first place), you can use the\nregular cross entropy to train the\nmodel, as illustrated in figure 9.5. \n \nMasking \nand \npredicting\nwords is not a completely new\nidea—it’s closely related to cloze\ntests, where the test-taker is asked\nto replace the removed words in\na sentence. This test form is\noften used to assess how well stu-\ndents can understand the language. As we saw earlier, completing missing words in a\nnatural language text requires deep understanding of the language, ranging from\nsimple associations to semantic relationships. As a result, by telling the model to solve\nthis fill-in-the-blank type of task over a huge amount of textual data, the neural net-\nwork model is trained so that it can produce contextualized embeddings that incorpo-\nrate deep linguistic knowledge.\n You may be wondering what this input [MASK] is and what you actually need to do\nif you want to implement pretraining BERT yourself. In training neural networks,\nStatue\nof\n[MASK]\nLiberty\nin\nTransformer layer\nTransformer layer\nSoftmax\nBERT\nFigure 9.5\nPretraining BERT with a masked language \nmodel\n",
      "content_length": 2495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "226\nCHAPTER 9\nTransfer learning with pretrained language models\npeople often use special tokens such as [MASK] that we mentioned here. These spe-\ncial tokens are just like other (naturally occurring) tokens such as the words “dog”\nand “cat,” except they don’t occur in text naturally (you can’t find any [MASK] in nat-\nural language corpora, no matter how hard you look) and the designers of the neural\nnetworks define what they mean. The model will learn to give representations to these\ntokens so that it can solve the task at hand. Other special tokens include BOS (begin-\nning of sentence), EOS (end of sentence), and UNK (unknown word), which we\nalready encountered in earlier chapters.\n Finally, BERT is pretrained not just with the masked language model but also with\nanother type of task called next-sentence prediction (NSP), where two sentences are given\nto BERT and the model is asked to predict whether the second sentence is the “real”\nnext sentence of the first. This is another type of self-supervised learning (“fake” task)\nfor which the training data can be created in an unlimited manner without much\nhuman intervention, because you can extract two consecutive sentences (or just stitch\ntogether two sentences at random) from any corpus and make the training data for\nthis task. The rationale behind this task is that by training with this objective, the\nmodel will learn how to infer the relationship of two sentences. However, the effective-\nness of this task has been actively debated (e.g., RoBERTa dropped this task whereas\nALBERT replaced it with another task called sentence-order prediction), and we will not\ngo into the details of this task here.\n All this pretraining sounds somewhat complex, but good news is that you rarely\nneed to implement this step yourself. Similar to word embeddings, developers and\nresearchers of these language models pretrain their models on a huge amount of nat-\nural language text (usually 10 GB-plus or even 100 GB-plus of uncompressed text)\nwith many GPUs and make the pretrained models publicly available so that anyone\ncan use them.\n9.2.4\nAdapting BERT\nAt the second (and final) stage of transfer learning, a pretrained model is adapted to\nthe target task so that the latter can leverage signals learned by the former. There are\ntwo main ways to adapt BERT to individual downstream tasks: fine-tuning and feature\nextraction. In fine-tuning, the neural network architecture is slightly modified so that it\ncan produce the type of predictions for the task in question, and the entire network is\ncontinuously trained on the training data for the task so that the loss function is mini-\nmized. This is exactly the way you train a neural network for NLP tasks, such as senti-\nment analysis, with one important difference—BERT “inherits” the model weights\nlearned through pretraining, instead of being initialized randomly and trained from\nscratch. In this way, the downstream task can leverage the powerful representations\nlearned by BERT through pretraining on a large amount of data.\n The exact way the BERT architecture is modified varies, depending on the final\ntask, but here I’m going to describe the simplest case where the task is to predict some\nsort of label for a given sentence. This is also called a sentence-prediction task, which\n",
      "content_length": 3301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "227\nBERT\nincludes sentiment analysis, which we covered in chapter 2. For downstream tasks to\nbe able to extract representations for a sentence, BERT prepends a special token\n[CLS] (for classification) to every sentence at the pretraining phase. You can extract\nthe hidden states of BERT with this token and use them as the representation of the\nsentence. As with other classification tasks, a linear layer can compress this representa-\ntion into a set of “scores” that correspond to how likely each label is the correct\nanswer. You can then use softmax to derive a probability distribution. For example, if\nyou are working on a sentiment analysis dataset with five labels (strongly negative to\nstrongly positive), you’ll use a linear layer to reduce the dimensionality to 5. This type\nof linear layer combined with softmax, which is plugged into a larger pretrained\nmodel such as BERT, is often called a head. In other words, we are attaching a classifica-\ntion head to BERT to solve a sentence-prediction task. The weights for the entire net-\nwork (the head and BERT) are adjusted so that the loss function is minimized. This\nmeans that the BERT weights initialized with pretrained ones also are adjusted (fine-\ntuned) through backpropagation. See figure 9.6 for an illustration.\nFigure 9.6\nPretraining and fine-tuning BERT with an attached classification head\nAnother variation in fine-tuning BERT uses all the embeddings, averaged over the\ninput tokens. In this method, called mean over time or bag of embeddings, all the embed-\ndings produced by BERT are summed up and divided by the length of input, just like\nthe bag-of-words model, to produce a single vector. This method is less popular than\nusing the CLS special token but may work better depending on the task. Figure 9.7\nillustrates this.\nClassification\nhead\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\nSoftmax\n[CLS]\nStatue\nof\nLiberty\n[MASK]\nTransformer layer\nTransformer layer\nSoftmax\nPretraining\nFine-tuning\nBackpropagation\nNegative Neutral Positive\n",
      "content_length": 2023,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "228\nCHAPTER 9\nTransfer learning with pretrained language models\nFigure 9.7\nPretraining and fine-tuning BERT using mean over time and a classification head\nAnother way to adapt BERT for downstream NLP tasks is feature extraction. Here BERT\nis used to extract features, which are simply a sequence of contextualized embeddings\nproduced by the final layer of BERT. You can simply feed these vectors to another\nmachine learning model as features and make predictions, as shown in figure 9.8.\nFigure 9.8\nPretraining and using BERT for feature extraction\nClassification\nhead\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\nSoftmax\n[CLS]\nStatue\nof\nLiberty\n[MASK]\nTransformer layer\nTransformer layer\nSoftmax\nPretraining\nFine-tuning\nBackpropagation\nMean over time\nNegative Neutral Positive\n[CLS]\nThis\nis\nthe\nTransformer layer\nTransformer layer\n[CLS]\nStatue\nof\nLiberty\n[MASK]\nTransformer layer\nTransformer layer\nSoftmax\nPretraining\nFeature extraction\nFeatures\nML Model\nNegative Neutral Positive\n",
      "content_length": 991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "229\nCase study 1: Sentiment analysis with BERT\nGraphically, this approach looks similar to fine-tuning. After all, you are feeding the\noutput from BERT to another ML model. However, there are two subtle but import-\nant differences: first, because you are no longer optimizing the neural network, the\nsecond ML model doesn’t have to be a neural network. Some machine learning tasks\n(e.g., unsupervised clustering) are not what neural networks are good at solving, and\nfeature extraction offers a perfect solution in these situations. Also, you are free to use\nmore “traditional” ML algorithms, such as SVMs (support vector machines), decision\ntrees, and gradient-boosted methods (such as GBDT, or gradient-boosted decision\ntrees), which may offer a better tradeoff in terms of computational cost and perfor-\nmance. Second, because BERT is used only as a feature extractor, there is no back-\npropagation and its internal parameters won’t be updated during the adaptation\nphase. In many cases, you get better accuracy in the downstream task if you fine-tune\nthe BERT parameters, because by doing so, you are also teaching BERT to get better\nat the task at hand. \n Finally, note that these two are not the only ways to adapt BERT. Transfer learning\nis an actively researched topic, not just in NLP but also in many fields of artificial intel-\nligence, and we have many other ways to use pretrained language models to make the\nbest of them. If you are interested in learning more, I recommend checking out the\ntutorial given at NAACL 2019 (one of the top NLP conferences) titled “Transfer\nLearning in Natural Language Processing” (http://mng.bz/o8qp).\n9.3\nCase study 1: Sentiment analysis with BERT\nIn this section, we will build a sentiment analyzer (again), but this time with BERT.\nInstead of AllenNLP, we will use the Transformers library developed by Hugging Face,\nwhich we used for making predictions with language models in the previous chapter.\nAll the code here is accessible on a Google Colab notebook (http://www.realworldnlp-\nbook.com/ch9.html#sst). The code snippets you see in this section all assume that\nyou import related modules, classes, and methods as follows:\nimport torch\nfrom torch import nn, optim\nfrom transformers import AutoTokenizer, AutoModel, AdamW, \nget_cosine_schedule_with_warmup\nIn the Transformers library, you specify the pretrained models by their names. We’ll\nuse the cased BERT-base model ('bert-base-cased') throughout this section, so\nlet’s define a constant first as follows: \nBERT_MODEL = 'bert-base-cased'\nThe Transformers library also supports other pretrained BERT models, which you\ncan see in their documentation (https://huggingface.co/transformers/pretrained_\nmodels.html). If you want to use other models, you can simply replace this constant\nwith the name of the model you want to use, and the rest of the code works as-is in\nmany cases (but not always).\n",
      "content_length": 2901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "230\nCHAPTER 9\nTransfer learning with pretrained language models\n9.3.1\nTokenizing input\nThe first step we took for building an NLP model is to build a dataset reader.\nAlthough AllenNLP (or more precisely speaking, the allennlp-modules package)\nis shipped with a dataset reader for the Stanford Sentiment Treebank, the dataset\nreader’s output is compatible only with AllenNLP. In this section, we are going to write\na simple method that reads the dataset and returns a sequence of batched input\ninstances.\n Tokenization is one of the most important steps in processing natural language\ninput. As we saw in the previous chapter, tokenizers in the Transformers library can be\ninitialized with the AutoTokenizer.from_pretrained() class method as follows:\ntokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\nBecause different pretrained models use different tokenizers, it is important to initial-\nize the one that matches the pretrained model you are going to use by supplying the\nsame model name.\n You can use the tokenizer to convert between a string and a list of token IDs back\nand forth, as shown next:\n>>> token_ids = tokenizer.encode('The best movie ever!')\n[101, 1109, 1436, 2523, 1518, 106, 102]\n>>> tokenizer.decode(token_ids)\n'[CLS] The best movie ever! [SEP]'\nNotice that BERT’s tokenizer added two special tokens—[CLS] and [SEP]—to your\nsentence. As discussed earlier, CLS is a special token used to extract the embedding\nfor the entire input, whereas SEP is used to separate two sentences if your task involves\nmaking predictions on a pair of sentences. Because we are making predictions for sin-\ngle sentences here, there’s no need to pay much attention to this token. We’ll discuss\nsentence-pair classification tasks later in section 9.5.\n Deep neural networks rarely operate on single instances. They usually are trained\non and make predictions for batches of instances for stability and performance rea-\nsons. The tokenizer also supports converting the given input in batches by invoking\nthe __call__ method (i.e., just use the object as a method) as follows:\n>>> result = tokenizer(\n>>>    ['The best movie ever!', 'Aweful movie'],\n>>>    max_length=10,\n>>>    pad_to_max_length=True,\n>>>    truncation=True,\n>>>    return_tensors='pt')\nWhen you run this, each string in the input list is tokenized and then resulting tensors\nare padded with 0s to have the same lengths. Padding here means adding 0s at the end\n",
      "content_length": 2427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "231\nCase study 1: Sentiment analysis with BERT\nof each sequence so that individual instances have the same length and can be bundled\nas a single tensor, which is needed for more efficient computation (we’ll cover padding\nin more detail in chapter 10). The method call contains several other parameters that\ncontrol the maximum length (max_length=10, meaning to pad everything to the\nlength of 10), whether to pad to the maximum length, whether to truncate sequences\nthat are too long, and the type of the returned tensors (return_tensors='pt',\nmeaning it returns PyTorch tensors). The result of this tokenizer() call is a dictio-\nnary that contains the following three keys and three different types of packed tensors:\n>>> result['input_ids']\ntensor([[ 101, 1109, 1436, 2523, 1518,  106,  102,    0,    0,    0],\n        [ 101,  138, 7921, 2365, 2523,  102,    0,    0,    0,    0]])\n>>> result['token_type_ids']\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> result['attention_mask']\ntensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\nThe input_ids tensor is a packed version of token IDs converted from the texts.\nNotice that each row is a vectorized token ID padded with 0s so that its length is always\n10. The token_type_ids tensor specifies which sentence each token comes from. As\nwith the SEP special token earlier, this is relevant only if you are working with sen-\ntence pairs, which is why the tensor is simply filled with just 0s. The attention_mask\ntensor specifies which tokens the Transformer should attend to. Because there are no\ntokens at the padded elements (0s in input_ids), the corresponding elements in\nattention_mask are all 0s, and attention to these tokens is simply ignored. Masking\nis a common technique often used in neural networks to ignore irrelevant elements in\nbatched tensors like the ones shown here. Chapter 10 covers masking in more detail.\n As you see here, the Transformers library’s tokenizers do more than just\ntokenizing—they take a list of strings and create batched tensors for you, including the\nauxiliary tensors (token_type_ids and attention_mask). You just need to create\nlists of strings from your dataset and pass them to tokenizer()to create batches to\npass on to the model. This logic for reading datasets is rather boring and a bit lengthy,\nso I packaged it in a method named read_dataset, which is not shown here. If you\nare interested, you can check the Google Colab notebook mentioned earlier. Using\nthis method, you can read a dataset and convert it to a list of batches as follows:\ntrain_data = read_dataset('train.txt', batch_size=32, tokenizer=tokenizer, \nmax_length=128)\ndev_data = read_dataset('dev.txt', batch_size=32, tokenizer=tokenizer, \nmax_length=128)\n",
      "content_length": 2788,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "232\nCHAPTER 9\nTransfer learning with pretrained language models\n9.3.2\nBuilding the model\nIn the next step, we’ll build the model to classify texts into their sentiment labels. The\nmodel we build here is nothing but a thin wrapper around BERT. All it does is pass the\ninput through BERT, take out its embedding at CLS, pass it through a linear layer to\nconvert to a set of scores (logits), and compute the loss.\n Note that we are building a PyTorch Module, not an AllenNLP Model, so make\nsure to inherit from nn.Module, although the structure of these two types of models\nare usually very similar (because AllenNLP’s Models inherit from PyTorch Modules).\nYou need to implement __init__(), where you define and initialize submodules of\nthe model, and forward(), where the main computation (“forward pass”) happens.\nThe entire code snippet is shown next.\nclass BertClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(BertClassifier, self).__init__()\n        self.bert_model = AutoModel.from_pretrained(model_name)    \n        self.linear = nn.Linear(self.bert_model.config.hidden_size, \nnum_labels)   \n        self.loss_function = nn.CrossEntropyLoss()\n    def forward(self, input_ids, attention_mask, token_type_ids, label=None):\n        bert_out = self.bert_model(     \n          input_ids=input_ids,\n          attention_mask=attention_mask,\n          token_type_ids=token_type_ids)\n        \n        logits = self.linear(bert_out.pooler_output)    \n        loss = None\n        if label is not None:\n            loss = self.loss_function(logits, label)    \n        return loss, logits\nThe module first defines the BERT model (via the AutoModel.from_pretrained()\nclass method), a linear layer (nn.Linear), and the loss function (nn.CrossEntropy-\nLoss) in __init__(). Note that the module has no way of knowing the number of\nlabels it needs to classify into, so we are passing it as a parameter (num_labels).\n In the forward() method, it first calls the BERT model. You can simply pass the\nthree types of tensors (input_ids, attention_mask, and token_type_ids) to the\nmodel. The model returns a data structure that contains last_hidden_state\nand pooler_output among other things, where last_hidden_state is a sequence\nof hidden states of the last layer, whereas pooler_output is a pooled output, which\nis basically the embedding at CLS transformed with a linear layer. Because we are\nListing 9.1\nSentiment analysis model with BERT\nInitializes \nBERT\nDefines a \nlinear layer\nApplies \nBERT\nApplies the \nlinear layer\nComputes \nthe loss\n",
      "content_length": 2561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "233\nCase study 1: Sentiment analysis with BERT\ninterested only in the pooled output that represents the entire input, we’ll pass the lat-\nter to the linear layer. Finally, the method computes the loss (if the label is supplied)\nand returns it, along with the logits, which are used for making predictions and mea-\nsuring the accuracy.\n Pay attention to the way we designed the method signature—it takes the three ten-\nsors we inspected earlier with their exact names. This lets us simply destruct a batch\nand pass it to the forward method, as shown here:\n>>> model(**train_data[0])\n(tensor(1.8050, grad_fn=<NllLossBackward>),\n tensor([[-0.5088,  0.0806, -0.2924, -0.6536, -0.2627],\n         [-0.3816,  0.3512, -0.1223, -0.5136, -0.4421],\n         ...\n         [-0.4220,  0.3026, -0.1723, -0.4913, -0.4106],\n         [-0.3354,  0.3871, -0.0787, -0.4673, -0.4169]],\n        grad_fn=<AddmmBackward>))\nNotice that the return value of the forward pass is a tuple of the loss and the logits.\nNow you are ready to train your model!\n9.3.3\nTraining the model\nIn the third and the final step of this case study, we will train and validate the model.\nAlthough AllenNLP took care of the training process in the previous chapters, in this\nsection we’ll write our own training loop from scratch so we can better understand\nwhat it takes to train a model yourself. Note that you can also choose to use the library’s\nown Trainer class (https://huggingface.co/transformers/main_classes/trainer.html),\nwhich works similarly to AllenNLP’s Trainer, to run the training loop by specifying its\nparameters.\n We covered the basics of training loops in section 2.5, but to recap, in modern\nmachine learning, every training loop looks somewhat similar. If you write it in\npseudocode, it would look like the one shown as follows. \nMAX_EPOCHS = 100\nmodel = Model()\nfor epoch in range(MAX_EPOCHS):\n    for batch in train_set:\n        loss, prediction = model.forward(**batch)\n        new_model = optimizer(model, loss)\n        model = new_model\nThis training loop is almost identical to listing 2.2, except it operates on batches instead\nof single instances. The dataset yields a series of batches, which are then passed to\nthe forward method of the model. The method returns the loss, which is then used to\nListing 9.2\nPseudocode for the neural network training loop\n",
      "content_length": 2339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "234\nCHAPTER 9\nTransfer learning with pretrained language models\noptimize the model. It is also common for the model to return the predictions so that\nthe caller can use the result to compute some metrics, such as accuracy.\n Before we move on to writing our own training loop, we need to note two things—\nit is customary to alternate between training and validation during each epoch. In the\ntraining phase, the model is optimized (the “magic constants” are changed) based on\nthe loss function and the optimizer. The training data is used during this phase. In the\nvalidation phase, the model’s parameters are fixed, and its accuracy of prediction is\nmeasured against validation data. Although the loss is not used for optimization\nduring validation, it is common to compute it to monitor how the loss changes during\nthe course of the training, as we did in section 6.3.\n Another thing to note is that when training Transformer-based models such as\nBERT, we usually use warm-up, a process where the learning rate (how much to change\nthe magic constants) is gradually increased for the first few thousand steps. A step\nhere is just another name for one execution of backpropagation, which corresponds\nto the inner loop of listing 9.2. This is useful for stabilizing training. We are not going\ninto the mathematical details of warm-up and controlling the learning rate here—we\njust note that a learning rate scheduler is usually used for controlling the learning rate\nover the course of the training. With the Transformers library, you can define an opti-\nmizer (AdamW) and a learning controller as follows:\noptimizer = AdamW(model.parameters(), lr=1e-5)\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=1000)\nThe controller we are using here (get_cosine_schedule_with_warmup) increases\nthe learning rate from zero to the maximum during the first 100 steps, then gradually\ndecreases it afterward (based on the cosine function, which is where it got its name).\nIf you plot how the learning rate changes over time, it’ll look like the graph in\nfigure 9.9.\nFigure 9.9\nWith a cosine learning rate schedule with warm-up, the learning rate ramps up \nfirst, then declines following a cosine function.\nNow we are ready to train our BERT-based sentiment analyzer. The next listing shows\nour training loop.\n",
      "content_length": 2354,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "235\nCase study 1: Sentiment analysis with BERT\n \nfor epoch in range(epochs):\n    print(f'epoch = {epoch}')\n    \n    model.train()    \n    losses = []\n    total_instances = 0\n    correct_instances = 0\n    for batch in train_data:\n        batch_size = batch['input_ids'].size(0)\n        move_to(batch, device)     \n        optimizer.zero_grad()     \n        \n        loss, logits = model(**batch)   \n        loss.backward()    \n        optimizer.step()\n        scheduler.step()\n    \n        losses.append(loss)\n        \n        total_instances += batch_size\n        correct_instances += torch.sum(torch.argmax(logits, dim=-1) \n   == batch['label']).item()   \n    \n    avr_loss = sum(losses) / len(losses)\n    accuracy = correct_instances / total_instances\n    print(f'train loss = {avr_loss}, accuracy = {accuracy}')\nWhen you train a model using PyTorch (and, consequently, AllenNLP and Transform-\ners, two libraries that are built on top of it), remember to call model.train() to turn\non the “training mode” of the model. This is important because some layers such as\nBatchNorm and dropout behave differently between training and evaluation (we’ll\ncover dropout in chapter 10). On the other hand, when you validate or test your\nmodel, be sure to call model.eval().\n The code in listing 9.3 does not show the validation phase, but the code for valida-\ntion would look almost the same as that for training. When you validate/test your\nmodel, pay attention to the following:\nAs mentioned previously, make sure to call model.eval() before validating/\ntesting your model.\nOptimization calls (loss.backward(), optimizer.step(), and sched-\nuler.step()) are not necessary because you are not updating the model.\nLosses are still recorded and reported for monitoring. Be sure to wrap your for-\nward pass call with with torch.no_grad()—this will disable gradient com-\nputation and save memory.\nAccuracy is computed in exactly the same way (this is the point of validation!).\nListing 9.3\nTraining loop for the BERT-based sentiment analyzer\nTurns on the \ntraining mode\nMoves the batch to \nGPU (if available)\nRemember to reset the gradients\n(in PyTorch gradients accumulate).\nForward\npass\nBackpropagation\nComputes the accuracy by counting \nthe number of correct instances\n",
      "content_length": 2262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "236\nCHAPTER 9\nTransfer learning with pretrained language models\nWhen I ran this, I got the following output to stdout (with intermediate epochs\nomitted):\nepoch = 0\ntrain loss = 1.5403757095336914, accuracy = 0.31624531835205993\ndev loss = 1.7507736682891846, accuracy = 0.2652134423251589\nepoch = 1\n...\nepoch = 8\ntrain loss = 0.4508829712867737, accuracy = 0.8470271535580525\ndev loss = 1.687158465385437, accuracy = 0.48319709355131696\nepoch = 9\n...\nThe dev accuracy peaked around 0.483 at epoch 8 and didn’t improve after that. Com-\npared to the result we got from LSTM (dev accuracy ~0.35, in chapter 2) and CNN\n(dev accuracy ~0.40, in chapter 7), this is the best result we have achieved on this data-\nset. We’ve done very little hyperparameter tuning, so it’s too early to conclude that\nBERT is the best model of the three we compared, but we at least know that it is a\nstrong baseline to start from!\n9.4\nOther pretrained language models\nBERT is neither the first nor the last of popular pretrained language models (PLMs)\ncommonly used in the NLP community nowadays. In this section, we’ll learn several\nother popular PLMs and how they are different from BERT. Most of these models are\nalready implemented and publicly available from the Transformers library, so you\ncan integrate them with your NLP application by changing just a couple of lines of\nyour code. \n9.4.1\nELMo\nELMo (Embeddings from Language Models), proposed4 in early 2018, is one of the\nearliest PLMs for deriving contextualized embeddings using unlabeled texts. Its core\nidea is simple—train an LSTM-based language model (similar to the one we trained\nback in chapter 5) and use its hidden states as additional “features” for downstream\nNLP tasks. Because the language model is trained to predict the next token given the\nprevious context, the hidden states can encode the information needed to “under-\nstand the language.” ELMo does the same with another, backward LM and combines\nthe embeddings from both directions so that it can also encode the information in\nboth directions. See figure 9.10 for an illustration.\n After pretraining LMs in both directions, downstream NLP tasks can simply use the\nELMo embeddings as features. Note that ELMo uses multilayer LSTM, so the features\nare the sum of hidden states taken from different layers, weighted in a task-specific way.\nThe inventors of ELMo showed that adding these features improves the performance\n4 Peters et al., “Deep Contextualized Word Representations,” (2018). https://arxiv.org/abs/1802.05365.\n",
      "content_length": 2529,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "237\nOther pretrained language models\nof a wide range of NLP tasks, including sentiment analysis, named entity recognition,\nand question answering. Although ELMo is not implemented in Hugging Face’s Trans-\nformers library, you can use it with AllenNLP fairly easily.5\n ELMo is a historically important PLM, although it is not often used in research or\nproduction anymore today—it predates BERT (and the advent of the Transformer)\nand there are other PLMs (including BERT) that outperform ELMo and are widely\navailable today. \n9.4.2\nXLNet\nXLNet, proposed in 2019, is an important successor of BERT and often referenced as\none of the most powerful PLMs as of today. XLNet addresses two main issues of how\nBERT is trained: train-test skew and the independence of masks. The first issue has to\n5 See here for the detailed documentation on how to use ELMo with AllenNLP: https://allennlp.org/elmo.\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nELMo\nis\nLSTM\nLSTM\nForward\nBackward\nLayer\n2\nForward\nBackward\nLayer\n1\n<s>\nThis\n</s>\nFigure 9.10\nELMo computes contextualized embeddings by combining forward and backward LSTMs.\n",
      "content_length": 1137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "238\nCHAPTER 9\nTransfer learning with pretrained language models\ndo with how BERT is pretrained using the masked language model (MLM) objective.\nDuring training time, BERT is trained so that it can accurately predict masked tokens,\nwhereas during prediction, it just sees the input sentence, which does not contain any\nmasks. This means that there’s a discrepancy of information to which BERT is\nexposed to between training and testing, and that creates the train-test skew problem.\n The second issue has to do with how BERT makes predictions for masked tokens.\nIf there is more than one [MASK] token in the input, BERT makes predictions for\nthem in parallel. There doesn’t seem to be anything wrong with this approach at first\nglance—for example, if the input was “The Statue of [MASK] in New [MASK],” the\nmodel wouldn’t have difficulties answering this as “Liberty” and “York.” If the input\nwas “The Statue of [MASK] in Washington, [MASK],” most of you (and probably a\nlanguage model) would predict “Lincoln” and “DC.” However, what if the input was\nthe following:\nThe Statue of [MASK] in [MASK] [MASK]\nThen there is no information to bias your prediction one way or the other. BERT\nwon’t learn the fact that “The Statue of Liberty in Washington, DC” or “The Statue of\nLincoln in New York” don’t make much sense during the training from this example,\nbecause these predictions are all made in parallel. This is a good example showing\nthat you cannot simply make independent predictions on tokens and combine them\nto create a sentence that makes sense.\nNOTE\nThis issue is related to the multimodality of natural language, which\nmeans there are multiple modes in the joint probability distribution, and\ncombinations of best decisions made independently do not necessarily lead to\nglobally best decisions. Multimodality is a big challenge in natural language\ngeneration.\nTo address this issue, instead of making predictions in parallel, you can make predic-\ntions sequentially. In fact, this is exactly what typical language models do—generate\ntokens from left to right, one by one. However, here we have a sentence interspersed\nwith masked tokens, and predictions depend not only on the tokens on the left (e.g.,\n“The Statue of” in the previous example) but also on the right (“in”). XLNet solves\nthis by generating missing tokens in a random order, as shown in figure 9.11. For\nexample, you can choose to generate “New” first, which gives a strong clue for the\nnext words, “York” and “Liberty,” and so on. Note that prediction is still made based\non all the tokens generated previously. If the model chose to generate “Washington”\nfirst, then the model would proceed to generate “DC” and “Lincoln” and would never\nmix up these two.\n XLNet is already implemented in the Transformers library, and you can use the\nmodel with only a few lines of code change.6 \n6  See https://huggingface.co/transformers/model_doc/xlnet.html for the documentation.\n",
      "content_length": 2944,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "239\nOther pretrained language models\nFigure 9.11\nXLNet generates tokens in an arbitrary order.\n9.4.3\nRoBERTa\nRoBERTa (from “robustly optimized BERT”)7 is another important PLM that is com-\nmonly used in research and industry. RoBERTa revisits and modifies many training\ndecisions of BERT, which makes it match or even exceed the performance of post-\nBERT PLMs, including XLNet, which we covered earlier. My personal impression is\nthat RoBERTa is the second most referenced PLM after BERT as of this writing (mid-\n2020), and it shows robust performance in many downstream NLP tasks in English. \n RoBERTa makes several improvements over BERT, but the most important (and\nthe most straightforward) is the amount of its training data. The developers of\nRoBERTa collected five English corpora of varying sizes and domains, which total over\n160 GB of text (versus 16 GB used for training BERT). Simply by using a lot more data\nfor training, RoBERTa overperforms some of the other powerful PLMs, including\nXLNet, in downstream tasks after fine-tuning. The second modification has to do with\nthe next-sentence prediction (NSP) objective we touched on in section 9.2.3, where\nBERT is pretrained to classify whether the second sentence is the “true” sentence that\nfollows the first one in a corpus. The developers of RoBERTa found that, by removing\nNSP (and training with the MLM objective only), the performance of downstream\n7 Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach,” (2019). https://arxiv.org/abs/\n1907.11692.\nStatue\nof\n[MASK]\nin\nBERT\nXLNet\n[MASK][MASK]\nStatue\nof\nLiberty\nin\nNew\nYork\nNew\nXLNet\nNew\nYork\nXLNet\nNew\nLiberty\nYork\n",
      "content_length": 1649,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "240\nCHAPTER 9\nTransfer learning with pretrained language models\ntasks stays about the same or slightly improves. In addition to these, they also revisited\nthe batch size and the way masking is done for MLM. Combined, the new pretrained\nlanguage model achieved the state-of-the-art results on downstream tasks such as ques-\ntion answering and reading comprehension.\n Because RoBERTa uses the identical architecture to BERT and both are imple-\nmented in Transformers, switching to RoBERTa is extremely easy if your application\nalready uses BERT.\nNOTE\nSimilar to BERT versus RoBERTa, the cross-lingual language model\nXLM (covered in section 8.4.4) has its “robustly optimized” sibling called\nXLM-R (short for XML-RoBERTa).8 XLM-R pretrains on 100 languages and\nshows competitive performance on many cross-lingual NLP tasks.\n9.4.4\nDistilBERT\nAlthough pretrained models such as BERT and RoBERTa are powerful, they are com-\nputationally expensive, not just for pretraining but also for tuning and making predic-\ntions. For example, BERT-base (the regular-sized BERT) and BERT-large (the larger\ncounterpart) have 110 million and 340 million parameters, respectively, and virtually\nevery input has to go through this huge network to get predictions. If you were to fine-\ntune and make predictions with a BERT-based model (such as the one we built in sec-\ntion 9.3), you’d most certainly need a GPU, which is not always available, depending\non your computational environment. For example, if you’d like to run some real-time\ntext analytics on a mobile phone, BERT wouldn’t be a great choice (and it might not\neven fit in the memory).\n To reduce the computational requirement of modern large neural networks,\nknowledge distillation (or simply distillation) is often used. This is a machine learning\ntechnique where, given a large pretrained model (called the teacher model), a smaller\nmodel (called the student model) is trained to mimic the behavior of the larger model.\nSee figure 9.12 for more details. The student model is trained with the masked lan-\nguage model (MLM) loss (same as BERT), as well as the cross-entropy loss between\nthe teacher and the student. This pushes the student model to produce the probabil-\nity distribution over predicted tokens that are as similar to the teacher as possible.\n Researchers at Hugging Face developed a distilled version of BERT called Distil-\nBERT,9 which is 40% smaller and 60% faster while retraining 97% of task performance\ncompared to BERT. You can use DistilBERT by simply replacing the model name you\npass to AutoModel.from_pretrained() for BERT (e.g., bert-base-cased) with\ndistilled versions (e.g., distilbert-base-cased), while keeping the rest of your\ncode the same.\n8 Conneau et al., “Unsupervised Cross-lingual Representation Learning at Scale,” (2019). https://arxiv.org/\nabs/1911.02116.\n9 Sanh et al., “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,” (2019). https://\narxiv.org/abs/1910.01108.\n",
      "content_length": 2977,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "241\nOther pretrained language models\n9.4.5\nALBERT\nAnother pretrained language model that addresses the computational complexity\nproblem of BERT is ALBERT,10 short for “A Lite BERT.” Instead of resorting to know-\nledge distillation, ALBERT makes a few changes to its model and the training procedure. \n One design change ALBERT makes to its model is how it handles word embed-\ndings. In most deep NLP models, word embeddings are represented by and stored in\na big lookup table that contains one word embedding vector per word. This way of\nmanaging embeddings is usually fine for smaller models such as RNNs and CNNs.\nHowever, for Transformer-based models such as BERT, the dimensionality (i.e., the\nlength) of input needs to match that of the hidden states, which is usually as big as 768\ndimensions. This means that the model needs to maintain a big lookup table of size V\ntimes 768, where V is the number of unique vocabulary items. Because in many NLP\nmodels V is also large (e.g., 30,000), the resulting lookup table becomes huge and\ntakes up a lot of memory and computation.\n ALBERT addresses this issue by decomposing word embedding lookup into two\nstages, as shown in figure 9.13. The first stage is similar to how word embeddings are\nretrieved from a mapping table, except that the output dimensionality of word\nembedding vectors is smaller (say, 128 dimensions). In the next stage, these shorter\n10 Lan et al., “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations,” (2020).\nhttps://arxiv.org/abs/1909.11942.\nBERT\n(teacher)\nDistilBERT\n(student)\nTraining data\nSoftmax\nSoftmax\nSoftmax\nCross\nentropy\nLiberty\nMasked\nLM\nDistillation\nFigure 9.12\nKnowledge \ndistillation combines \ncross entropy and the \nmasked LM objectives.\n",
      "content_length": 1751,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "242\nCHAPTER 9\nTransfer learning with pretrained language models\nvectors are expanded using a linear layer so that they match the desired input dimen-\nsionality of the model (say, 768). This is similar to how we expanded word embed-\ndings with the Skip-gram model (section 3.4). Thanks to this decomposition, ALBERT\nneeds to store only two smaller lookup tables (V × 128, plus 128 × 768) instead of one\nbig look-up table (V × 768).\n Another design change that ALBERT implements is parameter sharing between\nTransformer layers. Transformer models use a series of self-attention layers to trans-\nform the input vector. The way these layers transform the input is usually different\nfrom layer to layer—the first layer may transform the input one way (e.g., capture\nbasic phrases), and the second one may do so another way (e.g., capture some syntac-\ntic information). However, this means that the model needs to retain all the necessary\nparameters (projections for keys, queries, and values) per each layer, which is expen-\nsive and takes up a lot of memory. Instead, ALBERT’s layers all share the same set of\nparameters, meaning that the model applies the same transformation repeatedly to\nthe input. These parameters are adjusted in such a way that the series of transforma-\ntions are effective for predicting the objective, even though they are identical.\n Finally, ALBERT uses a training objective called sentence-order prediction (SOP) for\npretraining, instead of the next-sentence prediction (NSP) adopted by BERT. As men-\ntioned earlier, the developers of RoBERTa and some others found out that the NSP\nobjective is basically useless and decided to eliminate it. ALBERT replaces NSP with\nsentence-order prediction (SOP), a task where the model is asked to predict the\nordering of two consecutive segments of text. For example:11\n(A) She and her boyfriend decided to go for a long walk. (B) After walking for\nover a mile, something happened.\n(C) However, one of the teachers around the area helped me get up. (D) At\nfirst, no one was willing to help me up.\n11 These examples are taken from ROCStories: https://cs.rochester.edu/nlp/rocstories/.\ndog\nchocolate\ncat\ndog\nchocolate\ncat\nBERT\nLinear\nlayer\n \nALBERT\nchocolate\ndog\nchocolate\ncat\nFigure 9.13\nALBERT (right) decomposes word embeddings into two smaller \nprojections.\n",
      "content_length": 2324,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "243\nCase study 2: Natural language inference with BERT\nIn the first example, you can tell that A happens before B. In the second, the order is\nflipped, and D should come before C. This is an easy feat for humans, but a difficult\ntask for machines—an NLP model needs to learn to ignore superficial topical signals\n(e.g., “go for a long walk,” “walking for over a mile,” “helped me get up,” and “help me\nup”) and focus on discourse-level coherence. Training with this objective makes the\nmodel more robust and effective for deeper natural language understanding tasks.\n As a result, ALBERT was able to scale up its training and outperform BERT-large\nwith fewer parameters. As with DistilBERT, the model architecture of ALBERT is\nalmost identical to that of BERT, and you can use it by simply supplying the model\nname when you call AutoModel.from_pretrained() (e.g., albert-base-v1).\n9.5\nCase study 2: Natural language inference with BERT\nIn this  final section of this chapter, we will build an NLP model for natural language\ninference, a task where the system predicts logical relationship between sentences.\nWe’ll use AllenNLP for building the model while demonstrating how to integrate\nBERT (or any other Transformer-based pretrained models) into your pipeline. \n9.5.1\nWhat is natural language inference?\nNatural language inference (or NLI, for short) is the task of determining the logical rela-\ntionship between a pair of sentences. Specifically, given one sentence (called premise)\nand another sentence (called hypothesis), you need to determine whether the hypo-\nthesis is logically inferred from the premise. This is easier to see in the following\nexamples.12\nIn the first example, the hypothesis (“The man is sleeping”) clearly contradicts the\npremise (“A man inspects . . .”) because someone cannot be inspecting something\nwhile asleep. In the second example, you cannot tell if the hypothesis contradicts or is\nentailed by the premise (especially the “laughing at the cats” part), which makes the\nrelationship “neutral.” In the third example, you can logically infer the hypothesis\nfrom the premise—in other words, the hypothesis is entailed by the premise.\n As you can guess, NLI can be tricky even for humans. The task requires not only\nlexical knowledge (e.g., plural of “man” is “men,” soccer is one type of sport) but also\nPremise\nHypothesis\nLabel\nA man inspects the uniform of a figure in \nsome East Asian country.\nThe man is sleeping.\ncontradiction\nAn older and younger man smiling.\nTwo men are smiling and laughing at \nthe cats playing on the floor.\nneutral\nA soccer game with multiple males playing.\nSome men are playing a sport.\nentailment\n12 These examples are taken from http://nlpprogress.com/english/natural_language_inference.html.\n",
      "content_length": 2756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "244\nCHAPTER 9\nTransfer learning with pretrained language models\nsome “common sense” (e.g., you cannot inspect while sleeping). NLI is one of the\nmost typical natural language understanding (NLU) tasks. How can you build an NLP\nmodel to solve this task?\n Fortunately, NLI is a well-studied field in NLP. The most popular dataset for NLI,\nthe Standard Natural Language Inference (SNLI) corpus (https://nlp.stanford.edu/\nprojects/snli/), has been used in numerous NLP studies as a benchmark. In what fol-\nlows, we’ll build a neural NLI model with AllenNLP and learn how to use BERT for\nthis particular task. \n Before moving on, make sure that you have AllenNLP (we use version 2.5.0) and\nthe AllenNLP model’s modules installed. You can install them by running the follow-\ning code:\npip install allennlp==2.5.0\npip install allennlp-models==2.5.0\nThis also installs the Transformers library as a dependency.\n9.5.2\nUsing BERT for sentence-pair classification\nBefore we start building the model, notice that every input to the NLI task consists of\ntwo pieces: a premise and a hypothesis. Most of the NLP tasks we covered in this book\nhad just one part—usually a single sentence—as the input to the model. How can we\nbuild a model that makes predictions for instances that are pairs of sentences?\n We have multiple ways to deal with multipart input for NLP models. We can\nencode each sentence with an encoder and apply some mathematical operations\n(e.g., concatenation, subtractions) to the result to derive an embedding for the pair\n(which, by the way, is the basic idea of Siamese networks13). Researchers have also\ncome up with more complex neural network models with attention (such as BiDAF14).\n However, there’s inherently nothing preventing BERT from accepting more than\none sentence. Because the Transformer accepts a sequence of any tokens, you can\nsimply concatenate the two sentences and feed them to the model. If you are worried\nabout the model mixing up the two sentences, you can separate them with a special\ntoken, [SEP]. You can also add different values to each sentence as an extra signal to\nthe model. BERT uses these two techniques to solve sentence-pair classification tasks\nsuch as NLI with little modification to the model.\n The rest of the pipeline proceeds in a similar way to other classification tasks. A\nspecial token [CLS] is appended to every sentence pair, from which the final embed-\nding of the input is extracted. Finally, you can use a classification head to convert the\nembedding into a set of values (called logits) corresponding to the classes. This is illus-\ntrated in figure 9.14.\n13 Reimers and Gurevych, “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks,” (2019).\nhttps://arxiv.org/abs/1908.10084.\n14 Seo et al., “Bidirectional Attention Flow for Machine Comprehension,” (2018). https://arxiv.org/abs/1611\n.01603.\n",
      "content_length": 2863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "245\nCase study 2: Natural language inference with BERT\nIn practice, concatenating and inserting special tokens are both taken care of by\nSnliReader, an AllenNLP dataset reader specifically built for dealing with the SNLI\ndataset. You can initialize the dataset and observe how it turns the data into AllenNLP\ninstances with the following snippet:\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\nfrom allennlp_models.pair_classification.dataset_readers import SnliReader\nBERT_MODEL = 'bert-base-cased'\ntokenizer = PretrainedTransformerTokenizer(model_name=BERT_MODEL, \nadd_special_tokens=False)\nreader = SnliReader(tokenizer=tokenizer)\ndataset_url = 'https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_dev.jsonl'\nfor instance in reader.read():\n    print(instance)\nThe dataset reader takes a JSONL (JSON line) file from the Stanford NLI corpus and\nturns it into a series of AllenNLP instances. We specify the URL of a dataset file that I\nput online (S3). Note that you need to specify add_special_tokens=False when\ninitializing the tokenizer. This sounds a little bit strange—aren’t the special tokens the\nvery things we need to add here? This is necessary because the dataset reader (Snli-\nReader), not the tokenizer, will take care of the special tokens. If you were to use the\nTransformer library only (without AllenNLP), you wouldn’t need this option.\n The previous snippet produces the following dump of generated instances:\nInstance with fields:\n         tokens: TextField of length 29 with text:\nTransformer layer\nTransformer layer\nSoftmax\nContradict Neutral Entail\nSentence 1\nSentence 2\nBERT\n[CLS]A soccer ...[SEP]Some men ...\nFigure 9.14\nFeeding \nand classifying a pair of \nsentences with BERT\n",
      "content_length": 1732,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "246\nCHAPTER 9\nTransfer learning with pretrained language models\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, \ngo, packages,\n                ., [SEP], The, sisters, are, hugging, goodbye, while, hold-\ning, to, go, \n                packages, after, just, eating, lunch, ., [SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: neutral in namespace: 'labels'.'\nInstance with fields:\n         tokens: TextField of length 20 with text:\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, \ngo, packages,\n                ., [SEP], Two, woman, are, holding, packages, ., [SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: entailment in namespace: 'labels'.'\nInstance with fields:\n         tokens: TextField of length 23 with text:\n                [[CLS], Two, women, are, em, ##bracing, while, holding, to, \ngo, packages,\n                ., [SEP], The, men, are, fighting, outside, a, del, ##i, ., \n[SEP]]\n                and TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n         label: LabelField with label: contradiction in namespace: 'labels'.'\n...\nNotice that every sentence is tokenized, and the sentences are concatenated and sepa-\nrated by [SEP] special tokens. Each instance also has a label field containing the gold\nlabel. \nNOTE\nYou may have noticed some weird characters in the tokenized results,\nsuch as ##bracing and ##i. These are the results of byte-pair encoding (BPE),\na tokenization algorithm for splitting words into what’s called subword units.\nWe’ll cover BPE in detail in chapter 10.\n9.5.3\nUsing Transformers with AllenNLP\nNow we are ready to build our model with AllenNLP. The good news is you don’t need\nto write any Python code to build an NLI model thanks to AllenNLP’s built-in\nmodules—all you need to do is write a Jsonnet config file (as we did in chapter 4).\nAllenNLP also integrates Hugging Face’s Transformer library seamlessly, so you usu-\nally need to make little change, even if you want to integrate Transformer-based mod-\nels such as BERT into your existing models.\n You need to make changes to the following four components when integrating\nBERT into your model and pipeline:\nTokenizer—As you did in section 9.3 earlier, you need to use a tokenizer that\nmatches the pretrained model you are using.\nToken indexer—Token indexers turn tokens into integer indices. Because pre-\ntrained models come with their own predefined vocabularies, it is important\nthat you use a matching token indexer.\n",
      "content_length": 2612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "247\nCase study 2: Natural language inference with BERT\nToken embedder—Token embedders turn tokens into embeddings. This is where\nthe main computation of BERT happens.\nSeq2Vec encoder—The raw output from BERT is a sequence of embeddings. You\nneed a Seq2Vec encoder to turn it into a single embedding vector.\nDon’t worry if this sounds intimidating—in most cases, all you need to do is remember\nto initialize the right modules with the name of the model you want. I’ll walk you\nthrough these steps next.\n First, let’s define the dataset we use for reading and converting the SNLI dataset.\nWe already did this with Python code earlier, but here we will write the corresponding\ninitialization in Jsonnet. First, let’s define the model name we’ll use throughout the\npipeline using the following code. One of the cool features of Jsonnet over vanilla\nJSON is you can define and use variables:\nlocal bert_model = \"bert-base-cased\";\nThe first section of the config file where the dataset is initialized looks like the following:\n\"dataset_reader\": {\n    \"type\": \"snli\",\n    \"tokenizer\": {\n        \"type\": \"pretrained_transformer\",\n        \"model_name\": bert_model,\n        \"add_special_tokens\": false\n    },\n    \"token_indexers\": {\n        \"bert\": {\n            \"type\": \"pretrained_transformer\",\n            \"model_name\": bert_model,\n        }\n    }\n},\nAt the top level, this is initializing a dataset reader specified by the type snli, which is\nthe SnliReader we experimented with previously. The dataset reader takes two\nparameters—tokenizer and token_indexers. For the tokenizer, we initialize a\nPretrainedTransformerTokenizer (type: pretrained_transformer) with a\nmodel name. Again, this is the tokenizer we initialized and used earlier in the Python\ncode. Notice how the Python code and the Jsonnet config file correspond to each\nother nicely. Most of AllenNLP modules are designed in such a way that there’s nice\ncorrespondence between these two, as shown in the following table.\nPython code\nJsonnet config\ntokenizer = \nPretrainedTransformerTokenizer(\n    model_name=BERT_MODEL, \n    add_special_tokens=False)\n\"tokenizer\": {\n    \"type\": \"pretrained_transformer\",\n    \"model_name\": bert_model,\n    \"add_special_tokens\": false\n}\n",
      "content_length": 2226,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "248\nCHAPTER 9\nTransfer learning with pretrained language models\nThe section for initializing a token indexer may look a bit confusing. It is initializing\na PretrainedTransformerIndexer (type: pretrained_transformer) with a\nmodel name. The indexer will store the indexed result to a section named bert (the\nkey corresponding to the token indexer). Fortunately, this code is a boilerplate that\nchanges little from model to model, and chances are you can simply copy and paste\nthis section when you work on a new Transformer-based model.\n As for the training/validation data, we can use the ones in this book’s S3 reposi-\ntory, shown here:\n\"train_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_train.jsonl\",\n\"validation_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_dev.jsonl\",\nNow we are ready to move on to defining our model:\n\"model\": {\n    \"type\": \"basic_classifier\",\n    \"text_field_embedder\": {\n        \"token_embedders\": {\n            \"bert\": {\n                \"type\": \"pretrained_transformer\",\n                \"model_name\": bert_model\n            }\n        }\n    },\n    \"seq2vec_encoder\": {\n        \"type\": \"bert_pooler\",\n        \"pretrained_model\": bert_model\n    }\n},\nAt the top level, this section is defining a BasicClassifier model (type: basic_\nclassifier). It is a generic text classification model that embeds the input, encodes\nit with a Seq2Vec encoder, and classifies it with a classification head (with a softmax\nlayer). You can “plug in” embedders and encoders of your choice as the subcompo-\nnents of the model. For example, you can embed the tokens via word embeddings and\nencode the sequence with an RNN (this is what we did in chapter 4). Alternatively, you\ncan encode the sequence with a CNN, as we did in chapter 7. This is where the design\nof AllenNLP excels—the generic model specifies only what (e.g., a TextField-\nEmbedder and a Seq2VecEncoder) but not exactly how (e.g., word embeddings,\nRNNs, BERT). You can use any submodules for embedding/encoding input, as long\nas those submodules conform to the specified interfaces (i.e., they are subclasses of\nthe required classes).\n In this case study, we will use BERT to embed the input sequence first. This is\nachieved by a special token embedder, PretrainedTransformerEmbedder (type:\npretrained_transformer), which takes the result of a Transformer tokenizer, puts\nit through a pretrained BERT model, and produces the embedded input. You need to\n",
      "content_length": 2479,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "249\nCase study 2: Natural language inference with BERT\npass this embedder as the value for the bert key (the one you specified for\ntoken_indexers earlier) of the token_embedders parameter.\n The raw output from BERT, however, is a sequence of embeddings. Because we are\ninterested in classifying the given pair of sentences, we need to extract the embed-\ndings for the entire sequence, which can be done by taking out the embeddings corre-\nsponding to the CLS special token. AllenNLP implements a type of Seq2VecEncoder\ncalled BertPooler (type: bert_pooler) that does exactly this.\n After embedding and encoding the input, the basic classifier model takes care of\nthe rest—the embeddings go through a linear layer that converts them into a set of\nlogits, and the entire network is trained with a cross-entropy loss, just like other classi-\nfication models. The entire config file is shown here.\nlocal bert_model = \"bert-base-cased\";\n{\n    \"dataset_reader\": {\n        \"type\": \"snli\",\n        \"tokenizer\": {\n            \"type\": \"pretrained_transformer\",\n            \"model_name\": bert_model,\n            \"add_special_tokens\": false\n        },\n        \"token_indexers\": {\n            \"bert\": {\n                \"type\": \"pretrained_transformer\",\n                \"model_name\": bert_model,\n            }\n        }\n    },\n    \"train_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/snli/\nsnli_1.0_train.jsonl\",\n    \"validation_data_path\": \"https://realworldnlpbook.s3.amazonaws.com/data/\nsnli/snli_1.0_dev.jsonl\",\n    \"model\": {\n        \"type\": \"basic_classifier\",\n        \"text_field_embedder\": {\n            \"token_embedders\": {\n                \"bert\": {\n                    \"type\": \"pretrained_transformer\",\n                    \"model_name\": bert_model\n                }\n            }\n        },\n        \"seq2vec_encoder\": {\n            \"type\": \"bert_pooler\",\n            \"pretrained_model\": bert_model,\n        }\n    },\nListing 9.4\nConfig file for training an NLI model with BERT\n",
      "content_length": 1986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "250\nCHAPTER 9\nTransfer learning with pretrained language models\n    \"data_loader\": {\n        \"batch_sampler\": {\n            \"type\": \"bucket\",\n            \"sorting_keys\": [\"tokens\"],\n            \"padding_noise\": 0.1,\n            \"batch_size\" : 32\n        }\n    },\n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 5.0e-6\n        },\n        \"validation_metric\": \"+accuracy\",\n        \"num_epochs\": 30,\n        \"patience\": 10,\n        \"cuda_device\": 0\n    }\n}\nIt’s OK if you are not familiar with what’s going on in the data_loader and trainer\nsections. We’ll discuss these topics (batching, padding, optimizing, hyperparameter\ntuning) in chapter 10. After saving this config file under examples/nli/snli_\ntransformers.jsonnnet, you can start the training process by running the follow-\ning code:\nallennlp train examples/nli/snli_transformers.jsonnet --serialization-dir \nmodels/snli\nThis will run for a while (even on a fast GPU such as Nvidia V100) and produce a large\namount of log messages on stdout. The following is a snippet of log messages I got\nafter four epochs:\n...\nallennlp.training.trainer - Epoch 4/29\nallennlp.training.trainer - Worker 0 memory usage MB: 6644.208\nallennlp.training.trainer - GPU 0 memory usage MB: 8708\nallennlp.training.trainer - Training\nallennlp.training.trainer - Validating\nallennlp.training.tensorboard_writer -                        Training |  Validation\nallennlp.training.tensorboard_writer - accuracy           |     0.933  |     0.908\nallennlp.training.tensorboard_writer - gpu_0_memory_MB    |  8708.000  |       N/A\nallennlp.training.tensorboard_writer - loss               |     0.190  |     0.293\nallennlp.training.tensorboard_writer - reg_loss           |     0.000  |     0.000\nallennlp.training.tensorboard_writer - worker_0_memory_MB |  6644.208  |       N/A\nallennlp.training.checkpointer - Best validation performance so far. Copying weights \nto 'models/snli/best.th'.\nallennlp.training.trainer - Epoch duration: 0:21:39.687226\nallennlp.training.trainer - Estimated training time remaining: 9:04:56\n...\nPay attention to the validation accuracy (0.908). This looks very good considering that\nthis is a three-class classification and the random baseline would be just 0.3. In\n",
      "content_length": 2273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "251\nSummary\ncomparison, when I replaced BERT with an LSTM-based RNN, the best validation\naccuracy I got was around ~0.68. We need to run experiments more carefully to make\na fair comparison between different models, but this result seems to suggest that\nBERT is a powerful model for solving natural language understanding problems.\nSummary\nTransfer learning is a machine learning concept where a model learned for one\ntask is applied to another by transferring knowledge between them. It is an\nunderlying concept for many modern, powerful, pretrained models.\nBERT is a Transformer encoder pretrained with masked language modeling\nand next-sentence prediction objectives to produce contextualized embed-\ndings, a series of word embeddings that take context into account.\nELMo, XLNet, RoBERTa, DistilBERT, and ALBERT are other popular pre-\ntrained models commonly used in modern deep NLP.\nYou can build BERT-based NLP applications by using Hugging Face’s Trans-\nformers library directly, or by using AllenNLP, which integrates the Transform-\ners library seamlessly.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 1116,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "252\nCHAPTER 9\nTransfer learning with pretrained language models\n \n \n \n \n",
      "content_length": 72,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "Part 3\nPutting into production\nIn parts 1 and 2, we learned a lot about the “modeling” part of the modern\nNLP, including word embeddings, RNNs, CNNs, and the Transformer. However,\nyou still need to learn how to effectively train, serve, deploy, and interpret those\nmodels for building robust and practical NLP applications. \n Chapter 10 touches upon important machine learning techniques and best\npractices when developing NLP applications, including batching and padding,\nregularization, and hyperparameter optimization.\n Finally, if chapters 1 to 10 are about building NLP models, chapter 11 covers\neverything that happens outside NLP models. The chapter covers how to deploy,\nserve, explain, and interpret NLP models.\n \n \n",
      "content_length": 725,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "254\nCHAPTER \n",
      "content_length": 13,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "255\nBest practices in developing\nNLP applications\nWe’ve covered a lot of ground so far, including deep neural network models such as\nRNNs, CNNs, and the Transformer, and modern NLP frameworks such as AllenNLP\nand Hugging Face Transformers. However, we’ve paid little attention to the details\nof training and inference. For example, how do you train and make predictions effi-\nciently? How do you avoid having your model overfit? How do you optimize hyper-\nparameters? These factors could make a huge impact on the final performance and\ngeneralizability of your model. This chapter covers these important topics that you\nThis chapter covers\nMaking neural network inference more efficient \nby sorting, padding, and masking tokens\nApplying character-based and BPE tokenization\nfor splitting text into tokens\nAvoiding overfitting via regularization\nDealing with imbalanced datasets by using \nupsampling, downsampling, and loss weighting\nOptimizing hyperparameters\n",
      "content_length": 965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "256\nCHAPTER 10\nBest practices in developing NLP applications\nneed to consider to build robust and accurate NLP applications that perform well in\nthe real world.\n10.1\nBatching instances\nIn chapter 2, we briefly mentioned batching, a machine learning technique where\ninstances are grouped together to form batches and sent to the processor (CPU or,\nmore often, GPU). Batching is almost always necessary when training large neural\nnetworks—it is critical for efficient and stable training. In this section, we’ll dive into\nsome more techniques and considerations related to batching.\n10.1.1 Padding\nTraining large neural networks requires a number of linear algebra operations such as\nmatrix addition and multiplication, which involve executing basic mathematical oper-\nations on many, many numbers at once. This is why it requires specialized hardware\nsuch as GPUs, processors designed to execute such operations in a highly parallelized\nmanner. Data is sent to the GPU as tensors, which are just high-dimensional arrays of\nnumbers, along with some instructions as to what types of mathematical operations it\nneeds to execute. The result is sent back as another tensor.\n In chapter 2, we likened GPUs to factories overseas that are highly specialized and\noptimized for manufacturing the same type of products in a large quantity. Because\nthere is considerable overhead in communicating and shipping products, it is more\nefficient if you make a small number of orders for manufacturing a large quantity of\nproducts by shipping all the required materials in batches, rather than shipping mate-\nrials on demand.\n Materials and products are usually shipped back and forth in standardized contain-\ners. If you have ever loaded a moving pod or trailer yourself (or observed someone\nelse do it), you may know that there are many considerations that are important for\nsafe and reliable shipping. You need to put furniture and boxes in tightly so that they\ndon’t shift around in transition. You need to wrap them with blankets and fix them\nwith ropes to prevent them from being damaged. You need to put heavy stuff at the\nbottom so that lighter stuff won’t get crushed, and so on.\n Batches in machine learning are similar to containers for shipping stuff in the real\nworld. Just like shipping containers are all the same size and rectangular, batches in\nmachine learning are just rectangular tensors packed with numbers of the same type.\nIf you want to “ship” multiple instances of different shapes in a single batch to the\nGPU, you need to pack them so that the packed numbers form a rectangular tensor. \n In NLP, we often deal with sequences of text in different lengths. Because batches\nhave to be rectangular, we need to do padding, (i.e., append special tokens, <PAD>, to\neach sequence so that each row of the tensor has the same length. You need as many\npadding tokens as necessary to make the sequences the same length, which means\nthat you need to pad short sequences until they are all as long as the longest sequence\nin the same batch. This is illustrated in figure 10.1.\n",
      "content_length": 3071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "257\nBatching instances\nFigure 10.1\nPadding and batching. Black squares are tokens, gray ones are EOS tokens, and white ones are \npadding.\nIn reality, each token in natural language text is often represented as a vector of\nlength D, generated by the word embeddings method. This means that each batched\ntensor is a three-dimensional tensor that has a “depth” of D. In many NLP models,\nsequences are represented as batches of size N × L × D (see figure 10.2), where N, L, D\nare the number of instances per batch, the maximum length of the sequences, and\nthe dimension of word embeddings, respectively.\nFigure 10.2\nPadding and batching of embedded sequences create rectangular, three-dimensional tensors.\nThis is starting to look more like real containers!\n10.1.2 Sorting\nBecause each batch has to be rectangular, if one batch happens to include both short\nsequences and long sequences, you need to add a lot of padding to short sequences so\nthat they are as long as the longest sequence in the same batch. This often leads to\nPadding\n& batching\n \nBatch 1\nBatch 2\nBatch 3\nPadding \n& batching\nBatch 1\nBatch 2\nBatch 3\nD\nD\nN\nL\n",
      "content_length": 1121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "258\nCHAPTER 10\nBest practices in developing NLP applications\nsome wasted space in the batch—see “batch 1” in figure 10.3 for an illustration. The\nshortest sequence (six tokens) needs to be padded with eight more tokens to be\nequally long as the longest sequence (14 tokens). Wasted space in a tensor means\nwasted memory and computation, so it is best avoided, but how?\nFigure 10.3\nSorting instances before batching (right) reduces the total number of tensors.\nYou can reduce the amount of padding by putting instances of similar size in the same\nbatch. If shorter instances are batched only with other equally shorter ones, they don’t\nneed to get padded with many padding tokens. Similarly, if longer instances are\nbatched only with other longer ones, they don’t need a lot of padding either, because\nthey are already long. One idea is to sort instances by their length and batch accord-\ningly. Figure 10.3 compares two situations—one in which the instances are batched in\ntheir original order, and the other where instances are first sorted before batching.\nThe numbers below each batch indicate how many tokens are required to represent\nthe batch, including the padding tokens. Notice that the number of total tokens is\nreduced from 144 to 120 by sorting. Because the number of tokens in the original sen-\ntences doesn’t change, this is purely because sorting reduced the number of padding\nBatch 1 \n(4 × 14 = 56)\nBatch 2 \n(4 × 11 = 44)\nBatch 3 \n(4 × 11 = 44)\nBatch 1 \n(4 × 7 = 28)\nBatch 2 \n(4 × 9 = 36)\nBatch 3 \n(4 × 14 = 56)\nPad & batch\nPad & batch\nSort\nTotal \n(56 + 44 + 44 = 144)\nTotal \n(28 + 36 + 56 = 120)\n",
      "content_length": 1613,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "259\nBatching instances\ntokens. Smaller batches require less memory to store and less computation to process,\nso sorting instances before batching improves the efficiency of training.\n All these techniques sound somewhat complicated, but the good news is, you\nrarely need to write code for sorting, padding, and batching instances yourself as long\nas you use high-level frameworks such as AllenNLP. Recall that we used a combination\nof DataLoader and BucketBatchSampler for building our sentiment analysis\nmodel back in chapter 2 as follows:\ntrain_data_loader = DataLoader(train_dataset,\n                               batch_sampler=BucketBatchSampler(\n                                   train_dataset,\n                                   batch_size=32,\n                                   sorting_keys=[\"tokens\"]))\nThe sorting_keys given to BucketBatchSampler specifies which field to use for\nsorting. As you can guess from its name, by specifying “tokens” you are telling the data\nloader to sort the instances by the number of tokens (which is what you want in most\ncases). The pipeline will take care of padding and batching automatically, and the\ndata loader will give you a series of batches you can feed into your model.\n10.1.3 Masking\nOne final detail that you need\nto pay attention to is masking.\nMasking is an operation where\nyou ignore some part of the net-\nwork that corresponds to pad-\nding. This becomes relevant\nespecially when you are dealing\nwith a sequential-labeling or a\nlanguage-generation model. To\nrecap, sequential labeling is a\ntask where the system assigns a\nlabel per token in the input\nsequence. We built a POS tagger\nwith a sequential labeling model\n(RNN) in chapter 5. \n As shown in figure 10.4,\nsequential-labeling models are\ntrained by minimizing the per-\ntoken loss aggregated across all\ntokens in a given sentence. We\ndo this because we’d like to min-\nimize the number of “errors”\nthe network makes per token.\nThis is fine as long as we are\ndealing \nwith \n“real” \ntokens\nstate\nstate\nupdate\nupdate\nupdate\nstate\n \nLinear\nlayer\nLinear\nlayer\n \n \n \nRNN\nTotal \nloss\nlabel\n \nlabel\nlabel\nlogits\n+\n+\n=\nv(\"time\")\nv(\"flies\")\nv(\"like\")\nCross\nentropy\nCross\nentropy\nCross\nentropy\nLinear\nlayer\nFigure 10.4\nLoss for a sequence is the sum of per-\ntoken cross entropy.\n",
      "content_length": 2281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "260\nCHAPTER 10\nBest practices in developing NLP applications\n(“time,” “flies,” and “like” in the figure), although it becomes an issue when the input\nbatch includes padded tokens. Because they exist just to pad the batch, they should be\nignored when computing the total loss.\n We usually do this by creating an extra vector for masking the loss. The vector for\nmasking has the same length as the input, whose elements are 1s for “real” tokens and\n0s for padding. When computing the total loss, you can simply take an element-wise\nproduct between the per-token loss and the mask and then sum up the result.\n Fortunately, as long as you are building standard sequential-labeling models with\nAllenNLP, you rarely need to implement masking yourself. Remember, in chapter 5,\nwe wrote the forward pass of the POS tagger model as shown in listing 10.1. Here, we\nget the mask vector from the get_text_field_mask() helper function and com-\npute the final loss with sequence_cross_entropy_with_logits().\n    def forward(self,\n                words: Dict[str, torch.Tensor],\n                pos_tags: torch.Tensor = None,\n                **args) -> Dict[str, torch.Tensor]:\n        mask = get_text_field_mask(words)\n        embeddings = self.embedder(words)\n        encoder_out = self.encoder(embeddings, mask)\n        tag_logits = self.linear(encoder_out)\n        output = {\"tag_logits\": tag_logits}\n        if pos_tags is not None:\n            self.accuracy(tag_logits, pos_tags, mask)\n            output[\"loss\"] = sequence_cross_entropy_with_logits(\n                tag_logits, pos_tags, mask)\n        return output\nIf you take a peek at what’s inside mask (e.g., by inserting a print statement in this\nforward method), you’ll see the following tensor made of binary (True or False)\nvalues:\ntensor([[ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True,  True,  True],\nListing 10.1\nForward pass of the POS tagger\n",
      "content_length": 2630,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "261\nTokenization for neural models\nEach row of this tensor corresponds to one sequence of tokens, and locations with\nFalse are where padding occurred. The loss function (sequence_cross_entropy\n_with_logits) receives the prediction, the ground truth (the correct labels), and\nthe mask and computes the final loss while ignoring all the elements marked as\nFalse.\n10.2\nTokenization for neural models\nIn chapter 3, we covered the basic linguistic units (words, characters, and n-grams)\nand how to compute their embeddings. In this section, we will go deeper and focus on\nhow to analyze texts and obtain these units—a process called tokenization. Neural net-\nwork models pose a set of unique challenges on how to deal with tokens, and we’ll\ncover some of the modern models to deal with these challenges.\n10.2.1 Unknown words\nA vocabulary is a set of tokens that an NLP model deals with. Many neural NLP models\noperate within a fixed, finite set of tokens. For example, when we built a sentiment ana-\nlyzer in chapter 2, the AllenNLP pipeline first tokenized the training dataset and con-\nstructed a Vocabulary object that consists of all unique tokens that appeared more\nthan, say, three times. The model then uses an embedding layer to convert tokens into\nword embeddings, which are some abstract representation of the input tokens.\n So far, so good, right? But the number of all words in the world is not finite. We\nconstantly make up new words that didn’t exist before (I don’t think people talked\nabout “NLP” a hundred years ago). What if the model receives a word that it has never\nseen during training? Because the word is not part of the vocabulary, the model can-\nnot even convert it to an index, let alone look up its embeddings. Such words are\ncalled out-of-vocabulary (OOV) words, and they are one of the biggest problems when\nbuilding NLP applications. \n By far the most common (but not the best) way to deal with this problem is to rep-\nresent all the OOV tokens as a special token, which is conventionally named UNK (for\n“unknown”). The idea is that every time the model sees a token that is not part of the\nvocabulary, it pretends it saw a special token UNK instead and proceeds as usual. This\nmeans that the vocabulary and the embedding table both have a designated “slot” for\nUNK so that the model can deal with words that it has never seen. The embeddings (and\nany other parameters) for UNK are trained in the same manner as other regular tokens.\n Do you see any problems with this approach? Treating all OOV tokens with a single\nUNK token means that they are collapsed into a single embedding vector. It doesn’t\nmatter if the word is “NLP” or “doggy”—as long as it’s something unseen, it always gets\ntreated as a UNK token and assigned the same vector, which becomes a generic, catch-\nall representation of various words. Because of this, the model cannot tell the differ-\nences among OOV words, no matter what the identity of the words is.\n This may be fine if you are building, for example, a sentiment analyzer. OOV\nwords are, by definition, very rare and might not affect the prediction of most of the\n",
      "content_length": 3120,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "262\nCHAPTER 10\nBest practices in developing NLP applications\ninput sentences. However, this becomes a huge problem if you are building a machine\ntranslation system or a conversational engine. It wouldn’t be a usable MT system or a\nchatbot if it produces “I don’t know” every time it sees new words! In general, the\nOOV problem is more serious for language-generation systems (including machine\ntranslation and conversational AI) compared to NLP systems for prediction (senti-\nment analysis, POS tagging, and so on).\n How can we do better? OOV tokens are such a big problem in NLP that there has\nbeen a lot of research work on how to deal with them. In the following subsections,\nwe’ll cover character-based and subword-based models, two techniques commonly\nused for building robust neural NLP models.\n10.2.2 Character models\nThe simplest yet effective solution for dealing with the OOV problem is to treat char-\nacters as tokens. Specifically, we break the input text into individual characters, even\nincluding punctuation and whitespace, and treat them as if they are regular tokens.\nThe rest of the application is unchanged—“word” embeddings are assigned to charac-\nters, which are further processed by the model. If the model produces text, it does so\ncharacter-by-character.\n In fact, we used a character-level model in chapter 5 when we built a language gen-\nerator. Instead of generating text word-by-word, the RNN produces text one character\nat a time, as illustrated in figure 10.5. Thanks to this strategy, the model was able to\nproduce words that look like English but actually aren’t. Notice a number of peculiar\nwords (despoit, studented, redusention, distaples) that resemble English words in the out-\nput shown in listing 10.2. If the model operated on words, it produces only known\nwords (or UNKs when unsure), and this wouldn’t have been possible. \nFigure 10.5\nA language-generation model that generates text character-by-\ncharacter (including whitespace)\n<START>\nT\nT\nCharacter\nembeddings\n \nh\ng\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n.\n...\nh\ne\n.\n<END>\nHidden\nstates\n \nRecurrent\nneural\nnetwork\n(RNN)\n \n \n \nLogits\nLinear\nLinear\nLinear\nLinear\nLinear\nT h e _ q u i c k _ b r … _ d o g .\n",
      "content_length": 2185,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "263\nTokenization for neural models\nYou can say that you don't know it, and why decided of yourself.\nPike of your value is to talk of hubies.\nThe meeting despoit from a police?\nThat's a problem, but us?\nThe sky as going to send nire into better.\nWe'll be look of the best ever studented.\nThere's you seen anything every's redusention day.\nHow a fail is to go there.\nIt sad not distaples with money.\nWhat you see him go as famous to eat!\nCharacter-based models are versatile and put few assumptions on the structure of the\nlanguage. For languages with small sets of alphabets (like English), it effectively eradi-\ncates unknown words, because almost any words, no matter how rare they are, can be\nbroken down into characters. Tokenizing into characters is also an effective strategy\nfor languages with large alphabets (like Chinese), although you need to watch out for\n“unknown character” problems.\n However, this strategy is not without drawbacks. The biggest issue is its inefficiency.\nTo encode a sentence, the network (be it an RNN or the Transformer) needs to go\nover all the characters in it. For example, a character-based model needs to process\n“t,” “h,” “e,” and “_” (whitespace) to process a single word “the,” whereas a word-based\nmodel can finish this in a single step. This inefficiency takes its biggest toll on the\nTransformers, where the attention computation increases quadratically when the\ninput sequence gets longer.\n10.2.3 Subword models\nSo far, we studied two extremes—the word-based approach is efficient but not great at\ndealing with unknown words. The character-based approach is great at dealing with\nunknown words but is inefficient. Is there something in between? Can we use some\ntokenization that is both efficient and robust to unknown words?\n Subword models are a recent invention that addresses this problem for neural net-\nworks. In subword models, the input text is segmented into a unit called subwords,\nwhich simply means something smaller than words. There is no formal linguistic defi-\nnition as to what subwords actually are, but they roughly correspond to part of words\nthat appear frequently. For example, one way to segment “dishwasher” is “dish + wash\n+ er,” although some other segmentation is possible.\n Some varieties of algorithms (such as WordPiece1 and SentencePiece2) tokenize\ninput into subwords, but by far the most widely used is byte-pair encoding (BPE).3 BPE was\nListing 10.2\nGenerated sentences by a character-based language model\n1 Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine\nTranslation,” (2016). https://arxiv.org/abs/1609.08144.\n2 Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Can-\ndidates,” (2018). https://arxiv.org/abs/1804.10959.\n3 Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units,” (2016). https://arxiv.org/\nabs/1508.07909.\n",
      "content_length": 2923,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "264\nCHAPTER 10\nBest practices in developing NLP applications\noriginally invented as a compression algorithm,4 but since 2016, it’s been widely used as\na tokenization method for neural models, particularly in machine translation.\n The basic concept of BPE is to keep frequent words (such as “the” and “you”) and\nn-grams (such as “-able” and “anti-”) unsegmented, while breaking up rarer words\n(such as “dishwasher”) into subwords (“dish + wash + er”). Keeping frequent words\nand n-grams together helps the model process those tokens efficiently, whereas break-\ning up rare words ensures there are no UNK tokens, because everything can be ulti-\nmately broken up into individual characters, if necessary. By flexibly choosing where\nto tokenize based on the frequency, BPE achieves the best of two worlds—being effi-\ncient while addressing the unknown word problem.\n Let’s see how BPE determines where to tokenize with real examples. BPE is a\npurely statistical algorithm (it doesn’t use any language-dependent information) and\noperates by merging the most frequently occurring pair of consecutive tokens, one at\na time. First, BPE tokenizes all the input texts into individual characters. For example,\nif your input is four words low, lowest, newer, and wider, it will tokenize them into\nl o w _, l o w e s t _, n e w e r _, w i d e r _. Here, “_” is a special sym-\nbol that indicates the end of each word. Then, the algorithm identifies any two con-\nsecutive elements that appear most often. In this example, the pair l o appears most\noften (two times), so these two characters are merged, yielding lo w _, lo w e s t\n_, n e w e r _, w i d e r _. Then, lo w will be merged into low, e r into er, er\n_ into er_, at which time you have low _, low e s t _, n e w er_, w i d er_.\nThis process is illustrated in figure 10.6.\nFigure 10.6\nBPE learns subword units by iteratively merging consecutive units that cooccur frequently.\n4 See https://www.derczynski.com/papers/archive/BPE_Gage.pdf.\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\nl\no\nw\n_\nl\no\nw\ne\ns\nt\n_\nn\ne\nw\ne\nr\n_\nw\ni\nd\ne\nr\n_\n",
      "content_length": 2213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "265\nAvoiding overfitting\nNotice that, after four merge operations, lowest is segmented into low e s t where\nfrequent substrings such as low are merged together whereas infrequent ones such as\nest are broken apart. To segment a new input (e.g., lower), the same sequence of\nmerge operations is applied in order, yielding low e r _. If you start from 52 unique\nletters (26 upper- and lowercase letters), you will end up with 52 + N unique tokens in\nyour vocabulary, where N is the number of merge operations executed. In this way,\nyou have complete control over the size of the vocabulary.\n In practice, you rarely need to implement BPE (or any other subword tokenization\nalgorithms) yourself. These algorithms are implemented in many open source libraries\nand platforms. Two popular options are Subword-NMT (https://github.com/rsenn-\nrich/subword-nmt) and SentencePiece (https://github.com/google/sentencepiece)\n(which also supports a variant of subword tokenization using a unigram language\nmodel). Many of the default tokenizers shipped with NLP frameworks, such as the one\nimplemented in Hugging Face Transformers, support subword tokenization.\n10.3\nAvoiding overfitting\nOverfitting is one of the most common and important issues you need to address when\nbuilding any machine learning applications. An ML model is said to overfit when it\nfits the given data so well that it loses its generalization ability to unseen data. In other\nwords, the model may capture the training data very well and show good performance\non it, but it may not be able to capture its inherent patterns well and shows poor per-\nformance on data that the model has never seen before. \n Because overfitting is so prevalent in machine learning, researchers and practi-\ntioners have come up with a number of algorithms and techniques to combat overfit-\nting in the past. In this section, we’ll learn two such techniques—regularization and\nearly stopping. These are popular in any ML applications (not just NLP) and worth\ngetting under your belt.\n10.3.1 Regularization\nRegularization in machine learning refers to techniques that encourage the simplicity\nand the generalization of the model. You can think of it as one form of penalty you\nimpose on your ML model to ensure that it is as generic\nas possible. What does it mean? Say you are building an\n“animal classifier” by training word embeddings from a\ncorpus and by drawing a line between animals and\nother stuff in this embedding space (i.e., you represent\neach word as a multidimensional vector and classify\nwhether the word describes an animal based on the\ncoordinates of the vector). Let’s simplify this problem a\nlot and assume that each word is a two-dimensional vec-\ntor, and you end up with the plot shown in figure 10.7.\nYou can now visualize how a machine learning model\nmakes a classification decision by drawing lines where\nbat\nhot\ndog\ncat\nchocolate\npizza\nFigure 10.7\nAnimal vs. non-\nanimal classification plot\n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "266\nCHAPTER 10\nBest practices in developing NLP applications\nthe decision flips between different classes (animals and non-animals), which is called\nthe classification boundary. How would you draw a classification boundary so that ani-\nmals (blue circles) are separated from everything else (triangles)?\n One simple way to separate animals is to draw one straight line, as in the first plot\nin figure 10.8. This simple classifier makes several mistakes (in classifying words like\n“hot” and “bat”), but it correctly classifies the majority of data points. This sounds like\na good start.\nFigure 10.8\nClassification boundaries with increasing complexity\nWhat if you are told that the decision boundary doesn’t have to be a straight line? You\nmay want to draw something like the one shown in the middle in figure 10.8. This one\nlooks better—it makes fewer mistakes than the first one, although it is still not perfect.\nIt appears tractable for a machine learning model because the shape is simple. \n But there’s nothing that stops you here. If you want to make as few errors as possi-\nble, you can also draw something wiggly like the one shown in the third plot. That\ndecision boundary doesn’t even make any classification errors, which means that we\nachieved 100% classification accuracy!\n Not so fast—remember that up until here, we’ve been thinking only about the\ntraining time, but the main purpose of machine learning models is to achieve good\nclassification performance at the test time (i.e., they need to classify unobserved, new\ninstances as correctly as possible). Now let’s think about how the three decision bound-\naries described earlier fare at test time. If we assume the test instances are distributed\nsimilarly to the training instances we saw in figure 10.8, the new “animal” points are\nmost likely to fall in the upper-right region of the plot. The first two decision boundar-\nies will achieve decent accuracy by classifying the majority of new instances correctly.\nBut how about the third one? Training instances such as “hot” shown in the plot are\nmost likely exceptions rather than the norm, so the curved sections of the decision\nboundary that tried to accommodate as many training instances as possible may do\nmore harm than good at the test time by inadvertently misclassifying test instances.\nThis is exactly what overfitting looks like—the model fits the training data so well that\nit sacrifices its generalization ability, which is what’s happening here.\nbat\nhot\ndog\ncat\nchocolate\npizza\nbat\nhot\ndog\ncat\nchocolate\npizza\nbat\nhot\ndog\ncat\nchocolate\npizza\nUnderfitting\nJust right\nOverfitting\n",
      "content_length": 2611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "267\nAvoiding overfitting\n Then, the question is, how can we avoid having your model look like the third\ndecision boundary? After all, it is doing a very good job correctly classifying the train-\ning data. If you looked only at the training accuracy and/or the loss, there would be\nnothing to stop you from choosing it. One way to avoid overfitting is to use a separate,\nheld-out dataset (called a validation set; see section 2.2.3) to validate the performance\nof your model. But can we do this even without using a separate dataset?\n The third decision boundary just doesn’t look right—it’s overly complex. With all\nother things being equal, we should prefer simpler models, because in general, sim-\npler models generalize better. This is also in line with Occam’s razor, which states that\na simpler solution is preferable to a more complex one. How can we balance between\nthe training fit and the simplicity of the model?\n This is where regularization comes into play. Think of regularization as additional\nconstraints imposed on the model so that simpler and/or more general models are\npreferred. The model is optimized so that it achieves the best training fit while being\nas generic as possible.\n Numerous regularization techniques have been proposed in machine learning\nbecause overfitting is such an important topic. We are going to introduce only a few\nof the most important ones—L2 regularization (weight decay), dropout, and early\nstopping.\nL2 REGULARIZATION\nL2 regularization, also called weight decay, is one of the most common regularization\nmethods not just for NLP or deep learning but for a wide range of ML models. We are\nnot going into its mathematical details, but in short, L2 regularization adds a penalty\nfor the complexity of a model measured by how large its parameters are. To represent\na complex classification boundary, an ML model needs to adjust a large number of\nparameters (the “magic constants”) to extreme values, measured by the L2 loss, which\ncaptures how far away they are from zero. Such models incur a larger L2 penalty, which\nis why L2 encourages simpler models. If you are interested in learning more about L2\nregularization (and other related topics about NLP in general), check out textbooks\nsuch as Speech and Language Processing by Jurafsky and Martin (https://web.stanford\n.edu/~jurafsky/slp3/5.pdf) or Goodfellow et al.’s Deep Learning (https://www.deep\nlearningbook.org/contents/regularization.html).\nDROPOUT\nDropout is another popular regularization technique commonly used with neural net-\nworks. Dropout works by randomly “dropping” neurons during training, where a “neu-\nron” is basically a dimension of an intermediate layer and “dropping” means to mask it\nwith zeros. You can think of dropout as a penalty to the model’s structural complexity\nand its reliance on particular features and values. As a result, the network tries to make\nthe best guess with the remaining smaller number of values, which forces it to general-\nize well. Dropout is easy to implement and effective in practice and is used as a default\nregularization method in many deep learning models. For more information on drop-\nout, the regularization chapter of the Goodfellow book mentioned earlier provides a\ngood introduction and mathematical details of regularization techniques.\n",
      "content_length": 3303,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "268\nCHAPTER 10\nBest practices in developing NLP applications\n10.3.2 Early stopping\nAnother popular approach for combatting overfitting in machine learning is early stop-\nping. Early stopping is a relatively simple technique where you stop training your\nmodel when the model performance stops improving, usually measured by the valida-\ntion set loss. In chapter 6, we plotted learning curves when we built an English-\nSpanish machine translation model (shown again in figure 10.9). Notice that the vali-\ndation loss curve flattens out around the eighth epoch and starts to creep up after\nthat, which is a sign of overfitting. Early stopping would detect this, stop the training,\nand use the result from the best epoch when the loss is lowest. In general, early stop-\nping has a “patience” parameter, which is the number of nonimproving epochs for\nearly stopping to kick in. When patience is 10 epochs, for example, the training pipe-\nline will wait 10 epochs after the loss stops improving to stop the training.\nFigure 10.9\nThe validation loss curve flattens out around the eighth epoch and creeps back up.\nWhy does early stopping help mitigate overfitting? What does it have to do with model\ncomplexity? Without getting into mathematical details, it takes some time (training\nepochs) for the model to learn complex, overfitted decision boundaries. Most models\nstart from something simple (e.g., straight decision lines) and gradually increase their\ncomplexity over the course of training. By stopping the training early, early stopping\ncan prevent the model from becoming overly complex.\n Many machine learning frameworks have built-in support for early stopping. For\nexample, AllenNLP’s trainer supports early stopping by default. Recall that we used\nthe following configuration in section 9.5.3 when we trained a BERT-based natural\n0\n2\n4\n6\n0\n10\n20\n30\nepoch\nloss\nstage\ntrain\nvalid\n",
      "content_length": 1882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "269\nAvoiding overfitting\nlanguage inference model, where we used early stopping (with patience = 10) without\npaying much attention. This allows the trainer to stop if the validation metric doesn’t\nimprove for 10 epochs: \n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 1.0e-5\n        },\n        \"num_epochs\": 20,\n        \"patience\": 10,\n        \"cuda_device\": 0\n    }\n10.3.3 Cross-validation\nCross-validation is not exactly a regularization method, but it is one of the techniques\ncommonly used in machine learning. A common situation in building and validating\na machine learning model is this—you have only a couple of hundred instances avail-\nable for training. As we’ve seen so far in this book, you can’t train a reliable ML model\njust on the training set—you need a separate set for validation, and preferably another\nseparate set for testing. How much you use for validation/testing depends on the task\nand the data size, but in general, it is advised that you set aside 5–20% of your training\ninstances for validation and testing. This means that if your training data is small, your\nmodel is validated and tested on just a few dozen instances, which can make the esti-\nmated metrics unstable. Also, how you choose these instances has a large impact on\nthe evaluation metrics, which is not ideal.\n The basic idea of cross-validation is to iterate this phase (splitting the dataset into\ntraining and validation portions) multiple times with different splits to improve the\nstability of the result. Specifically, in a typical setting called k-fold cross validation, you\nfirst split the dataset into k different portions of equal size called folds. You use one of\nthe folds for validation while training the model on the rest (k – 1 folds), and repeat\nthis process k times, using a different fold for validation every time. See figure 10.10\nfor an illustration.\nFigure 10.10\nIn k-fold cross validation, the dataset is split into k equally sized \nfolds and one is used for validation.\nFold 1\nFold 2\nFold 3\nFold 4\nFold 5\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\nValidation\nValidation\nValidation\nValidation\nValidation\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\nTrain\n",
      "content_length": 2310,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "270\nCHAPTER 10\nBest practices in developing NLP applications\nThe validation metrics are computed for every fold, and the final metrics are averaged\nover all iterations. This way, you can obtain a more stable estimate of the evaluation\nmetrics that are not impacted heavily by the way the dataset is split. \n The use of cross-validation in deep learning models is not common, because these\nmodels require a large amount of data, and you don’t need cross-validation if you\nhave a large dataset, although its use is more common for more traditional and indus-\ntrial settings where the amount of training data is limited.\n10.4\nDealing with imbalanced datasets\nIn this section, we’ll focus on one of the most common problems you may encounter in\nbuilding NLP and ML models—the class imbalance problem. The goal of a classifica-\ntion task is to assign one of the classes (e.g., spam or nonspam) to each instance (e.g.,\nan email), but these classes are rarely distributed evenly. For example, in spam filtering,\nthe number of nonspam emails is usually larger than the number of spam emails. In\ndocument classification, some topics (such as poli-\ntics or sports) are usually more popular than other\ntopics. Classes are said to be imbalanced when some\nclasses have way more instances than others (see fig-\nure 10.11 for an example). \n   Many classification datasets have imbalanced\nclasses, which poses some additional challenges\nwhen you train your classifier. The signals your\nmodel gets from smaller classes are overwhelmed by\nlarger classes, which causes your model to perform\npoorly on minority classes. In the following subsec-\ntions, I’m going to discuss some techniques you can\nconsider when faced with an imbalanced dataset.\n10.4.1 Using appropriate evaluation metrics\nBefore you even begin tweaking your dataset or your model, make sure you are vali-\ndating your model with an appropriate metric. In section 4.3, we discussed why it is a\nbad idea to use accuracy as your evaluation metric when the dataset is imbalanced. In\none extreme case, if 90% of your instances belong to class A and the other 10%\nbelong to class B, even a stupid classifier that assigns class A to everything can achieve\n90% accuracy. This is called a majority class baseline. A slightly more clever (but still stu-\npid) classifier that randomly assigns label A 90% of the time and label B 10% of the\ntime without even looking at the instance will achieve 0.9 * 0.9 + 0.1 * 0.1 = 82% accu-\nracy. This is called a random baseline, and the more imbalanced your dataset is, the\nhigher the accuracy of these baseline models will become.\n But this kind of random baseline is rarely a good model for minority classes. Imag-\nine what would happen to class B if you used the random baseline. Because it will\nassign class A 90% of the time no matter what, 90% of the instances belonging to class\nB will be assigned class A. In other words, the accuracy of this random baseline for\nClass A\nClass B\nInstances\nFigure 10.11\nImbalanced \ndataset\n",
      "content_length": 3005,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "271\nDealing with imbalanced datasets\nclass B is only 10%. If this was a spam filter, it would let 90% of spam emails go\nthrough, no matter what the content is, just because 90% of emails you receive are not\nspam! This would make a terrible spam filter.\n If your dataset is imbalanced and you care about the classification performance on\nthe minority class, you should consider using metrics that are more appropriate for\nsuch settings. For example, if your task is a “needle in a haystack” type of setting, where\nthe goal is to find a very small number of instances among others, you may want to use\nthe F1-measure instead of accuracy. As we saw in chapter 4, the F-measure is some sort\nof average between precision (how hay-free your prediction is) and recall (how much\nof the needle you actually found). Because the F1-measure is calculated per class,\nit does not underrepresent minority classes. If you’d like to measure the model’s over-\nall performance including majority classes, you can compute the macro-averaged\nF-measure, which is simply an arithmetic average of F-measures computed per class.\n10.4.2 Upsampling and downsampling\nNow let’s look at concrete techniques that can mitigate the class imbalance problem.\nFirst of all, if you can collect more labeled training data, you should seriously consider\ndoing that first. Unlike academic and ML competition settings where the dataset is\nfixed while you tweak your model, in a real-world setting you are free to do whatever is\nnecessary to improve your model (of course, as long as it’s lawful and practical).\nOften, the best thing you can do to improve a model’s generalization is expose it to\nmore data.\n If your dataset is imbalanced and the model is making biased predictions, you\ncan either upsample or downsample your data so that classes have roughly equal\nrepresentations. \n In upsampling (see the second figure in figure 10.12), you artificially increase the\nsize of the minority class by copying the instances multiple times. Take the scenario we\ndiscussed earlier for example—if you duplicate the instances of class B and add eight\nextra copies of each instance to the dataset, they have an equal number of instances.\nClass A\nClass B\nInstances\nClass A\nClass B\nInstances\nClass A\nClass B\nInstances\nUpsampling\nDownsampling\nFigure 10.12\nUpsampling and downsampling\n",
      "content_length": 2332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "272\nCHAPTER 10\nBest practices in developing NLP applications\nThis can mitigate the biased prediction issue. More sophisticated data augmentation\nalgorithms such as SMOTE5 are available, although they are not widely used in NLP,\ndue to the inherent difficulty in generating linguistic examples artificially.\n If your model is biased not because the minority class is too small but because the\nmajority class is too large, you can instead choose to downsample (the third figure in\nfigure 10.12). In downsampling, you artificially decrease the size of the majority class by\nchoosing a subset of the instances belonging to that class. For example, if you sample\none out of nine instances from class A, you’ll end up with the equal number of instances\nin classes A and B. You can downsample in multiple ways—the easiest is to randomly\nchoose the subset. If you would like to make sure that the downsampled dataset still pre-\nserves the diversity in the original data, you can try stratified sampling, where you sample\nsome number of instances per group defined by some attributes. For example, if you\nhave too many nonspam emails and want to downsample, you can group them by the\nsender’s domain first, then sample a fixed number of emails per domain. This will\nensure that your sampled dataset will contain a diverse set of domains.\n Note that neither upsampling nor downsampling is a magic bullet. If you “correct”\nthe distribution of classes too aggressively, you risk making unfair predictions for the\nmajority class, if that’s what you care about. Always make sure to check your model\nwith a held-out validation set with appropriate evaluation metrics. \n10.4.3 Weighting losses\nAnother approach for mitigating the class imbalance problem is to use weighting\nwhen computing the loss, instead of making modification to your training data.\nRemember that the loss function is used to measure how “off” the model’s prediction\nfor an instance is compared against the ground truth. When you measure how bad the\nmodel’s prediction is, you can tweak the loss so that it penalizes more when the\nground truth belongs to the minority class.\n    Let’s take a look at a concrete example. The\nbinary cross-entropy loss, a common loss func-\ntion used for training a binary classifier, looks\nlike the curve shown in figure 10.13, when the\ncorrect label is 1. The x-axis is the predicted\nprobability of the target class, and the y-axis is the\namount of loss the prediction will incur. When\nthe prediction is perfectly correct (probability\n= 1), there’s no penalty, whereas as the predic-\ntion gets worse (probability < 1), the loss goes up.\n   If you care more about the model’s perfor-\nmance on the minority class, you can tweak this\nloss. Specifically, you can change the shape of\nthis loss (by simply multiplying it by a constant\n5 Chawla et al., “SMOTE: Synthetic Minority Over-Sampling Technique,” (2002). https://arxiv.org/abs/\n1106.1813.\nPrediction\n0\nLoss\n1\nFigure 10.13\nBinary cross-entropy loss \n(when the correct label is 1)\n",
      "content_length": 3017,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "273\nHyperparameter tuning\nnumber) just for that class so that the model\nincurs a larger loss when it makes mistakes on the\nminority class. One such tweaked loss curve is\nshown in the figure 10.14 as the top curve. This\nweighting has the same effect as upsampling the\nminority class, although modifying the loss is\ncomputationally cheaper because you don’t need\nto actually increase the amount of training data.\n It is easy to implement loss weighting in\nPyTorch and AllenNLP. PyTorch’s binary cross-\nentropy implementation BCEWithLogitLoss\nalready supports different weights for different\nclasses. You simply need to pass the weight as the\npos_weight parameter as follows:\n>>> import torch\n>>> import torch.nn as nn\n>>> input = torch.randn(3)\n>>> input\ntensor([-0.5565,  1.5350, -1.3066])\n>>> target = torch.empty(3).random_(2)\n>>> target\ntensor([0., 0., 1.])\n>>> loss = nn.BCEWithLogitsLoss(reduction='none')\n>>> loss(input, target)\ntensor([0.4531, 1.7302, 1.5462])\n>>> loss = nn.BCEWithLogitsLoss(reduction='none', \npos_weight=torch.tensor(2.))\n>>> loss(input, target)\ntensor([0.4531, 1.7302, 3.0923])\nIn this code snippet, we randomly generate prediction (input) and the ground truth\n(target). There are three instances in total, two of which are of class 0 (majority)\nand one belongs to class 1 (minority). We first compute the loss without weighting by\ncalling the BCEWithLogitsLoss object, which returns the three loss values, one for\neach instance. We then compute the loss with weighting by passing the weight 2—this\nmeans that the wrong prediction will be penalized twice as much if the target class is\npositive (class 1). Notice that the third element corresponding to class 1 is twice as\nlarge as the one returned by the unweighted loss function. \n10.5\nHyperparameter tuning\nIn this final section of this chapter, we’ll discuss hyperparameter tuning. Hyperparame-\nters are parameters about the model and the training algorithm. This term is used in\ncontrast with parameters, which are numbers that are used by the model to make\nPrediction\n0\nLoss\n1\nFigure 10.14\nWeighted binary cross \nentropy loss\n",
      "content_length": 2108,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "274\nCHAPTER 10\nBest practices in developing NLP applications\npredictions from the input. This is what we’ve been calling “magic constants” through-\nout this book—they work like constants in programming languages, although their\nexact values are automatically adjusted by optimization so that the prediction matches\nthe desired output as closely as possible.\n Correctly tuning hyperparameters is critical for many machine learning models to\nwork properly and achieve their highest potential, and ML practitioners spend a lot of\ntime tuning hyperparameters. Knowing how to tune hyperparameters effectively has a\nhuge impact on your productivity in building NLP and ML systems.\n10.5.1 Examples of hyperparameters\nHyperparameters are “meta”-level parameters—unlike model parameters, they are\nused not to make predictions but for controlling the structure of the model and how\nthe model is trained. For example, if you are working on word embeddings or an\nRNN, how many hidden units (dimensions) to use for representing words is one\nimportant hyperparameter. The number of RNN layers to use is another hyperparam-\neter. In addition to these two hyperparameters (the number of hidden units and lay-\ners), the Transformer model we covered in chapter 9 has a number of other\nparameters, such as the number of attention heads and the dimension of the feed-\nforward network. Even the type of architecture you use, such as RNN versus Trans-\nformer, can be thought of as one hyperparameter.\n Besides, the optimization algorithm you use may have hyperparameters, too. For\nexample, the learning rate (section 9.3.3), one of the most important hyperparame-\nters in many ML settings, determines how much to tweak the model parameters per\noptimization step. The number of epochs (iterations through the training dataset) is\nalso an important hyperparameter, too.\n So far, we have been paying little attention to those hyperparameters, let alone\noptimizing them. However, hyperparameters can have a huge impact on the perfor-\nmance of machine learning models. In fact, many ML models have a “sweet spot” of\nhyperparameters that makes them most effective, whereas using a set of hyperparame-\nters outside of this spot may make the model perform poorly. \n Many ML practitioners tune hyperparameters by hand. This means that you start\nfrom a set of hyperparameters that look reasonable and measure the model’s perfor-\nmance on a validation set. Then you change one or more of the hyperparameters\nslightly and measure the performance again. You repeat this process several times\nuntil you hit the “plateau,” where any change of hyperparameters provides only a mar-\nginal improvement.\n One issue with this manual tuning approach is that it is slow and arbitrary. Let’s say\nyou start from one set of hyperparameters. How do you know which ones to adjust\nnext, and how much? How do you know when to stop? If you have experience tuning\na wide range of ML models, you might have some “hunch” about how these models\nrespond to certain hyperparameter changes, but if not, it’s like shooting in the dark.\nHyperparameter tuning is such an important topic that ML researchers have been\nworking on better and more organized ways to optimize them.\n",
      "content_length": 3214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "275\nHyperparameter tuning\n10.5.2 Grid search vs. random search\nWe understand that manual optimization of hyperparameters is inefficient, but how\nshould we go about optimizing them, then? We have two more-organized ways of tun-\ning hyperparameters—grid search and random search. \n In grid search, you simply try every possible combination of the hyperparameter val-\nues you want to optimize. For example, let’s assume your model has just two hyperpa-\nrameters—the number of RNN layers and the embedding dimension. You first define\nreasonable ranges for these two hyperparameters, for example, [1, 2, 3] for the num-\nber of layers and [128, 256, 512] for the dimensionality. Then grid search measures\nthe model’s validation performance for every combination—(1, 128), (1, 256),\n(1, 512), (2, 128), . . . , (3, 512)—and simply picks the best-performing combination.\nIf you plot these combinations on a 2-D plot, it looks like a grid (see the illustration in\nfigure 10.15), which is why this is called grid search. \n Grid search is a simple and intuitive way to optimize the hyperparameters. How-\never, if you have many hyperparameters and/or their ranges are large, this method\ngets out of hand. The number of possible combinations is exponential, which makes it\nimpossible to explore all of them in a reasonable amount of time.\nFigure 10.15\nGrid search vs. random search for hyperparameter tuning. \n(Adapted from Bergstra and Bengio, 2012; https://www.jmlr.org/papers/\nvolume13/bergstra12a/bergstra12a.pdf.)\nA better alternative to grid search is random search. In random search, instead of trying\nevery possible combination of hyperparameter values, you randomly sample the val-\nues and measure the model’s performance on a specified number of combinations\n(which are called trials). For example, in the previous example, random search may\nchoose (2, 87), (1, 339), (2, 101), (3, 254), and so on until it hits the specified number\nof trials. See the illustration in figure 10.15 (right).\nGrid search\nUnimportant parameter\nImportant parameter\nUnimportant parameter\nImportant parameter\nRandom search\n",
      "content_length": 2097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "276\nCHAPTER 10\nBest practices in developing NLP applications\n Unless your hyperparameter search space is very small (like the first example), ran-\ndom search is usually recommended over grid search if you want to optimize hyper-\nparameters efficiently. Why? In many machine learning settings, not every hyper-\nparameter is made equal—there are usually only a small number of hyperparameters\nthat actually matter for the performance, whereas many others do not. Grid search\nwill waste a lot of computation searching for the best combination of hyperparameters\nthat do not really matter, while being unable to explore the few hyperparameters that\ndo matter in detail (figure 10.15, left). On the other hand, random search can\nexplore many possible points on the axis that matters for the performance (figure\n10.15, right). Notice that random search can find a better model by exploring more\npoints on the x-axis with the same number of trials (total of nine trials).\n10.5.3 Hyperparameter tuning with Optuna\nOK, we’ve covered some ways to tune hyperparameters including manual, grid, and\nrandom search, but how should you go about implementing it in practice? You can\nalways write your own for-loop (or “for-loops,” in the case of grid search), although it\nwould quickly get tiring if you need to write this type of boilerplate code for every\nmodel and task you work on. \n Hyperparameter optimization is such a universal topic that many ML researchers\nand engineers have been working on better algorithms and software libraries. For exam-\nple, AllenNLP has its own library called Allentune (https://github.com/allenai/allen-\ntune) that you can easily integrate with your AllenNLP training pipeline. In the\nremainder of this section, however, I’m going to introduce another hyperparameter\ntuning library called Optuna (https://optuna.org/) and show how to use it with Allen-\nNLP to optimize your hyperparameters. Optuna implements state-of-the-art algorithms\nthat search for optimal hyperparameters efficiently and provides integration with a wide\nrange of machine learning frameworks, including TensorFlow, PyTorch, and AllenNLP.\n First, we assume that you have installed AllenNLP (1.0.0+) and the Optuna plugin\nfor AllenNLP. These can be installed by running the following:\npip install allennlp\npip install allennlp_optuna\nAlso, as instructed by the official documentation (https://github.com/himkt/allennlp\n-optuna), you need to register the plugin with AllenNLP by running the next code:\necho 'allennlp_optuna' >> .allennlp_plugins\nWe are going to use the LSTM-based classifier for the Stanford Sentiment Treebank\ndataset we built in chapter 2. You can find the AllenNLP config file in the book repos-\nitory (http://www.realworldnlpbook.com/ch10.html#config). Note that you need to\nreference the variables (std.extVar) for Optuna to have control over the parameters.\nSpecifically, you need to define them at the beginning of the config file:\nlocal embedding_dim = std.parseJson(std.extVar('embedding_dim'));\nlocal hidden_dim = std.parseJson(std.extVar('hidden_dim'));\nlocal lr = std.parseJson(std.extVar('lr'));\n",
      "content_length": 3113,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "277\nHyperparameter tuning\nThen, you need to tell Optuna which parameters to optimize. You can do this by writ-\ning a JSON file (hparams.json (http://www.realworldnlpbook.com/ch10.html#\nhparams)). You need to specify every hyperparameter you want Optuna to optimize\nwith its types and ranges as follows:\n[\n    {\n        \"type\": \"int\",\n        \"attributes\": {\n            \"name\": \"embedding_dim\",\n            \"low\": 64,\n            \"high\": 256\n        }\n    },\n    {\n        \"type\": \"int\",\n        \"attributes\": {\n            \"name\": \"hidden_dim\",\n            \"low\": 64,\n            \"high\": 256\n        }\n    },\n    {\n        \"type\": \"float\",\n        \"attributes\": {\n            \"name\": \"lr\",\n            \"low\": 1e-4,\n            \"high\": 1e-1,\n            \"log\": true\n        }\n    }\n]\nNext, invoke this command to start the optimization:\nallennlp tune \\\n    examples/tuning/sst_classifier.jsonnet \\\n    examples/tuning/hparams.json \\\n    --include-package examples \\\n    --serialization-dir result \\\n    --study-name sst-lstm \\\n    --n-trials 20 \\\n    --metrics best_validation_accuracy \\\n    --direction maximize\nNote that we are running 20 trials (--n-trials) with validation accuracy (--metrics\nbest_validation_accuracy) as the metric to maximize (--direction maxi-\nmize). If you do not specify the metric and the direction, by default Optuna tries to\nminimize the validation loss.\n This will take a while, but after all the trials are finished you will see the following\none-line summary of the optimization:\nTrial 19 finished with value: 0.3469573115349682 and parameters: \n{'embedding_dim': 120, 'hidden_dim': 82, 'lr': 0.00011044322486693224}. \nBest is trial 14 with value: 0.3869209809264305.\n",
      "content_length": 1700,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "278\nCHAPTER 10\nBest practices in developing NLP applications\nFinally, Optuna supports a wide range of visualization of the optimization result,\nincluding very nice contour plots (http://www.realworldnlpbook.com/ch10.html#\ncontour), but here we'll simply use its web-based dashboard to quickly inspect the\noptimization process. All you need to do is invoke its dashboard from the command\nline as follows:\noptuna dashboard --study-name sst-lstm --storage sqlite:///allennlp_optuna.db\nNow you can access http:/./localhost:5006/dashboard to see the dashboard,\nshown in figure 10.16.\nFigure 10.16\nThe Optuna dashboard shows the evaluation metrics of the parameters for each trial.\nFrom this dashboard you can quickly see not only that your optimal trial was trial #14\nbut also the optimal hyperparameters at each trial.\nSummary\nInstances are sorted, padded, and batched together for more efficient\ncomputation.\nSubword tokenization algorithms such as BPE split words into units smaller than\nwords to mitigate the out-of-vocabulary problem in neural network models.\n",
      "content_length": 1062,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "279\nSummary\nRegularization (such as L2 and dropout) is a technique to encourage model\nsimplicity and generalizability in machine learning.\nYou can use data upsampling, downsampling, or loss weights for addressing the\ndata imbalance issue.\nHyperparameters are parameters about the model or the training algorithm.\nThey can be optimized using manual, grid, or random search. Even better, use\nhyperparameter optimization libraries such as Optuna, which integrates easily\nwith AllenNLP.\n",
      "content_length": 486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "280\nDeploying and serving\nNLP applications\nWhere chapters 1 through 10 of this book are about building NLP models, this chap-\nter covers everything that happens outside NLP models. Why is this important? Isn’t\nNLP all about building high-quality ML models? It may come as a surprise if you\ndon’t have much experience with production NLP systems, but a large portion of an\nNLP system has very little to do with NLP at all. As shown in figure 11.1, only a tiny\nfraction of a typical real-world ML system is the ML code, but the “ML code” part is\nsupported by numerous components that provide various functionalities, including\ndata collection, feature extraction, and serving. Let’s use a nuclear power plant as an\nanalogy. In operating a nuclear power plant, only a tiny fraction concerns nuclear\nThis chapter covers\nChoosing the right architecture for your NLP \napplication\nVersion-controlling your code, data, and model\nDeploying and serving your NLP model\nInterpreting and analyzing model predictions \nwith LIT (Language Interpretability Tool)\n",
      "content_length": 1050,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "281\nArchitecting your NLP application\nreaction. Everything else is a vast and complex infrastructure that supports safe and effi-\ncient generation and transportation of materials and electricity—how to use the gen-\nerated heat to turn the turbine to make electricity, how to cool and circulate water\nsafely, how to transmit the electricity efficiently, and so on. All those supporting infra-\nstructures have little to do with nuclear physics.\n Partly due to the “AI hype” in popular media, I personally think people pay too\nmuch attention to the ML modeling part and too little attention to how to serve the\nmodel in a useful way. After all, the goal of your product is to deliver values to the\nusers, not provide them with the raw predictions of the model. Even if your model is\n99% accurate, it’s not useful if you cannot make the most of the prediction so that\nusers can benefit from them. Using the previous analogy, users want to power their\nappliances and light their houses with electricity and do not care much how exactly\nthe electricity is generated in the first place.\n In the rest of this chapter, we’ll discuss how to architect your NLP applications—we\nfocus on some of the best practices when it comes to designing and developing NLP\napplications in a reliable and effective manner. Then we talk about deploying your\nNLP models—this is how we bring the NLP models to production and serve their\npredictions.\n11.1\nArchitecting your NLP application\nMachine learning engineering is still software engineering. All the best practices (decou-\npled software architectures, well-designed abstractions, clean and readable code, ver-\nsion control, continuous integration, etc.) apply to ML engineering as well. In this\nsection, we’ll discuss some best practices specific to designing and building NLP/ML\napplications. \nML model\nFeature pipeline\n…\nServing infrastructure\nTraining infrastructure\nTesting\nMonitoring\nDeployment infrastructure\nData \npipeline\nVisualization \n& \ninterpretation\nFigure 11.1\nA typical ML system consists of many different components, and the ML code is \nonly a tiny fraction of it. We cover the highlighted components in this chapter.\n",
      "content_length": 2163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "282\nCHAPTER 11\nDeploying and serving NLP applications\n11.1.1 Before machine learning\nI understand this is a book about NLP and ML, but you should seriously think about\nwhether you need ML at all for your product before you start working on your NLP\napplication. Building an ML system is no easy feat—it requires a lot of money and\ntime to collect data, train models, and serve predictions. If you can solve your prob-\nlem by writing some rules, by all means do so. As a rule of thumb, if a deep learning\nmodel can achieve an accuracy of 80%, a simpler, rule-based model can take you at\nleast halfway there.\n Also, you should consider using existing solutions, if any. Many open source NLP\nlibraries (including AllenNLP and Transformers, the two libraries that we’ve been\nusing extensively throughout the book) exist that come with a wide range of pretrained\nmodels. Cloud service providers (such as AWS AI services (https://aws.amazon.com/\nmachine-learning/ai-services/), Google Cloud AutoML (https://cloud.google.com/\nautoml), and Microsoft Azure Cognitive Services (https://azure.microsoft.com/en-us/\nservices/cognitive-services/)) offer a wide range of ML-related APIs for many domains,\nincluding NLP. If your task is something that can be solved using their offerings with no\nor little modification, that’d usually be a cost-efficient way to build your NLP applica-\ntion. After all, the most expensive component of any NLP application is usually highly\nskilled talent (i.e., your salary), and you should think twice before you go all-in and\nbuild in-house NLP solutions.\n In addition, you shouldn’t rule out “traditional” machine learning approaches.\nWe’ve paid little attention to traditional ML models in this book, but you can find rich\nliterature of statistical NLP models that were mainstream before the advent of deep\nNLP methods. Quickly building a prototype with statistical features (such as n-grams)\nand ML models (such as SVM) is often a great start. Non-deep algorithms such as\ngradient-boosted decision trees (GBDTs) often work almost as well as, if not better than,\ndeep learning methods at a fraction of the cost.\n Finally, I always recommend that practitioners start by developing the validation\nset and choosing the right evaluation metric first, even before starting to choose the\nright ML approach. A validation set doesn’t need to be big, and most people can\nafford to sit down for a couple of hours and manually annotate a couple of hundred\ninstances. Doing this offers many benefits—first, by solving the task manually, you get\na feel of what’s important when it comes to solving the problem and whether it’s\nsomething that a machine can really solve automatically. Second, by putting yourself\nin the machine’s shoes, you gain a lot of insights into the task (what the data looks\nlike, how the input and output data are distributed, and how they are related), which\nbecome valuable when it comes to actually designing an ML system to solve it. \n11.1.2 Choosing the right architecture\nExcept for rare occasions where the output of an ML system is the end product itself\n(such as machine translation), NLP modules usually interact with a larger system that\ncollectively provide some values to the end users. For example, a spam filter is usually\nimplemented as a module or a microservice embedded in a larger application (email\n",
      "content_length": 3352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "283\nArchitecting your NLP application\nservice). Voice assistant systems are usually large, complex combinations of many ML/\nNLP subcomponents, including voice recognition, sentence-intent classification, ques-\ntion answering, and speech generation, that interact with each other. Even machine\ntranslation models can be one tiny component in a larger complex system if you\ninclude data pipelines, the backend, and the translation interface that the end users\ninteract with.\n An NLP application can take many forms. Surprisingly, many NLP components\ncan be structured as a one-off task that takes some static data as its input and produces\ntransformed data as its output. For example, if you have a static database of some doc-\numents and you’d like to classify them by their topics, your NLP classifier can be a sim-\nple one-off Python script that runs this classification task. If you’d like to extract\ncommon entities (e.g., company names) from the same database, you can write a\nPython script that runs a named entity recognition (NER) model to do it. Even a text-\nbased recommender engine that finds objects based on textual similarity can be a\ndaily task that reads from and writes data to the database. You don’t need to architect\na complex software system with many services talking to each other.\n Many other NLP components can be structured as a (micro)service that runs pre-\ndiction in batches, which is the architecture that I recommend for many scenarios.\nFor example, a spam filter doesn’t need to classify every single email as soon as they\narrive—the system can queue a certain number of emails that arrive at the system and\npass the batched emails to the classifier service. The NLP application usually commu-\nnicates with the rest of the system via some intermediary (e.g., a RESTful API or a\nqueuing system). This configuration is great for applications that require some fresh-\nness for their prediction (after all, users do not want to wait for hours until their\nemails arrive to their inbox), but the requirement is not that strict.\n Finally, NLP components can also be designed so that they serve real-time predic-\ntion. This is necessary when, for example, an audience needs real-time subtitles for a\nspeech. Another example is when the system wants to show ads based on the user’s\nreal-time behavior. For these cases, the NLP service needs to receive a stream of input\ndata (such as audio or user events) and produce another stream of data (such as tran-\nscribed text or ad-click probabilities). Real-time streaming frameworks such as Apache\nFlink (https://flink.apache.org/) are often used for processing such stream data.\nAlso, if your application is based on a server-client architecture, as with typical mobile\nand web apps, and you want to show some real-time prediction to the users, you can\nchoose to run ML/NLP models on the client side, such as the web browser or the\nsmartphones. Client-side ML frameworks such as TensorFlow.js (https://www.tensor-\nflow.org/js), Core ML (https://developer.apple.com/documentation/coreml), and\nML Kit (https://developers.google.com/ml-kit) can be used for such purposes.\n11.1.3 Project structure\nMany NLP applications follow somewhat similar project structures. A typical NLP proj-\nect may need to manage datasets to train a model from, intermediate files generated\nby preprocessing data, model files produced as a result of training, source code for\n",
      "content_length": 3417,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "284\nCHAPTER 11\nDeploying and serving NLP applications\ntraining and inference, and log files that store additional information about the train-\ning and inference.\n Because typical NLP applications have many components and directories in com-\nmon, it’d be useful if you simply follow best practices as your default choice when\nstarting a new project. Here are my recommendations for structuring your NLP\nprojects:\nData management—Make a directory called data and put all the data in it. It\nmay also be helpful to subdivide this into raw, interim, and result directories.\nThe raw directory contains the unprocessed dataset files you obtained exter-\nnally (such as the Stanford Sentiment Treebank we’ve been using throughout\nthis book) or built internally. It is very critical that you do not modify any files in\nthis raw directory by hand. If you need to make changes, write a script that runs\nsome processing against the raw files and then writes the result to the interim\ndirectory, which serves as a place for intermediate results. Or make a patch file\nthat manages the “diff” you made to the raw file, and version-control the patch\nfiles instead. The final results such as predictions and metrics should be stored\nin the result directory.\nVirtual environment—It is strongly recommended that you work in a virtual envi-\nronment so that your dependencies are separated and reproducible. You can\nuse tools like Conda (https://docs.conda.io/en/latest/) (my recommenda-\ntion) and venv (https://docs.python.org/3/library/venv.html) to set up a sepa-\nrate environment for your project and use pip to install individual packages.\nConda can export the environment configuration into an environment.yml\nfile, which you can use to recover the exact Conda environment. You can also\nkeep track of pip packages for the project in a requirements.txt file. Even\nbetter, you can use Docker containers to manage and package the entire ML\nenvironment. This greatly reduces dependency-related issues and simplifies\ndeployment and serving.\nExperiment management—Training and inference pipelines for an NLP applica-\ntion usually consist of several steps, such as preprocessing and joining the data,\nconverting them into features, training and running the model, and converting\nthe results back to a human-readable format. These steps can easily get out of\nhand if you try to remember to manage them manually. A good practice is to\nkeep track of the steps for the pipeline in a shell script file so that the experi-\nments are reproducible with a single command, or use dependency manage-\nment software such as GNU Make, Luigi (https://github.com/spotify/luigi),\nand Apache Airflow (https://airflow.apache.org/).\nSource code—Python source code is usually put in a directory of the same name\nas the project, which is further subdivided into directories such as data (for\ndata-processing code), model (for model code), and scripts (for putting\nscripts for training and other one-off tasks).\n",
      "content_length": 2970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "285\nArchitecting your NLP application\n11.1.4 Version control\nYou probably don’t need to be convinced that version-controlling your source code is\nimportant. Tools like Git help you keep track of the changes and manage different\nversions of the source code. Development of NLP/ML applications is usually an itera-\ntive process where you (often with other people) make many changes to the source\ncode and experiment with many different models. You can easily end up with a num-\nber of slightly different versions of the same code. \n In addition to version-controlling your source code, it is also important to version-\ncontrol your data and models. This means that you should version-control your training\ndata, source code, and models separately, as shown in the dotted-line boxes in figure\n11.2. This is one of the major differences between regular software projects and ML\napplications. Machine learning is about improving computer algorithms through\ndata. By definition, the behavior of any ML system depends on data it is fed. This\ncould lead to a situation where the behavior of the system is different even if you use\nthe same code.\n Tools like Git Large File Storage (https://git-lfs.github.com/) and DVC (https://\ndvc.org) can version-control your data and models in a seamless way. Even if you are\nnot using these tools, you should at least manage different versions as separate files\nthat are named clearly.\nFigure 11.2\nMachine learning components to version-control: training data, source code, and models\nTraining infrastructure\nFeature pipeline\n…\nDataset\nreader\nTrainer\nOptimizer\nModel\nBatching\nFeature pipeline\n…\nServing infrastructure\nDataset\nreader\nPredictor \nModel\nNew instance\nTraining data\nPrediction\n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "286\nCHAPTER 11\nDeploying and serving NLP applications\nIn a larger and more complex ML project, you may want to version-control your\nmodel and your feature pipeline separately, because the behavior of an ML model can\nbe different depending on how you preprocess the input, even with the same model\nand input data. This will also mitigate the train-serve skew problem we’ll discuss later\nin section 11.3.2.\n Finally, when working on ML applications, you will experiment with a lot of different\nsettings—different combinations of training datasets, feature pipelines, models, and\nhyperparameters—which can easily get out of control. I recommend keeping track of\nthe training settings using some experiment management system, such as Weights &\nBiases (https://wandb.ai/), but you can also use something as simple as a spreadsheet\nin which you enter experiment information manually. When keeping track of experi-\nments, be sure to record the following information for each experiment:\nVersions of the model code, feature pipeline, and the training data used\nHyperparameters used to train the model\nEvaluation metrics for the training and the validation data\nPlatforms like AllenNLP support experiment configuration by default, which makes\nthe first two items easy. Tools like TensorBoard, which is supported by AllenNLP and\nHugging Face out of the box, make it trivial to keep track of various metrics.\n11.2\nDeploying your NLP model\nIn this section, we’ll move on to the deployment stage, where your NLP application is\nput on a server and becomes available for use. We’ll discuss practical considerations\nwhen deploying NLP/ML applications.\n11.2.1 Testing\nAs with software engineering, testing is an important part of building reliable NLP/\nML applications. The most fundamental and important tests are unit tests, which\nautomatically check whether small units of software (such as methods and classes) are\nworking as expected. In NLP/ML applications, it is important to unit-test your feature\npipeline. For example, if you write a method that converts raw text into a tensor repre-\nsentation, make sure that it works for typical and corner cases with unit tests. In my\nexperience, this is where bugs often sneak in. Reading a dataset, building a vocabulary\nfrom a corpus, tokenizing, converting tokens into integer IDs—these are all essential\nyet error-prone steps in preprocessing. Fortunately, frameworks such as AllenNLP\noffer standardized, well-tested components for these steps, which makes building NLP\napplications easier and bug-free.\n In addition to unit tests, you need to make sure that your model learns what it’s told\nto learn. This corresponds to testing logic errors in regular software engineering—\ntypes of errors where the software runs without crashing yet produces incorrect results.\nThis type of error is more difficult to catch and fix in NLP/ML, because you need more\ninsight into how the learning algorithm works mathematically. Moreover, many ML\n",
      "content_length": 2970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "287\nDeploying your NLP model\nalgorithms involve some randomness, such as random initialization and sampling,\nwhich makes testing even more difficult.\n One recommended technique for testing NLP/ML models is sanity checks against\nthe model output. You can start with a small and simple model and just a few toy\ninstances with obvious labels. If you are testing a sentiment analysis model, for exam-\nple, this goes as follows:\nCreate a small and simple model for debugging, such as a toy encoder that sim-\nply averages the input word embeddings with a softmax layer on top.\nPrepare a few toy instances, such as “The best movie ever!” (positive) and “This\nis an awful movie!” (negative).\nFeed these instances to the model, and train it until convergence. Because we\nare using a very small dataset without a validation set, the model will heavily\noverfit to the instances, and that’s totally fine. Check whether the training loss\ngoes down as expected.\nFeed the same instances to the trained model, and check whether the predicted\nlabels match the expected ones.\nTry the steps above with more toy instances and a larger model.\nAs a related technique, I always recommend you start with a smaller dataset, especially\nif the original dataset is large. Because training NLP/ML models takes a long time\n(hours or even days), you often find that your code has some errors only after your\ntraining is finished. You can subsample your training data, for example, by simply tak-\ning one out of every 10 instances, so that your entire training finishes quickly. Once\nyou are sure that your model works as expected, you can gradually ramp up the\namount of data you use for training. This technique is also great for quickly iterating\nand experimenting with many different architectures and hyperparameter settings.\nWhen you have just started building your model, you don’t usually have clear under-\nstanding of the best models for your task. With a smaller dataset, you can quickly vali-\ndate a large number of different options (RNN versus Transformers, different\ntokenizers, etc.) and narrow down the set of candidate models that work best. One\ncaveat to this approach is that the best model architectures and hyperparameters may\ndepend on the size of the training data. Because of this, don’t forget to run the valida-\ntion against the full dataset, too.\n Finally, you can use integration tests to verify whether the individual components\nof your application work in combination. For NLP, this usually means running the\nwhole pipeline to see if the prediction is correct. Similar to the unit tests, you can pre-\npare a small number of instances where the expected prediction is clear and run them\nagainst the trained model. Note that these instances are not for measuring how good\nthe model is, but rather to serve as a sanity check whether your model can produce\ncorrect predictions for “obvious” cases. It is a good practice to run integration tests\nevery time a new model or code is deployed. This is usually part of continuous integra-\ntion (CI) used for regular software engineering.\n",
      "content_length": 3081,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "288\nCHAPTER 11\nDeploying and serving NLP applications\n11.2.2 Train-serve skew\nOne common source of errors in ML applications is called train-serve skew, a situation\nwhere there’s a discrepancy between how instances are processed at the training and\nthe inference times. This could occur in various situations, but let’s discuss a concrete\nexample. Say you are building a sentiment-analyzer system with AllenNLP and would\nlike to convert texts into instances. You usually start writing a data loader first, which\nreads the dataset and produces instances. Then you write a Python script or a config\nfile that tells AllenNLP how the model should be trained. You train and validate your\nmodel. So far, so good. However, when it comes to using the model for prediction,\nthings look slightly different. You need to write a predictor, which, given an input text,\nconverts it into an instance and passes it to the model’s forward method. Notice that\nnow you have two independent pipelines that preprocess the input—one for the train-\ning in the dataset reader, and another for the inference in the predictor.\n What happens if you want to modify the way the input text is processed? For exam-\nple, let’s say you find something you want to improve in your tokenization process,\nand you make changes to how input text is tokenized in your data loader. You update\nyour data loader code, retrain the model, and deploy the model. However, you forgot\nto update the corresponding tokenization code in your predictor, effectively creating\na discrepancy in how input is tokenized between training and serving. This is illus-\ntrated in figure 11.3.\nFigure 11.3\nTrain-serve skew is caused by discrepancies in how input is processed between training and serving.\nTraining infrastructure\nFeature pipeline\n…\nDataset\nreader\nTrainer\nOptimizer \nModel\nBatching\nFeature pipeline\n…\nServing infrastructure\nTrain-serve skew\nPredictor\nPredictor \nModel\nNew instance\nTraining data\nPrediction\n",
      "content_length": 1958,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "289\nDeploying your NLP model\nThe best way to fix this—or even better, to prevent this from happening in the first\nplace—is to share as much of the feature pipeline as possible between the training and\nthe serving infrastructure. A common practice in AllenNLP is to implement a method\ncalled _text_to_instance() in the dataset reader, which takes an input and returns\nan instance. By making sure that both the dataset reader and the predictor refer to the\nsame method, you can minimize the discrepancy between the pipelines.\n In NLP, the fact that input text is tokenized and converted to numerical values\nmakes debugging your model even more difficult. For example, an obvious bug in\ntokenization that you can spot easily with your naked eyes can be quite difficult to\nidentify if everything is numerical values. A good practice is to log some intermediate\nresults into a log file that you can inspect later. \n Finally, note that some behaviors of neural networks do differ between training\nand serving. One notable example is dropout, a regularization method we briefly\ncovered in section 10.3.1. To recap, dropout regularizes the model by randomly mask-\ning activation values in a neural network. This makes sense in training, because by\nremoving activations, the model learns to make robust predictions based on available\nvalues. However, remember to turn it off at the serving time, because you don’t want\nyour model to randomly drop neurons. PyTorch models implement methods—\ntrain() and eval()—that switch between the training and prediction modes,\naffecting how layers like dropout behave. If you are implementing a training loop\nmanually, remember to call model.eval()to disable dropout. The good news is that\nframeworks such as AllenNLP can handle this automatically as long as you are using\ntheir default trainer.\n11.2.3 Monitoring\nAs with other software services, deployed ML systems should be monitored continu-\nously. In addition to the usual server metrics (e.g., CPU and memory usage), you\nshould also monitor metrics related to the input and the output of the model. Specifi-\ncally, you can monitor some higher-level statistics such as the distribution of input val-\nues and output labels. As mentioned earlier, logic errors, which are a type of error that\ncauses the model to produce wrong results without crashing it, are the most common\nand hardest to find in ML systems. Monitoring those high-level statistics makes it eas-\nier to find them. Libraries and platforms like PyTorch Serve and Amazon SageMaker\n(discussed in section 11.3) support monitoring by default.\n11.2.4 Using GPUs\nTraining large modern ML models almost always requires hardware accelerators such\nas GPUs. Recall that back in chapter 2, we used overseas factories as an analogy for\nGPUs, which are designed to execute a huge number of arithmetic operations such as\nvector and matrix addition and multiplications in parallel. In this subsection, we’ll\ncover how to use GPUs to accelerate the training and prediction of ML models.\n",
      "content_length": 3017,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "290\nCHAPTER 11\nDeploying and serving NLP applications\n If you don’t own GPUs or have never used cloud-based GPU solutions before, the\neasiest way to “try” GPUs for free is to use Google Colab. Go to its URL (https://\ncolab.research.google.com/), create a new notebook, go to the Runtime menu, and\nchoose “Change runtime type.” This will bring up the dialog box shown in figure 11.4.\nChoose GPU as the type of the hardware accelerator, and type !nvidia-smi in a\ncode block and execute it. Some detailed information about your GPU is displayed, as\nshown next:\n \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nThe nvidia-smi command (short for Nvidia System Management Interface) is a\nhandy tool for checking information about Nvidia GPUs on the machine. From the\nprevious snippet, you can see the version of the driver and CUDA (an API and a\nFigure 11.4\nGoogle Colab \nallows you to choose the type \nof hardware accelerator.\n",
      "content_length": 2395,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "291\nDeploying your NLP model\nlibrary for interacting with GPUs), type of GPUs (Tesla T4), available and used mem-\nory (15109 MiB and 3 MiB), and the list of processes that currently use GPUs (there\naren’t any). The most typical use of this command is to check how much memory your\ncurrent process(es) use, because in GPU programming, you can easily get an out-of-\nmemory error if your program uses more memory than is available.\n If you use cloud infrastructures such as AWS (Amazon Web Services) and GCP\n(Google Cloud Platform), you’ll find a wide array of virtual machine templates that\nyou can use to quickly create cloud instances that support GPUs. For example, GCP\nhas Nvidia’s official GPU-optimized images for PyTorch and TensorFlow, which you\ncan use as templates to launch your GPU instances. AWS offers Deep Learning AMIs\n(Amazon Machine Images), which preinstall basic GPU libraries such as CUDA, as\nwell as deep learning libraries such as PyTorch. With these templates, you don’t need\nto install necessary drivers and libraries manually—you can start building your ML\napplications right away. Note that although these templates are free, you do need to\npay for the infrastructure. The price for GPU-enabled virtual machines is usually sig-\nnificantly higher than CPU machines. Make sure to check their price before you keep\nthem running for an extended period of time.\n If you are setting up GPU instances from scratch, you can find detailed instruc-\ntions1 for how to set up necessary drivers and libraries. To build NLP applications\nwith the libraries that we covered in this book (namely, AllenNLP and Transformers),\nyou need to install CUDA drivers and toolkits, as well as a PyTorch version that sup-\nports GPU.\n If your machine has GPU(s), you can enable GPU acceleration by specifying\ncuda_device in an AllenNLP config file as follows:\n    \"trainer\": {\n        \"optimizer\": {\n            \"type\": \"huggingface_adamw\",\n            \"lr\": 1.0e-5\n        },\n        \"num_epochs\": 20,\n        \"patience\": 10,\n        \"cuda_device\": 0\n}\nThis tells the trainer to use the first GPU for training and validating the AllenNLP\nmodel. \n If you are writing PyTorch code from scratch, you need to manually transfer your\nmodel and tensors to the GPU. Using an analogy, this is when your materials get\nshipped to an overseas factory in container ships. First, you can specify the device\n(GPU ID) to use, and invoke the to() method of tensors and models to move them\n1 GCP: https://cloud.google.com/compute/docs/gpus/install-drivers-gpu; AWS: https://docs.aws.amazon\n.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html.\n",
      "content_length": 2626,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "292\nCHAPTER 11\nDeploying and serving NLP applications\nbetween devices. For example, you can use the following code snippet to run text gen-\neration on a GPU with Hugging Face Transformers:\ndevice = torch.device('cuda:0')\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\nmodel = AutoModelWithLMHead.from_pretrained(\"gpt2-large\")\ngenerated = tokenizer.encode(\"On our way to the beach \")\ncontext = torch.tensor([generated])\nmodel = model.to(device)\ncontext = context.to(device)\nThe rest is identical to the code we used in section 8.4.\n11.3\nCase study: Serving and deploying NLP applications\nIn this section, we will go over a case study where we serve and deploy an NLP model\nbuilt with Hugging Face. Specifically, we’ll take a pretrained language generation\nmodel (DistilGPT2), serve it with TorchServe, and deploy it to a cloud server using\nAmazon SageMaker. \n11.3.1 Serving models with TorchServe\nAs you have seen, deploying an NLP application is more than just writing an API for\nyour ML model. You need to take care of a number of production-related consider-\nations, including how to deal with high traffic by parallelizing model inference with\nmultiple workers, how to store and manage different versions of multiple ML models,\nhow to consistently handle pre- and postprocessing of the data, and how to monitor\nthe health of the server as well as various metrics about the data. \n Because these problems are so common, ML practitioners have been working on\ngeneral-purpose platforms for serving and deploying ML models. In this section, we’ll\nuse TorchServe (https://github.com/pytorch/serve), an easy-to-use framework for\nserving PyTorch models jointly developed by Facebook and Amazon. TorchServe is\nshipped with many functionalities that can address the issues mentioned earlier.\n TorchServe can be installed by running the following:\npip install torchserve torch-model-archiver\nIn this case study, we’ll use a pretrained language model called DistilGPT2. DistilGPT2\nis a smaller version of GPT-2 built using a technique called knowledge distillation.\nKnowledge distillation (or simply distillation) is a machine learning technique where a\nsmaller model (called a student) is trained in such a way that it mimics the predictions\nproduced by a larger model (called a teacher). It is a great way to train a smaller model\nthat produces high quality output, and it often produces a better model than training\na smaller model from scratch.\n First, let’s download the pretrained DistilGPT2 model from the Hugging Face\nrepository by running the following commands. Note that you need to install Git\n",
      "content_length": 2607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "293\nCase study: Serving and deploying NLP applications\nLarge File Storage (https://git-lfs.github.com/), a Git extension for handling large\nfiles under Git:\ngit lfs install\ngit clone https://huggingface.co/distilgpt2\nThis creates a subdirectory called distilgpt2, which contains files such as config.json\nand pytorch_model.bin.\n As the next step, you need write a handler for TorchServe, a lightweight wrapper\nclass that specifies how to initialize your model, preprocess and postprocess the input,\nand run the inference on the input. Listing 11.1 shows the handler code for serving\nthe DistilGPT2 model. In fact, nothing in the handler is specific to the particular\nmodel we use (DistilGPT2). You can use the same code for other GPT-2–like models,\nincluding the original GPT-2 models, as long as you use the Transformers library.\nfrom abc import ABC\nimport logging\nimport torch\nfrom ts.torch_handler.base_handler import BaseHandler\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nlogger = logging.getLogger(__name__)\nclass TransformersLanguageModelHandler(BaseHandler, ABC):\n    def __init__(self):\n        super(TransformersLanguageModelHandler, self).__init__()\n        self.initialized = False\n        self.length = 256\n        self.top_k = 0\n        self.top_p = .9\n        self.temperature = 1.\n        self.repetition_penalty = 1.\n    def initialize(self, ctx):           \n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        model_dir = properties.get(\"model_dir\")\n        self.device = torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\"))\n            if torch.cuda.is_available()\n            else \"cpu\"\n        )\n        self.model = GPT2LMHeadModel.from_pretrained(model_dir)\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n        self.model.to(self.device)\n        self.model.eval()\nListing 11.1\nHandler for TorchServe\nInitializes \nthe model\n",
      "content_length": 1928,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "294\nCHAPTER 11\nDeploying and serving NLP applications\n        logger.info('Transformer model from path {0} loaded \nsuccessfully'.format(model_dir))\n        self.initialized = True\n    def preprocess(self, data):             \n        text = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n        text = text.decode('utf-8')\n        logger.info(\"Received text: '%s'\", text)\n        encoded_text = self.tokenizer.encode(\n            text,\n            add_special_tokens=False,\n            return_tensors=\"pt\")\n        return encoded_text\n    def inference(self, inputs):            \n        output_sequences = self.model.generate(\n            input_ids=inputs.to(self.device),\n            max_length=self.length + len(inputs[0]),\n            temperature=self.temperature,\n            top_k=self.top_k,\n            top_p=self.top_p,\n            repetition_penalty=self.repetition_penalty,\n            do_sample=True,\n            num_return_sequences=1,\n        )\n        text = self.tokenizer.decode(\n            output_sequences[0],\n            clean_up_tokenization_spaces=True)\n        return [text]\n    def postprocess(self, inference_output):   \n        return inference_output\n_service = TransformersLanguageModelHandler()\ndef handle(data, context):                    \n    try:\n        if not _service.initialized:\n            _service.initialize(context)\n        if data is None:\n            return None\n        data = _service.preprocess(data)\n        data = _service.inference(data)\n        data = _service.postprocess(data)\nPreprocesses and tokenizes\nthe incoming data\nRuns inference \non the data\nPostprocesses \nthe prediction\nThe handler method \ncalled by TorchServe\n",
      "content_length": 1712,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "295\nCase study: Serving and deploying NLP applications\n        return data\n    except Exception as e:\n        raise e\nYour handler needs to inherit from BaseHandler and override a few methods includ-\ning initialize() and inference(). Your handler script also includes handle(),\na top-level method where the handler is initialized and called.\n The next step is to run torch-model-archiver, which is a command-line tool\nthat packages your model and your handler, as follows:\ntorch-model-archiver \\\n    --model-name distilgpt2 \\\n    --version 1.0 \\\n    --serialized-file distilgpt2/pytorch_model.bin \\\n    --extra-files \"distilgpt2/config.json,distilgpt2/vocab.json,distilgpt2/\ntokenizer.json,distilgpt2/merges.txt\" \\\n    --handler ./torchserve_handler.py\nThe first two options specify the name and the version of the model. The next option,\nserialized-file, specifies the main weight file of the PyTorch model you want to\npackage (which usually ends with .bin or .pt). You can also add any extra files (speci-\nfied by extra-files) that are needed for the model to run. Finally, you need to pass\nthe handler file you just wrote to the handler option.\n When finished, this creates a file named distilgpt2.mar (.mar stands for\n“model archive”) in the same directory. Let’s create a new directory named model_\nstore and move the .mar file there as follows. This directory serves as a model store,\na place where all the model files are stored and served from:\nmkdir model_store\nmv distilgpt2.mar model_store\nNow you are ready to spin up TorchServe and start serving your model! All you need is\nto run the following command:\ntorchserve --start --model-store model_store --models distilgpt2=distilgpt2.mar\nWhen the server is fully up, you can start making the HTTP requests to the server. It\nexposes a couple of endpoints, but if you just want to run inference, you need to\ninvoke http://127.0.0.1:8080/predictions/ with the model name as follows:\ncurl -d \"data=In a shocking finding, scientist discovered a herd of unicorns \nliving in a remote, previously unexplored valley, in the Andes \nMountains. Even more surprising to the researchers was the fact that the \nunicorns spoke perfect English.\" -X POST http://127.0.0.1:8080/\npredictions/distilgpt2\nHere, we are using a prompt from OpenAI’s original post about GPT-2 (https://\nopenai.com/blog/better-language-models/). This returns the generated sentences,\nshown next. The generated text is of decent quality, considering that the model is a\ndistilled, smaller version:\n",
      "content_length": 2513,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "296\nCHAPTER 11\nDeploying and serving NLP applications\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote,\npreviously unexplored valley, in the Andes Mountains. Even more surprising to the\nresearchers was the fact that the unicorns spoke perfect English. They used to speak\nthe Catalan language while working there, and so the unicorns were not just part of\nthe local herd, they were also part of a population that wasn't much less diverse than\ntheir former national-ethnic neighbors, who agreed with them.\n“In a sense they learned even better than they otherwise might have been,” says\nAndrea Rodriguez, associate professor of language at the University of California,\nIrvine. “They told me that everyone else was even worse off than they thought.”\nThe findings, like most of the research, will only support the new species that their\nnative language came from. But it underscores the incredible social connections\nbetween unicorns and foreigners, especially as they were presented with a hard new\nplatform for studying and creating their own language.\n“Finding these people means finding out the nuances of each other, and dealing with\ntheir disabilities better,” Rodriguez says.\n …\nWhen you are finished, you can run the following command to stop serving:\ntorchserve --stop\n11.3.2 Deploying models with SageMaker\nAmazon SageMaker is a managed platform for training and deploying machine learn-\ning models. It enables you to spin up a GPU server, run a Jupyter Notebook inside it,\nbuild and train ML models there, and directly deploy them in a hosted environment.\nOur next step is to deploy the machine learning model as a cloud SageMaker end-\npoint so that production systems can make requests to it. The concrete steps for\ndeploying an ML model with SageMaker consist of the following:\n1\nUpload your model to S3.\n2\nRegister and upload your inference code to Amazon Elastic Container Registry\n(ECR).\n3\nCreate a SageMaker model and an endpoint.\n4\nMake requests to the endpoint.\nWe are going to follow the official tutorial (http://mng.bz/p9qK) with a slight modifi-\ncation. First, let’s go to the SageMaker console (https://console.aws.amazon.com/\nsagemaker/home) and start a notebook instance. When you open the notebook, run\nthe following code to install the necessary packages and start a SageMaker session:\n!git clone https://github.com/shashankprasanna/torchserve-examples.git\n!cd torchserve-examples\n!git clone https://github.com/pytorch/serve.git\n!pip install serve/model-archiver/\n",
      "content_length": 2521,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "297\nCase study: Serving and deploying NLP applications\nimport boto3, time, json\nsess    = boto3.Session()\nsm      = sess.client('sagemaker')\nregion  = sess.region_name\naccount = boto3.client('sts').get_caller_identity().get('Account')\nimport sagemaker\nrole = sagemaker.get_execution_role()\nsagemaker_session = sagemaker.Session(boto_session=sess)\nbucket_name = sagemaker_session.default_bucket()\nThe variable bucket_name contains a string like sagemaker-xxx-yyy where xxx is\nthe region name (like us-east-1). Take note of this name—you need it to upload\nyour model to S3 in the next step.\n Next, you need to upload your model to an S3 bucket by running the following\ncommands from the machine where you just created the .mar file (not from the Sage-\nMaker notebook instance). Before uploading, you first need to compress your .mar\nfile into a tar.gz file, a format supported by SageMaker. Remember to replace sage-\nmaker-xxx-yyy with the actual bucket name specified by bucket_name:\ncd model_store\ntar cvfz distilgpt2.tar.gz distilgpt2.mar\naws s3 cp distilgpt2.tar.gz s3://sagemaker-xxx-yyy/torchserve/models/\nThe next step is to register and push the TorchServe inference code to ECR. Before\nyou start, in your SageMaker notebook instance, open torchserve-examples/\nDockerfile and modify the following line (add --no-cache-dir transformers):\nRUN pip install --no-cache-dir psutil \\\n                --no-cache-dir torch \\\n                --no-cache-dir torchvision \\\n                --no-cache-dir transformers\nNow you can build a Docker container and push it to ECR as follows:\nregistry_name = 'torchserve'\n!aws ecr create-repository --repository-name torchserve\nimage_label = 'v1'\nimage = f'{account}.dkr.ecr.{region}.amazonaws.com/\n{registry_name}:{image_label}'\n!docker build -t {registry_name}:{image_label} .\n!$(aws ecr get-login --no-include-email --region {region})\n!docker tag {registry_name}:{image_label} {image}\n!docker push {image}\nNow you are ready to create a SageMaker model and create an endpoint for it, as\nshown next:\nimport sagemaker\nfrom sagemaker.model import Model\n",
      "content_length": 2088,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "298\nCHAPTER 11\nDeploying and serving NLP applications\nfrom sagemaker.predictor import RealTimePredictor\nrole = sagemaker.get_execution_role()\nmodel_file_name = 'distilgpt2'\nmodel_data = f's3://{bucket_name}/torchserve/models/{model_file_name}.tar.gz'\nsm_model_name = 'torchserve-distilgpt2'\ntorchserve_model = Model(model_data = model_data, \n                         image_uri = image,\n                         role = role,\n                         predictor_cls=RealTimePredictor,\n                         name = sm_model_name)\nendpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", \ntime.gmtime())\npredictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n                                    initial_instance_count=1,\n                                    endpoint_name = endpoint_name)\nThe predictor object is something you can call directly to run the inference as follows:\nresponse = predictor.predict(data=\"In a shocking finding, scientist \ndiscovered a herd of unicorns living in a remote, previously unexplored \nvalley, in the Andes Mountains. Even more surprising to the researchers \nwas the fact that the unicorns spoke perfect English.\")\nThe content of the response should look something like this:\nb'In a shocking finding, scientist discovered a herd of unicorns living in a \nremote, previously unexplored valley, in the Andes Mountains. Even more \nsurprising to the researchers was the fact that the unicorns spoke \nperfect English. The unicorns said they would take a stroll in the \ndirection of scientists over the next month or so.\\n\\n\\n\\n\\nWhen \ncontacted by Animal Life and Crop.com, author Enrique Martinez explained \nhow he was discovered and how the unicorns\\' journey has surprised him. \nAccording to Martinez, the experience makes him more interested in \nresearch and game development.\\n\"This is really what I want to see this \nyear, and in terms of medical research, I want to see our population \nincrease.\"<|endoftext|>'\nCongratulations! We just completed our journey—we started building an ML model\nin chapter 2 and came all the way to deploying it to a cloud platform in this chapter.\n11.4\nInterpreting and visualizing model predictions\nPeople often talk about the metrics and leaderboard performance on standardized\ndatasets, but analyzing and visualizing model predictions and internal states is import-\nant for NLP applications in the real world. Although deep learning models can be\nreally good at what they do, often reaching human-level performance on some NLP\ntasks, those deep models are black boxes, and it is difficult to know why they make cer-\ntain predictions. \n Because of this (somewhat troubling) property of deep learning models, a growing\nfield in AI called explainable AI (XAI) is working to develop methods to explain the\n",
      "content_length": 2796,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "299\nInterpreting and visualizing model predictions\npredictions and behavior of ML models. Interpreting ML models is useful for\ndebugging—it gives you a lot of clues if you know why it made certain predictions. In\nsome domains such as medical applications and self-driving cars, making ML models\nexplainable is critical for legal and practical reasons. In this final section of the chap-\nter, we’ll go over a case study where we use the Language Interpretability Tool (LIT)\n(https://pair-code.github.io/lit/) for visualizing and interpreting the predictions and\nbehavior of NLP models.\n LIT is an open source toolkit developed by Google and offers a browser-based\ninterface for interpreting and visualizing ML predictions. Note that it is framework\nagnostic, meaning that it works with any Python-based ML frameworks of choice,\nincluding AllenNLP and Hugging Face Transformers.2 LIT offers a wide range of fea-\ntures, including the following:\nSaliency map—Visualizing in color which part of the input played an important\nrole to reach the current prediction\nAggregate statistics—Showing aggregate statistics such as dataset metrics and con-\nfusion matrices\nCounterfactuals—Observing how model predictions change for generated new\nexamples\nIn the remainder of this section, let’s take one of the AllenNLP models we trained\n(the BERT-based sentiment analysis model in chapter 9) and analyze it via LIT. LIT\noffers a set of extensible abstractions such as datasets and models to make it easier to\nwork with any Python-based ML models.\n First, let’s install LIT. It can be installed with a single call of pip as follows:\npip install lit-nlp\nNext, you need to wrap your dataset and model with the abstract classes defined by\nLIT. Let’s create a new script called run_lit.py, and import the necessary modules\nand classes, as shown here:\nimport numpy as np\nfrom allennlp.models.archival import load_archive\nfrom allennlp.predictors.predictor import Predictor\nfrom lit_nlp import dev_server\nfrom lit_nlp import server_flags\nfrom lit_nlp.api import dataset as lit_dataset\nfrom lit_nlp.api import model as lit_model\nfrom lit_nlp.api import types as lit_types\nfrom examples.sentiment.sst_classifier import LstmClassifier\nfrom examples.sentiment.sst_reader import \nStanfordSentimentTreeBankDatasetReaderWithTokenizer\n2 There is another toolkit called AllenNLP Interpret (https://allennlp.org/interpret) that offers a similar set of\nfeatures for understanding NLP models, although it is specifically designed to interact with AllenNLP models.\n",
      "content_length": 2532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "300\nCHAPTER 11\nDeploying and serving NLP applications\nThe next code shows how to define a dataset for LIT. Here, we are creating a toy data-\nset that consists of just four hardcoded examples, but in practice, you may want to\nread a real dataset that you want to explore. Remember to define the spec() method\nthat returns the type specification of the dataset:\nclass SSTData(lit_dataset.Dataset):\n    def __init__(self, labels):\n        self._labels = labels\n        self._examples = [\n            {'sentence': 'This is the best movie ever!!!', 'label': '4'},\n            {'sentence': 'A good movie.', 'label': '3'},\n            {'sentence': 'A mediocre movie.', 'label': '1'},\n            {'sentence': 'It was such an awful movie...', 'label': '0'}\n        ]\n    def spec(self):\n        return {\n            'sentence': lit_types.TextSegment(),\n            'label': lit_types.CategoryLabel(vocab=self._labels)\n        }\nNow, we are ready to define the main model, as shown next.\nclass SentimentClassifierModel(lit_model.Model):\n    def __init__(self):\n        cuda_device = 0\n        archive_file = 'model/model.tar.gz'\n        predictor_name = 'sentence_classifier_predictor'\n        archive = load_archive(                         \n            archive_file=archive_file,\n            cuda_device=cuda_device\n        )\n        predictor = Predictor.from_archive(archive, \npredictor_name=predictor_name)\n        self.predictor = predictor                         \n        label_map = \narchive.model.vocab.get_index_to_token_vocabulary('labels')\n        self.labels = [label for _, label in sorted(label_map.items())]\n    def predict_minibatch(self, inputs):\n        for inst in inputs:\n            pred = self.predictor.predict(inst['sentence'])     \n            tokens = self.predictor._tokenizer.tokenize(inst['sentence'])\n            yield {\n                'tokens': tokens,\n                'probas': np.array(pred['probs']),\n                'cls_emb': np.array(pred['cls_emb'])\n            }\nListing 11.2\nDefining the main model for LIT\nLoads the \nAllenNLP archive\nExtracts and sets \nthe predictor\nRuns the predict\nmethod of\nthe predictor\n",
      "content_length": 2143,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "301\nInterpreting and visualizing model predictions\n    def input_spec(self):\n        return {\n            \"sentence\": lit_types.TextSegment(),\n            \"label\": lit_types.CategoryLabel(vocab=self.labels, \nrequired=False)\n        }\n    def output_spec(self):\n        return {\n            \"tokens\": lit_types.Tokens(),\n            \"probas\": lit_types.MulticlassPreds(parent=\"label\", \nvocab=self.labels),\n            \"cls_emb\": lit_types.Embeddings()\n        }\nIn the constructor (__init__), we are loading an AllenNLP model from an archive\nfile and creating a predictor from it. We are assuming that your model is put under\nmodel/model.tar.gz and hard-coding its path, but feel free to modify this, depending\non where your model is located.\n The model prediction is computed in predict_minibatch(). Given the input\n(which is simply an array of dataset instances), it runs the model via the predictor and\nreturns the result. Note that the predictions are made instance-by-instance, although\nin practice, you should consider making predictions in batches because it will improve\nthroughput for larger input data. The method also returns the embeddings for pre-\ndicted classes (as cls_emb), which will be used for visualizing embeddings (figure\n11.5).\n Finally, here’s the code for running the LIT server:\nmodel = SentimentClassifierModel()\nmodels = {\"sst\": model}\ndatasets = {\"sst\": SSTData(labels=model.labels)}\nlit_demo = dev_server.Server(models, datasets, **server_flags.get_flags())\nlit_demo.serve()\nAfter running the script above, go to http:/./localhost:5432/ on your browser.\nYou should see a screen similar to the one shown in figure 11.5. You can see an array\nof panels corresponding to various information about the data and predictions,\nincluding embeddings, the dataset table and editor, classification results, and saliency\nmaps (which shows contributions of tokens computed via an automated method\ncalled LIME 3).\n Visualizing and interacting with model predictions are a great way to get insights\ninto how the model works and how you should improve it. \n \n3 Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” (2016). https://\narxiv.org/abs/1602.04938.\n",
      "content_length": 2207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "302\nCHAPTER 11\nDeploying and serving NLP applications\nFigure 11.5\nLIT can show saliency maps, aggregate statistics, and embeddings for analyzing your \nmodel and predictions.\n11.5\nWhere to go from here\nIn this book, we’ve only scratched the surface of this vast, long-historied field of natu-\nral language processing. If you are interested in learning the practical aspects of NLP\nfurther, Natural Language Processing in Action by Hobson Lane and others (Manning\nPublications, 2019) and Practical Natural Language Processing by Sowmya Vajjala and\nothers (O’Reilly, 2020) can be a good next step. Machine Learning Engineering by\nAndriy Burkov (True Positive Inc., 2020) is also a good book to learn engineering top-\nics for machine learning in general.\n If you are interested in learning more mathematical and theoretical aspects of\nNLP, I’d recommend giving some popular textbooks a try, such as Speech and Language\nProcessing by Dan Jurafsky and James H. Martin (Prentice Hall, 2008)4 and Introduction\nto Natural Language Processing by Jacob Eisenstein (MIT Press, 2019). Foundations of Sta-\ntistical Natural Language Processing by Christopher D. Manning and Hinrich Schütze\n(Cambridge, 1999), though a bit outdated, is also a classic textbook that can give you\na solid foundation for a wide variety of NLP methods and models.\n4 You can read the draft of the third edition (2021) for free at https://web.stanford.edu/~jurafsky/slp3/.\n",
      "content_length": 1434,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "303\nSummary\n Also remember that you can often find great resources online for free. A free Allen-\nNLP course, “A Guide to Natural Language Processing with AllenNLP” (https://guide\n.allennlp.org/), and the documentation for Hugging Face Transformers (https://\nhuggingface.co/transformers/index.html) are great places to go to if you want to learn\nthose libraries in depth.\n Finally, the most effective way to learn NLP is actually doing it yourself. If you have\nproblems for your hobby, work, or anything that involves dealing with natural lan-\nguage text, think whether any of the techniques you learned in this book are applica-\nble. Is it a classification, tagging, or sequence-to-sequence problem? Which models do\nyou use? How do you get the training data? How do you evaluate your model? If you\ndon’t have NLP problems laying around, don’t worry—head over to Kaggle, where\nyou can find a number of NLP-related competitions in which you can “get your hands\ndirty” and gain NLP experience while working on real-world problems. NLP confer-\nences and workshops often host shared tasks, where participants can compete on a\ncommon task, datasets, and evaluation metrics, which are also a great way to learn fur-\nther if you want to deep dive into a particular field of NLP.\nSummary\nMachine learning code is usually a small portion in real-world NLP/ML systems,\nsupported by a complex infrastructure for data collection, feature extraction,\nand model serving and monitoring.\nNLP modules can be developed as a one-off script, a batch prediction service,\nor a real-time prediction service. \nIt is important to version-control your model and data, in addition to the source\ncode. Beware of train-serve skew that causes discrepancies between the training\nand the testing times.\nYou can easily serve PyTorch models with TorchServe and deploy them to Ama-\nzon SageMaker.\nExplainable AI is a new field for explaining and interpreting ML models and\ntheir predictions. You can use LIT (Language Interpretability Tool) to visualize\nand interpret model predictions.\n",
      "content_length": 2057,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "304\nCHAPTER 11\nDeploying and serving NLP applications\n",
      "content_length": 54,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "305\nindex\nNumerics\n2-D convolution 176\nA\nabstraction 82–84\naccuracy 93–94\nactivation functions 84\nAdam optimizer 102\nadaptation 221\nadapting BERT 226–229\nadd_special_tokens 245\nadequacy 13, 163\nAI (artificial intelligence) 8–10\nALBERT 241–243\nalignment 144\nAllenNLP\nimplementing Skip-gram on 62–66\nloading SST datasets 33\nSeq2Seq encoder in 117–118\ntraining pipelines 96–102\nbuilding model 100–101\nconfiguring 102–105\ninstances and fields 97–98\nputting all together 101–102\ntoken embedders and RNNs 99–100\nvocabulary and token indexers 98–99\nTransformers with 246–251\nallennlp command 104\nallennlp-models package 33\nallennlp-modules package 230\nallennlp-server plugin 47\nAllentune library 276\nambiguity 5, 127\nAMIs (Amazon Machine Images) 291\nanalysis 22\napplications 13–15, 21–48, 255–303\narchitecting 281–286\nbefore machine learning 282\nchoosing right architecture 282–283\nproject structure 283–284\nversion control 285–286\navoiding overfitting 265–270\ncross-validation 269–270\nearly stopping 268–269\nregularization 265–267\nbatching instances 256–261\nmasking 259–261\npadding 256–257\nsorting 257–259\ncase study 292–298\ndeploying models with SageMaker 296–298\nserving models with TorchServe 292–296\ndealing with imbalanced dataset 270–273\nupsampling and downsampling 271–272\nuse appropriate evaluation metrics 270–271\nweighting losses 272–273\ndeploying 46–48, 286–292\nmaking predictions 46\nmonitoring 289\nserving predictions 46–48\ntesting 286–287\ntrain-serve skew 288–289\nusing GPUs (graphics processing units)\n289–292\ndevelopment of 21–24\nanalysis and experimenting 22\n",
      "content_length": 1569,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "INDEX\n306\napplications (continued)\ndata collection 22\ndeploying 23\nimplementation 23\nmonitoring 23–24\ntraining 23\ndialog systems 15\nevaluating classifier 45–46\ngrammatical and spelling error correction\n13–14\nhyperparameters 273–278\nexamples of 274\ngrid search vs. random search 275–276\ntuning with Optuna 276–278\ninterpreting and visualizing model \npredictions 298–301\nloss functions and optimization 41–43\nmachine translation (MT) 13\nneural networks 37–41\narchitecture for sentiment analysis 39–41\ndefined 37–38\nRNNs and linear layers 38–39\nsearch engine 14\nsentiment analysis 27\nstructure of 24–25\ntokenization for neural models 261–265\ncharacter models 262–263\nsubword models 263–265\nunknown words 261–262\ntraining classifier 43–44\nbatching 43–44\nputting everything together 44\nword embeddings 34–37\ndefined 34–36\nfor sentiment analysis 36–37\nworking with NLP datasets 28–33\ndefined 28–29\nloading Stanford Sentiment Treebank (SST) \ndatasets using AllenNLP 33\nStanford Sentiment Treebank (SST) 29–30\ntrain, validation, and test sets 30–32\n- -arch lstm 151, 189\n- -arch transformer option 197\narchitecting applications\nbefore machine learning 282\nchoosing right architecture 282–283\nproject structure 283–284\nversion control 285–286\narchitecture 34\nartificial intelligence (AI) 8–10\nartificial neural networks 37\nassociation 58\nattention 184, 186, 190–217\nlimitation of vanilla Seq2Seq models 185–186\nmechanism 186–187\nself-attention 192–200\nsequence-to-sequence with 187–192\nbuilding Seq2Seq machine translation (MT) \nwith attention 189–192\nencoder-decoder attention 188–189\nspell-checker case study 208–217\nimproving spell-checker 213–217\nspell correction as machine translation\n208–210\ntraining spell-checker 210–213\nTransformer-based language models 200–208\nGPT-2 (generative pretraining) 205–207\nTransformer as language model 200–203\nTransformer-XL 203–205\nXLM 207–208\nattention_mask tensor 231–232\nAutoModel.from_pretrained() class method 232\nAutoModelWithLMHead class 203–204\nautoregressive models 137, 157\nAutoTokenizer class 203\nAutoTokenizer.from_pretrained() class \nmethod 230\nAWS (Amazon Web Services) 291\nB\nb constant 60, 87, 179\nb parameter 59, 85, 89\nbackpropagation 42, 90\nbag of embeddings 227\nBagOfEmbeddingsEncoder 182\nBaseHandler 295\nbasic_classifier 248\nBasicTextFieldEmbedder 36\nbatch_size parameter 44\nbatching\nclassifiers 43–44\ninstances 256–261\nmasking 259–261\npadding 256–257\nsorting 257–259\nBCEWithLogitsLoss 273\n- -beam option 163\nbeam search decoding 161–163\nbeginning of sentence (BOS) token 226\nBERT (Bidirectional Encoder Representations \nfrom Transformers) 222–229\nadapting 226–229\nlimitations of word embeddings 222–224\nnatural language inference with BERT case \nstudy 243–251\n",
      "content_length": 2712,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "INDEX\n307\nnatural language inference (NLI) \ndefined 243–244\nusing BERT for sentence-pair \nclassification 244–246\nusing Transformers with AllenNLP 246–251\npretraining 225–226\nself-supervised learning 224\nsentiment analysis with BERT case study\n229–236\nbuilding model 232–233\ntokenizing input 230–231\ntraining model 233–236\nbert key 249\nbert_pooler 249\nbert-base-cased 229, 240\nBertPooler 249\nbias 59\nBidirectional Encoder Representations from \nTransformers. See BERT\nbidirectional RNN (recurrent neural \nnetwork) 124–125\nbigrams 54\nbinary classification 27\nbitexts 146\nBLEU (bilingual evaluation understudy) 152, 164\nBOS (beginning of sentence) token 226\nBPE (byte-pair encoding) 55, 263\nbucket_name variable 297\nBucketBatchSampler 44, 98, 259\nbyte-pair encoding (BPE) 55, 263\nC\n__call__ method 230\nCAT (computer-aided translation) 147\ncausal language modeling (CLM) 207\nCBOW (continuous bag of words) model 67–68\ncells 83, 172\nchain rule 132\nchannels 176\ncharacters 52, 134, 210\ncharacter models 262–263\nusing as input 106\nCharacterTokenizer 108, 134\nchatbot case study 165–170\ndialogue systems 165–166\npreparing dataset 166–167\ntraining and running chatbot 167–169\nCI (continuous integration) 23, 287\nCL (computational linguistics) 9\nclassification 27\nclassification boundary 266\nclassification head 227\nclassifiers\nevaluating 45–46\ntraining 43–44\nbatching 43–44\nputting everything together 44\nCLM (causal language modeling) 207\ncloze tests 225\nCLS token 230, 244, 249\nCnnEncoder 181–182\nCNNs (convolutional neural networks) 171–183\nconvolutional layers 175–179\ncombining scores 178–179\npattern matching using filters 175–176\nrectified linear unit (ReLU) 176–178\noverview 174\npattern matching for sentence classification 173\npooling layers 179–180\nrecurrent neural networks (RNNs) \nshortcomings 172–173\ntext classification case study 180–183\ntext classification defined 180–181\ntraining and running classifier 182–183\nusing CnnEncoder 181–182\ncomputer-aided translation (CAT) 147\nconditional generation task 202\nconditional language models 131, 156\nCoNLL-U format 119\ncontextualization 223\ncontextualized embeddings 222–223\ncontinuous bag of words (CBOW) model 67–68\nconvolution 174\nconvolutional layers 175–179\ncombining scores 178–179\npattern matching using filters 175–176\nrectified linear unit (ReLU) 176–178\ncoreference resolution 170\ncosine similarity 65\ncost 41\ncross entropy 62, 120\ncross-attention 187, 196\ncross-entropy loss 41\ncross-validation 269–270\ncuda_device parameter 44, 291\nD\ndata collection 22\ndata-to-text generation 20\nDataLoader 44, 259\ndataset_reader key 103\nDatasetReader 33, 97, 103, 107\n",
      "content_length": 2615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "INDEX\n308\ndatasets 28–33\nchatbot case study 166–167\ncreating dataset reader 106–108\ndefined 28–29\nimbalanced datasets 270–273\nupsampling and downsampling 271–272\nuse appropriate evaluation metrics 270–271\nweighting losses 272–273\nloading Stanford Sentiment Treebank (SST) \ndatasets using AllenNLP 33\nPOS (part-of-speech) tagging 118–119\nStanford Sentiment Treebank (SST) 29–30\ntrain, validation, and test sets 30–32\ntranslators 148–150\ndecoder\nencoder-decoder attention 188–189\noverview 156–158\ndeep learning (DL) 8–10\ndependency parsing 19\ndependency trees 19\ndeploying\napplications 23\nNLP model 286–292\nmonitoring 289\ntesting 286–287\ntrain-serve skew 288–289\nusing GPUs 289–292\ndetectors 110–111\ndev or development sets 30\ndialogue systems 15, 165–166\ndict 45, 59, 65, 70, 104\n- -direction maximize 277\nDistilBERT 240\nDistilGPT2 292\ndistillation 240, 292\ndistributed memory model of paragraph vectors \n(PV-DM) 74\ndistributed representations 58\ndistributional hypothesis 58\nDL (deep learning) 8–10\nDoc2Vec class 76\nDoc2Vec model 74\ndocument classification 180\ndocument-level embeddings 74–76\ndomain adaptation 222\ndownsampling 271–272\ndropout 267\nE\nearly stopping 152, 268–269\necho command 213\nElman RNN 84, 172\nELMo (Embeddings from Language \nModels) 236–237\nembedder 181\nEmbedding class 99\nEMBEDDING_DIM 36, 39, 64\nembedding_dim 100, 182\nEMNLP (Empirical Methods in Natural Language \nProcessing) 10\nencoder\nencoder-decoder attention 188–189\noverview 154–155\n- -[encoder|decoder]-embed-dim parameter 212\n- -[encoder|decoder]-attention-head \nparameter 212\n- -[encoder|decoder]-layers parameter 212\nencoder parameter 104\n- -[encoder/decoder]-ffn-embed-dim \nparameter 212\nentity linking 127\nEOS (end of sentence) token 226\nepochs 31\neval() method 289\nevaluation metrics 270–271\nEWT (English Web Treebank) 119\nF\nf function 187–189, 193\nF-measure 96\nf() function 84, 86, 90\nF1-measure 96, 129, 164\nfairseq-interactive command 153, 168, 189, 191, \n215\nfairseq-preprocess command 167, 189, 212, 214\nfairseq-train command 150, 152, 167, 189, 197, \n215\nfalse negatives (FN) 95\nfalse positives (FP) 95\nfastText 72–74\nsubword information 72–73\nusing toolkit 73–74\nfeature extraction 226, 228\nfidelity 163\nfields 97–98\nfilters 175–176\nfine-tuning 221, 226\nfluency 13, 163\nFN (false negatives) 95\nfolds 269\nforget() function 91–92\nform_instances() function 99\nforward method 260\nforward pass 42\n",
      "content_length": 2384,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "INDEX\n309\nforward_on_instances() method 110\nforward() function 40\nforward() method 100, 232\nFP (False positives) 95\nfrom_pretrained() method 203–204\nfully connected layers 39\nG\nGANs (generative adversarial networks) 21\nGated Recurrent Units (GRUs) 92–93\ngating 92\ngazetteers 127\nGBDTs (gradient-boosted decision trees) 282\ngenerative adversarial networks (GANs) 21\ngenerative pretraining (GPT-2) 205–207\nGeorgetown–IBM Experiment, The 10\nget_metrics() method 45\nget_text_field_mask() helper function 260\ngisting 147\nGloVe 68–71\nlearning word embeddings 68\nusing pretrained vectors 69–71\nGPT-2 (generative pretraining) 205–207\nGPUs (graphics processing units) 43, 289–292\ngradient-boosted decision trees (GBDTs) 282\ngradients 90\ngrammatical error correction 13–14\ngraphemes 52\ngreedy decoding 158–161\ngrid search 275–276\nGRUs (Gated Recurrent Units) 92–93\nH\nhandle() method 295\nhandler option 295\nhead command 166\nhidden state 172\nHIDDEN_DIM 39\nhidden_state 92\nHMMs (hidden Markov models) 16\nhyperbolic tangent function 86\nhyperparameters 31, 273–278\nexamples of 274\ngrid search vs. random search 275–276\ntuning with Optuna 276–278\nhypotheses 161, 243\nI\nICA (independent component analysis) 77\nIllustrated Transformer, The 194\nimage captioning 20\nimbalanced datasets 93, 270–273\nupsampling and downsampling 271–272\nuse appropriate evaluation metrics 270–271\nweighting losses 272–273\nimplementation phase 23\nin_features 40\n- -include-package 105\nindependent component analysis (ICA) 77\ninference() method 295\n__init__() method 232\n(__init__) constructor 301\ninit_state() function 136\ninitialize() method 295\ninner product 179\ninput_ids tensor 231–232\ninstances 28, 97–98\nbatching 256–261\nmasking 259–261\npadding 256–257\nsorting 257–259\nrunning detector on unseen 110–111\ninterpreting model predictions 298–301\nIOB2 tagging 128\nis_grammatical() function 89–91\nJ\nJsonnet format 103\nK\nk-fold cross validation 269\nkernels 176\nknowledge distillation 240, 292\nL\nL2 regularization 267\nlabels 29\nlanguage building blocks 52–54\ncharacters 52\nn-grams 53–54\nwords, tokens, morphemes, and phrases 53\nlanguage detection case study 105–111\nbuilding training pipeline 108–110\ncreating dataset reader 106–108\nrunning detector on unseen instances\n110–111\nusing characters as input 106\nLanguage Interpretability Tool (LIT) 280, 299\nlanguage model (LM) head 204\n",
      "content_length": 2341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "INDEX\n310\nlanguage modeling 130–133\nbased on Transformer 200–208\nGPT-2 (generative pretraining) 205–207\nTransformer as language model 200–203\nTransformer-XL 203–205\nXLM 207–208\ndefined 130–131\nimportance of 131–132\ntext generation using RNNs 133–138\nevaluating text using language model 134–136\nfeeding characters to an recurrent neural net-\nwork (RNN) 134\ngenerating text using language model\n136–138\ntraining RNN language model 132–133\nlanguage models 130–131, 146\nlanguage pair 144\nlast_hidden_state 232\nlayers 9, 39\nlemmatization 56–57\nlinear layers 38–39, 59–60\nlinear() function 137\nlinear2() function 84\nLIT (Language Interpretability Tool) 280, 299\nLM (language model) head 204\nlocally sensitive hashing (LSH) 70\nlogits 41, 204\nlong short-term memory (LSTM) 39, 90–92\nloss 41\nloss functions 41–43\nPOS (part-of-speech) tagging 119–121\nweighting losses 272–273\n'loss' key 65\nloss.backward() function 235\nLSH (locally sensitive hashing) 70\nLSTM (long short-term memory) 39, 90–92\nLstmClassifier class 104, 108\nM\nmacro average 129\nmajority class baseline 270\n[MASK] 225–226, 238\nmasked language model (MLM) 225, 238\nmasking 259–261\nmax pooling 179\n- -max-tokens parameter 212\nmean over time 227\n- -metrics best_validation_accuracy 277\nmicro average 129\nmin_count 64, 99\nML (machine learning) 8–10\narchitecting applications before 282\ntraditional 219–220\nMLM (masked language model) 225, 238\nModel class 100\nmodel selection 30, 148\nmodel_store 295\nmodel.eval() function 235, 289\nmodel() function 201\nmodified precision 164\nModule class 101\nmonitoring\napplications 23–24\ndeployed NLP model 289\nmorphemes 53\nmorphological analysis 53\nMT (machine translation) 13, 144–147\nbuilding Seq2Seq MT with attention 189–192\nspell correction as 208–210\nmulticlass classification 27\nmultihead self-attention 194\nmultilayer perceptron 60\nmultilayer RNNs (recurrent neural \nnetworks) 122–124\nN\nn-grams 53–54\nnamespaces 98\nnatural language generation (NLG) 20\nnegative classes 94\nNER (named entity recognition) 126–130\ndefined 127–128\nimplementing named entity recognizer\n128–130\ntagging spans 128\nneural machine translation (NMT) 123, 146\nneural network models 37–41\narchitecture for sentiment analysis 39–41\ndefined 37–38\nRNNs and linear layers 38–39\ntokenization for 261–265\ncharacter models 262–263\nsubword models 263–265\nunknown words 261–262\nneural networks, defined 37\nnext-sentence prediction (NSP) 226, 239, 242\nngram_filter_sizes argument 182\nNLG (natural language generation) 20\nNLI (natural language inference) 243–251\ndefined 243–244\nusing BERT for sentence-pair classification\n244–246\nusing Transformers with AllenNLP 246–251\nNLP (natural language processing) 3–25\napplications 13–15, 21–48, 255–303\n",
      "content_length": 2700,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "INDEX\n311\narchitecting 281–286\navoiding overfitting 265–270\nbatching instances 256–261\ncase study 292–298\ndealing with imbalanced dataset 270–273\ndeploying 46–48, 286–292\ndevelopment of 21–24\ndialog systems 15\nevaluating classifier 45–46\ngrammatical and spelling error \ncorrection 13–14\nhyperparameters 273–278\ninterpreting and visualizing model \npredictions 298–301\nloss functions and optimization 41–43\nmachine translation (MT) 13\nneural networks 37–41\nsearch engine 14\nsentiment analysis 27\nstructure of 24–25\ntokenization for neural models 261–265\ntraining classifier 43–44\nword embeddings 34–37\nworking with NLP datasets 28–33\nartificial intelligence, machine learning, deep \nlearning 8–10\ndefined 4–6\nreasons for 10–12\ntasks 15–21\nparsing 17–20\npart of speech (POS) tagging 16–17\ntext classification 15\ntext generation 20–21\nwhat is not 6–8\nNMT (neural machine translation) 123, 146\nnn.CrossEntropyLoss function 232\n- -no-cache-dir transformers 297\nnonlinearity 84–88\nNP (noun phrase) 17\nnp.argmax 110\nNSP (next-sentence prediction) 226, 239, 242\nnum_embeddings 100\nnum_filters argument 182\nnum_labels parameter 232\n—num_layers flag 125\nnum_layers parameter 124\nnvidia-smi command 290\nO\none-hot vectors 36\nOOV (out-of-vocabulary) 56, 72, 134, 192, 261\noptimization 41–43, 60\noptimizer.step() function 235\noptimizers 42\nOptuna 276–278\nout_feature 40\noverfitting 31, 265–270\ncross-validation 269–270\nearly stopping 268–269\nregularization 265–267\ndropout 267\nL2 regularization 267\nP\npadding 256–257\nparagraph vectors 74\nparallel corpora 146\nparameters 37, 60, 273\nparse trees 18\nparsing 17–20\npart-of-speech. See POS (part-of-speech)\npast variables 204\npaste command 166\npatience 152\npattern matching\nfor sentence classification 173\nusing filters 175–176\nPCA (principal component analysis) 77\nPenn Treebank (PTB) 28\nperceptrons 60\nperplexity 136\nphrases 53\npip install annoy 70\npip install fairseq 147\npip install gensim 75\npip install nltk 54\npip install scikit-learn 77\npip install spacy 54\npip install transformers 203\nPLMs (pretrained language models) 218–251\nALBERT 241–243\nBERT (Bidirectional Encoder Representations \nfrom Transformers) 222–229\nadapting 226–229\nlimitations of word embeddings 222–224\nnatural language inference with BERT case \nstudy 243–251\npretraining 225–226\nself-supervised learning 224\nsentiment analysis with BERT case study\n229–236\nDistilBERT 240\nELMo (Embeddings from Language \nModels) 236–237\n",
      "content_length": 2427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "INDEX\n312\nPLMs (pretrained language models) (continued)\nRoBERTa 239–240\ntransfer learning 219–222\noverview 220–222\ntraditional machine learning 219–220\nword embeddings 220\nXLNet 237–238\npolarity 27, 181\npooler_output 232\npooling layers 179–180\nPorter stemming algorithm 56\nPorter, Martin 56\nPorterStemmer class 56\nPOS (part-of-speech) 16, 112–113, 132, 142, 219\nPOS (part-of-speech) tagging 16–17, 118–122\nbuilding training pipeline 121–122\ndefining model and loss 119–121\nreading dataset 118–119\npos_weight parameter 273\npositional encoding 196\npositive classes 94\npositive_label 104\nPOST requests 47\npostediting 147\nPP (prepositional phrase) 19\nPP-attachment problem 6\nprecision 94–95\npredict_minibatch() function 301\npredict() method 130\npredictions\nmaking 46\nserving 46–48\npredictors 46\npremises 243\npretrained GloVe vectors 69–71\npretrained_transformer 247–248\nPretrainedTransformerEmbedder 248\nPretrainedTransformerIndexer 248\nPretrainedTransformerTokenizer 247\npretraining 221\npretraining BERT 225–226\nprincipal component analysis (PCA) 77\nprint statement 260\nprocess_adverb() 89\nprocess_main_subject() 89–90\nprocess_main_verb 89\nproject structure 283–284\nPTB (Penn Treebank) 28\nPV-DM (distributed memory model of paragraph \nvectors) 74\npython -m spacy download en 54\nPytorchSeq2SeqWrapper 117\nPytorchSeq2VecWrapper 39, 100, 117, 119\nR\nrandom baseline 270\nrandom search 275–276\nread_corpus() method 75\nread_dataset 231\nread() method 97, 107\nreal-time streaming frameworks 283\nrecall 94–95\nrectified linear units 178\nrecurrent neural networks. See RNNs (recurrent \nneural networks)\nregularization 265–267\ndropout 267\nL2 regularization 267\nrelation extraction 127\nReLU (rectified linear unit) 176–178\nrnn_vec() function 115–116\nrnn() function 88\nRNNs (recurrent neural networks) 17, 38–39, 112, \n184\nencoding sequences 115–117\nsentence classification 81–88\nabstraction 82–84\nhandling variable-length input 81–82\nsimple and nonlinearity 84–88\nshortcomings of 172–173\ntext generation using 133–138\nevaluating text using language model\n134–136\nfeeding characters to RNN 134\ngenerating text using language model\n136–138\ntoken embedders 99–100\ntraining RNN language model 132–133\nRoBERTa (robustly optimized BERT) 239–240\nS\nSageMaker 296–298\nsample_token() method 205\nsample() function 137\nscheduler.step() function 235\nscores, combining 178–179\nsearch engine 14\nself-attention 192–200\nself-supervised learning 61, 224\nsentence classification 80–111\naccuracy 93–94\nAllenNLP training pipelines 96–102\nbuilding model 100–101\nconfiguring 102–105\ninstances and fields 97–98\n",
      "content_length": 2570,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "INDEX\n313\nputting all together 101–102\ntoken embedders and RNNs 99–100\nvocabulary and token indexers 98–99\nF-measure 96\nGRUs (Gated Recurrent Units) 92–93\nlanguage detection case study 105–111\nbuilding training pipeline 108–110\ncreating dataset reader 106–108\nrunning detector on unseen instances\n110–111\nusing characters as input 106\nLSTM (long short-term memory) 90–92\npattern matching for 173\nprecision and recall 94–95\nRNNs (recurrent neural networks) 81–88\nabstraction 82–84\nhandling variable-length input 81–82\nsimple and nonlinearity 84–88\nsentence-pair classification 244–246\nvanishing gradients problem 88–90\nsentence pairs 148\nsentence pieces 210\nsentence-order prediction (SOP) 226, 242\nsentence-prediction task 226\nSentenceClassifierPredictor 46\nsentiment analysis 15\nneural networks for 39–41\noverview 27\nwith BERT case study 229–236\nbuilding model 232–233\ntokenizing input 230–231\ntraining model 233–236\nword embeddings for 36–37\n[SEP] token 230, 244, 246\nSeq2Seq (sequence-to-sequence) models\n141–170\nbuilding translator 147–154\npreparing datasets 148–150\nrunning translator 153–154\ntraining model 150–153\nchatbot case study 165–170\ndialogue systems 165–166\npreparing dataset 166–167\ntraining and running chatbot 167–169\ncomponents of 154–163\nbeam search decoding 161–163\ndecoder 156–158\nencoder 154–155\ngreedy decoding 158–161\nencoder in AllenNLP 117–118\nevaluating translation systems 163–165\nautomatic evaluation 163–165\nhuman evaluation 163\nlimitation of vanilla Seq2Seq models 185–186\nmachine translation (MT) 144–147\noverview 142–144\nwith attention 187–192\nbuilding Seq2Seq machine translation (MT) \nwith attention 189–192\nencoder-decoder attention 188–189\nSeq2SeqEncoder 117\nSeq2VecEncoder 181–182, 248–249\nsequence_cross_entropy_with_logits() \nfunction 260–261\nsequential labeling 112–138\nbidirectional RNN 124–125\nbuilding part-of-speech tagger 118–122\nbuilding training pipeline 121–122\ndefining model and loss 119–121\nreading dataset 118–119\ndefined 113–114\nimplementing Seq2Seq encoder in \nAllenNLP 117–118\nmultilayer RNNs 122–124\nnamed entity recognition 126–130\ndefined 127–128\nimplementing named entity recognizer\n128–130\ntagging spans 128\nusing RNNs to encode sequences 115–117\n- -serialization-dir 105\nserialized-file 295\nSGD (stochastic gradient descent) 42\nsimple RNNs (recurrent neural networks) 84–88\nSingleIdTokenIndexer 135\nSkip-gram 57–68\nCBOW (continuous bag of words) model\n67–68\nimplementing on AllenNLP 62–66\nlinear layers 59–60\norigin of word embeddings 57–58\nSoftmax 61–62\nword associations 58–59\nSMT (statistical machine translation) 146\nSNLI (Standard Natural Language Inference) 244\nsnli type 247\nSnliReader 245, 247\nSoftmax 61–62\nsoftmax() function 137\nSOP (sentence-order prediction) 242\nsorting 257–259\nsorting_keys 259\nsource language 13, 144\nSpanBasedF1Measure 129\nspans, tagging 128\nspec() method 300\n",
      "content_length": 2855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "INDEX\n314\nspell-checker case study 208–217\nimproving spell-checker 213–217\nspell correction as machine translation 208–210\ntraining spell-checker 210–213\nspelling error correction 13–14\nSST (Stanford Sentiment Treebank) datasets\nloading using AllenNLP 33\noverview 29–30\nsst_tokens 103\nStanfordSentimentTreeBankDatasetReader 33, \n97, 103, 107\nstate 82–83, 85, 89, 115–117\nstdout 236, 250\nstemming 55–56\nstochastic gradient descent (SGD) 42\nstratified sampling 272\nstudent model 240, 292\nsubword information 72–73\nsubword models 263–265\nsubword units 246\nsupervised machine learning paradigm 27\nswitch 92\nsyntactic ambiguity 6\nT\nt-SNE (t-distributed Stochastic Neighbor \nEmbedding) 77\nTaggedDocument 75\ntagging\nPOS (part-of-speech) 16–17\nspans 128\ntanh 86\ntarget language 13, 144\ntasks 15–21\nparsing 17–20\nPOS (part-of-speech) tagging 16–17\ntext classification 15\ntext generation 20–21\nteacher model 240, 292\ntest instances 46\ntest sets 30–32\ntesting, NLP model 286–287\ntext classification\ncase study 180–183\ntext classification defined 180–181\ntraining and running classifier 182–183\nusing CnnEncoder 181–182\noverview 15\ntext generation 20–21, 133–138\nevaluating text using language model 134–136\nfeeding characters to RNN 134\nusing language model 136–138\ntext mining 10\ntext_to_instance() method 107\ntext-to-text generation 20\nTextFieldEmbedder 181, 248\nTN (true negatives) 95\nto() method 291\ntoken_embedders parameter 249\ntoken_in 63, 65\ntoken_indexers 135, 247, 249\ntoken_min_padding_length parameter 182\ntoken_out 63, 65\ntoken_type_ids tensor 231–232\nTokenEmbedder 99\ntokenization 54–55\nBERT case study 230–231\nfor neural models 261–265\ncharacter models 262–263\nsubword models 263–265\nunknown words 261–262\ntokens\ndefined 53\ntoken embedders 99–100\ntoken indexers 98–99\ntokenize() method 108\n—tokenizer parameter 247\ntokenizer.decode() method 204–205\ntokenizer() function 231\ntokens 41, 53, 68–69, 97, 135\ntoolkit, fastText 73–74\ntorch-model-archiver 295\ntorch.multinomial() function 137\ntorch.nn.LSTM 100, 117\nTorchServe 292–296\nTP (true positives) 94\ntrain (or training) sets 30\ntrain method 44\ntrain sets 30–32\ntrain_data_path key 103\ntrain-serve skew 288–289\ntrain() function 102\ntrain() method 289\nTrainer class 44\ntraining\napplications 23\nchatbot case study 167–169\nclassifiers 43–44\nbatching 43–44\nputting everything together 44\nRNN language model 132–133\nspell-checker 210–213\ntranslators 150–153\ntraining pipelines\nAllenNLP 96–102\n",
      "content_length": 2442,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "INDEX\n315\nbuilding model 100–101\nconfiguring 102–105\ninstances and fields 97–98\nputting all together 101–102\ntoken embedders and recurrent neural net-\nworks (RNNs) 99–100\nvocabulary and token indexers 98–99\nlanguage detection case study 108–110\nPOS (part-of-speech) tagging 121–122\ntraining-serving skew 46\ntransfer learning 219–222\noverview 220–222\ntraditional machine learning 219–220\nword embeddings 220\ntransformation invariant CNNs 180\nTransformer\nlanguage models based on 200–208\nGPT-2 (generative pretraining) 205–207\nTransformer as language model 200–203\nTransformer-XL 203–205\nXLM 207–208\nself-attention and 192–200\nwith AllenNLP 246–251\ntransformers library 202–203, 206\ntranslation models 146\ntranslation systems 163–165\nautomatic evaluation 163–165\nhuman evaluation 163\ntranslators 147–154\npreparing datasets 148–150\nrunning translator 153–154\ntraining model 150–153\ntreebanks 28\ntrials 275\ntrigram 54\ntype key 104\ntypes 68–69\nU\nUD (Universal Dependencies) 20, 118\nunconditional generation 202\nunconditional language models 131, 156\nunigram 54\nuniversal part-of-speech tagsets 16, 118\nUniversalDependenciesDatasetReader 119\nUniversalPOSPredictor 122\n<UNK> 99, 134\nUNK tokens 226, 261, 264\nunknown words 261–262\nunrolling 88\nupdate_hidden() function 92\nupdate_simple() function 84, 87\nupdate() function 83–84, 88, 137\nupsampling 271–272\nV\nv() function 116\nvalidation sets 30–32, 191, 267, 282\nvalidation_data_path key 103\nvanilla 172\nvanilla Seq2Seq models 185–186\nvanishing gradients problem 88, 90\nvariable-length input 81–82\nvectors 34\nversion control 285–286\nvisualizing\nmodel predictions 298–301\nword embeddings 76–79\nvocabulary 34, 98–99\nVocabulary class 98–99\nvocabulary items 98\nVocabulary object 98, 261\nVP (verb phrase) 17\nVSO (verb-subject-object) 145\nW\nw constant 60, 179\nw parameter 59\nwarm-up 234\nweight decay 267\nweighting losses 272–273\nweights 37\nWER (word error rate) 164\nWikification 127\nwith torch.no_grad() function 235\nWK projection 193\nword associations 58–59\nword embeddings 34–37, 49–79\nbuilding blocks of language 52–54\ncharacters 52\nn-grams 53–54\nwords, tokens, morphemes, and phrases 53\ndefined 34–36, 50\ndocument-level embeddings 74–76\nfastText 72–74\nsubword information 72–73\nusing toolkit 73–74\nfor sentiment analysis 36–37\nGloVe 68–71\nlearning word embeddings 68\nusing pretrained vectors 69–71\nimportance of 50–52\nlemmatization 56–57\n",
      "content_length": 2377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "INDEX\n316\nword embeddings (continued)\nlimitations of 222–224\nSkip-gram 57–68\nCBOW (continuous bag of words) model\n67–68\nimplementing on AllenNLP 62–66\nlinear layers 59–60\norigin of word embeddings 57–58\nSoftmax 61–62\nword associations 58–59\nstemming 55–56\ntokenization 54–55\ntransfer learning 220\nvisualizing 76–79\nword order 144\nword stems 55\nword_embeddings parameter 104\nWordNetLemmatizer 57\nwords 53, 116\nX\nxlim() function 78\nXLM 207–208\nXLM-R (XML-RoBERTa) 240\nXLNet 237–238\nXOR (or exclusive-or) 87\nY\nylim() function 78\nZ\nzero pronoun 146\n",
      "content_length": 545,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "Masato Hagiwara\nISBN: 978-1-61729-642-0\nT\nraining computers to interpret and generate speech and \ntext is a monumental challenge, and the payoff  for reduc-\ning labor and improving human/computer interaction \nis huge! Th e fi eld of Natural Language Processing (NLP) is \nadvancing rapidly, with countless new tools and practices. Th is \nunique book off ers an innovative collection of NLP techniques \nwith applications in machine translation, voice assistants, text \ngeneration, and more.\nReal-world Natural Language Processing shows you how to \nbuild the practical NLP applications that are transforming \nthe way humans and computers work together. Guided by \nclear explanations of each core NLP topic, you’ll create many \ninteresting applications including a sentiment analyzer and a \nchatbot. Along the way, you’ll use Python and open source \nlibraries like AllenNLP and HuggingFace Transformers to \nspeed up your development process. \nWhat’s Inside\n● Design, develop, and deploy useful NLP applications\n● Create named entity taggers\n● Build machine translation systems\n● Construct language generation systems and chatbots\nFor Python programmers. No prior machine learning know-\nledge assumed.\nMasato Hagiwara received his computer science PhD from \nNagoya University in 2009. He has interned at Google and \nMicrosoft Research, and worked at Duolingo as a Senior \nMachine Learning Engineer. He now runs his own research \nand consulting company.\nRegister this print book to get free access to all ebook formats. \nVisit https://www.manning.com/freebook\n$59.99 / Can $79.99  [INCLUDING eBOOK]\nReal-World Natural Language Processing\nDATA SCIENCE/NATURAL LANGUAGE PROCESSING\nM A N N I N G\n“\nTh e defi nitive reference for \nthose of us trying to understand \nNLP and its applications.”\n \n—Richard Vaughan\nPurple Monkey Collective\n“\nVery practical book about \nNLP and how to use it \nsuccessfully in real-world \n applications.”\n \n—Salvatore Campagna, King\n“\nIf you need to step up your \ngame but were turned off  by \na diffi  cult learning curve, \n then this book is for you!”\n \n—Alain Lompo, ISO-Gruppe\n“\nAn excellent and \napproachable fi rst step in \nlearning NLP. Well written \n  and easy to follow.”\n—Marc-Anthony Taylor\nBlackshark.ai\nSee first page\n",
      "content_length": 2248,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}