{
  "metadata": {
    "title": "Artificial Intelligence Generated Code Considered",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 15,
    "conversion_date": "2025-12-19T17:19:31.881808",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Artificial Intelligence Generated Code Considered.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "4 2 0 2\n\np e S 7 2\n\n]\n\nR C . s c [\n\n1 v 2 8 1 9 1 . 9 0 4 2 : v i X r a\n\nArtificial-Intelligence Generated Code Considered Harmful: A Road Map for Secure and High-Quality Code Generation\n\nChun Jie Chong, Zhihao (Zephyr) Yao, Iulian Neamtiu {cc255, zhihao.yao, ineamtiu@njit.edu} New Jersey Institute of Technology\n\nAbstract Generating code via a LLM (rather than writing code from scratch), has exploded in popularity. However, the security implications of LLM-generated code are still unknown. We performed a study that compared the security and quality of human-written code with that of LLM-generated code, for a wide range of programming tasks, including data structures, algorithms, cryptographic routines, and LeetCode questions. To assess code security we used unit testing, fuzzing, and static analysis. For code quality, we focused on complexity and size. We found that LLM can generate incorrect code that fails to implement the required functionality, especially for more complicated tasks; such errors can be subtle. For example, for the cryptographic algorithm SHA1, LLM gener- ated an incorrect implementation that nevertheless compiles. In cases where its functionality was correct, we found that LLM-generated code is less secure, primarily due to the lack of defensive programming constructs, which invites a host of security issues such as buffer overflows or integer over- flows. Fuzzing has revealed that LLM-generated code is more prone to hangs and crashes than human-written code. Quality- wise,we found that LLM generates bare-bones code that lacks defensive programming constructs, and is typically more com- plex (per line of code) compared to human-written code. Next, we constructed a feedback loop that asked the LLM to re- generate the code and eliminate the found issues (e.g., malloc overflow, array index out of bounds, null dereferences). We found that the LLM fails to eliminate such issues consistently: while succeeding in some cases, we found instances where the re-generated, supposedly more secure code, contains new issues; we also found that upon prompting, LLM can intro- duce issues in files that were issues-free before prompting. Our study exposes the perils of LLM-generated code (and feedback loops), particularly in the critical security domain.\n\n1 Introduction\n\nSoftware security is of utmost importance in software engi- neering, as it directly affects the security and reliability of a\n\ndigital society increasingly dependent on software. For ex- ample, the 2024 CrowdStrike bug that crashed 8.5 million Microsoft Windows devices [18] highlights the worldwide impact of software bugs. Human experts have been trained to write, review, and test code to ensure its quality, despite the fact that the process is time-consuming and error-prone. However, the security and quality of Artificial Intelligence (AI)-generated code is an under-studied area. With the ad- vance of AI, the shift towards AI-assisted programming is rapidly gaining momentum, making the concerns more ur- gent.\n\nLarge Language Models (LLMs) are already widely used to assist developers in code completion and summarization [7,12,13], and in some cases, to automatically generate code from scratch to meet the requirements of specific tasks [44]. For example, GitHub Copilot [13] (“Copilot” for short), a widely-adopted AI coding assistant, has been available since 2022 [13]. While users report an improvement in productivity (81% and 88%, respectively reported by two independent user studies [3, 14]), an empirical investigation has shown that 40% of Copilot-generated programs are buggy [40]. The false sense of productivity when using LLM code generation in the workplace has mostly been driven by developers aiming to fulfill industry’s internal performance metrics, such as task completion time and lines of code produced [26], rather than code quality.\n\nThe security implications of AI code generation are largely ignored by the industry: Copilot Voice has been introduced as a new feature that allows inexperienced developers to generate full programs by simply speaking to the AI assistant, mar- keted as a new way to “write code without the keyboard” [16]. Naïve trust in AI code generation can lead to significant se- curity vulnerabilities and degradation in code quality. Under- standing the traits and limitations of the LLM-generated code is important to facilitate the adoption of LLMs in future soft- ware engineering practice. In October 2023, the White House issues an Executive Order on “Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence”, which em- phasize the importance of security in such AI use cases [4,10].\n\n1\n\nMotivated by the national goal, this work aims to study the security and quality of code generated by the state-of-the- art LLM model, OpenAI’s GPT-4o, which powers GitHub Copilot Enterprise [15]. Specifically, we create an evaluation framework, EXACT ( EXamination System for AI-Generated Code Testing), which uses a suite of program analyses, unit testing, fuzzing, complexity and size measurements, to assess code security and quality. EXACT compares these factors with the human-written code that implements the same functional- ity. We focus on three classes of representative coding tasks: over 200 LeetCode programming tests, fifteen commonly- used algorithms and data structure implementations, as well as popular cryptographic functions. For each of these tasks, we compare the security and quality of the human-written and LLM-generated code. We chose the C programming language due to its prevalence in systems and security-sensitive code. For each comparison experiment, EXACT checks function- alities of LLM-generated and human-written code using an empirical method: if a predefined test suite (e.g., LeetCode online submission [19]) is available, we submit both programs to the test suite; otherwise, we use unit testing and AFL [2] to create test cases and compare the runtime behavior of the two programs. Note that fuzzing findings are used to assess both security and functionality. A recent study [53] demon- strates that LLM is not capable of solving complicated tasks (tasks that take a human more than 11 minutes). We observe a similar trait in our study: among the 21 LeetCode tasks that LLM failed to solve, 20 tasks are rated at medium or hard difficulty. LLM also implements the SHA-1 hash func- tion incorrectly, where it generates the wrong hash values for all the inputs. We also demonstrate the unreliability of both the AI-generated and human-written code in the presence of fuzzing, as they both fail to handle corner cases in our test suites (though human code performs slightly better).\n\nWe also found that the security and quality of LLM- generated code is lacking, compared to its human-written equivalent. The security issues found by Clang static ana- lyzer in LLM-generated code are consistently higher than human-written code: by 11.2% in LeetCode and 7.1% in algo- rithm tasks. Additionally, LLM-generated code is 1.19x more complex than human-written code in LeetCode and 1.26x in algorithm tasks.\n\nA feedback loop is a state-of-the-art practice carried out by various research groups [32, 36, 47] to improve LLM’s code generation. Therefore, our study also investigates the effectiveness of a feedback loop in improving the security and correctness of the generated code by iteratively feeding the results of the evaluation back to the LLM model. We show that when we asked the model to fix the discovered security issues, it can introduce new bugs in code that was previously bug-free; moreover, the re-generated version has higher complexity per line. We also observe that by request- ing shorter line lengths, GPT-4o yields subsequent code with 11.3% lower cyclomatic complexity per line, and surprisingly,\n\n2\n\nit also fixed 17.9% security issues found by Clang static ana- lyzer. In contrast, by requesting fewer lines of code, GPT-4o yields code with 18.4% higher complexity per line, and fixed 43.2% of security issues.\n\nDuring the course of our study, we have identified 116 se- curity issues in public GitHub repositories, which we have responsibly disclosed to the repository owners. For the soft- ware security issues of the code generated by OpenAI’s GPT- 4o model, we have reached out to OpenAI for their further improvement. We will open source EXACT and our test data. We discuss the full details in the Ethics and Open Science Compliance appendix.\n\nThe main contributions of our study are:\n\nWe show that, for the same task, LLM generates less- secure and lower-quality code than the human equivalent\n\nWe characterize the substantial security issues in LLM- generated code\n\nWe demonstrate that prompt engineering with a feedback loop does not necessarily improve the security of LLM- generated code.\n\n2 Background\n\n2.1 AI Code Generation\n\nLLMs have been widely adopted for natural language tasks, and also for code generation. This adoption is easily justifi- able as LLMs have shown exceptional performance in code completion [43], translation [51], and full project code gen- eration [35]. With the introduction of large-scale pre-trained LLMs, such as OpenAI’s GPT-4o that powers GitHub Copi- lot Enterprise [15], the performance of AI code generation has been further advanced to a degree that it is presumed to replace human programmers [5].\n\nThe inception of AI code generation has brought about both a revolution and a debate in the software community. On one hand, AI code improves developers’ productivity by automating coding tasks, especially the repetitive and mun- dane ones [31]. However, the security implications of fully autonomous programming using LLMs [35] and automation of repetitive coding tasks [31] that have been proposed by recent research are not well understood.\n\nIn a Copilot study, 88% of human developers have reported that they are more productive when coding with the tool [3]. The drastic improvement in productivity demonstrates the potential of AI in software engineering. Indeed, the CEO of GitHub predicts that “sooner than later,” “Copilot will write 80% of code” [5]. But, unfortunately, existing AI coding assistants have been shown to write incompetent code [40, 41, 49]. According to a study conducted by GitHub, 92% of the surveyed developers have used AI coding tools, and 70% of them believe that the tools offered an advantage in\n\n1\n\nvoid setBuffer(char *buffer) {\n\n2\n\nfor (int i = 0; i < 26; i++)\n\n3\n\nbuffer[i] = ’\\0’;\n\n4\n\n5\n\n6\n\n}\n\nint main() {\n\nGPT-4o fills in an incorrect value. Intentionally missed letter ‘G’.\n\n7\n\nchar alphabet[] = \"ABCDEFH\n\nIJKLMNOPQRSTUVWXYZ\";\n\n8\n\n9\n\nint length = 0; while (alphabet[length] != ’\\0’)\n\n10\n\nlength++;\n\n11\n\nchar *buffer =\n\n12\n\n(char *)malloc(length*sizeof(char));\n\n13\n\n14\n\n15\n\nsetBuffer(buffer); free(buffer); return 0;\n\n16\n\n}\n\nFigure 1: Sample task 1 (§3.1): we prompt GPT-4o to find the size of the buffer on line 7 and use it on line 2 for the loop condition (in the red circle). GPT-4o produces an incorrect answer, leading to heap overflow.\n\ntheir work [26]. One impetus for AI code generation is to improve companies’ internal performance metrics, such as “time to complete a task”, or “lines of code written” [26]. The adoption of AI-generated code has raised both ethics and quality concerns.\n\nRecent studies have shown significant degradation in code quality [39–41]. NetBSD, a popular open-source operating system project,referredto AI-generatedcode as “taintedcode” and banned it from their codebase [21]. We present an illus- tration of GPT-4o-generated code buffer size in Figure 1 as an example of the quality concerns. We discuss this example in depth in Section 3.1.\n\n2.2 Automated Code Improvement\n\nLLMs have been using Reinforcement learning (RL) for auto- mated code improvement. RL updates a model’s parameters through interactions with human feedback or the environment, and has shown to be effective in improving various AI models’ performance [32,47]. Reinforcement learning from human feedback (RLHF) uses human feedback to fine tune a model’s parameters [30]. Despite the fact that RLHF can be used to improve the performance of LLM code generation, scalable deployment in practice has been limited by the capability of human evaluators to provide meaningful feedback to the out- puts [36]. To cope with the lack of human feedback, OpenAI has proposed a fine-tuned model based on GPT 4, namely CriticGPT, to automatically critique the LLM-generated code, where the feedback is used to improve the subsequent gener- ations [36]. A pool of bugs and human feedback originated from OpenAI’s previous RLHF pipeline is used as the training set of the fine-tuning of CriticGPT [36]. Likewise, GitHub Copilot has proposed a secondary LLM model to “approxi-\n\n3\n\nmate the behavior of static analysis tools” to provide feedback to the outputs of the primary LLM model [6]. Since these systems have been integrated into the LLM pipeline, our work aims to investigate the security of the overall system that may include these blackbox components.\n\n2.3\n\nImplications of AI Code Generation\n\nNaïve trust in AI code-generation tools can lead to deterio- rated code quality and software security hazards. Due to the lack of metrics to evaluate the trustworthiness of generated code [49], existing quantitative evaluation of AI-generated code is limited. A user study conducted in 2023 has found significant degradation in code quality and a false sense of security when programmers use AI assistants [41]. Likewise, Pearce et al. conducted an empirical study on 1,689 Copilot- generated programs and found that 40% of the programs are vulnerable [40]. Another user study in 2024 has shown that a majority of GitHub Copilot users felt that the tool’s sugges- tions were not always accurate, as the tool “may give false code suggestions that mislead developers” [14]. Fixing the errors in the generated code is a challenging task for most users because they did not author the code themselves. Con- sequently, when AI-generated code appears to be complex from the user’s perspective, users often give up on fixing the code and resort to other online resources [48]. Indeed, strin- gent testing shows that, currently, even the most advanced LLM models, such as Claude 3.5 Sonnet [8] and GPT-4o [17], are only able to solve basic, straightforward tasks, that take human developers at most 11 minutes to solve [53].\n\n3 Motivation\n\n3.1 Sample Task 1: Finding a Buffer’s Size\n\nAs motivation for our study, we present a preliminary exper- iment we conducted on OpenAI GPT-4o, where we found that the model fails a basic, yet critical, security task: finding the size of a buffer, even though the buffer size is given in the prompt in a way that a human can easily understand. The incorrect buffer size can lead to buffer overflow if it is larger, or leaking non-initialized memory if the buffer size is smaller than the actual buffer size in the buffer initialization func- tion. We present an example of wrong buffer size in Figure 1, where the buffer size is filled in as 26, but the actual buffer size should be 25 (as the length of an incomplete alphabet with the letter ‘G’ missing is 25). In our experiment, we asked GPT-4o to fill in the buffer size in the loop condition (the red circle in Figure 1) with various buffer size definitions in the referenced code in our prompt. Table 1 shows the buffer size definitions, examples, and the success rate of GPT-4o in filling in the correct buffer size. We repeat experiments for each buffer size definition 1000 times. We found that GPT-4o\n\nnot only fails to fill in the correct buffer size when calculation is involved, but also fails to fill in the constant buffer size in some cases.\n\nFigure 2: GPT-4o filled buffer size (the red circle in Figure 1) over 1000 trials.\n\nAs shown in the Table 1, GPT-4o has a high success rate (but not 100%) in filling in the buffer size when the buffer size is a constant integer. When the buffer size requires calculation, the success rate drops significantly. The success rate is the lowest when the buffer size is calculated using floating-point numbers, possibly due to the model’s lack of understanding of float to integer casting. To our surprise, GPT-4o’s success rate drops from 92.8% to 41.6% when the buffer size is calculated from the length of alphabet with a missing letter, showing that the model concludes the buffer size based on the pattern of the alphabet, rather than the actual length of the string. Figure 2 shows the distribution of the buffer size filled in by GPT-4o in the missing letter example over 1000 trials. We were also surprised that when we give the model a constant integer buffer size, but at the same time, present a irrelevant calculation of the length of a string, the success rate drops from 99.3% to 31.2%. In rare cases (0.08% of all results), the model has even filled the size (an expected int value) with meaningless non-English characters and symbols. This preliminary study demonstrates thatGPT-4o has a highchance of generating memory bugs in C code, and motivates us to further investigate the security of LLM-generated code.\n\n3.2 Sample Task 2: Implementing SHA1\n\nWe asked GPT-4o to complete a widely-used, security-critical algorithm: SHA1 (Secure Hash Algorithm 1). The SHA1 header file from OpenBSD’s GitHub repository [23] was pro- vided in the prompt to make sure that the specifications are clear, and there are no discrepancies in function input/out- put formats between the GPT-4o and human implementation. We tested the correctness of GPT-4o’s SHA1 implementation by using the reference test vectors (predefined inputs with their expected outputs) provided by the National Software Reference Library (NSRL) [22]. GPT-4o’s implementation produced incorrect hash values for all the inputs. Such silent errors will not prompt any error message, but incorrect hash values can lead to security vulnerabilities and malfunctions\n\n4\n\nin software that uses hash algorithm for authentication or integrity checks. As expected, OpenBSD’s SHA1 implemen- tation computed all the hash values correctly.\n\n3.3 Sample Task 3: Implementing Graph Data\n\nStructure\n\nThis task entailed generating a simple graph implementation, with functions for creating (allocating) a graph, adding and removing edges, printing the graph, and freeing (deallocat- ing) the graph memory from the heap. The relevant allocation code – creating a graph of size V – is shown in Figure 3. Note that the human-developed code, shown on the left, has checks for the value of V, whereas the LLM-generated code, on the right, does not contain such checks. The LLM code has two issues. First, it does not check the result of malloc() for NULL, hence opening the first entry for potential NULL pointer dereferences if the memory allocation fails. Second, the LLM-generated code does not check for a potential nega- tive value of V. If a negative V is passed as an argument,the call to malloc() on line 5 will most likely result in a NULL pointer (since malloc() takes an unsigned argument, the argument is interpreted as a positive value at the size of size_t_max - abs(V) + 1, where size_t_max is the max value of the type), or, less likely, in a very large memory allocation. Therefore, memory allocation either fails or its size is in the control of a potential adversary. We refer to this as malloc overflow, as a term used by Clang analyzer.\n\n3.4 Motivations\n\nAligned with the national goal to manage the risk of genera- tive AI [4,10], our research investigate the security of LLM- generated code. Our research is motivated by the following questions: Research Question 1: Does LLM generate insecure code? Research Question 2: For the same task, does LLM generate code that is more secure (or higher quality) than the human- written version? Research Question 3: Can prompt engineering improve the security of LLM-generated code?\n\n4 Methodology\n\nWe select two categories of coding tasks to evaluate the secu- rity of LLM-generated code: (1) LeetCode problems, and (2) data structures and algorithms, including three widely-used cryptographic algorithms. We chose these categories because they are widely used in almost all software engineering inter- views and actual coding tasks, and they have relatively clear input-output interfaces.\n\nFor LeetCode problems, we selected all 202 problems from a well-known LeetCode solution repository on GitHub with 2.3k forks and 5.5k stars [20]. We use the same set of data\n\nBuffer Size Definitions A constant integer The length of a string (alphabet) The exponentiation of an integer to the power of another integer Multiplying two integers Subtracting a float from an integer The exponentiation of an integer to the power of another integer, plus a float The length of a string (alphabet with a missing letter) A constant integer, but with irrelevant presence of length of a string Multiplying an integer and a float Multiplying two floats\n\nExample 402 len(ABC...XYZ) pow(4, 2) 415 * 495 922 - 174.05 pow(1,5) + 63.56 len(ABC...XYZ) 612 893 * 518.33 285.63 * 62.72\n\nSuccess Rate 99.3% 92.8% 91.4% 63.1% 50.1% 42.4% 41.6% 31.2% 10.6% 1.5%\n\nTable 1: Preliminary study on GPT-4o’s ability to correctly determine the buffer size in Figure 1.\n\nHuman\n\nGPT-4o\n\n1\n\n2\n\n3\n\n4\n\n5\n\nGraph newGraph(int V) {\n\nassert(V >= 0); Graph g = malloc(sizeof(GraphRep)); assert(g != NULL);\n\n1\n\n2\n\n3\n\n4\n\n5\n\nGraph newGraph(int V) // V can be negative! {\n\n// not checked for NULL Graph g = malloc(sizeof(GraphRep)); // not checked for NULL g->edges = malloc(V * sizeof(int *));\n\nFigure 3: LLM-generated code for a graph implementation, shown on the right, has no guardrails (comments added by us).\n\nstructures, algorithms, and LeetCode problems for both LLM- generated and human-written code to ensure a fair compari- son.\n\nFor data structures and algorithms, we selected four sources of human code from GitHub: a collection of 13 data struc- tures and algorithms in C, which has 4.3k forks and 18.8k stars [27]; a repo with an additional data structure, having 719 forks and 3.3k stars [11]; a hashmap algorithm repo, with 205 forks and 520 stars [24]; and the OpenBSD project for three cryptographic algorithms with 858 forks and 3.2k stars [23].\n\n4.1 LLM Code Generation\n\nWe use OpenAI’s GPT-4o model to generate the full task solution for each coding task. As shown in Figure 4, for each coding task, we provide its task description as prompt to GPT- 4o API, and collect the coding outputs.\n\nClang static analysis\n\nClang static analysis\n\nAFL fuzzing tests\n\nComplexity analysis\n\nPrompt Construction\n\nAFL fuzzing tests\n\nTask\n\nFunctionality Equivalency Test\n\nComplexity analysis\n\nHuman-written code\n\nOnlineUnit Tests Fuzzing\n\nAI-generated code\n\nFor LeetCode problems, we locate the problem description in the LeetCode website using the problem title in the Leet- Code solution repository, and use the description as a prompt. The prompt is constructed in a way as close as possible to the actual task description given to a human developer.\n\nFigure 4: The architecture of our framework, EXACT.\n\n4.2 Functionality Validation\n\n4.2.1 LeetCode Problems\n\nFordata structures and algorithms,we provided prompts for GPT-4o as follows: either the header files, or in the absence of header files, the function declarations (types) from the human-written code. This ensures that both GPT-4o and hu- man implement the same set of functionalities and eliminate the discrepancies in function signatures in code comparison.\n\nAfter obtaining the GPT-4o-generated code, we use test cases to validate the functionalities of the code against the human- written code for the same task. If test cases are available, such as LeetCode online submission [19], we submit GPT- 4o-generated code directly to the LeetCode platform (Sec- tion 5.1.1) for evaluation.\n\n5\n\n4.2.2 Data Structures and Algorithms\n\nFor data structures and algorithms, we employ unit testing and fuzzing to validate functionality, as follows.\n\nUnit testing. Test cases are manually written for unit test- ing with the help of CUnit [9]. We examine the human-written code to understand the functionality provided by the data structure or algorithm, such as insertion, deletion, sorting, etc. We then write test cases, divided into 2 categories: (1) regular cases, and (2) edge cases. Regular cases are typical scenar- ios that a function is expected to handle, such as inserting positive integers into a linked list, perform a merge sort on an unsorted array, etc. Edge cases are crafted in a way to test a function’s protection against unexpected inputs, such as dequeuing from an empty queue, performing a breadth- first search on an empty graph, sorting an empty array, etc. We ran the same test cases on both GPT-4o-generated and human-written code, since we prompt GPT-4o to implement the data structures and algorithms with the same header files or function declarations as used in the human-written code. By testing GPT-4o-generated and human-written code with the same test cases, we verify that they are achieving the same functionalities.\n\nFuzzing. In addition to unit testing, we use AFL [2] to fuzz both GPT-4o-generated code and human-written code. We create an entry point to fuzz each data structure and algorithm. This entry point is the main method in each program where it reads an input file that contains various instructions and input values. For example, the line insert 10 in an input file will carry out the insert operation in the binary search tree and insert the value 10 into the tree. The input file is used as a seed in fuzzing. Eachdata structure andalgorithm has its own setof instructions to make sure that all implemented functionalities will be tested. Since we only focus on functionality validation, any invalid instructions mutated by AFL [2] in the process of fuzzing will be ignored in the entry point. However, values will be mutated during fuzzing and GPT-4o-generated code and human-written code need to handle any kinds of invalid inputs. As it was not relevant to the comparison, the entry point code was not included in the security and complexity analysis.\n\n4.2.3 Cryptographic Algorithms\n\nFor cryptographic algorithms, we employ a different way of validating the functionality. For one-way hashing algorithms such as SHA1 and Message Digest Algorithm 5 (MD5), we encrypt the predefined inputs from test vectors and compare the computed hash values with the expected hash values from the test vectors; we used existing sets of test vectors (strings) originating from NSRL [22] (as discussed in Section 3.2). For two-way algorithms such as Advanced Encryption Standard (AES), we encrypt the predefined inputs and decrypt the en- crypted values; we then validate the decrypted values against the original inputs.\n\n6\n\n4.3 Static Analysis\n\n4.3.1 Security\n\nWe use the Clang static analyzer [1] to perform static analysis on both GPT-4o-generated and human-written code. Clang provides a set of default checkers, such as null dereference, memory leak, and null arguments. In addition, Clang provides a range of experimental (advanced) checkers in several cat- egories, as follows. “Core” experimental checkers include detectors for pointer arithmetic, invalid casting/conversion, etc. “Security” experimental checkers look for errors such as array index out of bounds, malloc overflow, etc. “Unix” experimental checkers look for issues such as memory leaks or null pointers passed to string functions. While the exper- imental checkers might, in theory, emit a higher number of false positives, our manual validation of the reported errors indicate a negligibly low rate of false positives.\n\n4.3.2 Complexity\n\nWe evaluate code complexity using several metrics: cyclo- matic complexity, normalized complexity, and lines of code. Cyclomatic complexity is used to measure the complexity of a program’s control flow [37]. High cyclomatic complexity can indicate that the code is harder to understand and maintain, and potentially prone to errors. As cyclomatic complexity is an absolute value that characterizes an entire file, larger files naturally have higher complexity. Therefore, to gauge the complexity of typical code in a file, we also compute the normalized complexity, i.e., divide complexity by the num- ber of lines of code in that file – this indicates the typical complexity expected for a code snippet. We use SCC [25] to obtain the cyclomatic complexity and the lines of code in each C file. By “lines of code” we mean actual source code, excluding comments and blank lines.\n\n5 Analysis Results\n\nWe now present our comparative analysis of GPT-4o- generated and human-written code in terms of functionality (Section 5.1), security (Section 5.2), and complexity/code quality (Section 5.3).\n\n5.1 Functionality Analysis\n\n5.1.1 LeetCode Problems\n\nThe LeetCode platform provides an online submission system [19] where submitted code has to pass all the test cases – only then it is considered a correct solution. All the human-written code that we gathered from the GitHub repository [20] had to pass all the test cases from LeetCode online submission system prior to being placed in the repository. Therefore, we only tested GPT-4o-generated code through LeetCode online\n\nsubmission [19]. We found that 87.6% of LeetCode solutions generated by GPT-4o passed the LeetCode online submission checks. The 12.4% of LeetCode solutions generated by GPT- 4o that failed were due to issues such as exceeding time limit, failing test cases, and generating runtime or compile time errors. The detailed analysis results are reported in Table 2 and discussed next.\n\nFailing test cases and exceeding time limit. Among the solutions with issues, 9 (39.6%) are failing test cases and exceeding time limit. Although detailed instructions with sample inputs and outputs are given in the prompts, GPT-4o’s solutions are not always able to fulfill all the requirements stated in the prompts.\n\nRuntime errors. In addition to functionality test cases, LeetCode’s online checker compiles our submitted code with Address Sanitizer enabled, and looks for runtime errors as well, such as memory violations and other crashes. Runtime errors take up a significant 48% of the solutions with issues. There are multiple categories of runtime errors such as buffer overflow, signed integer overflow, array index out of bounds, and load of address with insufficient space. The results high- light the importance of dynamic (runtime analysis) in con- junction with static analysis: 8 out of 12 of the runtime errors were not identified during our Clang static analysis. Conse- quently, these silent, though critical errors, could easily be neglected by users and introduce severe security threats if this code were to be used in real-world programs – a scenario that is very likely to happen, as pointed out in prior research and mentioned in Section 1.\n\nCompile time errors. Compile time errors are due to incor- rect function declarations generated by GPT-4o. These issues can be mitigated by providing specific function declarations required by LeetCode in the prompts.\n\n5.1.2 Data Structures and Algorithms\n\nSection 4.2.2 discussed our strategy for writing unit test cases and seed files, used in unit testing and fuzzing, respectively. We discuss our findings next.\n\nUnit testing. All of the data structures and algorithms im- plemented by GPT-4o passed the unit testing. For human- written code, 3 out of 15 of the data structures and algorithms did not pass unit testing due to failing test cases and caus- ing segmentation faults. For example, in the implementation of doubly-linked list, the usage of == for double comparison leads to failing test cases. In the implementation of red-black trees, a missed return statement causes nullptr to be used as a buffer address, leading to null pointer dereference.\n\nFuzzing. In terms of unique hangs, GPT-4o-generated code has 50% more hangs than human-written code, whereas for unique crashes, GPT-4o generated code has 23.9% more crashes than human-written code. We were unable to perform fuzzing on certain human-written code such as queue, doubly linked list, and red black tree due to the bug mentioned above\n\n7\n\nin this section (the null pointer dereference crash caused by the missed return statement). These files are listed as N/A in Table 3. Note that these unit tests focused on functionality alone; the security aspect will be discussed in Section 5.2.\n\n5.1.3 Cryptographic Algorithms\n\nWe performed unit testing on AES, MD5, and SHA1 as dis- cussed in Section 4.2.3. Human-written code from OpenBSD generates all the correct hash values, as anticipated. GPT- 4o’s implementations of MD5 and AES compute all values correctly. GPT-4o’s SHA1 implementation computes wrong hash values for all the inputs, which could be catastrophic if this implementation were to be used in real-world situations. Functionality issues like this have a high chance of going un- noticed, unless the LLM-generated code goes through strict testing.\n\n5.2 Security Analysis\n\nWe perform static analysis on both GPT-4o-generated and human-written code using Clang static analyzer [1]. Table 4 shows the analysis results of 220 files (202 LeetCode prob- lems, 18 data structures and algorithms). Overall, GPT-4o- generated code has 10.3% more security issues than human- written code. The three categories of issues thathave relatively higher count are malloc overflow, array index out of bounds, and memory leak. We found that GPT-4o-generated code has 32.8% more malloc overflow issues than human-written code. This is a consequence of GPT-4o’s tendency of assuming all inputs are valid (Figure 3). The counts for array index out of bounds issues are similar for both GPT-4o and human. Figure 5 illustrates the similar mistakes made by both GPT- 4o and human in one of the LeetCode problems where both code may access elements that are out of bounds. Memory leak issues, where human code has a 50% higher count than GPT-4o, are mainly due to the problem description, where the LeetCode problems specifically mention that the caller will invoke free(). However, the memory leak issues do exist if the code snippet are used outside the context of these LeetCode questions.\n\nOther than issues reported by Clang, we also look at the im- plementation details between GPT-4o-generated and human written code on data structures and algorithms. We do not focus on the implementation details of LeetCode solutions because the main objective of a LeetCode problem is to pro- vide a solution that solves a very particular problem with a given set of constraints that passes all the test cases provided. We found that GPT-4o has a tendency of generating ex- act same code as human-written code. We will refer to this as “code parroting” for the rest of the paper. Note that we do not include any implementation details of human-written code in the prompts with the exception of header files or function declarations. In GPT-4o-generated code, 2 out of\n\nQuestion# 215 853 115 297 332 22 152 179 377 523 846 981\n\nError Type\n\nDifficulty Error Details\n\nTime Limit Exceeded Medium Time Limit Exceeded Medium\n\nRuntime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error\n\nHard Hard Hard\n\nsigned integer overflow buffer overflow load of address with insufficient space\n\nMedium heap-buffer-overflow Medium signed integer overflow Medium stack buffer overflow Medium signed integer overflow Medium array index out of bounds Medium load of address with insufficient space Medium AddressSanitizer: requested allocation size exceeds maximum sup-\n\n1838 208 1046 51 212 40 138 435 1930 88 1299 1899 1968\n\nRuntime Error Runtime Error Failed Test Case Failed Test Case Failed Test Case Failed Test Case Failed Test Case Failed Test Case Failed Test Case Compile Error Compile Error Compile Error Compile Error\n\nported size Medium signed integer overflow Medium heap-buffer-overflow\n\nEasy Hard Hard Medium Medium Medium Medium Easy Easy\n\nfunction definition - number of parameters invalid function definition - number of parameters invalid Medium function definition - number of parameters invalid Medium function definition - number of parameters invalid\n\nTable 2: LeetCode-reported issues in GPT-4o-generated code.\n\nthe 18 files in data structures and algorithms are the exact replica of human written code, as follows. In AVL tree’s im- plementation, GPT-4o reproduces the human-written code from GitHub, including the helper functions. Similarly, in the graph data structure implementation, GPT-4o reproduces the human-written code, but fails to add validation for input values (Figure 3). Code parroting is extremely harmful con- sidering the fact that malicious code could possibly be the training data of LLM (more commonly known as data poison- ing if malicious information is injected into training data on purpose). Harmful code will then be generated by LLM and possibly utilized by developers.\n\n5.3 Complexity Analysis\n\nSCC [25] is the tool that we utilize to measure cyclomatic complexity and lines of code (LoC, excluding comments and blank lines) per file. We calculate mean, median, geometric mean, and trimmed mean for both complexity and normalized complexity (complexity per LoC).\n\nquality code. First, low (absolute) complexity is an artifact of GPT-4o generating less code than the human equivalent for the same task. In fact, human-written code always has a lower normalized complexity, which is a more indicative metric of the efforts needed to understand and maintain a typical snippet of code in a given file. Second, the low com- plexity is due to GPT-4o generating “bare-bones” code. For instance, the GPT-4o-generated hash map implementation (Figure 6) uses a simple hash function that does not check for collisions, while the human-written implementation has a more sophisticated hash function that checks for collisions (Figure 7). This example illustrates one of the reasons why the GPT-4o-generated code has lower cyclomatic complexity (when computing complexity for the entire file, i.e., with- out normalizing for lines of code). Another example is in Figure 8 where the human-written implementation validates input values before carrying out certain operations in one of the LeetCode problems, whereas GPT-4o makes the assump- tions of all inputs are valid. This undoubtedly increases the cyclomatic complexity of human-written code but illustrates that human-written code uses defensive programming, which in turn mitigates various security threats.\n\nAs shown in Table 5, GPT-4o-generated code seems to have lower complexity on the surface. However, it would be incorrect to assume that this lower complexity indicates high-\n\n8",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-15)",
      "start_page": 9,
      "end_page": 15,
      "detection_method": "topic_boundary",
      "content": "Human\n\nGPT-4o\n\n1\n\n2\n\n// j starts from 1 for (int j=1; j<=n; j++) {\n\n1\n\n2\n\nfor (int j=1; j<=n; j++) { // j starts from 1\n\nif (p[j-1] != ’*’) {\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nif (p[j-1] == s[i-1] || p[j-1] == ’.’) {\n\ndp[i][j] = dp[i-1][j-1];\n\n} else if (p[j - 1] == ’*’) { // [j-2] could be -1 dp[i][j] = dp[i][j-2] || ((s[i - 1]\n\n== p[j-2] || p[j-2] == ’.’) && dp [i-1][j]);\n\n3\n\n4\n\n5\n\n6\n\ndp[i][j] = i > 0 && dp[i-1][j-1] && ( s[i-1] == p[j-1] || p[j-1] == ’.’ );\n\n} else {\n\n// [j-2] could be -1 dp[i][j] = (j>=2 && dp[i][j-2]) || (i >0 && dp[i-1][j] && (s[i-1] == p[ j-2] || p[j-2] == ’.’));\n\nFigure 5: Array index out of bounds issue.\n\nStack Queue Singly Linked List Doubly Linked List Binary Search Tree AVL Tree Red Black Tree Hash Map Hash Set Hash Table Graph Breadth First Search Depth Search Merge Sort Bubble Sort Total\n\nFirst\n\nUnique Hangs Unique Crashes LLM Human LLM Human\n\n3 0 0\n\n4 N/A 0\n\n0 13 65\n\n0 N/A 73\n\n1\n\nN/A\n\n59\n\nN/A\n\n0\n\n0\n\n48\n\n65\n\n0 0 0 0 0 2 3\n\n0 N/A 0 0 0 0 0\n\n58 45 16 22 19 25 27\n\n64 N/A 27 15 15 40 33\n\n1\n\n0\n\n54\n\n32\n\n1 1 12\n\n1 1 6\n\n0 0 451\n\n0 0 364\n\nCategory Memory Leak Null pointer dereference Assigned value is garbage or unde- fined Malloc overflow Taint propagation Division by zero Address of stack memory returned to caller Undefined binary operator Resultof malloc convertedto invalid type Undefined or garbage value re- turned to caller Function call argument is an unini- tialized value Array index out of bounds Implicit conversion Nested function is not supported Cast from non struct to struct C string out of bounds Total\n\nGPT-4o Human\n\n8 6 2\n\n16 8 2\n\n81 0 0 0\n\n61 1 1 1\n\n3 0\n\n3 1\n\n3\n\n1\n\n2\n\n3\n\n16 4 1 1 1 128\n\n17 1 0 0 0 116\n\nTable 3: Unique hangs and crashes from AFL fuzzing on GPT- 4o and human-written code. Green means lower in values.\n\nTable 4: Number of issues found by the Clang static analyzer on GPT-4o and human-written code.\n\n6 Feedback Loop\n\nIn terms of LoC, the results in Table 6, Figure 9, and Fig- ure 10 demonstrate that GPT-4o always produces fewer LoC than human. However, this is due to the lack of defensive pro- gramming as shown in Figures 3 and 8 and simplification of certain functionalities as shown in Figure 6 in LLM-generated code. Undoubtedly, GPT-4o-generated code being more con- cise than human-written code could serve a valuable purpose, e.g.,in introductory programming classes. That being the case, introducing security risks into real-world programs seems to be unavoidable if LLM-generated code is utilized in a profes- sional environment.\n\n6.1 Rationale and Setup\n\nState-of-the art practice suggests using a reinforcement loop (Section 2.2) to improve code quality when using LLMs. Therefore, we constructed a feedback loop as a way to im- prove GPT-4o-generated code, starting by trying to eliminate the security issues identified in Section 5.2. For this experi- ment, we included 60 LeetCode files (solutions) generated by GPT-4o, as follows: 30 files with security issues (found by Clang), and 30 files without security issues. The list of secu- rity issues is listed in Table 7. We ask GPT-4o to regenerate\n\n9\n\nComplexity/Code (D&A) Mn Md GM TM Mn Md GM TM Mn Md GM TM Mn Md GM TM 0.21 0.16\n\nComplexity (LC)\n\nComplexity/Code (LC)\n\nComplexity (D&A)\n\nGPT-4o 7.37 Human 8.10\n\n0.21 0.17\n\n18.8 25.1\n\n16.4 21.2\n\n14 23\n\n25.7 34.5\n\n0.31 0.27\n\n0.23 0.16\n\n0.29 0.25\n\n0.33 0.28\n\n7.07 7.44\n\n6.03 6.19\n\n7 7\n\n0.21 0.16\n\n0.18 0.15\n\nTable 5: Cyclomatic complexity measurements. LC is LeetCode (202 files), D&A is data structures and algorithms (18 files), Mn is mean, Md is median, GM is geometric mean, and TM is trimmed mean. We use the same abbreviations for the rest of the paper. Trimmed mean is at 5% for LeetCode and 15% for data structures and algorithms.\n\nLines of Code (LC)\n\nLines of Code (D&A)\n\nGPT-4o Human\n\nMn Md GM TM Mn Md GM TM 99.1 23.6 172.8 29.6\n\n20 24\n\n21.2 25.6\n\n22.2 27.2\n\n103.5 192.1\n\n83 149\n\n88.7 142\n\nTable 6: Lines of Code measurements. Trimmed mean is at 5% for LeetCode and 10% for data structures and algorithms.\n\n1\n\nunsigned long hashFunction(const char *key) {\n\n1\n\nint hashmap_hash(map_t in, char* key){\n\n2\n\n3\n\nunsigned long hash = 0; while (*key) {\n\n2\n\n3\n\nint curr; int i;\n\n4\n\nhash = (hash << 5) + *key++;\n\n4\n\n5\n\n6\n\n} return hash;\n\n5\n\n6\n\n/* Cast the hashmap */ hashmap_map* m = (hashmap_map *) in;\n\n7\n\n}\n\n7\n\nFigure 6: GPT-4o implementation of hashing in Hash Map.\n\n8\n\n9\n\n10\n\n/* If full, return immediately */ if(m->size >= (m->table_size/2)) return MAP_FULL;\n\n11\n\nthese files based on different security categories. In each loop, prompts for files with security issues will be accompanied by prompts for files that have no security issues. For instance, we specifically ask GPT-4o to fix only malloc overflow issues in this loop with a total a 18 prompts related to files that have malloc overflow issues along with another 18 prompts related to files that have no malloc overflow issues. Note that the prompts (LeetCode problem descriptions) used in the loops are the same as the non-feedback setup, but with additional instructions that ask GPT-4o to make sure there are no certain security issues in the code. Since there are 3 categories of security issues, a total of 3 loops will be conducted, each loop to fix a specific security issue.\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n}\n\n/* Find the best index */ curr = hashmap_hash_int(m, key);\n\n/* Linear probing */ for(i = 0; i< MAX_CHAIN_LENGTH; i++){ if(m->data[curr].in_use == 0)\n\nreturn curr;\n\nif(m->data[curr].in_use==1 && (strcmp (m->data[curr].key,key)==0)) return curr;\n\ncurr = (curr + 1) % m->table_size;\n\n} return MAP_FULL;\n\nFigure 7: Human implementation of hashing in Hash Map.\n\n6.2 Results\n\nFiles with security issues. For files that did contain security issues, GPT-4o manages to lower the number of security is- sues found after one iteration on malloc overflow and null dereference from 40 to 24 and 3 to 2, respectively, as shown in Table 8. However, the security issues are not completely removed, even though we specifically asked GPT-4o to ensure the code will not have these security issues. Furthermore, for array index out of bounds issues, GPT-4o generated 3 more such issues. This is a critical finding since it shows the possi- bility that LLM may generate code that is the exact opposite of what users have specified in the prompt.\n\nFiles without security issues. For files without any se- curity issue before entering the loop, GPT-4o introduces 4 new malloc overflow issues as shown in Table 9. Figure 11 illustrates one of the code examples (before and after running the loop), where the new code has a malloc overflow issue. Malloc overflows account for 63.3% of the total security is- sues found across the code generated by GPT-4o. However, GPT-4o-generated code, on the right side of Figure 11, still does not check the value of k and the result of malloc after an effort has been made in the prompt to ask LLM to make no such mistakes. This outcome is especially troublesome, since\n\n10\n\nHuman\n\nGPT-4o\n\n1\n\nvoid timeMapSet(TimeMap* obj, char* key, char\n\n1\n\nvoid timeMapSet(TimeMap* obj, char* key, char\n\nvalue, int timestamp) {\n\nvalue, int timestamp) {\n\n2\n\n2\n\n3\n\n4\n\nif (obj == NULL || key == NULL || value\n\n== NULL) return;\n\n3\n\n4\n\n// no input verifications for (int i = 0; i < obj->node_count; i++)\n\n{\n\n5\n\n5\n\nobj->nodes[i].entries[obj->nodes[i].\n\n6\n\n7\n\nint index = -1; for (int i = 0; i < obj->size; i++) {\n\nentry_count].timestamp = timestamp;\n\nFigure 8: GPT-4o-generated code (right) does not verify inputs.\n\n# of files\n\nM A N Total 18\n\n10\n\n2\n\n30\n\nTable 7: Number of files used in loop testing based on security categories. M is malloc overflow, A is array index out of bounds, N is null dereference. We use the same abbreviations for the rest of the paper.\n\nBefore After\n\nM A N Total 40 24\n\n15 18\n\n3 2\n\n58 44\n\nTable 8: Number of security issues found before and after running the loop on files with security issues before based on security categories. Red means an increase in issues found.\n\nFigure 9: LoC per file in data structures and algorithms.\n\ntime error, i.e., exceeding memory limit when k = 1000000000 (exposed by LeetCode’s online submission system). As Leet- Code runtime checker caught this bug as well, it validates the findings from Clang analyzer.\n\n6.3 Why can’t Prompt Engineering Fix Secu-\n\nrity Issues?\n\nFigure 10: LoC per file in LeetCode.\n\nthe previously-produced solutions contained no such issues. Note that the particular solution on the left in Figure 11 had no prior issues found during static analysis but did have a run- time error (array index out of bounds) when submitted to the LeetCode online submission platform [19] before running the feedback loop. However, the regenerated solution, supposedly the “better” solution, on the right in Figure 11, still has a run-\n\nThe results from the feedback loop allow us to draw a conclu- sion that LLM so far is not capable of consistently removing security issues even though the prompt has explicitly asked it to do so. LLM might even go against the prompt in producing more security issues in the code. First of all, these security issues are found by running static analysis. Users who utilize LLM for code generation might not even notice that the code poses serious security threats without running static analy- sis. Furthermore, even static analysis is not enough: some of the runtime errors found by LeetCode’s online submis- sion system [19] were not identified during static analysis (Section 5.1.1). To conclude, even for users who are aware of potential issues and diligent in attempting to reduce or eliminate security risks in the code, the LLM route might be unfruitful: LLM regeneration may worsen the security problem in the generated code, e.g., as shown in Figure 11.\n\n11\n\nBefore\n\nAfter\n\n1\n\nbool checkSubarraySum(int* nums, int numsSize\n\n1\n\nbool checkSubarraySum(int* nums, int numsSize\n\n2\n\n3\n\n, int k) { if (numsSize < 2) return false;\n\n2\n\n3\n\n, int k) { // not checking k before malloc int *remToIndex = (int *)malloc(k *\n\n4\n\nsizeof(int));\n\n5\n\n6\n\n7\n\nint sum = 0; int remainder; int remainder_map[numsSize];\n\n4\n\n5\n\n6\n\n// not checking result of malloc for (int i = 0; i < k; i++) {\n\nremToIndex[i] = -1;\n\n8\n\n9\n\n10\n\nfor (int i = 0; i < numsSize; i++) {\n\nremainder_map[i] = -1;\n\n7\n\n8\n\n9\n\n} remToIndex[0] = 0; int cumulativeSum = 0;\n\n11\n\n}\n\n10\n\n12\n\n11\n\nfor (int i = 0; i < numsSize; i++) {\n\n13\n\n14\n\n15\n\nremainder_map[0] = 0;\n\nfor (int i = 0; i < numsSize; i++) {\n\n12\n\n13\n\n14\n\ncumulativeSum += nums[i]; int remainder = cumulativeSum % k; if (remainder < 0) remainder += k;\n\n16\n\n17\n\nsum += nums[i]; remainder = sum % k;\n\n15\n\n16\n\nif (remToIndex[remainder] != -1) {\n\n18\n\n17\n\nif (i + 1 - remToIndex[remainder]\n\n19\n\nif (remainder < 0) remainder += k;\n\n>= 2) {\n\n20\n\n21\n\nif (remainder_map[remainder] < 0) {\n\n18\n\n19\n\nfree(remToIndex); return true;\n\n22\n\n23\n\nremainder_map[remainder] = i + 1;\n\n} else if (i - remainder_map[\n\n20\n\n21\n\n} } else {\n\n24\n\nremainder] > 0) { return true;\n\n22\n\n23\n\n}\n\nremToIndex[remainder] = i + 1;\n\n25\n\n26\n\n27\n\n}\n\n} return false;\n\n24\n\n25\n\n26\n\n} free(remToIndex); return false;\n\n28\n\n}\n\n27\n\n}\n\nFigure 11: LeetCode solutions for problem number 523 generated by GPT-4o before and after running a loop.\n\nBefore After\n\nM A N Total 0 0 0 4\n\n0 0\n\n0 4\n\nTable 9: Number of security issues found before and after running the loop on files without security issues before based on security categories.\n\n7 Related Work\n\ning could be extremely dangerous when malicious code is injected into the training set (poisoning attack). In fact, nu- merous studies have shown that code completion tools that utilize LLM are vulnerable to poisoning attack such as sug- gesting insecure code in AES implementation [45]. A tool, CodeBreaker [50], was designed for back-door attacks with the help of LLM where poisoned data for code generation can avoid strong vulnerability detection. These works support our point that LLM-generated code is insecure, requiring further analyses and improvements.\n\n7.1 AI-based Code Generation Security\n\nSimilar studies have been done on the security aspect of LLM code generation. Perry et al. [41] conducted a user study on two groups of participants: one without and the other with, access to an AI assistant. Participants were asked to solve a few specific tasks using multiple programming languages. Their results show that participants with access to the AI assistant wrote significantly less secure code, which aligns with our findings. We share a similar goal but take a different approach to addressing the AI assistant-introduced issues, as the comparisons in our study are between LLM and human in solving algorithmic and data structure tasks.\n\n7.2 Benchmarks for LLM-Generated Code\n\nPurple Llama CyberSecEval [28] and Cybench [53] are bench- marks designed to tackle the security issues arising from LLM-generated code. CyberSecEval selects insecure code tests from real-world open source codebases whereas Cy- bench gathers 40 Capture the Flag (CTF) tasks for evaluat- ing LLM-generated code. These benchmarks are orthogonal to our purpose in evaluating an LLM’s coding capabilities. EXACT is a comprehensive evaluation framework that can in- corporate these benchmarks as a part of its testing if needed.\n\nWe discuss code parroting in Section 5.2. Code parrot-\n\n12\n\n7.3 Code Improvement in LLM\n\nAs we discussed in Section 2.2, RL has been employed by LLMs to improve code generation. Due to the implementation of these blackbox components, we would not know for sure if the improvements have been made in the security aspect of code generation. However, our study has shown that LLM- generated code produces a significant amount of security risks. Prompt-based learning [33] is an approach where LLMs are guided to generate desired output with carefully designed prompts without re-training the models. This technique could be applied on code generation as well. For instance, whenever malloc appears in code generation, it should come with an allocation size check before using malloc and a NULL check for the result of malloc. This would be one of the desired outputs to drastically reduce malloc overflow issues. However, based on the results we have seen so farin ourstudy,LLM-generated code still has a long way to go to achieve this level of security.\n\n7.4 Privacy and Security in AI\n\nCode generation is not the only domain that raises privacy and security issues regarding LLMs. Client-side prompt san- itization [29,46,52] is designed to protect private informa- tion shared with LLMs without affecting the performance of LLMs. Federated learning [38] can be used to train Machine Language models without sharing raw data that could be privacy-sensitive. Prompt injection is an attack where LLMs could generate harmful content due to malicious prompts; benchmarks [34] and fine-tuned models [42] can defend against such attacks. Our study shows the importance of scru- tinizing code security as well.\n\n8 Discussion\n\n8.1 Multiple LLMs\n\nOur study focuses on a single LLM, GPT-4o. We could extend our study by conducting the comparisons using the same human-written code with otherLLMs such as Llama3,Claude, etc.; which would reveal differences between different LLMs in terms of code security. However, the focal point of our study is to bring attention to the differences between LLM- generated and human-written code. We believe that GPT-4o, being the state-of-the-art of OpenAI’s LLM, has successfully revealed the security threats brought upon by LLMs-based code generation; the issues we found are critical, yet likely overlooked when programming with the help of LLMs.\n\n8.2 Coding Tasks\n\nThe coding tasks in our experiments are relatively small and independent as opposed to the code in large software projects such as an operating system kernel, a browser etc. Comparing\n\n13\n\ncode implemented for a subsystem can be an interesting fu- ture work since it requires more attention to interoperability, as these software has a plethora of components within them- selves. Another possible future work in terms of coding tasks could be asking LLM to fix known errors or identify possible issues in human-written code.\n\n9 Conclusions\n\nAs AI-assisted code writing is spreading, it is imperative to study its impact and consequences on software security. We developed a methodology and framework, EXACT, for com- paring the security and quality of LLM-generated and human- written code for the same task. We expose and quantify the disadvantages of naïvely using LLM-generated code in a critical domain such as security: the LLM-generated code might violate the expected functionality in subtle ways, or might contain security issues that do not manifest until late in program execution, and can be exploited by adversaries. In contrast, human code for equivalent tasks tends to contain guardrails and defensive constructs. We also show the poten- tial perils of using a feedback loop when programming with the help of LLMs. Our work sheds light on the importance of scrutinizing LLM-generated code (and code re-generated by prompting) for functionality, security, and quality issues. As long as LLM-generated code is prone to security risks it should be considered potentially harmful.\n\nAcknowledgments\n\nThis research was supported in part by the National Science Foundation grant CCF-2106710. The authors also thank Ope- nAI for donating API credits to this project.\n\nReferences\n\n[1] Clang: a C Language Family Frontend for LLVM. http:\n\n//clang.llvm.org/.\n\n[2] american fuzzy lop. https://github.com/google/\n\nAFL, 2021.\n\n[3] Research: lot’s and 2022-09-07-research-quantifying-github -copilots-impact-on-developer-productivity -and-happiness, 2022.\n\nquantifying\n\nCopi- developer productivity https://github.blog/\n\nGitHub\n\nimpact happiness.\n\non\n\n[4] Executive Order on the Safe, Secure, and Trust- In- worthy Development and Use of Artificial https://www.whitehouse.gov/ telligence. briefing-room/presidential-actions/2023/10/ 30/executive-order-on-the-safe-secure-and-\n\ntrustworthy-development-and-use-of -artificial-intelligence/, 2023.\n\n[5] GitHub CEO says Copilot will write 80% of code “sooner than later”. https://www.freethink.com/ robots-ai/github-copilot, 2023.\n\n[6] GitHub\n\nCopilot\n\nhas now new capabilities.\n\na\n\nbetter AI https://\n\nmodel github.blog/ai-and-ml/github-copilot/ github-copilot-now-has-a-better-ai-model -and-new-capabilities, 2023.\n\nand\n\n[7] AI Coding Assistant - Amazon Q Developer. https:\n\n//aws.amazon.com/q/developer, 2024.\n\n[8] Anthropic Claude 3.5 Sonnet.\n\nhttps://www.\n\nanthropic.com/news/claude-3-5-sonnet, 2024.\n\n[9] CUnit.\n\nhttps://cunit.sourceforge.net/index.\n\nhtml, 2024.\n\n[10] EXECUTIVE\n\nORDER\n\nON\n\nSAFE, SE- ARTIFI- https:\n\nCURE, AND CIAL INTELLIGENCE | NIST. //www.nist.gov/artificial-intelligence/ executive-order-safe-secure-and-trustworthy -artificial-intelligence, 2024.\n\nTRUSTWORTHY\n\n[11] fragglet/c-algorithms.\n\nhttps://github.com/\n\nfragglet/c-algorithms, 2024.\n\n[12] Gemini Code Assist. https://cloud.google.com/\n\nproducts/gemini/code-assist, 2024.\n\n[13] GitHub Copilot - Your AI pair programmer. https:\n\n//github.com/features/copilot, 2024.\n\n[14] GitHub Copilot Efficiency Explored: Key Take- https://akvelon.\n\naways from Akvelon’s Survey. com/github-copilot-efficiency-survey-data- revealed-akvelon/, 2024.\n\n[15] GitHub 4o. 2024-07-05-github-copilot-enterprise-on-gpt-4o, 2024.\n\nCopilot\n\nGPT- https://github.blog/changelog/\n\nEnterprise\n\non\n\n[16] GitHub Copilot Voice - Write code without\n\nthe https://githubnext.com/projects/\n\nkeyboard. copilot-voice, 2024.\n\n[17] Hello GPT-4o.\n\nhttps://openai.com/index/\n\nhello-gpt-4o/, 2024.\n\n[18] Helping our customers through the CrowdStrike out- age. https://blogs.microsoft.com/blog/2024/ 07/20/helping-our-customers-through-the- crowdstrike-outage, 2024.\n\n14\n\n[19] LeetCode. 2024.\n\nhttps://leetcode.com/problemset/,\n\n[20] neetcode_gh/leetcode.\n\nhttps://github.com/\n\nneetcode-gh/leetcode, 2024.\n\n[21] NetBSD Commit Guidelines. https://www.netbsd.\n\norg/developers/commit-guidelines.html, 2024.\n\n[22] NSRL Test Data. https://www.nist.gov/itl/ssd/ software-quality-group/nsrl-test-data, 2024.\n\n[23] OpenBSD. 2024.\n\nhttps://github.com/openbsd/src,\n\n[24] petewarden/c_hashmap.\n\nhttps://github.com/\n\npetewarden/c_hashmap, 2024.\n\n[25] SCC. https://github.com/boyter/scc, 2024.\n\n[26] Survey reveals AI’s impact on the developer experience. https://github.blog/news-insights/research/ survey-reveals-ais-impact-on-the-developer -experience, 2024.\n\n[27] The Algorithms/C.\n\nhttps://github.com/\n\nTheAlgorithms/C, 2024.\n\n[28] M. Bhatt, S Chennabasappa, C. Nikolaidis, S Wan, I Ev- timov, D. Gabi, D. Song, F. Ahmad, C. Aschermann, L. Fontana, S. Frolov, R. P. Giri, D. Kapil, Y Kozyrakis, D. LeBlanc, J. Milazzo, A. Straumann, G. Synnaeve, V. Vontimitta, S. Whitman, and J. Saxe. Purple llama cyberseceval: A secure coding benchmark for language models, 2023.\n\n[29] C. J. Chong, C. Hou, Z. Yao, and S. M. Seyed Talebi. Casper: Prompt sanitization for protecting user privacy in web-based large language models, 2024.\n\n[30] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In Proc. NIPS, 2017.\n\n[31] M. Dilhara, A. Bellur, T. Bryksin, and D. Dig. Unprece- dented code change automation: The fusion of llms and transformation by example. Proc. ACM Softw. Eng., 1(FSE), jul 2024.\n\n[32] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Re- inforcement learning: A survey. Journal of artificial intelligence research, 4:237–285, 1996.\n\n[33] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neu- big. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021.\n\n[34] Y. Liu, Y. Jia, R. Geng, J. Jia, and N. Z. Gong. For- malizing and benchmarking prompt injection attacks In 33rd USENIX Security Symposium and defenses. (USENIX Security 24), pages 1831–1847, Philadelphia, PA, August 2024. USENIX Association.\n\n[35] V. Liventsev, A. Grishina, A. Härmä, and L. Moonen. Fully autonomous programming with large language models. In Proc. ACM GECCO, 2023.\n\n[36] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024.\n\n[37] Thomas J. McCabe. A complexity measure. In ICSE\n\n’76, page 407, 1976.\n\n[38] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of In Artificial deep networks from decentralized data. intelligence and statistics, pages 1273–1282. PMLR, 2017.\n\n[39] R. Pan, A. R. Ibrahimzada, R. Krishna, D. Sankar, L. P. Wassi, M. Merler, B. Sobolev, R. Pavuluri, S. Sinha, and R. Jabbarvand. Lost in translation: A study of bugs introduced by large language models while translating code. In Proc. IEEE/ACM ICSE, 2024.\n\n[40] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri. Asleep at the keyboard? assessing the se- curity of github copilot’s code contributions. In Proc. IEEE Symposium on Security and Privacy (S&P), 2022.\n\n[41] N. Perry, M. Srivastava, D. Kumar, and D. Boneh. Do users write more insecure code with ai assistants? In Proc. ACM CCS, 2023.\n\n[42] J. Piet, M. Alrashed, C. Sitawarin, S. Chen, Z. Wei, Jatmo: Prompt\n\nE. Sun, B Alomair, and D. Wagner. injection defense by task-specific finetuning, 2024.\n\n[43] V. Raychev, M. Vechev, and E. Yahav. Code completion with statistical language models. In Proc. PLDI, 2014.\n\n[44] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Ba- log, M. P. Kumar, E. Dupont, F. J. R. Ruiz, J. S. Ellen- berg, P. Wang, O. Fawzi, et al. Mathematical discoveries\n\n15\n\nfrom program search with large language models. Na- ture, 625(7995):468–475, 2024.\n\n[45] R. Schuster, C Song, E Tromer, and V Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21), pages 1559–1575. USENIX As- sociation, August 2021.\n\n[46] Z. Shen, Z. Xi, Y. He, W. Tong, J. Hua, and S. Zhong. The fire thief is also the keeper: Balancing usability and privacy in prompts. arXiv preprint arXiv:2406.14318, 2024.\n\n[47] R. S. Sutton and A. G. Barto. Reinforcement learning:\n\nan introduction. MIT Press, 2018.\n\n[48] P. Vaithilingam, T. Zhang, and E. L. Glassman. Expec- tation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Proc. ACM CHI, 2022.\n\n[49] R. Wang, R. Cheng, D. Ford, and T. Zimmermann. In- vestigating and designing for trust in ai-powered code In The 2024 ACM Conference on generation tools. Fairness,Accountability,and Transparency,pages 1475– 1493, 2024.\n\n[50] S. Yan, S Wang, Y Duan, H Hong, K. Lee, D. Kim, and Y. Hong. An llm-assisted easy-to-trigger backdoor attack on code completion models: Injecting disguised vulnerabilities against strong detection, 2024.\n\n[51] X. Yin, C. Ni, T. N. Nguyen, S. Wang, and X. Yang. Rectifier: Code translation with corrector via llms. arXiv preprint arXiv:2407.07472, 2024.\n\n[52] X. Yue, M. Du, T. Wang, Y. Li, H. Sun, and S. S. M. Chow. Differential privacy for text analytics via natural In Findings of the Association for text sanitization. Computational Linguistics: ACL-IJCNLP 2021, pages 3853–3866, 2021.\n\n[53] A. K. Zhang, N. Perry, R. Dulepet, E. Jones, J. W. Lin, J. Ji, C. Menders, G. Hussein, S. Liu, D. Jasper, et al. Cybench: A framework for evaluating cybersecurity ca- pabilities and risk of language models. arXiv preprint arXiv:2408.08926, 2024.",
      "page_number": 9
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "4 2 0 2\n\np e S 7 2\n\n]\n\nR C . s c [\n\n1 v 2 8 1 9 1 . 9 0 4 2 : v i X r a\n\nArtificial-Intelligence Generated Code Considered Harmful: A Road Map for Secure and High-Quality Code Generation\n\nChun Jie Chong, Zhihao (Zephyr) Yao, Iulian Neamtiu {cc255, zhihao.yao, ineamtiu@njit.edu} New Jersey Institute of Technology\n\nAbstract Generating code via a LLM (rather than writing code from scratch), has exploded in popularity. However, the security implications of LLM-generated code are still unknown. We performed a study that compared the security and quality of human-written code with that of LLM-generated code, for a wide range of programming tasks, including data structures, algorithms, cryptographic routines, and LeetCode questions. To assess code security we used unit testing, fuzzing, and static analysis. For code quality, we focused on complexity and size. We found that LLM can generate incorrect code that fails to implement the required functionality, especially for more complicated tasks; such errors can be subtle. For example, for the cryptographic algorithm SHA1, LLM gener- ated an incorrect implementation that nevertheless compiles. In cases where its functionality was correct, we found that LLM-generated code is less secure, primarily due to the lack of defensive programming constructs, which invites a host of security issues such as buffer overflows or integer over- flows. Fuzzing has revealed that LLM-generated code is more prone to hangs and crashes than human-written code. Quality- wise,we found that LLM generates bare-bones code that lacks defensive programming constructs, and is typically more com- plex (per line of code) compared to human-written code. Next, we constructed a feedback loop that asked the LLM to re- generate the code and eliminate the found issues (e.g., malloc overflow, array index out of bounds, null dereferences). We found that the LLM fails to eliminate such issues consistently: while succeeding in some cases, we found instances where the re-generated, supposedly more secure code, contains new issues; we also found that upon prompting, LLM can intro- duce issues in files that were issues-free before prompting. Our study exposes the perils of LLM-generated code (and feedback loops), particularly in the critical security domain.\n\n1 Introduction\n\nSoftware security is of utmost importance in software engi- neering, as it directly affects the security and reliability of a\n\ndigital society increasingly dependent on software. For ex- ample, the 2024 CrowdStrike bug that crashed 8.5 million Microsoft Windows devices [18] highlights the worldwide impact of software bugs. Human experts have been trained to write, review, and test code to ensure its quality, despite the fact that the process is time-consuming and error-prone. However, the security and quality of Artificial Intelligence (AI)-generated code is an under-studied area. With the ad- vance of AI, the shift towards AI-assisted programming is rapidly gaining momentum, making the concerns more ur- gent.\n\nLarge Language Models (LLMs) are already widely used to assist developers in code completion and summarization [7,12,13], and in some cases, to automatically generate code from scratch to meet the requirements of specific tasks [44]. For example, GitHub Copilot [13] (“Copilot” for short), a widely-adopted AI coding assistant, has been available since 2022 [13]. While users report an improvement in productivity (81% and 88%, respectively reported by two independent user studies [3, 14]), an empirical investigation has shown that 40% of Copilot-generated programs are buggy [40]. The false sense of productivity when using LLM code generation in the workplace has mostly been driven by developers aiming to fulfill industry’s internal performance metrics, such as task completion time and lines of code produced [26], rather than code quality.\n\nThe security implications of AI code generation are largely ignored by the industry: Copilot Voice has been introduced as a new feature that allows inexperienced developers to generate full programs by simply speaking to the AI assistant, mar- keted as a new way to “write code without the keyboard” [16]. Naïve trust in AI code generation can lead to significant se- curity vulnerabilities and degradation in code quality. Under- standing the traits and limitations of the LLM-generated code is important to facilitate the adoption of LLMs in future soft- ware engineering practice. In October 2023, the White House issues an Executive Order on “Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence”, which em- phasize the importance of security in such AI use cases [4,10].\n\n1",
      "content_length": 4684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 2,
      "content": "Motivated by the national goal, this work aims to study the security and quality of code generated by the state-of-the- art LLM model, OpenAI’s GPT-4o, which powers GitHub Copilot Enterprise [15]. Specifically, we create an evaluation framework, EXACT ( EXamination System for AI-Generated Code Testing), which uses a suite of program analyses, unit testing, fuzzing, complexity and size measurements, to assess code security and quality. EXACT compares these factors with the human-written code that implements the same functional- ity. We focus on three classes of representative coding tasks: over 200 LeetCode programming tests, fifteen commonly- used algorithms and data structure implementations, as well as popular cryptographic functions. For each of these tasks, we compare the security and quality of the human-written and LLM-generated code. We chose the C programming language due to its prevalence in systems and security-sensitive code. For each comparison experiment, EXACT checks function- alities of LLM-generated and human-written code using an empirical method: if a predefined test suite (e.g., LeetCode online submission [19]) is available, we submit both programs to the test suite; otherwise, we use unit testing and AFL [2] to create test cases and compare the runtime behavior of the two programs. Note that fuzzing findings are used to assess both security and functionality. A recent study [53] demon- strates that LLM is not capable of solving complicated tasks (tasks that take a human more than 11 minutes). We observe a similar trait in our study: among the 21 LeetCode tasks that LLM failed to solve, 20 tasks are rated at medium or hard difficulty. LLM also implements the SHA-1 hash func- tion incorrectly, where it generates the wrong hash values for all the inputs. We also demonstrate the unreliability of both the AI-generated and human-written code in the presence of fuzzing, as they both fail to handle corner cases in our test suites (though human code performs slightly better).\n\nWe also found that the security and quality of LLM- generated code is lacking, compared to its human-written equivalent. The security issues found by Clang static ana- lyzer in LLM-generated code are consistently higher than human-written code: by 11.2% in LeetCode and 7.1% in algo- rithm tasks. Additionally, LLM-generated code is 1.19x more complex than human-written code in LeetCode and 1.26x in algorithm tasks.\n\nA feedback loop is a state-of-the-art practice carried out by various research groups [32, 36, 47] to improve LLM’s code generation. Therefore, our study also investigates the effectiveness of a feedback loop in improving the security and correctness of the generated code by iteratively feeding the results of the evaluation back to the LLM model. We show that when we asked the model to fix the discovered security issues, it can introduce new bugs in code that was previously bug-free; moreover, the re-generated version has higher complexity per line. We also observe that by request- ing shorter line lengths, GPT-4o yields subsequent code with 11.3% lower cyclomatic complexity per line, and surprisingly,\n\n2\n\nit also fixed 17.9% security issues found by Clang static ana- lyzer. In contrast, by requesting fewer lines of code, GPT-4o yields code with 18.4% higher complexity per line, and fixed 43.2% of security issues.\n\nDuring the course of our study, we have identified 116 se- curity issues in public GitHub repositories, which we have responsibly disclosed to the repository owners. For the soft- ware security issues of the code generated by OpenAI’s GPT- 4o model, we have reached out to OpenAI for their further improvement. We will open source EXACT and our test data. We discuss the full details in the Ethics and Open Science Compliance appendix.\n\nThe main contributions of our study are:\n\nWe show that, for the same task, LLM generates less- secure and lower-quality code than the human equivalent\n\nWe characterize the substantial security issues in LLM- generated code\n\nWe demonstrate that prompt engineering with a feedback loop does not necessarily improve the security of LLM- generated code.\n\n2 Background\n\n2.1 AI Code Generation\n\nLLMs have been widely adopted for natural language tasks, and also for code generation. This adoption is easily justifi- able as LLMs have shown exceptional performance in code completion [43], translation [51], and full project code gen- eration [35]. With the introduction of large-scale pre-trained LLMs, such as OpenAI’s GPT-4o that powers GitHub Copi- lot Enterprise [15], the performance of AI code generation has been further advanced to a degree that it is presumed to replace human programmers [5].\n\nThe inception of AI code generation has brought about both a revolution and a debate in the software community. On one hand, AI code improves developers’ productivity by automating coding tasks, especially the repetitive and mun- dane ones [31]. However, the security implications of fully autonomous programming using LLMs [35] and automation of repetitive coding tasks [31] that have been proposed by recent research are not well understood.\n\nIn a Copilot study, 88% of human developers have reported that they are more productive when coding with the tool [3]. The drastic improvement in productivity demonstrates the potential of AI in software engineering. Indeed, the CEO of GitHub predicts that “sooner than later,” “Copilot will write 80% of code” [5]. But, unfortunately, existing AI coding assistants have been shown to write incompetent code [40, 41, 49]. According to a study conducted by GitHub, 92% of the surveyed developers have used AI coding tools, and 70% of them believe that the tools offered an advantage in",
      "content_length": 5735,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "1\n\nvoid setBuffer(char *buffer) {\n\n2\n\nfor (int i = 0; i < 26; i++)\n\n3\n\nbuffer[i] = ’\\0’;\n\n4\n\n5\n\n6\n\n}\n\nint main() {\n\nGPT-4o fills in an incorrect value. Intentionally missed letter ‘G’.\n\n7\n\nchar alphabet[] = \"ABCDEFH\n\nIJKLMNOPQRSTUVWXYZ\";\n\n8\n\n9\n\nint length = 0; while (alphabet[length] != ’\\0’)\n\n10\n\nlength++;\n\n11\n\nchar *buffer =\n\n12\n\n(char *)malloc(length*sizeof(char));\n\n13\n\n14\n\n15\n\nsetBuffer(buffer); free(buffer); return 0;\n\n16\n\n}\n\nFigure 1: Sample task 1 (§3.1): we prompt GPT-4o to find the size of the buffer on line 7 and use it on line 2 for the loop condition (in the red circle). GPT-4o produces an incorrect answer, leading to heap overflow.\n\ntheir work [26]. One impetus for AI code generation is to improve companies’ internal performance metrics, such as “time to complete a task”, or “lines of code written” [26]. The adoption of AI-generated code has raised both ethics and quality concerns.\n\nRecent studies have shown significant degradation in code quality [39–41]. NetBSD, a popular open-source operating system project,referredto AI-generatedcode as “taintedcode” and banned it from their codebase [21]. We present an illus- tration of GPT-4o-generated code buffer size in Figure 1 as an example of the quality concerns. We discuss this example in depth in Section 3.1.\n\n2.2 Automated Code Improvement\n\nLLMs have been using Reinforcement learning (RL) for auto- mated code improvement. RL updates a model’s parameters through interactions with human feedback or the environment, and has shown to be effective in improving various AI models’ performance [32,47]. Reinforcement learning from human feedback (RLHF) uses human feedback to fine tune a model’s parameters [30]. Despite the fact that RLHF can be used to improve the performance of LLM code generation, scalable deployment in practice has been limited by the capability of human evaluators to provide meaningful feedback to the out- puts [36]. To cope with the lack of human feedback, OpenAI has proposed a fine-tuned model based on GPT 4, namely CriticGPT, to automatically critique the LLM-generated code, where the feedback is used to improve the subsequent gener- ations [36]. A pool of bugs and human feedback originated from OpenAI’s previous RLHF pipeline is used as the training set of the fine-tuning of CriticGPT [36]. Likewise, GitHub Copilot has proposed a secondary LLM model to “approxi-\n\n3\n\nmate the behavior of static analysis tools” to provide feedback to the outputs of the primary LLM model [6]. Since these systems have been integrated into the LLM pipeline, our work aims to investigate the security of the overall system that may include these blackbox components.\n\n2.3\n\nImplications of AI Code Generation\n\nNaïve trust in AI code-generation tools can lead to deterio- rated code quality and software security hazards. Due to the lack of metrics to evaluate the trustworthiness of generated code [49], existing quantitative evaluation of AI-generated code is limited. A user study conducted in 2023 has found significant degradation in code quality and a false sense of security when programmers use AI assistants [41]. Likewise, Pearce et al. conducted an empirical study on 1,689 Copilot- generated programs and found that 40% of the programs are vulnerable [40]. Another user study in 2024 has shown that a majority of GitHub Copilot users felt that the tool’s sugges- tions were not always accurate, as the tool “may give false code suggestions that mislead developers” [14]. Fixing the errors in the generated code is a challenging task for most users because they did not author the code themselves. Con- sequently, when AI-generated code appears to be complex from the user’s perspective, users often give up on fixing the code and resort to other online resources [48]. Indeed, strin- gent testing shows that, currently, even the most advanced LLM models, such as Claude 3.5 Sonnet [8] and GPT-4o [17], are only able to solve basic, straightforward tasks, that take human developers at most 11 minutes to solve [53].\n\n3 Motivation\n\n3.1 Sample Task 1: Finding a Buffer’s Size\n\nAs motivation for our study, we present a preliminary exper- iment we conducted on OpenAI GPT-4o, where we found that the model fails a basic, yet critical, security task: finding the size of a buffer, even though the buffer size is given in the prompt in a way that a human can easily understand. The incorrect buffer size can lead to buffer overflow if it is larger, or leaking non-initialized memory if the buffer size is smaller than the actual buffer size in the buffer initialization func- tion. We present an example of wrong buffer size in Figure 1, where the buffer size is filled in as 26, but the actual buffer size should be 25 (as the length of an incomplete alphabet with the letter ‘G’ missing is 25). In our experiment, we asked GPT-4o to fill in the buffer size in the loop condition (the red circle in Figure 1) with various buffer size definitions in the referenced code in our prompt. Table 1 shows the buffer size definitions, examples, and the success rate of GPT-4o in filling in the correct buffer size. We repeat experiments for each buffer size definition 1000 times. We found that GPT-4o",
      "content_length": 5198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "not only fails to fill in the correct buffer size when calculation is involved, but also fails to fill in the constant buffer size in some cases.\n\nFigure 2: GPT-4o filled buffer size (the red circle in Figure 1) over 1000 trials.\n\nAs shown in the Table 1, GPT-4o has a high success rate (but not 100%) in filling in the buffer size when the buffer size is a constant integer. When the buffer size requires calculation, the success rate drops significantly. The success rate is the lowest when the buffer size is calculated using floating-point numbers, possibly due to the model’s lack of understanding of float to integer casting. To our surprise, GPT-4o’s success rate drops from 92.8% to 41.6% when the buffer size is calculated from the length of alphabet with a missing letter, showing that the model concludes the buffer size based on the pattern of the alphabet, rather than the actual length of the string. Figure 2 shows the distribution of the buffer size filled in by GPT-4o in the missing letter example over 1000 trials. We were also surprised that when we give the model a constant integer buffer size, but at the same time, present a irrelevant calculation of the length of a string, the success rate drops from 99.3% to 31.2%. In rare cases (0.08% of all results), the model has even filled the size (an expected int value) with meaningless non-English characters and symbols. This preliminary study demonstrates thatGPT-4o has a highchance of generating memory bugs in C code, and motivates us to further investigate the security of LLM-generated code.\n\n3.2 Sample Task 2: Implementing SHA1\n\nWe asked GPT-4o to complete a widely-used, security-critical algorithm: SHA1 (Secure Hash Algorithm 1). The SHA1 header file from OpenBSD’s GitHub repository [23] was pro- vided in the prompt to make sure that the specifications are clear, and there are no discrepancies in function input/out- put formats between the GPT-4o and human implementation. We tested the correctness of GPT-4o’s SHA1 implementation by using the reference test vectors (predefined inputs with their expected outputs) provided by the National Software Reference Library (NSRL) [22]. GPT-4o’s implementation produced incorrect hash values for all the inputs. Such silent errors will not prompt any error message, but incorrect hash values can lead to security vulnerabilities and malfunctions\n\n4\n\nin software that uses hash algorithm for authentication or integrity checks. As expected, OpenBSD’s SHA1 implemen- tation computed all the hash values correctly.\n\n3.3 Sample Task 3: Implementing Graph Data\n\nStructure\n\nThis task entailed generating a simple graph implementation, with functions for creating (allocating) a graph, adding and removing edges, printing the graph, and freeing (deallocat- ing) the graph memory from the heap. The relevant allocation code – creating a graph of size V – is shown in Figure 3. Note that the human-developed code, shown on the left, has checks for the value of V, whereas the LLM-generated code, on the right, does not contain such checks. The LLM code has two issues. First, it does not check the result of malloc() for NULL, hence opening the first entry for potential NULL pointer dereferences if the memory allocation fails. Second, the LLM-generated code does not check for a potential nega- tive value of V. If a negative V is passed as an argument,the call to malloc() on line 5 will most likely result in a NULL pointer (since malloc() takes an unsigned argument, the argument is interpreted as a positive value at the size of size_t_max - abs(V) + 1, where size_t_max is the max value of the type), or, less likely, in a very large memory allocation. Therefore, memory allocation either fails or its size is in the control of a potential adversary. We refer to this as malloc overflow, as a term used by Clang analyzer.\n\n3.4 Motivations\n\nAligned with the national goal to manage the risk of genera- tive AI [4,10], our research investigate the security of LLM- generated code. Our research is motivated by the following questions: Research Question 1: Does LLM generate insecure code? Research Question 2: For the same task, does LLM generate code that is more secure (or higher quality) than the human- written version? Research Question 3: Can prompt engineering improve the security of LLM-generated code?\n\n4 Methodology\n\nWe select two categories of coding tasks to evaluate the secu- rity of LLM-generated code: (1) LeetCode problems, and (2) data structures and algorithms, including three widely-used cryptographic algorithms. We chose these categories because they are widely used in almost all software engineering inter- views and actual coding tasks, and they have relatively clear input-output interfaces.\n\nFor LeetCode problems, we selected all 202 problems from a well-known LeetCode solution repository on GitHub with 2.3k forks and 5.5k stars [20]. We use the same set of data",
      "content_length": 4921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Buffer Size Definitions A constant integer The length of a string (alphabet) The exponentiation of an integer to the power of another integer Multiplying two integers Subtracting a float from an integer The exponentiation of an integer to the power of another integer, plus a float The length of a string (alphabet with a missing letter) A constant integer, but with irrelevant presence of length of a string Multiplying an integer and a float Multiplying two floats\n\nExample 402 len(ABC...XYZ) pow(4, 2) 415 * 495 922 - 174.05 pow(1,5) + 63.56 len(ABC...XYZ) 612 893 * 518.33 285.63 * 62.72\n\nSuccess Rate 99.3% 92.8% 91.4% 63.1% 50.1% 42.4% 41.6% 31.2% 10.6% 1.5%\n\nTable 1: Preliminary study on GPT-4o’s ability to correctly determine the buffer size in Figure 1.\n\nHuman\n\nGPT-4o\n\n1\n\n2\n\n3\n\n4\n\n5\n\nGraph newGraph(int V) {\n\nassert(V >= 0); Graph g = malloc(sizeof(GraphRep)); assert(g != NULL);\n\n1\n\n2\n\n3\n\n4\n\n5\n\nGraph newGraph(int V) // V can be negative! {\n\n// not checked for NULL Graph g = malloc(sizeof(GraphRep)); // not checked for NULL g->edges = malloc(V * sizeof(int *));\n\nFigure 3: LLM-generated code for a graph implementation, shown on the right, has no guardrails (comments added by us).\n\nstructures, algorithms, and LeetCode problems for both LLM- generated and human-written code to ensure a fair compari- son.\n\nFor data structures and algorithms, we selected four sources of human code from GitHub: a collection of 13 data struc- tures and algorithms in C, which has 4.3k forks and 18.8k stars [27]; a repo with an additional data structure, having 719 forks and 3.3k stars [11]; a hashmap algorithm repo, with 205 forks and 520 stars [24]; and the OpenBSD project for three cryptographic algorithms with 858 forks and 3.2k stars [23].\n\n4.1 LLM Code Generation\n\nWe use OpenAI’s GPT-4o model to generate the full task solution for each coding task. As shown in Figure 4, for each coding task, we provide its task description as prompt to GPT- 4o API, and collect the coding outputs.\n\nClang static analysis\n\nClang static analysis\n\nAFL fuzzing tests\n\nComplexity analysis\n\nPrompt Construction\n\nAFL fuzzing tests\n\nTask\n\nFunctionality Equivalency Test\n\nComplexity analysis\n\nHuman-written code\n\nOnlineUnit Tests Fuzzing\n\nAI-generated code\n\nFor LeetCode problems, we locate the problem description in the LeetCode website using the problem title in the Leet- Code solution repository, and use the description as a prompt. The prompt is constructed in a way as close as possible to the actual task description given to a human developer.\n\nFigure 4: The architecture of our framework, EXACT.\n\n4.2 Functionality Validation\n\n4.2.1 LeetCode Problems\n\nFordata structures and algorithms,we provided prompts for GPT-4o as follows: either the header files, or in the absence of header files, the function declarations (types) from the human-written code. This ensures that both GPT-4o and hu- man implement the same set of functionalities and eliminate the discrepancies in function signatures in code comparison.\n\nAfter obtaining the GPT-4o-generated code, we use test cases to validate the functionalities of the code against the human- written code for the same task. If test cases are available, such as LeetCode online submission [19], we submit GPT- 4o-generated code directly to the LeetCode platform (Sec- tion 5.1.1) for evaluation.\n\n5",
      "content_length": 3339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "4.2.2 Data Structures and Algorithms\n\nFor data structures and algorithms, we employ unit testing and fuzzing to validate functionality, as follows.\n\nUnit testing. Test cases are manually written for unit test- ing with the help of CUnit [9]. We examine the human-written code to understand the functionality provided by the data structure or algorithm, such as insertion, deletion, sorting, etc. We then write test cases, divided into 2 categories: (1) regular cases, and (2) edge cases. Regular cases are typical scenar- ios that a function is expected to handle, such as inserting positive integers into a linked list, perform a merge sort on an unsorted array, etc. Edge cases are crafted in a way to test a function’s protection against unexpected inputs, such as dequeuing from an empty queue, performing a breadth- first search on an empty graph, sorting an empty array, etc. We ran the same test cases on both GPT-4o-generated and human-written code, since we prompt GPT-4o to implement the data structures and algorithms with the same header files or function declarations as used in the human-written code. By testing GPT-4o-generated and human-written code with the same test cases, we verify that they are achieving the same functionalities.\n\nFuzzing. In addition to unit testing, we use AFL [2] to fuzz both GPT-4o-generated code and human-written code. We create an entry point to fuzz each data structure and algorithm. This entry point is the main method in each program where it reads an input file that contains various instructions and input values. For example, the line insert 10 in an input file will carry out the insert operation in the binary search tree and insert the value 10 into the tree. The input file is used as a seed in fuzzing. Eachdata structure andalgorithm has its own setof instructions to make sure that all implemented functionalities will be tested. Since we only focus on functionality validation, any invalid instructions mutated by AFL [2] in the process of fuzzing will be ignored in the entry point. However, values will be mutated during fuzzing and GPT-4o-generated code and human-written code need to handle any kinds of invalid inputs. As it was not relevant to the comparison, the entry point code was not included in the security and complexity analysis.\n\n4.2.3 Cryptographic Algorithms\n\nFor cryptographic algorithms, we employ a different way of validating the functionality. For one-way hashing algorithms such as SHA1 and Message Digest Algorithm 5 (MD5), we encrypt the predefined inputs from test vectors and compare the computed hash values with the expected hash values from the test vectors; we used existing sets of test vectors (strings) originating from NSRL [22] (as discussed in Section 3.2). For two-way algorithms such as Advanced Encryption Standard (AES), we encrypt the predefined inputs and decrypt the en- crypted values; we then validate the decrypted values against the original inputs.\n\n6\n\n4.3 Static Analysis\n\n4.3.1 Security\n\nWe use the Clang static analyzer [1] to perform static analysis on both GPT-4o-generated and human-written code. Clang provides a set of default checkers, such as null dereference, memory leak, and null arguments. In addition, Clang provides a range of experimental (advanced) checkers in several cat- egories, as follows. “Core” experimental checkers include detectors for pointer arithmetic, invalid casting/conversion, etc. “Security” experimental checkers look for errors such as array index out of bounds, malloc overflow, etc. “Unix” experimental checkers look for issues such as memory leaks or null pointers passed to string functions. While the exper- imental checkers might, in theory, emit a higher number of false positives, our manual validation of the reported errors indicate a negligibly low rate of false positives.\n\n4.3.2 Complexity\n\nWe evaluate code complexity using several metrics: cyclo- matic complexity, normalized complexity, and lines of code. Cyclomatic complexity is used to measure the complexity of a program’s control flow [37]. High cyclomatic complexity can indicate that the code is harder to understand and maintain, and potentially prone to errors. As cyclomatic complexity is an absolute value that characterizes an entire file, larger files naturally have higher complexity. Therefore, to gauge the complexity of typical code in a file, we also compute the normalized complexity, i.e., divide complexity by the num- ber of lines of code in that file – this indicates the typical complexity expected for a code snippet. We use SCC [25] to obtain the cyclomatic complexity and the lines of code in each C file. By “lines of code” we mean actual source code, excluding comments and blank lines.\n\n5 Analysis Results\n\nWe now present our comparative analysis of GPT-4o- generated and human-written code in terms of functionality (Section 5.1), security (Section 5.2), and complexity/code quality (Section 5.3).\n\n5.1 Functionality Analysis\n\n5.1.1 LeetCode Problems\n\nThe LeetCode platform provides an online submission system [19] where submitted code has to pass all the test cases – only then it is considered a correct solution. All the human-written code that we gathered from the GitHub repository [20] had to pass all the test cases from LeetCode online submission system prior to being placed in the repository. Therefore, we only tested GPT-4o-generated code through LeetCode online",
      "content_length": 5423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "submission [19]. We found that 87.6% of LeetCode solutions generated by GPT-4o passed the LeetCode online submission checks. The 12.4% of LeetCode solutions generated by GPT- 4o that failed were due to issues such as exceeding time limit, failing test cases, and generating runtime or compile time errors. The detailed analysis results are reported in Table 2 and discussed next.\n\nFailing test cases and exceeding time limit. Among the solutions with issues, 9 (39.6%) are failing test cases and exceeding time limit. Although detailed instructions with sample inputs and outputs are given in the prompts, GPT-4o’s solutions are not always able to fulfill all the requirements stated in the prompts.\n\nRuntime errors. In addition to functionality test cases, LeetCode’s online checker compiles our submitted code with Address Sanitizer enabled, and looks for runtime errors as well, such as memory violations and other crashes. Runtime errors take up a significant 48% of the solutions with issues. There are multiple categories of runtime errors such as buffer overflow, signed integer overflow, array index out of bounds, and load of address with insufficient space. The results high- light the importance of dynamic (runtime analysis) in con- junction with static analysis: 8 out of 12 of the runtime errors were not identified during our Clang static analysis. Conse- quently, these silent, though critical errors, could easily be neglected by users and introduce severe security threats if this code were to be used in real-world programs – a scenario that is very likely to happen, as pointed out in prior research and mentioned in Section 1.\n\nCompile time errors. Compile time errors are due to incor- rect function declarations generated by GPT-4o. These issues can be mitigated by providing specific function declarations required by LeetCode in the prompts.\n\n5.1.2 Data Structures and Algorithms\n\nSection 4.2.2 discussed our strategy for writing unit test cases and seed files, used in unit testing and fuzzing, respectively. We discuss our findings next.\n\nUnit testing. All of the data structures and algorithms im- plemented by GPT-4o passed the unit testing. For human- written code, 3 out of 15 of the data structures and algorithms did not pass unit testing due to failing test cases and caus- ing segmentation faults. For example, in the implementation of doubly-linked list, the usage of == for double comparison leads to failing test cases. In the implementation of red-black trees, a missed return statement causes nullptr to be used as a buffer address, leading to null pointer dereference.\n\nFuzzing. In terms of unique hangs, GPT-4o-generated code has 50% more hangs than human-written code, whereas for unique crashes, GPT-4o generated code has 23.9% more crashes than human-written code. We were unable to perform fuzzing on certain human-written code such as queue, doubly linked list, and red black tree due to the bug mentioned above\n\n7\n\nin this section (the null pointer dereference crash caused by the missed return statement). These files are listed as N/A in Table 3. Note that these unit tests focused on functionality alone; the security aspect will be discussed in Section 5.2.\n\n5.1.3 Cryptographic Algorithms\n\nWe performed unit testing on AES, MD5, and SHA1 as dis- cussed in Section 4.2.3. Human-written code from OpenBSD generates all the correct hash values, as anticipated. GPT- 4o’s implementations of MD5 and AES compute all values correctly. GPT-4o’s SHA1 implementation computes wrong hash values for all the inputs, which could be catastrophic if this implementation were to be used in real-world situations. Functionality issues like this have a high chance of going un- noticed, unless the LLM-generated code goes through strict testing.\n\n5.2 Security Analysis\n\nWe perform static analysis on both GPT-4o-generated and human-written code using Clang static analyzer [1]. Table 4 shows the analysis results of 220 files (202 LeetCode prob- lems, 18 data structures and algorithms). Overall, GPT-4o- generated code has 10.3% more security issues than human- written code. The three categories of issues thathave relatively higher count are malloc overflow, array index out of bounds, and memory leak. We found that GPT-4o-generated code has 32.8% more malloc overflow issues than human-written code. This is a consequence of GPT-4o’s tendency of assuming all inputs are valid (Figure 3). The counts for array index out of bounds issues are similar for both GPT-4o and human. Figure 5 illustrates the similar mistakes made by both GPT- 4o and human in one of the LeetCode problems where both code may access elements that are out of bounds. Memory leak issues, where human code has a 50% higher count than GPT-4o, are mainly due to the problem description, where the LeetCode problems specifically mention that the caller will invoke free(). However, the memory leak issues do exist if the code snippet are used outside the context of these LeetCode questions.\n\nOther than issues reported by Clang, we also look at the im- plementation details between GPT-4o-generated and human written code on data structures and algorithms. We do not focus on the implementation details of LeetCode solutions because the main objective of a LeetCode problem is to pro- vide a solution that solves a very particular problem with a given set of constraints that passes all the test cases provided. We found that GPT-4o has a tendency of generating ex- act same code as human-written code. We will refer to this as “code parroting” for the rest of the paper. Note that we do not include any implementation details of human-written code in the prompts with the exception of header files or function declarations. In GPT-4o-generated code, 2 out of",
      "content_length": 5769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Question# 215 853 115 297 332 22 152 179 377 523 846 981\n\nError Type\n\nDifficulty Error Details\n\nTime Limit Exceeded Medium Time Limit Exceeded Medium\n\nRuntime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error Runtime Error\n\nHard Hard Hard\n\nsigned integer overflow buffer overflow load of address with insufficient space\n\nMedium heap-buffer-overflow Medium signed integer overflow Medium stack buffer overflow Medium signed integer overflow Medium array index out of bounds Medium load of address with insufficient space Medium AddressSanitizer: requested allocation size exceeds maximum sup-\n\n1838 208 1046 51 212 40 138 435 1930 88 1299 1899 1968\n\nRuntime Error Runtime Error Failed Test Case Failed Test Case Failed Test Case Failed Test Case Failed Test Case Failed Test Case Failed Test Case Compile Error Compile Error Compile Error Compile Error\n\nported size Medium signed integer overflow Medium heap-buffer-overflow\n\nEasy Hard Hard Medium Medium Medium Medium Easy Easy\n\nfunction definition - number of parameters invalid function definition - number of parameters invalid Medium function definition - number of parameters invalid Medium function definition - number of parameters invalid\n\nTable 2: LeetCode-reported issues in GPT-4o-generated code.\n\nthe 18 files in data structures and algorithms are the exact replica of human written code, as follows. In AVL tree’s im- plementation, GPT-4o reproduces the human-written code from GitHub, including the helper functions. Similarly, in the graph data structure implementation, GPT-4o reproduces the human-written code, but fails to add validation for input values (Figure 3). Code parroting is extremely harmful con- sidering the fact that malicious code could possibly be the training data of LLM (more commonly known as data poison- ing if malicious information is injected into training data on purpose). Harmful code will then be generated by LLM and possibly utilized by developers.\n\n5.3 Complexity Analysis\n\nSCC [25] is the tool that we utilize to measure cyclomatic complexity and lines of code (LoC, excluding comments and blank lines) per file. We calculate mean, median, geometric mean, and trimmed mean for both complexity and normalized complexity (complexity per LoC).\n\nquality code. First, low (absolute) complexity is an artifact of GPT-4o generating less code than the human equivalent for the same task. In fact, human-written code always has a lower normalized complexity, which is a more indicative metric of the efforts needed to understand and maintain a typical snippet of code in a given file. Second, the low com- plexity is due to GPT-4o generating “bare-bones” code. For instance, the GPT-4o-generated hash map implementation (Figure 6) uses a simple hash function that does not check for collisions, while the human-written implementation has a more sophisticated hash function that checks for collisions (Figure 7). This example illustrates one of the reasons why the GPT-4o-generated code has lower cyclomatic complexity (when computing complexity for the entire file, i.e., with- out normalizing for lines of code). Another example is in Figure 8 where the human-written implementation validates input values before carrying out certain operations in one of the LeetCode problems, whereas GPT-4o makes the assump- tions of all inputs are valid. This undoubtedly increases the cyclomatic complexity of human-written code but illustrates that human-written code uses defensive programming, which in turn mitigates various security threats.\n\nAs shown in Table 5, GPT-4o-generated code seems to have lower complexity on the surface. However, it would be incorrect to assume that this lower complexity indicates high-\n\n8",
      "content_length": 3772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Human\n\nGPT-4o\n\n1\n\n2\n\n// j starts from 1 for (int j=1; j<=n; j++) {\n\n1\n\n2\n\nfor (int j=1; j<=n; j++) { // j starts from 1\n\nif (p[j-1] != ’*’) {\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nif (p[j-1] == s[i-1] || p[j-1] == ’.’) {\n\ndp[i][j] = dp[i-1][j-1];\n\n} else if (p[j - 1] == ’*’) { // [j-2] could be -1 dp[i][j] = dp[i][j-2] || ((s[i - 1]\n\n== p[j-2] || p[j-2] == ’.’) && dp [i-1][j]);\n\n3\n\n4\n\n5\n\n6\n\ndp[i][j] = i > 0 && dp[i-1][j-1] && ( s[i-1] == p[j-1] || p[j-1] == ’.’ );\n\n} else {\n\n// [j-2] could be -1 dp[i][j] = (j>=2 && dp[i][j-2]) || (i >0 && dp[i-1][j] && (s[i-1] == p[ j-2] || p[j-2] == ’.’));\n\nFigure 5: Array index out of bounds issue.\n\nStack Queue Singly Linked List Doubly Linked List Binary Search Tree AVL Tree Red Black Tree Hash Map Hash Set Hash Table Graph Breadth First Search Depth Search Merge Sort Bubble Sort Total\n\nFirst\n\nUnique Hangs Unique Crashes LLM Human LLM Human\n\n3 0 0\n\n4 N/A 0\n\n0 13 65\n\n0 N/A 73\n\n1\n\nN/A\n\n59\n\nN/A\n\n0\n\n0\n\n48\n\n65\n\n0 0 0 0 0 2 3\n\n0 N/A 0 0 0 0 0\n\n58 45 16 22 19 25 27\n\n64 N/A 27 15 15 40 33\n\n1\n\n0\n\n54\n\n32\n\n1 1 12\n\n1 1 6\n\n0 0 451\n\n0 0 364\n\nCategory Memory Leak Null pointer dereference Assigned value is garbage or unde- fined Malloc overflow Taint propagation Division by zero Address of stack memory returned to caller Undefined binary operator Resultof malloc convertedto invalid type Undefined or garbage value re- turned to caller Function call argument is an unini- tialized value Array index out of bounds Implicit conversion Nested function is not supported Cast from non struct to struct C string out of bounds Total\n\nGPT-4o Human\n\n8 6 2\n\n16 8 2\n\n81 0 0 0\n\n61 1 1 1\n\n3 0\n\n3 1\n\n3\n\n1\n\n2\n\n3\n\n16 4 1 1 1 128\n\n17 1 0 0 0 116\n\nTable 3: Unique hangs and crashes from AFL fuzzing on GPT- 4o and human-written code. Green means lower in values.\n\nTable 4: Number of issues found by the Clang static analyzer on GPT-4o and human-written code.\n\n6 Feedback Loop\n\nIn terms of LoC, the results in Table 6, Figure 9, and Fig- ure 10 demonstrate that GPT-4o always produces fewer LoC than human. However, this is due to the lack of defensive pro- gramming as shown in Figures 3 and 8 and simplification of certain functionalities as shown in Figure 6 in LLM-generated code. Undoubtedly, GPT-4o-generated code being more con- cise than human-written code could serve a valuable purpose, e.g.,in introductory programming classes. That being the case, introducing security risks into real-world programs seems to be unavoidable if LLM-generated code is utilized in a profes- sional environment.\n\n6.1 Rationale and Setup\n\nState-of-the art practice suggests using a reinforcement loop (Section 2.2) to improve code quality when using LLMs. Therefore, we constructed a feedback loop as a way to im- prove GPT-4o-generated code, starting by trying to eliminate the security issues identified in Section 5.2. For this experi- ment, we included 60 LeetCode files (solutions) generated by GPT-4o, as follows: 30 files with security issues (found by Clang), and 30 files without security issues. The list of secu- rity issues is listed in Table 7. We ask GPT-4o to regenerate\n\n9",
      "content_length": 3081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Complexity/Code (D&A) Mn Md GM TM Mn Md GM TM Mn Md GM TM Mn Md GM TM 0.21 0.16\n\nComplexity (LC)\n\nComplexity/Code (LC)\n\nComplexity (D&A)\n\nGPT-4o 7.37 Human 8.10\n\n0.21 0.17\n\n18.8 25.1\n\n16.4 21.2\n\n14 23\n\n25.7 34.5\n\n0.31 0.27\n\n0.23 0.16\n\n0.29 0.25\n\n0.33 0.28\n\n7.07 7.44\n\n6.03 6.19\n\n7 7\n\n0.21 0.16\n\n0.18 0.15\n\nTable 5: Cyclomatic complexity measurements. LC is LeetCode (202 files), D&A is data structures and algorithms (18 files), Mn is mean, Md is median, GM is geometric mean, and TM is trimmed mean. We use the same abbreviations for the rest of the paper. Trimmed mean is at 5% for LeetCode and 15% for data structures and algorithms.\n\nLines of Code (LC)\n\nLines of Code (D&A)\n\nGPT-4o Human\n\nMn Md GM TM Mn Md GM TM 99.1 23.6 172.8 29.6\n\n20 24\n\n21.2 25.6\n\n22.2 27.2\n\n103.5 192.1\n\n83 149\n\n88.7 142\n\nTable 6: Lines of Code measurements. Trimmed mean is at 5% for LeetCode and 10% for data structures and algorithms.\n\n1\n\nunsigned long hashFunction(const char *key) {\n\n1\n\nint hashmap_hash(map_t in, char* key){\n\n2\n\n3\n\nunsigned long hash = 0; while (*key) {\n\n2\n\n3\n\nint curr; int i;\n\n4\n\nhash = (hash << 5) + *key++;\n\n4\n\n5\n\n6\n\n} return hash;\n\n5\n\n6\n\n/* Cast the hashmap */ hashmap_map* m = (hashmap_map *) in;\n\n7\n\n}\n\n7\n\nFigure 6: GPT-4o implementation of hashing in Hash Map.\n\n8\n\n9\n\n10\n\n/* If full, return immediately */ if(m->size >= (m->table_size/2)) return MAP_FULL;\n\n11\n\nthese files based on different security categories. In each loop, prompts for files with security issues will be accompanied by prompts for files that have no security issues. For instance, we specifically ask GPT-4o to fix only malloc overflow issues in this loop with a total a 18 prompts related to files that have malloc overflow issues along with another 18 prompts related to files that have no malloc overflow issues. Note that the prompts (LeetCode problem descriptions) used in the loops are the same as the non-feedback setup, but with additional instructions that ask GPT-4o to make sure there are no certain security issues in the code. Since there are 3 categories of security issues, a total of 3 loops will be conducted, each loop to fix a specific security issue.\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n}\n\n/* Find the best index */ curr = hashmap_hash_int(m, key);\n\n/* Linear probing */ for(i = 0; i< MAX_CHAIN_LENGTH; i++){ if(m->data[curr].in_use == 0)\n\nreturn curr;\n\nif(m->data[curr].in_use==1 && (strcmp (m->data[curr].key,key)==0)) return curr;\n\ncurr = (curr + 1) % m->table_size;\n\n} return MAP_FULL;\n\nFigure 7: Human implementation of hashing in Hash Map.\n\n6.2 Results\n\nFiles with security issues. For files that did contain security issues, GPT-4o manages to lower the number of security is- sues found after one iteration on malloc overflow and null dereference from 40 to 24 and 3 to 2, respectively, as shown in Table 8. However, the security issues are not completely removed, even though we specifically asked GPT-4o to ensure the code will not have these security issues. Furthermore, for array index out of bounds issues, GPT-4o generated 3 more such issues. This is a critical finding since it shows the possi- bility that LLM may generate code that is the exact opposite of what users have specified in the prompt.\n\nFiles without security issues. For files without any se- curity issue before entering the loop, GPT-4o introduces 4 new malloc overflow issues as shown in Table 9. Figure 11 illustrates one of the code examples (before and after running the loop), where the new code has a malloc overflow issue. Malloc overflows account for 63.3% of the total security is- sues found across the code generated by GPT-4o. However, GPT-4o-generated code, on the right side of Figure 11, still does not check the value of k and the result of malloc after an effort has been made in the prompt to ask LLM to make no such mistakes. This outcome is especially troublesome, since\n\n10",
      "content_length": 3899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Human\n\nGPT-4o\n\n1\n\nvoid timeMapSet(TimeMap* obj, char* key, char\n\n1\n\nvoid timeMapSet(TimeMap* obj, char* key, char\n\nvalue, int timestamp) {\n\nvalue, int timestamp) {\n\n2\n\n2\n\n3\n\n4\n\nif (obj == NULL || key == NULL || value\n\n== NULL) return;\n\n3\n\n4\n\n// no input verifications for (int i = 0; i < obj->node_count; i++)\n\n{\n\n5\n\n5\n\nobj->nodes[i].entries[obj->nodes[i].\n\n6\n\n7\n\nint index = -1; for (int i = 0; i < obj->size; i++) {\n\nentry_count].timestamp = timestamp;\n\nFigure 8: GPT-4o-generated code (right) does not verify inputs.\n\n# of files\n\nM A N Total 18\n\n10\n\n2\n\n30\n\nTable 7: Number of files used in loop testing based on security categories. M is malloc overflow, A is array index out of bounds, N is null dereference. We use the same abbreviations for the rest of the paper.\n\nBefore After\n\nM A N Total 40 24\n\n15 18\n\n3 2\n\n58 44\n\nTable 8: Number of security issues found before and after running the loop on files with security issues before based on security categories. Red means an increase in issues found.\n\nFigure 9: LoC per file in data structures and algorithms.\n\ntime error, i.e., exceeding memory limit when k = 1000000000 (exposed by LeetCode’s online submission system). As Leet- Code runtime checker caught this bug as well, it validates the findings from Clang analyzer.\n\n6.3 Why can’t Prompt Engineering Fix Secu-\n\nrity Issues?\n\nFigure 10: LoC per file in LeetCode.\n\nthe previously-produced solutions contained no such issues. Note that the particular solution on the left in Figure 11 had no prior issues found during static analysis but did have a run- time error (array index out of bounds) when submitted to the LeetCode online submission platform [19] before running the feedback loop. However, the regenerated solution, supposedly the “better” solution, on the right in Figure 11, still has a run-\n\nThe results from the feedback loop allow us to draw a conclu- sion that LLM so far is not capable of consistently removing security issues even though the prompt has explicitly asked it to do so. LLM might even go against the prompt in producing more security issues in the code. First of all, these security issues are found by running static analysis. Users who utilize LLM for code generation might not even notice that the code poses serious security threats without running static analy- sis. Furthermore, even static analysis is not enough: some of the runtime errors found by LeetCode’s online submis- sion system [19] were not identified during static analysis (Section 5.1.1). To conclude, even for users who are aware of potential issues and diligent in attempting to reduce or eliminate security risks in the code, the LLM route might be unfruitful: LLM regeneration may worsen the security problem in the generated code, e.g., as shown in Figure 11.\n\n11",
      "content_length": 2777,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Before\n\nAfter\n\n1\n\nbool checkSubarraySum(int* nums, int numsSize\n\n1\n\nbool checkSubarraySum(int* nums, int numsSize\n\n2\n\n3\n\n, int k) { if (numsSize < 2) return false;\n\n2\n\n3\n\n, int k) { // not checking k before malloc int *remToIndex = (int *)malloc(k *\n\n4\n\nsizeof(int));\n\n5\n\n6\n\n7\n\nint sum = 0; int remainder; int remainder_map[numsSize];\n\n4\n\n5\n\n6\n\n// not checking result of malloc for (int i = 0; i < k; i++) {\n\nremToIndex[i] = -1;\n\n8\n\n9\n\n10\n\nfor (int i = 0; i < numsSize; i++) {\n\nremainder_map[i] = -1;\n\n7\n\n8\n\n9\n\n} remToIndex[0] = 0; int cumulativeSum = 0;\n\n11\n\n}\n\n10\n\n12\n\n11\n\nfor (int i = 0; i < numsSize; i++) {\n\n13\n\n14\n\n15\n\nremainder_map[0] = 0;\n\nfor (int i = 0; i < numsSize; i++) {\n\n12\n\n13\n\n14\n\ncumulativeSum += nums[i]; int remainder = cumulativeSum % k; if (remainder < 0) remainder += k;\n\n16\n\n17\n\nsum += nums[i]; remainder = sum % k;\n\n15\n\n16\n\nif (remToIndex[remainder] != -1) {\n\n18\n\n17\n\nif (i + 1 - remToIndex[remainder]\n\n19\n\nif (remainder < 0) remainder += k;\n\n>= 2) {\n\n20\n\n21\n\nif (remainder_map[remainder] < 0) {\n\n18\n\n19\n\nfree(remToIndex); return true;\n\n22\n\n23\n\nremainder_map[remainder] = i + 1;\n\n} else if (i - remainder_map[\n\n20\n\n21\n\n} } else {\n\n24\n\nremainder] > 0) { return true;\n\n22\n\n23\n\n}\n\nremToIndex[remainder] = i + 1;\n\n25\n\n26\n\n27\n\n}\n\n} return false;\n\n24\n\n25\n\n26\n\n} free(remToIndex); return false;\n\n28\n\n}\n\n27\n\n}\n\nFigure 11: LeetCode solutions for problem number 523 generated by GPT-4o before and after running a loop.\n\nBefore After\n\nM A N Total 0 0 0 4\n\n0 0\n\n0 4\n\nTable 9: Number of security issues found before and after running the loop on files without security issues before based on security categories.\n\n7 Related Work\n\ning could be extremely dangerous when malicious code is injected into the training set (poisoning attack). In fact, nu- merous studies have shown that code completion tools that utilize LLM are vulnerable to poisoning attack such as sug- gesting insecure code in AES implementation [45]. A tool, CodeBreaker [50], was designed for back-door attacks with the help of LLM where poisoned data for code generation can avoid strong vulnerability detection. These works support our point that LLM-generated code is insecure, requiring further analyses and improvements.\n\n7.1 AI-based Code Generation Security\n\nSimilar studies have been done on the security aspect of LLM code generation. Perry et al. [41] conducted a user study on two groups of participants: one without and the other with, access to an AI assistant. Participants were asked to solve a few specific tasks using multiple programming languages. Their results show that participants with access to the AI assistant wrote significantly less secure code, which aligns with our findings. We share a similar goal but take a different approach to addressing the AI assistant-introduced issues, as the comparisons in our study are between LLM and human in solving algorithmic and data structure tasks.\n\n7.2 Benchmarks for LLM-Generated Code\n\nPurple Llama CyberSecEval [28] and Cybench [53] are bench- marks designed to tackle the security issues arising from LLM-generated code. CyberSecEval selects insecure code tests from real-world open source codebases whereas Cy- bench gathers 40 Capture the Flag (CTF) tasks for evaluat- ing LLM-generated code. These benchmarks are orthogonal to our purpose in evaluating an LLM’s coding capabilities. EXACT is a comprehensive evaluation framework that can in- corporate these benchmarks as a part of its testing if needed.\n\nWe discuss code parroting in Section 5.2. Code parrot-\n\n12",
      "content_length": 3520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "7.3 Code Improvement in LLM\n\nAs we discussed in Section 2.2, RL has been employed by LLMs to improve code generation. Due to the implementation of these blackbox components, we would not know for sure if the improvements have been made in the security aspect of code generation. However, our study has shown that LLM- generated code produces a significant amount of security risks. Prompt-based learning [33] is an approach where LLMs are guided to generate desired output with carefully designed prompts without re-training the models. This technique could be applied on code generation as well. For instance, whenever malloc appears in code generation, it should come with an allocation size check before using malloc and a NULL check for the result of malloc. This would be one of the desired outputs to drastically reduce malloc overflow issues. However, based on the results we have seen so farin ourstudy,LLM-generated code still has a long way to go to achieve this level of security.\n\n7.4 Privacy and Security in AI\n\nCode generation is not the only domain that raises privacy and security issues regarding LLMs. Client-side prompt san- itization [29,46,52] is designed to protect private informa- tion shared with LLMs without affecting the performance of LLMs. Federated learning [38] can be used to train Machine Language models without sharing raw data that could be privacy-sensitive. Prompt injection is an attack where LLMs could generate harmful content due to malicious prompts; benchmarks [34] and fine-tuned models [42] can defend against such attacks. Our study shows the importance of scru- tinizing code security as well.\n\n8 Discussion\n\n8.1 Multiple LLMs\n\nOur study focuses on a single LLM, GPT-4o. We could extend our study by conducting the comparisons using the same human-written code with otherLLMs such as Llama3,Claude, etc.; which would reveal differences between different LLMs in terms of code security. However, the focal point of our study is to bring attention to the differences between LLM- generated and human-written code. We believe that GPT-4o, being the state-of-the-art of OpenAI’s LLM, has successfully revealed the security threats brought upon by LLMs-based code generation; the issues we found are critical, yet likely overlooked when programming with the help of LLMs.\n\n8.2 Coding Tasks\n\nThe coding tasks in our experiments are relatively small and independent as opposed to the code in large software projects such as an operating system kernel, a browser etc. Comparing\n\n13\n\ncode implemented for a subsystem can be an interesting fu- ture work since it requires more attention to interoperability, as these software has a plethora of components within them- selves. Another possible future work in terms of coding tasks could be asking LLM to fix known errors or identify possible issues in human-written code.\n\n9 Conclusions\n\nAs AI-assisted code writing is spreading, it is imperative to study its impact and consequences on software security. We developed a methodology and framework, EXACT, for com- paring the security and quality of LLM-generated and human- written code for the same task. We expose and quantify the disadvantages of naïvely using LLM-generated code in a critical domain such as security: the LLM-generated code might violate the expected functionality in subtle ways, or might contain security issues that do not manifest until late in program execution, and can be exploited by adversaries. In contrast, human code for equivalent tasks tends to contain guardrails and defensive constructs. We also show the poten- tial perils of using a feedback loop when programming with the help of LLMs. Our work sheds light on the importance of scrutinizing LLM-generated code (and code re-generated by prompting) for functionality, security, and quality issues. As long as LLM-generated code is prone to security risks it should be considered potentially harmful.\n\nAcknowledgments\n\nThis research was supported in part by the National Science Foundation grant CCF-2106710. The authors also thank Ope- nAI for donating API credits to this project.\n\nReferences\n\n[1] Clang: a C Language Family Frontend for LLVM. http:\n\n//clang.llvm.org/.\n\n[2] american fuzzy lop. https://github.com/google/\n\nAFL, 2021.\n\n[3] Research: lot’s and 2022-09-07-research-quantifying-github -copilots-impact-on-developer-productivity -and-happiness, 2022.\n\nquantifying\n\nCopi- developer productivity https://github.blog/\n\nGitHub\n\nimpact happiness.\n\non\n\n[4] Executive Order on the Safe, Secure, and Trust- In- worthy Development and Use of Artificial https://www.whitehouse.gov/ telligence. briefing-room/presidential-actions/2023/10/ 30/executive-order-on-the-safe-secure-and-",
      "content_length": 4709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "trustworthy-development-and-use-of -artificial-intelligence/, 2023.\n\n[5] GitHub CEO says Copilot will write 80% of code “sooner than later”. https://www.freethink.com/ robots-ai/github-copilot, 2023.\n\n[6] GitHub\n\nCopilot\n\nhas now new capabilities.\n\na\n\nbetter AI https://\n\nmodel github.blog/ai-and-ml/github-copilot/ github-copilot-now-has-a-better-ai-model -and-new-capabilities, 2023.\n\nand\n\n[7] AI Coding Assistant - Amazon Q Developer. https:\n\n//aws.amazon.com/q/developer, 2024.\n\n[8] Anthropic Claude 3.5 Sonnet.\n\nhttps://www.\n\nanthropic.com/news/claude-3-5-sonnet, 2024.\n\n[9] CUnit.\n\nhttps://cunit.sourceforge.net/index.\n\nhtml, 2024.\n\n[10] EXECUTIVE\n\nORDER\n\nON\n\nSAFE, SE- ARTIFI- https:\n\nCURE, AND CIAL INTELLIGENCE | NIST. //www.nist.gov/artificial-intelligence/ executive-order-safe-secure-and-trustworthy -artificial-intelligence, 2024.\n\nTRUSTWORTHY\n\n[11] fragglet/c-algorithms.\n\nhttps://github.com/\n\nfragglet/c-algorithms, 2024.\n\n[12] Gemini Code Assist. https://cloud.google.com/\n\nproducts/gemini/code-assist, 2024.\n\n[13] GitHub Copilot - Your AI pair programmer. https:\n\n//github.com/features/copilot, 2024.\n\n[14] GitHub Copilot Efficiency Explored: Key Take- https://akvelon.\n\naways from Akvelon’s Survey. com/github-copilot-efficiency-survey-data- revealed-akvelon/, 2024.\n\n[15] GitHub 4o. 2024-07-05-github-copilot-enterprise-on-gpt-4o, 2024.\n\nCopilot\n\nGPT- https://github.blog/changelog/\n\nEnterprise\n\non\n\n[16] GitHub Copilot Voice - Write code without\n\nthe https://githubnext.com/projects/\n\nkeyboard. copilot-voice, 2024.\n\n[17] Hello GPT-4o.\n\nhttps://openai.com/index/\n\nhello-gpt-4o/, 2024.\n\n[18] Helping our customers through the CrowdStrike out- age. https://blogs.microsoft.com/blog/2024/ 07/20/helping-our-customers-through-the- crowdstrike-outage, 2024.\n\n14\n\n[19] LeetCode. 2024.\n\nhttps://leetcode.com/problemset/,\n\n[20] neetcode_gh/leetcode.\n\nhttps://github.com/\n\nneetcode-gh/leetcode, 2024.\n\n[21] NetBSD Commit Guidelines. https://www.netbsd.\n\norg/developers/commit-guidelines.html, 2024.\n\n[22] NSRL Test Data. https://www.nist.gov/itl/ssd/ software-quality-group/nsrl-test-data, 2024.\n\n[23] OpenBSD. 2024.\n\nhttps://github.com/openbsd/src,\n\n[24] petewarden/c_hashmap.\n\nhttps://github.com/\n\npetewarden/c_hashmap, 2024.\n\n[25] SCC. https://github.com/boyter/scc, 2024.\n\n[26] Survey reveals AI’s impact on the developer experience. https://github.blog/news-insights/research/ survey-reveals-ais-impact-on-the-developer -experience, 2024.\n\n[27] The Algorithms/C.\n\nhttps://github.com/\n\nTheAlgorithms/C, 2024.\n\n[28] M. Bhatt, S Chennabasappa, C. Nikolaidis, S Wan, I Ev- timov, D. Gabi, D. Song, F. Ahmad, C. Aschermann, L. Fontana, S. Frolov, R. P. Giri, D. Kapil, Y Kozyrakis, D. LeBlanc, J. Milazzo, A. Straumann, G. Synnaeve, V. Vontimitta, S. Whitman, and J. Saxe. Purple llama cyberseceval: A secure coding benchmark for language models, 2023.\n\n[29] C. J. Chong, C. Hou, Z. Yao, and S. M. Seyed Talebi. Casper: Prompt sanitization for protecting user privacy in web-based large language models, 2024.\n\n[30] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In Proc. NIPS, 2017.\n\n[31] M. Dilhara, A. Bellur, T. Bryksin, and D. Dig. Unprece- dented code change automation: The fusion of llms and transformation by example. Proc. ACM Softw. Eng., 1(FSE), jul 2024.\n\n[32] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Re- inforcement learning: A survey. Journal of artificial intelligence research, 4:237–285, 1996.\n\n[33] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neu- big. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021.",
      "content_length": 3689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "[34] Y. Liu, Y. Jia, R. Geng, J. Jia, and N. Z. Gong. For- malizing and benchmarking prompt injection attacks In 33rd USENIX Security Symposium and defenses. (USENIX Security 24), pages 1831–1847, Philadelphia, PA, August 2024. USENIX Association.\n\n[35] V. Liventsev, A. Grishina, A. Härmä, and L. Moonen. Fully autonomous programming with large language models. In Proc. ACM GECCO, 2023.\n\n[36] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024.\n\n[37] Thomas J. McCabe. A complexity measure. In ICSE\n\n’76, page 407, 1976.\n\n[38] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient learning of In Artificial deep networks from decentralized data. intelligence and statistics, pages 1273–1282. PMLR, 2017.\n\n[39] R. Pan, A. R. Ibrahimzada, R. Krishna, D. Sankar, L. P. Wassi, M. Merler, B. Sobolev, R. Pavuluri, S. Sinha, and R. Jabbarvand. Lost in translation: A study of bugs introduced by large language models while translating code. In Proc. IEEE/ACM ICSE, 2024.\n\n[40] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri. Asleep at the keyboard? assessing the se- curity of github copilot’s code contributions. In Proc. IEEE Symposium on Security and Privacy (S&P), 2022.\n\n[41] N. Perry, M. Srivastava, D. Kumar, and D. Boneh. Do users write more insecure code with ai assistants? In Proc. ACM CCS, 2023.\n\n[42] J. Piet, M. Alrashed, C. Sitawarin, S. Chen, Z. Wei, Jatmo: Prompt\n\nE. Sun, B Alomair, and D. Wagner. injection defense by task-specific finetuning, 2024.\n\n[43] V. Raychev, M. Vechev, and E. Yahav. Code completion with statistical language models. In Proc. PLDI, 2014.\n\n[44] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Ba- log, M. P. Kumar, E. Dupont, F. J. R. Ruiz, J. S. Ellen- berg, P. Wang, O. Fawzi, et al. Mathematical discoveries\n\n15\n\nfrom program search with large language models. Na- ture, 625(7995):468–475, 2024.\n\n[45] R. Schuster, C Song, E Tromer, and V Shmatikov. You autocomplete me: Poisoning vulnerabilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21), pages 1559–1575. USENIX As- sociation, August 2021.\n\n[46] Z. Shen, Z. Xi, Y. He, W. Tong, J. Hua, and S. Zhong. The fire thief is also the keeper: Balancing usability and privacy in prompts. arXiv preprint arXiv:2406.14318, 2024.\n\n[47] R. S. Sutton and A. G. Barto. Reinforcement learning:\n\nan introduction. MIT Press, 2018.\n\n[48] P. Vaithilingam, T. Zhang, and E. L. Glassman. Expec- tation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Proc. ACM CHI, 2022.\n\n[49] R. Wang, R. Cheng, D. Ford, and T. Zimmermann. In- vestigating and designing for trust in ai-powered code In The 2024 ACM Conference on generation tools. Fairness,Accountability,and Transparency,pages 1475– 1493, 2024.\n\n[50] S. Yan, S Wang, Y Duan, H Hong, K. Lee, D. Kim, and Y. Hong. An llm-assisted easy-to-trigger backdoor attack on code completion models: Injecting disguised vulnerabilities against strong detection, 2024.\n\n[51] X. Yin, C. Ni, T. N. Nguyen, S. Wang, and X. Yang. Rectifier: Code translation with corrector via llms. arXiv preprint arXiv:2407.07472, 2024.\n\n[52] X. Yue, M. Du, T. Wang, Y. Li, H. Sun, and S. S. M. Chow. Differential privacy for text analytics via natural In Findings of the Association for text sanitization. Computational Linguistics: ACL-IJCNLP 2021, pages 3853–3866, 2021.\n\n[53] A. K. Zhang, N. Perry, R. Dulepet, E. Jones, J. W. Lin, J. Ji, C. Menders, G. Hussein, S. Liu, D. Jasper, et al. Cybench: A framework for evaluating cybersecurity ca- pabilities and risk of language models. arXiv preprint arXiv:2408.08926, 2024.",
      "content_length": 3790,
      "extraction_method": "Unstructured"
    }
  ]
}