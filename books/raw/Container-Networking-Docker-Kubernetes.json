{
  "metadata": {
    "title": "Container-Networking-Docker-Kubernetes",
    "author": "Michael Hausenblas",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 72,
    "conversion_date": "2025-12-19T18:44:22.002739",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Container-Networking-Docker-Kubernetes.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-8)",
      "start_page": 1,
      "end_page": 8,
      "detection_method": "topic_boundary",
      "content": "Michael Hausenblas\nFrom Docker to Kubernetes\nContainer \nNetworking\nCompliments of\n\n\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/products/\nhttps://www.nginx.com/products/\nhttps://www.nginx.com/products/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nFREE TRIAL\nLEARN MORE\nLoad\nBalancing\nCloud\nSecurity\nWeb & Mobile\nPerformance\nLearn more at nginx.com\nAPI\nGateway\nMicroservices\n The NGINX Application Platform \npowers Load Balancers, \nMicroservices & API Gateways\n\n\nMichael Hausenblas\nContainer Networking\nFrom Docker to Kubernetes\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n\n\n978-1-492-03681-4\n[LSI]\nContainer Networking\nby Michael Hausenblas\nCopyright © 2018 O’Reilly Media. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online edi‐\ntions are also available for most titles (http://oreilly.com/safari). For more information, contact our\ncorporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Nikki McDonald\nProduction Editors: Melanie Yarbrough\nand Justin Billing\nCopyeditor: Rachel Head\nProofreader: Charles Roumeliotis\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nMay 2018:\n First Edition\nRevision History for the First Edition\n2018-04-17: First Release\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Container Networking, the cover\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsi‐\nbility for errors or omissions, including without limitation responsibility for damages resulting from\nthe use of or reliance on this work. Use of the information and instructions contained in this work is\nat your own risk. If any code samples or other technology this work contains or describes is subject\nto open source licenses or the intellectual property rights of others, it is your responsibility to ensure\nthat your use thereof complies with such licenses and/or rights.\nThis work is part of a collaboration between O’Reilly and NGINX. See our statement of editorial\nindependence.\n\n\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Motivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nIntroducing Pets Versus Cattle                                                                                 1\nGo Cattle!                                                                                                                     2\nThe Container Networking Stack                                                                            3\nDo I Need to Go “All In”?                                                                                          4\n2. Introduction to Container Networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  5\nSingle-Host Container Networking 101                                                                  5\nModes for Docker Networking                                                                                 7\nAdministrative Considerations                                                                              10\nWrapping It Up                                                                                                         11\n3. Multi-Host Networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  13\nMulti-Host Container Networking 101                                                                 13\nOptions for Multi-Host Container Networking                                                  13\nDocker Networking                                                                                                  15\nAdministrative Considerations                                                                              16\nWrapping It Up                                                                                                         16\n4. Orchestration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\nWhat Does a Scheduler Actually Do?                                                                    19\nDocker                                                                                                                        20\nApache Mesos                                                                                                           21\nHashicorp Nomad                                                                                                    23\nCommunity Matters                                                                                                 25\nWrapping It Up                                                                                                         25\nv\n\n\n5. Service Discovery. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  27\nThe Challenge                                                                                                           27\nTechnologies                                                                                                              28\nLoad Balancing                                                                                                         32\nWrapping It Up                                                                                                         34\n6. The Container Network Interface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\nHistory                                                                                                                       38\nSpecification and Usage                                                                                           38\nContainer Runtimes and Plug-ins                                                                         40\nWrapping It Up                                                                                                         41\n7. Kubernetes Networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  43\nA Gentle Kubernetes Introduction                                                                        43\nKubernetes Networking Overview                                                                        45\nIntra-Pod Networking                                                                                             46\nInter-Pod Networking                                                                                              47\nService Discovery in Kubernetes                                                                            50\nIngress and Egress                                                                                                    53\nAdvanced Kubernetes Networking Topics                                                           55\nWrapping It Up                                                                                                         57\nA. References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59\nvi \n| \nTable of Contents\n\n\nPreface\nWhen you start building your first containerized application, you’re excited\nabout the capabilities and opportunities you encounter: it runs the same in dev\nand in prod, it’s straightforward to put together a container image using Docker,\nand the distribution is taken care of by a container registry.\nSo, you’re satisfied with how quickly you were able to containerize an existing,\nsay, Python app, and now you want to connect it to another container that has a\ndatabase, such as PostgreSQL. Also, you don’t want to have to manually launch\nthe containers and implement your own system that takes care of checking if the\ncontainers are still running and, if not, relaunching them.\nAt this juncture, you might realize there’s a challenge you’re running into: con‐\ntainer networking. Unfortunately, there are still a lot of moving parts in this\ndomain and there are currently few best practice resources available in a central\nplace. Fortunately, there are tons of articles, repos, and recipes available on the\nwider internet and with this book you have a handy way to get access to many of\nthem in a simple and comprehensive format.\nWhy I Wrote This Book\nI thought to myself: what if someone wrote a book providing basic guidance for\nthe container networking topic, pointing readers in the right direction for each of\nthe involved technologies, such as overlay networks, the Container Network\nInterface (CNI), and load balancers?\nThat someone turned out to be me. With this book, I want to provide you with an\noverview of the challenges and available solutions for container networking, con‐\ntainer orchestration, and (container) service discovery. I will try to drive home\nthree points throughout this book:\nvii\n\n\n• Without a proper understanding of the networking aspect of (Docker) con‐\ntainers and a sound strategy in place, you will have more than one bad day\nwhen adopting containers.\n• Service discovery and container orchestration are two sides of the same coin.\n• The space of container networking and service discovery is still relatively\nyoung: you will likely find yourself starting out with one set of technologies\nand then changing gears and trying something else. Don’t worry, you’re in\ngood company.\nWho Is This Book For?\nMy hope is that you’ll find the book useful if one or more of the following applies\nto you:\n• You are a software developer who drank the (Docker) container Kool-Aid.\n• You work in network operations and want to brace yourself for the upcom‐\ning onslaught of your enthusiastic developer colleagues.\n• You are an aspiring Site Reliability Engineer (SRE) who wants to get into the\ncontainer business.\n• You are an (enterprise) software architect who is in the process of migrating\nexisting workloads to a containerized setup.\nLast but not least, distributed application developers and backend engineers\nshould also be able to extract some value out of it.\nNote that this is not a hands-on book. Besides some single-host Docker network‐\ning stuff in Chapter 2 and some of the material about Kubernetes in Chapter 7, I\ndon’t show a lot of commands or source code; consider this book more like a\nguide, a heavily annotated bookmark collection. You will also want to use it to\nmake informed decisions when planning and implementing containerized appli‐\ncations.\nAbout Me\nI work at Red Hat in the OpenShift team, where I help devops to get the most out\nof the software. I spend my time mainly upstream—that is, in the Kubernetes\ncommunity, for example in the Autoscaling, Cluster Lifecycle, and Apps Special\nInterest Groups (SIGs).\nBefore joining Red Hat in the beginning of 2017 I spent some two years at Meso‐\nsphere, where I also did containers, in the context of (surprise!) Mesos. I also\nhave a data engineering background, having worked as Chief Data Engineer at\nviii \n| \nPreface\n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 9-20)",
      "start_page": 9,
      "end_page": 20,
      "detection_method": "topic_boundary",
      "content": "MapR Inc. prior to Mesosphere, mainly on distributed query engines and data‐\nstores as well as building data pipelines.\nLast but not least, I’m a pragmatist and tried my best throughout the book to\nmake sure to be unbiased toward the technologies discussed here.\nAcknowledgments\nA big thank you to the O’Reilly team, especially Virginia Wilson. Thanks for your\nguidance and feedback on the first iteration of the book (back then called Docker\nNetworking and Service Discovery), which came out in 2015, and for putting up\nwith me again.\nA big thank you to Nic (Sheriff) Jackson of HashiCorp for your time around\nNomad. You rock, dude!\nThanks a million Bryan Boreham of Weaveworks! You provided super-valuable\nfeedback and I appreciate your suggestions concerning the flow as well as your\ndiligence, paying attention to details and calling me out when I drifted off and/or\nmade mistakes. Bryan, who’s a container networking expert and CNI 7th dan, is\nthe main reason this book in its final version turned out to be a pretty good read\n(I think).\nLast but certainly not least, my deepest gratitude to my awesome and supportive\nfamily: our two girls Saphira (aka The Real Unicorn—love you hun :) and Ranya\n(whose talents range from Scratch programming to Irish Rugby), our son Iannis\n(sigh, told you so, you ain’t gonna win the rowing championship with a broken\nhand, but you’re still dope), and my wicked smart and fun wife Anneliese (did I\nempty the washing machine? Not sure!).\nPreface \n| \nix\n\n\n1 In all fairness, Randy did attribute the origins to Bill Baker of Microsoft.\nCHAPTER 1\nMotivation\nIn this chapter I’ll introduce you to the pets versus cattle approach concerning\ncompute infrastructure as well as what container networking entails. It sets the\nscene, and if you’re familiar with the basics you may want to skip this chapter.\nIntroducing Pets Versus Cattle\nIn February 2012, Randy Bias gave an impactful talk on architectures for open\nand scalable clouds. In his presentation, he established the pets versus cattle\nmeme:1\n• With the pets approach to infrastructure, you treat the machines as individu‐\nals. You give each (virtual) machine a name, and applications are statically\nallocated to machines. For example, db-prod-2 is one of the production\nservers for a database. The apps are manually deployed, and when a machine\ngets ill you nurse it back to health and manually redeploy the app it ran onto\nanother machine. This approach is generally considered to be the dominant\nparadigm of a previous (non–cloud native) era.\n• With the cattle approach to infrastructure, your machines are anonymous;\nthey are all identical (modulo hardware upgrades), they have numbers rather\nthan names, and apps are automatically deployed onto any and each of the\nmachines. When one of the machines gets ill, you don’t worry about it\nimmediately; you replace it—or parts of it, such as a faulty hard disk drive—\nwhen you want and not when things break.\n1\n\n\n2 Typically even heterogeneous hardware. For example, see slide 7 of Thorvald Natvig’s talk “Challenging\nFundamental Assumptions of Datacenters: Decoupling Infrastructure from Hardware” from Velocity\n2015.\nWhile the original meme was focused on virtual machines, we apply the cattle\napproach to infrastructure.\nGo Cattle!\nThe beautiful thing about applying the cattle approach to infrastructure is that it\nallows you to scale out on commodity hardware.2\nIt gives you elasticity with the implication of hybrid cloud capabilities. This is a\nfancy way of saying that you can have parts of your deployments on premises and\nburst into the public cloud—using services provided by the likes of Amazon,\nMicrosoft, and Google, or the infrastructure-as-a-service (IaaS) offerings of dif‐\nferent provides like VMware—if and when you need to.\nMost importantly, from an operator’s point of view, the cattle approach allows\nyou to get a decent night’s sleep, as you’re no longer paged at 3 a.m. just to replace\na broken hard disk drive or to relaunch a hanging app on a different server, as\nyou would have done with your pets.\nHowever, the cattle approach poses some challenges that generally fall into one of\nthe following two categories:\nSocial challenges\nI dare say most of the challenges are of a social nature: How do I convince\nmy manager? How do I get buy-in from my CTO? Will my colleagues oppose\nthis new way of doing things? Does this mean we will need fewer people to\nmanage our infrastructure?\nI won’t pretend to offer ready-made solutions for these issues; instead, go\nbuy a copy of The Phoenix Project by Gene Kim, Kevin Behr, and George\nSpafford (O’Reilly), which should help you find answers.\nTechnical challenges\nThis category includes issues dealing with things like base provisioning of\nthe machines—e.g., using Ansible to install Kubernetes components, how to\nset up the communication links between the containers and to the outside\nworld, and most importantly, how to ensure the containers are automatically\ndeployed and are discoverable.\nNow that you know about pets versus cattle, you are ready to have a look at the\noverall container networking stack.\n2 \n| \nChapter 1: Motivation\n\n\nThe Container Networking Stack\nThe overall stack we’re dealing with here is comprised of the following:\nThe low-level networking layer\nThis includes networking gear, iptables, routing, IPVLAN, and Linux\nnamespaces. You usually don’t need to know the details of this layer unless\nyou’re on the networking team, but you should at least be aware of it. Note\nthat the technologies here have existed and been used for a decade or more.\nThe container networking layer\nThis layer provides some abstractions, such as the single-host bridge net‐\nworking mode and the multi-host, IP-per-container solution. I cover this\nlayer in Chapters 2 and 3.\nThe container orchestration layer\nHere, we’re marrying the container scheduler’s decisions on where to place a\ncontainer with the primitives provided by lower layers. In Chapter 4 we look\nat container orchestration systems in general, and in Chapter 5 we focus on\nthe service discovery aspect, including load balancing. Chapter 6 deals with\nthe container networking standard, CNI, and finally in Chapter 7 we look at\nKubernetes networking.\nSoftware-Defined Networking (SDN)\nSDN is really an umbrella (marketing) term, providing essentially the same\nadvantages to networks that virtual machines (VMs) introduced over bare-metal\nservers. With this approach, the network administration team becomes more\nagile and can react faster to changing business requirements. Another way to\nview it is this: SDN is the configuration of networks using software, whether that\nis via APIs, complementing network function virtualization, or the construction\nof networks from software.\nEspecially if you’re a developer or an architect, I suggest taking a quick look at\nCisco’s nice overview on this topic as well as SDxCentral’s article, “What Is\nSoftware-Defined Networking (SDN)?”\nIf you are on the network operations team, you’re probably good to go for the\nnext chapter. However, if you’re an architect or developer and your networking\nknowledge might be a bit rusty, I suggest brushing up by studying the Linux Net‐\nwork Administrators Guide before advancing.\nThe Container Networking Stack \n| \n3\n\n\nDo I Need to Go “All In”?\nOftentimes, when I’m at conferences or user groups, I meet people who are very\nexcited about the opportunities in the container space. At the same time, folks\nrightfully worry about how deeply they need to commit to containers in order to\nbenefit from them. The following table provides an informal overview of deploy‐\nments I have seen in the wild, grouped by level of commitment expressed via\nstages:\nStage\nTypical setup\nExamples\nTraditional\nBare metal or VM, no containers\nMajority of today’s prod\ndeployments\nSimple\nManually launched containers used for app-level dependency\nmanagement\nDevelopment and test\nenvironments\nAd hoc\nA custom, homegrown scheduler to launch and potentially restart\ncontainers\nRelateIQ, Uber\nFull-blown\nAn established scheduler from Chapter 4 to manage containers; fault\ntolerant, self-healing\nGoogle, Zulily, Gutefrage.de\nNote that the stage doesn’t necessarily correspond with the size of the deploy‐\nment. For example, Gutefrage.de only has six bare-metal servers under manage‐\nment but uses Apache Mesos to manage them, and you can run a Kubernetes\ncluster easily on a Raspberry Pi.\nOne last remark before we move on: by now, you might have realized that we are\ndealing with distributed systems in general here. Given that we will usually want\nto deploy containers into a network of computers, may I suggest reading up on\nthe fallacies of distributed computing, in case you are not already familiar with\nthis topic?\nAnd now let’s move on to the deep end of container networking.\n4 \n| \nChapter 1: Motivation\n\n\nCHAPTER 2\nIntroduction to Container Networking\nThis chapter focuses on networking topics for single-host container networking,\nwith an emphasis on Docker. We’ll also have a look at administrative challenges\nsuch as IP address management and security considerations. In Chapter 3, we\nwill discuss multi-host scenarios.\nSingle-Host Container Networking 101\nA container needs a host to run on. This can be a physical machine, such as a\nbare-metal server in your on-premises datacenter, or a virtual machine, either on\npremises or in the cloud.\nIn the case of a Docker container the host has a daemon and a client running, as\ndepicted in Figure 2-1, enabling you to interact with a container registry. Further,\nyou can pull/push container images and start, stop, pause, and inspect containers. \nNote that nowadays most (if not all) containers are compliant with the Open\nContainer Initiative (OCI), and alongside Docker there are interesting alterna‐\ntives, especially in the context of Kubernetes, available.\n5\n\n\nFigure 2-1. Simplified Docker architecture for a single host\nThe relationship between a host and containers is 1:N. This means that one host\ntypically has several containers running on it. For example, Facebook reports that\n—depending on how beefy the machine is—it sees on average some 10 to 40 con‐\ntainers per host running.\nNo matter if you have a single-host deployment or use a cluster of machines, you\nwill likely have to deal with networking:\n• For single-host deployments, you almost always have the need to connect to\nother containers on the same host; for example, an application server like\nWildFly might need to connect to a database.\n• In multi-host deployments, you need to consider two aspects: how containers\nare communicating within a host and how the communication paths look\nbetween different hosts. Both performance considerations and security\naspects will likely influence your design decisions. Multi-host deployments\nusually become necessary either when the capacity of a single host is insuffi‐\ncient, for resilience reasons, or when one wants to employ distributed sys‐\ntems such as Apache Spark or Apache Kafka.\nDistributed Systems and Data Locality\nThe basic idea behind using a distributed system (for computation or storage) is\nto benefit from parallel processing, usually together with data locality. By data\nlocality I mean the principle of shipping the code to where the data is rather than\nthe (traditional) other way around.\n6 \n| \nChapter 2: Introduction to Container Networking\n\n\nThink about the following for a moment: if your dataset size is in TB scale and\nyour code size is in MB scale, it’s more efficient to move the code across the clus‐\nter than it would be to transfer all the data to a central processing place. In addi‐\ntion to being able to process things in parallel, you usually gain fault tolerance\nwith distributed systems, as parts of the system can continue to work more or\nless independently.\nSimply put, Docker networking is the native container SDN solution you have at\nyour disposal when working with Docker.\nModes for Docker Networking\nIn a nutshell, there are four single-host networking modes available for Docker:\nBridge mode\nUsually used for apps running in standalone containers; this is the default\nnetwork driver. See “Bridge Mode Networking” on page 7 for details.\nHost mode\nAlso used for standalone containers; removes network isolation to the host.\nSee “Host Mode Networking” on page 8 for details.\nContainer mode\nLets you reuse the network namespace of another container. Used in Kuber‐\nnetes. See “Container Mode Networking” on page 9 for details.\nNo networking\nDisables support for networking from the Docker side and allows you to, for\nexample, set up custom networking. See “No Networking” on page 10 for\ndetails.\nWe’ll take a closer look at each of these modes now, and end this chapter with\nsome administrative considerations, including IP/port management and security.\nBridge Mode Networking\nIn this mode (see Figure 2-2), the Docker daemon creates docker0, a virtual \nEthernet bridge that automatically forwards packets between any other network\ninterfaces that are attached to it. By default, the daemon then connects all con‐\ntainers on a host to this internal network by creating a pair of peer interfaces,\nassigning one of the peers to become the container’s eth0 interface and placing\nthe other peer in the namespace of the host, as well as assigning an IP address/\nsubnet from the private IP range to the bridge. Here’s an example of using bridge\nmode:\nModes for Docker Networking \n| \n7\n\n\n$ docker run -d -P --net=bridge nginx:1.9.1\n$ docker ps\nCONTAINER ID   IMAGE                  COMMAND    CREATED\nSTATUS         PORTS                  NAMES\n17d447b7425d   nginx:1.9.1            nginx -g   19 seconds ago\nUp 18 seconds  0.0.0.0:49153->443/tcp,\n               0.0.0.0:49154->80/tcp  trusting_feynman\nBecause bridge mode is the Docker default, you could have used docker\nrun -d -P nginx:1.9.1 in the previous command instead. If you do\nnot use the -P argument, which publishes all exposed ports of the con‐\ntainer, or -p <host_port>:<container_port>, which publishes a spe‐\ncific port, the IP packets will not be routable to the container outside of\nthe host.\nFigure 2-2. Bridge mode networking setup\nHost Mode Networking\nThis mode effectively disables network isolation of a Docker container. Because\nthe container shares the network namespace of the host, it may be directly\nexposed to the public network if the host network is not firewalled. As a conse‐\nquence of the shared namespace, you need to manage port allocations somehow.\nHere’s an example of host mode networking in action:\n$ docker run -d --net=host ubuntu:14.04 tail -f /dev/null\n$ ip addr | grep -A 2 eth0:\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group\ndefault qlen 1000\n    link/ether 06:58:2b:07:d5:f3 brd ff:ff:ff:ff:ff:ff\n    inet **10.0.7.197**/22 brd 10.0.7.255 scope global dynamic eth0\n$ docker ps\nCONTAINER ID  IMAGE         COMMAND  CREATED\n8 \n| \nChapter 2: Introduction to Container Networking\n\n\nSTATUS        PORTS         NAMES\nb44d7d5d3903  ubuntu:14.04  tail -f  2 seconds ago\nUp 2 seconds                jovial_blackwell\n$ docker exec -it b44d7d5d3903 ip addr\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group\ndefault qlen 1000\n    link/ether 06:58:2b:07:d5:f3 brd ff:ff:ff:ff:ff:ff\n    inet **10.0.7.197**/22 brd 10.0.7.255 scope global dynamic eth0\nAnd there we have it: the container has the same IP address as the host, namely\n10.0.7.197.\nIn Figure 2-3 we see that when using host mode networking, the container effec‐\ntively inherits the IP address from its host. This mode is faster than the bridge\nmode because there is no routing overhead, but it exposes the container directly\nto the public network, with all its security implications.\nFigure 2-3. Docker host mode networking setup\nContainer Mode Networking\nIn this mode, you tell Docker to reuse the network namespace of another con‐\ntainer. In general, this mode is useful if you want to have fine-grained control\nover the network stack and/or to control its lifecycle. In fact, Kubernetes net‐\nworking uses this mode, and you can read more about it in Chapter 7. Here it is\nin action:\n$ docker run -d -P --net=bridge nginx:1.9.1\n$ docker ps\nCONTAINER ID  IMAGE        COMMAND   CREATED         STATUS\nPORTS                      NAMES\neb19088be8a0  nginx:1.9.1  nginx -g  3 minutes ago   Up 3 minutes\n0.0.0.0:32769->80/tcp,\n0.0.0.0:32768->443/tcp     admiring_engelbart\n$ docker exec -it admiring_engelbart ip addr\n8: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state\nModes for Docker Networking \n| \n9\n\n\nUP group default\n    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\n    inet **172.17.0.3**/16 scope global eth0\n$ docker run -it --net=container:admiring_engelbart ubuntu:14.04 ip addr\n...\n8: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state\nUP group default\n    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\n    inet **172.17.0.3**/16 scope global eth0\nThe result as shown in this example is what we would have expected: the second\ncontainer, started with --net=container, has the same IP address as the first\ncontainer (namely 172.17.0.3), with the glorious autoassigned name admir\ning_engelbart.\nNo Networking\nThis mode puts the container inside its own network namespace but doesn’t con‐\nfigure it. Effectively, this turns off networking and is useful for two cases: for con‐\ntainers that don’t need a network, such as batch jobs writing to a disk volume, or\nif you want to set up your own custom networking (see Chapter 3 for a number\nof options that leverage this). Here’s an example:\n$ docker run -d -P --net=none nginx:1.9.1\n$ docker ps\nCONTAINER ID  IMAGE          COMMAND   CREATED\nSTATUS        PORTS          NAMES\nd8c26d68037c  nginx:1.9.1    nginx -g  2 minutes ago\nUp 2 minutes                 grave_perlman\n$  docker inspect d8c26d68037c | grep IPAddress\n    \"IPAddress\": \"\",\n    \"SecondaryIPAddresses\": null,\nAs this example shows, there is no network configured, precisely as we would\nhave expected.\nYou can read more about networking and learn about configuration options via\nthe Docker docs.\nAdministrative Considerations\nWe will now briefly discuss other aspects you should be aware of from an admin‐\nistrative point of view. Most of these issues are equally relevant for multi-host\ndeployments:\n10 \n| \nChapter 2: Introduction to Container Networking\n\n\n1 New Relic, for example, found the majority of the overall uptime of the containers in one particular\nsetup in the low minutes; see also the update here.\nAllocating IP addresses\nManually allocating IP addresses when containers come and go frequently\nand in large numbers is not sustainable.1 The bridge mode takes care of this\nissue to a certain extent. To prevent ARP collisions on a local network, the\nDocker daemon generates a MAC address from the allocated IP address.\nManaging ports\nThere are two approaches to managing ports: fixed port allocation or\ndynamic allocation of ports. The allocation can be per service (or applica‐\ntion) or it can be applied as a global strategy. For bridge mode, Docker can\nautomatically assign (UDP or TCP) ports and consequently make them rout‐\nable. Systems like Kubernetes that sport a flat, IP-per-container networking\nmodel don’t suffer from this issue.\nNetwork security\nOut of the box, Docker has inter-container communication enabled (mean‐\ning the default is --icc=true). This means containers on a host can commu‐\nnicate with each other without any restrictions, which can potentially lead to \ndenial-of-service attacks. Further, Docker controls the communication\nbetween containers and the wider world through the --ip_forward and\n--iptables flags. As a good practice, you should study the defaults of these\nflags and loop in your security team concerning company policies and how\nto reflect them in the Docker daemon setup.\nSystems like CRI-O, the Container Runtime Interface (CRI) using OCI, offer\nalternative runtimes that don’t have one big daemon like Docker has and\npotentially expose a smaller attack surface.\nAnother network security aspect is that of on-the-wire encryption, which\nusually means TLS/SSL as per RFC 5246. \nWrapping It Up\nIn this chapter, we had a look at the four basic single-host networking modes and\nrelated admin issues. Now that you have a basic understanding of the single-host\ncase, let’s have a look at a likely more interesting case: multi-host container net‐\nworking.\nWrapping It Up \n| \n11\n",
      "page_number": 9
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 21-29)",
      "start_page": 21,
      "end_page": 29,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 3\nMulti-Host Networking\nAs long as you’re using containers on a single host, the techniques introduced in\nChapter 2 are sufficient. However, if the capacity of a single host is not enough to\nhandle your workload or you want more resilience, you’ll want to scale out hori‐\nzontally.\nMulti-Host Container Networking 101\nWhen scaling out horizontally you end up with a network of machines, also\nknown as a cluster of machines, or cluster for short. Now, a number of questions\narise: How do containers talk to each other on different hosts? How do you con‐\ntrol communication between containers, and with the outside world? How do\nyou keep state, such as IP address assignments, consistent in a cluster? What are\nthe integration points with the existing networking infrastructure? What about\nsecurity policies?\nIn order to address these questions, we’ll review technologies for multi-host con‐\ntainer networking in the remainder of this chapter. Since different use cases and\nenvironments have different requirements, I will abstain from providing a rec‐\nommendation for a particular project or product. You should be aware of the\ntrade-offs and make an informed decision.\nOptions for Multi-Host Container Networking\nIn a nutshell, Docker itself offers support for overlay networks (creating a dis‐\ntributed network across hosts on top of the host-specific network) as well as net‐\nwork plug-ins for third-party providers.\nThere are a number of multi-host container networking options that are often\nused in practice, especially in the context of Kubernetes. These include:\n13\n\n\n• Flannel by CoreOS (see “flannel” on page 14)\n• Weave Net by Weaveworks (see “Weave Net” on page 14)\n• Metaswitch’s Project Calico (see “Project Calico” on page 14)\n• Open vSwitch from the OpenStack project (see “Open vSwitch” on page 15)\n• OpenVPN (see “OpenVPN” on page 15)\nIn addition, Docker offers multi-host networking natively; see “Docker Network‐\ning” on page 15 for details.\nflannel\nCoreOS’s flannel is a virtual network that assigns a subnet to each host for use\nwith container runtimes. Each container—or pod, in the case of Kubernetes—has\na unique, routable IP inside the cluster. flannel supports a range of backends,\nsuch as VXLAN, AWS VPC, and the default layer 2 UDP network. The advantage\nof flannel is that it reduces the complexity of doing port mapping. For example, \nRed Hat’s Project Atomic uses flannel.\nWeave Net\nWeaveworks’s WeaveNet creates a virtual network that connects Docker contain‐\ners deployed across multiple hosts. Applications use the network just as if the\ncontainers were all plugged into the same network switch, with no need to con‐\nfigure port mappings and links. Services provided by application containers on\nthe Weave network can be made accessible to the outside world, regardless of\nwhere those containers are running.\nSimilarly, existing internal systems can be exposed to application containers irre‐\nspective of their location. Weave can traverse firewalls and operate in partially\nconnected networks. Traffic can be encrypted, allowing hosts to be connected\nacross an untrusted network. You can learn more about Weave’s discovery fea‐\ntures in the blog post “Automating Weave Deployment on Docker Hosts with\nWeave Discovery” by Alvaro Saurin.\nIf you want to give Weave a try, check out its Katacoda scenarios.\nProject Calico\nMetaswitch’s Project Calico uses standard IP routing—to be precise, the venera‐\nble Border Gateway Protocol (BGP), as defined in RFC 1105—and networking\ntools to provide a layer 3 solution. In contrast, most other networking solutions\nbuild an overlay network by encapsulating layer 2 traffic into a higher layer.\nThe primary operating mode requires no encapsulation and is designed for data‐\ncenters where the organization has control over the physical network fabric.\n14 \n| \nChapter 3: Multi-Host Networking\n\n\nSee also Canal, which combines Calico’s network policy enforcement with the\nrich superset of Calico and flannel overlay and nonoverlay network connectivi‐\nties.\nOpen vSwitch\nOpen vSwitch is a multilayer virtual switch designed to enable network automa‐\ntion through programmatic extension while supporting standard management\ninterfaces and protocols, such as NetFlow, IPFIX, LACP, and 802.1ag. In addi‐\ntion, it is designed to support distribution across multiple physical servers and is\nused in Red Hat’s Kubernetes distro OpenShift, the default switch in Xen, KVM,\nProxmox VE, and VirtualBox. It has also been integrated into many private cloud\nsystems, such as OpenStack and oVirt.\nOpenVPN\nOpenVPN, another OSS project that has a commercial offering, allows you to\ncreate virtual private networks (VPNs) using TLS. These VPNs can also be used\nto securely connect containers to each other over the public internet. If you want\nto try out a Docker-based setup, I suggest taking a look at DigitalOcean’s “How to\nRun OpenVPN in a Docker Container on Ubuntu 14.04” walk-through tutorial. \nDocker Networking\nDocker 1.9 introduced a new docker network command. With this, containers\ncan dynamically connect to other networks, with each network potentially\nbacked by a different network driver.\nIn March 2015, Docker Inc. acquired the SDN startup SocketPlane and rebran‐\nded its product as the Overlay Driver. Since Docker 1.9, this is the default for\nmulti-host networking. The Overlay Driver extends the normal bridge mode\nwith peer-to-peer communication and uses a pluggable key-value store backend\nto distribute cluster state, supporting Consul, etcd, and ZooKeeper.\nTo learn more, I suggest checking out the following blog posts:\n• Aleksandr Tarasov’s “Splendors and Miseries of Docker Network”\n• Project Calico’s “Docker 1.9 Includes Network Plugin Support and Calico Is\nReady!”\n• Weaveworks’s “Life and Docker Networking – One Year On”\nDocker Networking \n| \n15\n\n\nAdministrative Considerations\nIn the last section of this chapter we will discuss some administrative aspects you\nshould be aware of:\nIPVLAN\nLinux kernel version 3.19 introduced an IP-per-container feature. This\nassigns each container on a host a unique and routable IP address. Effec‐\ntively, IPVLAN takes a single network interface and creates multiple virtual\nnetwork interfaces with different MAC addresses assigned to them.\nThis feature, which was contributed by Mahesh Bandewar of Google, is con‐\nceptually similar to the macvlan driver but is more flexible because it’s oper‐\nating both on L2 and L3. If your Linux distro already has a kernel > 3.19,\nyou’re in luck. Otherwise, you cannot yet benefit from this feature.\nIP address management (IPAM)\nOne of the key challenges of multi-host networking is the allocation of IP\naddresses to containers in a cluster. There are two strategies one can pursue:\neither find a way to realize it in your existing (corporate) network or spawn\nan orthogonal, practically hidden networking layer (that is, an overlay net‐\nwork). Note that with IPv6 this situation is relaxed, since it should be a lot\neasier to find a free address space.\nOrchestration tool compatibility\nMany of the multi-host networking solutions discussed in this chapter are\neffectively coprocesses wrapping the Docker API and configuring the net‐\nwork for you. This means that before you select one, you should make sure\nto check for any compatibility issues with the container orchestration tool\nyou’re using. You’ll find more on this topic in Chapter 4.\nIPv4 versus IPv6\nTo date, most Docker deployments use the standard IPv4, but IPv6 is wit‐\nnessing some uptake. Docker has supported IPv6 since v1.5, released in Feb‐\nruary 2015; however, the IPv6 support in Kubernetes is not yet complete.\nThe ever-growing address shortage in IPv4-land might encourage more IPv6\ndeployments down the line, also getting rid of network address translation\n(NAT), but it is unclear when exactly the tipping point will be reached.\nWrapping It Up\nIn this chapter, we reviewed multi-host networking options and touched on\nadmin issues such as IPAM and orchestration. At this juncture you should have a\ngood understanding of the low-level single-host and multi-host networking\noptions and their challenges. Let’s now move on to container orchestration, look‐\ning at how it depends on networking and how it interacts with it.\n16 \n| \nChapter 3: Multi-Host Networking\n\n\nCHAPTER 4\nOrchestration\nWith the cattle approach to managing infrastructure, you don’t manually allocate\ncertain machines for running an application. Instead, you leave it up to an\norchestrator to manage the life cycle of your containers. In Figure 4-1, you can\nsee that container orchestration includes a range of functions, including but not\nlimited to:\n• Organizational primitives, such as labels in Kubernetes, to query and group\ncontainers\n• Scheduling of containers to run on a host\n• Automated health checks to determine if a container is alive and ready to\nserve traffic and to relaunch it if necessary\n• Autoscaling (that is, increasing or decreasing the number of containers based\non utilization or higher-level metrics)\n• Upgrade strategies, from rolling updates to more sophisticated techniques\nsuch as A/B and canary deployments\n• Service discovery to determine which host a scheduled container ended\nupon, usually including DNS support\n17\n\n\nFigure 4-1. Orchestration and its constituents\nSometimes considered part of orchestration but outside the scope of this book is\nthe topic of base provisioning—that is, installing or upgrading the local operating\nsystem on a node or setting up the container runtime there.\nService discovery (covered in greater detail in Chapter 5) and scheduling are\nreally two sides of the same coin. The scheduler decides where in a cluster a con‐\ntainer is placed and supplies other parts with an up-to-date mapping in the form\ncontainers -> locations. This mapping can then be represented in various\nways, be it in a distributed key-value store such as etcd, via DNS, or through\nenvironment variables.\nIn this chapter we will discuss networking and service discovery from the point\nof view of the following container orchestration solutions: Docker Swarm and\nswarm mode, Apache Mesos, and HashiCorp Nomad. These three are (along\nwith Kubernetes, which we will cover in detail in Chapter 7) alternatives your\norganization may already be using, and hence, for the sake of completeness, it’s\nworth exploring them here. To make it clear, though, as of early 2018 the indus‐\ntry has standardized on Kubernetes as the portable way of doing container\norchestration.\nIn addition to the three orchestrators discussed in this chapter, there are\nother (closed source) solutions out there you could have a look at,\nincluding Facebook’s Bistro or hosted solutions such as Amazon ECS.\nShould you want to more fully explore the topic of distributed system\nscheduling, I suggest reading Google’s research papers on Borg and\nOmega.\n18 \n| \nChapter 4: Orchestration\n\n\nBefore we dive into container orchestration systems, though, let’s step back and\nreview what the scheduler—which is the core component of orchestration—\nactually does in the context of containerized workloads.\nWhat Does a Scheduler Actually Do?\nA scheduler for a distributed system takes an application—binary or container\nimage—as requested by a user and places it on one or more of the available hosts.\nFor example, a user might request to have 100 instances of the app running, so\nthe scheduler needs to find space (CPU, RAM) to run these 100 instances on the\navailable hosts.\nIn the case of a containerized setup, this means that the respective container\nimage must exist on a host (if not, it must be pulled from a container registry\nfirst), and the scheduler must instruct the container runtime on that host to\nlaunch a container based on the image.\nLet’s look at a concrete example. In Figure 4-2, you can see that the user reques‐\nted three instances of the app running in the cluster. The scheduler decides the\nplacement based on its knowledge of the state of the cluster. The cluster state may\ninclude the utilization of the machines, the resources necessary to successfully\nlaunch the app, and constraints such as launch this app only on a machine that is\nSSD-backed.\nFurther, quality of service might be taken into account for the placement deci‐\nsion; see Michael Gasch’s great article “QoS, Node allocatable and the Kubernetes\nScheduler” for more details.\nFigure 4-2. Distributed system scheduler in action\nIf you want to learn more about scheduling in distributed systems I suggest you\ncheck out the excellent resource “Cluster Management at Google” by John\nWilkes.\nWhat Does a Scheduler Actually Do? \n| \n19\n\n\n1 Essentially, this means that you can simply keep using docker run commands and the deployment of\nyour containers in a cluster happens automagically.\nBeware of the semantics of constraints that you can place on scheduling\ncontainers. For example, I once gave a demo using Marathon that\nwouldn’t work as planned because I screwed up the placement con‐\nstraints: I used a combination of unique hostname and a certain role,\nand it wouldn’t scale because there was only one node with the specified\nrole in the cluster. The same thing can happen with Kubernetes labels.\nDocker\nDocker at the time of writing uses the so-called swarm mode in a distributed set‐\nting, whereas previous to Docker 1.12 the standalone Docker Swarm model was\nused. We will discuss both here.\nSwarm Mode\nSince Docker 1.12, swarm mode has been integrated with Docker Engine. The\norchestration features embedded in Docker Engine are built using SwarmKit.\nA swarm in Docker consists of multiple hosts running in swarm mode and acting\nas managers and workers—hosts can be managers, workers, or perform both\nroles at once. A task is a running container that is part of a swarm service and\nmanaged by a swarm manager, as opposed to a standalone container. A service in\nthe context of Docker swarm mode is a definition of the tasks to execute on the\nmanager or worker nodes. Docker works to maintain that desired state; for\nexample, if a worker node becomes unavailable, Docker schedules the tasks onto\nanother host.\nDocker running in swarm mode doesn’t prevent you from running standalone\ncontainers on any of the hosts participating in the swarm. The essential differ‐\nence between standalone containers and swarm services is that only swarm man‐\nagers can manage a swarm, while standalone containers can be started on any\nhost.\nTo learn more about Docker’s swarm mode, check out the official “Getting\nStarted with Swarm Mode” tutorial or check out the Katacoda “Docker Orches‐\ntration – Getting Started with Swarm Mode” scenario.\nDocker Swarm\nDocker historically had a native clustering tool called Docker Swarm. Docker\nSwarm builds upon the Docker API1 and works as follows: there’s one Swarm\n20 \n| \nChapter 4: Orchestration\n\n\nmanager that’s responsible for scheduling, and on each host an agent runs that\ntakes care of the local resource management (Figure 4-3).\nFigure 4-3. Docker Swarm architecture, based on the T-Labs presentation “Swarm –\nA Docker Clustering System”\nDocker Swarm supports different backends: etcd, Consul, and ZooKeeper. You\ncan also use a static file to capture your cluster state with Swarm, and recently a\nDNS-based service discovery tool for Swarm called wagl has been introduced.\nOut of the box, Docker provides a basic service discovery mechanism\nfor single-node deployments called Docker links. Linking allows a user\nto let any container discover both the IP address and exposed ports of\nother Docker containers on the same host. In order to accomplish this,\nDocker provides the --link flag. But hard-wiring of links between con‐\ntainers is neither fun nor scalable. In fact, it’s so bad that this feature has\nbeen deprecated.\nApache Mesos\nApache Mesos (Figure 4-4) is a general-purpose cluster resource manager that\nabstracts the resources of a cluster (CPU, RAM, etc.) in such a way that the clus‐\nter appears like one giant computer to the developer. In a sense, Mesos acts like\nthe kernel of a distributed operating system. It is hence never used on its own,\nbut always together with so-called frameworks such as Marathon (for long-\nrunning stuff like a web server) or Chronos (for batch jobs), or big data and fast\ndata frameworks like Apache Spark or Apache Cassandra.\nApache Mesos \n| \n21\n",
      "page_number": 21
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 30-38)",
      "start_page": 30,
      "end_page": 38,
      "detection_method": "topic_boundary",
      "content": "Figure 4-4. Apache Mesos architecture at a glance\nMesos supports both containerized workloads (that is, running Docker contain‐\ners) and plain executables (for example, bash scripts or Linux ELF format binar‐\nies for both stateless and stateful services).\nIn the following discussion, I’m assuming you’re familiar with Mesos and its ter‐\nminology. If you’re new to Mesos, I suggest checking out David Greenberg’s won‐\nderful book Building Applications on Mesos (O’Reilly), a gentle introduction to\nthis topic that’s particularly useful for distributed application developers.\nThe networking characteristics and capabilities mainly depend on the Mesos\ncontainerizer used:\n• For the Mesos containerizer there are a few prerequisites, such as having a\nLinux Kernel version > 3.16 and libnl installed. You can then build a Mesos\nagent with network isolator support enabled. At launch, you would use\nsomething like the following:\n$mesos-slave --containerizer=mesos\n --isolation=network/port_mapping\n --resources=ports:[31000-32000];ephemeral_ports:[33000-35000]\nThis would configure the Mesos agent to use nonephemeral ports in the\nrange from 31,000 to 32,000 and ephemeral ports in the range from 33,000 to\n22 \n| \nChapter 4: Orchestration\n\n\n35,000. All containers share the host’s IP, and the port ranges are spread over\nthe containers (with a 1:1 mapping between destination port and container\nID). With the network isolator, you also can define performance limitations\nsuch as bandwidth, and it enables you to perform per-container monitoring\nof the network traffic. See Jie Yu’s MesosCon 2015 talk “Per Container Net‐\nwork Monitoring and Isolation in Mesos” for more details on this topic.\n• For the Docker containerizer, see Chapter 2.\nNote that Mesos supports IP-per-container since version 0.23. If you want to\nlearn more about Mesos networking check out Christos Kozyrakis and Spike\nCurtis’s “Mesos Networking” talk from MesosCon 2015.\nWhile Mesos is not opinionated about service discovery, there is a Mesos-specific\nsolution that is often used in practice: Mesos-DNS (see “Pure-Play DNS-Based\nSolutions” on page 31). There are also a multitude of emerging solutions, such as\ntraefik (see “Wrapping It Up” on page 34) that are integrated with Mesos and\ngaining traction.\nBecause Mesos-DNS is the recommended default service discovery\nmechanism with Mesos, it’s important to pay attention to how Mesos-\nDNS represents tasks. For example, a running task might have the (logi‐\ncal) service name webserver.marathon.mesos, and you can find out the\nport allocations via DNS SRV records.\nIf you want to try out Mesos online for free you can use the Katacoda “Deploying\nContainers to DC/OS” scenario.\nHashicorp Nomad\nNomad is a cluster scheduler by HashiCorp, the makers of Vagrant. It was intro‐\nduced in September 2015 and primarily aims at simplicity. The main idea is that \nNomad is easy to install and use. Its scheduler design is reportedly inspired by\nGoogle’s Omega, borrowing concepts such as having a global state of the cluster\nas well as employing an optimistic, concurrent scheduler.\nNomad has an agent-based architecture with a single binary that can take on dif‐\nferent roles, supporting rolling upgrades as well as draining nodes for re-\nbalancing. Nomad makes use of both a consensus protocol (strongly consistent)\nfor all state replication and scheduling and a gossip protocol used to manage the\naddresses of servers for automatic clustering and multiregion federation. In\nFigure 4-5, you can see Nomad’s architecture:\n• Servers are responsible for accepting jobs from users, managing clients, and\ncomputing task placements.\nHashicorp Nomad \n| \n23\n\n\n• Clients (one per VM instance) are responsible for interacting with the tasks\nor applications contained within a job. They work in a pull-based manner;\nthat is, they register with the server and then they poll it periodically to\nwatch for pending work.\nFigure 4-5. Nomad architecture\nJobs in Nomad are defined in a HashiCorp-proprietary format called HCL or in\nJSON, and Nomad offers a command-line interface as well as an HTTP API to\ninteract with the server process. Nomad models infrastructure as regions and\ndatacenters. Regions may contain multiple datacenters, depending on what scale\nyou are operating at. You can think of a datacenter like a zone in AWS, Azure, or\nGoogle Cloud (say, us-central1-b), and a region might be something like Iowa\n(us-central1).\nI’m assuming you’re familiar with Nomad and its terminology. If not, I suggest\nyou watch “Nomad: A Distributed, Optimistically Concurrent Schedule: Armon\nDadgar, HashiCorp”, a nice introduction to Nomad, and also read the well-done\ndocs.\nTo try out Nomad, use the UI Demo HashiCorp provides or try it out online for\nfree using the Katacoda “Introduction to Nomad” scenario.\nNomad comes with a couple of so-called task drivers, from general-purpose exec\nto java to qemu and docker. For the docker driver Nomad requires, at the time of\nthis writing, Docker version 1.10 or greater and uses port binding to expose serv‐\nices running in containers using the port space on the host’s interface. It provides\nautomatic and manual mapping schemes for Docker, binding both TCP and\nUDP protocols to ports used for Docker containers.\nFor more details on networking options, such as mapping ports and using labels,\nsee the documentation.\nWith v0.2, Nomad introduced a Consul-based (see “Consul” on page 30) service\ndiscovery mechanism. It includes health checks and assumes that tasks running\ninside Nomad also need to be able to connect to the Consul agent, which can, in\nthe context of containers using bridge mode networking, pose a challenge.\n24 \n| \nChapter 4: Orchestration\n\n\n2 Now, you might argue that this is not specific to the container orchestration domain but a general OSS\nissue, and you’d be right. Still, I believe it is important enough to mention it, as many people are new to\nthis area and can benefit from these insights.\nCommunity Matters\nAn important aspect you’ll want to consider when selecting an orchestration sys‐\ntem is the community behind and around it.2 Here are a few indicators and met‐\nrics you can use:\n• Is the governance backed by a formal entity or process, such as the Apache\nSoftware Foundation (ASF) or the Linux Foundation (LF)?\n• How active are the mailing list, the IRC channel, the bug/issue tracker, the\nGit repo (number of patches or pull requests), and other community initia‐\ntives?\nTake a holistic view, but make sure that you actually pay attention to the\nactivity there. Healthy and hopefully growing communities will tend to have\nhigh participation in at least one of these areas.\n• Is the orchestration tool (implicitly) controlled by a single entity? For exam‐\nple, in the case of Nomad HashiCorp is in control, for Apache Mesos it’s\nmainly Mesosphere (and to some extent Twitter), etc.\n• Are multiple independent providers and support channels available? For\nexample, you can run Kubernetes in many different environments and get\nhelp from many (commercial) organizations as well as individuals on Slack,\nmailing lists, or forums.\nWrapping It Up\nAs of early 2018, Kubernetes (discussed in Chapter 7) can be considered the de\nfacto container orchestration standard. All major providers, including Docker\nand DC/OS (Mesos), support Kubernetes.\nNext, we’ll move on to service discovery, a vital part of container orchestration.\nCommunity Matters \n| \n25\n\n\nCHAPTER 5\nService Discovery\nOne challenge arising from adopting the cattle approach to managing infrastruc‐\nture is service discovery. If you subscribe to the cattle approach, you treat all of\nyour machines equally and you do not manually allocate certain machines for\ncertain applications; instead, you leave it up to a piece of software (the scheduler)\nto manage the life cycle of the containers.\nThe question then is, how do you determine which host your container ended up\nbeing scheduled on so that you can connect to it? This is called service discovery,\nand we touched on it already in Chapter 4.\nThe Challenge\nService discovery has been around for a while—essentially, as long as distributed\nsystems and services have existed. In the context of containers, the challenge\nboils down to reliably maintaining a mapping between a running container and\nits location. By location, I mean its IP address and the port on which it is reacha‐\nble. This mapping has to be done in a timely manner and accurately across\nrelaunches of the container throughout the cluster. Two distinct operations must\nbe supported by a container service discovery solution:\nRegistration\nEstablishes the container -> location mapping. Because only the con‐\ntainer scheduler knows where containers “live,” we can consider it to be the\nabsolute source of truth concerning a container’s location.\nLookup\nEnables other services or applications to look up the mapping we stored dur‐\ning registration. Interesting properties include the freshness of the informa‐\ntion and the latency of a query (average, p50, p90, etc.).\n27\n\n\n1 ZooKeeper was originally developed at Yahoo! in order to get its ever-growing zoo of software tools,\nincluding Hadoop, under control.\nLet’s examine a few slightly orthogonal considerations for the selection process:\n• Rather than simply sending a requestor in a certain direction, how about\nexcluding unhealthy hosts or hanging containers from the lookup path?\nYou’ve guessed it, this is the strongly related topic of load balancing, and\nbecause it is of such importance we’ll discuss options in the last section of\nthis chapter.\n• Some argue it’s an implementation detail, others say the position in the CAP\ntriangle matters: the choice of strong consistency versus high availability in\nthe context of the service discovery tool might influence your decision. Be at\nleast aware of it.\n• Your choice might also be impacted by scalability considerations. Sure, if you\nonly have a handful of nodes under management then all of the solutions\ndiscussed here will be a fit. If your cluster, however, has several hundred or\neven thousands of nodes, then you will want to make sure you do some\nproper load testing before you commit to one particular technology.\nIn this chapter you’ll learn about service discovery options and how and where to\nuse them.\nIf you want to learn more about the requirements and fundamental challenges in\nthis space, read Jeff Lindsay’s “Understanding Modern Service Discovery with\nDocker” and check out what Simon Eskildsen of Shopify shared on this topic at a\nrecent DockerCon.\nTechnologies\nThis section briefly introduces a variety of service discovery technologies, listing\npros and cons and pointing to further discussions on the web. For a more in-\ndepth treatment, check out Adrian Mouat’s excellent book Using Docker\n(O’Reilly).\nZooKeeper\nApache ZooKeeper is an ASF top-level project and a JVM-based, centralized tool\nfor configuration management,1 providing comparable functionality to what\nGoogle’s Chubby brings to the table. ZooKeeper (ZK) organizes its payload data\nsomewhat like a filesystem, in a hierarchy of so-called znodes. In a cluster, a\nleader is elected and clients can connect to any of the servers to retrieve data. You\nwant 2n+1 nodes in a ZK cluster. The most often found configurations in the\n28 \n| \nChapter 5: Service Discovery\n\n\n2 Did you know that etcd comes from /etc distributed? What a name!\nwild are three, five, or seven nodes. Beyond that, you’ll experience diminishing\nreturns concerning the fault tolerance/throughput trade-off.\nZooKeeper is a battle-proven, mature, and scalable solution, but it has some\noperational downsides. Some people consider the installation and management\nof a ZK cluster a not-so-enjoyable experience. Most ZK issues I’ve seen come\nfrom the fact that certain services (Apache Storm comes to mind) misuse it. They\neither put too much data into the znodes or, worse, have an unhealthy read/write\nratio, essentially writing too fast. If you plan to use ZK, at least consider using\nhigher-level interfaces such as Apache Curator, which is a wrapper library\naround ZK implementing a number of recipes, and Netflix’s Exhibitor for man‐\naging and monitoring ZK clusters.\nLooking at Figure 5-1, you see two components: R/W (which stands for registra‐\ntion watcher, a piece of software you need to provide yourself), and HAProxy,\nwhich is controlled by the R/W. Whenever a container is scheduled on a node it\nregisters with ZK, using a znode with a path like /$nodeID/$containerID and the\nIP address as its payload. The R/W watches changes on those znodes and config‐\nures HAProxy accordingly.\nFigure 5-1. Example service discovery setup with ZooKeeper\netcd\nWritten in the Go language, etcd is a product of the CoreOS team.2 It is a light‐\nweight, distributed key-value store that uses the Raft algorithm for consensus (a\nleader–follower model, with leader election) and employs a replicated log across\nTechnologies \n| \n29\n\n\n3 That is, in contrast to ZK, all you need to interact with etcd is curl or the like.\nthe cluster to distribute the writes a leader receives to its followers. In a sense,\netcd is conceptually quite similar to ZK. While the payload can be arbitrary, etcd’s\nHTTP API is JSON-based,3 and as with ZK, you can watch for changes in the val‐\nues etcd makes available to the cluster. A very useful feature of etcd is that of\nTTLs on keys, which is a great building block for service discovery. In the same\nmanner as ZK, you want 2n+1 nodes in an etcd cluster, for the same reasons.\nThe security model etcd provides allows on-the-wire encryption through\nTLS/SSL as well as client certificate authentication, both between clients and the\ncluster and between the etcd nodes.\nIn Figure 5-2, you can see that the etcd service discovery setup is quite similar to\nthe ZK setup. The main difference is the usage of confd, which configures\nHAProxy, rather than having you write your own script.\nFigure 5-2. Example service discovery setup with etcd\nConsul\nConsul, a HashiCorp product also written in the Go language, exposes function‐\nality for service registration, discovery, and health checking in an opinionated\nway. Services can be queried using the HTTP API or through DNS. Consul sup‐\nports multi-datacenter deployments.\nOne of Consul’s features is a distributed key-value store, akin to etcd. It also uses\nthe Raft consensus algorithm (and again the same observations concerning 2n+1\nnodes as with ZK and etcd apply), but the deployment is different. Consul has the\n30 \n| \nChapter 5: Service Discovery\n\n\n4 Java, I’m looking at you.\nconcept of agents, which can be run in either of the two available modes—as a\nserver (provides a key-value store and DNS) or as a client (registers services and\nruns health checks)—and with the membership and node discovery imple‐\nmented by Serf.\nWith Consul, you have essentially four options to implement service discovery\n(ordered from most desirable to least desirable):\n• Use a service definition config file, interpreted by the Consul agent.\n• Use a tool like traefik that has a Consul backend.\n• Write your own sidekick process that registers the service through the HTTP\nAPI.\n• Bake the registration into the service itself by leveraging the HTTP API.\nWant to learn more about using Consul for service discovery? Check out these\ntwo great blog posts: “Consul Service Discovery with Docker” by Jeff Lindsay and\n“Docker DNS & Service Discovery with Consul and Registrator” by Joseph\nMiller.\nPure-Play DNS-Based Solutions\nDNS has been a robust and battle-proven workhorse on the internet for many\ndecades. The eventual consistency of the DNS system, the fact that certain clients\naggressively cache DNS lookups,4 and also the reliance on SRV records make this\noption something you will want to use when you know that it is the right one.\nI’ve titled this section “Pure-Play DNS-Based Solutions” because Consul techni‐\ncally also has a DNS server, but that’s only one option for how you can use it to\ndo service discovery. Here are some popular and widely used pure-play DNS-\nbased service discovery solutions:\nMesos-DNS\nThis solution is specific for service discovery in Apache Mesos. Written in\nGo, Mesos-DNS polls the active Mesos master process for any running tasks\nand exposes the <ip>:<port> info via DNS as well as through an HTTP API.\nFor DNS requests for other hostnames or services, Mesos-DNS can either\nuse an external nameserver or leverage your existing DNS server to forward\nonly the requests for Mesos tasks to Mesos-DNS.\nSkyDNS\nUsing etcd, you can announce your services to SkyDNS, which stores service\ndefinitions into etcd and updates its DNS records. Your client application\nTechnologies \n| \n31\n",
      "page_number": 30
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 39-52)",
      "start_page": 39,
      "end_page": 52,
      "detection_method": "topic_boundary",
      "content": "issues DNS queries to discover the services. Thus, functionality-wise it is\nquite similar to Consul, without the health checks.\nWeaveDNS\nWeaveDNS was introduced in Weave 0.9 as a simple solution for service dis‐\ncovery on the Weave network, allowing containers to find other containers’\nIP addresses by their hostnames. In Weave 1.1, a so-called Gossip DNS pro‐\ntocol was introduced, making lookups faster through a cache as well as\nincluding timeout functionality. In the new implementation, registrations are\nbroadcast to all participating instances, which subsequently hold all entries\nin memory and handle lookups locally.\nAirbnb’s SmartStack and Netflix’s Eureka\nIn this section, we’ll take a look at two bespoke systems that were developed to\naddress specific requirements. This doesn’t mean you can’t or shouldn’t use them,\njust that you should be aware of this heritage.\nAirbnb’s SmartStack is an automated service discovery and registration frame‐\nwork, transparently handling creation, deletion, failure, and maintenance work.\nSmartStack uses two separate services that run on the same host as your con‐\ntainer: Nerve (writing into ZK) for service registration, and Synapse (dynamically\nconfiguring HAProxy) for lookup. It is a well-established solution for non-\ncontainerized environments, and time will tell if it will also be as useful with\nDocker.\nNetflix’s Eureka is different, mainly because it was born in the AWS environment\n(where all of Netflix runs). Eureka is a REST-based service used for locating serv‐\nices for the purpose of load balancing and failover of middle-tier servers, and it\nalso comes with a Java-based client component, which makes interactions with\nthe service straightforward. This client has a built-in load balancer that does\nbasic round-robin load balancing. At Netflix, Eureka is used for red/black\ndeployments, for Cassandra and memcached deployments, and for carrying\napplication-specific metadata about services.\nParticipating nodes in a Eureka cluster replicate their service registries between\neach other asynchronously. In contrast to ZK, etcd, or Consul, Eureka favors ser‐\nvice availability over strong consistency; it leaves it up to the client to deal with\nstale reads, but has the upside of being more resilient in case of networking parti‐\ntions. And, you know, the network is reliable. Not.\nLoad Balancing\nAn orthogonal but related topic to that of service discovery is load balancing. \nLoad balancing enables you to spread the load—that is, the inbound service\n32 \n| \nChapter 5: Service Discovery\n\n\nrequests—across a number of containers. In the context of containers and micro‐\nservices, load balancing achieves a couple of things at the same time:\n• It allows throughput to be maximized and response time to be minimized.\n• It can avoid hot-spotting—that is, overwhelming a single container with\nwork.\n• It can help with overly aggressive DNS caching, such as that found with Java.\nThe following list outlines some popular load balancing options for container‐\nized setups, in alphabetical order:\nBamboo\nA daemon that automatically configures HAProxy instances, deployed on\nApache Mesos and Marathon. See the p24e guide “Service Discovery with\nMarathon, Bamboo and HAProxy” for a concrete recipe.\nEnvoy\nA high-performance distributed proxy written in C++, originally built at\nLyft. Envoy was designed to be used for single services and applications, and\nto provide a communication bus and data plane for service meshes. It’s the\ndefault data plane in Istio.\nHAProxy\nA stable, mature, and battle-proven (if not very feature-rich) workhorse.\nOften used in conjunction with NGINX, HAProxy is reliable and integra‐\ntions with pretty much everything under the sun exist.\nkube-proxy\nRuns on each node of a Kubernetes cluster and updates services IPs. It sup‐\nports simple TCP/UDP forwarding and round-robin load balancing. Note\nthat it’s only for cluster-internal load balancing and also serves as a service\ndiscovery support component.\nMetalLB\nA load-balancer implementation for bare-metal Kubernetes clusters,\naddressing the fact that Kubernetes does not offer a default implementation\nfor such clusters. In other words, you need to be in a public cloud environ‐\nment to benefit from this functionality. Note that you may need one or more\nrouters capable of speaking BGP in order for MetalLB to work.\nNGINX\nThe leading solution in this space. With NGINX you get support for round-\nrobin, least-connected, and ip-hash strategies, as well as on-the-fly con‐\nfiguration, monitoring, and many other vital features.\nLoad Balancing \n| \n33\n\n\nservicerouter.py\nA simple script that gets app configurations from Marathon and updates\nHAProxy; see also the p24e guide “Service Discovery with Marathon, Bam‐\nboo and HAProxy”.\ntraefik\nThe rising star in this category. Emile Vauge (traefik’s lead developer) must\nbe doing something right. I like it a lot, because it’s like HAProxy but comes\nwith a bunch of backends, such as Marathon and Consul, out of the box.\nVamp-router\nInspired by Bamboo and Consul–HAProxy, Magnetic.io wrote Vamp-router,\nwhich supports updates of the config through a REST API or ZooKeeper,\nroutes and filters for canary releasing and A/B testing, and ACLs, as well as\nproviding statistics.\nVulcand\nA reverse proxy for HTTP API management and microservices, inspired by\nHystrix.\nIf you want to learn more about load balancing, check out Kevin Reedy’s talk\nfrom nginx.conf 2014 on load balancing with NGINX and Consul.\nWrapping It Up\nTo close out this chapter, I’ve put together a table that provides an overview of\nthe service discovery solutions we’ve discussed. I explicitly do not aim at declar‐\ning a winner, because I believe the best choice depends on your use case and\nrequirements. So, take the following table as a quick orientation and summary\nbut not as a shootout (also, note that in the context of Kubernetes you don’t need\nto choose one—it’s built into the system):\nName\nConsistency\nLanguage Registration\nLookup\nZooKeeper\nStrong\nJava\nClient\nBespoke clients\netcd\nStrong\nGo\nSidekick + client\nHTTP API\nConsul\nStrong\nGo\nAutomatic and through traefik (Consul\nbackend)\nDNS + HTTP/JSON API\nMesos-DNS\nStrong\nGo\nAutomatic and through traefik\n(Marathon backend)\nDNS + HTTP/JSON API\nSkyDNS\nStrong\nGo\nClient registration\nDNS\nWeaveDNS\nEventual\nGo\nAuto\nDNS\nSmartStack Strong\nJava\nClient registration\nAutomatic through HAProxy config\nEureka\nEventual\nJava\nClient registration\nBespoke clients\n34 \n| \nChapter 5: Service Discovery\n\n\nBecause container service discovery is a moving target, you are well\nadvised to reevaluate your initial choices on an ongoing basis, at least\nuntil some consolidation has taken place.\nIn this chapter you learned about service discovery and how to tackle it, as well\nas about load balancing options. We will now switch gears and move on to\nKubernetes, the de facto container orchestration standard that comes with built-\nin service discovery (so you don’t need to worry about the topics discussed in this\nchapter) and has its own very interesting approach to container networking\nacross machines.\nWrapping It Up \n| \n35\n\n\nCHAPTER 6\nThe Container Network Interface\nThe Container Network Interface (CNI), as depicted in Figure 6-1, provides a\nplug-in-oriented networking solution for containers and container orchestrators.\nIt consists of a specification and libraries for writing plug-ins to configure net‐\nwork interfaces in Linux containers.\nFigure 6-1. 100,000 ft view on CNI\nThe CNI specification is lightweight; it only deals with the network connectivity\nof containers, as well as the garbage collection of resources once containers are\ndeleted.\nWe will focus on CNI in this book since it’s the de facto standard for container\norchestrators, adopted by all major systems such as Kubernetes, Mesos, and\nCloud Foundry. If you’re exclusively using Docker Swarm you’ll need to use\n37\n\n\nDocker’s libnetwork and might want to read the helpful article by Lee Calcote\ntitled “The Container Networking Landscape: CNI from CoreOS and CNM from\nDocker”, which contrasts CNI with the Docker model and provides you with\nsome guidance.\nHistory\nCNI was pioneered by CoreOS in the context of the container runtime rkt, to\ndefine a common interface between the network plug-ins and container runtimes\nand orchestrators. Docker initially planned to support it but then came up with\nthe Docker-proprietary libnetwork approach to container networking.\nCNI and the libnetwork plug-in interface were developed in parallel from April\nto June 2015, and after some discussion the Kubernetes community decided not\nto adopt libnetwork but rather to use CNI. Nowadays pretty much every con‐\ntainer orchestrator with the exception of Docker Swarm uses CNI; all runtimes\nsupport it and there’s a long list of supported plug-ins, as discussed in “Container\nRuntimes and Plug-ins” on page 40.\nIn May 2017, the Cloud Native Computing Foundation (CNCF) made CNI a full-\nblown top-level project.\nSpecification and Usage\nIn addition to the CNI specification, at time of writing in version 0.3.1-dev, the\nrepository contains the Go source code of a library for integrating CNI into\napplications as well as an example command-line tool for executing CNI plug-\nins. The plug-ins repository contains reference plug-ins and a template for creat‐\ning new plug-ins.\nBefore we dive into the usage, let’s look at two central definitions in the context of\nCNI:\nContainer\nSynonymous with a Linux network namespace. What unit this corresponds\nto depends on the container runtime implementation (single container or\npod).\nNetwork\nA uniquely addressable group of entities that can communicate with one\nanother. These entities might be an individual container, a machine, or some\nother network device such as a router.\nLet’s now have a look at how CNI—on a high level—works, as depicted in\nFigure 6-2. First, the container runtime takes some configuration and issues a\ncommand to a plug-in. The plug-in then goes off and configures the network.\n38 \n| \nChapter 6: The Container Network Interface\n\n\nFigure 6-2. CNI high-level architecture\nSo, what CNI conceptually enables you to do is to add containers to a network as\nwell as remove them. The current version of CNI defines the following opera‐\ntions:\n• Add container to one or more networks\n• Delete container from network\n• Report CNI version\nIn order for CNI to add a container to a network, the container runtime must\nfirst create a new network namespace for the container and then invoke one or\nmore of the defined plug-ins. The network configuration is in JSON format and\nincludes mandatory fields such as name and type as well as plug-in type–specific\nfields. The actual command (for example, ADD) is passed in as an environment\nvariable aptly named CNI_COMMAND.\nIP Allocation with CNI\nA CNI plug-in is expected to assign an IP address to the interface and set up net‐\nwork routes relevant for it. This gives the CNI plug-in great flexibility but also\nplaces a large burden on it. To accommodate this, CNI defines a dedicated IP\nAddress Management (IPAM) plug-in that takes care of the IP range manage‐\nment independently.\nLet’s have a look at a concrete CNI command in action:\n$ CNI_COMMAND=ADD \\\n  CNI_CONTAINERID=875410a4c38d7 \\\n  CNI_NETNS=/proc/1234/ns/net \\\n  CNI_IFNAME=eth0 \\\n  CNI_PATH=/opt/cni/bin \\\n  someplugin < /etc/cni/net.d/someplugin.conf\nThis example shows how a certain plug-in (someplugin) is applied to a given\ncontainer (875410a4c38d7) using a specific configuration (someplugin.conf). Note\nthat while initially all configuration parameters were passed in as environment\nSpecification and Usage \n| \n39\n\n\nvariables, the people behind the spec are moving more and more toward using\nthe (JSON) configuration file.\nYou can learn more about using CNI in the excellent blog post “Understanding\nCNI” by Jon Langemak.\nContainer Runtimes and Plug-ins\nIn addition to pretty much any container orchestrator and container runtime\n(Kubernetes, Mesos, Cloud Foundry) supporting CNI, it ships with a number of\nbuilt-in plug-ins, such as loopback and vlan. There’s also a long list of third-\nparty CNI plug-ins available. Here is a selection of the most important ones, in\nalphabetical order:\nAmazon ECS CNI Plugins\nA collection of CNI plug-ins used by the Amazon ECS Agent to configure\nthe network namespace of containers with Elastic Network Interfaces (ENIs).\nBonding\nA CNI plug-in for failover and high availability of networking in cloud-\nnative environments, by Intel.\nCalico\nProject Calico’s network plug-in for CNI. Project Calico manages a flat layer\n3 network, assigning each workload a fully routable IP address. For environ‐\nments requiring an overlay network Calico uses IP-in-IP tunneling or can\nwork with other overlay networks, such as flannel.\nCilium\nA BPF-based solution providing connectivity between containers, operating\nat layer 3/4 to provide networking and security services as well as layer 7 to\nprotect modern protocols such as HTTP and gRPC.\nCNI-Genie\nA generic CNI network plug-in by Huawei.\nInfoblox\nAn IPAM driver for CNI that interfaces with Infoblox to provide IP Address\nManagement services.\nLinen\nA CNI plug-in designed for overlay networks with Open vSwitch.\nMultus\nA powerful multi–plug-in environment by Intel.\nNuage CNI\nA Nuage Networks SDN plug-in supporting network policies for Kubernetes.\n40 \n| \nChapter 6: The Container Network Interface\n\n\nRomana\nA layer 3 CNI plug-in supporting network policy for Kubernetes.\nSilk\nA network fabric for containers, inspired by flannel, designed for Cloud\nFoundry.\nVhostuser\nA plug-in to run with Open vSwitch and OpenStack VPP along with the\nMultus CNI plug-in in Kubernetes for bare-metal container deployment\nmodels.\nWeave Net\nA multi-host Docker network by Weaveworks.\nWrapping It Up\nWith this we conclude the CNI chapter and move on to Kubernetes and its net‐\nworking approach. CNI plays a central role in Kubernetes (networking), and you\nmight want to check the docs there as well.\nWrapping It Up \n| \n41\n\n\nCHAPTER 7\nKubernetes Networking\nThis chapter will first quickly bring you up to speed concerning Kubernetes, then\nintroduce you to the networking concepts on a high level. Then we’ll jump into\nthe deep end, looking at how container networking is realized in Kubernetes,\nwhat traffic types exist and how you can make services talk to each other within\nthe cluster, as well as how you can get traffic into your cluster and to a specific\nservice.\nA Gentle Kubernetes Introduction\nKubernetes is an open source container orchestration system. It captures Google’s\nlessons learned from running containerized workloads on Borg for more than a\ndecade. As of early 2018 Kubernetes is considered the de facto industry standard\nfor container orchestration, akin to the Linux kernel for the case of a single\nmachine.\nI’d argue that there are at least two significant points in time concerning\nthe birth of Kubernetes. The first was on June 7, 2014, with Joe Beda’s\ninitial commit on GitHub that marked the beginning of the open sourc‐\ning of the project. The second was almost a year later, on July 20, 2015,\nwhen Google launched Kubernetes 1.0 and announced the formation of\na dedicated entity to host and govern Kubernetes, the Cloud Native\nComputing Foundation (CNCF). As someone who was at the launch\nevent (and party), I can tell you, that’s certainly one way to celebrate the\nbirth of a project.\nKubernetes’s architecture (Figure 7-1) provides support for a number of work‐\nloads, allowing you to run stateless as well as stateful containerized applications.\nYou can launch long-running processes, such as low-latency app servers, as well\nas batch jobs.\n43\n\n\nFigure 7-1. An overview of the Kubernetes architecture\nThe unit of scheduling in Kubernetes is a pod. Essentially, this is a tightly coupled\nset of one or more containers that are always collocated (that is, scheduled onto a\nnode as a unit); they cannot be spread over nodes. The number of running\ninstances of a pod—called replicas—can be declaratively stated and enforced\nthrough controllers. The logical organization of all resources, such as pods,\ndeployments, or services, happens through labels.\nWith Kubernetes you almost always have the option to swap out the default\nimplementations with some open source or closed source alternative, be it DNS\nor monitoring. Kubernetes is highly extensible, from defining new workloads and\nresource types in general to customizing its user-facing parts, such as the CLI\ntool kubectl (pronounced cube cuddle).\nThis chapter assumes you’re somewhat familiar with Kubernetes and its termi‐\nnology. Should you need to brush up your knowledge of how Kubernetes works,\nI suggest checking out the Concepts section in the official docs or the book\nKubernetes Up and Running (O’Reilly) by Brendan Burns, Kelsey Hightower, and\nJoe Beda.\n44 \n| \nChapter 7: Kubernetes Networking\n\n\nKubernetes Networking Overview\nRather than prescribing a certain networking solution, Kubernetes only states\nthree fundamental requirements:\n• Containers can communicate with all other containers without NAT.\n• Nodes can communicate with all containers (and vice versa) without NAT.\n• The IP a container sees itself is the same IP as others see it.\nHow you meet these requirements is up to you. This means you have a lot of free‐\ndom to realize networking with and for Kubernetes. It also means, however, that\nKubernetes on its own will only provide so much; for example, it supports CNI\n(Chapter 6) but it doesn’t come with a default SDN solution. In the networking\narea, Kubernetes is at the same time strangely opinionated (see the preceding\nrequirements) and not at all (no batteries included).\nFrom a network traffic perspective we differentiate between three types in Kuber‐\nnetes, as depicted in Figure 7-2:\nIntra-pod networking\nAll containers within a pod share a network namespace and see each other\non localhost. Read “Intra-Pod Networking” on page 46 for details.\nInter-pod networking\nTwo types of east–west traffic are supported: pods can directly communicate\nwith other pods or, preferably, pods can leverage services to communicate\nwith other pods. Read “Inter-Pod Networking” on page 47 for details.\nIngress and egress\nIngress refers to routing traffic from external users or apps to pods, and\negress refers to calling external APIs from pods. Read “Ingress and Egress”\non page 53 for details.\nKubernetes Networking Overview \n| \n45\n\n\n1 See pause.go for details; basically it blocks until it receives a SIGTERM.\nFigure 7-2. Kubernetes network traffic types\nKubernetes requires each pod to have an IP in a flat networking name‐\nspace with full connectivity to other nodes and pods across the network.\nThis IP-per-pod model yields a backward-compatible way for you to\ntreat a pod almost identically to a VM or a physical host, in the context\nof naming, service discovery, or port allocations. The model allows for a\nsmoother transition from non–cloud native apps and environments.\nIntra-Pod Networking\nWithin a pod there exists a so-called infrastructure container. This is the first con‐\ntainer that the kubelet launches, and it acquires the pod’s IP and sets up the net‐\nwork namespace. All the other containers in the pod then join the infra\ncontainer’s network and IPC namespace. The infra container has network bridge\nmode enabled (see “Bridge Mode Networking” on page 7) and all the other con‐\ntainers in the pod join this namespace via container mode (covered in “Container\nMode Networking” on page 9). The initial process that runs in the infra container\ndoes effectively nothing,1 as its sole purpose is to act as the home for the name‐\n46 \n| \nChapter 7: Kubernetes Networking\n\n\nspaces. If the infra container dies, the kubelet kills all the containers in the pod\nand then starts the process over.\nAs a result of above, all containers within a pod can communicate amongst each\nother using localhost (or 127.0.0.1 in IPv4). You are responsible yourself to\nmake sure containers within a pod do not conflict with each other in terms of\nports used. Note also that the Kubernetes approach here also means reduced iso‐\nlation between containers within a pod; however, this is by design and since we\nconsider the tight coupling here a good thing, it is probably not something you\nneed to worry about, though it’s good to be aware of it.\nIf you want to learn more about the infra container, read The Almighty Pause\nContainer by Ian Lewis.\nInter-Pod Networking\nIn Kubernetes, each pod has a routable IP, allowing pods to communicate across\ncluster nodes without NAT and no need to manage port allocations. Because\nevery pod gets a real (that is, not machine-local) IP address, pods can communi‐\ncate without proxies or translations (such as NAT). The pod can use well-known\nports and can avoid the use of higher-level service discovery mechanisms such as\nthose we discussed in Chapter 5.\nWe distinguish between two types of inter-pod communication, sometimes also\ncalled East-West traffic:\n• Pods can directly communicate with other pods; in this case the caller pod\nneeds to find out the IP address of the callee and risks repeating this opera‐\ntion since pods come and go (cattle behaviour).\n• Preferably, pods use services to communicate with other pods. In this case,\nthe service provides a stable (virtual) IP address that can be discovered, for\nexample, via DNS.\nDifference to Docker Model\nNote that the Kubernetes flat address space model is different from the default\nDocker model. There, each container gets an IP address in the 172.x.x.x space\nand only sees this 172. address. If this container connects to another container\nthe peer would see the connection coming from a different IP than the container\nitself sees. This means you can never self-register anything from a container\nbecause a container cannot be reached on its private IP.\nInter-Pod Networking \n| \n47\n",
      "page_number": 39
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 53-62)",
      "start_page": 53,
      "end_page": 62,
      "detection_method": "topic_boundary",
      "content": "When a container tries to obtain the address of network interface it sees the same\nIP that any peer container would see them coming from; each pod has its own IP\naddress that other pods can find and use. By making IP addresses and ports the\nsame both inside and outside the pods, Kubernetes creates a flat address space\nacross the cluster. For more details on this topic see also the article “Understand‐\ning Kubernetes Networking: Pods” by Mark Betz.\nLet’s now focus on the service, as depicted in Figure 7-3.\nFigure 7-3. The Kubernetes service concept\nA service provides a stable virtual IP (VIP) address for a set of pods. While pods\nmay come and go, services allow clients to reliably discover and connect to the\ncontainers running in the pods by using the VIP. The “virtual” in VIP means it’s\nnot an actual IP address connected to a network interface; its purpose is purely to\nact as the stable front to forward traffic to one or more pods, with IP addresses\nthat may come and go.\n48 \n| \nChapter 7: Kubernetes Networking\n\n\nWhat VIPs Really Are\nIt’s essential to realize that VIPs do not exist as such in the networking\nstack. For example, you can’t ping them. They are only Kubernetes-\ninternal administrative entities. Also note that the format is IP:PORT, so\nthe IP address along with the port make up the VIP. Just think of a VIP\nas a kind of index into a data structure mapping to actual IP addresses.\nAs you can see in Figure 7-3, the service with the VIP 10.104.58.143 routes the\ntraffic to one of the pods 172.17.0.3 or 172.17.0.4. Note here the different sub‐\nnets for the service and pods, see Network Ranges for further details on the rea‐\nson behind that. Now, you might be wondering how this actually works? Let’s\nhave a look at it.\nYou specify the set of pods you want a service to target via a label selector, for\nexample, for spec.selector.app=someapp Kubernetes would create a service\nthat targets all pods with a label app=someapp. Note that if such a selector exists,\nthen for each of the targeted pods a sub-resource of type Endpoint will be cre‐\nated, and if no selector exists then no endpoints are created. For example, see in\nthe following code example the output of the kubectl describe command. Such\nendpoints are also not created in the case of so-called headless services, which\nallow you to exercise great control over how the IP management and service dis‐\ncovery takes place.\nKeeping the mapping between the VIP and the pods up-to-date is the job of\nkube-proxy (see also the docs on kube-proxy), a process that runs on every node\non the cluster.\nThis kube-proxy process queries the API server to learn about new services in\nthe cluster and updates the node’s iptables rules accordingly, to provide the nec‐\nessary routing information. To learn more how exactly services work, check out\nKubernetes Services By Example.\nLet’s see how this works in practice: assuming there’s an existing deployment\ncalled nginx (for example, execute kubectl run webserver --image nginx)\nyou can automatically create a service like so:\n$ kubectl expose deployment/webserver --port 80\nservice \"webserver\" exposed\n$ kubectl describe service/webserver\nName:              webserver\nNamespace:         default\nLabels:            run=webserver\nAnnotations:       <none>\nSelector:          run=webserver\nType:              ClusterIP\nIP:                10.104.58.143\nInter-Pod Networking \n| \n49\n\n\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         172.17.0.3:8080,172.17.0.4:8080\nSession Affinity:  None\nEvents:            <none>\nAfter executing the above kubectl expose command, you will see the service\nappear:\n$ kubectl get service -l run=webserver\nNAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nwebserver   ClusterIP   10.104.58.143   <none>        80/TCP    1m\nAbove, note two things: the service has got itself a cluster-internal IP (CLUSTER-\nIP column) and the EXTERNAL-IP column tells you that this service is only avail‐\nable from within the cluster, that is, no traffic from outside of the cluster can\nreach this service (yet)—see “Ingress and Egress” on page 53 to learn how to\nchange this situation.\nFigure 7-4. Kubernetes service in the dashboard\nIn Figure 7-4 you can see the representation of the service in the Kubernetes\ndashboard.\nService Discovery in Kubernetes\nLet us now talk about how service discovery works in Kubernetes.\nConceptually, you can use one of the two built-in discovery mechanisms:\n• Through environment variables (limited)\n50 \n| \nChapter 7: Kubernetes Networking\n\n\n• Using DNS, which is available cluster-wide if a respective DNS cluster add-\non has been installed\nEnvironment Variables–Based Service Discovery\nFor the environment variables–based discovery method, a simple example might\nlook like the example code to follow: using a jump pod to get us into the cluster\nand then running from there the env built-in shell command (note that the out‐\nput has been edited to be easier to digest).\n$ kubectl run -it --rm jump --restart=Never \\\n              --image=quay.io/mhausenblas/jump:v0.1 -- sh\nIf you don't see a command prompt, try pressing enter.\n/ # env\nHOSTNAME=jump\nWEBSERVER_SERVICE_HOST=10.104.58.143\nWEBSERVER_PORT=tcp://10.104.58.143:80\nWEBSERVER_SERVICE_PORT=80\nWEBSERVER_PORT_80_TCP_ADDR=10.104.58.143\nWEBSERVER_PORT_80_TCP_PORT=80\nWEBSERVER_PORT_80_TCP_PROTO=tcp\nWEBSERVER_PORT_80_TCP=tcp://10.104.58.143:80\n...\nAbove, you can see the service discovery in action: the environment variables WEB\nSERVER_XXX give you the IP address and port you can use to connect to the ser‐\nvice. For example, while still in the jump pod, you could execute curl\n10.104.58.143 and you should see the NGINX welcome page.\nWhile convenient, note that discovery via environment variables has a funda‐\nmental drawback: any service that you want to discover must be created before\nthe pod from which you want to discover it as otherwise the environment vari‐\nables will not be populated by Kubernetes. Luckily there exists a better way: DNS.\nDNS-Based Service Discovery\nMapping a fully qualified domain name (FQDN) like example.com to an IP\naddress such as 123.4.5.66 is what DNS was designed for and has been doing\nfor us on a daily basis on the internet for more than 30 years.\nChoosing a DNS Solution\nWhen rolling your own Kubernetes distro, that is, putting together all\nthe required components such as SDN or the DNS add-on yourself\nrather than using an offering from the more than 30 certified Kuber‐\nnetes offerings, it’s worth considering the CNCF project CoreDNS over\nthe older and less feature-rich kube-dns DNS cluster add-on (which is\npart of Kubernetes proper).\nService Discovery in Kubernetes \n| \n51\n\n\nSo how can we use DNS to do service discovery in Kubernetes? It’s easy, if you\nhave the DNS cluster add-on installed and enabled. This DNS server watches on\nthe Kubernetes API for services being created or removed. It creates a set of DNS\nrecords for each service it observes.\nIn the next example, let’s use our webserver service from above and assume we\nhave it running in the default namespace. For this service, a DNS record web\nserver.default (with a FQDN of webserver.default.cluster.local) should\nbe present.\n$ kubectl run -it --rm jump --restart=Never \\\n              --image=quay.io/mhausenblas/jump:v0.1 -- sh\nIf you don't see a command prompt, try pressing enter.\n/ # curl webserver.default\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\nPods in the same namespace can reach the service by its shortname webserver,\nwhereas pods in other namespaces must qualify the name as webserver.default.\nNote that the result of these FQDN lookups is the pod’s cluster IP. Further,\nKubernetes supports DNS service (SRV) records for named ports. So if our web\nserver service had a port named, say, http with the protocol type TCP, you\ncould issue a DNS SRV query for _http._tcp.webserver from the same name‐\nspace to discover the port number for http. Note also that the virtual IP for a\nservice is stable, so the DNS result does not have to be requeried.\n52 \n| \nChapter 7: Kubernetes Networking\n\n\nNetwork Ranges\nFrom an administrative perspective, you are conceptually dealing with\nthree networks: the pod network, the service network, and the host net‐\nwork (the machines hosting Kubernetes components such as the kube\nlet). You will need a strategy regarding how to partition the network\nranges; one often found strategy is to use networks from the private\nrange as defined in RFC 1918, that is, 10.0.0.0/8, 172.16.0.0/12, and\n192.168.0.0/16.\nIngress and Egress\nIn the following we’ll have a look at how traffic flows in and out of a Kubernetes\ncluster, also called North-South traffic.\nIngress\nUp to now we have discussed how to access a pod or service from within the\ncluster. Accessing a pod from outside the cluster is a bit more challenging. Kuber‐\nnetes aims to provide highly available, high-performance load balancing for serv‐\nices.\nInitially, the only available options for North-South traffic in Kubernetes were\nNodePort, LoadBalancer, and ExternalName, which are still available to you. For\nlayer 7 traffic (i.e., HTTP) a more portable option is available, however: intro‐\nduced in Kubernetes 1.2 as a beta feature, you can use Ingress to route traffic\nfrom the external world to a service in our cluster.\nIngress in Kubernetes works as shown in Figure 7-5: conceptually, it is split up\ninto two main pieces, an Ingress resource, which defines the routing to the back‐\ning services, and the Ingress controller, which listens to the /ingresses endpoint\nof the API server, learning about services being created or removed. On service\nstatus changes, the Ingress controller configures the routes so that external traffic\nlands at a specific (cluster-internal) service.\nIngress and Egress \n| \n53\n\n\nFigure 7-5. Ingress concept\nThe following example presents a concrete example of an Ingress resource, to\nroute requests for myservice.example.com/somepath to a Kubernetes service\nnamed service1 on port 9876.\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n  - host: myservice.example.com\n    http:\n      paths:\n      - path: /somepath\n        backend:\n          serviceName: service1\n          servicePort: 9876\nNow, the Ingress resource definition is nice, but without a controller, nothing\nhappens. So let’s deploy an ingress controller, in this case using Minikube.\n$ minikube addons enable ingress\nOnce you’ve enabled Ingress on Minikube, you should see it appear as enabled in\nthe list of Minikube add-ons. After a minute or so, two new pods will start in the\nkube-system namespace, the backend and the controller. So now you can use it,\nusing the manifest in the following example, which configures a path to an\nNGINX webserver.\n$ cat nginx-ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata:\n  name: nginx-public\n54 \n| \nChapter 7: Kubernetes Networking\n\n\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host:\n    http:\n      paths:\n      - path: /web\n        backend:\n          serviceName: nginx\n          servicePort: 80\n$ kubectl create -f nginx-ingress.yaml\nNow NGINX is available via the IP address 192.168.99.100 (in this case my\nMinikube IP) and the manifest file defines that it should be exposed via the\npath /web.\nNote that Ingress controllers can technically be any system capable of reverse\nproxying, but NGINX is most commonly used. Further, Ingress can also be\nimplemented by a cloud-provided load balancer, such as Amazon’s ALB.\nFor more details on Ingress, read the excellent article “Understanding Kubernetes\nNetworking: Ingress” by Mark Betz and make sure to check out the results of the\nsurvey the Kubernetes SIG Network carried out on this topic.\nEgress\nWhile in the case of Ingress we’re interested in routing traffic from outside the\ncluster to a service, in the case of Egress we are dealing with the opposite: how\ndoes an app in a pod call out to (cluster-)external APIs?\nOne may want to control which pods are allowed to have a communication path\nto outside services and on top of that impose other policies. Note that by default\nall containers in a pod can perform Egress. These policies can be enforced using\nnetwork policies as described in “Network Policies” on page 55 or by deploying a\nservice mesh as in “Service Meshes” on page 56.\nAdvanced Kubernetes Networking Topics\nIn the following I’ll cover two advanced and somewhat related Kubernetes net‐\nworking topics: network policies and service meshes.\nNetwork Policies\nNetwork policies in Kubernetes are a feature that allow you to specify how\ngroups of pods are allowed to communicate with each other. From Kubernetes\nAdvanced Kubernetes Networking Topics \n| \n55\n\n\nversion 1.7 and above network policies are considered stable and hence you can\nuse them in production.\nLet’s take a look at a concrete example of how this works in practice. For example,\nsay you want to suppress all traffic to pods in the namespace superprivate.\nYou’d create a default Egress policy for that namespace as in the following exam‐\nple:\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: bydefaultnoegress\n  namespace: superprivate\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\nNote that different Kubernetes distros support network policies to different\ndegrees: for example, in OpenShift they are supported as first-class citizens and a\nrange of examples is available via the redhat-cop/openshift-toolkit GitHub repo.\nIf you want to learn more about how to use network policies, check out Ahmet\nAlp Balkan’s brilliant and detailed hands-on blog post, “Securing Kubernetes\nCluster Networking”.\nService Meshes\nGoing forward, you can make use of service meshes such as the two discussed in\nthe following. The idea of a service mesh is that rather than putting the burden of\nnetworking communication and control onto you, the developer, you outsource\nthese nonfunctional things to the mesh. So you benefit from traffic control,\nobservability, security, etc. without any changes to your source code. Sound fan‐\ntastic? It is, believe you me.\nIstio\nIstio is a modern and popular service mesh, available for Kubernetes but not\nexclusively so. It’s using Envoy as the default data plane and mainly focusing\non the control-plane aspects. It supports monitoring (Prometheus), tracing\n(Zipkin/Jaeger), circuit breakers, routing, load balancing, fault injection,\nretries, timeouts, mirroring, access control, and rate limiting out of the box,\nto name a few features. Istio takes the battle-tested Envoy proxy (cf. “Load\nBalancing” on page 32) and packages it up as a sidecar container in your pod.\nLearn more about Istio via Christian Posta’s wonderful resource: Deep Dive\nEnvoy and Istio Workshop. \n56 \n| \nChapter 7: Kubernetes Networking\n\n\nBuoyant’s Conduit\nThis service mesh is deployed on a Kubernetes cluster as a data plane (writ‐\nten in Rust) made up of proxies deployed as sidecar containers alongside\nyour app and a control plane (written in Go) of processes that manages these\nproxies, akin to what you’ve seen in Istio above. After the CNCF project\nLinkerd this is Buoyant’s second iteration on the service mesh idea; they are\nthe pioneers in this space, establishing the service mesh idea in 2016. Learn\nmore via Abhishek Tiwari’s excellent blog post, “Getting started with Con‐\nduit - lightweight service mesh for Kubernetes”. \nOne note before we wrap up this chapter and also the book: service meshes are\nstill pretty new, so you might want to think twice before deploying them in prod\n—unless you’re Lyft or Google or the like ;)\nWrapping It Up\nIn this chapter we’ve covered the Kubernetes approach to container networking\nand showed how to use it in various setups. With this we conclude the book;\nthanks for reading and if you have feedback, please do reach out via Twitter.\nWrapping It Up \n| \n57\n",
      "page_number": 53
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 63-71)",
      "start_page": 63,
      "end_page": 71,
      "detection_method": "topic_boundary",
      "content": "APPENDIX A\nReferences\nReading stuff is fine, and here I’ve put together a collection of links that contain\neither background information on topics covered in this book or advanced mate‐\nrial, such as deep dives or teardowns. However, for a more practical approach I\nsuggest you check out Katacoda, a free online learning environment that contains\n100+ scenarios from Docker to Kubernetes (see for example the screenshot in\nFigure A-1).\nFigure A-1. Katacoda Kubernetes scenarios\nYou can use Katacoda in any browser; sessions are typically terminated after one\nhour.\n59\n\n\nContainer Networking References\nNetworking 101\n• “Network Protocols” from the Programmer’s Compendium\n• “Demystifying Container Networking” by Michele Bertasi\n• “An Empirical Study of Load Balancing Algorithms” by Khalid Lafi\nLinux Kernel and Low-Level Components\n• “The History of Containers” by thildred\n• “A History of Low-Level Linux Container Runtimes” by Daniel J. Walsh\n• “Networking in Containers and Container Clusters” by Victor Marmol,\nRohit Jnagal, and Tim Hockin\n• “Anatomy of a Container: Namespaces, cgroups & Some Filesystem Magic”\nby Jérôme Petazzoni\n• “Network Namespaces” by corbet\n• Network classifier cgroup documentation\n• “Exploring LXC Networking” by Milos Gajdos\nDocker\n• Docker networking overview\n• “Concerning Containers’ Connections: On Docker Networking” by Federico\nKereki\n• “Unifying Docker Container and VM Networking” by Filip Verloy\n• “The Tale of Two Container Networking Standards: CNM v. CNI” by Har‐\nmeet Sahni\nKubernetes Networking References\nKubernetes Proper and Docs\n• Kubernetes networking design\n• Services\n• Ingress\n60 \n| \nAppendix A: References\n\n\n• Cluster Networking\n• Provide Load-Balanced Access to an Application in a Cluster\n• Create an External Load Balancer\n• Kubernetes DNS example\n• Kubernetes issue 44063: Implement IPVS-based in-cluster service load bal‐\nancing\n• “Data and analysis of the Kubernetes Ingress survey 2018” by the Kubernetes\nSIG Network\nGeneral Kubernetes Networking\n• “Kubernetes Networking 101” by Bryan Boreham of Weaveworks\n• “An Illustrated Guide to Kubernetes Networking” by Tim Hockin of Google\n• “The Easy—Don’t Drive Yourself Crazy—Way to Kubernetes Networking” by\nGerard Hickey (KubeCon 2017, Austin)\n• “Understanding Kubernetes Networking: Pods”, “Understanding Kubernetes\nNetworking: Services”, and “Understanding Kubernetes Networking:\nIngress” by Mark Betz\n• “Understanding CNI (Container Networking Interface)” by Jon Langemak\n• “Operating a Kubernetes Network” by Julia Evans\n• “nginxinc/kubernetes-ingress” Git repo\n• “The Service Mesh: Past, Present, and Future” by William Morgan (KubeCon\n2017, Austin)\n• “Meet Bandaid, the Dropbox Service Proxy” by Dmitry Kopytkov and Pat‐\nrick Lee\n• “Kubernetes NodePort vs LoadBalancer vs Ingress? When Should I Use\nWhat?” by Sandeep Dinesh\nReferences \n| \n61\n\n\nAbout the Author\nMichael Hausenblas is a developer advocate for Go, Kubernetes, and OpenShift\nat Red Hat, where he helps appops to build and operate distributed services. His\nbackground is in large-scale data processing and container orchestration and he’s\nexperienced in advocacy and standardization at the W3C and IETF. Before Red\nHat, Michael worked at Mesosphere and MapR and in two research institutions\nin Ireland and Austria. He contributes to open source software (mainly using\nGo), speaks at conferences and user groups, blogs, and hangs out on Twitter too\nmuch.\n",
      "page_number": 63
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 72-72)",
      "start_page": 72,
      "end_page": 72,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 72
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Michael Hausenblas\nFrom Docker to Kubernetes\nContainer \nNetworking\nCompliments of\n",
      "content_length": 82,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "https://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/solutions/adc/\nhttps://www.nginx.com/products/\nhttps://www.nginx.com/products/\nhttps://www.nginx.com/products/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/microservices/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/cloud/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/application-security/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/web-mobile-acceleration/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/solutions/api-gateway/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nhttps://www.nginx.com/\nFREE TRIAL\nLEARN MORE\nLoad\nBalancing\nCloud\nSecurity\nWeb & Mobile\nPerformance\nLearn more at nginx.com\nAPI\nGateway\nMicroservices\n The NGINX Application Platform \npowers Load Balancers, \nMicroservices & API Gateways\n",
      "content_length": 4298,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Michael Hausenblas\nContainer Networking\nFrom Docker to Kubernetes\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n",
      "content_length": 146,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "978-1-492-03681-4\n[LSI]\nContainer Networking\nby Michael Hausenblas\nCopyright © 2018 O’Reilly Media. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online edi‐\ntions are also available for most titles (http://oreilly.com/safari). For more information, contact our\ncorporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Nikki McDonald\nProduction Editors: Melanie Yarbrough\nand Justin Billing\nCopyeditor: Rachel Head\nProofreader: Charles Roumeliotis\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nMay 2018:\n First Edition\nRevision History for the First Edition\n2018-04-17: First Release\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Container Networking, the cover\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsi‐\nbility for errors or omissions, including without limitation responsibility for damages resulting from\nthe use of or reliance on this work. Use of the information and instructions contained in this work is\nat your own risk. If any code samples or other technology this work contains or describes is subject\nto open source licenses or the intellectual property rights of others, it is your responsibility to ensure\nthat your use thereof complies with such licenses and/or rights.\nThis work is part of a collaboration between O’Reilly and NGINX. See our statement of editorial\nindependence.\n",
      "content_length": 1815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Motivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nIntroducing Pets Versus Cattle                                                                                 1\nGo Cattle!                                                                                                                     2\nThe Container Networking Stack                                                                            3\nDo I Need to Go “All In”?                                                                                          4\n2. Introduction to Container Networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  5\nSingle-Host Container Networking 101                                                                  5\nModes for Docker Networking                                                                                 7\nAdministrative Considerations                                                                              10\nWrapping It Up                                                                                                         11\n3. Multi-Host Networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  13\nMulti-Host Container Networking 101                                                                 13\nOptions for Multi-Host Container Networking                                                  13\nDocker Networking                                                                                                  15\nAdministrative Considerations                                                                              16\nWrapping It Up                                                                                                         16\n4. Orchestration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\nWhat Does a Scheduler Actually Do?                                                                    19\nDocker                                                                                                                        20\nApache Mesos                                                                                                           21\nHashicorp Nomad                                                                                                    23\nCommunity Matters                                                                                                 25\nWrapping It Up                                                                                                         25\nv\n",
      "content_length": 2861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "5. Service Discovery. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  27\nThe Challenge                                                                                                           27\nTechnologies                                                                                                              28\nLoad Balancing                                                                                                         32\nWrapping It Up                                                                                                         34\n6. The Container Network Interface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\nHistory                                                                                                                       38\nSpecification and Usage                                                                                           38\nContainer Runtimes and Plug-ins                                                                         40\nWrapping It Up                                                                                                         41\n7. Kubernetes Networking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  43\nA Gentle Kubernetes Introduction                                                                        43\nKubernetes Networking Overview                                                                        45\nIntra-Pod Networking                                                                                             46\nInter-Pod Networking                                                                                              47\nService Discovery in Kubernetes                                                                            50\nIngress and Egress                                                                                                    53\nAdvanced Kubernetes Networking Topics                                                           55\nWrapping It Up                                                                                                         57\nA. References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59\nvi \n| \nTable of Contents\n",
      "content_length": 2408,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "Preface\nWhen you start building your first containerized application, you’re excited\nabout the capabilities and opportunities you encounter: it runs the same in dev\nand in prod, it’s straightforward to put together a container image using Docker,\nand the distribution is taken care of by a container registry.\nSo, you’re satisfied with how quickly you were able to containerize an existing,\nsay, Python app, and now you want to connect it to another container that has a\ndatabase, such as PostgreSQL. Also, you don’t want to have to manually launch\nthe containers and implement your own system that takes care of checking if the\ncontainers are still running and, if not, relaunching them.\nAt this juncture, you might realize there’s a challenge you’re running into: con‐\ntainer networking. Unfortunately, there are still a lot of moving parts in this\ndomain and there are currently few best practice resources available in a central\nplace. Fortunately, there are tons of articles, repos, and recipes available on the\nwider internet and with this book you have a handy way to get access to many of\nthem in a simple and comprehensive format.\nWhy I Wrote This Book\nI thought to myself: what if someone wrote a book providing basic guidance for\nthe container networking topic, pointing readers in the right direction for each of\nthe involved technologies, such as overlay networks, the Container Network\nInterface (CNI), and load balancers?\nThat someone turned out to be me. With this book, I want to provide you with an\noverview of the challenges and available solutions for container networking, con‐\ntainer orchestration, and (container) service discovery. I will try to drive home\nthree points throughout this book:\nvii\n",
      "content_length": 1720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "• Without a proper understanding of the networking aspect of (Docker) con‐\ntainers and a sound strategy in place, you will have more than one bad day\nwhen adopting containers.\n• Service discovery and container orchestration are two sides of the same coin.\n• The space of container networking and service discovery is still relatively\nyoung: you will likely find yourself starting out with one set of technologies\nand then changing gears and trying something else. Don’t worry, you’re in\ngood company.\nWho Is This Book For?\nMy hope is that you’ll find the book useful if one or more of the following applies\nto you:\n• You are a software developer who drank the (Docker) container Kool-Aid.\n• You work in network operations and want to brace yourself for the upcom‐\ning onslaught of your enthusiastic developer colleagues.\n• You are an aspiring Site Reliability Engineer (SRE) who wants to get into the\ncontainer business.\n• You are an (enterprise) software architect who is in the process of migrating\nexisting workloads to a containerized setup.\nLast but not least, distributed application developers and backend engineers\nshould also be able to extract some value out of it.\nNote that this is not a hands-on book. Besides some single-host Docker network‐\ning stuff in Chapter 2 and some of the material about Kubernetes in Chapter 7, I\ndon’t show a lot of commands or source code; consider this book more like a\nguide, a heavily annotated bookmark collection. You will also want to use it to\nmake informed decisions when planning and implementing containerized appli‐\ncations.\nAbout Me\nI work at Red Hat in the OpenShift team, where I help devops to get the most out\nof the software. I spend my time mainly upstream—that is, in the Kubernetes\ncommunity, for example in the Autoscaling, Cluster Lifecycle, and Apps Special\nInterest Groups (SIGs).\nBefore joining Red Hat in the beginning of 2017 I spent some two years at Meso‐\nsphere, where I also did containers, in the context of (surprise!) Mesos. I also\nhave a data engineering background, having worked as Chief Data Engineer at\nviii \n| \nPreface\n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "MapR Inc. prior to Mesosphere, mainly on distributed query engines and data‐\nstores as well as building data pipelines.\nLast but not least, I’m a pragmatist and tried my best throughout the book to\nmake sure to be unbiased toward the technologies discussed here.\nAcknowledgments\nA big thank you to the O’Reilly team, especially Virginia Wilson. Thanks for your\nguidance and feedback on the first iteration of the book (back then called Docker\nNetworking and Service Discovery), which came out in 2015, and for putting up\nwith me again.\nA big thank you to Nic (Sheriff) Jackson of HashiCorp for your time around\nNomad. You rock, dude!\nThanks a million Bryan Boreham of Weaveworks! You provided super-valuable\nfeedback and I appreciate your suggestions concerning the flow as well as your\ndiligence, paying attention to details and calling me out when I drifted off and/or\nmade mistakes. Bryan, who’s a container networking expert and CNI 7th dan, is\nthe main reason this book in its final version turned out to be a pretty good read\n(I think).\nLast but certainly not least, my deepest gratitude to my awesome and supportive\nfamily: our two girls Saphira (aka The Real Unicorn—love you hun :) and Ranya\n(whose talents range from Scratch programming to Irish Rugby), our son Iannis\n(sigh, told you so, you ain’t gonna win the rowing championship with a broken\nhand, but you’re still dope), and my wicked smart and fun wife Anneliese (did I\nempty the washing machine? Not sure!).\nPreface \n| \nix\n",
      "content_length": 1491,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "1 In all fairness, Randy did attribute the origins to Bill Baker of Microsoft.\nCHAPTER 1\nMotivation\nIn this chapter I’ll introduce you to the pets versus cattle approach concerning\ncompute infrastructure as well as what container networking entails. It sets the\nscene, and if you’re familiar with the basics you may want to skip this chapter.\nIntroducing Pets Versus Cattle\nIn February 2012, Randy Bias gave an impactful talk on architectures for open\nand scalable clouds. In his presentation, he established the pets versus cattle\nmeme:1\n• With the pets approach to infrastructure, you treat the machines as individu‐\nals. You give each (virtual) machine a name, and applications are statically\nallocated to machines. For example, db-prod-2 is one of the production\nservers for a database. The apps are manually deployed, and when a machine\ngets ill you nurse it back to health and manually redeploy the app it ran onto\nanother machine. This approach is generally considered to be the dominant\nparadigm of a previous (non–cloud native) era.\n• With the cattle approach to infrastructure, your machines are anonymous;\nthey are all identical (modulo hardware upgrades), they have numbers rather\nthan names, and apps are automatically deployed onto any and each of the\nmachines. When one of the machines gets ill, you don’t worry about it\nimmediately; you replace it—or parts of it, such as a faulty hard disk drive—\nwhen you want and not when things break.\n1\n",
      "content_length": 1457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "2 Typically even heterogeneous hardware. For example, see slide 7 of Thorvald Natvig’s talk “Challenging\nFundamental Assumptions of Datacenters: Decoupling Infrastructure from Hardware” from Velocity\n2015.\nWhile the original meme was focused on virtual machines, we apply the cattle\napproach to infrastructure.\nGo Cattle!\nThe beautiful thing about applying the cattle approach to infrastructure is that it\nallows you to scale out on commodity hardware.2\nIt gives you elasticity with the implication of hybrid cloud capabilities. This is a\nfancy way of saying that you can have parts of your deployments on premises and\nburst into the public cloud—using services provided by the likes of Amazon,\nMicrosoft, and Google, or the infrastructure-as-a-service (IaaS) offerings of dif‐\nferent provides like VMware—if and when you need to.\nMost importantly, from an operator’s point of view, the cattle approach allows\nyou to get a decent night’s sleep, as you’re no longer paged at 3 a.m. just to replace\na broken hard disk drive or to relaunch a hanging app on a different server, as\nyou would have done with your pets.\nHowever, the cattle approach poses some challenges that generally fall into one of\nthe following two categories:\nSocial challenges\nI dare say most of the challenges are of a social nature: How do I convince\nmy manager? How do I get buy-in from my CTO? Will my colleagues oppose\nthis new way of doing things? Does this mean we will need fewer people to\nmanage our infrastructure?\nI won’t pretend to offer ready-made solutions for these issues; instead, go\nbuy a copy of The Phoenix Project by Gene Kim, Kevin Behr, and George\nSpafford (O’Reilly), which should help you find answers.\nTechnical challenges\nThis category includes issues dealing with things like base provisioning of\nthe machines—e.g., using Ansible to install Kubernetes components, how to\nset up the communication links between the containers and to the outside\nworld, and most importantly, how to ensure the containers are automatically\ndeployed and are discoverable.\nNow that you know about pets versus cattle, you are ready to have a look at the\noverall container networking stack.\n2 \n| \nChapter 1: Motivation\n",
      "content_length": 2190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "The Container Networking Stack\nThe overall stack we’re dealing with here is comprised of the following:\nThe low-level networking layer\nThis includes networking gear, iptables, routing, IPVLAN, and Linux\nnamespaces. You usually don’t need to know the details of this layer unless\nyou’re on the networking team, but you should at least be aware of it. Note\nthat the technologies here have existed and been used for a decade or more.\nThe container networking layer\nThis layer provides some abstractions, such as the single-host bridge net‐\nworking mode and the multi-host, IP-per-container solution. I cover this\nlayer in Chapters 2 and 3.\nThe container orchestration layer\nHere, we’re marrying the container scheduler’s decisions on where to place a\ncontainer with the primitives provided by lower layers. In Chapter 4 we look\nat container orchestration systems in general, and in Chapter 5 we focus on\nthe service discovery aspect, including load balancing. Chapter 6 deals with\nthe container networking standard, CNI, and finally in Chapter 7 we look at\nKubernetes networking.\nSoftware-Defined Networking (SDN)\nSDN is really an umbrella (marketing) term, providing essentially the same\nadvantages to networks that virtual machines (VMs) introduced over bare-metal\nservers. With this approach, the network administration team becomes more\nagile and can react faster to changing business requirements. Another way to\nview it is this: SDN is the configuration of networks using software, whether that\nis via APIs, complementing network function virtualization, or the construction\nof networks from software.\nEspecially if you’re a developer or an architect, I suggest taking a quick look at\nCisco’s nice overview on this topic as well as SDxCentral’s article, “What Is\nSoftware-Defined Networking (SDN)?”\nIf you are on the network operations team, you’re probably good to go for the\nnext chapter. However, if you’re an architect or developer and your networking\nknowledge might be a bit rusty, I suggest brushing up by studying the Linux Net‐\nwork Administrators Guide before advancing.\nThe Container Networking Stack \n| \n3\n",
      "content_length": 2121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Do I Need to Go “All In”?\nOftentimes, when I’m at conferences or user groups, I meet people who are very\nexcited about the opportunities in the container space. At the same time, folks\nrightfully worry about how deeply they need to commit to containers in order to\nbenefit from them. The following table provides an informal overview of deploy‐\nments I have seen in the wild, grouped by level of commitment expressed via\nstages:\nStage\nTypical setup\nExamples\nTraditional\nBare metal or VM, no containers\nMajority of today’s prod\ndeployments\nSimple\nManually launched containers used for app-level dependency\nmanagement\nDevelopment and test\nenvironments\nAd hoc\nA custom, homegrown scheduler to launch and potentially restart\ncontainers\nRelateIQ, Uber\nFull-blown\nAn established scheduler from Chapter 4 to manage containers; fault\ntolerant, self-healing\nGoogle, Zulily, Gutefrage.de\nNote that the stage doesn’t necessarily correspond with the size of the deploy‐\nment. For example, Gutefrage.de only has six bare-metal servers under manage‐\nment but uses Apache Mesos to manage them, and you can run a Kubernetes\ncluster easily on a Raspberry Pi.\nOne last remark before we move on: by now, you might have realized that we are\ndealing with distributed systems in general here. Given that we will usually want\nto deploy containers into a network of computers, may I suggest reading up on\nthe fallacies of distributed computing, in case you are not already familiar with\nthis topic?\nAnd now let’s move on to the deep end of container networking.\n4 \n| \nChapter 1: Motivation\n",
      "content_length": 1566,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "CHAPTER 2\nIntroduction to Container Networking\nThis chapter focuses on networking topics for single-host container networking,\nwith an emphasis on Docker. We’ll also have a look at administrative challenges\nsuch as IP address management and security considerations. In Chapter 3, we\nwill discuss multi-host scenarios.\nSingle-Host Container Networking 101\nA container needs a host to run on. This can be a physical machine, such as a\nbare-metal server in your on-premises datacenter, or a virtual machine, either on\npremises or in the cloud.\nIn the case of a Docker container the host has a daemon and a client running, as\ndepicted in Figure 2-1, enabling you to interact with a container registry. Further,\nyou can pull/push container images and start, stop, pause, and inspect containers. \nNote that nowadays most (if not all) containers are compliant with the Open\nContainer Initiative (OCI), and alongside Docker there are interesting alterna‐\ntives, especially in the context of Kubernetes, available.\n5\n",
      "content_length": 1008,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Figure 2-1. Simplified Docker architecture for a single host\nThe relationship between a host and containers is 1:N. This means that one host\ntypically has several containers running on it. For example, Facebook reports that\n—depending on how beefy the machine is—it sees on average some 10 to 40 con‐\ntainers per host running.\nNo matter if you have a single-host deployment or use a cluster of machines, you\nwill likely have to deal with networking:\n• For single-host deployments, you almost always have the need to connect to\nother containers on the same host; for example, an application server like\nWildFly might need to connect to a database.\n• In multi-host deployments, you need to consider two aspects: how containers\nare communicating within a host and how the communication paths look\nbetween different hosts. Both performance considerations and security\naspects will likely influence your design decisions. Multi-host deployments\nusually become necessary either when the capacity of a single host is insuffi‐\ncient, for resilience reasons, or when one wants to employ distributed sys‐\ntems such as Apache Spark or Apache Kafka.\nDistributed Systems and Data Locality\nThe basic idea behind using a distributed system (for computation or storage) is\nto benefit from parallel processing, usually together with data locality. By data\nlocality I mean the principle of shipping the code to where the data is rather than\nthe (traditional) other way around.\n6 \n| \nChapter 2: Introduction to Container Networking\n",
      "content_length": 1513,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "Think about the following for a moment: if your dataset size is in TB scale and\nyour code size is in MB scale, it’s more efficient to move the code across the clus‐\nter than it would be to transfer all the data to a central processing place. In addi‐\ntion to being able to process things in parallel, you usually gain fault tolerance\nwith distributed systems, as parts of the system can continue to work more or\nless independently.\nSimply put, Docker networking is the native container SDN solution you have at\nyour disposal when working with Docker.\nModes for Docker Networking\nIn a nutshell, there are four single-host networking modes available for Docker:\nBridge mode\nUsually used for apps running in standalone containers; this is the default\nnetwork driver. See “Bridge Mode Networking” on page 7 for details.\nHost mode\nAlso used for standalone containers; removes network isolation to the host.\nSee “Host Mode Networking” on page 8 for details.\nContainer mode\nLets you reuse the network namespace of another container. Used in Kuber‐\nnetes. See “Container Mode Networking” on page 9 for details.\nNo networking\nDisables support for networking from the Docker side and allows you to, for\nexample, set up custom networking. See “No Networking” on page 10 for\ndetails.\nWe’ll take a closer look at each of these modes now, and end this chapter with\nsome administrative considerations, including IP/port management and security.\nBridge Mode Networking\nIn this mode (see Figure 2-2), the Docker daemon creates docker0, a virtual \nEthernet bridge that automatically forwards packets between any other network\ninterfaces that are attached to it. By default, the daemon then connects all con‐\ntainers on a host to this internal network by creating a pair of peer interfaces,\nassigning one of the peers to become the container’s eth0 interface and placing\nthe other peer in the namespace of the host, as well as assigning an IP address/\nsubnet from the private IP range to the bridge. Here’s an example of using bridge\nmode:\nModes for Docker Networking \n| \n7\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "$ docker run -d -P --net=bridge nginx:1.9.1\n$ docker ps\nCONTAINER ID   IMAGE                  COMMAND    CREATED\nSTATUS         PORTS                  NAMES\n17d447b7425d   nginx:1.9.1            nginx -g   19 seconds ago\nUp 18 seconds  0.0.0.0:49153->443/tcp,\n               0.0.0.0:49154->80/tcp  trusting_feynman\nBecause bridge mode is the Docker default, you could have used docker\nrun -d -P nginx:1.9.1 in the previous command instead. If you do\nnot use the -P argument, which publishes all exposed ports of the con‐\ntainer, or -p <host_port>:<container_port>, which publishes a spe‐\ncific port, the IP packets will not be routable to the container outside of\nthe host.\nFigure 2-2. Bridge mode networking setup\nHost Mode Networking\nThis mode effectively disables network isolation of a Docker container. Because\nthe container shares the network namespace of the host, it may be directly\nexposed to the public network if the host network is not firewalled. As a conse‐\nquence of the shared namespace, you need to manage port allocations somehow.\nHere’s an example of host mode networking in action:\n$ docker run -d --net=host ubuntu:14.04 tail -f /dev/null\n$ ip addr | grep -A 2 eth0:\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group\ndefault qlen 1000\n    link/ether 06:58:2b:07:d5:f3 brd ff:ff:ff:ff:ff:ff\n    inet **10.0.7.197**/22 brd 10.0.7.255 scope global dynamic eth0\n$ docker ps\nCONTAINER ID  IMAGE         COMMAND  CREATED\n8 \n| \nChapter 2: Introduction to Container Networking\n",
      "content_length": 1516,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "STATUS        PORTS         NAMES\nb44d7d5d3903  ubuntu:14.04  tail -f  2 seconds ago\nUp 2 seconds                jovial_blackwell\n$ docker exec -it b44d7d5d3903 ip addr\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group\ndefault qlen 1000\n    link/ether 06:58:2b:07:d5:f3 brd ff:ff:ff:ff:ff:ff\n    inet **10.0.7.197**/22 brd 10.0.7.255 scope global dynamic eth0\nAnd there we have it: the container has the same IP address as the host, namely\n10.0.7.197.\nIn Figure 2-3 we see that when using host mode networking, the container effec‐\ntively inherits the IP address from its host. This mode is faster than the bridge\nmode because there is no routing overhead, but it exposes the container directly\nto the public network, with all its security implications.\nFigure 2-3. Docker host mode networking setup\nContainer Mode Networking\nIn this mode, you tell Docker to reuse the network namespace of another con‐\ntainer. In general, this mode is useful if you want to have fine-grained control\nover the network stack and/or to control its lifecycle. In fact, Kubernetes net‐\nworking uses this mode, and you can read more about it in Chapter 7. Here it is\nin action:\n$ docker run -d -P --net=bridge nginx:1.9.1\n$ docker ps\nCONTAINER ID  IMAGE        COMMAND   CREATED         STATUS\nPORTS                      NAMES\neb19088be8a0  nginx:1.9.1  nginx -g  3 minutes ago   Up 3 minutes\n0.0.0.0:32769->80/tcp,\n0.0.0.0:32768->443/tcp     admiring_engelbart\n$ docker exec -it admiring_engelbart ip addr\n8: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state\nModes for Docker Networking \n| \n9\n",
      "content_length": 1621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "UP group default\n    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\n    inet **172.17.0.3**/16 scope global eth0\n$ docker run -it --net=container:admiring_engelbart ubuntu:14.04 ip addr\n...\n8: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state\nUP group default\n    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\n    inet **172.17.0.3**/16 scope global eth0\nThe result as shown in this example is what we would have expected: the second\ncontainer, started with --net=container, has the same IP address as the first\ncontainer (namely 172.17.0.3), with the glorious autoassigned name admir\ning_engelbart.\nNo Networking\nThis mode puts the container inside its own network namespace but doesn’t con‐\nfigure it. Effectively, this turns off networking and is useful for two cases: for con‐\ntainers that don’t need a network, such as batch jobs writing to a disk volume, or\nif you want to set up your own custom networking (see Chapter 3 for a number\nof options that leverage this). Here’s an example:\n$ docker run -d -P --net=none nginx:1.9.1\n$ docker ps\nCONTAINER ID  IMAGE          COMMAND   CREATED\nSTATUS        PORTS          NAMES\nd8c26d68037c  nginx:1.9.1    nginx -g  2 minutes ago\nUp 2 minutes                 grave_perlman\n$  docker inspect d8c26d68037c | grep IPAddress\n    \"IPAddress\": \"\",\n    \"SecondaryIPAddresses\": null,\nAs this example shows, there is no network configured, precisely as we would\nhave expected.\nYou can read more about networking and learn about configuration options via\nthe Docker docs.\nAdministrative Considerations\nWe will now briefly discuss other aspects you should be aware of from an admin‐\nistrative point of view. Most of these issues are equally relevant for multi-host\ndeployments:\n10 \n| \nChapter 2: Introduction to Container Networking\n",
      "content_length": 1805,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "1 New Relic, for example, found the majority of the overall uptime of the containers in one particular\nsetup in the low minutes; see also the update here.\nAllocating IP addresses\nManually allocating IP addresses when containers come and go frequently\nand in large numbers is not sustainable.1 The bridge mode takes care of this\nissue to a certain extent. To prevent ARP collisions on a local network, the\nDocker daemon generates a MAC address from the allocated IP address.\nManaging ports\nThere are two approaches to managing ports: fixed port allocation or\ndynamic allocation of ports. The allocation can be per service (or applica‐\ntion) or it can be applied as a global strategy. For bridge mode, Docker can\nautomatically assign (UDP or TCP) ports and consequently make them rout‐\nable. Systems like Kubernetes that sport a flat, IP-per-container networking\nmodel don’t suffer from this issue.\nNetwork security\nOut of the box, Docker has inter-container communication enabled (mean‐\ning the default is --icc=true). This means containers on a host can commu‐\nnicate with each other without any restrictions, which can potentially lead to \ndenial-of-service attacks. Further, Docker controls the communication\nbetween containers and the wider world through the --ip_forward and\n--iptables flags. As a good practice, you should study the defaults of these\nflags and loop in your security team concerning company policies and how\nto reflect them in the Docker daemon setup.\nSystems like CRI-O, the Container Runtime Interface (CRI) using OCI, offer\nalternative runtimes that don’t have one big daemon like Docker has and\npotentially expose a smaller attack surface.\nAnother network security aspect is that of on-the-wire encryption, which\nusually means TLS/SSL as per RFC 5246. \nWrapping It Up\nIn this chapter, we had a look at the four basic single-host networking modes and\nrelated admin issues. Now that you have a basic understanding of the single-host\ncase, let’s have a look at a likely more interesting case: multi-host container net‐\nworking.\nWrapping It Up \n| \n11\n",
      "content_length": 2072,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "CHAPTER 3\nMulti-Host Networking\nAs long as you’re using containers on a single host, the techniques introduced in\nChapter 2 are sufficient. However, if the capacity of a single host is not enough to\nhandle your workload or you want more resilience, you’ll want to scale out hori‐\nzontally.\nMulti-Host Container Networking 101\nWhen scaling out horizontally you end up with a network of machines, also\nknown as a cluster of machines, or cluster for short. Now, a number of questions\narise: How do containers talk to each other on different hosts? How do you con‐\ntrol communication between containers, and with the outside world? How do\nyou keep state, such as IP address assignments, consistent in a cluster? What are\nthe integration points with the existing networking infrastructure? What about\nsecurity policies?\nIn order to address these questions, we’ll review technologies for multi-host con‐\ntainer networking in the remainder of this chapter. Since different use cases and\nenvironments have different requirements, I will abstain from providing a rec‐\nommendation for a particular project or product. You should be aware of the\ntrade-offs and make an informed decision.\nOptions for Multi-Host Container Networking\nIn a nutshell, Docker itself offers support for overlay networks (creating a dis‐\ntributed network across hosts on top of the host-specific network) as well as net‐\nwork plug-ins for third-party providers.\nThere are a number of multi-host container networking options that are often\nused in practice, especially in the context of Kubernetes. These include:\n13\n",
      "content_length": 1581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "• Flannel by CoreOS (see “flannel” on page 14)\n• Weave Net by Weaveworks (see “Weave Net” on page 14)\n• Metaswitch’s Project Calico (see “Project Calico” on page 14)\n• Open vSwitch from the OpenStack project (see “Open vSwitch” on page 15)\n• OpenVPN (see “OpenVPN” on page 15)\nIn addition, Docker offers multi-host networking natively; see “Docker Network‐\ning” on page 15 for details.\nflannel\nCoreOS’s flannel is a virtual network that assigns a subnet to each host for use\nwith container runtimes. Each container—or pod, in the case of Kubernetes—has\na unique, routable IP inside the cluster. flannel supports a range of backends,\nsuch as VXLAN, AWS VPC, and the default layer 2 UDP network. The advantage\nof flannel is that it reduces the complexity of doing port mapping. For example, \nRed Hat’s Project Atomic uses flannel.\nWeave Net\nWeaveworks’s WeaveNet creates a virtual network that connects Docker contain‐\ners deployed across multiple hosts. Applications use the network just as if the\ncontainers were all plugged into the same network switch, with no need to con‐\nfigure port mappings and links. Services provided by application containers on\nthe Weave network can be made accessible to the outside world, regardless of\nwhere those containers are running.\nSimilarly, existing internal systems can be exposed to application containers irre‐\nspective of their location. Weave can traverse firewalls and operate in partially\nconnected networks. Traffic can be encrypted, allowing hosts to be connected\nacross an untrusted network. You can learn more about Weave’s discovery fea‐\ntures in the blog post “Automating Weave Deployment on Docker Hosts with\nWeave Discovery” by Alvaro Saurin.\nIf you want to give Weave a try, check out its Katacoda scenarios.\nProject Calico\nMetaswitch’s Project Calico uses standard IP routing—to be precise, the venera‐\nble Border Gateway Protocol (BGP), as defined in RFC 1105—and networking\ntools to provide a layer 3 solution. In contrast, most other networking solutions\nbuild an overlay network by encapsulating layer 2 traffic into a higher layer.\nThe primary operating mode requires no encapsulation and is designed for data‐\ncenters where the organization has control over the physical network fabric.\n14 \n| \nChapter 3: Multi-Host Networking\n",
      "content_length": 2288,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "See also Canal, which combines Calico’s network policy enforcement with the\nrich superset of Calico and flannel overlay and nonoverlay network connectivi‐\nties.\nOpen vSwitch\nOpen vSwitch is a multilayer virtual switch designed to enable network automa‐\ntion through programmatic extension while supporting standard management\ninterfaces and protocols, such as NetFlow, IPFIX, LACP, and 802.1ag. In addi‐\ntion, it is designed to support distribution across multiple physical servers and is\nused in Red Hat’s Kubernetes distro OpenShift, the default switch in Xen, KVM,\nProxmox VE, and VirtualBox. It has also been integrated into many private cloud\nsystems, such as OpenStack and oVirt.\nOpenVPN\nOpenVPN, another OSS project that has a commercial offering, allows you to\ncreate virtual private networks (VPNs) using TLS. These VPNs can also be used\nto securely connect containers to each other over the public internet. If you want\nto try out a Docker-based setup, I suggest taking a look at DigitalOcean’s “How to\nRun OpenVPN in a Docker Container on Ubuntu 14.04” walk-through tutorial. \nDocker Networking\nDocker 1.9 introduced a new docker network command. With this, containers\ncan dynamically connect to other networks, with each network potentially\nbacked by a different network driver.\nIn March 2015, Docker Inc. acquired the SDN startup SocketPlane and rebran‐\nded its product as the Overlay Driver. Since Docker 1.9, this is the default for\nmulti-host networking. The Overlay Driver extends the normal bridge mode\nwith peer-to-peer communication and uses a pluggable key-value store backend\nto distribute cluster state, supporting Consul, etcd, and ZooKeeper.\nTo learn more, I suggest checking out the following blog posts:\n• Aleksandr Tarasov’s “Splendors and Miseries of Docker Network”\n• Project Calico’s “Docker 1.9 Includes Network Plugin Support and Calico Is\nReady!”\n• Weaveworks’s “Life and Docker Networking – One Year On”\nDocker Networking \n| \n15\n",
      "content_length": 1964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "Administrative Considerations\nIn the last section of this chapter we will discuss some administrative aspects you\nshould be aware of:\nIPVLAN\nLinux kernel version 3.19 introduced an IP-per-container feature. This\nassigns each container on a host a unique and routable IP address. Effec‐\ntively, IPVLAN takes a single network interface and creates multiple virtual\nnetwork interfaces with different MAC addresses assigned to them.\nThis feature, which was contributed by Mahesh Bandewar of Google, is con‐\nceptually similar to the macvlan driver but is more flexible because it’s oper‐\nating both on L2 and L3. If your Linux distro already has a kernel > 3.19,\nyou’re in luck. Otherwise, you cannot yet benefit from this feature.\nIP address management (IPAM)\nOne of the key challenges of multi-host networking is the allocation of IP\naddresses to containers in a cluster. There are two strategies one can pursue:\neither find a way to realize it in your existing (corporate) network or spawn\nan orthogonal, practically hidden networking layer (that is, an overlay net‐\nwork). Note that with IPv6 this situation is relaxed, since it should be a lot\neasier to find a free address space.\nOrchestration tool compatibility\nMany of the multi-host networking solutions discussed in this chapter are\neffectively coprocesses wrapping the Docker API and configuring the net‐\nwork for you. This means that before you select one, you should make sure\nto check for any compatibility issues with the container orchestration tool\nyou’re using. You’ll find more on this topic in Chapter 4.\nIPv4 versus IPv6\nTo date, most Docker deployments use the standard IPv4, but IPv6 is wit‐\nnessing some uptake. Docker has supported IPv6 since v1.5, released in Feb‐\nruary 2015; however, the IPv6 support in Kubernetes is not yet complete.\nThe ever-growing address shortage in IPv4-land might encourage more IPv6\ndeployments down the line, also getting rid of network address translation\n(NAT), but it is unclear when exactly the tipping point will be reached.\nWrapping It Up\nIn this chapter, we reviewed multi-host networking options and touched on\nadmin issues such as IPAM and orchestration. At this juncture you should have a\ngood understanding of the low-level single-host and multi-host networking\noptions and their challenges. Let’s now move on to container orchestration, look‐\ning at how it depends on networking and how it interacts with it.\n16 \n| \nChapter 3: Multi-Host Networking\n",
      "content_length": 2461,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "CHAPTER 4\nOrchestration\nWith the cattle approach to managing infrastructure, you don’t manually allocate\ncertain machines for running an application. Instead, you leave it up to an\norchestrator to manage the life cycle of your containers. In Figure 4-1, you can\nsee that container orchestration includes a range of functions, including but not\nlimited to:\n• Organizational primitives, such as labels in Kubernetes, to query and group\ncontainers\n• Scheduling of containers to run on a host\n• Automated health checks to determine if a container is alive and ready to\nserve traffic and to relaunch it if necessary\n• Autoscaling (that is, increasing or decreasing the number of containers based\non utilization or higher-level metrics)\n• Upgrade strategies, from rolling updates to more sophisticated techniques\nsuch as A/B and canary deployments\n• Service discovery to determine which host a scheduled container ended\nupon, usually including DNS support\n17\n",
      "content_length": 953,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "Figure 4-1. Orchestration and its constituents\nSometimes considered part of orchestration but outside the scope of this book is\nthe topic of base provisioning—that is, installing or upgrading the local operating\nsystem on a node or setting up the container runtime there.\nService discovery (covered in greater detail in Chapter 5) and scheduling are\nreally two sides of the same coin. The scheduler decides where in a cluster a con‐\ntainer is placed and supplies other parts with an up-to-date mapping in the form\ncontainers -> locations. This mapping can then be represented in various\nways, be it in a distributed key-value store such as etcd, via DNS, or through\nenvironment variables.\nIn this chapter we will discuss networking and service discovery from the point\nof view of the following container orchestration solutions: Docker Swarm and\nswarm mode, Apache Mesos, and HashiCorp Nomad. These three are (along\nwith Kubernetes, which we will cover in detail in Chapter 7) alternatives your\norganization may already be using, and hence, for the sake of completeness, it’s\nworth exploring them here. To make it clear, though, as of early 2018 the indus‐\ntry has standardized on Kubernetes as the portable way of doing container\norchestration.\nIn addition to the three orchestrators discussed in this chapter, there are\nother (closed source) solutions out there you could have a look at,\nincluding Facebook’s Bistro or hosted solutions such as Amazon ECS.\nShould you want to more fully explore the topic of distributed system\nscheduling, I suggest reading Google’s research papers on Borg and\nOmega.\n18 \n| \nChapter 4: Orchestration\n",
      "content_length": 1634,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "Before we dive into container orchestration systems, though, let’s step back and\nreview what the scheduler—which is the core component of orchestration—\nactually does in the context of containerized workloads.\nWhat Does a Scheduler Actually Do?\nA scheduler for a distributed system takes an application—binary or container\nimage—as requested by a user and places it on one or more of the available hosts.\nFor example, a user might request to have 100 instances of the app running, so\nthe scheduler needs to find space (CPU, RAM) to run these 100 instances on the\navailable hosts.\nIn the case of a containerized setup, this means that the respective container\nimage must exist on a host (if not, it must be pulled from a container registry\nfirst), and the scheduler must instruct the container runtime on that host to\nlaunch a container based on the image.\nLet’s look at a concrete example. In Figure 4-2, you can see that the user reques‐\nted three instances of the app running in the cluster. The scheduler decides the\nplacement based on its knowledge of the state of the cluster. The cluster state may\ninclude the utilization of the machines, the resources necessary to successfully\nlaunch the app, and constraints such as launch this app only on a machine that is\nSSD-backed.\nFurther, quality of service might be taken into account for the placement deci‐\nsion; see Michael Gasch’s great article “QoS, Node allocatable and the Kubernetes\nScheduler” for more details.\nFigure 4-2. Distributed system scheduler in action\nIf you want to learn more about scheduling in distributed systems I suggest you\ncheck out the excellent resource “Cluster Management at Google” by John\nWilkes.\nWhat Does a Scheduler Actually Do? \n| \n19\n",
      "content_length": 1723,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "1 Essentially, this means that you can simply keep using docker run commands and the deployment of\nyour containers in a cluster happens automagically.\nBeware of the semantics of constraints that you can place on scheduling\ncontainers. For example, I once gave a demo using Marathon that\nwouldn’t work as planned because I screwed up the placement con‐\nstraints: I used a combination of unique hostname and a certain role,\nand it wouldn’t scale because there was only one node with the specified\nrole in the cluster. The same thing can happen with Kubernetes labels.\nDocker\nDocker at the time of writing uses the so-called swarm mode in a distributed set‐\nting, whereas previous to Docker 1.12 the standalone Docker Swarm model was\nused. We will discuss both here.\nSwarm Mode\nSince Docker 1.12, swarm mode has been integrated with Docker Engine. The\norchestration features embedded in Docker Engine are built using SwarmKit.\nA swarm in Docker consists of multiple hosts running in swarm mode and acting\nas managers and workers—hosts can be managers, workers, or perform both\nroles at once. A task is a running container that is part of a swarm service and\nmanaged by a swarm manager, as opposed to a standalone container. A service in\nthe context of Docker swarm mode is a definition of the tasks to execute on the\nmanager or worker nodes. Docker works to maintain that desired state; for\nexample, if a worker node becomes unavailable, Docker schedules the tasks onto\nanother host.\nDocker running in swarm mode doesn’t prevent you from running standalone\ncontainers on any of the hosts participating in the swarm. The essential differ‐\nence between standalone containers and swarm services is that only swarm man‐\nagers can manage a swarm, while standalone containers can be started on any\nhost.\nTo learn more about Docker’s swarm mode, check out the official “Getting\nStarted with Swarm Mode” tutorial or check out the Katacoda “Docker Orches‐\ntration – Getting Started with Swarm Mode” scenario.\nDocker Swarm\nDocker historically had a native clustering tool called Docker Swarm. Docker\nSwarm builds upon the Docker API1 and works as follows: there’s one Swarm\n20 \n| \nChapter 4: Orchestration\n",
      "content_length": 2193,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "manager that’s responsible for scheduling, and on each host an agent runs that\ntakes care of the local resource management (Figure 4-3).\nFigure 4-3. Docker Swarm architecture, based on the T-Labs presentation “Swarm –\nA Docker Clustering System”\nDocker Swarm supports different backends: etcd, Consul, and ZooKeeper. You\ncan also use a static file to capture your cluster state with Swarm, and recently a\nDNS-based service discovery tool for Swarm called wagl has been introduced.\nOut of the box, Docker provides a basic service discovery mechanism\nfor single-node deployments called Docker links. Linking allows a user\nto let any container discover both the IP address and exposed ports of\nother Docker containers on the same host. In order to accomplish this,\nDocker provides the --link flag. But hard-wiring of links between con‐\ntainers is neither fun nor scalable. In fact, it’s so bad that this feature has\nbeen deprecated.\nApache Mesos\nApache Mesos (Figure 4-4) is a general-purpose cluster resource manager that\nabstracts the resources of a cluster (CPU, RAM, etc.) in such a way that the clus‐\nter appears like one giant computer to the developer. In a sense, Mesos acts like\nthe kernel of a distributed operating system. It is hence never used on its own,\nbut always together with so-called frameworks such as Marathon (for long-\nrunning stuff like a web server) or Chronos (for batch jobs), or big data and fast\ndata frameworks like Apache Spark or Apache Cassandra.\nApache Mesos \n| \n21\n",
      "content_length": 1498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "Figure 4-4. Apache Mesos architecture at a glance\nMesos supports both containerized workloads (that is, running Docker contain‐\ners) and plain executables (for example, bash scripts or Linux ELF format binar‐\nies for both stateless and stateful services).\nIn the following discussion, I’m assuming you’re familiar with Mesos and its ter‐\nminology. If you’re new to Mesos, I suggest checking out David Greenberg’s won‐\nderful book Building Applications on Mesos (O’Reilly), a gentle introduction to\nthis topic that’s particularly useful for distributed application developers.\nThe networking characteristics and capabilities mainly depend on the Mesos\ncontainerizer used:\n• For the Mesos containerizer there are a few prerequisites, such as having a\nLinux Kernel version > 3.16 and libnl installed. You can then build a Mesos\nagent with network isolator support enabled. At launch, you would use\nsomething like the following:\n$mesos-slave --containerizer=mesos\n --isolation=network/port_mapping\n --resources=ports:[31000-32000];ephemeral_ports:[33000-35000]\nThis would configure the Mesos agent to use nonephemeral ports in the\nrange from 31,000 to 32,000 and ephemeral ports in the range from 33,000 to\n22 \n| \nChapter 4: Orchestration\n",
      "content_length": 1235,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "35,000. All containers share the host’s IP, and the port ranges are spread over\nthe containers (with a 1:1 mapping between destination port and container\nID). With the network isolator, you also can define performance limitations\nsuch as bandwidth, and it enables you to perform per-container monitoring\nof the network traffic. See Jie Yu’s MesosCon 2015 talk “Per Container Net‐\nwork Monitoring and Isolation in Mesos” for more details on this topic.\n• For the Docker containerizer, see Chapter 2.\nNote that Mesos supports IP-per-container since version 0.23. If you want to\nlearn more about Mesos networking check out Christos Kozyrakis and Spike\nCurtis’s “Mesos Networking” talk from MesosCon 2015.\nWhile Mesos is not opinionated about service discovery, there is a Mesos-specific\nsolution that is often used in practice: Mesos-DNS (see “Pure-Play DNS-Based\nSolutions” on page 31). There are also a multitude of emerging solutions, such as\ntraefik (see “Wrapping It Up” on page 34) that are integrated with Mesos and\ngaining traction.\nBecause Mesos-DNS is the recommended default service discovery\nmechanism with Mesos, it’s important to pay attention to how Mesos-\nDNS represents tasks. For example, a running task might have the (logi‐\ncal) service name webserver.marathon.mesos, and you can find out the\nport allocations via DNS SRV records.\nIf you want to try out Mesos online for free you can use the Katacoda “Deploying\nContainers to DC/OS” scenario.\nHashicorp Nomad\nNomad is a cluster scheduler by HashiCorp, the makers of Vagrant. It was intro‐\nduced in September 2015 and primarily aims at simplicity. The main idea is that \nNomad is easy to install and use. Its scheduler design is reportedly inspired by\nGoogle’s Omega, borrowing concepts such as having a global state of the cluster\nas well as employing an optimistic, concurrent scheduler.\nNomad has an agent-based architecture with a single binary that can take on dif‐\nferent roles, supporting rolling upgrades as well as draining nodes for re-\nbalancing. Nomad makes use of both a consensus protocol (strongly consistent)\nfor all state replication and scheduling and a gossip protocol used to manage the\naddresses of servers for automatic clustering and multiregion federation. In\nFigure 4-5, you can see Nomad’s architecture:\n• Servers are responsible for accepting jobs from users, managing clients, and\ncomputing task placements.\nHashicorp Nomad \n| \n23\n",
      "content_length": 2425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "• Clients (one per VM instance) are responsible for interacting with the tasks\nor applications contained within a job. They work in a pull-based manner;\nthat is, they register with the server and then they poll it periodically to\nwatch for pending work.\nFigure 4-5. Nomad architecture\nJobs in Nomad are defined in a HashiCorp-proprietary format called HCL or in\nJSON, and Nomad offers a command-line interface as well as an HTTP API to\ninteract with the server process. Nomad models infrastructure as regions and\ndatacenters. Regions may contain multiple datacenters, depending on what scale\nyou are operating at. You can think of a datacenter like a zone in AWS, Azure, or\nGoogle Cloud (say, us-central1-b), and a region might be something like Iowa\n(us-central1).\nI’m assuming you’re familiar with Nomad and its terminology. If not, I suggest\nyou watch “Nomad: A Distributed, Optimistically Concurrent Schedule: Armon\nDadgar, HashiCorp”, a nice introduction to Nomad, and also read the well-done\ndocs.\nTo try out Nomad, use the UI Demo HashiCorp provides or try it out online for\nfree using the Katacoda “Introduction to Nomad” scenario.\nNomad comes with a couple of so-called task drivers, from general-purpose exec\nto java to qemu and docker. For the docker driver Nomad requires, at the time of\nthis writing, Docker version 1.10 or greater and uses port binding to expose serv‐\nices running in containers using the port space on the host’s interface. It provides\nautomatic and manual mapping schemes for Docker, binding both TCP and\nUDP protocols to ports used for Docker containers.\nFor more details on networking options, such as mapping ports and using labels,\nsee the documentation.\nWith v0.2, Nomad introduced a Consul-based (see “Consul” on page 30) service\ndiscovery mechanism. It includes health checks and assumes that tasks running\ninside Nomad also need to be able to connect to the Consul agent, which can, in\nthe context of containers using bridge mode networking, pose a challenge.\n24 \n| \nChapter 4: Orchestration\n",
      "content_length": 2033,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "2 Now, you might argue that this is not specific to the container orchestration domain but a general OSS\nissue, and you’d be right. Still, I believe it is important enough to mention it, as many people are new to\nthis area and can benefit from these insights.\nCommunity Matters\nAn important aspect you’ll want to consider when selecting an orchestration sys‐\ntem is the community behind and around it.2 Here are a few indicators and met‐\nrics you can use:\n• Is the governance backed by a formal entity or process, such as the Apache\nSoftware Foundation (ASF) or the Linux Foundation (LF)?\n• How active are the mailing list, the IRC channel, the bug/issue tracker, the\nGit repo (number of patches or pull requests), and other community initia‐\ntives?\nTake a holistic view, but make sure that you actually pay attention to the\nactivity there. Healthy and hopefully growing communities will tend to have\nhigh participation in at least one of these areas.\n• Is the orchestration tool (implicitly) controlled by a single entity? For exam‐\nple, in the case of Nomad HashiCorp is in control, for Apache Mesos it’s\nmainly Mesosphere (and to some extent Twitter), etc.\n• Are multiple independent providers and support channels available? For\nexample, you can run Kubernetes in many different environments and get\nhelp from many (commercial) organizations as well as individuals on Slack,\nmailing lists, or forums.\nWrapping It Up\nAs of early 2018, Kubernetes (discussed in Chapter 7) can be considered the de\nfacto container orchestration standard. All major providers, including Docker\nand DC/OS (Mesos), support Kubernetes.\nNext, we’ll move on to service discovery, a vital part of container orchestration.\nCommunity Matters \n| \n25\n",
      "content_length": 1724,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "CHAPTER 5\nService Discovery\nOne challenge arising from adopting the cattle approach to managing infrastruc‐\nture is service discovery. If you subscribe to the cattle approach, you treat all of\nyour machines equally and you do not manually allocate certain machines for\ncertain applications; instead, you leave it up to a piece of software (the scheduler)\nto manage the life cycle of the containers.\nThe question then is, how do you determine which host your container ended up\nbeing scheduled on so that you can connect to it? This is called service discovery,\nand we touched on it already in Chapter 4.\nThe Challenge\nService discovery has been around for a while—essentially, as long as distributed\nsystems and services have existed. In the context of containers, the challenge\nboils down to reliably maintaining a mapping between a running container and\nits location. By location, I mean its IP address and the port on which it is reacha‐\nble. This mapping has to be done in a timely manner and accurately across\nrelaunches of the container throughout the cluster. Two distinct operations must\nbe supported by a container service discovery solution:\nRegistration\nEstablishes the container -> location mapping. Because only the con‐\ntainer scheduler knows where containers “live,” we can consider it to be the\nabsolute source of truth concerning a container’s location.\nLookup\nEnables other services or applications to look up the mapping we stored dur‐\ning registration. Interesting properties include the freshness of the informa‐\ntion and the latency of a query (average, p50, p90, etc.).\n27\n",
      "content_length": 1596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "1 ZooKeeper was originally developed at Yahoo! in order to get its ever-growing zoo of software tools,\nincluding Hadoop, under control.\nLet’s examine a few slightly orthogonal considerations for the selection process:\n• Rather than simply sending a requestor in a certain direction, how about\nexcluding unhealthy hosts or hanging containers from the lookup path?\nYou’ve guessed it, this is the strongly related topic of load balancing, and\nbecause it is of such importance we’ll discuss options in the last section of\nthis chapter.\n• Some argue it’s an implementation detail, others say the position in the CAP\ntriangle matters: the choice of strong consistency versus high availability in\nthe context of the service discovery tool might influence your decision. Be at\nleast aware of it.\n• Your choice might also be impacted by scalability considerations. Sure, if you\nonly have a handful of nodes under management then all of the solutions\ndiscussed here will be a fit. If your cluster, however, has several hundred or\neven thousands of nodes, then you will want to make sure you do some\nproper load testing before you commit to one particular technology.\nIn this chapter you’ll learn about service discovery options and how and where to\nuse them.\nIf you want to learn more about the requirements and fundamental challenges in\nthis space, read Jeff Lindsay’s “Understanding Modern Service Discovery with\nDocker” and check out what Simon Eskildsen of Shopify shared on this topic at a\nrecent DockerCon.\nTechnologies\nThis section briefly introduces a variety of service discovery technologies, listing\npros and cons and pointing to further discussions on the web. For a more in-\ndepth treatment, check out Adrian Mouat’s excellent book Using Docker\n(O’Reilly).\nZooKeeper\nApache ZooKeeper is an ASF top-level project and a JVM-based, centralized tool\nfor configuration management,1 providing comparable functionality to what\nGoogle’s Chubby brings to the table. ZooKeeper (ZK) organizes its payload data\nsomewhat like a filesystem, in a hierarchy of so-called znodes. In a cluster, a\nleader is elected and clients can connect to any of the servers to retrieve data. You\nwant 2n+1 nodes in a ZK cluster. The most often found configurations in the\n28 \n| \nChapter 5: Service Discovery\n",
      "content_length": 2280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "2 Did you know that etcd comes from /etc distributed? What a name!\nwild are three, five, or seven nodes. Beyond that, you’ll experience diminishing\nreturns concerning the fault tolerance/throughput trade-off.\nZooKeeper is a battle-proven, mature, and scalable solution, but it has some\noperational downsides. Some people consider the installation and management\nof a ZK cluster a not-so-enjoyable experience. Most ZK issues I’ve seen come\nfrom the fact that certain services (Apache Storm comes to mind) misuse it. They\neither put too much data into the znodes or, worse, have an unhealthy read/write\nratio, essentially writing too fast. If you plan to use ZK, at least consider using\nhigher-level interfaces such as Apache Curator, which is a wrapper library\naround ZK implementing a number of recipes, and Netflix’s Exhibitor for man‐\naging and monitoring ZK clusters.\nLooking at Figure 5-1, you see two components: R/W (which stands for registra‐\ntion watcher, a piece of software you need to provide yourself), and HAProxy,\nwhich is controlled by the R/W. Whenever a container is scheduled on a node it\nregisters with ZK, using a znode with a path like /$nodeID/$containerID and the\nIP address as its payload. The R/W watches changes on those znodes and config‐\nures HAProxy accordingly.\nFigure 5-1. Example service discovery setup with ZooKeeper\netcd\nWritten in the Go language, etcd is a product of the CoreOS team.2 It is a light‐\nweight, distributed key-value store that uses the Raft algorithm for consensus (a\nleader–follower model, with leader election) and employs a replicated log across\nTechnologies \n| \n29\n",
      "content_length": 1621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "3 That is, in contrast to ZK, all you need to interact with etcd is curl or the like.\nthe cluster to distribute the writes a leader receives to its followers. In a sense,\netcd is conceptually quite similar to ZK. While the payload can be arbitrary, etcd’s\nHTTP API is JSON-based,3 and as with ZK, you can watch for changes in the val‐\nues etcd makes available to the cluster. A very useful feature of etcd is that of\nTTLs on keys, which is a great building block for service discovery. In the same\nmanner as ZK, you want 2n+1 nodes in an etcd cluster, for the same reasons.\nThe security model etcd provides allows on-the-wire encryption through\nTLS/SSL as well as client certificate authentication, both between clients and the\ncluster and between the etcd nodes.\nIn Figure 5-2, you can see that the etcd service discovery setup is quite similar to\nthe ZK setup. The main difference is the usage of confd, which configures\nHAProxy, rather than having you write your own script.\nFigure 5-2. Example service discovery setup with etcd\nConsul\nConsul, a HashiCorp product also written in the Go language, exposes function‐\nality for service registration, discovery, and health checking in an opinionated\nway. Services can be queried using the HTTP API or through DNS. Consul sup‐\nports multi-datacenter deployments.\nOne of Consul’s features is a distributed key-value store, akin to etcd. It also uses\nthe Raft consensus algorithm (and again the same observations concerning 2n+1\nnodes as with ZK and etcd apply), but the deployment is different. Consul has the\n30 \n| \nChapter 5: Service Discovery\n",
      "content_length": 1593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "4 Java, I’m looking at you.\nconcept of agents, which can be run in either of the two available modes—as a\nserver (provides a key-value store and DNS) or as a client (registers services and\nruns health checks)—and with the membership and node discovery imple‐\nmented by Serf.\nWith Consul, you have essentially four options to implement service discovery\n(ordered from most desirable to least desirable):\n• Use a service definition config file, interpreted by the Consul agent.\n• Use a tool like traefik that has a Consul backend.\n• Write your own sidekick process that registers the service through the HTTP\nAPI.\n• Bake the registration into the service itself by leveraging the HTTP API.\nWant to learn more about using Consul for service discovery? Check out these\ntwo great blog posts: “Consul Service Discovery with Docker” by Jeff Lindsay and\n“Docker DNS & Service Discovery with Consul and Registrator” by Joseph\nMiller.\nPure-Play DNS-Based Solutions\nDNS has been a robust and battle-proven workhorse on the internet for many\ndecades. The eventual consistency of the DNS system, the fact that certain clients\naggressively cache DNS lookups,4 and also the reliance on SRV records make this\noption something you will want to use when you know that it is the right one.\nI’ve titled this section “Pure-Play DNS-Based Solutions” because Consul techni‐\ncally also has a DNS server, but that’s only one option for how you can use it to\ndo service discovery. Here are some popular and widely used pure-play DNS-\nbased service discovery solutions:\nMesos-DNS\nThis solution is specific for service discovery in Apache Mesos. Written in\nGo, Mesos-DNS polls the active Mesos master process for any running tasks\nand exposes the <ip>:<port> info via DNS as well as through an HTTP API.\nFor DNS requests for other hostnames or services, Mesos-DNS can either\nuse an external nameserver or leverage your existing DNS server to forward\nonly the requests for Mesos tasks to Mesos-DNS.\nSkyDNS\nUsing etcd, you can announce your services to SkyDNS, which stores service\ndefinitions into etcd and updates its DNS records. Your client application\nTechnologies \n| \n31\n",
      "content_length": 2147,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "issues DNS queries to discover the services. Thus, functionality-wise it is\nquite similar to Consul, without the health checks.\nWeaveDNS\nWeaveDNS was introduced in Weave 0.9 as a simple solution for service dis‐\ncovery on the Weave network, allowing containers to find other containers’\nIP addresses by their hostnames. In Weave 1.1, a so-called Gossip DNS pro‐\ntocol was introduced, making lookups faster through a cache as well as\nincluding timeout functionality. In the new implementation, registrations are\nbroadcast to all participating instances, which subsequently hold all entries\nin memory and handle lookups locally.\nAirbnb’s SmartStack and Netflix’s Eureka\nIn this section, we’ll take a look at two bespoke systems that were developed to\naddress specific requirements. This doesn’t mean you can’t or shouldn’t use them,\njust that you should be aware of this heritage.\nAirbnb’s SmartStack is an automated service discovery and registration frame‐\nwork, transparently handling creation, deletion, failure, and maintenance work.\nSmartStack uses two separate services that run on the same host as your con‐\ntainer: Nerve (writing into ZK) for service registration, and Synapse (dynamically\nconfiguring HAProxy) for lookup. It is a well-established solution for non-\ncontainerized environments, and time will tell if it will also be as useful with\nDocker.\nNetflix’s Eureka is different, mainly because it was born in the AWS environment\n(where all of Netflix runs). Eureka is a REST-based service used for locating serv‐\nices for the purpose of load balancing and failover of middle-tier servers, and it\nalso comes with a Java-based client component, which makes interactions with\nthe service straightforward. This client has a built-in load balancer that does\nbasic round-robin load balancing. At Netflix, Eureka is used for red/black\ndeployments, for Cassandra and memcached deployments, and for carrying\napplication-specific metadata about services.\nParticipating nodes in a Eureka cluster replicate their service registries between\neach other asynchronously. In contrast to ZK, etcd, or Consul, Eureka favors ser‐\nvice availability over strong consistency; it leaves it up to the client to deal with\nstale reads, but has the upside of being more resilient in case of networking parti‐\ntions. And, you know, the network is reliable. Not.\nLoad Balancing\nAn orthogonal but related topic to that of service discovery is load balancing. \nLoad balancing enables you to spread the load—that is, the inbound service\n32 \n| \nChapter 5: Service Discovery\n",
      "content_length": 2554,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "requests—across a number of containers. In the context of containers and micro‐\nservices, load balancing achieves a couple of things at the same time:\n• It allows throughput to be maximized and response time to be minimized.\n• It can avoid hot-spotting—that is, overwhelming a single container with\nwork.\n• It can help with overly aggressive DNS caching, such as that found with Java.\nThe following list outlines some popular load balancing options for container‐\nized setups, in alphabetical order:\nBamboo\nA daemon that automatically configures HAProxy instances, deployed on\nApache Mesos and Marathon. See the p24e guide “Service Discovery with\nMarathon, Bamboo and HAProxy” for a concrete recipe.\nEnvoy\nA high-performance distributed proxy written in C++, originally built at\nLyft. Envoy was designed to be used for single services and applications, and\nto provide a communication bus and data plane for service meshes. It’s the\ndefault data plane in Istio.\nHAProxy\nA stable, mature, and battle-proven (if not very feature-rich) workhorse.\nOften used in conjunction with NGINX, HAProxy is reliable and integra‐\ntions with pretty much everything under the sun exist.\nkube-proxy\nRuns on each node of a Kubernetes cluster and updates services IPs. It sup‐\nports simple TCP/UDP forwarding and round-robin load balancing. Note\nthat it’s only for cluster-internal load balancing and also serves as a service\ndiscovery support component.\nMetalLB\nA load-balancer implementation for bare-metal Kubernetes clusters,\naddressing the fact that Kubernetes does not offer a default implementation\nfor such clusters. In other words, you need to be in a public cloud environ‐\nment to benefit from this functionality. Note that you may need one or more\nrouters capable of speaking BGP in order for MetalLB to work.\nNGINX\nThe leading solution in this space. With NGINX you get support for round-\nrobin, least-connected, and ip-hash strategies, as well as on-the-fly con‐\nfiguration, monitoring, and many other vital features.\nLoad Balancing \n| \n33\n",
      "content_length": 2032,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "servicerouter.py\nA simple script that gets app configurations from Marathon and updates\nHAProxy; see also the p24e guide “Service Discovery with Marathon, Bam‐\nboo and HAProxy”.\ntraefik\nThe rising star in this category. Emile Vauge (traefik’s lead developer) must\nbe doing something right. I like it a lot, because it’s like HAProxy but comes\nwith a bunch of backends, such as Marathon and Consul, out of the box.\nVamp-router\nInspired by Bamboo and Consul–HAProxy, Magnetic.io wrote Vamp-router,\nwhich supports updates of the config through a REST API or ZooKeeper,\nroutes and filters for canary releasing and A/B testing, and ACLs, as well as\nproviding statistics.\nVulcand\nA reverse proxy for HTTP API management and microservices, inspired by\nHystrix.\nIf you want to learn more about load balancing, check out Kevin Reedy’s talk\nfrom nginx.conf 2014 on load balancing with NGINX and Consul.\nWrapping It Up\nTo close out this chapter, I’ve put together a table that provides an overview of\nthe service discovery solutions we’ve discussed. I explicitly do not aim at declar‐\ning a winner, because I believe the best choice depends on your use case and\nrequirements. So, take the following table as a quick orientation and summary\nbut not as a shootout (also, note that in the context of Kubernetes you don’t need\nto choose one—it’s built into the system):\nName\nConsistency\nLanguage Registration\nLookup\nZooKeeper\nStrong\nJava\nClient\nBespoke clients\netcd\nStrong\nGo\nSidekick + client\nHTTP API\nConsul\nStrong\nGo\nAutomatic and through traefik (Consul\nbackend)\nDNS + HTTP/JSON API\nMesos-DNS\nStrong\nGo\nAutomatic and through traefik\n(Marathon backend)\nDNS + HTTP/JSON API\nSkyDNS\nStrong\nGo\nClient registration\nDNS\nWeaveDNS\nEventual\nGo\nAuto\nDNS\nSmartStack Strong\nJava\nClient registration\nAutomatic through HAProxy config\nEureka\nEventual\nJava\nClient registration\nBespoke clients\n34 \n| \nChapter 5: Service Discovery\n",
      "content_length": 1901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "Because container service discovery is a moving target, you are well\nadvised to reevaluate your initial choices on an ongoing basis, at least\nuntil some consolidation has taken place.\nIn this chapter you learned about service discovery and how to tackle it, as well\nas about load balancing options. We will now switch gears and move on to\nKubernetes, the de facto container orchestration standard that comes with built-\nin service discovery (so you don’t need to worry about the topics discussed in this\nchapter) and has its own very interesting approach to container networking\nacross machines.\nWrapping It Up \n| \n35\n",
      "content_length": 618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "CHAPTER 6\nThe Container Network Interface\nThe Container Network Interface (CNI), as depicted in Figure 6-1, provides a\nplug-in-oriented networking solution for containers and container orchestrators.\nIt consists of a specification and libraries for writing plug-ins to configure net‐\nwork interfaces in Linux containers.\nFigure 6-1. 100,000 ft view on CNI\nThe CNI specification is lightweight; it only deals with the network connectivity\nof containers, as well as the garbage collection of resources once containers are\ndeleted.\nWe will focus on CNI in this book since it’s the de facto standard for container\norchestrators, adopted by all major systems such as Kubernetes, Mesos, and\nCloud Foundry. If you’re exclusively using Docker Swarm you’ll need to use\n37\n",
      "content_length": 763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "Docker’s libnetwork and might want to read the helpful article by Lee Calcote\ntitled “The Container Networking Landscape: CNI from CoreOS and CNM from\nDocker”, which contrasts CNI with the Docker model and provides you with\nsome guidance.\nHistory\nCNI was pioneered by CoreOS in the context of the container runtime rkt, to\ndefine a common interface between the network plug-ins and container runtimes\nand orchestrators. Docker initially planned to support it but then came up with\nthe Docker-proprietary libnetwork approach to container networking.\nCNI and the libnetwork plug-in interface were developed in parallel from April\nto June 2015, and after some discussion the Kubernetes community decided not\nto adopt libnetwork but rather to use CNI. Nowadays pretty much every con‐\ntainer orchestrator with the exception of Docker Swarm uses CNI; all runtimes\nsupport it and there’s a long list of supported plug-ins, as discussed in “Container\nRuntimes and Plug-ins” on page 40.\nIn May 2017, the Cloud Native Computing Foundation (CNCF) made CNI a full-\nblown top-level project.\nSpecification and Usage\nIn addition to the CNI specification, at time of writing in version 0.3.1-dev, the\nrepository contains the Go source code of a library for integrating CNI into\napplications as well as an example command-line tool for executing CNI plug-\nins. The plug-ins repository contains reference plug-ins and a template for creat‐\ning new plug-ins.\nBefore we dive into the usage, let’s look at two central definitions in the context of\nCNI:\nContainer\nSynonymous with a Linux network namespace. What unit this corresponds\nto depends on the container runtime implementation (single container or\npod).\nNetwork\nA uniquely addressable group of entities that can communicate with one\nanother. These entities might be an individual container, a machine, or some\nother network device such as a router.\nLet’s now have a look at how CNI—on a high level—works, as depicted in\nFigure 6-2. First, the container runtime takes some configuration and issues a\ncommand to a plug-in. The plug-in then goes off and configures the network.\n38 \n| \nChapter 6: The Container Network Interface\n",
      "content_length": 2161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "Figure 6-2. CNI high-level architecture\nSo, what CNI conceptually enables you to do is to add containers to a network as\nwell as remove them. The current version of CNI defines the following opera‐\ntions:\n• Add container to one or more networks\n• Delete container from network\n• Report CNI version\nIn order for CNI to add a container to a network, the container runtime must\nfirst create a new network namespace for the container and then invoke one or\nmore of the defined plug-ins. The network configuration is in JSON format and\nincludes mandatory fields such as name and type as well as plug-in type–specific\nfields. The actual command (for example, ADD) is passed in as an environment\nvariable aptly named CNI_COMMAND.\nIP Allocation with CNI\nA CNI plug-in is expected to assign an IP address to the interface and set up net‐\nwork routes relevant for it. This gives the CNI plug-in great flexibility but also\nplaces a large burden on it. To accommodate this, CNI defines a dedicated IP\nAddress Management (IPAM) plug-in that takes care of the IP range manage‐\nment independently.\nLet’s have a look at a concrete CNI command in action:\n$ CNI_COMMAND=ADD \\\n  CNI_CONTAINERID=875410a4c38d7 \\\n  CNI_NETNS=/proc/1234/ns/net \\\n  CNI_IFNAME=eth0 \\\n  CNI_PATH=/opt/cni/bin \\\n  someplugin < /etc/cni/net.d/someplugin.conf\nThis example shows how a certain plug-in (someplugin) is applied to a given\ncontainer (875410a4c38d7) using a specific configuration (someplugin.conf). Note\nthat while initially all configuration parameters were passed in as environment\nSpecification and Usage \n| \n39\n",
      "content_length": 1584,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "variables, the people behind the spec are moving more and more toward using\nthe (JSON) configuration file.\nYou can learn more about using CNI in the excellent blog post “Understanding\nCNI” by Jon Langemak.\nContainer Runtimes and Plug-ins\nIn addition to pretty much any container orchestrator and container runtime\n(Kubernetes, Mesos, Cloud Foundry) supporting CNI, it ships with a number of\nbuilt-in plug-ins, such as loopback and vlan. There’s also a long list of third-\nparty CNI plug-ins available. Here is a selection of the most important ones, in\nalphabetical order:\nAmazon ECS CNI Plugins\nA collection of CNI plug-ins used by the Amazon ECS Agent to configure\nthe network namespace of containers with Elastic Network Interfaces (ENIs).\nBonding\nA CNI plug-in for failover and high availability of networking in cloud-\nnative environments, by Intel.\nCalico\nProject Calico’s network plug-in for CNI. Project Calico manages a flat layer\n3 network, assigning each workload a fully routable IP address. For environ‐\nments requiring an overlay network Calico uses IP-in-IP tunneling or can\nwork with other overlay networks, such as flannel.\nCilium\nA BPF-based solution providing connectivity between containers, operating\nat layer 3/4 to provide networking and security services as well as layer 7 to\nprotect modern protocols such as HTTP and gRPC.\nCNI-Genie\nA generic CNI network plug-in by Huawei.\nInfoblox\nAn IPAM driver for CNI that interfaces with Infoblox to provide IP Address\nManagement services.\nLinen\nA CNI plug-in designed for overlay networks with Open vSwitch.\nMultus\nA powerful multi–plug-in environment by Intel.\nNuage CNI\nA Nuage Networks SDN plug-in supporting network policies for Kubernetes.\n40 \n| \nChapter 6: The Container Network Interface\n",
      "content_length": 1761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "Romana\nA layer 3 CNI plug-in supporting network policy for Kubernetes.\nSilk\nA network fabric for containers, inspired by flannel, designed for Cloud\nFoundry.\nVhostuser\nA plug-in to run with Open vSwitch and OpenStack VPP along with the\nMultus CNI plug-in in Kubernetes for bare-metal container deployment\nmodels.\nWeave Net\nA multi-host Docker network by Weaveworks.\nWrapping It Up\nWith this we conclude the CNI chapter and move on to Kubernetes and its net‐\nworking approach. CNI plays a central role in Kubernetes (networking), and you\nmight want to check the docs there as well.\nWrapping It Up \n| \n41\n",
      "content_length": 603,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "CHAPTER 7\nKubernetes Networking\nThis chapter will first quickly bring you up to speed concerning Kubernetes, then\nintroduce you to the networking concepts on a high level. Then we’ll jump into\nthe deep end, looking at how container networking is realized in Kubernetes,\nwhat traffic types exist and how you can make services talk to each other within\nthe cluster, as well as how you can get traffic into your cluster and to a specific\nservice.\nA Gentle Kubernetes Introduction\nKubernetes is an open source container orchestration system. It captures Google’s\nlessons learned from running containerized workloads on Borg for more than a\ndecade. As of early 2018 Kubernetes is considered the de facto industry standard\nfor container orchestration, akin to the Linux kernel for the case of a single\nmachine.\nI’d argue that there are at least two significant points in time concerning\nthe birth of Kubernetes. The first was on June 7, 2014, with Joe Beda’s\ninitial commit on GitHub that marked the beginning of the open sourc‐\ning of the project. The second was almost a year later, on July 20, 2015,\nwhen Google launched Kubernetes 1.0 and announced the formation of\na dedicated entity to host and govern Kubernetes, the Cloud Native\nComputing Foundation (CNCF). As someone who was at the launch\nevent (and party), I can tell you, that’s certainly one way to celebrate the\nbirth of a project.\nKubernetes’s architecture (Figure 7-1) provides support for a number of work‐\nloads, allowing you to run stateless as well as stateful containerized applications.\nYou can launch long-running processes, such as low-latency app servers, as well\nas batch jobs.\n43\n",
      "content_length": 1651,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "Figure 7-1. An overview of the Kubernetes architecture\nThe unit of scheduling in Kubernetes is a pod. Essentially, this is a tightly coupled\nset of one or more containers that are always collocated (that is, scheduled onto a\nnode as a unit); they cannot be spread over nodes. The number of running\ninstances of a pod—called replicas—can be declaratively stated and enforced\nthrough controllers. The logical organization of all resources, such as pods,\ndeployments, or services, happens through labels.\nWith Kubernetes you almost always have the option to swap out the default\nimplementations with some open source or closed source alternative, be it DNS\nor monitoring. Kubernetes is highly extensible, from defining new workloads and\nresource types in general to customizing its user-facing parts, such as the CLI\ntool kubectl (pronounced cube cuddle).\nThis chapter assumes you’re somewhat familiar with Kubernetes and its termi‐\nnology. Should you need to brush up your knowledge of how Kubernetes works,\nI suggest checking out the Concepts section in the official docs or the book\nKubernetes Up and Running (O’Reilly) by Brendan Burns, Kelsey Hightower, and\nJoe Beda.\n44 \n| \nChapter 7: Kubernetes Networking\n",
      "content_length": 1210,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "Kubernetes Networking Overview\nRather than prescribing a certain networking solution, Kubernetes only states\nthree fundamental requirements:\n• Containers can communicate with all other containers without NAT.\n• Nodes can communicate with all containers (and vice versa) without NAT.\n• The IP a container sees itself is the same IP as others see it.\nHow you meet these requirements is up to you. This means you have a lot of free‐\ndom to realize networking with and for Kubernetes. It also means, however, that\nKubernetes on its own will only provide so much; for example, it supports CNI\n(Chapter 6) but it doesn’t come with a default SDN solution. In the networking\narea, Kubernetes is at the same time strangely opinionated (see the preceding\nrequirements) and not at all (no batteries included).\nFrom a network traffic perspective we differentiate between three types in Kuber‐\nnetes, as depicted in Figure 7-2:\nIntra-pod networking\nAll containers within a pod share a network namespace and see each other\non localhost. Read “Intra-Pod Networking” on page 46 for details.\nInter-pod networking\nTwo types of east–west traffic are supported: pods can directly communicate\nwith other pods or, preferably, pods can leverage services to communicate\nwith other pods. Read “Inter-Pod Networking” on page 47 for details.\nIngress and egress\nIngress refers to routing traffic from external users or apps to pods, and\negress refers to calling external APIs from pods. Read “Ingress and Egress”\non page 53 for details.\nKubernetes Networking Overview \n| \n45\n",
      "content_length": 1547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "1 See pause.go for details; basically it blocks until it receives a SIGTERM.\nFigure 7-2. Kubernetes network traffic types\nKubernetes requires each pod to have an IP in a flat networking name‐\nspace with full connectivity to other nodes and pods across the network.\nThis IP-per-pod model yields a backward-compatible way for you to\ntreat a pod almost identically to a VM or a physical host, in the context\nof naming, service discovery, or port allocations. The model allows for a\nsmoother transition from non–cloud native apps and environments.\nIntra-Pod Networking\nWithin a pod there exists a so-called infrastructure container. This is the first con‐\ntainer that the kubelet launches, and it acquires the pod’s IP and sets up the net‐\nwork namespace. All the other containers in the pod then join the infra\ncontainer’s network and IPC namespace. The infra container has network bridge\nmode enabled (see “Bridge Mode Networking” on page 7) and all the other con‐\ntainers in the pod join this namespace via container mode (covered in “Container\nMode Networking” on page 9). The initial process that runs in the infra container\ndoes effectively nothing,1 as its sole purpose is to act as the home for the name‐\n46 \n| \nChapter 7: Kubernetes Networking\n",
      "content_length": 1249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "spaces. If the infra container dies, the kubelet kills all the containers in the pod\nand then starts the process over.\nAs a result of above, all containers within a pod can communicate amongst each\nother using localhost (or 127.0.0.1 in IPv4). You are responsible yourself to\nmake sure containers within a pod do not conflict with each other in terms of\nports used. Note also that the Kubernetes approach here also means reduced iso‐\nlation between containers within a pod; however, this is by design and since we\nconsider the tight coupling here a good thing, it is probably not something you\nneed to worry about, though it’s good to be aware of it.\nIf you want to learn more about the infra container, read The Almighty Pause\nContainer by Ian Lewis.\nInter-Pod Networking\nIn Kubernetes, each pod has a routable IP, allowing pods to communicate across\ncluster nodes without NAT and no need to manage port allocations. Because\nevery pod gets a real (that is, not machine-local) IP address, pods can communi‐\ncate without proxies or translations (such as NAT). The pod can use well-known\nports and can avoid the use of higher-level service discovery mechanisms such as\nthose we discussed in Chapter 5.\nWe distinguish between two types of inter-pod communication, sometimes also\ncalled East-West traffic:\n• Pods can directly communicate with other pods; in this case the caller pod\nneeds to find out the IP address of the callee and risks repeating this opera‐\ntion since pods come and go (cattle behaviour).\n• Preferably, pods use services to communicate with other pods. In this case,\nthe service provides a stable (virtual) IP address that can be discovered, for\nexample, via DNS.\nDifference to Docker Model\nNote that the Kubernetes flat address space model is different from the default\nDocker model. There, each container gets an IP address in the 172.x.x.x space\nand only sees this 172. address. If this container connects to another container\nthe peer would see the connection coming from a different IP than the container\nitself sees. This means you can never self-register anything from a container\nbecause a container cannot be reached on its private IP.\nInter-Pod Networking \n| \n47\n",
      "content_length": 2190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "When a container tries to obtain the address of network interface it sees the same\nIP that any peer container would see them coming from; each pod has its own IP\naddress that other pods can find and use. By making IP addresses and ports the\nsame both inside and outside the pods, Kubernetes creates a flat address space\nacross the cluster. For more details on this topic see also the article “Understand‐\ning Kubernetes Networking: Pods” by Mark Betz.\nLet’s now focus on the service, as depicted in Figure 7-3.\nFigure 7-3. The Kubernetes service concept\nA service provides a stable virtual IP (VIP) address for a set of pods. While pods\nmay come and go, services allow clients to reliably discover and connect to the\ncontainers running in the pods by using the VIP. The “virtual” in VIP means it’s\nnot an actual IP address connected to a network interface; its purpose is purely to\nact as the stable front to forward traffic to one or more pods, with IP addresses\nthat may come and go.\n48 \n| \nChapter 7: Kubernetes Networking\n",
      "content_length": 1026,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "What VIPs Really Are\nIt’s essential to realize that VIPs do not exist as such in the networking\nstack. For example, you can’t ping them. They are only Kubernetes-\ninternal administrative entities. Also note that the format is IP:PORT, so\nthe IP address along with the port make up the VIP. Just think of a VIP\nas a kind of index into a data structure mapping to actual IP addresses.\nAs you can see in Figure 7-3, the service with the VIP 10.104.58.143 routes the\ntraffic to one of the pods 172.17.0.3 or 172.17.0.4. Note here the different sub‐\nnets for the service and pods, see Network Ranges for further details on the rea‐\nson behind that. Now, you might be wondering how this actually works? Let’s\nhave a look at it.\nYou specify the set of pods you want a service to target via a label selector, for\nexample, for spec.selector.app=someapp Kubernetes would create a service\nthat targets all pods with a label app=someapp. Note that if such a selector exists,\nthen for each of the targeted pods a sub-resource of type Endpoint will be cre‐\nated, and if no selector exists then no endpoints are created. For example, see in\nthe following code example the output of the kubectl describe command. Such\nendpoints are also not created in the case of so-called headless services, which\nallow you to exercise great control over how the IP management and service dis‐\ncovery takes place.\nKeeping the mapping between the VIP and the pods up-to-date is the job of\nkube-proxy (see also the docs on kube-proxy), a process that runs on every node\non the cluster.\nThis kube-proxy process queries the API server to learn about new services in\nthe cluster and updates the node’s iptables rules accordingly, to provide the nec‐\nessary routing information. To learn more how exactly services work, check out\nKubernetes Services By Example.\nLet’s see how this works in practice: assuming there’s an existing deployment\ncalled nginx (for example, execute kubectl run webserver --image nginx)\nyou can automatically create a service like so:\n$ kubectl expose deployment/webserver --port 80\nservice \"webserver\" exposed\n$ kubectl describe service/webserver\nName:              webserver\nNamespace:         default\nLabels:            run=webserver\nAnnotations:       <none>\nSelector:          run=webserver\nType:              ClusterIP\nIP:                10.104.58.143\nInter-Pod Networking \n| \n49\n",
      "content_length": 2374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "Port:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         172.17.0.3:8080,172.17.0.4:8080\nSession Affinity:  None\nEvents:            <none>\nAfter executing the above kubectl expose command, you will see the service\nappear:\n$ kubectl get service -l run=webserver\nNAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nwebserver   ClusterIP   10.104.58.143   <none>        80/TCP    1m\nAbove, note two things: the service has got itself a cluster-internal IP (CLUSTER-\nIP column) and the EXTERNAL-IP column tells you that this service is only avail‐\nable from within the cluster, that is, no traffic from outside of the cluster can\nreach this service (yet)—see “Ingress and Egress” on page 53 to learn how to\nchange this situation.\nFigure 7-4. Kubernetes service in the dashboard\nIn Figure 7-4 you can see the representation of the service in the Kubernetes\ndashboard.\nService Discovery in Kubernetes\nLet us now talk about how service discovery works in Kubernetes.\nConceptually, you can use one of the two built-in discovery mechanisms:\n• Through environment variables (limited)\n50 \n| \nChapter 7: Kubernetes Networking\n",
      "content_length": 1153,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "• Using DNS, which is available cluster-wide if a respective DNS cluster add-\non has been installed\nEnvironment Variables–Based Service Discovery\nFor the environment variables–based discovery method, a simple example might\nlook like the example code to follow: using a jump pod to get us into the cluster\nand then running from there the env built-in shell command (note that the out‐\nput has been edited to be easier to digest).\n$ kubectl run -it --rm jump --restart=Never \\\n              --image=quay.io/mhausenblas/jump:v0.1 -- sh\nIf you don't see a command prompt, try pressing enter.\n/ # env\nHOSTNAME=jump\nWEBSERVER_SERVICE_HOST=10.104.58.143\nWEBSERVER_PORT=tcp://10.104.58.143:80\nWEBSERVER_SERVICE_PORT=80\nWEBSERVER_PORT_80_TCP_ADDR=10.104.58.143\nWEBSERVER_PORT_80_TCP_PORT=80\nWEBSERVER_PORT_80_TCP_PROTO=tcp\nWEBSERVER_PORT_80_TCP=tcp://10.104.58.143:80\n...\nAbove, you can see the service discovery in action: the environment variables WEB\nSERVER_XXX give you the IP address and port you can use to connect to the ser‐\nvice. For example, while still in the jump pod, you could execute curl\n10.104.58.143 and you should see the NGINX welcome page.\nWhile convenient, note that discovery via environment variables has a funda‐\nmental drawback: any service that you want to discover must be created before\nthe pod from which you want to discover it as otherwise the environment vari‐\nables will not be populated by Kubernetes. Luckily there exists a better way: DNS.\nDNS-Based Service Discovery\nMapping a fully qualified domain name (FQDN) like example.com to an IP\naddress such as 123.4.5.66 is what DNS was designed for and has been doing\nfor us on a daily basis on the internet for more than 30 years.\nChoosing a DNS Solution\nWhen rolling your own Kubernetes distro, that is, putting together all\nthe required components such as SDN or the DNS add-on yourself\nrather than using an offering from the more than 30 certified Kuber‐\nnetes offerings, it’s worth considering the CNCF project CoreDNS over\nthe older and less feature-rich kube-dns DNS cluster add-on (which is\npart of Kubernetes proper).\nService Discovery in Kubernetes \n| \n51\n",
      "content_length": 2140,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "So how can we use DNS to do service discovery in Kubernetes? It’s easy, if you\nhave the DNS cluster add-on installed and enabled. This DNS server watches on\nthe Kubernetes API for services being created or removed. It creates a set of DNS\nrecords for each service it observes.\nIn the next example, let’s use our webserver service from above and assume we\nhave it running in the default namespace. For this service, a DNS record web\nserver.default (with a FQDN of webserver.default.cluster.local) should\nbe present.\n$ kubectl run -it --rm jump --restart=Never \\\n              --image=quay.io/mhausenblas/jump:v0.1 -- sh\nIf you don't see a command prompt, try pressing enter.\n/ # curl webserver.default\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n<style>\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n</style>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n<p>If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.</p>\n<p>For online documentation and support please refer to\n<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\nCommercial support is available at\n<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\nPods in the same namespace can reach the service by its shortname webserver,\nwhereas pods in other namespaces must qualify the name as webserver.default.\nNote that the result of these FQDN lookups is the pod’s cluster IP. Further,\nKubernetes supports DNS service (SRV) records for named ports. So if our web\nserver service had a port named, say, http with the protocol type TCP, you\ncould issue a DNS SRV query for _http._tcp.webserver from the same name‐\nspace to discover the port number for http. Note also that the virtual IP for a\nservice is stable, so the DNS result does not have to be requeried.\n52 \n| \nChapter 7: Kubernetes Networking\n",
      "content_length": 1955,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "Network Ranges\nFrom an administrative perspective, you are conceptually dealing with\nthree networks: the pod network, the service network, and the host net‐\nwork (the machines hosting Kubernetes components such as the kube\nlet). You will need a strategy regarding how to partition the network\nranges; one often found strategy is to use networks from the private\nrange as defined in RFC 1918, that is, 10.0.0.0/8, 172.16.0.0/12, and\n192.168.0.0/16.\nIngress and Egress\nIn the following we’ll have a look at how traffic flows in and out of a Kubernetes\ncluster, also called North-South traffic.\nIngress\nUp to now we have discussed how to access a pod or service from within the\ncluster. Accessing a pod from outside the cluster is a bit more challenging. Kuber‐\nnetes aims to provide highly available, high-performance load balancing for serv‐\nices.\nInitially, the only available options for North-South traffic in Kubernetes were\nNodePort, LoadBalancer, and ExternalName, which are still available to you. For\nlayer 7 traffic (i.e., HTTP) a more portable option is available, however: intro‐\nduced in Kubernetes 1.2 as a beta feature, you can use Ingress to route traffic\nfrom the external world to a service in our cluster.\nIngress in Kubernetes works as shown in Figure 7-5: conceptually, it is split up\ninto two main pieces, an Ingress resource, which defines the routing to the back‐\ning services, and the Ingress controller, which listens to the /ingresses endpoint\nof the API server, learning about services being created or removed. On service\nstatus changes, the Ingress controller configures the routes so that external traffic\nlands at a specific (cluster-internal) service.\nIngress and Egress \n| \n53\n",
      "content_length": 1709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "Figure 7-5. Ingress concept\nThe following example presents a concrete example of an Ingress resource, to\nroute requests for myservice.example.com/somepath to a Kubernetes service\nnamed service1 on port 9876.\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: example-ingress\nspec:\n  rules:\n  - host: myservice.example.com\n    http:\n      paths:\n      - path: /somepath\n        backend:\n          serviceName: service1\n          servicePort: 9876\nNow, the Ingress resource definition is nice, but without a controller, nothing\nhappens. So let’s deploy an ingress controller, in this case using Minikube.\n$ minikube addons enable ingress\nOnce you’ve enabled Ingress on Minikube, you should see it appear as enabled in\nthe list of Minikube add-ons. After a minute or so, two new pods will start in the\nkube-system namespace, the backend and the controller. So now you can use it,\nusing the manifest in the following example, which configures a path to an\nNGINX webserver.\n$ cat nginx-ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata:\n  name: nginx-public\n54 \n| \nChapter 7: Kubernetes Networking\n",
      "content_length": 1122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "  annotations:\n    ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host:\n    http:\n      paths:\n      - path: /web\n        backend:\n          serviceName: nginx\n          servicePort: 80\n$ kubectl create -f nginx-ingress.yaml\nNow NGINX is available via the IP address 192.168.99.100 (in this case my\nMinikube IP) and the manifest file defines that it should be exposed via the\npath /web.\nNote that Ingress controllers can technically be any system capable of reverse\nproxying, but NGINX is most commonly used. Further, Ingress can also be\nimplemented by a cloud-provided load balancer, such as Amazon’s ALB.\nFor more details on Ingress, read the excellent article “Understanding Kubernetes\nNetworking: Ingress” by Mark Betz and make sure to check out the results of the\nsurvey the Kubernetes SIG Network carried out on this topic.\nEgress\nWhile in the case of Ingress we’re interested in routing traffic from outside the\ncluster to a service, in the case of Egress we are dealing with the opposite: how\ndoes an app in a pod call out to (cluster-)external APIs?\nOne may want to control which pods are allowed to have a communication path\nto outside services and on top of that impose other policies. Note that by default\nall containers in a pod can perform Egress. These policies can be enforced using\nnetwork policies as described in “Network Policies” on page 55 or by deploying a\nservice mesh as in “Service Meshes” on page 56.\nAdvanced Kubernetes Networking Topics\nIn the following I’ll cover two advanced and somewhat related Kubernetes net‐\nworking topics: network policies and service meshes.\nNetwork Policies\nNetwork policies in Kubernetes are a feature that allow you to specify how\ngroups of pods are allowed to communicate with each other. From Kubernetes\nAdvanced Kubernetes Networking Topics \n| \n55\n",
      "content_length": 1821,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "version 1.7 and above network policies are considered stable and hence you can\nuse them in production.\nLet’s take a look at a concrete example of how this works in practice. For example,\nsay you want to suppress all traffic to pods in the namespace superprivate.\nYou’d create a default Egress policy for that namespace as in the following exam‐\nple:\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: bydefaultnoegress\n  namespace: superprivate\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\nNote that different Kubernetes distros support network policies to different\ndegrees: for example, in OpenShift they are supported as first-class citizens and a\nrange of examples is available via the redhat-cop/openshift-toolkit GitHub repo.\nIf you want to learn more about how to use network policies, check out Ahmet\nAlp Balkan’s brilliant and detailed hands-on blog post, “Securing Kubernetes\nCluster Networking”.\nService Meshes\nGoing forward, you can make use of service meshes such as the two discussed in\nthe following. The idea of a service mesh is that rather than putting the burden of\nnetworking communication and control onto you, the developer, you outsource\nthese nonfunctional things to the mesh. So you benefit from traffic control,\nobservability, security, etc. without any changes to your source code. Sound fan‐\ntastic? It is, believe you me.\nIstio\nIstio is a modern and popular service mesh, available for Kubernetes but not\nexclusively so. It’s using Envoy as the default data plane and mainly focusing\non the control-plane aspects. It supports monitoring (Prometheus), tracing\n(Zipkin/Jaeger), circuit breakers, routing, load balancing, fault injection,\nretries, timeouts, mirroring, access control, and rate limiting out of the box,\nto name a few features. Istio takes the battle-tested Envoy proxy (cf. “Load\nBalancing” on page 32) and packages it up as a sidecar container in your pod.\nLearn more about Istio via Christian Posta’s wonderful resource: Deep Dive\nEnvoy and Istio Workshop. \n56 \n| \nChapter 7: Kubernetes Networking\n",
      "content_length": 2067,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "Buoyant’s Conduit\nThis service mesh is deployed on a Kubernetes cluster as a data plane (writ‐\nten in Rust) made up of proxies deployed as sidecar containers alongside\nyour app and a control plane (written in Go) of processes that manages these\nproxies, akin to what you’ve seen in Istio above. After the CNCF project\nLinkerd this is Buoyant’s second iteration on the service mesh idea; they are\nthe pioneers in this space, establishing the service mesh idea in 2016. Learn\nmore via Abhishek Tiwari’s excellent blog post, “Getting started with Con‐\nduit - lightweight service mesh for Kubernetes”. \nOne note before we wrap up this chapter and also the book: service meshes are\nstill pretty new, so you might want to think twice before deploying them in prod\n—unless you’re Lyft or Google or the like ;)\nWrapping It Up\nIn this chapter we’ve covered the Kubernetes approach to container networking\nand showed how to use it in various setups. With this we conclude the book;\nthanks for reading and if you have feedback, please do reach out via Twitter.\nWrapping It Up \n| \n57\n",
      "content_length": 1072,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "APPENDIX A\nReferences\nReading stuff is fine, and here I’ve put together a collection of links that contain\neither background information on topics covered in this book or advanced mate‐\nrial, such as deep dives or teardowns. However, for a more practical approach I\nsuggest you check out Katacoda, a free online learning environment that contains\n100+ scenarios from Docker to Kubernetes (see for example the screenshot in\nFigure A-1).\nFigure A-1. Katacoda Kubernetes scenarios\nYou can use Katacoda in any browser; sessions are typically terminated after one\nhour.\n59\n",
      "content_length": 568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "Container Networking References\nNetworking 101\n• “Network Protocols” from the Programmer’s Compendium\n• “Demystifying Container Networking” by Michele Bertasi\n• “An Empirical Study of Load Balancing Algorithms” by Khalid Lafi\nLinux Kernel and Low-Level Components\n• “The History of Containers” by thildred\n• “A History of Low-Level Linux Container Runtimes” by Daniel J. Walsh\n• “Networking in Containers and Container Clusters” by Victor Marmol,\nRohit Jnagal, and Tim Hockin\n• “Anatomy of a Container: Namespaces, cgroups & Some Filesystem Magic”\nby Jérôme Petazzoni\n• “Network Namespaces” by corbet\n• Network classifier cgroup documentation\n• “Exploring LXC Networking” by Milos Gajdos\nDocker\n• Docker networking overview\n• “Concerning Containers’ Connections: On Docker Networking” by Federico\nKereki\n• “Unifying Docker Container and VM Networking” by Filip Verloy\n• “The Tale of Two Container Networking Standards: CNM v. CNI” by Har‐\nmeet Sahni\nKubernetes Networking References\nKubernetes Proper and Docs\n• Kubernetes networking design\n• Services\n• Ingress\n60 \n| \nAppendix A: References\n",
      "content_length": 1092,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "• Cluster Networking\n• Provide Load-Balanced Access to an Application in a Cluster\n• Create an External Load Balancer\n• Kubernetes DNS example\n• Kubernetes issue 44063: Implement IPVS-based in-cluster service load bal‐\nancing\n• “Data and analysis of the Kubernetes Ingress survey 2018” by the Kubernetes\nSIG Network\nGeneral Kubernetes Networking\n• “Kubernetes Networking 101” by Bryan Boreham of Weaveworks\n• “An Illustrated Guide to Kubernetes Networking” by Tim Hockin of Google\n• “The Easy—Don’t Drive Yourself Crazy—Way to Kubernetes Networking” by\nGerard Hickey (KubeCon 2017, Austin)\n• “Understanding Kubernetes Networking: Pods”, “Understanding Kubernetes\nNetworking: Services”, and “Understanding Kubernetes Networking:\nIngress” by Mark Betz\n• “Understanding CNI (Container Networking Interface)” by Jon Langemak\n• “Operating a Kubernetes Network” by Julia Evans\n• “nginxinc/kubernetes-ingress” Git repo\n• “The Service Mesh: Past, Present, and Future” by William Morgan (KubeCon\n2017, Austin)\n• “Meet Bandaid, the Dropbox Service Proxy” by Dmitry Kopytkov and Pat‐\nrick Lee\n• “Kubernetes NodePort vs LoadBalancer vs Ingress? When Should I Use\nWhat?” by Sandeep Dinesh\nReferences \n| \n61\n",
      "content_length": 1194,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "About the Author\nMichael Hausenblas is a developer advocate for Go, Kubernetes, and OpenShift\nat Red Hat, where he helps appops to build and operate distributed services. His\nbackground is in large-scale data processing and container orchestration and he’s\nexperienced in advocacy and standardization at the W3C and IETF. Before Red\nHat, Michael worked at Mesosphere and MapR and in two research institutions\nin Ireland and Austria. He contributes to open source software (mainly using\nGo), speaks at conferences and user groups, blogs, and hangs out on Twitter too\nmuch.\n",
      "content_length": 572,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}