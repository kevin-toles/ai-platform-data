{
  "metadata": {
    "title": "Python Architecture Patterns",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 595,
    "conversion_date": "2025-11-11T10:45:05.973586",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Python Architecture Patterns.pdf"
  },
  "chapters": [],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Copyright  2022. Packt Publishing. All rights reserved. May not be reproduced in any form without permission from the publisher, except fair uses permitted under U.S. or applicable copyright law.\nEBSCO Publishing : eBook Collection (EBSCOhost) - printed on 2/9/2023 8:47 AM via \nAN: 3134268 ; Jaime Buelta.; Python Architecture Patterns : Master API Design, Event-driven Structures, and Package Management in Python\nAccount: ns335141\n",
      "content_length": 434,
      "extraction_method": "Direct"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Python Architecture Patterns\nMaster API design, event-driven structures, and  \npackage management in Python\nJaime Buelta\nBIRMINGHAM—MUMBAI\n\"Python\" and the Python Logo are trademarks of the Python Software Foundation.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 319,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Python Architecture Patterns\nCopyright © 2022 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, \nor transmitted in any form or by any means, without the prior written permission of the \npublisher, except in the case of brief quotations embedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy of the \ninformation presented. However, the information contained in this book is sold without \nwarranty, either express or implied. Neither the author, nor Packt Publishing or its \ndealers and distributors, will be held liable for any damages caused or alleged to have \nbeen caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the \ncompanies and products mentioned in this book by the appropriate use of capitals. \nHowever, Packt Publishing cannot guarantee the accuracy of this information.\nProducer: Tushar Gupta\nAcquisition Editor – Peer Reviews: Saby D'silva\nProject Editor: Parvathy Nair\nContent Development Editor: Alex Patterson\nCopy Editor: Safis Editor\nTechnical Editor: Tejas Mhasvekar\nProofreader: Safis Editor\nIndexer: Pratik Shirodkar\nPresentation Designer: Pranit Padwal\nFirst published: January 2022\nProduction reference: 2020222\nPublished by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham\nB3 2PB, UK.\nISBN 978-1-80181-999-2\nwww.packt.com\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "Contributors\nAbout the author\nJaime Buelta has been a professional programmer for 20 years and a full-time \nPython developer for over 10. During that time, he has been exposed to a lot of \ndifferent technologies while working for different industries and helping them \nachieve their goals; these industries include aerospace, industrial systems, video \ngame online services, finance services and educational tools. He has been writing \ntechnical books since 2018, reflecting on lessons learned over his career, including \nPython Automation Cookbook and Hands On Docker for Microservices in Python. He is \ncurrently living in Dublin, Ireland.\nWriting a book is always more than a single person's work. There're not \nonly the people involved directly in polishing and improving the drafts, but \nalso a lot of conversations and talks with exceptional people in the Python \nand tech community that shape the ideas in it. It also wouldn't be possible \nwithout the love and support from Dana, my amazing wife.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "About the reviewer\nPradeep Pant is a computer programmer, software architect, AI researcher and \nopen source advocate. Pradeep has been writing computer programs for more than 2 \ndecades in various programming languages and platforms, such as microprocessor/\nAssembly, C, C++, Perl, Python, R, JavaScript, AI/ML, Linux, the cloud and many \nmore. Pradeep holds a master's degree in physics and another master's in computer \nscience. In his free time, Pradeep likes to write about his tech journey and learnings \nat https://pradeeppant.com. \nPradeep works with Ockham BV, a Belgium-based software development company. \nThe company develops software in the quality and document management systems \nspace.\nPradeep can be contacted through email or through professional networks:\n•\t\nEmail: pp@pradeeppant.com\n•\t\nLinkedIn: https://www.linkedin.com/in/ppant/\n•\t\nGitHub: https://github.com/ppant\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "[ v ]\nTable of Contents\nPreface\b\nxiii\nChapter 1: Introduction to Software Architecture\b\n1\nDefining the structure of a system\b\n2\nDivision into smaller units\b\n4\nIn-process communication\b\n6\nConway's Law – Effects on software architecture\b\n7\nApplication example – Overview \b\n9\nSecurity aspects of software architecture\b\n11\nSummary\b\n13\nPart I: Design\b\n15\nChapter 2: API Design\b\n17\nAbstractions\b\n18\nUsing the right abstractions\b\n21\nLeaking abstractions\b\n22\nResources and action abstractions\b\n23\nRESTful interfaces\b\n25\nA more practical definition\b\n26\nHeaders and statuses\b\n29\nDesigning resources\b\n32\nResources and parameters\b\n34\nPagination\b\n35\nDesigning a RESTful API process\b\n37\nUsing the Open API specification\b\n40\nAuthentication\b\n44\nAuthenticating HTML interfaces\b\n44\nAuthenticating RESTful interfaces\b\n46\nSelf-encoded tokens\b\n49\nVersioning the API\b\n51\nWhy versioning?\b\n51\nInternal versus external versioning\b\n51\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "Table of Contents\n[ vi ]\nSemantic versioning\b\n52\nSimple versioning\b\n54\nFrontend and backend\b\n55\nModel View Controller structure\b\n57\nHTML interfaces\b\n58\nTraditional HTML interfaces\b\n58\nDynamic pages\b\n59\nSingle-page apps\b\n61\nHybrid approach\b\n63\nDesigning the API for the example\b\n63\nEndpoints\b\n65\nReview of the design and implementation\b\n73\nSummary\b\n73\nChapter 3: Data Modeling\b\n75\nTypes of databases\b\n76\nRelational databases\b\n77\nNon-relational databases\b\n79\nKey-value stores\b\n80\nDocument stores\b\n81\nWide-column databases\b\n82\nGraph databases\b\n83\nSmall databases\b\n83\nDatabase transactions\b\n85\nDistributed relational databases\b\n87\nPrimary/replica\b\n88\nSharding\b\n90\nPure sharding\b\n92\nMixed sharding\b\n93\nTable sharding\b\n95\nAdvantages and disadvantages of sharding\b\n96\nSchema design\b\n97\nSchema normalization \b\n101\nDenormalization\b\n103\nData indexing\b\n105\nCardinality\b\n107\nSummary\b\n110\nChapter 4: The Data Layer\b\n111\nThe Model layer\b\n112\nDomain-Driven Design\b\n112\nUsing ORM\b\n114\nIndependence from the database\b\n116\nIndependence from SQL and the Repository pattern\b\n116\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "Table of Contents\n[ vii ]\nNo problems related to composing SQL\b\n118\nThe Unit of Work pattern and encapsulating the data\b\n122\nCQRS, using different models for read and write\b\n126\nDatabase migrations\b\n130\nBackward compatibility\b\n130\nRelational schema changes\b\n131\nChanging the database without interruption\b\n132\nData migrations\b\n136\nChanges without enforcing a schema\b\n137\nDealing with legacy databases\b\n139\nDetecting a schema from a database\b\n139\nSyncing the existing schema to the ORM definition\b\n141\nSummary\b\n143\nPart II: Architectural Patterns\b\n145\nChapter 5: The Twelve-Factor App Methodology\b\n147\nIntro to the Twelve-Factor App\b\n148\nContinuous Integration\b\n149\nScalability\b\n151\nConfiguration\b\n153\nThe Twelve Factors\b\n156\nBuild once, run multiple times\b\n157\nDependencies and configurations\b\n159\nScalability\b\n163\nMonitoring and admin\b\n166\nContainerized Twelve-Factor Apps\b\n169\nSummary\b\n171\nChapter 6: Web Server Structures\b\n173\nRequest-response\b\n174\nWeb architecture\b\n177\nWeb servers\b\n177\nServing static content externally\b\n180\nReverse proxy\b\n182\nLogging\b\n185\nAdvanced usages\b\n186\nuWSGI\b\n186\nThe WSGI application\b\n187\nInteracting with the web server\b\n189\nProcesses\b\n190\nProcess lifecycle\b\n191\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1296,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "Table of Contents\n[ viii ]\nPython worker\b\n195\nDjango MVT architecture\b\n195\nRouting a request towards a View\b\n197\nThe View\b\n199\nHttpRequest\b\n201\nHttpResponse\b\n203\nMiddleware\b\n205\nDjango REST framework\b\n208\nModels\b\n209\nURL routing\b\n210\nViews\b\n210\nSerializer\b\n212\nExternal layers\b\n216\nSummary\b\n217\nChapter 7: Event-Driven Structures\b\n219\nSending events\b\n220\nAsynchronous tasks\b\n221\nSubdividing tasks\b\n226\nScheduled tasks\b\n227\nQueue effects\b\n227\nSingle code for all workers\b\n232\nCloud queues and workers\b\n233\nCelery\b\n235\nConfiguring Celery\b\n236\nCelery worker\b\n237\nTriggering tasks\b\n240\nConnecting the dots\b\n241\nScheduled tasks\b\n244\nCelery Flower\b\n249\nFlower HTTP API\b\n252\nSummary\b\n254\nChapter 8: Advanced Event-Driven Structures\b\n255\nStreaming events\b\n256\nPipelines\b\n260\nPreparation\b\n261\nBase task\b\n263\nImage task\b\n264\nVideo task\b\n266\nConnecting the tasks\b\n267\nRunning the task\b\n270\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 980,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "Table of Contents\n[ ix ]\nDefining a bus\b\n272\nMore complex systems\b\n274\nTesting event-driven systems\b\n278\nSummary\b\n280\nChapter 9: Microservices vs Monolith\b\n281\nMonolithic architecture\b\n282\nThe microservices architecture\b\n283\nWhich architecture to choose\b\n285\nA side note about similar designs\b\n289\nThe key factor – team communication\b\n290\nMoving from a monolith to microservices\b\n294\nChallenges for the migration\b\n294\nA move in four acts\b\n296\n1. Analyze\b\n297\n2. Design\b\n298\n3. Plan\b\n301\n4. Execute\b\n303\nContainerizing services\b\n306\nBuilding and running an image\b\n308\nBuilding and running a web service\b\n311\nuWSGI configuration\b\n314\nnginx configuration\b\n314\nStart script\b\n315\nBuilding and running\b\n316\nCaveats\b\n319\nOrchestration and Kubernetes\b\n320\nSummary\b\n322\nPart III: Implementation\b\n325\nChapter 10: Testing and TDD\b\n327\nTesting the code\b\n328\nDifferent levels of testing\b\n331\nUnit tests\b\n331\nIntegration tests\b\n332\nSystem tests\b\n333\nTesting philosophy\b\n334\nHow to design a great test\b\n336\nStructuring tests\b\n339\nTest-Driven Development\b\n342\nIntroducing TDD into new teams\b\n345\nProblems and limitations\b\n346\nExample of the TDD process\b\n347\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "Table of Contents\n[ x ]\nIntroduction to unit testing in Python\b\n350\nPython unittest\b\n350\nPytest\b\n354\nTesting external dependencies\b\n358\nMocking\b\n361\nDependency injection\b\n364\nDependency injection in OOP\b\n366\nAdvanced pytest\b\n369\nGrouping tests\b\n370\nUsing fixtures\b\n374\nSummary\b\n379\nChapter 11: Package Management\b\n381\nThe creation of a new package\b\n382\nTrivial packaging in Python\b\n384\nThe Python packaging ecosystem\b\n386\nPyPI\b\n387\nVirtual environments\b\n390\nPreparing an environment\b\n392\nA note on containers\b\n393\nPython packages\b\n394\nCreating a package\b\n395\nDevelopment mode\b\n398\nPure Python package\b\n398\nCython\b\n401\nPython package with binary code\b\n405\nUploading your package to PyPI\b\n408\nCreating your own private index\b\n415\nSummary\b\n419\nPart IV: Ongoing operations\b\n421\nChapter 12: Logging\b\n423\nLog basics\b\n424\nProducing logs in Python\b\n426\nDetecting problems through logs\b\n430\nDetecting expected errors\b\n430\nCapturing unexpected errors\b\n431\nLog strategies\b\n434\nAdding logs while developing\b\n437\nLog limitations\b\n438\nSummary\b\n439\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "Table of Contents\n[ xi ]\nChapter 13: Metrics\b\n441\nMetrics versus logs\b\n442\nKinds of metrics\b\n443\nGenerating metrics with Prometheus\b\n445\nPreparing the environment\b\n445\nConfiguring Django Prometheus\b\n447\nChecking the metrics\b\n449\nStarting a Prometheus server\b\n450\nQuerying Prometheus\b\n454\nProactively working with metrics\b\n459\nAlerting\b\n460\nSummary\b\n461\nChapter 14: Profiling\b\n463\nProfiling basics\b\n464\nTypes of profilers\b\n465\nProfiling code for time\b\n468\nUsing the built-in cProfile module\b\n470\nLine profiler\b\n475\nPartial profiling \b\n481\nExample web server returning prime numbers\b\n482\nProfiling the whole process\b\n486\nGenerating a profile file per request\b\n489\nMemory profiling\b\n492\nUsing memory_profiler\b\n494\nMemory optimization\b\n496\nSummary\b\n498\nChapter 15: Debugging\b\n501\nDetecting and processing defects\b\n502\nInvestigation in production\b\n504\nUnderstanding the problem in production\b\n506\nLogging a request ID\b\n507\nAnalyzing data\b\n513\nIncreasing logging\b\n514\nLocal debugging\b\n515\nPython introspection tools \b\n518\nDebugging with logs\b\n522\nDebugging with breakpoints\b\n524\nSummary\b\n528\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "Table of Contents\n[ xii ]\nChapter 16: Ongoing Architecture\b\n531\nAdjusting the architecture\b\n532\nScheduled downtime\b\n533\nMaintenance window\b\n534\nIncidents\b\n535\nPostmortem analysis\b\n537\nPremortem analysis\b\n540\nLoad testing\b\n541\nVersioning\b\n543\nBackward compatibility\b\n546\nIncremental changes\b\n548\nDeploying without interruption\b\n551\nFeature flags\b\n554\nTeamwork aspects of changes\b\n555\nSummary\b\n557\nOther Books You May Enjoy\b\n561\nIndex\b\n567\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "[ xiii ]\nPreface\nThe evolution of software means that, over time, systems grow to be more and more \ncomplex, and require more and more developers working on them in a coordinated \nfashion. As the size increases, a general structure arises from there. This structure, if \nnot well planned, can become really chaotic and difficult to work with. \nThe challenge of software architecture is to plan and design this structure. A well-\ndesigned architecture makes different teams able to interact with each other while at \nthe same time having a clear understanding of their own responsibilities and their \ngoals.\nThe architecture of a system should be designed in a way that day-to-day software \ndevelopment is possible with minimal resistance, allowing for adding features and \nexpanding the system. The architecture in a live system is also always in flux, and \ncan be adjusted and expanded as well, reshaping the different software elements in a \ndeliberate and smooth fashion.\nIn this book we will see the different aspects of software architecture, from the top \nlevel to some of the lower-level details that support the higher view. The book is \nstructured in four sections, covering all the different aspects in the life cycle:\n•\t\nDesign before writing any code\n•\t\nArchitectural patterns to use proven approaches\n•\t\nImplementation of the design in actual code\n•\t\nOngoing operation to cover changes, and verification that it's all working as \nexpected\nDuring the book we will cover different techniques across all these aspects.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Preface\n[ xiv ]\nWho this book is for\nThis book is for software developers that want to expand their knowledge of \nsoftware architecture, whether experienced developers that want to expand and \nsolidify their intuitions about complex systems, or less experienced developers who \nwant to learn and grow their abilities, facing bigger systems with a broader view.\nWe will use code written in Python for the examples. Though you're not required to \nbe an expert, some basic knowledge of Python is advisable.\nWhat this book covers\nChapter 1, Introduction to Software Architecture, presents the topic of what software \narchitecture is and why it is useful, as well as presenting a design example.\nThe first section of the book covers the Design phase, before the software is written:\nChapter 2, API Design, shows the basics of designing useful APIs that abstract the \noperations conveniently.\nChapter 3, Data Modeling, talks about the particularities of storage systems and how to \ndesign the proper data representation for the application.\nChapter 4, The Data Layer, goes over the code handling of the stored data, and how to \nmake it fit for purpose.\nNext, we will present a section that covers the different Architectural patterns \navailable, which reuse proven structures:\nChapter 5, The Twelve-Factor App Methodology, shows how this methodology includes \ngood practices that can be useful when operating with web services and can be \napplied in a variety of situations.\nChapter 6, Web Server Structures, explains web services and the different elements to \ntake into consideration when settling on both the operative and the software design.\nChapter 7, Event-Driven Structures, describes another kind of system that works \nasynchronously, receiving information without returning an immediate response.\nChapter 8, Advanced Event-Driven Structures, explains more advanced usages for \nasynchronous systems, and some different patterns that can be created.\nChapter 9, Microservices vs Monolith, presents these two architectures for complex \nsystems, and goes over their differences.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "Preface\n[ xv ]\nThe Implementation section of the book covers how the code is written:\nChapter 10, Testing and TDD, talks about the fundaments of testing and how Test \nDriven Development can be used in the coding process.\nChapter 11, Package Management, follows the process of creating reusable parts of code \nand how to distribute them. \nFinally, the last section deals about Ongoing operations, where the system is in \noperation and requires monitoring at the same time that is adjusted and changed:\nChapter 12, Logging, describes how to record what working systems are doing.\nChapter 13, Metrics, discusses aggregating different values to see how the whole \nsystem is behaving.\nChapter 14, Profiling, explains how to understand how code is executed to improve its \nperformance.\nChapter 15, Debugging, covers the process of digging deep into the execution of code \nto find and fix errors.\nChapter 16, Ongoing Architecture, describes how to successfully operate architectural \nchanges on running systems.\nTo get the most out of this book\n•\t\nThe book uses Python language for code examples, and assumes that the \nreader is comfortable reading it, though an expert level is not needed.\n•\t\nPrevious exposure to complex systems with multiple services will be \nadvantageous to understand the different challenges software architecture \npresents. This should be simple for developers with a couple of years of \nexperience or more.\n•\t\nFamiliarity with web services and REST interfaces is useful to better \nunderstand some elements.\nDownload the example code files\nThe code bundle for the book is hosted on GitHub at https://github.com/\nPacktPublishing/Python-Architecture-Patterns. We also have other code bundles \nfrom our rich catalog of books and videos available at https://github.com/\nPacktPublishing/. Check them out!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "Preface\n[ xvi ]\nDownload the color images\nWe also provide a PDF file that has color images of the screenshots/diagrams \nused in this book. You can download it here: https://static.packt-cdn.com/\ndownloads/9781801819992_ColorImages.pdf\nConventions used\nThere are a number of text conventions used throughout this book.\nCodeInText: Indicates code words in text, object names, module names, folder names, \nfilenames, file extensions, pathnames, dummy URLs and user input. Here is an \nexample: \"For this recipe, we need to import the requests module.\" \nA block of code is set as follows: \ndef leonardo(number):\n    if number in (0, 1):\n        return 1\n    # EXAMPLE COMMENT\n    return leonardo(number - 1) + leonardo(number - 2) + 1\nNote that code may be edited for concision and clarity. Refer to the full code when \nnecessary, which is available on GitHub. \nAny command-line input or output is written as follows (notice the $ symbol): \n$ python example_script.py parameters \nAny input in the Python interpreter is written as follows (notice the >>> symbol). \nExpected output will be reflected without the >>> symbol:\n>>> import logging\n>>> logging.warning('This is a warning')\nWARNING:root:This is a warning\nTo enter the Python interpreter, call the python3 command with no parameters: \n$ python3\nPython 3.9.7 (default, Oct 13 2021, 06:45:31)\n[Clang 13.0.0 (clang-1300.0.29.3)] on darwin\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1489,
      "extraction_method": "Direct"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Preface\n[ xvii ]\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\nAny command-line input or output is written as follows:\n$ cp example.txt copy_of_example.txt\nBold: Indicates a new term, an important word, or words that you see on the screen, \nfor example, in menus or dialog boxes, also appear in the text like this. For example: \n\"Select System info from the Administration panel.\"\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: Email feedback@packtpub.com, and mention the book's title in the \nsubject of your message. If you have questions about any aspect of this book, please \nemail us at questions@packtpub.com.\nErrata: Although we have taken every care to ensure the accuracy of our content, \nmistakes do happen. If you have found a mistake in this book we would be grateful \nif you would report this to us. Please visit, http://www.packtpub.com/submit-errata, \nselecting your book, clicking on the Errata Submission Form link, and entering the \ndetails.\nPiracy: If you come across any illegal copies of our works in any form on the \nInternet, we would be grateful if you would provide us with the location address \nor website name. Please contact us at copyright@packtpub.com with a link to the \nmaterial.\nIf you are interested in becoming an author: If there is a topic that you have \nexpertise in and you are interested in either writing or contributing to a book, please \nvisit http://authors.packtpub.com.\nWarnings or important notes appear like this.\nTips and tricks appear like this.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "Preface\n[ xviii ]\nShare your thoughts\nOnce you've read Python Architecture Patterns, we'd love to hear your thoughts! Please \nclick here to go straight to the Amazon review page for this book and share \nyour feedback.\nYour review is important to us and the tech community and will help us make sure \nwe're delivering excellent quality content.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 445,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "[ 1 ]\n1\nIntroduction to Software \nArchitecture\nThe objective of this chapter is to present an introduction to what software \narchitecture is and where it's useful. We will look at some of the basic techniques \nused when defining the architecture of a system and a baseline example of the web \nservices architecture.\nThis chapter includes a discussion of the implications that software structure has \nfor team structure and communication. As the successful building of any non-tiny \npiece of software depends heavily on successful communication and collaboration \nbetween one or more teams of multiple developers, this factor should be taken into \nconsideration. Also, the structure of the software can have a profound effect on how \ndifferent elements are accessed, so how software is structured has ramifications for \nsecurity.\nAlso, in this chapter, there will be a brief introduction to the architecture of \nan example system that we will be using to present the different patterns and \ndiscussions throughout the rest of the book.\nIn this chapter, we'll cover the following topics:\n•\t\nDefining the structure of a system\n•\t\nDividing into smaller units\n•\t\nConway's Law in software architecture\n•\t\nGeneral overview of the example\n•\t\nSecurity aspects of software architecture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1377,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "Introduction to Software Architecture\n[ 2 ]\nLet's dive in.\nDefining the structure of a system\nAt its core, software development is about creating and managing complex systems.\nIn the early days of computing, programs were relatively simple. At most, they \nperhaps could calculate a parabolic trajectory or factorize numbers. The very first \ncomputer program, designed in 1843 by Ada Lovelace, calculated a sequence of \nBernoulli numbers. A hundred years after that, during the Second World War, \nelectronic computers were invented to break encryption codes. As the possibilities \nof the new invention started to be explored, more and more complex operations and \nsystems were designed. Tools like compilers and high-level languages multiplied the \nnumber of possibilities and the rapid advancement of hardware allowed more and \nmore operations to be performed. This quickly created a need to manage the growing \ncomplexity and apply consistent engineering principles to the creation of software.\nMore than 50 years after the birth of the computing industry, the software tools at \nour disposal are incredibly varied and powerful. We stand on the shoulders of giants \nto build our own software. We can quickly add a lot of functionalities with relatively \nlittle effort, either leveraging high-level languages and APIs or using out-of-the-\nbox modules and packages. With this great power comes the great responsibility of \nmanaging the explosion of complexity that it produces.\nIn the most simple terms, software architecture defines the structure of a software \nsystem. This architecture can develop organically, usually in the early stages of \na project, but after system growth and a few change requests, the need to think \ncarefully about the architecture becomes more and more important. As the system \nbecomes bigger, the structure becomes more difficult to change, which affects future \nefforts. It's easier to make changes following the structure rather than against the \nstructure.\nMaking it so that certain changes are difficult to do is not \nnecessarily always a bad thing. Changes that should be made \ndifficult could involve elements that need to be overseen by \ndifferent teams or perhaps elements that can affect external \ncustomers. While the main focus is to create a system that's easy \nand efficient to change in the future, a smart architectural design \nwill have a proper balance of ease and difficulty based on the \nrequirements. Later in the chapter, we will study security as a clear \nexample of when to keep certain operations difficult to implement.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "Chapter 1\n[ 3 ]\nAt the core of software architecture, then, is taking a look at the big picture: to focus \non where the system is going to be in the future, to be able to materialize this view, \nbut also to help the present situation. The usual choice between short-term wins \nand long-term operation is very important in development, and its most common \noutcome is the creation of technical debt. Software architecture deals mostly with \nlong-term implications.\nThe considerations for software architecture can be quite numerous and there needs \nto be a balance between them. Some examples may include:\n•\t\nBusiness vision, if the system is going to be commercially exploited. This \nmay include requirements coming from stakeholders like marketing, sales, \nor management. Business vision is typically driven by customers.\n•\t\nTechnical requirements, like being sure that the system is scalable and can \nhandle a certain number of users, or that the system is fast enough for its use \ncase. A news website requires different update times than a real-time trading \nsystem.\n•\t\nSecurity and reliability concerns, the seriousness of which depends on how \nrisky or critical the application and the data stored are.\n•\t\nDivision of tasks, to allow multiple teams, perhaps specialized in different \nareas, to work in a flexible way at the same time on the same system. As \nsystems grow, the need to divide them into semi-autonomous, smaller \ncomponents becomes more pressing. Small projects may live longer with a \n\"single-block\" or monolithic approach.\n•\t\nUse specific technologies, for example, to allow integration with other \nsystems or leverage the existing knowledge in the team.\nThese considerations will influence the structure and design of a system. In a sense, \nthe software architect is responsible for implementing the application vision and \nmatching it with the specific technologies and teams that will develop it. That \nmakes the software architect an important intermediary between the business \nteams and the technology teams, as well as between the different technology teams. \nCommunication is a critical aspect of the job.\nTo enable successful communication, a good architecture should define boundaries \nbetween the different aspects and assign clear responsibilities. The software architect \nshould, in addition to defining clear boundaries, facilitate the creation of interface \nchannels between the system components and follow up on the implementation \ndetails.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2581,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "Introduction to Software Architecture\n[ 4 ]\nIdeally, the architectural design should happen at the beginning of system design, \nwith a well thought-out design based on the requirements for the project. This is \nthe general approach in this book because it's the best way to explain the different \noptions and techniques. But it's not the most common use case in real life.\nOne of the main challenges for a software architect is working with existing systems \nthat need to be adapted, making incremental approaches toward a better system, all \nwhile not interrupting the normal daily operation that keeps the business running.\nDivision into smaller units\nThe main technique for software architecture is to divide the whole system into \nsmaller elements and describe how they interact with each other. Each smaller \nelement, or unit, should have a clear function and interface.\nFor example, a common architecture for a typical system could be a web service \narchitecture composed of:\n•\t\nA database that stores all the data in MySQL\n•\t\nA web worker that serves dynamic HTML content written in PHP\n•\t\nAn Apache web server that handles all the web requests, returns any static \nfiles, like CSS and images, and forwards the dynamic requests to the web \nworker\nFigure 1.1: Typical web architecture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "Chapter 1\n[ 5 ]\nAs you can see, every different element has a distinct function in the system. They \ninteract with each other in clearly defined ways. This is known as the Single-\nResponsibility principle. When presented with new features, most use cases will fall \nclearly within one of the elements of the system. Any style changes will be handled \nby the web server and dynamic changes by the web worker. There are dependencies \nbetween the elements, as the data stored in the database may need to be changed to \nsupport dynamic requests, but they can be detected early in the process.\nEach element has different requirements and characteristics:\n•\t\nThe database needs to be reliable, as it stores all the data. Maintenance work \nlike backup- and recovery-related work will be important. The database \nwon't be updated very frequently, as databases are very stable. Changes to \nthe table schemas will be made through restarts in the web worker.\n•\t\nThe web worker needs to be scalable and not store any state. Instead, any \ndata will be sent and received from the database. This element will be \nupdated often. Multiple copies can be run, either in the same machine or in \nmultiple ones to allow horizontal scalability.\n•\t\nThe web server will require some changes for new styling, but that won't \nhappen very often. Once the configuration is properly set up, this element \nwill remain quite stable. Only one web server per machine is required, as it's \ncapable of load-balancing between multiple web workers.\nThis architecture and tech stack has been extremely popular \nsince the early 2000s and was called LAMP, an acronym made \nfrom the different open source projects involved: (L)inux as an \noperating system, (A)pache, (M)ySQL, and (P)HP. Nowadays, \nthe technologies can be swapped for equivalent ones, like using \nPostgreSQL instead of MySQL or Nginx instead of Apache, but still \nusing the LAMP name. The LAMP architecture can be considered \nthe default starting point when designing web-based client/server \nsystems using HTTP, creating a solid and proven foundation to \nstart building a more complex system.\nWe will describe this architecture in greater detail in Chapter 9.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "Introduction to Software Architecture\n[ 6 ]\nAs we can see, the work balance between elements is very different, as the web \nworker will be the focus for most new work, while the other two elements will be \nmuch more stable. The database will require specific work for us to be sure that it's in \ngood shape, as it's arguably the most critical element of the three. The other two can \nrecover quickly if there's a problem, but any corruption in the database will generate \na lot of problems.\nThe communication protocols are also unique. The web worker talks to the database \nusing SQL statements. The web server talks to the web worker using a dedicated \ninterface, normally FastCGI or a similar protocol. The web server communicates with \nthe external clients via HTTP requests. The web server and the database don't talk to \neach other.\nThese three protocols are different. This doesn't have to be the case for all systems; \ndifferent components can share the same protocol. For example, there can be \nmultiple RESTful interfaces, which is common in microservices.\nIn-process communication\nThe typical way of looking at different units is as different processes running \nindependently, but that's not the only option. Two different modules inside the \nsame process can still follow the Single-Responsibility principle.\nThe most critical and valuable element of a system is almost always \nthe stored data.\nThe Single-Responsibility principle can be applied at different \nlevels and is used to define the divisions between functions or \nother blocks. So, it can be applied in smaller and smaller scopes. \nIt's turtles all the way down! But, from the point of view of \narchitecture, the higher-level elements are the most important, as \nit's the higher level that defines the structure. Knowing how far \nto go in terms of detail is clearly important, but when taking an \narchitectural approach, it is better to err on the \"big picture\" side \nrather than the \"too much detail\" one.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "Chapter 1\n[ 7 ]\nA clear example of this would be a library that's maintained independently, but it \ncould also be certain modules within a code base. For example, you could create a \nmodule that performs all the external HTTP calls and handles all the complexity \nof keeping connections, retries, handling errors, and so on, or you could create a \nmodule to produce reports in multiple formats, based on some parameters.\nThe important characteristic is that in order to create an independent element, the \nAPI needs to be clearly defined and the responsibility needs to be well defined. It \nshould be possible for the module to be extracted into a different repo and installed \nas a third-party element for it to be considered truly independent.\nInside the same component, communication is typically straightforward, as internal \nAPIs will be used. In the vast majority of cases, the same programming language will \nbe used.\nConway's Law – Effects on software \narchitecture\nA critical concept to always keep in mind while dealing with architectural designs \nis Conway's Law. Conway's Law is a well-known adage that postulates that the \nsystems introduced in organizations mirror the communication pattern of the \norganization structure (https://www.thoughtworks.com/insights/articles/\ndemystifying-conways-law):\nAny organization that designs a system (defined broadly) will produce a design \nwhose structure is a copy of the organization's communication structure.\n– Melvin E. Conway\nCreating a big component with internal divisions only is a well-\nknown pattern called a monolithic architecture. The LAMP \narchitecture described above is an example of that, as most of the \ncode is defined inside the web worker. Monoliths are the usual de \nfacto starts of projects, as normally at the start there's no big plan \nand dividing things strictly into multiple components doesn't have \na big advantage when the code base is small. As the code base and \nsystem grow more and more complex, the division of elements \ninside the monolith starts to make sense, and later it may start to \nmake sense to split it into several components. We will discuss \nmonoliths further in Chapter 9, Microservices vs Monolith.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2304,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "Introduction to Software Architecture\n[ 8 ]\nThis means that the structure of the organization's people is replicated, either \nexplicitly or otherwise, to form the software structure created by an organization. In \na very simple example, a company that has two big departments – say, purchases \nand sales – will tend to create two big systems, one focused on buying and another \non selling, that talk to each other, instead of other possible structures, like a system \nwith divisions by product.\nThis can feel natural; after all, communication between teams is more difficult than \ncommunication within teams. Communication between teams would need to be \nmore structured and require more active work. Communication inside a single group \nwould be more fluid and less rigid. These elements are key for the design of a good \nsoftware architecture.\nThe main thing for the successful application of any software architecture is that \nthe team structure needs to follow the designed architecture quite closely. Trying \nto deviate too much will result in difficulties, as the tendency will be to structure, \nde facto, everything following group divisions. In the same way, changing the \narchitecture of a system would likely necessitate restructuring the organization. \nThis is a difficult and painful process, as anyone who has experienced a company \nreorganization will attest.\nDivision of responsibilities is also a key aspect. A single software element should \nhave a clear owner, and this shouldn't be distributed across multiple teams. Different \nteams have different goals and focuses, which will complicate the long-term vision \nand create tensions.\nIf there's a big imbalance in the mapping of work units to teams (for example, too \nmany work units for one team and too few for another team), it is likely that there's \na problem with the architecture of the system.\nAs remote work becomes more common and teams increasingly become located \nin different parts of the world, communication is also impacted. That's why it \nhas become very common to set up different branches to take care of different \nelements of the system and to use detailed APIs to overcome the physical barriers \nof geographical distance. Communication improvements also have an effect on the \ncapacity for collaboration, making remote work more effective and allowing fully \nremote teams to work closely together on the same code base.\nThe reverse, a single team taking ownership of multiple elements, \nis definitely possible but also requires careful consideration to \nensure that this doesn't overstress the team.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2691,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "Chapter 1\n[ 9 ]\nConway's Law should not be considered an impediment to overcome but a \nreflection of the fact that organizational structure has an impact on the structure \nof the software. Software architecture is tightly related to how different teams \nare coordinated and responsibilities are divided. It has an important human \ncommunication component.\nKeeping this in mind will help you design a successful software architecture so \nthat the communication flow is fluid at all times and you can identify problems in \nadvance. Software architecture is, of course, closely tied to the human factor, as the \narchitecture will ultimately be implemented and maintained by engineers.\nApplication example – Overview \nIn this book, we will be using an application as an example to demonstrate the \ndifferent elements and patterns presented. This application will be simple but \ndivided into different elements for demonstration purposes. The full code for the \nexample is available on GitHub, and different parts of it will be presented in the \ndifferent chapters. The example is written in Python, using well-known frameworks \nand modules.\nThe example application is a web application for microblogging, very similar to \nTwitter. In essence, users will write short text messages that will be available for \nother users to read.\nThe recent COVID-19 crisis has greatly increased the trend of \nremote working, especially in software. This is resulting in more \npeople working remotely and in better tools that are adapted to \nwork in this way. While time zone differences are still a big barrier \nto communication, more and more companies and teams are \nlearning to work effectively in full-remote mode. Remember that \nConway's Law is very much dependent on the communication \ndependencies of organizations, but communication itself can \nchange and improve.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "Introduction to Software Architecture\n[ 10 ]\nThe architecture of the example system is described in this diagram:\nFigure 1.2: Example architecture\nIt has the following high-level functional elements:\n•\t\nA public website in HTML that can be accessed. This includes functionality \nfor login, logout, writing new micro-posts, and reading other users' micro-\nposts (no need to be logged in for this).\n•\t\nA public RESTful API, to allow the usage of other clients (mobile, JavaScript, \nand so on) instead of the HTML site. This will authenticate the users using \nOAuth and perform actions similar to the website.\n•\t\nA task manager that will execute event-driven tasks. We will add periodic \ntasks that will calculate daily statistics and send email notifications to users \nwhen they are named in a micro-post.\n•\t\nA database that stores all the information. Note that access to it is shared \nbetween the different elements.\n•\t\nInternally, a common package to ensure that the database is accessed \ncorrectly for all the services. This package works as a different element.\nThese two elements, while distinct, will be made into a \nsingle application, as shown in the diagram. The front-\nfacing part of the application will include a web server, as \nwe saw in the LAMP architecture description, which has \nnot been displayed here for simplicity.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1437,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "Chapter 1\n[ 11 ]\nSecurity aspects of software architecture\nAn important element to take into consideration when creating an architecture is \nthe security requirements. Not every application is the same, so some can be more \nrelaxed in this aspect than others. For example, a banking application needs to be 100 \ntimes more secure than, say, an internet forum for discussing cats. The most common \nexample of this is the storage of passwords. The most naive approach to passwords \nis to store them, in plain text, associated with a username or email address – say, in a \nfile or a database table. When the user tries to log in, we receive the input password, \ncompare it with the one stored previously, and, if they are the same, we allow the \nuser to log in. Right?\nWell, this is a very bad idea, because it can produce serious problems:\n•\t\nIf an attacker has access to the storage for the application, they'll be able to \nread the passwords of all the users. Users tend to reuse passwords (even if \nit's a bad idea), so, paired with their emails, they'll be exposed to attacks on \nmultiple applications, not only the breached one.\n•\t\nAnother real issue is insider threats, workers who may have legitimate access \nto the system but copy data for nefarious purposes or by mistake. For very \nsensitive data, this can be a very important consideration.\n•\t\nMistakes like displaying the password of a user in status logs.\nTo make things secure, data needs to be structured in a way that's as protected as \npossible from access or even copying, without exposing the real passwords of users. \nThe usual solution to this is to have the following schema:\n1.\t The password itself is not stored. Instead, a cryptographical hash of the \npassword is stored. This applies a mathematical function to the password \nand generates a replicable sequence of bits, but the reverse operation is \ncomputationally very difficult.\n2.\t As the hash is deterministic based on the input, a malicious actor could \ndetect duplicated passwords, as their hashes are the same. To avoid this \nproblem, a random sequence of characters, called a salt, is added for each \naccount. This will be added to each password before hashing, meaning two \nusers with the same password but different salts will have different hashes.\nThis may seem unlikely, but keep in mind that any copy of \nthe data stored is susceptible to attack, including backups.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "Introduction to Software Architecture\n[ 12 ]\n3.\t Both the resulting hash and the salt are stored.\n4.\t When a user tries to log in, their input password is added to the salt, and the \nresult is compared with the stored hash. If it's correct, the user is logged in.\nNote that in this design, the actual password is unknown to the system. It's not \nstored anywhere and is only accepted temporarily to compare it with the expected \nhash, after being processed.\nThis kind of system is more secure than one that stores the password directly, as \nthe password is not known by the people operating the system, nor is it stored \nanywhere.\nIn certain cases, the same approach as for passwords can be taken to encrypt other \nstored data, so that only customers can access their own data. For example, you can \nenable end-to-end encryption for a communication channel.\nSecurity has a very close relationship with the architecture of a system. As we saw \nbefore, the architecture defines which aspects are easy and difficult to change and \ncan make some unsafe things impossible to do, like knowing the password of a user, \nas we described in the previous example. Other options include not storing data \nfrom the user to keep privacy or reducing the data exposed in internal APIs, for \nexample. Software security is a very difficult problem and is often a double-edged \nsword, and trying to make a system more secure can have the side effect of making \noperations long-winded and inconvenient.\nThis example is presented in a simplified way. There are multiple \nways of using this schema and different ways of comparing a \nhash. For example, the bcrypt function can be applied multiple \ntimes, increasing encryption each time, which can increase the time \nrequired to produce a valid hash, making it more resistant to brute-\nforce attacks.\nThe problem of mistakenly displaying the password of a user in \nstatus logs may still happen! Extra care should be taken to make \nsure that sensitive information is not being logged by mistake.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "Chapter 1\n[ 13 ]\nSummary\nIn this chapter, we looked at what software architecture is and when it is required, as \nwell as its focus on the long-term approach, which is characteristic of the discipline. \nWe learned that the underlying structure of software is difficult to change and that \nthat aspect should be taken into consideration when designing and changing a \nsoftware system.\nWe described how the most important thing is to divide a complex system into \nsmaller parts and assign clear goals and objectives to each of them, keeping in \nmind that these smaller parts can use multiple programming languages and refer \nto different scopes. We also described the LAMP architecture and how it's a widely \nsuccessful starting point when creating simple web service systems.\nWe talked about how Conway's Law affects the architecture of a system, as \nunderlying team structures have a direct impact on the implementation and \nstructure of software. After all, software is operated and developed by humans, and \nhuman communication needs to be accounted for to implement it successfully.\nWe described the example that we will use throughout the book to describe the \ndifferent elements and patterns we will present. Finally, we commented on the \nsecurity aspects of software architecture and how creating barriers to accessing data \nas part of the structural design of a system can mitigate security issues.\nIn the next section of the book, we will talk about the different aspects of designing a \nsystem.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "We will first spend some time explaining the basic steps to designing a system. My \nsuggestion is as follows: \"Design is the first stage of any successful system, and \nencompasses everything that you work on before you begin implementation.\" In this \nsection, we will focus on the general principles and core aspects of each element of \nthe system.\nTwo main core elements should be at the forefront when designing each part of the \nsystem: The interface, or how an element of the system connects to the rest, and data \nstorage, how this element stores information that can be retrieved later.\nBoth are critical. The interface defines what the system is and its functionality from \nthe point of view of any user. A well-designed interface hides the implementation \ndetails and provides some abstractions that allow for a consistent and comprehensive \nway of performing actions.\nThe heart of virtually every successful working system is the data. This is where the \nvalue of the system lies. Any seasoned engineer will tell you that an organization can \nreconstruct a system when the data is available, even if the code that produced it is \nlost, rather than recover from a total loss of the data, even if the application code is \navailable.\nThe storage of data is, then, the core of the system. There are many options we can \nchoose from when it comes to storing our data. What kind of database? Store the \ndata in one data storage facility, or several? The traditional way of using raw access \nto the database, typically in plain SQL statements, is not the most efficient option, \nand it's prone to problems when complex systems are involved. Other kinds of \ndatabases exist that don't even use SQL. We will look at multiple options along with \ntheir pros and cons.\nPart I\nDesign\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "Changing how the data is stored in the system is hard once the system is in \noperation. It isn't impossible but will require a lot of work. The storage option is \narguably the founding stone when designing a new system, so be sure that the \nchosen option fits your requirements. It can be difficult to design something that isn't \noverly complex but also allows the allocated space to grow as the application starts \nto store more and more data as it's used.\nThis section of the book comprises the following chapters:\n1.\t API Design, describing how to create useful, yet flexible, interfaces\n2.\t Data Modeling, with different ways of handling and representing data to \nensure that this critical aspect is well thought through from the outset\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "[ 17 ]\n2\nAPI Design\nIn this chapter, we will talk about the basic application programming interface \n(API) design principles. We will see how to start our design by defining useful \nabstractions that will create the foundation for the design.\nWe will then present the principles for RESTful interfaces, covering both the strict, \nacademic definition and a more practical definition to help when making designs. \nWe will look at design approaches and techniques to help create a useful API based \non standard practices. We will also spend some time talking about authentication, as \nthis is a critical element for most APIs.\nWe will cover how to create a versioning system for the API, attending to the \ndifferent use cases that can be affected.\nWe will focus in this book on RESTful interfaces, as they are the \nmost common right now. Before that, there were other alternatives, \nincluding Remote Procedure Call (RPC) in the 80s, a way to make \na remote function call, or Single Object Access Protocol (SOAP) in \nthe early 2000s, which standardized the format of the remote call. \nCurrent RESTful interfaces are easier to read and take advantage \nof the already established usage of HTTP more strongly, although, \nin essence, they could potentially be integrated via these older \nspecifications. \nThey are still available nowadays, although predominantly in older \nsystems.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1475,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "API Design\n[ 18 ]\nWe will see the difference between the frontend and the backend, and its interaction. \nAlthough the main objective of the chapter is to talk about API interfaces, we will \nalso talk about HTML interfaces to see the differences and how they interact with \nother APIs.\nFinally, we will describe the design for the example that we will use later in the book.\nIn this chapter, we'll cover the following topics:\n•\t\nAbstractions\n•\t\nRESTful interfaces\n•\t\nAuthentication\n•\t\nVersioning the API\n•\t\nFrontend and backend\n•\t\nHTML interfaces\n•\t\nDesigning the API for the example\nLet's take a look at abstractions first.\nAbstractions\nAn API allows us to use a piece of software without totally understanding all the \ndifferent steps that are involved. It presents a clear menu of actions that can be \nperformed, enabling an external user, who doesn't necessarily understand the \ncomplexities of the operation, to perform them efficiently. It presents a simplification \nof the process.\nThese actions can be purely functional, where the output is only related to the input; \nfor example, a mathematical function that calculates the barycenter of a planet and a \nstar, given their orbits and masses. \nAlternatively, they can deal with state, as the same action repeated twice may have \ndifferent effects; for example, retrieving the time in the system. Perhaps even a call \nallows the time zone of the computer to be set, and two subsequent calls to retrieve \nthe time may return very different results.\nIn both cases, the APIs are defining abstractions. Retrieving the time of the system \nin a single operation is simple enough, but perhaps the details of doing so are not \nso easy. It may involve reading in a certain way some piece of hardware that keeps \ntrack of time. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "Chapter 2\n[ 19 ]\nDifferent hardware may report the time differently, but the result should always be \ntranslated in a standard format. Time zones and time savings need to be applied. All \nthis complexity is handled by the developers of the module that exposes the API and \nprovides a clear and understandable contract with any user. \"Call this function, and \nthe time in ISO format will be returned.\"\nThis is, of course, a simple example, but APIs can hide a tremendous amount of \ncomplexity under their interfaces. A good example to think about is a program like \ncurl. Even when just sending an HTTP request to a URL and printing the returned \nheaders, there is a huge amount of complexity associated with this:\n$ curl -IL http://google.com\nHTTP/1.1 301 Moved Permanently\nLocation: http://www.google.com/\nContent-Type: text/html; charset=UTF-8\nDate: Tue, 09 Mar 2021 20:39:09 GMT\nExpires: Thu, 08 Apr 2021 20:39:09 GMT\nCache-Control: public, max-age=2592000\nServer: gws\nContent-Length: 219\nX-XSS-Protection: 0\nX-Frame-Options: SAMEORIGIN\nHTTP/1.1 200 OK\nContent-Type: text/html; charset=ISO-8859-1\nP3P: CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"\nDate: Tue, 09 Mar 2021 20:39:09 GMT\nServer: gws\nX-XSS-Protection: 0\nX-Frame-Options: SAMEORIGIN\nTransfer-Encoding: chunked\nExpires: Tue, 09 Mar 2021 20:39:09 GMT\nWhile we are mainly talking about APIs, and throughout the book \nwe will describe mostly ones related to online services, the concept \nof abstractions really can be applied to anything. A web page to \nmanage a user is an abstraction, as it defines the concept of \"user \naccount\" and the associated parameters. Another omnipresent \nexample is the \"Shopping cart\" for e-commerce. It's good to create \na clear mental image, as it helps to create a clearer and more \nconsistent interface for the user.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1929,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "API Design\n[ 20 ]\nCache-Control: private\nSet-Cookie: NID=211=V-jsXV6z9PIpszplstSzABT9mOSk7wyucnPzeCz-TUSfOH9_F-\n07V6-fJ5t9L2eeS1WI-p2G_1_zKa2Tl6nztNH-ur0xF4yIk7iT5CxCTSDsjAaasn4c6mfp3\nfyYXMp7q1wA2qgmT_hlYScdeAMFkgXt1KaMFKIYmp0RGvpJ-jc; expires=Wed, 08-\nSep-2021 20:39:09 GMT; path=/; domain=.google.com; HttpOnly\nThis makes a call to www.google.com and displays the headers of the response using \nthe -I flag. The -L flag is added to automatically redirect any request which is what \nis happening here.\nMaking a remote connection to a server requires a lot of different moving parts:\n•\t\nDNS access to translate the server address www.google.com to an actual IP \naddress.\n•\t\nThe communication between both servers, which involves using the TCP \nprotocol to generate a persistent connection and guarantee the reception of \nthe data.\n•\t\nRedirection based on the result from the first request, as the server returns \na code pointing to another URL. This was done owing to the usage of the -L \nflag.\n•\t\nThe redirection points to an HTTPS URL, which requires adding a \nverification and encryption layer on top of that.\nEach of these steps also makes use of other APIs to perform smaller actions, which \ncould involve the functionality of the operating system or even calling remote \nservers such as the DNS one to obtain data from there.\nBut, from the point of view of the user of curl, this is not very relevant. It is \nsimplified to the point where a single command line with a few flags can perform \na well-defined operation without worrying about the format to get data from the \nDNS or how to encrypt a request using SSL.\nHere, the curl interface is used from the command line. While the \nstrict definition of an API discard stipulates that the end user is a \nhuman, there's not really a big change. Good APIs should be easily \ntestable by human users. Command-line interfaces can also be \neasily automated by bash scripts or other languages.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2043,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "Chapter 2\n[ 21 ]\nUsing the right abstractions\nFor a successful interface, the root is to create a series of abstractions and present \nthem to the user so that they can perform actions. The most important question when \ndesigning a new API is, therefore, to decide which are the best abstractions.\nWhen the process happens organically, the abstractions are decided mostly on the \ngo. There is an initial idea, acknowledged as an understanding of the problem, that \nthen gets tweaked. \nFor example, it's very common to start a user management system by adding \ndifferent flags to the users. So, a user has permission to perform action A, and then \na parameter to perform action B, and so on. By adding one flag at a time, come the \ntenth flag, the process becomes very confusing.\nThen, a new abstraction can be used; roles and permissions. Certain kinds of users \ncan perform different actions, such as admin roles. A user can have a role, and the \nrole is the one that describes the related permissions.\nNote that this simplifies the problem, as it's easy to understand and manage. \nHowever, moving from \"an individual collection of flags\" to \"several roles\" can \nbe a complicated process. There is a reduction in the number of possible options. \nPerhaps some existing users have a peculiar combination of flags. All this needs to \nbe handled carefully.\nWhile designing a new API, it is good to try to explicitly describe the inherent \nabstractions that the API uses to clarify them, at least at a high level. This also has \nthe advantage of being able to think about that as a user of the API and see if things \nadd up.\nHowever, every abstraction has its limits.\nOne of the most useful viewpoints in the work of software \ndevelopers is to detach yourself from your \"internal view\" and take \nthe position of the actual user of the software. This is more difficult \nthan it sounds, but it's certainly a skill worth developing. This \nwill make you a better designer. Don't be afraid to ask a friend or \ncoworker to detect blind spots in your design.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "API Design\n[ 22 ]\nLeaking abstractions\nWhen an abstraction is leaking details from the implementation, and not presenting a \nperfectly opaque image, it's called a leaky abstraction.\nWhile a good API should try to avoid this, sometimes it happens. This can be caused \nby underlying bugs in the code serving the API, or sometimes directly from the way \nthe code operates in certain operations.\nA common case for this is relational databases. SQL abstracts the process of \nsearching data from how it is actually stored in the database. You can search with \ncomplex queries and get the result, and you don't need to know how the data \nis structured. But sometimes, you'll find out that a particular query is slow, and \nreorganizing the parameters of the query has a big impact on how this happens. This \nis a leaky abstraction.\nOperating systems are good examples of a system that generates good abstractions \nthat don't leak the majority of the time. There are lots of examples. Not being able \nto read or write a file due to a lack of space (a less common problem now than three \ndecades ago); breaking a connection with a remote server due to a network problem; \nor not being able to create a new connection due to reaching a limit in terms of the \nnumber of open file descriptors.\nLeaky abstractions are, to a certain degree, unavoidable. They are the result of not \nliving in a perfect world. Software is fallible. Understanding and preparing for that is \ncritical.\n\"All non-trivial abstractions, to some degree, are leaky.\"\n– Joel Spolsky's Law of Leaky Abstractions\nWhen designing an API, it is important to take this fact into account for several \nreasons:\n•\t\nTo present clear errors and hints externally. A good design will always \ninclude cases for things going wrong and try to present them clearly with \nproper error codes or error handling.\nThis is very common, and the reason why there are significant \ntools to help ascertain what is going on when running a SQL query, \nwhich is very detached from the implementation. The main one is \nthe EXPLAIN command. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "Chapter 2\n[ 23 ]\n•\t\nTo deal with errors that could come from dependent services internally. \nDependent services can fail or have other kinds of problems. The API \nshould abstract this to a certain degree, recovering from the problem if \npossible, failing gracefully if not, and returning a proper result if recovery \nis impossible.\nThe best design is the one that not only designs things when they work as expected, \nbut also prepares for unexpected problems and is sure that they can be analyzed \nand corrected.\nResources and action abstractions\nA very useful pattern to consider when designing an API is to produce a set \nof resources that can perform actions. This pattern uses two kinds of elements: \nresources and actions. \nResources are passive elements that are referenced, while actions are performed on \nresources.\nFor example, let's define a very simple interface to play a simple game guessing coin \ntosses. This is a game consisting of three guesses for three coin tosses, and the user \nwins if at least two of these guesses are correct.\nThe resource and actions may be as follows:\nResource\nActions\nDetails\nHEADS\nNone\nA coin toss result.\nTAILS\nNone\nA coin toss result.\nGAME\nSTART\nStart a new GAME.\nREAD\nReturns the current round (1 \nto 3) and the current correct \nguesses.\nCOIN_TOSS\nTOSS\nToss the coin. If the GUESS \nhasn't been produced, it \nreturns an error.\nGUESS\nAccepts HEADS or TAILS as \nthe guess.\nRESULT\nIt returns HEADS or TAILS \nand whether the GUESS was \ncorrect.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1588,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "API Design\n[ 24 ]\nA possible sequence for a single game could be:\nGAME START\n> (GAME 1)\nGAME 1 COIN_TOSS GUESS HEAD\nGAME 1 COIN_TOSS TOSS\nGAME 1 COIN_TOSS RESULT\n> (TAILS, INCORRECT)\nGAME 1 COIN_TOSS GUESS HEAD\nGAME 1 COIN_TOSS TOSS\nGAME 1 COIN_TOSS RESULT\n> (HEAD, CORRECT)\nGAME 1 READ\n> (ROUND 2, 1 CORRECT, IN PROCESS)\nGAME 1 COIN_TOSS GUESS HEAD\nGAME 1 COIN_TOSS TOSS\nGAME 1 COIN_TOSS RESULT\n> (HEAD, CORRECT)\nGAME 1 READ\n> (ROUND 3, 2 CORRECT, YOU WIN)\nNote how each resource has its own set of actions that can be performed. Actions can \nbe repeated if that's convenient, but it's not required. Resources can be combined into \na hierarchical representation (like here, where COIN_TOSS depends on a higher GAME \nresource). Actions can require parameters that can be other resources.\nHowever, the abstractions are organized around having a consistent set of resources \nand actions. This way of explicitly organizing an API is useful as it clarifies what is \npassive and what's active in the system.\nThis is a common pattern, and it's used in RESTful interfaces, as we will see next.\nObject-oriented programming (OOP) uses these abstractions, as \neverything is an object that can receive messages to perform some \nactions. Functional programming, on the other hand, doesn't fit \nneatly into this structure, as \"actions\" can work like resources.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "Chapter 2\n[ 25 ]\nRESTful interfaces\nRESTful interfaces are incredibly common these days, and for good reason. They've \nbecome the de facto standard in web services that serve other applications.\nRepresentational State Transfer (REST) was defined in 2000 in a Ph.D. dissertation \nby Roy Fielding, and it uses HTTP standards as a basis to create a definition of a \nsoftware architecture style. \nFor a system to be considered RESTful, it should follow certain rules:\n•\t\nClient-server architecture. It works through remote calling.\n•\t\nStateless. All the information related to a particular request should be \ncontained in the request itself, making it independent from the specific \nserver serving the request.\n•\t\nCacheability. The cacheability of the responses should be clear, either to say \nthey are cacheable or not.\n•\t\nLayered system. The client cannot tell if they are connected to a final server \nor if there's an intermediate server.\n•\t\nUniform interface, with four prerequisites:\n•\t\nResource identification in requests, meaning a resource is \nunequivocally represented, and its representation is independent\n•\t\nResource manipulation through representations, allowing clients to \nhave all the required information to make changes when they have \nthe representation\n•\t\nSelf-descriptive messages, meaning messages are complete in \nthemselves\n•\t\nHypermedia as the Engine of Application State, meaning the client \ncan walk through the system using referenced hyperlinks\n•\t\nCode on demand. This is an optional requirement, and it's normally not \nused. Servers can submit code in response to help perform operations or \nimprove the client; for example, submitting JavaScript to be executed in the \nbrowser.\nThis is the most formal definition. As you can see, it's not necessarily based on \nHTTP requests. For more convenient usage, we need to limit the possibilities \nsomewhat and set a common framework.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "API Design\n[ 26 ]\nA more practical definition\nWhen people talk colloquially about RESTful interfaces, normally they are \nunderstood as interfaces based on HTTP resources using JSON formatted requests. \nThis is wholly compatible with the definition that we've seen before, but taking some \nkey elements into consideration.\nThe main one is that URIs (Uniform Resource Identifiers) should describe clear \nresources, as well as HTTP methods and actions to perform on them, using the \nCRUD (Create Retrieve Update Delete) approach.\nThere are two kinds of URIs, whether they describe a single resource or a collection \nof resources, as can be seen in the following table:\nResource\nExample\nMethod\nDescription\nCollection\n/books\nGET\nList operation. Returns all the available \nelements of the collection, for example, all \nbooks.\nPOST\nCreate operation. Creates a new element of the \ncollection. Returns the newly created resource.\nSingle\n/books/1\nGET\nRetrieve operation. Returns the data from the \nresource, for example, the book with an ID of 1.\nPUT\nSet (Update) operation. Sends the new data \nfor the resource. If it doesn't exist, it will be \ncreated. If it does, it will be overwritten.\nPATCH\nPartial update operation. Overwrites only the \npartial values for the resource, for example, \nsends and writes only the email for the user \nobject.\nDELETE\nDelete operation. It deletes the resource.\nThese key elements are sometimes ignored, leading to pseudo-\nRESTful interfaces, which don't have the same properties.\nCRUD interfaces facilitate the performance of those actions: Create \n(save a new entry), Retrieve (read), Update (overwrite), and Delete \nentries. These are the basic operations for any persistent storage \nsystem.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "Chapter 2\n[ 27 ]\nThe key element of this design is the definition of everything as a resource, as we \nsaw before. Resources are defined by their URIs, which contain a hierarchical view of \nthe resources, for example:\n/books/1/cover defines the resource of the cover image from the book with an ID \nof 1.\nMost of the input and output of the resources will be represented in JSON format. \nFor example, this could be an example of a request and response to retrieve a user:\nGET /books/1\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"name\": \"Frankenstein\", \"author\": \"Mary Shelley\", \"cover\": \"http://\nlibrary.lbr/books/1/cover\"}\nThe response is formatted in JSON, as specified in Content-Type. This makes it easy \nto parse and analyze automatically. Note that the avatar field returns a hyperlink \nto another resource. This makes the interface walkable and reduces the amount of \ninformation that the client requires beforehand.\nFor simplicity, we will use integer IDs to identify the resources in \nthis chapter. In real-world operations, this is not recommended. \nThey have no meaning at all, and, even worse, they can sometimes \nleak information about the number of elements in the system or \ntheir internal order. For example, a competitor could estimate how \nmany new entries are being added each week. To detach from \nwhatever internal representation, try to always use a natural key \nexternally, if available, such as the ISBN number for books, or \ncreate a random Universally Unique Identifier (UUID).\nAnother problem with sequential integers is that, at high rates, \nthe system may struggle to create them correctly, as it won't be \npossible to create two at the same time. This can limit the growth of \na system.\nThis is one of the most forgotten properties when designing \nRESTful interfaces. It is preferable to return full URIs to resources \ninstead of indirect references, such as no-context IDs.\nFor example, when creating a new resource, include the new URI \nin the response, in the Location header.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2115,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "API Design\n[ 28 ]\nTo send new values to overwrite, the same format should be used. Note that some \nelements may be read-only, such as cover, and aren't required:\nPUT /books/1\nContent-Type: application/json\n{\"name\": \"Frankenstein or The Modern Prometheus\", \"author\": \"Mary \nShelley\"}\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"name\": \"Frankenstein or The Modern Prometheus\", \"author\": \"Mary \nShelley\", \"cover\": \"http://library.com/books/1/cover\"}\nThe same representation should be used for input and output, making it easy for \nthe client to retrieve a resource, modify it, and then resubmit it.\nWhen the resource will be directly represented by binary content, it can return the \nproper format, specified in the Content-Type header. For example, retrieving the \navatar resource may return an image file:\nGET /books/1/cover\nHTTP/1.1 200 OK\nContent-Type: image/png\n... \nIn the same way, when creating or updating a new avatar, it should be sent in the \nproper format. \nAnother important property is ensuring that some actions are idempotent, and \nothers are not. Idempotent actions can be repeated multiple times, producing the \nsame result, while repeating not-idempotent actions will generate different results. \nEvidently, the action should be identical.\nThis is really handy and creates a level of consistency that's very \nmuch appreciated when implementing a client. While testing, try \nto ensure that retrieving a value and resubmitting it is valid and \ndoesn't create a problem.\nWhile the original intention of RESTful interfaces was to make use \nof multiple formats, for example, accepting XML and JSON, this \nis not very common in practice. JSON is, by and large, the most \nstandard format these days. Some systems may benefit from using \nmultiple formats, though. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "Chapter 2\n[ 29 ]\nA clear case of this is the creation of a new element. If we submit two identical POST \ncreations of a new element of a resource list, it will create two new elements. For \nexample, submitting two books with the same name and author will create two \nidentical books.\nOn the other hand, two GET requests will produce the same result. The same is true \nfor PUT or DELETE, as they'll overwrite or \"delete again\" the resource. \nThe fact that the only non-idempotent requests are POST actions simplifies \nsignificantly the design of measures to deal with problems when there's the question \nof whether it should be retried. Idempotent requests are safe to retry at any time, \nthereby simplifying the handling of errors such as network problems.\nHeaders and statuses\nAn important detail of the HTTP protocol that can sometimes be overlooked is the \ndifferent headers and status codes.\nHeaders include metadata information about the request or response. Some of it \nis added automatically, like the size of the body of the request or response. Some \ninteresting headers to consider are the following:\nHeader\nType\nDetails\nAuthorization\nStandard\nCredentials to authenticate the request.\nContent-Type\nStandard\nThe type of the body of the request, like \napplication/json or text/html. \nDate\nStandard\nWhen the message was created.\nIf-Modified-Since\nStandard\nThe sender has a copy of the resource at \nthis time. If it hasn't changed since then, \na 304 Not Modified response (with an \nempty body) can be returned. This allows \nthe caching of data and saves time and \nbandwidth by not returning duplicated \ninfo. This can be used in GET requests.\nThis is assuming that there's no limitation to the content of the \nresource. If they are, the second request will fail, which will \nproduce a different result to the first in any case.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "API Design\n[ 30 ]\nX-Forwarded-From\nDe facto \nstandard\nStores the IP where the message was \noriginated, and the different proxies it \nwent through.\nForwarded\nStandard\nSame as X-Forwarded-From. This is a \nnewer header and less common still than \nX-Forwarded-From.\nA well-designed API will make use of headers to communicate proper information, \nfor example, setting Content-Type correctly or accepting cache parameters if possible.\nAnother important detail is to make good use of available status codes. Status codes \nprovide significant information about what happened, and using the most detailed \ninformation possible for each situation will provide a better interface.\nSome common status codes are as follows:\nStatus code\nDescription\n200 OK\nA successful resource access or modification. It \nshould return a body; if it doesn't, use 204 No \nContent.\n201 Created\nA successful POST request that creates a new resource.\n204 No Content\nA successful request that doesn't return a body, for \nexample, a successful DELETE request.\n301 Moved Permanently\nThe accessed resource is now permanently located in \na different URI. It should return a Location header \nwith the new URI. Most libraries will follow up \nautomatically for GET accesses. For example, the API \nis only accessible in HTTPS, but it was accessed in HTTP.\n302 Found\nThe accessed resource is temporarily located in a \ndifferent URI. A typical example is being redirected \nto a login page if authenticated.\n304 Not Modified\nA cached resource is still valid. The body should be \nempty. This status code is only returned if the client \nrequested cached information, for example, using the \nIf-Modified-Since header.\nA comprehensive list of headers can be found at https://\ndeveloper.mozilla.org/en-US/docs/Web/HTTP/Headers.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "Chapter 2\n[ 31 ]\n400 Bad Request\nA generic error in the request. This is the server \nsaying, \"something went wrong on your end.\" A \nmore descriptive message should be added to the \nbody. If a more descriptive status code is possible, it \nshould be preferred.\n401 Unauthorized\nThe request is not allowed, as the request is not \nproperly authenticated. The request may lack valid \nheaders for authentication.\n403 Forbidden\nThe request is authenticated, but it can't access this \nresource. This is different from the 401 Unauthorized \nstatus in that the request is already correctly \nauthenticated but doesn't have access.\n404 Not Found\nProbably the most famous status code! The resource \ndescribed by the URI cannot be found.\n405 Method Not Allowed \nThe requested method cannot be used; for example, \nthe resource cannot be deleted.\n429 Too Many Requests\nThe server should return this status code if there's a \nlimit to the number of requests the client can do. It \nshould return a description or more info in the body, \nand ideally, a Retry-After header indicating the time \nin seconds to the next retry. \n500 Server Error\nA generic error in the server. This status should \nonly be used if an unexpected error happened in the \nserver.\n502 Bad Gateway\nThe server is redirecting the request to a different \nserver, and the communication was incorrect. This \nerror normally appears when some backend service is \nunavailable or incorrectly configured.\n503 Service Unavailable\nThe server is currently unable to handle requests. \nNormally, this is a temporary situation, such as a \nload problem. It could be used to mark maintenance \ndowntime, but this is generally rare.\n504 Gateway Timeout\nSimilar to 502 Bad Gateway, but in this case, the \nbackend service didn't respond, provoking a timeout.\nIn general, non-descriptive error codes such as 400 Bad Request and 500 Server \nError should be left for general situations. However, if there is a better, more \ndescriptive status code, this should be used instead.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "API Design\n[ 32 ]\nFor example, a PATCH request to overwrite a parameter should return 400 Bad \nRequest if the parameter is incorrect for any reason, but 404 Not Found if the resource \nURI is not found.\nIn any error, please include some extra feedback to the user with a reason. A general \ndescriptor can help the handling of unexpected cases and simplify debugging issues.\nFor example, the mentioned PATCH may return this body:\n{\n    \"message\": \"Field 'address' is unknown\"\n}\nThis will give specific details about the problem. Other options include returning \nerror codes, multiple messages in case there are multiple possible errors, and also \nduplicating the status code in the body.\nDesigning resources\nThe available actions in a RESTful API are limited to CRUD operations. Therefore, \nresources are the basic construction blocks for the API. \nMaking everything a resource helps to create very explicit APIs and helps with the \nstateless requirement for RESTful interfaces.\nThere are other status codes. You can check a comprehensive list, \nincluding details on each one, here: https://httpstatuses.com/.\nThis is especially useful for 4XX errors as they will help users of the \nAPI to fix their own bugs and iteratively improve their integration.\nA stateless service means that all the information required to fulfill \na request is either provided by the caller or retrieved externally, \nnormally from a database. This excludes other ways of keeping \ninformation, such as storing information locally in the same \nserver's hard drive. This makes any server capable of handling \nevery single request, and it's critical in achieving scalability.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1747,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "Chapter 2\n[ 33 ]\nElements that could be activated by creating different actions could be separated \ninto different resources. For example, an interface simulating a pen could require the \nfollowing elements:\n•\t\nOpening and closing the pen.\n•\t\nWriting something. Only an open pen can write.\nIn some APIs, like an object-oriented one, this could involve creating a pen object \nand changing its state:\npen = Pen()\npen.open()\npen.write(\"Something\")\npen.close()\nIn a RESTful API, we need to create different resources for both the pen and its \nstatus:\n# Create a new pen with id 1\nPOST /pens\n# Create a new open pen for pen 1\nPOST /pens/1/open\n# Update the new open text for the open pen 1\nPUT /pens/1/open/1/text\n# Delete the open pen, closing the pen\nDELETE /pens/1/open/1\nThis may look a bit cumbersome, but RESTful APIs should aim to be higher level \nthan the typical object-oriented one. Either create the text directly, or create a pen \nand then the text, without having to perform the open/close operation.\nNote also that every single aspect and step gets registered and has its own set of \nidentifiers and is addressable. This is more explicit than the internal state that can be \nfound in OOP. As we've seen, we want it to be stateless, while objects are very much \nstateful.\nKeep in mind that RESTful APIs are used in the context of remote \ncalls. This means that they can't be low level, as each call is a big \ninvestment compared with a local API, as the time per call will be a \nsensible part of the operation.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1620,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "API Design\n[ 34 ]\nDealing only with resources can require certain adaptations if coming from a more \ntraditional OOP environment, but they are a pretty flexible tool and can allocate \nmultiple ways of performing actions.\nResources and parameters\nWhile everything is a resource, some elements make more sense as a parameter \nthat interacts with the resource. This is very natural when modifying the resource. \nAny change needs to be submitted to update the resource. But, in other cases, some \nresources could be modified for other causes. The most common case is searches.\nA typical search endpoint will define a search resource and retrieve its results. \nHowever, a search without parameters to filter is not really useful, so extra \nparameters will be required to define the search, for example:\n# Return every pen in the system\nGET /pens/search\n# Return only red pens\nGET /pens/search?color=red\n# Return only red pens, sorted by creation date\nGET /pens/search?color=red&sort=creation_date\nThese parameters are stored in query parameters, which are natural extensions to \nretrieve them.\nKeep in mind that a resource doesn't need to be translated directly \ninto a database object. That's thinking backward, from the storage \nto the API. Remember that you are not limited to that, and can \ncompose resources that obtain information from multiple sources \nor that don't fit into a direct translation. We will see examples in \nthe next chapter.\nAs a general rule, only GET requests should have query parameters. \nOther kinds of request methods should provide any parameters as \npart of the body.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "Chapter 2\n[ 35 ]\nGET requests are also easy to cache if including the query parameters. If the search is \nreturning the same values for each request, given that that's an idempotent request, \nthe full URI, including the query parameters, can be cached even externally from the \nservice.\nBy convention, all logs that store GET requests will also store the query params, while \nany parameter sent as a header or in the body of the request won't be logged. This \nhas security implications, as any sensible parameter, such as a password, shouldn't \nbe sent as a query parameter.\nSometimes, that's the reason to create POST operations that typically would be a GET \nrequest, but prefer to set parameters in the body of the request instead of query \nparameters. While it is possible in the HTTP protocol to set the body in a GET request, \nit's definitely very unusual.\nAnother reason to use POST requests is to allow a bigger space for parameters, as the \nfull URL, including query parameters, is normally limited to 2K in size, while bodies \nare much less restricted in size.\nPagination\nIn a RESTful interface, any LIST request that returns a sensible number of elements \nshould be paginated.\nThis means that the number of elements and pages can be tweaked from the request, \nreturning only a specific page of elements. This limits the scope of the request and \navoids very slow response times and waste transmission bytes.\nAn example could involve using the parameters page and size, for example:\n# Return only first 10 elements\nGET /pens/search?page=1&size=10\nA well-constructed response will have a similar format to this:\n{\n    \"next\": \"http://pens.pns/pens/search?page=2&size=10\",\n    \"previous\": null,\nAn example of this could be searching by phone number, email, or \nother personal information, so a middle-man agent could intercept \nand learn about them.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "API Design\n[ 36 ]\n    \"result\": [\n        # elements\n    ]\n}\nIt contains a result field with the resulting list and next and previous fields that are \nhyperlinks to the next and previous page, with a value of null if it is not available. \nThis makes it easy to walk through all the results.\nThis technique also allows multiple pages to be retrieved in parallel, which can speed \nup the downloading of information, doing several small requests instead of one big \none. The objective, though, is to provide sufficient filter parameters for generally \nreturning not too much information, being able to retrieve only the relevant \ninformation.\nPagination has a problem, which is that the data in the collection may change \nbetween multiple requests, especially if retrieving many pages. The problem is as \nfollows:\n# Obtain first page\nGET /pens/search?page=1&size=10&sort=name\n# Create a new resource that is added to the first page\nPOST /pens\n# Obtain second page\nGET /pens/search?page=2&size=10&sort=name\nThe second page now has a repeated element that used to be on the first page but \nhas now moved to the second, and then there's one element that's not returned. \nNormally, the non-return of the new resource is not that much of a problem, as, after \nall, the retrieval of information started before its creation. However, the return of the \nsame resource twice can be.\nTo avoid this kind of problem, there's the possibility of sorting by default the values \nby creation date or something analogous. This way, any new resource will be added \nat the end of pagination and will be consistently retrieved.\nA sort parameter could also be useful to ensure consistency in \npages.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1776,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "Chapter 2\n[ 37 ]\nCreating a flexible pagination system increases the usefulness of any API. Be sure \nthat your pagination definition is consistent across any different resources.\nDesigning a RESTful API process\nThe best way to start designing a RESTful API is to clearly state the resources and \nthen describe them, including the following details:\n•\t\nDescription: Description of the action\n•\t\nResource URI: Note that this may be shared for several actions, differentiated \nby the method (for example, GET to retrieve and DELETE to delete)\n•\t\nMethods applicable: The HTTP method to use for the action defined in this \nendpoint\n•\t\n(Only if relevant) Input body: The input body of the request\n•\t\nExpected result in the body: Result\n•\t\nPossible expected errors: Returning status codes depending on specific errors\n•\t\nDescription: Description of the action\n•\t\n(Only if relevant) Input query parameters: Query parameters to add to the URI \nfor extra functionality\n•\t\n(Only if relevant) Relevant headers: Any supported header\n•\t\n(Only if relevant) Returning status codes out of the ordinary (200 and 201): \nDifferent from errors, in case there's a status code that's considered a success \nbut it's not the usual case; for example, a success returns a redirection\nThis will be enough to create a design document that can be understood by other \nengineers and allow them to work on the interface.\nIt is good practice, though, to start with a quick draft of the different URIs and \nmethods, and to have a quick look at all the different resources that the system has \nwithout getting into too much detail, such as a body description or errors. This \nhelps to detect missing resource gaps or other kinds of inconsistencies in the API.\nFor resources that return inherently \"new\" elements, like \nnotifications or similar, add an updated_since parameter to \nretrieve only the new resources since the most recent access. This \nspeeds up access in a practical way and retrieves only the relevant \ninformation.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "API Design\n[ 38 ]\nFor example, the API described in this chapter has the following actions:\nGET    /pens\nPOST   /pens\nPOST   /pens/<pen_id>/open\nPUT    /pens/<pen_id>/open/<open_pen_id>/text\nDELETE /pens/<pen_id>/open/<open_pen_id>\nGET    /pens/search\nThere are a couple of details that can be tweaked and improved here:\n•\t\nIt looks like we forgot to add the action to remove a pen, once created\n•\t\nThere are a couple of GET actions for retrieving information about the created \nresource that should be added\n•\t\nIn the PUT action, it feels a bit redundant to have to add /text\nWith this feedback, we can again describe the API as follows (modifications have an \narrow):\nGET    /pens\nPOST   /pens\nGET    /pens/<pen_id> \nDELETE /pens/<pen_id> ←\nPOST   /pens/<pen_id>/open\nGET    /pens/<pen_id>/open/<open_pen_id> ←\nPUT    /pens/<pen_id>/open/<open_pen_id> ←\nDELETE /pens/<pen_id>/open/<open_pen_id>\nGET    /pens/search\nNote how the organization in the hierarchical structure helps to take a good look \nat all the elements and find either gaps or relations that may not be obvious at first \nglance.\nAfter that, we can get into details. We can use the template described at the start \nof the section, or any other one that works for you. For example, we can define the \nendpoints to create a new pen and read a pen in the system:\nCreating a new pen:\n•\t\nDescription: Creates a new pen, specifying the color.\n•\t\nResource URI: /pens\n•\t\nMethod: POST\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1543,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "Chapter 2\n[ 39 ]\n•\t\nInput body:\n{\n    \"name\": <pen name>,\n    \"color\": (black|blue|red)\n}\n•\t\nErrors:\n400 Bad Request \nError in the body, such as an unrecognized color, a duplicated name, or a bad \nformat.\nRetrieving an existing pen:\n•\t\nDescription: Retrieves an existing pen.\n•\t\nResource URI: /pens/<pen id>\n•\t\nMethod: GET\n•\t\nReturn body:\n{\n    \"name\": <pen name>,\n    \"color\": (black|blue|red)\n    }\n•\t\nErrors:\n404 Not Found \nThe pen ID is not found.\nThe main objective is that these small templates are useful and to the point. Feel free \nto tweak them as expected, and don't worry about being too completist with the \nerrors or details. The most important part is that they are useful; for example, adding \na 405 Method Not Allowed message could be redundant.\nThe API can also be designed using tools such as Postman (www.\npostman.com), which is an API platform that can be used to \neither design or test/debug existing APIs. While useful, it is good \nto be able to design an API without external tools, in case that's \nrequired, and because it forces you to think about the design and \nnot necessarily the tool itself. We will also see how to use Open \nAPI, which is based more on the definition, and not so much on \nproviding a test environment.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1352,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "API Design\n[ 40 ]\nDesigning and defining an API can also enable it to be structured in a standard \nmanner afterward to take advantage of tools.\nUsing the Open API specification\nA more structured alternative is to use a tool such as Open API (https://www.\nopenapis.org/). Open API is a specification for defining a RESTful API through a \nYAML or JSON document. This allows this definition to interact with other tools to \ngenerate automatic documentation for the API.\nIt allows the definition of different components that can be repeated, both as input \nand output. This makes it easy to build consistent reusable objects. There are also \nways of inheriting or composing from one another, thereby creating a rich interface.\nFor example, this is a YAML file that describes the two endpoints described above. \nThe file is available on GitHub: https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/blob/main/pen_example.yaml:\nopenapi: 3.0.0\ninfo:\n  version: \"1.0.0\"\n  title: \"Swagger Pens\"\npaths:\n  /pens:\n    post:\n      tags:\n      - \"pens\"\n      summary: \"Add a new pen\"\n      requestBody:\n        description: \"Pen object that needs to be added to the store\"\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: \"#/components/schemas/Pen\"\nDescribing the whole Open API specification in detail is beyond \nthe scope of this book. Most common web frameworks allow \nintegration with it, generating the YAML file automatically or the \nweb documentation that we'll see later. It was previously called \nSwagger and its web page (https://swagger.io/) has a very \nuseful editor and other resources. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "Chapter 2\n[ 41 ]\n      responses:\n        \"201\":\n          description: \"Created\"\n        \"400\":\n          description: \"Invalid input\"\n  /pens/{pen_id}:\n    get:\n      tags:\n      - \"pens\"\n      summary: \"Retrieve an existing pen\"\n      parameters:\n      - name: \"pen_id\"\n        in: path\n        description: \"Pen ID\"\n        required: true\n        schema:\n          type: integer\n          format: int64\n      responses:\n        \"200\":\n          description: \"OK\"\n          content:\n            application/json:\n              schema:\n                $ref: \"#/components/schemas/Pen\"\n        \"404\":\n          description: \"Not Found\"\ncomponents:\n  schemas:\n    Pen:\n      type: \"object\"\n      properties:\n        name:\n          type: \"string\"\n        color:\n          type: \"string\"\n          enum:\n            - black\n            - blue\n            - red\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "API Design\n[ 42 ]\nIn the components part, the Pen object gets defined, and then is used in both \nendpoints. You can see how both endpoints, POST /pens and GET /pens/{pen_id}, \nare defined and describe the expected input and output, taking into account the \ndifferent errors that can be produced.\nOne of the most interesting aspects of Open API is the ability to automatically \ngenerate a documentation page with all the information to help any possible \nimplementation. The generated documentation looks like this:\nFigure 2.1: Swagger Pens documentation\nIf the YAML file describes your interface correctly and fully, this can be really useful. \nIn some cases, it could be advantageous to work from the YAML to the API. This \nfirst generates the YAML file and allows work in both directions from there, both in \nthe frontend direction and the backend direction. For an API-first approach, it may \nmake sense. It's even possible to automatically create skeletons of clients and servers \nin multiple languages, for example, servers in Python Flask or Spring, and clients in \nJava or Angular.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "Chapter 2\n[ 43 ]\nEach of the endpoints contains further information and can even be tested in the \nsame documentation, thereby significantly helping an external developer who wants \nto use the API, as we can see in the next graphic:\nFigure 2.2: Swagger Pens expanded documentation\nKeep in mind that it's up to you to make the implementation match \nthe definition closely. These skeletons will still require enough \nwork to make them work correctly. Open API will simplify the \nprocess, but it won't magically solve all integration problems.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "API Design\n[ 44 ]\nGiven that it's very easy to ensure that the server can generate this automatic \ndocumentation, even if the design is not started from an Open API YAML file, it's a \ngood idea to generate it so as to create self-generating documentation.\nAuthentication\nA critical part of virtually any API is the ability to distinguish between authorized \nand unauthorized access. Being able to log the user properly is critical, and a \nheadache from the point of view of security.\nSecurity is hard, so it's better to rely on standards to simplify the operation.\nThe most important security issue regarding authentication is to always use \nHTTPS endpoints in production. This allows the channel to be protected against \neavesdropping and makes communication private. Note that an HTTP website just \nmeans that the communication is private; you could be talking with the devil. But it's \nthe bare minimum required to allow users of your API to send you passwords and \nother sensitive information without the fear that an external user is going to receive \nthis information.\nHTTPS endpoints are valid for all access, but other details are specific depending on \nwhether they are HTML interfaces or RESTful ones.\nAuthenticating HTML interfaces\nIn HTML web pages, normally, the flow to authenticate is as follows:\n1.\t A login screen gets presented to the user.\n2.\t The user enters their login and password and sends them to the server.\nAs we said before, these are just general tips, but in no way a \ncomprehensive set of secure practices. This book is not focused on \nsecurity. Please keep up with security issues and solutions, as this \nis a field that is always evolving.\nNormally, most architectures use HTTPS until the request \nreaches the data center or secure network, and then use HTTP \ninternally. This permits a check on the data flowing internally \nbut also protects data that is traveling across the internet. While \nless important these days, it also improves efficiency, as encoding \nrequests in HTTPS require extra processing power.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2147,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "Chapter 2\n[ 45 ]\n3.\t The server verifies the password. If correct, it returns a cookie with a session \nID.\n4.\t The browser receives the response and stores the cookie.\n5.\t All new requests will send the cookie. The server will verify the cookie and \nproperly identify the user.\n6.\t The user can log out, removing the cookie. If this is done explicitly, a request \nwill be sent to the server to delete the session ID. Typically, the session ID \nwill have an expiry time for cleaning itself. This expiry can renew itself on \neach access or force the user to log in again from time to time.\nIt's important to set up the cookie as Secure, HttpOnly, and SameSite. Secure ensures \nthat the cookie is only sent to HTTPS endpoints, and not to HTTP ones. HttpOnly \nrenders the cookie inaccessible by JavaScript, which makes it more difficult to obtain \nthe cookie via malicious code. The cookie will be sent automatically to the host that \nsets it. SameSite ensures that cookies are only sent when the origin of the source is \na page from the same host. It can be set to Strict, Lax, and None. Lax allows you to \nnavigate to the page from a different site, thereby sending the cookie, while Strict \ndoesn't allow it.\nPossible bad usage of the cookie is through XSS (cross-site scripting) attacks. A \ncompromised script reads that cookie, and then forges bad requests authenticated as \nthe user.\nAnother important kind of security problem is cross-site request forgery (CSRF). \nIn this case, the fact that the user is logged in on an external service is exploited by \npresenting a URL that will be automatically executed in a different, compromised \nwebsite.\nFor example, while accessing a forum, a URL from a common bank is called, \npresented as an image, for example. If the user is logged in to this bank, the \noperation will be executed.\nThe SameSite attribute greatly reduces the risk of CSRF, but in case the attribute \nis not understood by older browsers, operations presented to the user by the bank \nshould present a random token, making the user send both the authenticated request \nwith the cookie and a valid token. An external page won't know a valid random \ntoken, making this exploit much more difficult. \nYou can obtain more information at the Mozilla SameSite Cookie \npage: https://developer.mozilla.org/en-US/docs/Web/\nHTTP/Headers/Set-Cookie/SameSite.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2462,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "API Design\n[ 46 ]\nThe session ID that the cookie contains can either be stored in the database, being just \na random unique identifier, or a rich token.\nA random identifier is just that, a random number that stores the related information \nin the database, mainly, who is accessing and when the session expires. With \nevery access, this session ID is queried to the server and the related information is \nretrieved. On very big deployments, with many accesses, this can create problems \nas it's less scalable. The database where the session ID is stored needs to be accessed \nby all workers, which can create a bottleneck.\nOne possible solution is to create a rich data token. This works by adding all the \nrequired information directly to the cookie; for example, storing the user ID, expiry, \nand so on, directly. This avoids database access, but makes the cookie possible to \nforge, as all information is in the open. To fix it, the cookie is signed.\nThe signature proves that the data was originated by a trusted login server and can \nbe verified independently by any other server. This is more scalable and avoids \nbottlenecks. Optionally, the content can also be encrypted to avoid being read.\nAnother advantage of this system is that the generation of the token can be \nindependent of the general system. If the token can be validated independently, \nthere's no need for the login server to be the same as the general server.\nEven more so, a single token signer can issue tokens for multiple services. This is the \nbasis for SSO (Single Sign-On): log in to an auth provider and then use the same \naccount in several related services. This is very common in common services such as \nGoogle, Facebook or GitHub, to avoid having to create a specific login for some web \npages.\nThat operation mode, having a token authority, is the basis of the OAuth \nauthorization framework.\nAuthenticating RESTful interfaces\nOAuth has become a common standard for authenticating access for APIs, and \nRESTful APIs in particular.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "Chapter 2\n[ 47 ]\nIt is based on the idea that there's an authorizer who can check the identity of the \nuser and provide them with a token with information allowing the user to log in. \nThe service will receive this token and will log the user:\nFigure 2.3: Authentication flow\nThe most common version at the moment is OAuth 2.0, which allows flexibility in \nterms of logging in and flow. Keep in mind that OAuth is not exactly a protocol, but \nprovides certain ideas that can be tweaked to the specific use case.\nThere's a difference between authenticating and authorizing, and \nin essence, OAuth is an authorization system. Authenticating is \ndetermining who the user is, while authorizing is what the user is \ncapable of doing. OAuth uses the concept of scope to return what \nthe capabilities of a user are.\nMost implementations of OAuth, such as OpenID Connect, \nalso include the user information in the returning token to also \nauthenticate the user, returning who the user is.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1082,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "API Design\n[ 48 ]\nThere's an important difference in terms of whether the system accessing the API \nis the final user directly, or whether it accesses it on behalf of a user. An example of \nthe latter could be a smartphone app to access a service like Twitter, or a service that \nneeds to access the data stored for the user in GitHub, such as a code analysis tool. \nThe app itself is not the one that performs the actions but transfers the actions of a \nuser.\nThis flow is called the Authorization Code grant. The main characteristic is that \nthe auth provider will present a login page to the user and redirect them with the \nauthentication token. \nFor example, this could be the sequence of calls for the Authorization Code grant:\nGET https://myservice.com/login\n    Return a page with a form to initiate the login with authorizer.com\nFollow the flow in the external authorize until login, with something \nlike.\nPOST https://authorizer.com/authorize\n  grant_type=authorization_code\n  redirect_uri=https://myservice.com/redirect\n  user=myuser\n  password=mypassword\n    Return 302 Found to https://myservice.com/redirect?code=XXXXX\nGET https://myservice.com/redirect?code=XXXXX\n-> Login into the system and set proper cookie, \n   return 302 to https://myservice.com\nIf the system accessing the API is from the end user directly, the Client Credentials \ngrant type flow can be used instead. In this case, the first call will send client_id \n(user ID) and client_secret (password) to retrieve the authentication token directly. \nThis token will be set in new calls as a header, authenticating the request. \nThis means that there are different ways in which you can \nimplement OAuth, and, crucially, that different authorizers will \nimplement it differently. Please verify their documentation with \ncare when implementing the integration. \nGenerally, authorizers use the OpenID Connect protocol, which is \nbased on OAuth.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "Chapter 2\n[ 49 ]\nNote that this skips a step, and is easier to automate:\nPOST /token HTTP/1.1\n  grant_type=authorization_code\n  &client_id=XXXX\n  &client_secret=YYYY\n    Returns a JSON body with\n    {\n  \"access_token\":\"ZZZZ\",\n  \"token_type\":\"bearer\",\n  \"expires_in\":86400,\n}\nMake new requests setting the header\nAuthorization: \"Bearer ZZZZ\"\nWhile OAuth allows you to use an external server to retrieve the access token, that's \nnot strictly required. It can be the same server as the rest. This is useful for this \nlast flow, where the ability to log in with an external provider such as Facebook or \nGoogle is not as useful. Our example system will use the Client Credentials flow.\nSelf-encoded tokens\nThe returned tokens from the authorization server can contain sufficient information \nsuch that no external check with the authorizer is required.\nTo do so, the token is typically encoded in a JSON Web Token (JWT). A JWT is a \nstandard that encodes a JSON object in a URL-safe sequence of characters.\nA JWT has the following elements:\n•\t\nA header. This contains information on how the token is encoded.\n•\t\nA payload. The body of the token. Some of the fields in this object, called \nclaims, are standard, but it can allocate custom claims as well. Standard \nclaims are not required and can describe elements such as the issuer (iss), \nor the expiration time of the token as Unix Epoch (exp).\nAs we've seen, including the user information in the token is \nimportant to determine who the user is. If not, we will end with a \nrequest that is capable of doing the work, but without information \non behalf of who.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1713,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "API Design\n[ 50 ]\n•\t\nA signature. This verifies that the token was generated by the proper source. \nThis uses different algorithms, based on the information in the header.\nIn general, a JWT is encoded, but it's not encrypted. A standard JWT library will \ndecode its parts and verify that the signature is correct.\nFor example, to generate a token using pyjwt (https://pypi.org/project/PyJWT/), \nyou'll need to install PyJWT using pip if not previously installed:\n$ pip install PyJWT\nThen, while opening a Python interpreter, to create a token with a payload with \na user ID and an HS256 algorithm to sign it with the \"secret\" secret, you use the \nfollowing code:\n>>> import jwt\n>>> token = jwt.encode({\"user_id\": \"1234\"}, \"secret\", \nalgorithm=\"HS256\")\n>>> token\n'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjoiMTIzNCJ9.\nvFn0prsLvRu00Kgy6M8s6S2Ddnuvz-FgtQ7nWz6NoC0'\nThe JWT token can then be decoded and the payload extracted. If the secret is \nincorrect, it will produce an error:\n>>> jwt.decode(token,\"secret\", algorithms=['HS256'])\n{'user_id': '1234'}\n>>> jwt.decode(token,\"badsecret\", algorithms=['HS256'])\nTraceback (most recent call last):\n …\n  jwt.exceptions.InvalidSignatureError: Signature verification failed\nYou can test the different fields and systems in the interactive tool: \nhttps://jwt.io/.\nThe algorithm to be used is stored in the headers, but it's a \ngood idea, for reasons of security, to only validate the token \nwith the expected algorithm and not rely on the header. In the \npast, there have been some security problems with certain JWT \nimplementations and forgery of the tokens, as you can read \nhere: https://www.chosenplaintext.ca/2015/03/31/jwt-\nalgorithm-confusion.html.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1807,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "Chapter 2\n[ 51 ]\nThe most interesting algorithms, though, are not symmetrical ones like HS256, \nwhere the same value is added for encoding and decoding, but public-private keys \nlike RSA-256 (RS256). This allows the token to be encoded with the private key and \nverified with the public key.\nThis schema is very common, as the public key can be distributed widely, but only \nthe proper authorizer who has the private key can be the source of the tokens.\nIncluding the payload information that can be used to identify the user allows \nauthentication of the requests using just the information in the payload, once \nverified, as we discussed earlier.\nVersioning the API\nInterfaces are rarely created fully formed from scratch. They are constantly being \ntweaked, with new features added, and bugs or inconsistencies fixed. To better \ncommunicate these changes, it's useful to create some sort of versioning to transmit \nthis information.\nWhy versioning?\nThe main advantage of versioning is to shape the conversation about what things are \nincluded when. This can be bug fixes, new features, or even newly introduced bugs.\nIf we know that the current interface released is version v1.2.3, and we are about \nto release version v1.2.4, which fixes bug X, we can talk about it more easily, as well \nas creating release notes informing users of that fact.\nInternal versus external versioning\nThere are two kinds of versions that can get a bit confused. One is the internal \nversion, which is something that makes sense for the developers of a project. This is \nnormally related to the version of the software, usually with some help from version \ncontrol, such as Git.\nThis version is very detailed and can cover very small changes, including small bug \nfixes. The aim of it is to be able to detect even minimal changes between software \nto allow the detection of bugs or the introduction of code.\nThe other is the external version. The external version is the version that people \nusing the external service are going to be able to perceive. While this can be as \ndetailed as the internal one, that is normally not that helpful to users and can \nprovide a confusing message.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "API Design\n[ 52 ]\nFor example, an internal version may distinguish between two different bug fixes, \nas this is useful to replicate. An externally communicated version can combine them \nboth in \"multiple bug fixes and improvements.\"\nAnother good example of when it's useful to make a difference is when the interface \nchanges massively. For example, a brand-new revamp of the look and feel of a site \ncould use \"Version 2 interface,\" but this can happen over multiple internal new \nversions, to be tested internally or by a selected group (for example, beta testers). \nFinally, when the \"Version 2 interface\" is ready, it can be activated for all users.\nOne way of describing the external version could be to call it a \"marketing version.\"\nThis version will be more dependent on marketing efforts than technical \nimplementation.\nSemantic versioning\nA common pattern for defining versions is to use semantic versioning. Semantic \nversioning describes a method with three increasing integers that carry different \nmeanings, in descending order of incompatibility: \nvX.Y.Z\nX is called the major version. Any change in the major version will mean backward-\nincompatible changes.\nY is the minor version. Minor changes may add new features, but any change will be \nbackward compatible.\nZ is the patch version. It will only make small changes such as bug fixes and security \npatches, but it doesn't change the interface itself.\nThis largely depends on the kind of system and who their expected \nusers are. A highly technical user will appreciate the extra details, \nbut a more casual one will not.\nNote that here we are avoiding the term \"release version\" as it \ncould be misleading. This version is only used to communicate \ninformation externally.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "Chapter 2\n[ 53 ]\nThis means that software designed to work with v1.2.15 will work with versions \nv1.2.35 and v1.3.5, but it won't work with version v2.1.3 or version v1.1.4. It may \nwork with version v1.2.14, but it may have some bug that was corrected later.\nSometimes, extra details can be added to describe interfaces that are not ready, for \nexample, v1.2.3-rc1 (release candidate) or v1.2.3-dev0 (development version).\nThis semantic versioning is very easy to understand and gives good information \nabout changes. It is widely used, but it has some problems in certain cases:\n•\t\nStrictly adopting the major version for systems that don't have clear \nbackward compatibility can be difficult. This was the reason why the Linux \nkernel stopped using proper semantic versioning, because they will never \nupdate the major version, as every single release needed to be backward \ncompatible. In that case, a major version can be frozen for years and years \nand stops being a useful reference. In the Linux kernel, that happened with \nversion 2.6.X, which remained for 8 years until version 3.0 was released in \n2011 without any backward-incompatible change.\n•\t\nSemantic versioning requires a pretty strict definition of the interface. If the \ninterface changes often with new features, as happens typically with online \nservices, the minor version increases quickly, and the patch version is of \nalmost no use.\nFor online services, the combination of both will make only a single number useful, \nwhich is not a great use of it. Semantic versioning works better for cases that require \nmultiple API versions working at the same time, for example:\n•\t\nThe API is very stable and changes very rarely, though there are regular \nsecurity updates. Every couple of years, there's a major update. A good \nexample is databases, such as MySQL. Operative systems are another \nexample.\nThe v at the start is optional but helps to indicate that it's a version \nnumber.\nNormally, before the software is ready for release, the major \nnumber is set to zero (for example, v0.1.3), making version \nv1.0.0 the first one to be publicly available.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "API Design\n[ 54 ]\n•\t\nThe API belongs to a software library that can be used by multiple supported \nenvironments; for example, a Python library compatible with Python 2 for \nversion v4 and Python 3 for v5. This can allow several versions to be kept \nalive if required.\nIf the system effectively has a single version running at the same time, it is better to \nnot add the extra effort to keep proper semantic versioning in place as the effort is \nnot worth the reward in terms of the kind of investment required.\nSimple versioning\nInstead of doing strict semantic versioning, a simplified version can be done instead. \nThis won't carry the same kind of meaning, but it will be a constantly increasing \ncounter. This will work to coordinate teams, although it won't require the same kind \nof commitment.\nThis is the same idea as the build number that can be created automatically by \ncompilers, an increasing number to distinguish one version from another and work \nas a reference. However, a plain build number can be a bit dry to use.\nIt is better to use a similar structure to semantic versioning, as it will be \nunderstandable by everyone; but instead of using it with specific rules, it is looser \nthan that: \n•\t\nNormally, for a new version, increase the patch version.\n•\t\nIf either the patch version gets too high (in other words, 100, 10, or another \narbitrary number), increase the minor version and set the patch version to \nzero.\n•\t\nAlternatively, if there's any special milestone for the project, as defined by \nthe people working on it, increase the minor number earlier.\n•\t\nDo the same with the major version number.\nThis will allow the numbers to be increased in a consistent way without worrying \ntoo much about meaning.\nThis structure works very well for things like online cloud services, which, in \nessence, require an increasing counter, as they have a single version deployed at the \nsame time. In this case, the most important use of the version is internal usage and \nwon't require the maintenance that strict semantic versioning requires.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "Chapter 2\n[ 55 ]\nFrontend and backend\nThe usual way of dividing different services is by talking about the \"frontend\" and \nthe \"backend.\" They describe the layers of software, where the layer closer to the end \nuser is the frontend, and the one behind is the backend. \nTraditionally, the frontend is the layer that takes care of the presentation layer, next \nto the user, and the backend is the data access layer, which serves the business logic. \nIn a client-server architecture, the client is the frontend and the server is the backend:\nFigure 2.4: Client-Server architecture\nAs architectures grow more complex, these terms become somewhat polysemic, \nand they are usually understood depending on the context. While frontend is \nalmost always understood as the user interface directly, backend can be applied to \nmultiple layers, meaning the next layer that gives support to whatever system is \nbeing discussed. For example, in a cloud application, the web application may use a \ndatabase such as MySQL as the storage backend, or in-memory storage such as Redis \nas the cache backend.\nThe general approach for the frontend and backend is quite different. \nThe frontend focuses on the user experience, so the most important elements are \nusability, pleasing design, responsiveness, and so on. A lot of that requires an eye for \nthe \"final look\" and how to make things easy to use. Frontend code is executed in the \nfinal user, so compatibility between different types of hardware can be important. \nAt the same time, it distributes the load, so performance is most important from the \npoint of view of the user interface.\nThe backend focuses more on stability. Here, the hardware is under strict control, \nbut the load is not distributed, making performance important in terms of controlling \nthe total resources used. Modifying the backend is also easier, as changing it once \nchanges it for all the users at the same time. But it's riskier, as a problem here may \naffect every single user. This environment primes more to focus on solid engineering \npractices and replicability.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "API Design\n[ 56 ]\nIn general, some common technologies used for the frontend are as follows:\n•\t\nHTML and associated technologies such as CSS \n•\t\nJavaScript and libraries or frameworks to add interactivity, such as jQuery or \nReact\n•\t\nDesign tools\nBackend technologies, as they are under more direct control, can be more varied, for \nexample:\n•\t\nMultiple programming languages, either scripting languages such as Python, \nPHP, Ruby, or even JavaScript using Node.js, or compiled languages such as \nJava or C#. They can even be mixed, making different elements in different \nlanguages.\n•\t\nDatabases, either relational databases such as MySQL or PostgreSQL, or non-\nrelational ones such as MongoDB, Riak, or Cassandra.\n•\t\nWeb servers, such as Nginx or Apache.\n•\t\nScalability and high-availability tools, such as load balancers.\n•\t\nInfrastructure and cloud technologies, such as AWS services.\n•\t\nContainer-related tech, like Docker or Kubernetes.\nThe frontend will make use of interfaces defined by the backend to present the \nactions in a user-friendly way. There can be several frontends for the same backend, \na typical example being multiple smartphone interfaces for different platforms, but \nthat use the same API to communicate with the backend.\nThe term full stack engineer is commonly used to describe \nsomeone who is comfortable doing both kinds of work. While \nthis can work in certain aspects, it's actually quite difficult to find \nsomeone who is equally comfortable or who is inclined to work on \nboth elements in the longer term. \nMost engineers will naturally tend toward one of the sides, and \nmost companies will have different teams working on both \naspects. In a certain way, the personality traits for each work are \ndifferent, with frontend work requiring more of an eye for design, \nand backend users being comfortable with stability and reliability \npractices.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1982,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Chapter 2\n[ 57 ]\nKeep in mind that frontend and backend are conceptual divisions, but they don't \nnecessarily need to be divided into different processes or repositories. A common \ncase where the frontend and backend live together are web frameworks such as \nRuby on Rails or Django, where you can define the frontend HTML interface at the \nsame time as the backend controllers that handle the data access and business logic. \nIn this case, the HTML code is served directly from the same process that performs \naccess to the data. This process separates the concerns using the Model View \nController structure.\nModel View Controller structure\nThe Model View Controller, or MVC, is a design that separates the logic of a \nprogram into three distinct components.\n•\t\nThis structure is really successful as it creates a clear separation of concepts:\n•\t\nThe Model manages the data\n•\t\nThe Controller accepts input from the user and transforms it into the \nmanipulation of the model\n•\t\nThe View represents the information for the user to understand\nIn essence, the Model is the core of the system, as it deals with the manipulation of \nthe data. The Controller represents the input, and the View represents the output of \nthe operations.\nFigure 2.5: The Model View Controller pattern\nThe Model View Controller pattern started very early in the design \nof graphic user interfaces and has been used in that area since \nthe first full graphic interactive interfaces in the 80s. In the 90s, it \nstarted being introduced as a way of handling web applications.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1649,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "API Design\n[ 58 ]\nThe MVC structure can be considered at different levels, and it can be regarded as \nfractal. If several elements interact, they can have their own MVC structure, and the \nmodel part of a system can talk to a backend that provides information.\nThe Model is arguably the most important element of the three as it's the core part \nof it. It contains the data access, but also the business logic. A rich Model component \nworks as a way of abstracting the logic of the application from the input and output.\nCommonly, some of the barriers between controllers get a bit blurry. Different inputs \nmay be dealt with in the Controller, producing different calls to the Model. At the \nsame time, the output can be tweaked in the Controller before being passed to the \nview. While it's always difficult to enforce clear, strict boundaries, it's good to keep \nin mind what the main objective of each component is so as to provide clarity.\nHTML interfaces\nWhile the strict definition of APIs works for interfaces that are designed to be \naccessed by other programs, it's good to spend a bit of time talking about the basics \nof how to create a successful human interface. For this purpose, we will talk mainly \nabout HTML interfaces, aimed at being used by the end user in a browser.\nHTML technologies are highly related to RESTful ones because they were developed \nin parallel during the early days of the internet. Typically, they are presented \nintertwined in modern web applications.\nTraditional HTML interfaces\nThe way traditional web interfaces work is through HTTP requests, only using the \nGET and POST methods. GET retrieves a page from the server, while POST is paired \nwith some form that submits data to the server.\nThe MVC pattern can be implemented in different ways. For \nexample, Django claims it is a Model View Template, as the \ncontroller is more the framework itself. However, these are minor \ndetails that don't contradict the general design.\nMost of the concepts that we will deal with apply to other kinds of \nhuman interfaces, such as GUIs or mobile applications.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "Chapter 2\n[ 59 ]\nWhile this is certainly more restrictive than all the available options, it can work well \nfor simple website interfaces.\nFor example, a blog is read way more often than is written, so readers make use of \na lot of GET requests to get the information, and perhaps some POST requests to send \nback some comments. The need to remove or change a comment was traditionally \nsmall, although it can be allocated with other URLs where POST is used.\nAn HTML interface doesn't work in the same way as a RESTful interface because of \nthese limitations, but it can also improve with a design that takes the abstractions \nand resources approach in mind.\nFor example, some common abstractions for a blog are as follows:\n•\t\nEach post, with associated comments\n•\t\nA main page with the latest posts\n•\t\nA search page that can return posts that contain a certain word or tag\nThis is very similar to the interface in resources, where only the two resources of \n\"comment\" and \"post,\", which will be separated in a RESTful way, will be joined in \nthe same concept.\nThe main limitation of traditional HTML interfaces is that every change needs to \nrefresh the whole page. For simple applications like a blog, this can work quite well, \nbut more complex applications may require a more dynamic approach.\nDynamic pages\nTo add interactivity to the browser, we can add some JavaScript code that will \nperform actions to change the page directly on the browser representation; for \nexample, selecting the color of the interface from a drop-down selector.\nThis was a prerequisite, as browsers only implemented these \nmethods. While, nowadays, most modern browsers can use all \nHTTP methods in requests, it's still a common requirement to \nallow compatibility with older browsers.\nNote that browsers will ask you before retrying a POST request as \nthey are not idempotent.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1962,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "API Design\n[ 60 ]\nFrom JavaScript, independent HTTP requests can also be done, so we can use that to \nmake specific calls to retrieve details that can be added to improve the experience of \nthe user.\nFor example, for a form to input an address, a dropdown may select the country. \nOnce selected, a call to the server will retrieve the proper regions to incorporate the \ninput. If the user selects United States, the list of all states will be retrieved and be \navailable in the next dropdown. If the user selects Canada, the list of territories and \nprovinces will be used instead:\nFigure 2.5: Improving user experience with appropriate dropdowns\nAnother example, that reverses the interface somewhat, could be to use the ZIP code \nto determine the state automatically.\nThis is called manipulating the Document Object Model (DOM), \nwhich contains the representation of the document as defined \nby the HTML and possibly the CSS. JavaScript can access this \nrepresentation and change it by editing any parameters or even \nadding or removing elements.\nThere is actually a service to retrieve this information called \nhttps://zippopotam.us/. It can be called and returns not only \nthe state but further information, in JSON format.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1329,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "Chapter 2\n[ 61 ]\nThese kinds of calls are called Asynchronous JavaScript And XML (AJAX). \nAlthough the name mentions XML, it's not required, and any format can be \nretrieved. At the moment, it is very common to use JSON or even plain text. One \npossibility is to use HTML, so an area of the page can be replaced with snippets that \ncome from the server: \nFigure 2.6: Using HTML to replace areas of the page\nRaw HTML, although somewhat inelegant, can be effective, so it's very common \nto use a RESTful API returning JSON to retrieve the expected data for these small \nelements and then modify the DOM with it through JavaScript code. Given that \nthe objective of this API is not to replace the HTML interface in its entirety, but \ncomplement it, this RESTful API will likely be incomplete. It won't be possible to \ncreate a full experience using only these RESTful calls.\nOther applications go directly to the point of creating an API-first approach and \ncreate the browser experience from there.\nSingle-page apps\nThe idea behind a single-page app is easy. Let's open a single HTML page and \nchange its content dynamically. If there's any new data to be required, it will be \naccessed through a specific (typically RESTful) API.\nThis completely detaches the human interface, understood as the elements that have \nthe responsibility of displaying the information to a human, from the service. The \nservice serves a RESTful API exclusively, without worrying about the representation \nof the data.\nThis kind of approach is sometimes called API-first as it designs a \nsystem from the API to the representation, instead of creating it the \nother way around, which is the natural way in which it is created \nin an organic service.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "API Design\n[ 62 ]\nAlthough there are specific frameworks and tools designed with this objective in \nmind, such as React or AngularJS, there are two main challenges with this kind of \napproach:\n•\t\nThe technical skill required to create a successful human interface on a \nsingle page is quite high, even with the help of tools. Any non-trivial \nrepresentation of a valid interface will require keeping a lot of state and \ndealing with multiple calls. This is prone to have errors that compromise the \nstability of the page. The traditional approach for browser pages works with \nindependent pages that limit the scope of each step, which is easier to handle.\n•\t\nThe need to design and prepare the API beforehand can result in a slow \nstart for the project. It requires more planification and upfront commitment, \neven if both sides are developed in parallel, which also has its challenges.\nThese issues ensure that this approach is not usually done for new applications \nstarting from scratch. However, if the application started with another kind of user \ninterface, like a smartphone application, it could leverage the already existing REST \nAPI to generate an HTML interface that replicates the functionality.\nThe main advantage of this approach is detaching the application from the user \ninterface. Where an application starts its development as a small project with a \nregular HTML interface, the risk is that any other user interface will tend to conform \nto the HTML interface. This can quickly add up to a lot of technical debt and \ncompromise the design of the API, as the abstractions that are used will likely be \nderived from the existing interface, instead of the most adequate ones.\nA whole API-first approach greatly separates the interface, so creating a new \ninterface is as easy to use as the already existing API. For applications that require \nmultiple interfaces, such as an HTML interface, but also different smartphones \napplications for iOS and Android, that could be a good solution.\nA single-page application can also be quite innovative in terms of presenting a full \ninterface. This can create rich and complex interfaces that deviate from what could be \nunderstood as a \"web page,\" as in the case of a game or an interactive application. \nKeep in mind that there are interface expectations carried \nby the browser that can be difficult to avoid or replace, for \nexample, hitting the back button.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "Chapter 2\n[ 63 ]\nHybrid approach\nGoing all-in with a single-page application, as we've seen, can be quite challenging. \nTo a certain degree, it is using a browser to overwrite its usage. \nThat's why normally the design doesn't go that far and creates a more traditional \nweb interface. This interface is still recognizable as a web application but relies \nheavily on JavaScript to obtain information using a RESTful interface. This can \nhappen as a natural step to migrating from a traditional HTML interface to a single-\npage app, but it may also be a conscious decision.\nThis approach combines the previous two. On the one hand, it still requires an \nHTML interface for the general approach of the interface, with clear pages to \nnavigate. On the other, it creates a RESTful API that fills most of the information and \nuses JavaScript to make use of this API.\nIn practice, this tends to create a less complete RESTful API, as some of the elements \nmay be added directly to the HTML part of it. But, at the same time, it allows the \niterative migration of elements into the API, starting with certain elements, but \nadding more as time goes by. This stage is very flexible.\nDesigning the API for the example\nAs we described in the first chapter, General Overview of the Example, we need to set \nthe definition for the different interfaces that we will be working on in the example. \nRemember that the example is a microblogging application that will allow users to \nwrite their own text microposts so that they are available for others to read.\nThere are two main interfaces in the example:\n•\t\nAn HTML interface for allowing users to interact with the service using a \nbrowser\n•\t\nA RESTful interface for allowing the creation of other clients like a \nsmartphone app\nThis approach is similar to the dynamic page one, but there is an \nimportant difference, which is the intention to create a coherent \nAPI that can be used without being totally tailored to the HTML \ninterface. That changes the approach significantly.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2119,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "API Design\n[ 64 ]\nIn this chapter, we will describe the design of the second interface. We will start with \na description of the different basic definitions and resources that we will use:\n•\t\nUser: A representation of the user of the application. It will be defined by a \nusername and a password to be able to login. \n•\t\nMicropost: A small text of up to 255 characters posted by a User. A Micropost \ncan be optionally addressed to a User. It has also the time it was created.\n•\t\nCollection: The display of Microposts from a User.\n•\t\nFollower: A User can follow another User.\n•\t\nTimeline: An ordered list of the Microposts by the followed Users.\n•\t\nSearch: Allow a search by User or by text contained in Microposts.\nWe can define these elements as resources in a RESTful way, in the way introduced \nearlier in the chapter, first as a quick description of the URIs:\nPOST   /api/token\nDELETE /api/token\nGET    /api/user/<username>\nGET    /api/user/<username>/collection\nPOST   /api/user/<username>/collection\nGET    /api/user/<username>/collection/<micropost_id>\nPUT    /api/user/<username>/collection/<micropost_id>\nPATCH  /api/user/<username>/collection/<micropost_id>\nDELETE /api/user/<username>/collection/<micropost_id>\nGET    /api/user/<username>/timeline\nGET    /api/user/<username>/following\nPOST   /api/user/<username>/following\nDELETE /api/user/<username>/following/<username>\nGET    /api/user/<username>/followers\nGET    /api/search\nOnce this brief design is complete, we can flesh out the definition of each endpoint.\nNote that we added POST and DELETE resources for /token to deal \nwith login and logout.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "Chapter 2\n[ 65 ]\nEndpoints\nWe will describe all the API endpoints in a bit more detail, following the template \nintroduced previously in this chapter.\nLogin:\n•\t\nDescription: Using the proper authentication credentials, return a valid access \ntoken. The token needs to be included in the requests as the Authorization \nheader.\n•\t\nResource URI: /api/token\n•\t\nMethod: POST\n•\t\nRequest body:\n{  \n \"grant_type\": \"authorization_code\"\n    \"client_id\": <client id>,\n    \"client_secret\": <client secret>\n}\n•\t\nReturn body:\n{\n  \"access_token\": <access token>,\n  \"token_type\":\"bearer\",\n  \"expires_in\":86400,\n}\n•\t\nErrors:\n400 Bad Request Incorrect body.\n400 Bad Request Bad credentials.\nLogout:\n•\t\nDescription: Invalidate the bearer token. If successful, it will return a 204 No \nContent error.\n•\t\nResource URI: /api/token\n•\t\nMethod: DELETE\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nErrors: \n401 Unauthorized Trying to access this URI without being \nproperly authenticated.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1066,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "API Design\n[ 66 ]\nRetrieve user:\n•\t\nDescription: Returns the username resource.\n•\t\nResource URI: /api/users/<username>\n•\t\nMethod: GET\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nQuery Parameters:\nsize Page size.\npage Page number.\n•\t\nReturn body:\n{\n    \"username\": <username>,\n    \"collection\": /users/<username>/collection,\n}\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n404 Not Found Username does not exist.\nRetrieve user's collection:\n•\t\nDescription: Returns the collection of all microposts from a user, in paginated \nform.\n•\t\nResource URI: /api/users/<username>/collection\n•\t\nMethod: GET\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nReturn body:\n{\n    \"next\": <next page or null>,\n    \"previous\": <previous page or null>,\n    \"result\": [\n        {\n            \"id\": <micropost id>,\n            \"href\": <micropost url>,\n            \"user\": <user url>,\n            \"text\": <Micropost text>,\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "Chapter 2\n[ 67 ]\n            \"timestamp\": <timestamp for micropost in ISO 8601>\n        },\n        ...\n    ]\n}\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n404 Not Found Username does not exist.\nCreate new micropost:\n•\t\nDescription: Create a new micropost. \n•\t\nResource URI: /api/users/<username>/collection\n•\t\nMethod: POST\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nRequest body:\n{\n    \"text\": <Micropost text>,\n    \"referenced\": <optional username of referenced user>\n}\n•\t\nErrors:\n400 Bad Request Incorrect body.\n400 Bad Request Invalid text (for example, more than 255 \ncharacters).\n400 Bad Request Referenced user not found.\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n403 Forbidden Trying to create a micropost of a different user \nto the one logged in.\nRetrieve micropost:\n•\t\nDescription: Returns a single micropost.\n•\t\nResource URI: /api/users/<username>/collection/<micropost_id>\n•\t\nMethod: GET\n•\t\nHeaders: Authentication: Bearer: <token>\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1120,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "API Design\n[ 68 ]\n•\t\nReturn body:\n        {\n            \"id\": <micropost id>,\n            \"href\": <micropost url>,\n            \"user\": <user url>,\n            \"text\": <Micropost text>,\n            \"timestamp\": <timestamp for micropost in ISO 8601>,\n      \"referenced\": <optional username of referenced user>\n        }\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n404 Not Found Username does not exist.\n404 Not Found Micropost ID does not exist.\nUpdate micropost:\n•\t\nDescription: Update the text for a micropost.\n•\t\nResource URI: /api/users/<username>/collection/<micropost_id>\n•\t\nMethod: PUT, PATCH\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nRequest body:\n        {\n            \"text\": <Micropost text>,\n      \"referenced\": <optional username of referenced user>\n        }\n•\t\nErrors:\n400 Bad Request Incorrect body.\n400 Bad Request Invalid text (for example, more than 255 \ncharacters).\n400 Bad Request Referenced user not found.\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n403 Forbidden Trying to update a micropost of a different user \nto the one logged in.\n404 Not Found Username does not exist.\n404 Not Found Micropost ID does not exist.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1315,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "Chapter 2\n[ 69 ]\nDelete micropost:\n•\t\nDescription: Delete a micropost. If successful, it will return a 204 No Content \nerror.\n•\t\nResource URI: /api/users/<username>/collection/<micropost_id>\n•\t\nMethod: DELETE\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n403 Forbidden Trying to delete a micropost of a different user \nto the one logged in.\n404 Not Found Username does not exist.\n404 Not Found Micropost ID does not exist.\nRetrieve user's timeline:\n•\t\nDescription: Returns the collection of all microposts from the timeline of a \nuser, in paginated form. The microposts will be returned by timestamp order, \nwith the oldest being returned first.\n•\t\nResource URI: /api/users/<username>/timeline\n•\t\nMethod: GET\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nReturn body:\n{\n    \"next\": <next page or null>,\n    \"previous\": <previous page or null>,\n    \"result\": [\n        {\n            \"id\": <micropost id>,\n            \"href\": <micropost url>,\n            \"user\": <user url>,\n            \"text\": <Micropost text>,\n            \"timestamp\": <timestamp for micropost in ISO 8601>,\n            \"referenced\": <optional username of referenced user>\n        },\n        ...\n    ]\n}\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1355,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "API Design\n[ 70 ]\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n404 Not Found Username does not exist.\nRetrieve the users a user is following:\n•\t\nDescription: Returns a collection of all users that the selected user is following.\n•\t\nResource URI: /api/users/<username>/following\n•\t\nMethod: GET\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nReturn body:\n{\n    \"next\": <next page or null>,\n    \"previous\": <previous page or null>,\n    \"result\": [\n        {\n            \"username\": <username>,\n            \"collection\": /users/<username>/collection,\n        },\n        ...\n    ]\n}\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n404 Not Found Username does not exist.\nFollow a user:\n•\t\nDescription: Causes the selected user to follow a different user.\n•\t\nResource URI: /api/users/<username>/following\n•\t\nMethod: POST\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nRequest body:\n{    \n    \"username\": <username>\n}\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1086,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "Chapter 2\n[ 71 ]\n•\t\nErrors:\n400 Bad Request The username to follow is incorrect or does not \nexist.\n400 Bad Request Bad body.\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n404 Not Found Username does not exist.\nStop following a user:\n•\t\nDescription: Stops following a user. If successful, it will return a 204 No \nContent error.\n•\t\nResource URI: /api/users/<username>/following/<username>\n•\t\nMethod: DELETE\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n403 Forbidden Trying to stop following a user who is not the \nauthenticated one.\n404 Not Found Username to stop following does not exist.\nRetrieve a user's followers:\n•\t\nDescription: Returns, in paginated form, all followers of this user.\n•\t\nResource URI: /api/users/<username>/followers\n•\t\nMethod: GET\n•\t\nHeaders:Authentication: Bearer: <token>\n•\t\nReturn body:\n{\n    \"next\": <next page or null>,\n    \"previous\": <previous page or null>,\n    \"result\": [\n        {\n            \"username\": <username>,\n            \"collection\": /users/<username>/collection,\n        },\n        ...\n    ]\n}\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "API Design\n[ 72 ]\n•\t\nErrors:\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n404 Not Found Username does not exist.\nSearch microposts:\n•\t\nDescription: Returns, in paginated form, microposts that fulfill the search \nquery.\n•\t\nResource URI: /api/search\n•\t\nMethod: GET\n•\t\nHeaders: Authentication: Bearer: <token>\n•\t\nQuery parameters:\nusername: Optional username to search. Partial matches will be \nreturned.\ntext: Mandatory text to search, with a minimum of three \ncharacters. Partial matches will be returned.\n•\t\nReturn body:\n{\n    \"next\": <next page or null>,\n    \"previous\": <previous page or null>,\n    \"result\": [\n        {\n            \"id\": <micropost id>,\n            \"href\": <micropost url>,\n            \"user\": <user url>,\n            \"text\": <Micropost text>,\n            \"timestamp\": <timestamp for micropost in ISO 8601>,\n            \"referenced\": <optional username of referenced user>\n        },\n    ]\n}\n•\t\nErrors:\n400 Bad Request No mandatory query parameters.\n400 Bad Request Incorrect value in query parameters.\n401 Unauthorized Trying to access this URI without being \nauthenticated.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1225,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "Chapter 2\n[ 73 ]\nReview of the design and implementation\nThis two-step approach of presenting and designing a new API enables you to \nquickly see whether something is out of place as regards the design. Then, it can be \niterated over until fixed. The next step is to start with the implementation, as we will \nsee in forthcoming chapters.\nSummary\nIn this chapter, we described how the basics of API design are to create a set of \nuseful abstractions that allow users to perform actions without having to care about \nthe internal details. This led to describing how to define an API with resources and \nactions.\nThis definition of an API has evolved to cover RESTful interfaces that follow certain \nproperties that make them very interesting for web server design. We described a \nbunch of useful standards and techniques when designing RESTful interfaces to \ncreate consistent and complete interfaces, including the OpenAPI tools. We went \nthrough authentication details as it's a very important element for APIs.\nWe covered the ideas behind versioning and how to create a proper versioning \nschema that's tailored to the specific use case for the API. We also covered the \ndifferences between the frontend and the backend and how this can be generalized. \nWe also covered the MVC pattern, which is a very common way to structure \nsoftware.\nWe described the different options for HTML interfaces to provide a complete \noverview of the different interfaces in web services. We covered different options in \nterms of how an HTML service can be constructed and interact with other APIs.\nFinally, we presented the design for the RESTful interface for the example, while \nreviewing the general design and endpoints. \nAnother critical element of design is the data structure. We will cover this next.\nRemember that extra care should be advised when securing APIs \nthat have external usage. We went through some general ideas \nand common strategies, but note that this book does not focus \non security. This is a critical aspect of the design of any API and \nshould be done carefully.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "API Design\n[ 74 ]\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "[ 75 ]\n3\nData Modeling\nThe core of any application is its data. At the very root of any computer application, \nit's a system designed to deal with information, receiving it, transforming it, and \nreturning either the same information or insightful elements extracted from it. The \nstored data is a crucial part of this cycle, as it allows you to use information that has \nbeen communicated before.\nIn this chapter, we will talk about how we can model the stored data from our \napplication and what the different options are to store and structure the data to be \npersisted.\nWe will start by describing the different database options that are available, which \nare critical to understanding their different applications, but we will mostly focus, \nduring the chapter, on relational databases, as they are the most common type. We \nwill describe the concept of a transaction to ensure that different changes are applied \nin one go.\nWe will describe different ways that we can increase the scope of a relational \ndatabase by using multiple servers, and what the use cases for each option are.\nAfter that, we will describe different alternatives when designing a schema to ensure \nthat our data is structured in the best possible way. We will discuss how to enable \nfast access to data through the usage of indices. \nIn this chapter, we'll cover the following topics:\n•\t\nTypes of databases\n•\t\nDatabase transactions\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "Data Modeling\n[ 76 ]\n•\t\nDistributed relational databases\n•\t\nSchema design\n•\t\nData indexing\nLet's start with an introduction to the different databases out there.\nTypes of databases\nAll the persistent data from an application should live in a database. As we've \ndiscussed, data is the most critical aspect of any application, and proper handling of \nit is critical to ensure the viability of the project.\nDatabases have been a critical tool for most of the time software systems have \nbeen available. They create an abstraction layer that allows accessing data without \nhaving to worry too much about how the data is structured by the hardware. Most \ndatabases allow the structure of the data to be defined without having to worry \nabout how that's implemented behind the curtains.\nDBMSes are among the most invested and mature projects in software. Each DBMS \nhas its own quirks, to the point where there's a specific job role for a \"database \nexpert\": the Database Administrator (DBA).\nTechnically, databases are collections of data themselves and \nare handled by the database management system (DBMS), the \nsoftware that allows the input and output of data. Normally, \nthe word \"database\" is used for both the collection and the \nmanagement system, depending on the context. Most DBMSes will \nallow access to multiple databases of the same kind, without being \nable to cross data between them, to allow logical separation of the \ndata.\nAs we saw in Chapter 2, API Design, this abstraction is not perfect \nand sometimes we will have to understand the internals of \ndatabases to improve the performance or do things in \"the proper \nway.\"\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "Chapter 3\n[ 77 ]\nPerformance improvements in hardware and software and external tools to handle \ndatabase complexity have made this role less common, though it's still in use by \nsome organizations. To a certain degree, the architect role overtakes parts of this role, \nthough with more of a supervising role and less of a gatekeeping one.\nThere are multiple DBMSes available on the market, with a good selection of open \nsource software that covers most use cases. Roughly speaking, we can divide the \nexisting DBMS alternatives into this non-exhaustive classification:\n•\t\nRelational databases: The default standard in databases. Use SQL query \nlanguage and have a defined schema. Examples are open source projects like \nMySQL or PostgreSQL, or commercial ones like Oracle or MS SQL Server.\n•\t\nNon-relational databases: New alternatives to the traditional databases. This \nis a diverse group with multiple alternatives, and includes very different \noptions like MongoDB, Riak, or Cassandra.\n•\t\nSmall databases: These databases are aimed to be embedded into the system. \nThe most famous example is SQLite.\nLet's take a more in-depth look at them.\nRelational databases\nThese are the most common databases and the first idea that comes to mind when \ntalking about databases. The relational model for databases was developed in the \n1970s, and it's based on creating a series of tables that can be related to each other. \nSince the 1980s, they have become incredibly popular.\nEach defined table has a number of fields or columns that are fixed and data is \ndescribed as records or rows. Tables are theoretically infinite, so more and more \nrows can be added. One of the columns is defined as the primary key and uniquely \ndescribes the row. Therefore, it needs to be unique.\nThe DBA role was quite popular for a long time and required \nhighly specialized engineers, to the point of DBAs specializing \nin a single specific DBMS. The DBA will act as the expert in the \ndatabase, both in knowing how to access it and ensuring that any \nchanges done to it work adequately. They normally are the only \nones allowed to perform changes or maintenance tasks in the \ndatabase.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2266,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "Data Modeling\n[ 78 ]\nThe primary key is used to reference that record, when necessary, in other tables. \nThis creates the relation aspect of the database. When a column in a table makes \nreference to another table, this is called a foreign key.\nThese references can produce one-to-one relationships; one-to-many, when a single \nrow can be referenced in multiple rows in another table; or even many-to-many, \nwhich requires an intermediary table to cross over the data.\nAll this information needs to be described in the schema. The schema describes each \ntable, what the fields and types of each are, as well as the relations between them.\nIt's important to note that defining the schema requires thinking ahead and being \naware of the changes that can be made. Defining types before having data also \nrequires keeping in mind possible improvements. While the schema can be changed, \nit's always a serious operation that, if not taken with proper care, can lead to the \ndatabase not being available for some time, or, in the worst-case scenario, data can \nbe changed or processed inconsistently. \nA query can also be executed that searches for data fulfilling certain conditions. \nFor that, tables can be joined based on their relationships.\nVirtually all relational databases are interacted with using Structured Query \nLanguage, or SQL. This language has become the standard to work with relational \ndatabases and follow the same concepts that we've described here. It describes both \nhow to query the database and how to add or change data contained there. \nThe most relevant characteristic of SQL is that it is a declarative language. This \nmeans that the statements describe the result instead of the procedure to obtain \nit, as is typical with imperative languages. This abstracts the internal details away \nfrom the how to focus on the what.\nIf there is a value that's unique and descriptive enough, it can \nbe used for the primary key; this is called a natural key. Natural \nkeys can also be a combination of fields, though that limits their \nconvenience. When a natural key is not available, an increasing \ncounter can be handled directly by the database to ensure it is \nunique per row. This is called a surrogate key.\nRelations in relational databases are really constraints. That means \nthat a value can't be deleted if it's still referenced somewhere. \nRelational databases come from a strict mathematical background, \nthough that background's implemented in different degrees of \nstrictness.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2606,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "Chapter 3\n[ 79 ]\nThis characteristic makes SQL portable between systems, as the internals of the \nhow can be different in different databases. Using a specific relational database and \nadapting to another is relatively easy.\nWhile relational databases are very mature and flexible and are used in very \ndifferent scenarios, there are two main problems that are difficult to deal with. One \nis requiring a predefined schema, as we said above. The other, and more serious \nafter a certain size, is dealing with scale. Relational databases are thought to be a \ncentral access point that's accessed, and there need to be some techniques to scale \nonce the limit of vertical scaling has been reached.\nWe will talk about specific techniques to deal with this issue and increase the \nscalability of relational databases later in this chapter.\nNon-relational databases\nNon-relational databases are a diverse group of DBMSes that do not fit into the \nrelational paradigm. \nImperative languages describe the control flow and are the most \ncommon languages. Examples of imperative languages are Python, \nJavaScript, C, and Java. Declarative languages are normally \nrestricted to specific domains (Domain-Specific Languages, or \nDSLs) that allow you to describe the result in simpler terms, while \nimperative languages are more flexible.\nThis is used sometimes to set up a local database for running tests \nthat's different from the final database that will be in place once the \nsystem is in production. This is possible in some web frameworks, \nbut it requires some caveats, as complex systems sometimes have \nto use specific characteristics for a particular database, making it \nimpossible to perform an easy replacement of this kind.\nNon-relational databases are also called NoSQL, emphasizing \nthe relational nature of the SQL language, standing for either \n\"not SQL\" or \"Not Only SQL,\" to be more reflective of adding \npossibilities and not removing them.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "Data Modeling\n[ 80 ]\nWhile there have been non-relational databases even before the introduction \nof relational databases and alongside them, since the 2000s, there has been an \nintroduction or recovery of methods and designs that look to alternative options. \nMost of them aim to address the two main weak spots in relational databases, \ntheir strictness and scalability issues.\nThey are very varied and have very different structures, but the most common \nkinds of non-relational systems are the following groups:\n•\t\nKey-value stores\n•\t\nDocument stores \n•\t\nWide-column databases\n•\t\nGraph databases\nLet's describe each of them.\nKey-value stores\nKey-value stores are arguably the simplest of all databases in terms of functionality. \nThey define a single key that stores a value. The value is totally opaque to the system, \nnot being able to be queried in any way. There's even, in some implementations, \nno way of querying keys in the system; instead, they need to be an input to any \noperation.\nThis is very similar to a hash table or dictionary but on a bigger scale. Cache systems \nare normally based on this kind of data store.\nWhile the technology is similar, there's an important differentiation \nbetween a cache and a database. A cache is a system that stores \ndata already calculated to speed up its retrieval, while a database \nstores raw data. If the data is not in the cache, it can be retrieved \nfrom a different system, but if it's not in the database, either the \ndata is not stored or there has been a big problem. \nThat's why cache systems tend to store information only in \nmemory and are more resilient to restarts or problems, making \nthem easier to deal with. If a cache is missing, the system works, \njust slower.\nIt's very important that information is not ultimately stored in \ncache systems that are not backed up by proper storage. It's a \nmistake that sometimes happens inadvertently, for example, with \ntemporal data, but the risk is to get a problem at the wrong time \nand lose the data, so be aware of it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2137,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "Chapter 3\n[ 81 ]\nThe main advantage of this system is, on the one hand, the simplicity of it, allowing \nthe quick storage and retrieval of data. It also allows you to horizontally scale to \na great extent. As each key is independent of the rest, they can even be stored in \ndifferent servers. Redundancy can also be introduced in the system, making multiple \ncopies for each key and value, though this makes the retrieval of information slower, \nas the multiple copies need to be compared to detect data corruption.\nSome examples of key-value databases are Riak and Redis (if used with durability \nenabled).\nDocument stores\nDocument stores revolve around the concept of a \"document,\" which is similar to a \n\"record\" in relational databases. Documents, though, are more flexible, as they don't \nneed to follow a predefined format. They also typically allow embedding more data \nin subfields, something that relational databases normally don't do, relying instead \non creating a relationship and storing that data in a different table. \nFor example, a document can look like this, here represented as JSON:\n{\n    \"id\": \"ABCDEFG\"\n    \"name\": {\n        \"first\": \"Sherlock\",\n        \"surname\": \"Holmes\"\n     }\n    \"address\": {\n        \"country\": \"UK\",\n        \"city\": \"London\",\n        \"street\": \"Baker Street\",\n        \"number\": \"221B\",\n        \"postcode\": \"NW16XE\"\n    }\n}\nDocuments are normally grouped in collections, which are similar to \"tables.\" \nNormally documents are retrieved by a unique ID that acts as the primary key, but \nqueries can also be constructed to search fields created in the document. \nSo, in our case, we could retrieve the key (ID) ABCDEFG, like in a key-value store; \nor make richer queries like \"get me all entries in the detectives collection whose \naddress.country equals UK,\" for example. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "Data Modeling\n[ 82 ]\nDocuments in one collection can be related to documents in another collection by \ntheir ID, creating a reference, but normally these databases don't allow you to create \njoin queries. Instead, the application layer should allow you to retrieve this linked \ninformation.\nSome examples of document stores are MongoDB (https://www.mongodb.com/) and \nElasticsearch (https://www.elastic.co/elasticsearch/).\nWide-column databases\nWide-column databases are structured with their data separated by columns. They \ncreate tables with certain columns, but they are optional. They also can't natively \nrelate a record in one table with another.\nThey are a bit more capable of being queried than pure key-value stores but require \nmore upfront design work on what kinds of queries are possible in the system. This \nis more restrictive than in document-oriented stores where there is more flexibility in \ndoing that after the design is done.\nThey are aimed at very big database deployments with high availability and \nreplicated data. Some examples of wide-column databases are Apache Cassandra \n(https://cassandra.apache.org/) and Google's Bigtable (https://cloud.google.\ncom/bigtable).\nKeep in mind that, while it is technically possible to create a \ncollection with documents totally independent and with different \nformats, in practice, all documents in a collection will follow a \nsomewhat similar format, with optional fields or embedded data.\nIn general, documents favor embedding information over \ncreating references. This could lead to denormalizing information, \nrepeating the information in several places. We will talk more \nabout denormalization later in the chapter.\nNormally, columns are related and can only be queried in a \nparticular order, meaning that if columns A, B, and C exist, a row \ncan query based on either A, A and B, or A, B, and C, but not just C \nor B and C, for example.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "Chapter 3\n[ 83 ]\nGraph databases\nWhile the previous non-relational databases are based on giving up the ability to \ncreate relationships between elements to gain other features (like scalability or \nflexibility), graph databases go in the opposite direction. They greatly enhance the \nrelationship aspect of the elements to create complex graphs.\nThey store objects that are nodes and edges, or relationships between the nodes. Both \nedges and nodes may have properties to better describe them.\nThe query capabilities of graph databases are aimed at retrieving information based \non relationships. For example, given a list of companies and providers, is there any \nprovider in a supply chain of a specific company that is in a specific country? Up \nto how many levels? These questions may be easy to answer for the first level in a \nrelational database (obtain the suppliers of the company and their countries), but \nquite complex and consuming for the third-level relations.\nFigure 3.1: Example of data that is typical of graph databases\nThey are typically used for social graphs, where there are connections between \npeople or organizations. Some examples are Neo4j (https://neo4j.com/) or \nArangoDB (https://www.arangodb.com/). \nSmall databases\nThis group is a bit special compared with the rest. It's composed of database systems \nthat are not differentiated as an independent process, working as an independent \nclient-server structure. Instead, they are embedded into the code of the application, \nreading directly from the hard drive. They are normally used in simple applications \nthat run as a single process and want to keep the information in a structured way.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "Data Modeling\n[ 84 ]\nA crude, yet effective, way of representing this method is to save information as \na JSON object into a file and recover it when required, for example, client settings \nfor a smartphone app. The settings file is loaded when the application starts from \nmemory, then saved if there's any change.\nFor example, in Python code, this could be represented like this:\n>>> import json\n>>> with open('settings.json') as fp:\n...   settings = json.load(fp)\n...\n>>> settings\n{'custom_parameter': 5}\n>>> settings['custom_parameter'] = 3\n>>> with open('settings.json', 'w') as fp:\n...   json.dump(settings, fp)\nFor small amounts of data, this structure may work, but it has the limitation that it's \ndifficult to query. The most complete alternative is SQLite, which is a full-fledged \nSQL database, but it's embedded into the system, without requiring external calls. \nThe database is stored in a binary file. \nSQLite is so popular that it's even supported in a lot of standard libraries, without \nrequiring an external module, for example, in the Python standard library. \n>>> import sqlite3\n>>> con = sqlite3.connect('database.db')\n>>> cur = con.cursor()\n>>> cur.execute('''CREATE TABLE pens (id INTEGER PRIMARY KEY DESC, \nname, color)''')\n<sqlite3.Cursor object at 0x10c484c70>\n>>> con.commit()\n>>> cur.execute('''INSERT INTO pens VALUES (1, 'Waldorf', 'blue')''')\n<sqlite3.Cursor object at 0x10c484c70>\n>>> con.commit()\n>>> cur.execute('SELECT * FROM pens');\n<sqlite3.Cursor object at 0x10c484c70>\n>>> cur.fetchall()\n[(1, 'Waldorf', 'blue')]\nThis module follows the DB-API 2.0 standard, which is the Python standard to \nconnect to databases. It aims to standardize access to different database backends. \nThis makes it easy to create a higher-level module that can access multiple SQL \ndatabases and swap them with minimal changes.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1945,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "Chapter 3\n[ 85 ]\nSQLite implements most of the SQL standard.\nDatabase transactions\nStoring data can be a complex operation internally for a database. In some cases, \nit can include changing the data in a single place, but there are operations that can \naffect millions of records in a single operation, for example, \"update all records \ncreated before this timestamp.\"\nHow broad and possible these operations are highly depends on the database, \nbut they are very similar to relational databases. In that case, normally there's the \nconcept of a transaction.\nA transaction is an operation that happens in one go. It either happens or it doesn't, \nbut the database is not left in an inconsistent state in the middle. For example, if the \noperation described before of \"update all records created before this timestamp\" can \nproduce an effect where, through an error, only half of the records are changed, then \nit's not a transaction, but multiple independent operations.\nThis characteristic can become a strong requirement for the database in some \napplications, and it's called atomicity. That means the transaction is atomic when it's \napplied. This characteristic is the main one of the so-called ACID properties.\nThe other properties are consistency, isolation, and durability. The four properties \nare, then:\n•\t\nAtomicity, which means that the transaction is applied as one unit. It is either \napplied completely or not.\n•\t\nConsistency, which means that the transaction is applied taking into account \nall restrictions that are defined in the database. For example, foreign key \nconstraints are respected, or any stored triggers that modify the data applied.\nYou can check the full DB-API 2.0 specification in PEP-249: \nhttps://www.python.org/dev/peps/pep-0249/.\nIt can happen that there's an error in the middle of a transaction. In \nthat case, it will go back all the way to the start of it, so no record \nwill change.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "Data Modeling\n[ 86 ]\n•\t\nIsolation, which means that parallel transactions work in the same way \nthat they were run one after the other, ensuring that one transaction is not \naffecting another. Obviously, the exception is the order in which they are run, \nwhich may have an impact.\n•\t\nDurability, which means that, after a transaction is reported as completed, \nit won't be lost even in the event of a catastrophic failure, like the database \nprocess crashing.\nThese properties are the gold standard to take care of data. It means that the data is \nsafe and consistent.\nMost relational databases have the concept of starting a transaction, performing \nseveral operations, and then finally committing the transaction so all the changes \nare applied in one go. If there's a problem, the transaction will fail, reverting to \nthe previous state. A transaction can also be aborted if, during the performance of \noperations, any problem, like a constraint issue, is detected.\nACID transactions have a cost in terms of performance, and especially in terms \nof scalability. The need for durability means that data needs to be stored on disk \nor other permanent support before being returned from the transaction. The \nrequirement for isolation means that each open transaction requires operating in a \nway that it can't see new updates, which may require temporary data to be stored \nuntil the transaction is completed. Consistency also requires checks to ensure that all \nconstraints are fulfilled, which may require complex checks.\nVirtually all relational databases are fully ACID compliant, and that has become a \ndefining characteristic of them. In the non-relational world, things are more flexible.\nScaling the database with multiple servers or nodes with these properties proves \ndifficult, though. This system creates distributed transactions, running on multiple \nservers at the same time. Maintaining full ACID transactions in databases with \nmore than one server is extremely difficult, and has a heavy penalty in terms of \nperformance, because of the extra delay caused by understanding what the other \nnodes have done and rolling back the transaction if there's a failure in any of \nthem. The problems also increase in a non-linear way, sort of working against the \nadvantages of having multiple servers.\nThis way of operating allows creating extra verification steps, as \ninside the transaction, data can still be queried and be validated \nbefore finally committing it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "Chapter 3\n[ 87 ]\nWhile this is possible, a lot of applications can work around these limitations. We \nwill see some useful patterns.\nDistributed relational databases\nAs we've discussed before, relational databases weren't designed with scalability \nin mind. They are great for enforcing strong data assurances, including ACID \ntransactions, but their preferred way of operating is through a single server. \nThis can impose limitations in terms of how big an application can be using \nrelational databases. \nThe disadvantage of the ACID properties is eventual consistency. Instead of an atomic \noperation that gets processed in a single go, the system gradually translates to the \ndesired system. Not every part of the system has the same state at the same time. \nInstead, there are certain delays while this change is propagating in the system. \nThe other big advantage is that we can increase the availability, as it won't depend \non a single node to make the change, and any non-available elements will catch up \nafter recovering. Because of the distributed nature of the cluster, this may involve \nconsulting different sources and trying to reach a quorum between them.\nIt is worth noting that a database server can grow vertically, which \nmeans using better hardware. Increasing the capacity of a server \nor replacing it with a bigger one is an easier solution for high \ndemand than applying some of these techniques, but there's a \nlimit. In any case, please double-check that the expected size is big \nenough. These days, there are servers in cloud providers that reach \n1 terabyte of RAM or more. That's enough to cover a huge number \nof cases.\nNote that these techniques are useful to grow a system after it is \nup and running, and can be added to most usages of relational \ndatabases.\nIt depends greatly on the application you have in mind when \nconsidering if loosening some of the ACID properties is worth \ndoing. Critical data, where a delay or data corruption has a higher \nimpact and may not be acceptable, may not be a good fit for a \ndistributed database.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "Data Modeling\n[ 88 ]\nIn order to increase the capacity, the first thing is to understand what the data access \nof the application is.\nPrimary/replica\nA very common case is that the number of reads is much higher than writes. Or, \ntalking in SQL terms, the number of SELECT statements is much higher than the \nUPDATE or DELETE ones. This is very typical of applications where there's way more \naccess to information than updates to information, for example, a newspaper, \nwhere there's a lot of access to read a news article, but not so many new articles \ncomparatively. \nA common pattern for this situation is to create a cluster adding one or more read-\nonly copies of the database, and then spread the reads across them, a situation \nsimilar to this one:\nFigure 3.2: Dealing with multiple Read queries\nAll the writes go to the primary node, and then that gets disseminated to the replica \nnodes automatically. Because the replicas contain the whole database, and the only \nwrite activity comes from the primary, this increases the number of queries that can \nrun at the same time in the system.\nThis system is natively supported by most relational databases, especially the most \ncommon ones, MySQL and PostgreSQL. The write nodes are configured as primary, \nand the replicas are pointed at the primary to start copying the data. After some \ntime, they'll be up to date and in sync with the primary.\nEvery new change in the primary will be replicated automatically. This, though, \nhas a delay, called a replication lag. This means that the data just written won't be \navailable to read for some time, typically less than a second.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1734,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "Chapter 3\n[ 89 ]\nAn operation to avoid, then, is to write and immediately read the same or related \ndata in an external operation, as this can cause inconsistent results. This can be \nsolved either by keeping the data temporarily, avoiding the need for the query, or by \nmaking it possible to address a specific read to the primary node, to ensure that the \ndata is consistent.\nFigure 3.3: A specific Read query on the primary node\nThis system also allows there to be redundancy of data, as it's always being copied to \nthe replicas. If there's a problem, a replica can be promoted to be the new primary.\nReplication lag is a good indicator of the wellbeing of the \ndatabase. If the lag increases over time, it's an indication that the \ncluster is not capable of handling the level of traffic and requires \nadjustments. This time will be greatly influenced by the network \nand general performance of each of the nodes.\nThese direct reads should be used only when necessary, as they go \nagainst the idea of reducing the number of queries to the primary \nserver. That was the reason to set up multiple servers!\nA replica server doesn't fulfill exactly the same role as a backup, \nthough it can be used with a similar intent. A replica is intended to \nperform a quick action and maintain the availability of the system. \nBackups are easier and cheaper to run and allow you to keep a \nhistorical record of the data. Backups can also be located in a very \ndifferent location than the replica, while a replica requires a good \nnetwork connection with the primary.\nDo not skip doing backups, even if there are replicas available. \nBackups will add a security layer in case of catastrophic failure. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "Data Modeling\n[ 90 ]\nNote that this way of structuring the database may require adapting the application \nlevel to be aware of all the changes and access to different database servers. There \nare existing tools such as Pgpool (for PostgreSQL) or ProxySQL (for MySQL) that \nstay in the middle of the path and redirect the queries. The application addresses the \nqueries to the proxies, and then the proxy redirects them based on the configuration. \nThere are cases, like the write and read pattern that we've seen above, that are not \ncovered easily and may require specific changes in the application code. Be sure to \nunderstand how these kinds of tools work and run some tests before running them \nin your application.\nA simpler case of this structure is to create offline replicas. These can be created from \na backup and not updated from the live system. These replicas can be useful to create \nqueries that don't require up-to-date information, in cases where perhaps a daily \nsnapshot is good enough. They are common in applications like statistical analysis or \ndata warehousing.\nSharding\nIf the application has a higher number of writes, the primary-replica structure may \nnot be good enough. Too many writes are directed to the same server, which creates \na bottleneck. Or if the system traffic grows enough, there's a limit to the number of \nwrites that a single server can accept.\nA possible solution is to horizontally partition the data. This means dividing the data \ninto different databases according to a specific key, so all related data can go to the \nsame server. Each of the different partitions is called a shard. \nThe partition key is called the shard key, and based on its value, each row will be \nallocated a specific shard.\nNote that \"partitioning\" and \"sharding\" can be considered \nsynonyms, though in reality sharding is only if the partition \nis horizontal, separating a single table into different servers. \nPartitioning can be more general, like dividing a table into two, \nor splitting into different columns, which is not typically called \nsharding.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "Chapter 3\n[ 91 ]\nFigure 3.4: Shard keys\nAny query needs to be able to determine what the proper shard is to be applied to. \nAny query that affects two or more shards may be impossible to do or can only be \nperformed in succession. Of course, this excludes the possibility of performing these \nqueries in a single transaction. In any case, these operations will be very expensive, \nand should be avoided as much as possible. Sharding is a fantastic idea when the \ndata is naturally partitioned, and very bad when queries affecting multiple shards \nare performed.\nThe choosing of the sharding key is also crucial. A good key should follow natural \npartitions between data, so performing cross-shard queries is not required. For \nexample, if the data of a user is independent of the rest, which may happen with a \nphoto-sharing application, the user identifier could be a good shard key.\nAnother important quality is that the shard to address the query needs to be \ndetermined based on the shard key. That means that every query needs to have \nthe shard key available. This means that the shard key should be an input of every \noperation. \nThe name shard comes from the videogame Ultima Online, which, \nin the late 90s, used this strategy to create a \"multiverse\" where \ndifferent players could play the same game on different servers. \nThey called them \"shards,\" as they were aspects of the same reality, \nbut contained different players in them. The name stuck and it's \nstill used to describe the architecture.\nSome NoSQL databases allow native sharding that will take care of \nall these options automatically. A common example is MongoDB, \nwhich is even capable of running queries in multiple shards in a \ntransparent manner. These queries will be slow, in any case.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1869,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "Data Modeling\n[ 92 ]\nAnother property of the shard key is that the data should be ideally portioned in a \nway that shards have the same size, or at least they are similar enough. If one shard \nis much bigger than the rest, that could lead to problems of imbalanced data, not \nenough distributing of the queries, and having one shard being the bottleneck. \nPure sharding\nOn pure shards, the data is all partitioned in shards and the shard key is an input of \nevery operation. The shard is determined based on the shard key.\nTo ensure that the shards are balanced, each key is hashed in a way that is equally \ndistributed between the number of shards. A typical case is to use a modulo \noperation, for example. If we have 8 shards, we determine which shard the data is \npartitioned into based on a number that's equally distributed.\nUser ID\nOperation\nShard\n1234\n1234 mod 8\n2\n2347\n2347 mod 8\n3\n7645\n7645 mod 8\n5\n1235\n1235 mod 8\n3\n4356\n4356 mod 8\n4\n2345\n2345 mod 8\n1\n2344\n2344 mod 8\n0\nIf the shard key is not a number, or if it's not evenly distributed, a hash function can \nbe applied. For example, in Python:\n>>> import hashlib\n>>> shard_key = 'ABCDEF'\n>>> hashlib.md5(shard_key.encode()).hexdigest()[-6:]\n'b9fcf6'\n>>> int('b9fcf6', 16)  # Transform in number for base 16\n12188918\n>>> int('b9fcf6', 16) % 8\n6\nThis strategy is only possible if the shard key is always available as input for every \noperation. When this is not an option, we need to look at other options.\nChanging the number of shards is not an easy task, as the destination for each key is \ndecided by a fixed formula. It is possible, though, to grow or reduce the number of \nshards with some preparation in advance. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "Chapter 3\n[ 93 ]\nWe can create \"virtual shards\" that point to the same server. For example, to create \n100 shards, and use two servers, initially the virtual shard distribution will be like \nthis:\nVirtual Shard\nServer\n0-49\nServer A\n50-99\nServer B\nIf the number of servers needs to be increased, the virtual shard structure will change \nin this way.\nVirtual Shard\nServer\n0-24\nServer A\n25-49\nServer C\n50-74\nServer B\n75-99\nServer D\nThis change to the specific server that corresponds to each shard may require some \ncode change, but it's easier to handle as the shard key calculation won't change. The \nsame operation can be applied in reverse, though it may create imbalance, so it needs \nto be done with care.\nVirtual Shard\nServer\n0-24\nServer A\n25-49\nServer C\n50-99\nServer B\nEach of the operations requires changing the location of data based on the shard key. \nThis is a costly operation, especially if a lot of data needs to be exchanged.\nMixed sharding\nSometimes it is not possible to create pure shards and a translation from the input \nis required to determine the shard key. This is the case, for example, when a user is \nlogging in if the shard key is the user ID. The user will log in using their email, but \nthat needs to be translated to the user ID to be able to determine the shard to search \nthe information.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1422,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "Data Modeling\n[ 94 ]\nIn that case, an external table can be used purely to translate the input of a particular \nquery to the shard key.\nFigure 3.5: External tables to translate the input of shard keys\nThis creates a situation where a single shard is responsible for this translation layer. \nThis shard can be used exclusively for this, or also act as any other shard.\nKeep in mind that this requires a translation layer for each possible input parameter \nthat's not directly the shard key, and that it requires keeping all the information of \nall shards in a single database. This needs to be kept under control and store as little \ninformation as possible, to avoid issues.\nThis strategy can be used as well to store, directly, what shard key goes to what \nshard, and perform a query instead of a direct operation, as we saw above.\nFigure 3.6: Storing shard keys to shards\nThis has the inconvenience that determining the shard based on the key requires a \nquery in a database, especially with a big database. But it also allows changing the \nshard of the data in a consistent way, which can be used to adapt the number of \nshards, like growing or reducing the number. And it can be done without requiring \ndowntime.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "Chapter 3\n[ 95 ]\nIf the specific shard, not only the shard key, is stored in this translation table, the \nassignment of the shard to the key can be changed one by one, and in a continuous \nmanner. The process will be approximately like this:\n1.\t Shard key X is assigned to server A in the reference table. This is the start \nstate.\n2.\t Data from server A for shard key X is copied to server B. Note that no query \ninvolving shard key X is directed to server B yet.\n3.\t Once all the data is copied, the entry for the reference table for shard key X is \nchanged to server B.\n4.\t All queries for shard key X are directed to server B.\n5.\t Data from shard key X in server A can be cleaned.\nStep 3 is the critical step, and needs to happen only after all the data is copied, and \nbefore any new write is performed. A way of ensuring this is to create a flag in \nthe reference table that can stop or delay the writing of data while the operation \nis in place. This flag will be set up right before step 2 and removed after step 3 is \ncompleted.\nThis process will produce a smooth migration over time, but it requires enough \nspace to work, and can take a significant amount of time.\nPlease allow ample time to complete the migration. Depending on the size and \ncomplexity of the dataset, it can take a lot of time to migrate, up to hours or even \ndays for extreme cases.\nTable sharding\nAn alternative to sharding by shard key, for smaller clusters, is to separate tables or \ncollections by server. This means that any query in table X is directed to a specific \nserver, and the rest of the queries are directed to another. This strategy only works \nfor unrelated tables, as it's not possible to perform joins between tables in different \nservers.\nDownscale operations are more complicated than upscale, as the \nincrease in space allows for ample room. Fortunately, it is rare that \na database cluster needs to downscale, as most applications will \ngrow over time. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2059,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "Data Modeling\n[ 96 ]\nThis works as a less complicated alternative, but it's way less flexible. It's only \nrecommended for relatively small clusters where there's a big imbalance in size \nbetween one or two tables and the rest, for example, if one table stores logs that \nare much bigger than the rest of the database and are sparingly accessed.\nAdvantages and disadvantages of sharding\nIn summary, the main advantages of sharding are:\n•\t\nAllows spreading writes over multiple servers, increasing the write \nthroughput of the system\n•\t\nThe data gets stored in multiple servers, so massive amounts of data can be \nstored, without limiting the data that can be stored in a single server\nIn essence, sharding allows the creation of big, scalable systems. But it also has \ndisadvantages:\n•\t\nSharded systems are more complicated to run and have some overhead \nin terms of configuring different servers, and so forth. While any big \ndeployment will have its problems, sharding requires more work than a \nprimary-replica setup, as the maintenance and operation need to be planned \nwith more care and operations will take longer.\n•\t\nNative support for sharding is available only in a small number of databases, \nlike MongoDB, but relational databases don't have the feature implemented \nnatively. This means that the complexity needs to be handled with ad hoc \ncode, which will require an investment in developing it.\n•\t\nSome queries will be impossible or almost impossible to do once the data is \nsharded. Aggregation and joins, depending on how the data is partitioned, \nwon't be possible. The shard key needs to be selected carefully, as it will have \na big implication on what queries are possible or not. We also lose the ACID \nproperties, as some operations may need to involve more than one shard. A \nsharded database is less flexible.\nAs we've seen, designing, operating, and maintaining a sharded database only \nmakes sense for very big systems, when the number of actions in the database \nrequires such a complex system.\nNote that this can be considered, being pedantic, as not properly \nsharding, though the structure is similar. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2234,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "Chapter 3\n[ 97 ]\nSchema design\nFor databases that need to define a schema, the specific design to use is something \nthat needs to be considered. \nChanging the schema is an important action that will require planning and, certainly, \na long-term view needs to be applied to the design.\nThe best way to start the design of a schema is to draw the different tables, fields, \nand their relationships, if there are foreign keys pointing to other tables.\nFigure 3.7: Drawing a schema\nThe presentation of this data should allow you to detect possible blind spots or \nthe repetition of elements. If the number of tables is too big, it may be necessary to \ndivide it into several groups.\nThis section will talk specifically about relational databases, as they \nare the ones that enforce a stricter schema. Other databases are \nmore flexible in their changes, but they also benefit from spending \nsome time thinking about their structure.\nWe will talk later in the chapter about how to change the schema \nof a database. We only need to remark here that mutating the \ndatabase schema is an unavoidable part of the process of building \na system. Nonetheless, it's a process that needs to be taken with \nrespect and understanding what the possible problems are. It's \ndefinitely a good idea to spend time thinking about and ensuring \nan adequate design for the schema.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1457,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "Data Modeling\n[ 98 ]\nEach of the tables can have foreign key relationships with others of different kinds: \n•\t\nOne-to-many, where a single reference is added for multiple elements of \nanother table. For example, a single author is referenced in all their books. \nA simple foreign key relationship works in this case, as the Books table will \nhave a foreign key to the entry in Authors. Multiple book rows can have a \nreference to the same author.\nFigure 3.8: The key in the first table references multiple rows in the second\n•\t\nOne-to-zero or -one are specific cases where a row can be related to only \nanother. For example, let's assume an editor can be working on a book (and \nonly one book at a time). The reference for the editor in the Books table is \na foreign key that can be set to null if there's no editing process. Another \nback reference from the editor to the book will ensure that the relationship is \nunique. Both references need to be changed in a transaction.\nFigure 3.9: The relationship only makes it possible to match two rows\nThough there are tools that can help with this work, personally, \nit helps me to hand-draw these relationships, as it helps me think \nof the different relationships and construct a mental image of the \ndesign.\nStrict one-to-one relationships, like a book and a title, \nwhere both are always related, are typically better \nmodeled as adding all the information into a single table.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "Chapter 3\n[ 99 ]\n•\t\nMany-to-many, where there can be multiple assignments in both directions. \nFor example, a book may be categorized under different genres. A single \ngenre will be assigned to multiple books, and a single book can be assigned \nto more than one genre. Under a relational data structure, there's a need for \nan intermediary extra table that makes that relationship, which will point to \nboth the book and the genre.\nFigure 3.10: Note the intermediary table allows multiple combinations.  \nThe first table can reference multiple rows of the second, and the second multiple rows of the first\nIn most cases, the types of fields to store for each of the tables are straightforward, \nthough certain details should be considered:\n•\t\nAllowing enough space to grow in the future. Some fields, like strings, \nrequire defining a maximum size to store. For example, storing a string \nrepresenting an email address will require a maximum of 254 characters. But \nsometimes the size is not obvious, like storing the name of a customer. In \nthese cases, it's better to err on the safe size and increase the limit.\n•\t\nThe limits should be enforced not only for the database, but also above this \nlevel, to always allow any API or UI that deals with the field to handle it \ngracefully.\nThis extra table may include more information, for example, how \naccurate the genre is for the book. That way, it could describe \nbooks that are 50% horror and 90% adventure.\nOutside of the relational data world, sometimes there's not such \na pressing need for creating many-to-many relationships, and \ninstead they can be directly added as a collection of tags. Some \nrelational databases now allow more flexibility in allowing fields \nthat are lists or JSON objects, which can be used in this way, \nsimplifying the design.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1911,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "Data Modeling\n[ 100 ]\nWhen dealing with numbers, in most cases regular integers will be enough to \nrepresent most used numbers. Though some databases accept categories like \nsmallint for 2 bytes or tinyint for 1-byte values, it's not recommended to \nmake use of them. The difference in space used will be minimal.\n•\t\nThe internal database representation doesn't need to be the same as what's \nexternally available. For example, the time stored in the database should \nalways be in UTC, and then translated to the user's time zone.\nAnother example is if prices are stored, it's better to store them in cents, to \navoid float numbers, and then present them as dollars and cents. \nThe internal representation doesn't need to follow the same conventions if \nstoring them in a different format is better for some reason.\n•\t\nAt the same time, it's better to represent the data naturally. A typical \nexample of that is the overabundance of using numeric IDs to represent rows \nthat have natural keys or using Enums (small integers assigned to represent \na list of options) instead of using short strings instead. While these choices \nmade sense some time ago, when space and processing power were more \nrestrictive, now the performance improvement is negligible, and storing data \nin an understandable way helps greatly while developing.\nStoring the time always in UTC format allows using a \nconsistent time in the server, in particular if there are users \nin different time zones. Storing the time by applying the \ntime zone for the user produces non-comparable times in \nthe database and using the default time zone of the server \ncan produce different results based on the position of the \nserver, or even worse, inconsistent data if more than one \nserver in different time zones is involved. Ensure that all \ntimes are stored in the database in UTC.\nFor example, this means that a price of $99.95 will be \nstored as the integer 9995. Dealing with float arithmetic \ncan create problems for prices, and prices can be translated \ninto cents for easy handling.\nFor example, instead of using an integer field to store \ncolors, where 1 means RED, 2 means BLUE, and 3 means \nYELLOW, use a short string field using the strings RED, \nBLUE, and YELLOW. The storing difference is negligible \neven if there are millions of records, and it's way easier to \nnavigate the database.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2465,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "Chapter 3\n[ 101 ]\nWe will see a bit later about normalization and denormalization, which are \nrelated to this concept.\n•\t\nNo design will be perfect or complete. In a system under development, the \nschema will always require changes. This is totally normal and expected and \nshould be accepted as such. Perfect is the enemy of good. The design should \ntry to be as simple as possible to adjust for the current needs of the system. \nOverdesign, trying to advance every possible future need and complicating \nthe design, is a real problem that can waste efforts in laying the ground for \nneeds that never materialize. Keep your design simple and flexible.\nSchema normalization \nAs we've seen, in relational databases, a key concept is the foreign key one. Data \ncan be stored in a table and linked to another. This split in data means that a set of \nlimited data can, instead of being stored in a single table, be split in two. \nFor example, let's take a look at this table, initially with the field House as a string:\n\t\n\t\n\t\nCharacters\nid\nName\nHouse\n1\nEddard Stark\nStark\n2\nJon Snow\nStark\n3\nDaenerys \nTargaryen\nTargaryen\n4\nJaime Lannister\nLannister\nTo ensure that the data is consistent and there are no errors, the field House can \nbe normalized. This means that it's stored in a different table, and a FOREIGN KEY \nconstraint is enforced, in this way.\nCharacters\t\n\t\n\t\n\t\n\t\n        Houses\nid\nName\nHouseId \n(FK)\nid\nName\nWords\n1\nEddard Stark\n1\n1\nStark\nWinter is coming\n2\nJon Snow\n1\n2\nLannister\nHear me roar\n3\nDaenerys \nTargaryen\n3\n3\nTargaryen\nFire and blood\n4\nJaime Lannister\n2\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "Data Modeling\n[ 102 ]\nThis way of operating normalizes the data. No new entry with a new House can be \nadded unless it is first introduced in the Houses table. In the same way, an entry in \nHouses cannot be deleted while a single entry in Characters contains a reference. This \nensures that the data is very consistent and there are no problems, like introducing \na typo like House Lanister (single n) for a new entry, which may complicate later \nqueries.\nIt also has the advantage of being able to add extra information for each of the entries \nin Houses. In this case, we can add the Words of the House. The data is also more \ncompact, as repeated information is stored in a single pace.\nOn the other hand, this has a couple of issues. First of all, any reference to a \nCharacter that needs to know the information of the House needs to perform a JOIN \nquery. In the first Characters table, we could generate our query in this way:\nSELECT Name, House FROM Characters;\nWhile in the second schema, we will require this one:\nSELECT Characters.Name, Houses.Name \nFROM Characters JOIN Houses ON Characters.HouseId = Houses.id;\nThis query will take longer to execute, as information needs to be compounded \nfrom two tables. For big tables, this time can be extensive. This can also require a \nJOIN from different tables if we add, for example, a PreferredWeapon field and a \nWeapons normalized table for each Character. Or we can add even more tables as \nthe Characters table grows in fields.\nIt will also take longer to insert and delete data, as more checks need to be \nperformed. In general, operations will take longer.\nNormalized data is also difficult to shard. The concept of normalization of keeping \nevery element described in its own table and reference from there is inherently \ndifficult to shard, as it makes partitioning very difficult.\nAnother problem is that the database is more difficult to read and operate. Deletes \nneed to happen in an ordered fashion, which gets more difficult to follow as more \nfields are being added. Also, complex JOIN queries need to be performed for simple \noperations. The queries are longer and more complicated to generate.\nWhile this normalization structure, creating foreign keys through numerical \nidentifiers, is pretty typical, it's not the only option. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "Chapter 3\n[ 103 ]\nTo improve the clarity of the database, natural keys can be used to simplify them, \ndescribing the data in this way. Instead of using an integer as the primary key, we \nuse the Name field on the Houses table.\nCharacters\t\n\t\n\t\n\t\n\t\n        Houses\nId\nName\nHouse (FK)\nName (PK)\nWords\n1\nEddard Stark\nStark\nStark\nWinter is coming\n2\nJon Snow\nStark\nLannister\nHear me roar\n3\nDaenerys \nTargaryen\nTargaryen\nTargaryen\nFire and blood\n4\nJaime Lannister\nLannister\nThis not only removes the usage of an extra field, but it also allows you to make the \nreference with a descriptive value. We recover our original query, even if the data is \nnormalized. \nOnly when we want to obtain the information in the Words field will we need to \nperform a JOIN query:\nSELECT Name, House FROM Characters;\nThis trick, anyway, may not avoid the usage of JOIN queries in normal operation. \nPerhaps there are a lot of references and the system is having problems with the \namount of time that it's taking to perform queries. In that case, it may be necessary to \nreduce the need to JOIN tables.\nDenormalization\nDenormalization is the opposing action to normalization. Where normalizing data \nsplits it into different tables to ensure that all the data is consistent, denormalizing \nregroups information into a single table to avoid the necessity to JOIN tables.\nAs we described before, the extra space of storing a string \ninstead of a single integer is negligible. Some developers are very \nmuch against natural keys and prefer to use integer values, but \nnowadays there's not really a solid technical reason for limiting \nyourself.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1718,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "Data Modeling\n[ 104 ]\nFollowing our example above, we want to replace a JOIN query like this:\nSELECT Characters.Name, Houses.Name, House.Words \nFROM Characters JOIN Houses ON Characters.House = Houses.Name;\nWhich follows this schema:\nCharacters\t\n\t\n\t\n\t\n\t\n        Houses\nId\nName\nHouse (FK)\nName (PK)\nWords\n1\nEddard Stark\nStark\nStark\nWinter is coming\n2\nJon Snow\nStark\nLannister\nHear me roar\n3\nDaenerys \nTargaryen\nTargaryen\nTargaryen\nFire and blood\n4\nJaime Lannister\nLannister\nFor a query similar to this, querying a single table, use something like this:\nSELECT Name, House, Words FROM Characters\nTo do so, the data needs to be structured in a single table.\nCharacters\nid\nName\nHouse\nWords\n1\nEddard Stark\nStark\nWinter is coming\n2\nJon Snow\nStark\nWinter is coming\n3\nDaenerys \nTargaryen\nTargaryen\nFire and blood\n4\nJaime Lannister\nLannister\nHear me roar\nNote that information is duplicated. Every Character has a copy of the Words of the \nHouse, something that was not required before. This means denormalization uses \nmore space; in a big table with many rows, way more space. \nDenormalization also increases the risk of inconsistent data, as there's nothing \nensuring that there's not a new value that's a typo of an old value, or that, by \nmistake, incorrect Words are added to a different House.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1393,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "Chapter 3\n[ 105 ]\nBut, on the other hand, we are now free of having to JOIN tables. For big tables \nthis can speed up processing, both read and writes, quite a lot. It also removes the \nconcerns for sharding, as now the table can be partitioned on whatever shard key \nthat's convenient and will contain all the information.\nDenormalization is an extremely common option for the use cases that typically fall \nunder NoSQL databases, which remove the capability to perform JOIN queries. For \nexample, document databases embed data as subfields into a bigger entity. While it \ncertainly has its cons, it's a trade-off that makes sense in some operations.\nData indexing\nAs data grows, the access to data starts getting slower. Retrieving exactly the \nproper data from a big table full of information requires performing more internal \noperations to locate it.\nThis process can be greatly speeded up by organizing the data smartly in a way that \nis easy to search. This leads to creating indexes that allow you to locate data very \nquickly by searching through them. The basics of an index is to create an external \nsorted data structure that points to one or more fields of each of the records of the \ndatabase. This index structure is always kept sorted as data in the table changes.\nFor example, a short table may contain this information\nid\nName\nHeight (cm)\n1\nEddard\n178\n2\nJohn\n173\n3\nDaenerys\n157\n4\nJaime\n188\nWhile we will describe data indexing in relation to relational \ndatabases, most of the fundamentals are applicable to other \ndatabases.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1645,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "Data Modeling\n[ 106 ]\nIn the absence of an index, to query what entry has the highest height, the database \nwill need to individually check each of the rows and sort them. This is called a full \ntable scan. A full table scan can be very costly if the table has millions of rows.\nBy creating an index for the Height field, a data structure that is always sorted is kept \nin sync with the data. \nid\nName\nHeight (cm)\nHeight (cm)\nid\n1\nEddard\n178\n188\n4\n2\nJohn\n173\n178\n1\n3\nDaenerys\n157\n173\n5\n4\nJaime\n188\n157\n3\nBecause it is always sorted, making any query related to the height is easy to fulfill. \nFor example, obtaining what the top 3 heights are doesn't require any checking, just \nretrieving the first three records from the index, and determining heights between \n180 and 170 is also easy, using search methods in sorted lists, like a binary search. \nOnce again, if this index doesn't exist, the only way to find these queries is by \nchecking each record in the table.\nNote that the index doesn't cover all the fields. The Name field is not indexed, for \nexample. Another index may be required to cover other fields. The same table \naccepts multiple indices.\nIndexes can be combined, creating an index for two or more fields. These composite \nindices sort the data based on the ordered combination of both fields, for example, \na composite index that is (Name, Height) will quickly return the height for Names \nstarting with J. A composite index of (Height, Name) will do the opposite, priming \nthe height and then sorting the Name field.\nQuerying in composite indices for only the first part of the index is possible. In our \nexample, an index of (Height, Name) will always work for querying Height.\nThe usage or not of indexes to retrieve the information is done automatically by \nthe database; the SQL query doesn't change at all. Internally, the database will run \nthe query analyzer before running a query. This part of the database software will \ndetermine how to retrieve the data, and what indexes to use, if any.\nThe primary key of a table is always indexed, as it needs to be a \nunique value.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2203,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "Chapter 3\n[ 107 ]\nIndexes greatly speed up the queries that use them, especially for big tables with \nthousands or millions of rows. They are also used automatically, so they don't add \nextra complexity to the generation of queries. So, if they are so great, why not index \nabsolutely everything? Well, indices also have some issues:\n•\t\nEach index requires extra space. While this is optimized, adding a lot of \nindexes in a single table will use more space, both in the hard drive and in \nRAM.\n•\t\nEach time the table changes, all indices in the table need to be adjusted to be \nsure that the index is properly sorted. This is more noticeable in new data \nbeing written, like records being added or indexed fields being updated. \nIndices are a trade-off between spending more time on writing to speed \nup the reading. For tables that are write heavy, this trade-off may not be \nadequate, and maintaining one or more indices can be counterproductive.\n•\t\nSmall tables don't really benefit from being indexed. The difference between \na full table scan and an indexed search is small if the number of rows is \nbelow the thousands.\nAs a rule of thumb, it's better to try to create indices after the need is detected. Once \na slow query is discovered, analyze if an index will improve the situation, and only \nthen create it.\nCardinality\nAn important characteristic of the usefulness of each index is its cardinality. This is \nthe number of different values that an index contains.\nThe query analyzer needs to run quickly, as determining what the \nbest possible way to search for information is can take more time \nthan running a naïve approach and returning the data. This means \nthat, sometimes, it will make mistakes and not use the optimal \ncombination. The SQL command EXPLAIN, used before another \nSQL statement, will display how the query will be interpreted and \nrun, which allows you to understand and tweak it to improve its \nexecution time.\nKeep in mind that using different independent indices in the same \nquery may not be possible. Sometimes the database won't be able \nto perform a faster query by combining two indices as the data \nneeds to be correlated between them, and that may be a costly \noperation.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2317,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "Data Modeling\n[ 108 ]\nFor example, the Height index in this table has a cardinality of 4. There are four \ndifferent values.\nid\nHeight (cm)\n1\n178\n2\n165\n3\n167\n4\n192\nA table like this has only a cardinality of 2.\nid\nHeight (cm)\n1\n178\n2\n165\n3\n178\n4\n165\nAn index with low cardinality has low quality, as it's not able to speed up the search \nas much as expected. An index can be understood as a filter that allows you to \nreduce the number of rows to search. If, after applying the filter, the table has not \nbeen greatly reduced, the index won't be useful. Let's use an extreme example to \ndescribe it.\nImagine a table with a million rows indexed by a field that's the same in all of them. \nNow imagine that we make a query to find a single row in a different field that's not \nindexed. If we use the index, we won't be able to speed up the process, as the index \nwill return every single row in the database.\nFigure 3.11: Returning every single row from a query using an unhelpful index\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "Chapter 3\n[ 109 ]\nNow imagine it with two values. Half of the rows of the table are returned first, and \nthen we need to query them. This is better, but using the index has some overhead \ncompared with just performing a full table scan, so in practice, this is not very \nadvantageous.\nFigure 3.12: Returning rows using an index with two values\nAs we increase the cardinality of the index, adding more and more values, the index \nis more useful. \nFigure 3.13: Returning rows using an index with four values\nWith a higher cardinality, the database is able to discriminate better and to point to a \nsmaller subsection of values, which speeds up greatly access to the proper data.\nAs a rule of thumb, ensure that the cardinality of an index is \nalways 10 or higher. Lower than that is probably not good enough \nto use as an index. The query analyzer will take the cardinality \nvalue into account to see whether to use the index or not.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "Data Modeling\n[ 110 ]\nKeep in mind that the cardinality of fields that only allow a small number of values, \nsuch as Booleans and Enums, is always limited and makes them bad candidates to be \nindexed, at least on their own. On the other hand, values that are unique will always \nhave the highest possible cardinality and they are good candidates for indexing. \nPrimary keys are always indexed automatically for this reason.\nSummary\nIn this chapter, we described different methods and techniques to deal with the \nstorage layer, both from the point of view of the different capacities and options \navailable in the database itself, and how the code of our application can interact to \nstore and retrieve information.\nWe described the different kinds of databases, both relational and non-relational, and \nwhat the differences and usages of each are, and how the concept of a transaction, \none of the fundamental characteristics of relational databases, allows compliance \nwith ACID properties. As some of the non-relational databases are aimed at dealing \nwith data on a large scale and are distributed, we presented some of the techniques \nto scale up relational systems, as that kind of database was not initially designed to \ndeal with multiple servers.\nWe continued by describing how we can design a schema and what the pros and \ncons are for normalizing and denormalizing the data. We also described why we \nindex fields and when it's counterproductive.\nIn Chapter 4, The Data Layer, we will see how to design the data layer.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1790,
      "extraction_method": "Direct"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "[ 111 ]\n4\nThe Data Layer\nThe modeling of data when interacting with the application code is as important as \nhow that data is stored in storage. The data layer is the layer that developers will \ninteract with most often, so creating a good interface is critical to create a productive \nenvironment.\nIn this chapter, we will describe how to create a software data layer that interacts \nwith storage to abstract the specifics of storing data. We will see what Domain-\nDriven Design is, how to use an Object-Relational Mapping framework, and more \nadvanced patterns, like Command Query Responsibility Segregation.\nWe will also talk about how to make changes to the database as the application \nevolves and, finally, techniques to deal with legacy databases when the structure \nhas already been defined before our involvement.\nIn this chapter, we will look at the following topics:\n•\t\nThe Model layer\n•\t\nDatabase migrations\n•\t\nDealing with legacy databases\nLet's start by giving the context of the data design as part of the Model part of the \nModel-View-Controller (MVC) pattern.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "The Data Layer\n[ 112 ]\nThe Model layer\nAs we saw when we presented the Model-View-Controller architecture in Chapter 2, \nAPI Design, the Model layer is the part that's intimately related with the data and \nstoring and retrieving it.\nThe Model abstracts all the data handling. This not only includes database access but \nalso the related business logic. This creates a two-layer structure:\n•\t\nThe internal data modeling layer, handling the storage and retrieval of data \nfrom the database. This layer needs to understand the way the data is stored \nin the database and handles it accordingly.\n•\t\nThe next layer creates business logic and uses the internal data modeling \nlayer to support it. This layer is in charge of ensuring that the data to be \nstored is consistent and enforces any relationships or constraints.\nIt's very common to deal with the data layer as a pure extension of the database \ndesign, removing the business level or storing it as code in the Controller part. \nWhile this is doable, it's better to think about whether it's good to explicitly add \nthe business layer on top and ensure there's separation between the entity models, \nwhich makes good business sense, and the database models, which contain the \ndetails on how to access the database.\nDomain-Driven Design\nThis way of operating has become common as part of Domain-Driven Design. \nWhen DDD was first introduced, it was aimed mainly at bridging the gap between \nthe specific application and the technology implementing it to try to use proper \nnomenclature and ensure that the code was in sync with the real operations that the \nusers of the code would use. For example, banking software will use methods for \nlodging and withdrawing funds, instead of adding and subtracting from an account.\nWhen paired with Object-Oriented Programming (OOP), DDD techniques will \nreplicate the concepts required by the specific domain as objects. In our previous \nexample, we would have an Account object that accepts the methods lodge() and \nwithdraw(). These would probably accept a Transfer object that would keep the \nproper balance in the source of the funds.\nDDD is not only naming methods and attributes in a way \nthat's consistent with the proper jargon of the domain, but also \nreplicating the uses and flows.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "Chapter 4\n[ 113 ]\nThese days, DDD is understood as the creation of this business-oriented interface \nin the Model layer, so we can abstract the internals on how that's being mapped \ninto accesses to the database and present a consistent interface that replicates the \nbusiness flows.\nFor a lot of different concepts, the Model works purely as a replication of the \nschema of the database. This way, if there's a table, it gets translated into a Model \nthat accesses that table, replicates the fields, etc. An example of this is storing the \nuser in a table with username, full name, subscription, and password fields.\nBut remember that it is not a hard requirement. A Model can use multiple tables \nor combine multiple fields in a way that makes more business sense, even not \nexposing some fields as they should remain internal.\nFor example, the example of the user above has the following fields in the database \nas columns in a SQL table:\nField\nType\nDescription\nUsername\nString\nUnique username\nPassword\nString\nString describing the hashed password\nFull name\nString\nName of the user\nSubscription \nend\nDatetime\nTime when the subscription ends\nSubscription \ntype\nEnum (Normal, Premium, \nNotSubscribed)\nKind of subscription\nDDD requires an intimate knowledge of the specific domain \nat hand to create an interface that makes sense and properly \nmodels the business actions. It requires close communication and \ncollaboration with business experts to be sure that all possible gaps \nare covered.\nWe will use a relational database using SQL as our default \nexample, as it is the most common kind of database. But everything \nthat we are discussing is highly applicable to other kinds of \ndatabases, especially to document-based databases.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "The Data Layer\n[ 114 ]\nBut the Model may expose the following:\nAttribute/Method\nType\nDescription\nusername\nString attribute\nDirectly translates the username \ncolumn\nfull_name\nString attribute\nDirectly translates the full_name \ncolumn\nsubscription\nRead-only \nproperty\nReturns the subscription type column. \nIf the subscription has ended (as \nindicated in the subscription end \ncolumn), it returns NotSubscribed\ncheck_password(password)\nMethod\nInternally checks whether the \npassword input is valid by comparing \nit with the hash password column and \nreturns whether it is correct or not\nNote that this hides the password itself, as its internal details are not relevant outside \nthe database. It also hides the internal subscription fields, presenting instead a single \nattribute that performs all the relevant checks.\nThis Model transforms the actions from the raw database access to a fully defined \nobject that abstracts the access to the database. This way of operating, when mapping \nan object to a table or collection, is called Object-Relational Mapping (ORM).\nUsing ORM\nAs we've seen above, in essence, ORM is performing mapping between the \ncollections or tables in a database, and generating objects in an OOP environment.\nWhile ORM itself refers to the technique, the way it is usually understood is as a tool. \nThere are multiple ORM tools available that do the conversion from SQL tables to \nPython objects. This means that, instead of composing SQL statements, we will set \nup properties defined in classes and objects that will then be translated automatically \nby the ORM tool and will connect to the database.\nFor example, a low-level access for a query in the \"pens\" table could look like this:\n>>> cur = con.cursor()\n>>> cur.execute('''CREATE TABLE pens (id INTEGER PRIMARY KEY DESC, \nname, color)''')\n<sqlite3.Cursor object at 0x10c484c70>\n>>> con.commit()\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1976,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "Chapter 4\n[ 115 ]\n>>> cur.execute('''INSERT INTO pens VALUES (1, 'Waldorf', 'blue')''')\n<sqlite3.Cursor object at 0x10c484c70>\n>>> con.commit()\n>>> cur.execute('SELECT * FROM pens');\n<sqlite3.Cursor object at 0x10c484c70>\n>>> cur.fetchall()\n[(1, 'Waldorf', 'blue')]\nNote that we are using the DB-API 2.0 standard Python interface, which abstracts \naway the differences between different databases, and allows us to retrieve the \ninformation using the standard fetchall() method.\nUsing an ORM, like the one available in the Django framework, instead of creating a \nCREATE TABLE statement, we describe the table in code as a class:\nfrom django.db import models\nclass Pens(models.Model):\n     name = models.CharField(max_length=140)\n     color = models.CharField(max_length=30)\nThis class allows us to retrieve and add elements using the class.\n>>> new_pen = Pens(name='Waldorf', color='blue')\n>>> new_pen.save()\n>>> all_pens = Pens.objects.all()\n>>> all_pens[0].name\n'Waldorf'\nThe operation that in raw SQL is an INSERT is to create a new object and then use \nthe .save() method to persist the data into the database. In the same way, instead of \ncomposing a SELECT query, the search API can be called. For example, this code:\n>>> red_pens = Pens.objects.filter(color='red')\nTo connect Python and an SQL database, the most common \nORMs are the ones included in the Django framework (https://\nwww.djangoproject.com/) and SQLAlchemy (https://www.\nsqlalchemy.org/). There are other less-used options, such as \nPony (https://ponyorm.org/) or Peewee (https://github.\ncom/coleifer/peewee), that aim to have a simpler approach. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1721,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "The Data Layer\n[ 116 ]\nIs equivalent to this code:\nSELECT * FROM Pens WHERE color = 'red;\nUsing an ORM, compared with composing SQL directly, has some advantages:\n•\t\nUsing an ORM detaches the database from the code\n•\t\nIt removes the need for using SQL (or learning it)\n•\t\nIt removes some problems with composing SQL queries, like security issues\nLet's take a closer look at these advantages and see their limits.\nIndependence from the database\nFirst of all, using an ORM detaches the database usage from the code. This means \nthat a specific database can be changed, and the code will run unchanged. This can \nbe useful sometimes to run code in different environments or to quickly change to \nuse a different database.\nThis approach is not problem-free, as some options may be available in one database \nand not in another. It may be a viable tactic for new projects, but the best approach \nis to run tests and production in the same technologies to avoid unexpected \ncompatibility problems.\nIndependence from SQL and the Repository pattern\nAnother advantage is that you don't need to learn SQL (or whatever language is \nused in the database backend) to work with the data. Instead, the ORM uses its own \nAPI, which can be intuitive and closer to OOP. This can reduce the barrier to entry to \nwork with the code, as developers that are not familiar with SQL can understand the \nORM code faster. \nA very common use case for this is to run tests in SQLite and use \nanother database like MySQL or PostgreSQL once the code is \ndeployed in production.\nUsing classes to abstract the access to the persistent layer from \nthe database usage is called the Repository pattern. Using an \nORM will make use of this pattern automatically, as it will use \nprogrammatic actions without requiring any internal knowledge of \nthe database. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1924,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "Chapter 4\n[ 117 ]\nThis advantage also has the counterpart that the translation of some actions can be \nclunky and produce highly inefficient SQL statements. This is especially true for \ncomplicated queries that require you to JOIN multiple tables.\nA typical example of this is the following example code. The Books objects have a \nreference to their author that's stored in a different table and stored as a foreign key \nreference.\nfor book in Books.objects.find(publisher='packt'):\n    author = book.author\n    do_something(author)\nThis code is interpreted in the following way:\nProduce a query to retrieve all the books from publisher 'packt'\nFor each book, make a query to retrieve the author\nPerform the action with the author\nWhen the number of books is high, all those extra queries can be very costly. What \nwe really want to do is\nProduce a query to retrieve all the books from publisher 'packt', \njoining with their authors\nFor each book, perform the action with the author\nThis way, only a single query is generated, which is much more efficient than the \nfirst case. \nThis join has to be manually indicated to the API, in the following way.\nfor book in Books.objects.find(publisher='packt').select_\nrelated('author'):\n    author = book.author\n    do_something(author)\nThis balance for ORM frameworks, between being intuitive to work with and \nsometimes requiring an understanding of the underlying implementation details, \nis a balance that needs to be defined. The framework itself will take a more or less \nflexible approach depending on how the specific SQL statements used are abstracted \nover a convenient API.\nThe need to require the addition of extra information is actually \na good example of leaking abstractions, as discussed in Chapter 2. \nYou are still required to understand the details of the database to \nbe able to create efficient code.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "The Data Layer\n[ 118 ]\nNo problems related to composing SQL\nEven if the developer knows how to deal with SQL, there's a lot of gotchas when \nworking with it. A pretty important advantage is that using an ORM avoids some \nof the problems of dealing with the direct manipulation of SQL statements. When \ndirectly composing SQL, it ends up becoming a pure string manipulation to generate \nthe desired query. This can create a lot of problems.\nThe most obvious ones are the requirement to compose the proper SQL statement, \nand not to generate a syntactically invalid SQL statement. For example, consider the \nfollowing code:\n>>> color_list = ','.join(colors)\n>>> query = 'SELECT * FROM Pens WHERE color IN (' + color_list + ')'\nThis code works for values of colors that contain values but will produce an error if \ncolors is empty.\nEven worse, if the query is composed using input parameters directly, it can produce \nsecurity problems. There's a kind of attack called an SQL injection attack that is \naimed at precisely this kind of behavior.\nFor example, let's say that the query presented above is produced when the user \nis calling a search that can be filtered by different colors. The user is directly asked \nfor the colors. A malicious user may ask for the color 'red'; DROP TABLE users;. \nThis will take advantage of the fact that the query is composed as a pure string to \ngenerate a malicious string that contains a hidden, non-expected operation.\nTo avoid this problem, any input that may be used as part of a SQL query (or any \nother language) needs to be sanitized. This means removing or escaping characters \nthat may affect the behavior of the expected query. \nEscaping characters means that they are properly encoded to be \nunderstood as a regular string, and not part of the syntax. For \nexample, in Python, to escape the character \" to be included in \na string instead of ending the string definition, it needs to be \npreceded by the \\ character. Of course, the \\ character needs to be \nescaped if it needs to be used in a string, in this case doubling it, \nusing \\\\.\nFor example: \n\"This string contains the double quote character \\\" \nand the backslash character \\\\.\"\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "Chapter 4\n[ 119 ]\nWhile there are specific techniques to manually compose SQL statements and \nsanitize the inputs, any ORM will sanitize them automatically, greatly reducing \nthe risk of SQL injection by default. This is a great win in terms of security and it's \nprobably the biggest advantage for ORM frameworks. Manually composing SQL \nstatements is generally understood as a bad idea, relying instead on an indirect way \nthat guarantees that any input is safe.\nThe counterpart is that, even when having a good understanding of the ORM API, \nthere are limits to the way elements are read for certain queries or results, which \nmay lead to operations that are much more complicated or inefficient using an ORM \nframework than creating a bespoke SQL query.\nORM frameworks will also have an impact in terms of performance, as they require \ntime to compose the proper SQL query, encode and decode data, and do other \ncheckups. While for most queries this time will be negligible, for specific ones \nperhaps this will greatly increase the time taken to retrieve the data. Unfortunately, \nthere's a good chance that, at some point, a specific, tailored SQL query will need to \nbe created for some action. When dealing with ORM frameworks, there's always a \nbalance between convenience and being able to create exactly the right query for the \ntask at hand.\nIf using SQL is the way to go, a common approach is to use prepared statements, \nwhich are immutable queries with parameters, so they are replaced as part of the \nexecution in the DB API. For example, the following code will work in a similar way \nto a print statement.\ndb.execute('SELECT * FROM Pens WHERE color={color}', color=color_input)\nThis typically happens when creating complex joins. The queries \ncreated from the ORM are good for straightforward queries but can \nstruggle to create queries when there are too many relationships, as \nit will overcomplicate them.\nAnother limit of ORM frameworks is that SQL access may allow \noperations that are not possible in the ORM interface. This may be \na product of specific plugins or capabilities that are unique to the \ndatabase in use.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2245,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "The Data Layer\n[ 120 ]\nThis code will safely replace the color with the proper input, encoded in a safe way. \nIf there's a list of elements that need to be replaced, that could be done in two steps: \nfirst, preparing the proper template, with one parameter per input, and second, \nmaking the replacement. For example:\n# Input list\n>>> color_list = ['red', 'green', 'blue']\n# Create a dictionary with a unique name per parameter (color_X) and \nthe value\n>>> parameters = {f'color_{index}': value for index, value in \nenumerate(color_list)}\n>>> parameters\n{'color_0': 'red', 'color_1': 'green', 'color_2': 'blue'}\n# Create a clausule with the name of the parameters to be replaced\n# by string substitution\n# Note that {{ will be replaced by a single {\n>>> query_params = ','.join(f'{{{param}}}' for param in  parameters.\nkeys())\n>>> query_params\n'{color_0},{color_1},{color_2}'\n# Compose the full query, replacing the prepared string\n>>> query = f'SELECT * FROM Pens WHERE color IN ({query_params})'\n>>> query\n'SELECT * FROM Pens WHERE color IN ({color_0},{color_1},{color_2})'\n# To execute, using ** on front of a dictionary will put all its keys \nas \n# input parameters\n>>> query.format(**parameters)\n'SELECT * FROM Pens WHERE color IN (red,green,blue)'\n# Execute the query in a similar way, it will handle all \n# required encoding and escaping from the string input\n   >>> db.execute(query, **query_params)\nIn our examples, we are using a SELECT * statement that will return all the columns \nin the table for simplicity, but this is not the correct way of addressing them and \nshould be avoided. The problem is that returning all columns may not be stable.\nNew columns can be added to a table, so retrieving all columns may change the \nretrieved data, increasing the chance of producing a formatting error. For example:\n>>> cur.execute('SELECT * FROM pens');\n<sqlite3.Cursor object at 0x10e640810>\n# This returns a row\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "Chapter 4\n[ 121 ]\n>>> cur.fetchone()\n(1, 'Waldorf', 'blue')\n>>> cur.execute('ALTER TABLE pens ADD brand')\n<sqlite3.Cursor object at 0x10e640810>\n>>> cur.execute('SELECT * FROM pens');\n<sqlite3.Cursor object at 0x10e640810>\n# This is the same row as above, but now it returns an extra element\n>>> cur.fetchone()\n(1, 'Waldorf', 'blue', None)\nAn ORM will handle this case automatically, but using raw SQL requires you to take \nthis effect into account and always include explicitly the columns to retrieve to avoid \nproblems when making changes in the schema later on.\n>>> cur.execute('SELECT name, color FROM pens');\n<sqlite3.Cursor object at 0x10e640810>\n>>> cur.fetchone()\n('Waldorf', 'blue')\nQueries generated programmatically by composing them are called dynamic \nqueries. While the default strategy should be to avoid them, preferring prepared \nstatements, in certain cases dynamic queries are still very useful. There's a level of \ncustomization that can be impossible to produce unless there's a dynamic query \ninvolved.\nEven if the selected way to access the database is raw SQL statements, it's good to \ncreate an abstraction layer that deals with all the specific details of the access. This \nlayer should be responsible for storing data, in the proper format in the database, \nwithout business logic on that.\nBackward compatibility is critical when dealing with stored data. \nWe will talk more about that later in the chapter.\nExactly what is considered a dynamic query may depend on the \nenvironment. In some cases, any query that's not a stored query \n(a query stored in the database itself beforehand and called with \nsome parameters) may be considered dynamic. From our point of \nview, we will consider dynamic queries any queries that require \nstring manipulation to produce the query.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "The Data Layer\n[ 122 ]\nORM frameworks will typically work a bit against this, as they are capable of \nhandling a lot of complexity and will invite you to overload each of the defined \nobjects with business logic. When the translation between the business concept \nand the database table is direct, for example, a user object, this is fine. But it's \ndefinitely possible to create an extra intermediate layer between the storage and the \nmeaningful business object. \nThe Unit of Work pattern and encapsulating \nthe data\nAs we've seen before, ORM frameworks directly translate between tables in the \ndatabase and objects. This creates a representation of the data itself, in the way it's \nstored in the database.\nIn most situations, the design of the database will be tightly related to the business \nentities that we've introduced in the DDD philosophy. But that design may require \nan extra step, as some entities may be detached from the internal representation of \nthe data, as it's stored inside the database.\nThe creation of methods representing actions that are unique entities is called \nthe Unit of Work pattern. This means that everything that happens in this high-\nlevel action is performed as a single unit, even if internally it is implemented with \nmultiple database operations. The operation acts atomically for the caller.\nFor example, we saw earlier the example of an Account class that accepts .lodge() \nand .withdraw() methods. While it is possible to directly implement an Account table \nthat contains an integer representing the funds, we can also automatically create with \nany change a double-entry accountability system that keeps track of the system.\nIf the database allows for it, all the operations in a unit of work \nshould be produced inside a transaction to ensure that the whole \noperation is done in one go. The name Unit of Work is very \ntightly associated with transactions and relational databases and \nnormally is not used in databases that are not capable of creating \ntransactions, though the pattern can still be used conceptually.\nAccount can be called a Domain Model to indicate that it's \nindependent of the database representation.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "Chapter 4\n[ 123 ]\nTo do so, each Account should have debit and credit internal values that change \naccordingly. If we also add an extra Log entry, in a different table, for keeping track \nof movements, it may be implemented as three different classes. The Account class \nwill be the one to be used to encapsulate the log, while InternalAccount and Log will \ncorrespond to tables in the database. Note that a single .lodge() or .withdraw() call \nwill generate multiple accesses to the database, as we'll see later.\nFigure 4.1: Design of the Account class\nThe code could be something like this:\nclass InternalAccount(models.Model):\n    ''' This is the model related to a DB table '''\n    account_number = models.IntegerField(unique=True)\n    initial_amount = models.IntegerField(default=0)\n     amount = models.IntegerField(default=0)\nclass Log(models.Model):\n    ''' This models stores the operations '''\n    source = models.ForeignKey('InternalAccount', \n                               related_name='debit')\n    destination = models.ForeignKey('InternalAccount',  \n                                    related_name='credit')\n    amount = models.IntegerField()\n    timestamp = models.DateTimeField(auto_now=True)\n    def commit():\n        ''' this produces the operation '''\n        with transaction.atomic():\n            # Update the amounts\n               self.source.amount -= self.amount\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1491,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "The Data Layer\n[ 124 ]\n        self.destination.amount += self.amount\n            # save everything\n            self.source.save()\n            self.destination.save()\n            self.save()\nclass Account(object):\n    ''' This is the exposed object that handled the operations '''\n    def __init__(self, account_number, amount=0):\n        # Retrieve or create the account\n        self.internal, _ = InternalAccount.objects.get_or_create(\n            account_number=account_number,\n            initial_amount=amount,\n            amount=amount)\n       @property\n       def amount(self):\n           return self.internal.amount\n    def lodge(source_account, amount):\n        '''\n        This operation adds funds from the source\n        '''\n        log = Log(source=source_account, destination=self,\n                   amount=amount)\n        log.commit()\n    def withdraw(dest_account, amount):\n        '''\n        This operation transfer funds to the destination\n        '''\n        log = Log(source=self, destination=dest_account,\n                   amount=amount)\n        log.commit()\nThe Account class is the expected interface. It is not related directly to anything in the \ndatabase but keeps a relation to the InternalAccount using the unique reference of \nthe account_number.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1381,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "Chapter 4\n[ 125 ]\nWhenever there's an operation, it requires another account, and then a new Log is \ncreated. The Log references the source, destination, and amount of the funds, and, in \na single transaction, performs the operation. This is done in the commit method.\n    def commit():\n        ''' this produces the operation '''\n        with transaction.atomic():\n            # Update the amounts\n               self.source.amount -= self.amount\n               self.destination.amount += self.amount\n            # save everything\n            self.source.save()\n            self.destination.save()\n            self.save()\nIn a single transaction, indicated by the usage of the with transaction.atomic() \ncontext manager, it adds and subtracts funds from the accounts, and then saves the \nthree related rows, the source, the destination, and the log itself.\nThe logic to store the different elements is presented in a different \nclass than the ORM models. This can be understood in the way \nthat the ORM model classes are the Repositories classes and the \nAccount model is the Unit of Work class.\nIn some manuals, they use Unit of Work classes, leaving them \nwithout much context, just as a container to perform the action \nto store the multiple elements. Nevertheless, it's more useful to \nassign a clear concept behind the Account class to give context and \nmeaning. And there could be several actions that are appropriate \nfor the business entity.\nThe Django ORM requires you to set this atomic decorator, but \nother ORMs can work differently. For example, SQLAlchemy tends \nto work more by adding operations to a queue and requiring you \nto explicitly apply all of them in a batch operation. Please check the \ndocumentation of the specific software you are using for each case. \nA missing detail due to simplicity is the validation that there are \nenough funds to perform the operation. In cases where there aren't \nenough funds, an exception should be produced that will abort the \ntransaction.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "The Data Layer\n[ 126 ]\nNote how this format allows for each InternalAccount to retrieve every Log \nassociated to the transactions, both debits and credits. That means it can be checked \nthat the current amount is correct. This code will calculate the amount in an account, \nbased on the logs, and that can be used to check the amount is correct.\nclass InternalAccount(models.Model):\n    ...\n    def recalculate(self):\n        '''\n        Recalculate the amount, based on the logs\n        '''\n        total_credit = sum(log.amount for log in self.credit.all())\n        total_debit = sum(log.amount for log in self.debit.all())\n        return self.initial_amount + total_credit - total_debit\nThe initial amount is required. The debit and credit fields are back-references to \nthe Log, as defined in the Log class.\nFrom the point of view of a user only interested in operating with Account objects, \nall these details are irrelevant. This extra layer allows us to cleanly abstract from the \ndatabase implementation and store any relevant business logic there. This can be the \nexposed business Model layer (of the Domain Model) that handles relevant business \noperations with the proper logic and nomenclature.\nCQRS, using different models for read and \nwrite\nSometimes a simple CRUD model for the database is not descriptive of how the data \nflows in the system. In some complex settings, it may be necessary to use different \nways to read the data and to write or interact with the data.\nA possibility is that sending data and reading it happen at different ends of a \npipeline. For example, this is something that happens in event-driven systems, \nwhere the data is recorded in a queue, and then later processed. In most cases, \nthis data is processed or aggregated in a different database. \nLet's see a more specific example. We store sales for different products. These sales \ncontain the SKU (a unique identifier of the product sold) and the price. But we don't \nknow, at the time of the sale, what the profit from the sale is, as the buying of the \nproduct depends on fluctuations of the market. The storing of a sale goes to a queue \nto start the process to reconcile it with the price paid. Finally, a relational database \nstores the final sale entry, which includes the purchase price and profit.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "Chapter 4\n[ 127 ]\nThe flow of information goes from the Domain Model to the queue, then by some \nexternal process to the relational database, where it is then represented with a \nrelational model in an ORM way, and then back to the Domain Model.\nThis structure is called Command Query Responsibility Segregation (CQRS), \nmeaning that the Command (write operations) and Query (read operations) are \nseparated. The pattern is not unique to event-driven structures; they are typically \nseen in these systems because their nature is to detach the input data from the output \ndata.\nThe Domain Model may require different methods to deal with the information. \nThe input and output data has a different internal representation, and sometimes it \nmay be easier to clearly distinguish them. It's anyway a good idea to use an explicit \nDomain Model layer for CQRS to group the functionality and treat it as a whole. \nIn certain cases, the models and data may be quite different for read and write. For \nexample, if there's a step where aggregated results are generated, that may create \nextra data in the read part that's never written.\nA description of the process of how the read and write parts connect is out of scope \nin our examples. In our example, that process would be how the data is stored in the \ndatabase, including the amount paid.\nThe following diagram depicts the flow of information in a CQRS structure:\nFigure 4.2: The flow of information in a CQRS structure\nOur model's definition could be like this:\nClass SaleModel(models.Model):\n    ''' This is the usual ORM model '''\n    Sale_id = models.IntegerField(unique=True)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "The Data Layer\n[ 128 ]\n    sku = models.IntegerField()\n    amount = models.IntegerField()\n    price = models.IntegerField()\nclass Sale(object):\n    ''' \n        This is the exposed Domain Model that handled the operations \n        In a domain meaningful way, without exposing internal info\n    '''\n    def __init__(self, sale_id, sku, amount):\n        self.sale_id = sale_id\n        self.sku = sku\n        self.amount = amount\n        # These elements are won't be filled when creating a new \nelement\n        self._price = None\n        self._profit = None\n    @property\n    def price(self):\n        if self._price is None:\n            raise Exception('No price yet for this sale')\n        return self._price\n    @property\n    def profit(self):\n        if self._profit is None:\n            raise Exception('No price yet for this sale')\n        return self._profit\n       def save(self):\n            # This sends the sale to the queue\n            event = {\n                'sale_id': self.sale_id,\n                'sku': self.sku,\n                'amount': self.amount,\n            }\n            # This sends the event to the external queue\n            Queue.send(event)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1270,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "Chapter 4\n[ 129 ]\n       @classmethod\n       def get(cls, sale_id):\n           # if the sale is still not available it will raise an\n           # Exception\n           sale = SaleModel.objects.get(sale_id=sale_id)\n           full_sale = Sale(sale_id=sale_id, sku=sale.sku,\n                            amount=sale.amount)\n           # fill the private attributes\n           full_sale._price = sale.price\n           full_sale._profit = sale.amount - full_sale._price\n           return full_sale\nNote how the flow is different for save and retrieve:\n       \n      # Create a new sale\n      sale = Sale(sale_id=sale_id, sku=sale.sku, amount=sale.amount)\n      sale.save()\n      # Wait some time until totally processed\n      full_sale = Sale.get(sale_id=sale_id)\n      # retrieve the profit\n      full_sale.profit\nCQRS systems are complex, as the data in and the data out is different. They also \nnormally incur some delay in being able to retrieve the information back, which can \nbe inconvenient. \nAnother important problem in CQRS systems is the fact that the different pieces \nneed to be in sync. This includes both the read and write models, but also \nany transformation that happens within the pipeline. Over time, this creates a \nmaintenance requirement, especially when backward compatibility needs to be \nmaintained. \nAll these problems make CQRS systems complicated. They should \nbe used with care only when strictly necessary.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "The Data Layer\n[ 130 ]\nDatabase migrations\nAn unavoidable fact of development is that software systems are always changing. \nWhile the pace of changes in the database is typically not as fast as other areas, there \nare still changes and they need to be treated carefully.\nData changes are roughly categorized into two different kinds:\n•\t\nFormat or schema changes: New elements, like fields or tables, to be added \nor removed; or changes in the format of some fields.\n•\t\nData changes: Requiring changing the data itself, without modifying the \nformat. For example, normalizing an address field including the zip code, \nor making a string field uppercase.\nBackward compatibility\nThe basic principle related to changes in the database is backward compatibility. \nThis means that any single change in the database needs to work without any change \nin the code.\nThis allows you to make changes without interrupting the service. If the changes in \nthe database require a change in the code to understand it, the service will have to \nbe interrupted. This is because you can't apply both changes at the same time, and if \nthere is more than one server executing the code, it can't be applied simultaneously.\nDepending on the database, there are different approaches to data changes. \nFor relational databases, given that they require a fixed structure to be defined, \nany change in the schema needs to be applied to the whole database as a single \noperation. \nFor other databases that don't force defining a schema, there are ways of updating \nthe database in a more iterative way.\nLet's take a look at the different approaches.\nOf course, there's another option, which is to stop the service, \nperform all the changes, and restart again. While this is not great, \nit could be an option for small services or if scheduled downtime is \nacceptable.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1941,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "Chapter 4\n[ 131 ]\nRelational schema changes\nIn relational databases, each individual schema change is applied as a SQL statement \nthat operates like a transaction. The schema change, called a migration, can happen \nat the same time that some transformation of the data (for example, transforming \nintegers to strings) takes place. \nMigrations are SQL commands that perform changes in an atomic way. They can \ninvolve changing the format of tables in the database, but also more operations like \nchanging the data or multiple changes in one go. This can be achieved by creating \na single transaction that groups these changes. Most ORM frameworks include \nsupport to create migrations and perform these operations natively.\nFor example, Django will automatically create a migration file by running the \ncommand makemigrations. This command needs to be run manually, but it will \ndetect any change in the models and make the proper changes.\nFor example, if we add an extra value branch_id in the class introduced before\nclass InternalAccount(models.Model):\n    ''' This is the model related to a DB table '''\n    account_number = models.IntegerField(unique=True)\n    initial_amount = models.IntegerField(default=0)\n    amount = models.IntegerField(default=0)\n    branch_id = models.IntegerField()\nRunning the command makemigrations will generate the proper file that describes \nthe migration.\n$ python3 manage.py makemigrations\nMigrations for 'example':\n  example/migrations/0002_auto_20210501_1843.py\n    - Add field branch_id to internalaccount\nNote that Django keeps track of the state in the models and automatically adjusts the \nchanges creating the proper migration files. The pending migrations can be applied \nautomatically with the command migrate.\n$ python3 manage.py migrate\nOperations to perform:\n  Apply all migrations: admin, auth, contenttypes, example, sessions\nRunning migrations:\n  Applying example.0002_auto_20210501_1843... OK\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "The Data Layer\n[ 132 ]\nFor more details about Django migrations, check the documentation at https://\ndocs.djangoproject.com/en/3.2/topics/migrations/.\nChanging the database without interruption\nThe process to migrate the data, then, needs to happen in the following order:\n1.\t The old code and the old database schema are in place. This is the starting \npoint.\n2.\t The database applies a migration that's backward compatible with the old \ncode. As the database can apply this change while in operation, the service is \nnot interrupted.\n3.\t The new code taking advantage of the new schema is deployed. This \ndeployment won't require any special downtime and can be performed \nwithout interrupting the process.\nThe critical element of this process is step 2, to ensure that the migration is backward \ncompatible with the previous code.\nMost of the usual changes are relatively simple, like adding a new table or column \nto a table, and you'll have no problem with that. The old code won't make use of \nthe column or table, and that will be totally fine. But other migrations can be more \ncomplex. \nDjango will store in the database the status of the applied \nmigrations, to be sure that each one is applied exactly once. \nKeep in mind that, to properly use migrations through Django \nno alterations outside of this method should be made, as this can \nconfuse and create conflicts. If you need to apply changes that \ncan't be replicated automatically with a change in the model, like a \ndata migration, you can create an empty migration and fill it with \nyour custom SQL statements. This can create complex, custom \nmigrations, but that will be applied and kept in track with the rest \nof the automatically created Django migrations. Models can also \nbe explicitly marked as not-handled by Django to manage them \nmanually.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "Chapter 4\n[ 133 ]\nFor example, let's imagine that a field Field1 that has so far been an integer needs to \nbe translated into a string. There'll be numbers stored, but also some special values \nlike NaN or Inf that are not supported by the database. The new code will decode \nthem and deal with them correctly.\nBut obviously, a change that migrates the code from an integer to a string is going to \nproduce an error if this is not taken into account in the old code.\nTo solve this problem, it needs to be approached as a series of steps instead:\n1.\t The old code and the old database schema are in place. This is the starting \npoint.\n2.\t The database applies a migration adding a new column, Field2. In this \nmigration, the value from Field1 is translated into a string and copied.\n3.\t A new version of the code, intermediate code, is deployed. This version \nunderstands there may be one (Field2) or two columns (Field1 and Field2). \nIt uses the value in Field2, not the one in Field1, but if there's a write, it \nshould overwrite both.\n4.\t A new migration removing Field1, now unused, can be applied. \n5.\t The new code that is only aware of Field2 can now be deployed safely.\nDepending on whether Field2 is an acceptable name or not, it may be possible that \na further migration is deployed changing the name from Field2 to Field1. In that \ncase, the new code needs to be prepared in advance to use Field2 or, if not present, \nField1. \nTo avoid having a problem with possible updates between \nthe application of the migration and the new code, the \ncode will need to check if the column Field1 exists, and if \nit does and has a different value than Field2, update the \nlatter before performing any operation. \nIn the same migration, the same caveat as above should be \napplied – if the value in Field1 is different from the one \nin Field2, overwrite it with Field1. Note how the only \ncase where this may happen is if it has been updated with \nthe old code.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "The Data Layer\n[ 134 ]\nA new deployment could be done after that to use only Field1 again:\nFigure 4.3: Migrating from Field1 to Field2\nIf this seems like a lot of work, well, it is. All these steps are required to enforce \nsmooth operation and achieve no downtime. The alternative is to stop the old code, \nperform the migration with the format change in Field1, and then start the new \ncode. But this can cause several problems.\nThe most obvious is the downtime. While the impact can be minimized by trying \nto set up a proper maintenance window, most modern applications are expected to \nwork 24x7 and any downtime is considered a problem. If the application has a global \naudience, it may be difficult to justify a stop just for avoidable maintenance.\nThe downtime also may last a while, depending on the migration side. A common \nproblem is testing the migration in a database much smaller than the production one. \nThis can create an unexpected problem when running in production, taking much \nlonger than anticipated. Depending on the size of the data, a complex migration may \ntake hours to complete. And, given that it will run as part of a transaction, it needs to \nbe totally completed before proceeding or it will be rolled back.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "Chapter 4\n[ 135 ]\nBut another problem is the risk of introducing a step, at the start of the new code, \nthat can have problems and bugs, either related to the migration, or unrelated. \nWith this process, after the migration is applied, there's no possibility of using the \nold code. If there's a bug in the new code, it needs to be fixed and a newer version \ndeployed. This can create big trouble.\nWhile it's true that, as migrations are not reversible, applying a migration is always \na risk, the fact that the code stays stable helps mitigate problems. Changing a single \npiece of code is less risky than changing two without being able to revert either of \nthem.\nKeep in mind how migrations inter-operate with the techniques that we talked about \nrelated to distributed databases. For example, a sharded database will need to apply \neach migration independently to each of the shards, which may be a time-consuming \noperation.\nIf possible, try to test the migrations of the system with a big \nenough test database that's representative. Some operations can \nbe quite costly. It's possible that some migrations may need to be \ntweaked to run faster or even divided into smaller steps so each \none can run in its own transaction to run in a reasonable time. It's \neven possible in some cases that the database will require more \nmemory to allow the migration to run in a reasonable amount of \ntime.\nMigrations may be reversible, as there could be steps that \nperform the reverse operation. While this is theoretically true, it is \nextremely difficult to enforce in real operations. It's possible that a \nmigration like removing a column is effectively not reversible, as \ndata gets lost.\nThis way migrations need to be applied very carefully and by \nensuring that each step is small and deliberate.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1901,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "The Data Layer\n[ 136 ]\nData migrations\nData migrations are changes in the database that don't change the format but change \nthe values of some fields.\nThese migrations are produced normally either to correct some error in the data, like \na bug that stores a value with some encoding error, or to move old records to a more \nup-to-date format. For example, including zip codes in all addresses, if not already \npresent, or to change the scale of a measurement from inches to centimeters.\nIn either case, these actions may need to be performed for all rows or only for a \nselection of them. Applying them only to the relevant subset, if possible, can greatly \nspeed up the process, especially for big databases.\nIn cases like the scale change described above, the process may require more steps \nto ensure that the code can handle both scales and differentiate between them. For \nexample, with an extra field describing the scale. In this case, the process will be as \nfollows:\n1.\t Create a migration to set a new column, scale, to all rows, with a default \nvalue of inches. Any new row introduced by the old code will automatically \nset up the values correctly, by using a default value.\n2.\t Deploy a new version of the code able to work with both inches and \ncentimeters reading the value in scale.\n3.\t Set up another migration to change the value of measurement. Each row will \nchange both the scale and the measurement accordingly. Set the default value \nfor scale to centimeters.\n4.\t Now all the values in the database are in centimeters.\n5.\t Optionally, clean up by deploying a new version of the code that doesn't \naccess the scale field and understands only centimeters, as both scales are \nnot used. After that, a new migration removing the column can also be run.\nStep 5 is optional and normally there's not a great appetite for this kind of cleanup, \nas it's not strictly necessary and the versatility of having the extra column may be \nworth keeping for future usage.\nAs we discussed before, the key element is to deploy code that's able to work with \nboth database values, the old and the new, and understand them. This allows for a \nsmooth transition between the values.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "Chapter 4\n[ 137 ]\nChanges without enforcing a schema\nOne of the flexible aspects of non-relational databases is the fact that there's typically \nnot an enforced schema. Instead, the stored documents accept different formats.\nThis means that, instead of an all-or-nothing change as for relational databases, a \nmore continuous change and dealing with multiple formats is preferred.\nInstead of the application of migrations, which is a concept not really applicable \nhere, the code will have to perform the changes over time. In this case, the steps are \nlike this:\n1.\t The old code and the old database schema are in place. This is the starting \npoint.\n2.\t Each of the documents in the database has a version field.\n3.\t The new code contains a Model layer with the migration instructions from \nthe previous version to the new one – in our example above, to translate \nField1 from integer to string.\n4.\t Every time that a particular document is accessed, the version is checked. \nIf it's not the latest, Field1 is transformed into a string, and the version is \nupdated. This action happens before performing any operation. After the \nupdate, the operation is performed normally.\nThis operation runs alongside the normal operation of the system. Given \nenough time, it will migrate, document by document, the whole database.\nThe version field may not be strictly necessary, as the type \nof Field1 may be easy to infer and change. But it presents \nthe advantage that it makes the process explicit, and can be \nconcatenated, migrating an old document from different versions \nin a single access.\nIf the version field is not present, it may be understood as \nversion 0 and be migrated to version 1, now including the field.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1819,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "The Data Layer\n[ 138 ]\nFigure 4.4: Changes over time\nThis process is very clean, but sometimes leaving data in the old format for a long \ntime, even if it's not accessed, may not be advisable. It may cause that code to \nmigrate from version 1 to 2, version 2 to 3, etc, if still present in the code. If this \nis the case, an extra process running alongside may be covering every document, \nupdating and saving it until the whole database is migrated.\nAlso note that, if this functionality is encapsulated in the internal database access \nlayer, the logic above this one may use the newer functionality without caring about \nold formats, as they'll be translated on the fly.\nWhile there's still data in the database with the old version, the code needs to be \nable to interpret it. This can cause some accumulation of old tech, so it's also possible \nto migrate all the data in the background, as it can be done document to document, \nfiltering by the old version, while everything is in operation. Once this background \nmigration is done, the code can be refactored and cleaned to remove the handling of \nobsolete versions.\nThis process is similar to the one described for data migration, \nthough databases enforcing schemas need to perform migrations \nto change the format. In a schema-less database, the format can be \nchanged at the same time as the value.\nIn the same way, a pure data change, like the example seen \nbefore where it was changing the scale, can be performed without \nthe need for a migration, slowly changing the database as we \ndescribed here. Doing it with a migration ensures a cleaner change, \nthough, and may allow a simultaneous change in format.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "Chapter 4\n[ 139 ]\nDealing with legacy databases\nORM frameworks can generate the proper SQL commands to create the database \nschema. When designing and implementing a database from scratch, that means \nthat we can create the ORM Model in code and the ORM framework will make the \nproper adjustments. \nBut sometimes, we need to work with an existing database that was created \npreviously by manually running SQL commands. There are two possible use cases:\n•\t\nThe schema will never be under the control of the ORM framework. In \nthis case, we need a way to detect the existing schema and use it.\n•\t\nWe want to use the ORM framework from this situation to control the \nfields and any new changes. In this scenario, we need to create a Model that \nreflects the current situation and move from there to a declarative situation.\nLet's take a look at how to approach these situations.\nDetecting a schema from a database\nFor certain applications, if the database is stable or it's simple enough, it can be used \nas-is, and you can try to minimize the code to deal with it. SQLAlchemy allows you \nto automatically detect the schema of the database and work with it.\nThis way of describing the schema in code is called declarative.\nSQLAlchemy is a very powerful ORM-capable library and \narguably the best solution to perform complex and tailored \naccesses to a relational database. It allows complex definitions on \nhow exactly tables relate to each other and allows you to tweak \nqueries and create precise mappings. It's also more complex and \npotentially more difficult to use than other ORM frameworks such \nas the Django ORM.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1721,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "The Data Layer\n[ 140 ]\nTo automatically detect a database, you can automatically detect the tables and \ncolumns:\n>>> from sqlalchemy.ext.automap import automap_base\n>>> from sqlalchemy.sql import select\n>>> from sqlalchemy import create_engine\n# Read the database and detect it\n>>> engine = create_engine(\"sqlite:///database.db\")\n>>> Base = automap_base()\n>>> Base.prepare(engine, reflect=True)\n# The Pens class maps the table called \"pens\" in the DB\n>>> Pens = Base.classes.pens\n# Create a session to query \n>>> session = Session(engine)\n# Create a select query\n>>> query = select(Pens).where(Pens.color=='blue')\n# Execute the query\n>>> result = session.execute(query)\n>>> for row, in result:\n...     print(row.id, row.name, row.color)\n...\n1 Waldorf blue\nNote how the described names for the table pens and columns id, name, and color \nare detected automatically. The format of the query is also very similar to what a \nSQL construction will be.\nThe Django ORM also has a command that allows you to dump a definition of the \ndefined tables and relationships, using inspectdb.\n$ python3 manage.py inspectdb > models.py\nThis creates a models.py file that contains the interpretation of the database based on \nthe discovery that Django can do. This file may require adjustments.\nSQLAlchemy allows more complex usages and the creation \nof classes. For more information, refer to its documentation: \nhttps://docs.sqlalchemy.org/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1527,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "Chapter 4\n[ 141 ]\nThese methods of operation work perfectly for simple situations, where the most \nimportant part is to not spend too much effort having to replicate a schema in code. \nOther situations, where the schema gets mutated and requires better handling and \ncontrol over the code, require a different approach.\nCheck the Django documentation for more information: https://docs.\ndjangoproject.com/en/3.2/howto/legacy-databases/.\nSyncing the existing schema to the ORM \ndefinition\nIn other situations, there's a legacy database that was created by a method that \ncannot be replicated. Perhaps it was done through manual commands. The current \ncode may use the database, but we want to migrate the code so we are up-to-date \nwith it so we can, on one hand, understand exactly what the different relations \nand formats are, and on another, allow the ORM to make controlled changes to the \nschema in a compatible way. We will see the latter as migrations.\nThe challenge in this case is to create a bunch of Models in the ORM framework that \nare up-to-date with the definition of the database. This is easier said than done, for \nseveral reasons:\n•\t\nThere can be database features that are not exactly translated by the ORM. \nFor example, ORM frameworks don't deal with stored procedures natively. \nIf the database has stored procedures, they need to be either removed or \nreplicated as part of the software operation.\nStored procedures are code functions inside the database \nthat modify it. They can be manually called by using a SQL \nquery, but normally they are triggered by certain operations, \nlike inserting a new row or changing a column. Stored \nprocedures are not very common these days, as they can \nbe confusing to operate, and instead, in most cases, system \ndesigns tend to see databases as storage-only facilities, \nwithout the capacity to change the data that is stored. \nManaging stored procedures is complicated, as they can be \ndifficult to debug and keep in sync with external code.\nStored procedures can be replicated by code that handles \nthat complexity as part of a single Unit of Work action \nwhen the action will be triggered. This is the most common \napproach these days. But, of course, migrating already-\nexisting stored procedures into external code is a process \nthat may not be easy and requires care and planning.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "The Data Layer\n[ 142 ]\n•\t\nORM frameworks can have their quirks in how to set up certain elements, \nwhich may not be compatible with the already-existing database. For \nexample, how certain elements are named. The Django ORM doesn't allow \nyou to set custom names for the indices and constraints. For a while, the \nconstraint can remain only in the database, but \"hidden\" in the ORM, but \nin the long run that can create problems. This means that at some point, the \nindex name needs to be changed externally to the compatible name.\n•\t\nAnother example of this is the lack of support for composite primary keys in \nthe Django ORM, which may require you to create a new numeric column to \ncreate a surrogate key.\nThese limitations require that the creation of Models is done carefully and there are \nchecks needed to ensure that they work as expected with the current schema. The \ncreated schema based on the code Models in the ORM framework can be produced \nand compared with the actual schema until there's parity or they are close enough.\nFor example, for Django, the following general procedure can be used:\n1.\t Create a dump of the database schema. This will be used as a reference.\n2.\t Create the proper Model files. The starting point could be the output from \nthe inspectdb command described above.\n3.\t Create a single migration with all the required changes for the database. \nThis migration is created normally, with makemigrations.\n4.\t Use the command sqlmigrate to produce a SQL dump of the SQL statements \nthat will be applied by the migration. This generates a database schema that \ncan be compared with the reference.\n5.\t Adjust the differences and repeat from step 2 onward. Remember to delete \nthe migration file each time to generate it from scratch.\nOnce the migration is tweaked to produce exactly the results that are \ncurrently applied, this migration can be applied using the parameter --fake \nor –fake-initial, meaning that it will be registered as applied, but the SQL \nwon't run. \nNote that the inspectdb creates the Models with their \nmetadata set to not track changes in the database. That \nmeans that Django labels the Models as not tracked for \nchanges as migrations. Once verified, this will need to be \nchanged.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2342,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "Chapter 4\n[ 143 ]\nAfter that, changes can be applied normally by changing the Models and then \nautogenerating migrations.\nSummary\nIn this chapter, we described what the principles behind Domain-Driven Design \nare, to orient the abstraction of storing data and use rich objects that follow business \nprinciples. We also described ORM frameworks and how they can be useful to \nremove the need to deal with low-level interaction with specific libraries to work \nwith the storage layer. We described different useful techniques for the code to \ninteract with the database, like the Unit of Work pattern, which is related to the \nconcept of a transaction, and CQRS for advanced cases where the write and read are \naddressed to different backends.\nWe also discussed how to deal with database changes, both with explicit migrations \nthat change the schema and with more soft changes that migrate the data as the \napplication is running. \nFinally, we described different methods to deal with legacy databases, and how to \ncreate models to create a proper software abstraction when there's no control over \nthe current schema of the data.\nThis is a very simplified method. As we discussed above, there \nare some elements that can be difficult to replicate. Changes to \nthe external database to solve incompatibility problems may be \nrequired.\nOn the other hand, sometimes it can be okay to live with small \ndifferences that are not creating any problems. For example, a \ndifferent name in the primary key index may be something that can \nbe acceptable and fixed later. Normally, these kinds of operations \nrequire a long time to be totally completed from a complex schema. \nPlan accordingly and do it in small increments.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "The Data Layer\n[ 144 ]\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "To be able to produce successful designs, it's not necessary to start from scratch. \nInstead, your efforts are better put into understanding which common architectural \npatterns have already been proven successful.\nIn this section of the book, we will see different ideas that are common across a lot of \nsuccessful systems. All these elements are useful in specific contexts, and we will see \nwhat their strengths and limitations are over the following chapters:\n1.\t The Twelve-Factor App Methodology, explaining this methodology\n2.\t Web Server Structures, describing how to deal effectively with response-\nrequest services\n3.\t Event-Driven Structure Basics, introducing how to work with events and \ncommunicate different services with them\n4.\t Advanced Event-Driven Structures, for creating complex flows of \ninformation, priorities, and CQRS\n5.\t Microservices vs Monolith, explaining the differences between them and the \ntools for dealing with them\nWe will introduce you to the Twelve-Factor App methodology, as it contains a list \nof useful suggestions for dealing with the specifics of services. We will also get into \nthe specifics of web server request-response structures, which are normally the \nfoundation of servers. \nPart II\nArchitectural \nPatterns\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "We will also cover event-driven systems, taking two chapters to be sure to cover the \nbasic and more advanced uses. Event-driven systems are asynchronous by nature, \nmeaning that the calling system won't wait until the processing is done, and in a lot \nof cases, there won't even be something similar to a response. These systems are very \nuseful for dealing either with triggering multiple services with the same input or for \ngenerating results that take a long time to process. \nWe'll also discuss monolithic systems compared with microservices, and the different \ntools and techniques to use in both cases, including migrating from one to the other.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 755,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "[ 147 ]\n5\nThe Twelve-Factor \nApp Methodology\nWhen designing a software system, it's not a good idea to reinvent the wheel each \ntime for each new project. Certain parts of software are common to most web service \nprojects. Learning some of the known practices that have proven successful over \ntime is important to avoid making easily fixed mistakes.\nIn this chapter, we will focus on the Twelve-Factor App methodology. This \nmethodology is a series of recommendations that are well proven for web services \nthat are deployed on the web.\nWe will present the base details for this methodology during the chapter and will \nspend some time describing in more detail some of the most important factors that \nthis methodology covers.\nThe Twelve-Factor App has its origins in Heroku, a company that \nprovides easy access to deployments. Some of the factors are more \ngeneral than others, and everything should be considered general \nadvice and not necessarily an imposition. The methodology is less \napplicable outside of web cloud services, but it's still a good idea to \nreview it and try to extract useful information.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1217,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 148 ]\nIn this chapter, we'll cover the following topics:\n•\t\nIntro to the Twelve-Factor App\n•\t\nContinuous Integration\n•\t\nScalability\n•\t\nConfiguration\n•\t\nThe Twelve factors\n•\t\nContainerized Twelve-Factor Apps\nLet's start by introducing the basic concepts of the Twelve-Factor App.\nIntro to the Twelve-Factor App\nThe Twelve-Factor App is a methodology with 12 different aspects or factors that \ncover good practices to follow while designing a web system. They aim to provide \nclarity and simplify some of the possibilities, detailing patterns that are known to \nwork.\nThe factors are generic enough to not be prescriptive in how to implement them or \nforce the use of specific tools, and at the same time, give clear direction. The Twelve-\nFactor App Methodology is opinionated in the sense that it aims to cover cloud \nservices in a scalable way, and also promotes the idea of Continuous Integration \n(CI) as a critical aspect of these kinds of operations. This also leads to a reduction \nin the differences between a local, development environment and a production \nenvironment. \nThese two aspects, consistency between local and production deployments, and \nCI, interact, as it allows the system to be tested in a consistent way, both in a \ndevelopment environment and while running the tests in a CI system.\nScalability is another key element. As cloud services require working with a variable \nworkload, we need to allow our service to be capable of growing and be able to \nprocess more requests coming into the system without any issues.\nA third general problem that we will cover, and which is also central to the Twelve-\nFactor App, is the challenge of configuration. Configuration allows the same code \nto be set up in different environments, while also tweaking some features to adjust \nthem in certain situations.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "Chapter 5\n[ 149 ]\nContinuous Integration\nContinuous Integration, or CI, is the practice of automating the running of tests when \nnew code is submitted to a central repository. Whereas, when originally introduced \nback in 1991, it could be understood as running a \"nightly build\", as running the tests \ntook time and was expensive, these days, it is commonly understood as running a set \nof tests with each new code submission.\nThe objective is to produce code that always works. After all, if it's not, it is detected \nquickly by the failing tests. This fast feedback loop helps developers to increase their \nspeed and create a safety net that allows them to focus on whatever feature they are \nimplementing and leave it to the CI system to run the totality of tests. The discipline \nof running the tests automatically and on every single test greatly helps to ensure \nhigh-quality code, as any error is detected quickly.\nThis is also dependent on the quality of the tests that are run, so in order to have a \ngood CI system, it is important to understand the importance of good tests and to \nrefine the testing procedure regularly, both to ensure that it gives us an adequate \nlevel of confidence and that it runs fast enough not to cause a problem.\nCI is based on the capacity of automating whatever system is used as a central \nrepository for code, so tests are launched as soon as new changes are forthcoming \nfrom a developer. It is very common to use a source control system like git, and add \na hook that automatically runs the tests.\nIn a more practical approach, git is normally used under a cloud system like GitHub \n(https://github.com/) or GitLab (https://about.gitlab.com/). Both of them have \nother services that integrate with them and allow operations to be run automatically \nthrough some configuration. Examples include TravisCI (https://www.travis-ci.\ncom/) and CircleCI (https://circleci.com/). In the case of GitHub, they have their \nown native system called GitHub Actions. All of these are based on the idea of \nadding a special file to configure the service, thereby simplifying the setup and run \nof a pipeline.\nFast enough, when dealing with a CI system can vary. Keep in \nmind that the tests will run in the background, automatically, \nwithout the involvement of a developer, so they may take a \nwhile to return a result, compared with the quick feedback that \na developer will expect when debugging a problem. As a very \ngeneral approximation, aim to have your test pipeline finished in \naround 20 minutes or less, if that is possible.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2665,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 150 ]\nA CI pipeline is a succession of steps that are run in order. If there's an error, it will \nstop the execution of the pipeline and report whatever problem has been detected, \nallowing for early detection and feedback for developers. Typically, we build the \nsoftware into a testable state and then run the tests. If there are different kinds of \ntests, such as unit and integration tests, run them both, either one after the other or \nin parallel.\nA typical pipeline to run tests could do the following:\n1.\t As it starts in a new, empty environment, install the required dependency \ntools to run the tests; for example, a particular version of Python and a \ncompiler, or a static analysis tool that will be used in step 3.\n2.\t Perform any build command to prepare the code, such as compiling or \npacketizing.\n3.\t Run static analysis tools like flake8 to detect style problems. If the results \nreveal problems, stop here and report.\n4.\t Run the unit tests. If the results are incorrect, stop here and show the errors.\n5.\t Prepare and run other tests, such as integration or system tests.\nThese stages may be run, in certain cases, in parallel. For example, steps 3 and 4 may \nrun at the same time as there is no dependency between the cases, whereas step 2 \nneeds to be completed before moving on to step 3. These steps can be described in \nsome CI systems to allow for faster execution.\nThe keyword in a CI pipeline is automation. To allow the pipeline to be executed, all \nthe steps need to be able to be run automatically, without any manual intervention. \nThis requires that any dependency is also able to be set up automatically. For \nexample, elements like databases or other dependencies, if required for tests, need to \nbe allocated. \nA common pattern is that CI tools allocate a virtual machine \nthat allows a database to start up so that it's available in the \nenvironment, including the usual suspects such as MySQL, \nPostgreSQL, and MongoDB. Keep in mind that the database \nwill start empty, and if test data needs to be seeded, it will need \nto be done during the setting up of the environment. Check the \ndocumentation for your specific tool for more details.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2316,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "Chapter 5\n[ 151 ]\nOne possibility is to use Docker to build one or more containers that will standardize \nthe process and make all dependencies explicit in the building process. This is \nbecoming an increasingly common option.\nSome of the factors of the Twelve-Factor App play a part in the setup of a CI \npipeline, as they aim to have code that is easy to build, to be deployed either for \ntesting or operating and configuration.\nScalability\nCloud systems are expected to behave correctly under high loads, or at least to adjust \nbetween different loads. This requires the software to be scalable. Scalability is the \nability of the software to be allowed to grow and accept more requests, mostly by \nincreasing resources.\nThere are two types of scalability:\n•\t\nVertical scalability: Increasing resources to each node, making them more \npowerful. This is the equivalent of buying a more powerful computer; \nadding more RAM, more hard drive space, a faster CPU…\n•\t\nHorizontal scalability: Adding more nodes to the system, without them being \nnecessarily more powerful. For example, instead of having two web servers, \nincrease them to five.\nIn general, horizontal scalability is considered more desirable. In a cloud system, the \ncapacity of adding and removing nodes can be automated, allowing for deployments \nto adjust automatically based on the number of current requests flowing into the \nsystem. Compared with the traditional way of operating, where the system had to be \ndimensioned for the moment of maximum system load, this can greatly reduce costs \nsince, most of the time, the system will be underutilized.\nFor example, let's compare a situation where, at noon, the system requires 11 \nservers, when most customers are connected. At midnight, the system is at its lowest \nutilization point, and only 2 servers are required. \nWe will talk more about Docker in Chapter 8, Advanced Event-\nDriven Structures.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 152 ]\nThe following diagram shows a typical situation when the number of servers grows \nbased on the load:\nFigure 5.1: Service scaling up and down over time\nThe traditional situation will make use of 264 cost units (11 servers * 24 hours), \nwhile automatically scaling uses approximately 166 cost units, saving a considerable \nnumber of resources. \nEven more so, a traditional system requires extra headroom to allow for unexpected \nspikes that could occur. Normally, a system will be set up to allow at least a 30% \nextra load, maybe even more. In that case, the cost is permanently added.\nTo allow a system to be horizontally scalable, it needs to be stateless. This means that \neach node is indistinguishable. Each request will be allocated to a node in some sort \nof rotation, distributing the load across all nodes. All state from each request needs \nto come either from the request itself (input parameters) or from an external storage \nsource. From the point of view of the application, each request comes in an empty \nspace and cannot be carried over in any event. That means not storing anything in \nthe local hard drive or local memory between requests.\nThe external storage source will typically be a database, but it's also common to use \nstorage services more oriented to store files or other big blobs of binary data, for \nexample, AWS S3. \nStoring information intra-request, for example, composing a file \nwith information from the database to return it in the request is \nOK, although keeping it in memory, if possible, will likely be faster \nthan using the hard drive.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "Chapter 5\n[ 153 ]\nA cache should also be kept outside of each individual node, using tools such as \nRiak or memcached. Internal caches, using local memory, have the problem that they \nlikely won't be used, as the next relevant request will likely be served by another \nnode in the system. Using an external service allows all nodes to access the cache and \nimproves the general performance of the system.\nKeep in mind that the whole system cannot be stateless. In particular, the storage \nelements, such as databases and caches, require a different way of operating, as they \nare the ones storing the data. We discussed how to scale storage systems in Chapter 3, \nData Modeling.\nConfiguration\nOne of the basic ideas of the Twelve-Factor App is that the code is unique, but it \ncan be adjusted through configuration. This enables the same code to be used and \ndeployed in different environments.\nThe use of different environments allows testing environments to be set up, where \ntests can be run without affecting production data. They are a more controlled \nplace for experimenting or trying to replicate real problems in a sandbox. There's \nalso another environment that is not typically thought of as such, which is the local \ndevelopment environment, where developers are able to check that the system \nworks.\nAWS S3 is a web service that allows a file to be stored and retrieved \nfrom a URL. It allows the creation of a bucket, which will contain \na number of keys or paths; for example, accessing a URL similar \nto https://s3.amazonaws.com/mybucket/path/to/file so it \ncan upload and download file-like objects. There are also plenty of \nlibraries to help deal with the service, such as boto3 for Python.\nThis service is very useful for working with files in a scalable way, \nand it allows configuration in such a way that access for reading \ncan be done publicly, enabling the pattern of storing the data \nthrough your system, and then allowing the user to read it from \nthe public URL, thereby simplifying the system.\nRefer to the AWS documentation for more information: https://\naws.amazon.com/s3/\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 154 ]\nConfiguring the system is more difficult than it appears at first sight. There's always \na growing number of parameters to take care of. In complex systems, it is important \nto structure parameters in certain ways to allow them to be divided into more \nmanageable parts.\nConfiguration parameters can be divided into two main categories: \n•\t\nOperational configuration: These are parameters that connect different parts \nof the system or that are related to monitoring; for example, the address \nand credentials for the database, the URL to use to access an external API, \nor setting the level of logging to INFO. These config parameters are only \nchanged when there's a change in the cluster, but the external behavior of \nthe application doesn't change; for example, a change to log only WARNING \nlogs or higher, or the credentials are replaced to rotate them.\n•\t\nThese parameters are under the control of operations and are normally \nchanged transparently or during maintenance. A misconfiguration in these \nparameters is normally a serious problem as it can affect the functionality of \nthe system.\n•\t\nFeature configuration: These parameters change external behavior, enabling \nor disabling features or changing aspects of the software; for example, \ntheming parameters to set the color and header images; or enabling the \npremium feature to allow a charge for premium access, or updating \nthe parameters of a mathematical model that changes how the internal \ncalculation of orbits are performed.\nThese parameters are irrelevant as regards the operation of the software. A \nmisconfiguration here will likely not cause problems, as it will continue to \noperate normally. Changes here are more under the control of developers \nor even business managers to enable a feature at a particular point in time.\nCreating a comprehensive and easy-to-use local environment is \na critical aspect of developer productivity. When working with a \nsingle service or process, such as a web server, it is relatively easy \nto set up, as most projects will allow starting in a dev mode, but \nonce there are more elements, it becomes more difficult to set up.\nComplex settings have been quite common for years. There has \nbeen a relatively recent push to use virtual machines that could be \nset up from scratch, and more recently, containerization to ensure \nthat it's easy to start it from a known point.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2529,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "Chapter 5\n[ 155 ]\nThese two categories have different aims and, normally, are maintained by different \npeople. While the operational configuration parameters are tightly related to a single \nenvironment and require parameters that are correct for the environment, the feature \nconfiguration normally moves between the local development to test it until it is \nchanged in the production one with the same value.\nTraditionally, the configuration has been stored in one or more files, typically \ngrouped by environment. This creates a file called production.cnf and another \none called staging.cnf that are attached to the code base, and depending on the \nenvironment, one or the other is used. This entails certain problems:\n•\t\nMaking a configuration change is, de facto, a code change. This limits the \nspeed of changes that can be performed and cause problems with scope.\n•\t\nWhen the number of environments grows, the number of files grows at the \nsame time. This can cause errors as a result of duplication; for example, \na mistake that changes the wrong file is not reverted and is unexpectedly \ndeployed later. Old files may also not be removed.\n•\t\nCentralizing control among developers. As we've seen, some of these \nparameters are not necessarily under the control of developers, but ops \nteams. Storing all the data in the code base makes it more difficult to create \na division between jobs, requiring both teams to access the same files. \nWhile this is fine for small teams, over time, it makes sense to try to reduce \nthe need to have big groups of people accessing the same file to only care \nabout half of it.\nConfiguration parameters that aim to activate or deactivate a full \nfeature are known as feature flags. They are used to produce a \n\"business release\" at a particular time, deploying new code into a \nproduction environment without the feature, while the feature is \nbeing worked on internally. \nOnce the feature is ready for release, after thoroughly testing \nit, the code can be deployed beforehand in production, and the \nfull feature can be activated just by changing the proper config \nparameter.\nThis allows us to keep working in small increases toward a big \nfeature, such as a revamp of the user interface, while at the same \ntime building and releasing small increments frequently. Once \nthe feature is released, the code can be refactored to remove the \nparameter.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 156 ]\n•\t\nStoring sensitive parameters such as passwords in files and storing them in \nthe code repo is an obvious security risk, as anyone with access to the repo \ncan use these credentials to access all environments, including production.\nThese problems render it unadvisable to store the configuration directly as \nfiles inside the code base. We will see how the Twelve-Factor App deals with it \nspecifically in the Configuration factor.\nThe Twelve Factors\nThe factors for Twelve-Factor Apps are as follows:\n1.\t Code base. Store the code in a single repo and differentiate only by \nconfiguration.\n2.\t Dependencies. Declare them explicitly and clearly.\n3.\t Config. Config through the environment.\n4.\t Backing services. Any backing service should be treated as an attached \nresource.\n5.\t Build, release, run. Differentiate between build and run states.\n6.\t Processes. Execute the app as a stateless process.\n7.\t Port binding. Expose services through ports.\n8.\t Concurrency. Set up the services as processes.\n9.\t Disposability. Fast start and graceful shutdown.\n10.\t Dev/prod parity. All environments should be as similar as possible.\n11.\t Logs. Send logs to event streams.\n12.\t Admin processes. Run one-off admin processes independently.\nThe factors can be grouped around different concepts: \n•\t\nCode base, Build, release, run, and Dev/prod parity work around the idea \nof generating a single application that runs in different environments, \ndifferentiating only through configuration\n•\t\nConfig, Dependencies, Port binding, and Backing services work around the \nconfiguration and connectivity of different services\n•\t\nProcesses, Disposability, and Concurrency are related to the scalability concept\n•\t\nLogs and Admin processes are practical ideas involved with monitoring and \none-off processes\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "Chapter 5\n[ 157 ]\nLet's take a look at all four of these groups.\nBuild once, run multiple times\nOne of the key concepts around the Twelve-Factor App is that it's easy to build and \nmanage, but at the same time, it's a unified system. This means that there's no ad hoc \ncode that's changed from one version to another, just configurable options.\nThe aim of the Code base factor is that all the software for an app is a single repo, with \na single state, without special branches for each customer, or a special functionality \nthat's only available in a particular environment.\nThis means that the code to deploy is always the same, and only the configuration \nchanges. This allows easy testing of all the configuration changes and does not \nintroduce blind spots.\nNote that a single system may have multiple projects, living in multiple repos, that \nindividually fulfill the Twelve-Factor App and work together. Other factors talk \nabout interoperation on applications.\nA single code base allows a strict differentiation of the stages in the Build, release, run \nfactor. This factor ensures that there are three distinct stages: \n•\t\nThe build stage transforms the content of the code repo into a package or \nexecutable that will be run later\nVery specific environments are typically called snowflake \nenvironments. Anyone that has dealt with them knows how \npainfully difficult they are to maintain, and that's why the objective \nfor the Twelve-Factor App is to remove them, or at least make \nthem change based just on the configuration.\nKeeping multiple applications working together, through \ncoordinated APIs, is always a challenge and requires good \ncoordination across teams. Some companies adopt the monorepo \napproach, where there's a single repository with all the company \nprojects living in multiple subdirectories, to be sure that there's a \ncomplete view of the whole system and a single state across the \nwhole organization.\nThis also has its own challenges, and requires greater coordination \nacross teams and can present big challenges for big repos.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 158 ]\n•\t\nThe release stage uses this built package, combines it with the proper \nconfiguration for the selected environment, and sets it ready for execution\n•\t\nThe run stage finally executes the package in the selected environment\nBecause stages are strictly divided, it's not possible to change the configuration or the \ncode after the code is deployed. This requires a new release in any case. This makes \nthe releases very explicit, and each one should be executed independently. Note \nthat the run stage may need to be executed again in case there's a new server or the \nserver crashes, so the aim should be for this to be as easy to do as possible. As we are \nseeing, a common thread through the Twelve-Factor App is strict separation, so that \neach element is easy to recognize and to operate. We will check how to define the \nconfiguration in other factors.\nBecause of this strict separation, in particular, in the build stage, it's easy to follow \nthe Dev/prod parity. In essence, a development environment is the same as a \nproduction one, as they use the same building stage, but with proper configuration \nto run locally. This factor also makes it possible to use the same (or as close as \npossible) backing services, like databases or queues, to ensure that local development \nis as representative as a production environment. Container tools such as Docker, or \nprovisioning tools such as Chef or Puppet, can also help in automatically setting up \nenvironments that contain all the required dependencies.\nObtaining a fast and easy process to develop, build, and deploy is critical for \nspeeding up the cycle and adjusting quickly.\nAs we discussed previously, the configuration lives in a different \nplace to the code base. This separation makes sense, and it could be \nalso under source control. It may be stored as files, but the access \ncan then be separated by environment, something that makes \nsense, as some environments, like production, are more critical \nthan others. Storing the configuration as part of the code base \nmakes it difficult to perform that separation.\nKeep in mind that more than one file can be combined, allowing \nthe parameters to be separated into feature and operational \nconfigurations.\nPerforming tests after the build stage also ensures that the code \nremains without changes between the tests and the release and \noperation.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "Chapter 5\n[ 159 ]\nDependencies and configurations\nThe Twelve-Factor App advocates the explicit definition of dependencies and \nconfiguration and, at the same time, is opinionated in terms of how to do them and \nprovides solid standards that are proven.\nThat is why, in the Config factor, it talks about storing all the configuration for the \nsystem in environment variables. Environment variables are independent from \ncode, which allows retention of the strict differentiation that we talked about in the \nBuild, release, run factor and avoidance of the problems that we described previously \nin storing them in files inside the code base. They are also language- and OS-\nindependent, and easy to work with. Injecting environment variables into a new \nenvironment is also easy.\nThis is preferred to other alternatives, such as setting different files into the code \nbase describing environments like staging or production, because they allow more \ngranularity, and because this kind of handling ends up creating too many files and \nchanging the code for environments that are not affected; for example, having to \nupdate the code base for a demo environment that is short-lived.\nConfiguration can be obtained in configuration files directly from the environment \nusing standard libraries; for example, in Python:\nimport os\nPARAMETER = os.environ['PATH']\nThis code will store in the constant PARAMETER the value of the PATH environment \nvariable. Be careful as the lack of a PATH environment variable will generate a \nKeyError as it won't be present in the environ dictionary. \nAlthough the Twelve-Factor App encourages dealing with \nconfigurations in a variable-independent way, the reality of the \nwork means that there are a limited number of environments and \ntheir configuration should be stored somewhere. The key element \nis storing it in a different place to the code base, managed only on \nthe release stage. This allows plenty of flexibility.\nKeep in mind that for local development, these environment \nvariables may need to be changed independently to test or debug \ndifferent features.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2199,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 160 ]\nTo allow for optional environment variables, and protect against them going \nmissing, use .get to set up a default value:\nPARAMETER = os.environ.get('MYENVVAR', 'DEFAULT VALUE')\nNote that environment variables are always defined as text. If the value needs to be \nin a different format, it needs to be converted, for example:\nNUMBER_PARAMETER = int(os.environ['ENVINTEGERPARAMETER'])\nThis presents a common problem when defining a Boolean value. Defining this \ntranslation code as follows is incorrect:\nBOOL_PARAMETER = bool(os.environ['ENVBOOLPARAMETER'])\nIf the value of ENVPARAMETER is \"TRUE\", the value of BOOL_PARAMETER is True (Boolean). \nBut if the value of ENVPARAMETER is \"FALSE\", the value of BOOL_PARAMETER is also True. \nThis is because the string \"FALSE\" is a non-empty string and gets converted into True. \nInstead, the standard library package, distutils, can be used:\nimport os\nfrom distutils.util import strtobool\nBOOL_PARAMETER = strtobool(os.environ['ENVBOOLPARAMETER'])\nFor the following examples, keep in mind that the defined \nenvironment variables need to be defined in your environment. \nThese definitions are not included, to simplify the explanation. \nYou can run Python, adding a local environment, by running $ \nMYENVVAR=VALUE python3.\nAs a general recommendation, it's better to raise an exception \nbecause there's a missing configuration variable than to continue \nwith a default parameter. This makes configuration problems \neasier to spot, as it will stop when the process starts, failing loudly. \nRemember, following the Twelve-Factor App ideas, you want to \ndescribe things explicitly and any problem should fail as early as \npossible in order to be able to fix it correctly instead of passing \nwithout detection.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1891,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "Chapter 5\n[ 161 ]\nEnvironment variables also allow the injection of sensitive values such as secrets \nwithout storing them in the code base. Keep in mind that the secret will be available \nto inspect in the environment of the execution, but typically that's protected so only \nauthorized team members can access it through ssh or similar in the environment. \nAs part of this configuration, any backing services should be defined as well as \nenvironment variables. Backing services are external services that the app uses over \nthe network. They could be databases, queues, caching systems, or suchlike. They \ncan be local to the same network or external services, such as APIs handled by an \nexternal company or AWS services.\nFrom the point of view of the app, this differentiation should be irrelevant. \nThe resources should be accessed by a URI and credentials, and, as part of the \nconfiguration, can be changed based on the environment. This makes the resources \nloosely coupled, and means they can be replaced easily. If there is a migration \nand the database needs to be moved between two networks, we can start the new \ndatabase, perform a new release with a configuration change, and the app will point \nto the new database. This can be done with no code changes.\nTo allow the concatenation of multiple applications, the Port binding factor ensures \nthat any service exposed is a port, which may be different depending on the service. \nThis makes it easy to consider each app a backing service. Preferably, it should be \nexposed in HTTP as this makes it very standard to connect to.\nstrtobool returns not True or False as Booleans, but 0 or \n1 as integers. This normally works correctly, but if you need \nstrict Boolean values, add bool like this: bool(strtobool(os.\nenviron['ENVPARAMETER']))\nFor applications, use HTTP over port 80 when possible. This makes \nall connections easy with URLs such as http://service-a.\nlocal/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2034,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 162 ]\nSome applications require the combination of several processes working in \nconjunction. For example, it is typical for a web server for a Python application, such \nas Django, to use an application server like uWSGI to run it, and then a web server \nlike nginx or Apache to serve it and the static files.\nFigure 5.2: Connecting a web server and application server\nThey all connect by exposing a known port and protocol, which makes the setup \neasy.\nOn the same note, for clarity, all library dependencies should be explicitly set up and \nnot rely on the pre-installation of certain packages in the existing operating system. \nThe dependencies should be described through a dependency declaration, like a \nrequisites.txt pip file for Python. \nDependencies should then be installed as part of the build stage, with commands \nsuch as pip install -r requirements.txt.\nEven more so, dependencies should be isolated to ensure that there are no implicit \ndependencies that are not tightly controlled. Dependencies should also be defined as \ntightly as possible, to avoid the problem of different versions of dependencies being \ninstalled if new versions are released upstream.\nFor example, in a pip file, a dependency can be described in different ways:\nrequests\nrequests>=v2.22.0\nrequests==v2.25.1\nKeep in mind that the specific Python version is also a dependency \nthat should be controlled tightly. The same is true of other required \nOS dependencies. Ideally, the OS environment should be created \nfrom scratch with the dependencies specified.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1683,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Chapter 5\n[ 163 ]\nThe first way accepts any version, so it will typically use the latest. The second \ndescribes a minimum (and optionally maximum) version. The third version pins a \nspecific version.\nUsing very explicit dependencies makes the builds repeatable and deterministic. It \nensures a lack of unknown changes during the build stage because a new version has \nbeen released. While most new packages will be compatible, it may also sometimes \nintroduce changes that affect the behavior of the system. Even worse, those changes \nwill be introduced inadvertently, causing severe problems.\nScalability\nWe talked earlier in the chapter about the why's of scalability. The Twelve-Factor \nApp also talks about how to successfully grow or reduce the system.\nThe Processes factor talks about making sure that the run stage consists of starting \none or more processes. These processes should be stateless and share nothing, \nmeaning that all the data needs to be retrieved from an external backing service like \na database. A temporal local disk can be used for temporal data within the same \nrequest, although their use should be kept to a minimum.\nThe next property that processes need to fulfill is their disposability. The processes \nneed to be able to be started and stopped quickly, and at any time. \nStarting quickly allows the system to react quickly to releases or restarts. The aim \nshould be to take not more than a few seconds to have the process up and running. \nQuick turnaround is also important to allow rapid growth of the system in case \nmore processes are being added for scale purposes.\nThis is equivalent to other package management systems in \noperative systems, like apt in Ubuntu. You can install a specific \nversion with apt-get install dependency=version.\nFor example, a file upload may use the local hard drive to store \na temporal copy and then process the data. After the data is \nprocessed, the file should be deleted from the disk.\nIf possible, try to use memory for this temporal storage as it will \nmake this distinction more strict.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2166,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 164 ]\nThe opposite is to allow the graceful shutdown of the process. This can be required \nfor scale-down situations, to be sure that any request is not interrupted in this case. \nBy convention, processes should be stopped by sending the SIGTERM signal.\nFor example, for a web request, a graceful shutdown first will curtail the acceptance \nof any new requests, will finish any requests in the queue, and finally, will shut \ndown the process. Web requests are typically quick to answer, but for other \nprocesses, such as long asynchronous tasks, it may take a long time to stop if they \nneed to finish the current task.\nInstead, long task workers should return the job to the queue and cancel the \nexecution. This way, the task will be performed again, and to ensure that this doesn't \nduplicate actions, we need to ensure that all tasks can be canceled by waiting until \nthe end of it to save its results and wrapping them into a transaction or similar.\nProcesses should also be robust against unexpected stoppages. These stoppages \ncould be caused by bugs, hardware errors, or, in general, unexpected surprises that \nalways appear in software. Creating a resilient queue system that can retry in case a \ntask is interrupted will help greatly in these instances.\nBecause the system is created through processes, based on that, we can scale out by \ncreating more of them. Processes are independent and can be run at the same time on \nthe same server or others. This is the basis of the Concurrency factor. \nWorking with Docker containers automatically uses this \nconvection by sending a SIGTERM signal to the main process \nwhenever the container needs to be stopped. If the process doesn't \nstop itself after a grace period, it will be killed instead. The grace \nperiod can be configured if necessary.\nBe sure that the main process for the container can receive SIGTERM \nand deal with it properly to ensure a graceful stopping of the \ncontainer.\nIn some cases, it may be necessary to distinguish between the bulk \nof the preparation job and the saving of results part. We either \nwant to wait, if the job is saving the results at the time of shut \ndown, or stop execution and return the task to the queue. Some \nsave operations may require calling systems that don't accept \ntransactions. The acceptable time for shutting down long-running \nprocesses may be longer than for web servers.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "Chapter 5\n[ 165 ]\nKeep in mind that the same application can use multiple processes that coordinate \namong them to handle different tasks and each process may have a different number \nof copies. In our previous example above, with an nginx server and uWSGI one, \nthe optimal number may be to have a single nginx process for many more times the \nnumber of uWSGI workers.\nAdding more nodes, as they are independent and stateless, becomes an easy \noperation under a Twelve-Factor App. That allows the size of the entire operation to \nbe adjusted to the load of the system. This can be a manual operation, to slowly add \nnew nodes as the system grows in load and requests, or it can be done automatically, \nas we described earlier in the chapter.\nThe Twelve-factor App processes should also be run by some sort of operating \nsystem process manager, like upstart or systemd. These systems ensure that the \nprocesses remain running, even in the event of a crash, handle graceful manual \nrestarts, and also manage output streams gracefully. We will talk more about output \nstreams as part of logs.\nThe traditional deployment process was to set up a physical server \n(or virtual machine) for a node and fit a number of elements, which \nnormally included tailoring the number of workers until finding \nthe optimal figure to make proper use of the hardware. \nWith containers, this process is somehow reversed. Containers \ntend to be more lightweight and more can be created. While the \noptimization process is still required, with containers, it's more \nabout creating a unit and then checking how many of them a single \nnode can fit, as containers can be moved around nodes more easily, \nand the resulting apps tend to be smaller. Instead of finding out \nwhat is the proper size of the application for a given server, we \nfigure out how many copies of a small application fit in a server, \nknowing that we can use different server sizes or add more servers \nwith ease.\nThe Twelve-Factor App doesn't demand that this scale is done \nautomatically, but definitively enables it. Automating the \nadjustment should be treated with care, as it requires careful \nmetrics on the load of the system. Allow time to perform tests to \nmake the proper adjustments.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2343,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 166 ]\nRestarting the processes automatically, combined with a quick start up time and \nresilience in shutdown situations, makes the app dynamic and capable of self-\nrepairing in case there is an unexpected problem that causes a process to crash. It \nalso allows controlled shutdowns to be used as part of a general operation to avoid \nlong-running processes and act as a contingency plan for memory leaks or other \nkinds of long-running problems.\nMonitoring and admin\nA comprehensive monitoring system is important for detecting problems and \nanalyzing the operation of the system. While it's not the only monitoring tool, logs \nare a critical part of any monitoring system.\nLogs are text strings that provide visibility of the behavior of a running app. \nThey should always include a timestamp on when they were generated. They are \ngenerated as the code is being executed, giving information on the different actions \nas they happen. The specifics about what to log can vary significantly by application, \nbut typically frameworks will automatically create logs based on common practices.\nFor example, any web-related software will log requests received, something like \nthis, for example:\n[16/May/2021 13:32:16] \"GET /path HTTP/1.1\" 200 10697\nNote that it includes:\n•\t\nA timestamp for when it was generated [16/May/2021 13:32:16]\n•\t\nThe HTTP GET method and the HTTP/1.1 protocol\nWhen working with containers, this changes a bit, as the equivalent \nis mostly handling containers more than processes. Instead of an \noperating system process manager, the work is performed by a \ncontainer orchestrator that ensures that the containers are running \nproperly and capturing any output stream. Inside the container, \nthe processes can start without being under the control of a \nmanager. The container will stop if the process is stopped.\nThis is equivalent to the old trick of turning it off and then turning \nit back on! If it can be done very quickly, it can save a lot of \nsituations!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "Chapter 5\n[ 167 ]\n•\t\nThe accessed path – /path\n•\t\nThe returned status code – 200\n•\t\nThe size of the request – 10697\nThis kind of log is called an access log and will be generated in different formats. \nAt the very least, it should always include the timestamp, HTTP method, path, and \nstatus code, but it can be configured to return extra information, such as the IP of the \nclient making the request, or the time that it took to process the request.\nAccess logs are not the only useful ones. Application logs are also very useful. \nApplication logs are generated inside the code and can be used to communicate \nsignificant milestones or errors. Web frameworks prepare the logs, so it's easy to \ngenerate new ones. For example, in Django, you can create logs this way:\nimport logging\nlogger = logging.getLogger(__name__)\n...\ndef view(request, arg):\n    logger.info('Testing condition')\n    if something_bad:\n        logger.warning('Something bad happened')\nThis will generate logs like these:\n2021-05-16 14:01:37,269 INFO Testing condition\n2021-05-16 14:01:37,269 WARNING Something bad happened\nAccess logs are also generated by web servers including nginx \nand Apache. Configuring them properly to adjust the information \nproduced is important for operational purposes.\nWe will get into more details about logs in Chapter 11, Package \nManagement.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 168 ]\nThe Logs factor suggests that logs shouldn't be managed by the process itself. Instead, \nlogs should be printed in their own standard output without any intermediate \nstep. The environment surrounding the process, like the operating system process \nmanager described in the Concurrency factor, should be charged with receiving \nthe logs, combining them, and routing them properly to a long-term archival and \nmonitoring system. Note that this configuration is totally out of the application's \ncontrol. \nThis is in contrast to storing the logs as log files in the hard drive. This has the \nproblem of requiring the logs to be rotated and ensure that there's enough space. \nThis also requires the different processes to coordinate in terms of having a similar \npolicy for log rotation and storage. Instead, standard outputs can be combined and \naggregated together for a whole image of the system, and not a single process.\nThe logs can also be directed toward an external log indexing system, such as \nthe ELK Stack (Elasticsearch, Kibana, and Logstash: https://www.elastic.co/\nproducts/), which will capture logs and provide analytic tools to search through \nthem. External tools are also available, including Loggly (https://www.loggly.com/) \nor Splunk (https://www.splunk.com/) to avoid maintenance. All these tools allow \nstandard output logs to be captured and redirected to their solutions.\nThese other tools can provide capabilities like searching and finding specific events \nin a particular time window, observing trends such as changes in the number of \nrequests per hour, and even creating automatic alerts based on certain rules, such \nas an increase in the number of ERROR logs over a period of time over and above a \ncertain value.\nThe Admin processes factor covers some processes that sometimes need to be run for \nspecific operations, but are not part of the app's normal operation. Examples include \nthe following:\n•\t\nDatabase migrations\nFor local development, just showing the logs in a terminal may be \nenough for development purposes. \nIn the container world, this recommendation makes even more \nsense. Docker orchestration tools can easily capture the standard \noutput from the containers and then redirect them to somewhere \nelse. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2398,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "Chapter 5\n[ 169 ]\n•\t\nThe production of ad hoc reports, such as generating a one-off report for \ncertain sales or detecting how many records are affected by a bug\n•\t\nRunning a console for debugging purposes\nThese operations are not part of the day-to-day operation, but may need to be run. \nThe interface is clearly different. To execute them, they should run in the same \nenvironment as the regular processes, using the same code base and configuration. \nThese admin operations should be included as part of the code base to avoid \nproblems with mismatched code.\nIn traditional environments, it may be necessary to log in to a server through ssh to \nallow the execution of this process. In container environments, a full container can be \nstarted exclusively to execute the process.\nThis is very common in cases of migrations, for example. A preparation command \nmay consist of running the build to execute migrations. \nTo run these admin commands in containers, the container image should be the same \none that runs the application, but called with a different command, so the code and \nenvironment are the same as in the running application. \nContainerized Twelve-Factor Apps\nAlthough the Twelve-Factor App methodology is older than the current trend \ntoward containerization using Docker and related tools, it's very aligned. Both tools \nare oriented toward scalable services in the cloud, and containers help to create \npatterns that match the ones described in the Twelve-Factor methodology.\nExecuting commands in a console in a production environment \nshould be used only when no other alternative is available, and \nnot as a way of removing the need to create specific scripts for \nrecurring operations. Extreme caution should apply. Keep in mind \nthat an error in a production environment can create a serious \nproblem. Treat your production environment with the proper \nrespect.\nThis should be done before the actual release, to ensure that \nthe database is migrated. Refer to Chapter 4 for more details on \nmigrations.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2130,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 170 ]\nThe most important, arguably, is the fact that the creation of an invariant container \nimage that then gets run works very well with the Build, release, run factor and with \nbeing very explicit with Dependencies, as the whole image will include details such \nas the specific OS to use and any library. Including the build process as part of the \nrepository also helps in the implementation of the Code base factor. \nEach container also works as a Process, which allows scaling by creating multiple \ncopies of the same container, using the Concurrency model.\nThe concept of containers makes them easy to start and stop, leaning into the \nDisposability factor, and connecting one to another through an orchestration tool such \nas Kubernetes makes it easy to also set up the Backing services factor, and it's also \neasy to share services between specific ports in containers following the Port binding \nfactor. In most cases, however, they'll be shared as web interfaces on the standard \nport 80.\nIn Docker and orchestrator tools like Kubernetes, it is very easy to set up different \nenvironments injecting environment variables, thereby fulfilling the Configuration \nfactor. This environment configuration, as well as a description of the cluster, can \nbe stored in files, which allow multiple environments to be created easily. It also \nincludes tools for handling properly secrets, so they are properly encrypted and are \nnot stored in the configuration files to avoid leaking secrets.\nAnother critical advantage of containers is the fact that a cluster can be replicated \neasily locally, as the same image that runs in production can run in a local \nenvironment, with only small changes in its configuration. This helps greatly in \nensuring that the different environments are kept up to date, as demanded by the \nDev/Prod parity factor. \nWe will talk more about Docker containers in Chapter 8, Advanced \nEvent-Driven Structures.\nWhile containers are usually thought of conceptually as \nlightweight virtual machines, it's better to think of them as a \nprocess wrapped in its own filesystem. This is closer to the way \nthey operate.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2276,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "Chapter 5\n[ 171 ]\nSending information to standard output as per the Logs factor is also a great way \nto store logs as container tools will receive and deal with or redirect those logs \nadequately.\nFinally, the Admin processes can be handled by launching the same container image \nwith a different command that runs the specific admin command. This can be \nhandled by the orchestrator if it needs to happen regularly, such as running the \nmigrations prior to a deployment, or if it's a periodic task.\nAs we can see, working with containers is a great way of following the \nrecommendations for the Twelve-Factor App, as the tools work in the same direction. \nThis doesn't mean that they are done for free, but that there's a significant degree of \nalignment between the methodology and the ideas behind containers.\nThis is not surprising as both come from a similar background, dealing with web \nservices that need to be run in the cloud.\nSummary\nIn this chapter, we saw that it's good to have solid and reliable patterns to build \nsoftware to be sure that we stand over the shoulder of tested decisions that we \ncan use to shape new designs. For web services living in the cloud, we can use the \nTwelve-Factor App methodology as a guideline for a lot of useful advice.\nWe discussed how the Twelve-Factor App is aligned with two main ideas – CI and \nscalability.\nCI is the practice of constantly validating any new code by running tests automatically \nafter the code is shared. This creates a safety net that enables developers to move \nquickly, although it requires discipline to properly add automated tests as new \nfeatures are being developed.\nIn general, the container approach works toward defining a cluster \nand instigating a clear separation between different services and \ncontainers in a consistent manner. This brings together different \nenvironments, as the development environment can replicate the \nproduction setup on a small scale.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "The Twelve-Factor App Methodology\n[ 172 ]\nWe also discussed the concept of scalability, or the capacity for software to allow \nmore load by adding more resources. We talked about why it is important to allow \nthe software to grow and reduce based on the load, even to the point to be able to \nadjust dynamically. We also saw how making the system stateless is key to achieving \nscalable software.\nWe saw the challenges for configuration, something that the Twelve-Factor App \nalso deals with, and how not every configuration parameter is equal. We described \nhow configuration can be divided into Operational configuration and Feature \nconfiguration, which can help divide and give the proper context to each parameter. \nWe went through each of the factors for the Twelve-Factor App, and divided them \ninto four different groups, relating them, and explaining how the different factors \nsupport each other. We divided the factors into groups: \n•\t\nBuild once, run multiple times, based on the idea of generating a single \npackage that runs in a different environment\n•\t\nDependencies and configuration, around the configuration and software and \nservice dependencies of the application\n•\t\nScalability, to achieve the scalability that we talked about before\n•\t\nMonitoring and admin with other elements to deal with the operation of the \nsoftware while in operation\nFinally, we spent some time talking about how the Twelve-Factor App ideas are very \nmuch in line with what containerization is about, and how different Docker features \nand concepts allow us to easily create Twelve-Factor Apps.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1850,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "[ 173 ]\n6\nWeb Server Structures\nWeb servers are the most common servers for remote access at the moment. Web \nservices based on HTTP are flexible and powerful.\nIn this chapter, we will see how web servers are structured, starting by describing \nhow the basic request-response architecture works, and then diving into a LAMP-\nstyle architecture in three layers: the web server itself, the workers executing \nthe code, and an intermediate layer that controls those workers and presents a \nstandardized connection to the web server.\nWe will describe each layer in detail, presenting a specific tool, such as nginx for the \nweb server, uWSGI for the intermediate layer, and the Python Django framework for \nthe specific code inside the worker. We will describe each of them in detail.\nWe will also include the Django REST framework, as it's a tool that builds on top of \nDjango to generate RESTful API interfaces.\nFinally, we will describe how extra layers can be added on top for greater flexibility, \nscalability, and performance.\nIn this chapter, we'll cover the following topics:\n•\t\nRequest-response\n•\t\nWeb architecture\n•\t\nWeb servers\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1236,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "Web Server Structures\n[ 174 ]\n•\t\nuWSGI\n•\t\nPython workers\n•\t\nExternal layers\nLet's start by describing the basis of the request-response architecture.\nRequest-response\nThe classical server architecture is heavily based on request-response to \ncommunicate. A client sends a request to a remote server and the server processes it \nand returns a response.\nThis communication pattern has been prevalent since the era of mainframes and \nworks in an analog manner as software communicates internally with a library, but \nover a network. The software calls a library and receives a response from it. \nAn important element is the time delay between the sending of the request and the \nreception of the response. Internally, it is rare that a call takes more than a couple of \nmilliseconds, but for a network, it may be measured in hundreds of milliseconds and \nseconds, very commonly.\nTimes will also be highly variable, as the network conditions may affect them \ngreatly. This time difference makes it important to handle it properly.\nThe usual strategy when making requests is to make them synchronously. That \nmeans that the code stops and waits until the response is ready. This is convenient, \nas the code will be simple, but it's also inefficient, as the computer will be not doing \nanything while the server is calculating the response and it's being transferred \nthrough the network.\nNetwork calls are very dependent on where the server is located. \nA call within the same data center will be fast, perhaps taking less \nthan 100 milliseconds, while a connection to an external API will \nlikely take close to a second or more.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "Chapter 6\n[ 175 ]\nThe fact that the network is more unreliable than a local call, requires better error \nhandling that understands this fact. Any request-response system should take extra \ncare about capturing different errors, and retry, as network problems typically are \ntransient, and can be recovered if retried after waiting.\nAnother characteristic of the request-response pattern is that a server cannot call \nthe client proactively, only return information. This simplifies the communication, \nas it's not entirely bidirectional. The client is required to initiate the request, and \nthe server only needs to listen for new requests coming. This also makes both roles \nasymmetrical and requires the client to know where the server is, usually by its DNS \naddress and the port to access (by default, port 80 for HTTP and 443 for HTTPS).\nThis characteristic makes some communication patterns difficult to achieve. For \nexample, full bidirectional communication, where two parts want to initiate the \nsending of messages, is difficult to achieve with request-response.\nThe client can be improved to perform multiple requests at the \nsame time. This can be done when the requests are independent \nof each other, allowing it to make them in parallel. An easy way to \nachieve this is to use a multithreaded system to perform them, so \nthey can speed up the process. \nTypically, a flow will be required, with some requests that can \nbe performed in parallel and others that require waiting until \ninformation is received. For example, a common request to retrieve \na web page will make one request to retrieve the page and later \nwill download multiple files referenced (e.g. header files, images) \nin parallel.\nWe will see later in the chapter how this effect can be designed to \nincrease the responsiveness of web pages.\nAs we saw in Chapter 2, API Design the multiple status codes from \nHTTP can give detailed information. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "Web Server Structures\n[ 176 ]\nA crude example of this is a message server implemented only in request-response. \nTwo clients require the usage of an intermediate server. \nEach user can perform two actions:\n•\t\nRequest any new message addressed to them\n•\t\nSend a new message to another user\nA user needs to check periodically whether there are new messages available \nthrough polling. This is inefficient, as it's likely that for any new message there'll be \na significant number of checks that return \"no new messages available.\" Even worse, \nthere could be a significant delay before noticing that a new message is available if \nthe checks are not performed often enough.\nEven with these limitations, request-response architecture is the basis of web services \nand has been proven to be very reliable over the decades. The possibility of having a \ncentral server that controls communication and can take a passive role in accepting \nnew requests makes the architecture simple to implement and quick to evolve, and \nsimplifies the client's work. The centralized aspect allows a lot of control.\nThis basic structure is common in applications like forums or \nsocial networks that allow the users to have some sort of direct \nmessaging between users.\nIn real applications, normally this polling is avoided by sending \na notification in a way that's proactive towards the client. For \nexample, mobile OSes have a system to deliver notifications, \nenabling the server to send a notification through an external API \nprovided by the OS to notify the user of a new message. An older \nalternative is to send an email with the same goal.\nThere are other alternatives, of course. There are P2P alternatives, \nwhere two clients can connect to each other, and there are \nconnections with a server through websockets that can remain \nopen, allowing the server to notify the user of new information. \nThey both deviate from the request-response architecture.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2045,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "Chapter 6\n[ 177 ]\nWeb architecture\nWe introduced in the introduction of the chapter the LAMP architecture, which is the \nbase for the web server architecture:\nFigure 6.1: The LAMP architecture\nThe LAMP architecture is more general, but we will take a closer look at the web \nserver and web worker. We will use specific tools, based on the Python ecosystem, \nbut we will discuss possible alternatives.\nFigure 6.2: More detailed architecture in a Python environment\nFrom the point of view of an incoming request, a web request accesses the different \nelements.\nWeb servers\nThe web server exposes the HTTP port, accepts incoming connections, and redirects \nthem towards the backend. One common option is nginx (https://www.nginx.com/). \nAnother common option is Apache (https://httpd.apache.org/). The web server \ncan directly serve a request, for example, by directly returning static files, permanent \nredirects, or similar simple requests. If the request requires more computation, it will \nbe directed towards the backend, acting as a reverse proxy.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1152,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "Web Server Structures\n[ 178 ]\nThe primary objective of the web server in the presented architecture is to work as a \nreverse proxy, accepting HTTP requests, stabilizing the input of data, and queuing \nthe incoming requests. \nA basic configuration for nginx could look like this. The code is available on GitHub \nat https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/\nmain/chapter_06_web_server/nginx_example.conf.\nserver {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    error_log /dev/stdout;\n    access_log /dev/stdout;\n       root /opt/;\n    location /static/ {\n        autoindex on;\n        try_files $uri $uri/ =404;\n    }\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n         uwsgi_pass unix:///tmp/uwsgi.sock;\n         include uwsgi_params;\n    }\n}\nThe directive server opens and closes the basic block to define how to serve the data. \nNote each line ends with a semicolon.\nIn nginx parlance, each server directive defines a virtual server. \nNormally there will be only one, but multiple can be configured, \nfor example, to define different behaviors based on the DNS \naddressed.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "Chapter 6\n[ 179 ]\nInside, we have a basic configuration on what port to serve – in our case, port 80 and \nboth IPv4 and IPv6 addresses. The default_server clause means this is the server to \nbe used by default:\n    listen 80 default_server;\n    listen [::]:80 default_server;\nNext, we define where the static files are, both in terms of the external URL, and \nwhat is the mapping with some section of the hard drive.\nNote the static location needs to be defined before the reverse proxy: \n    root /opt/;\n    location /static/ {\n        autoindex on;\n        try_files $uri $uri/ =404;\n    }\nroot defines the starting point, while location starts a section that will serve the URL \n/static/file1.txt from the file located in the hard drive at /opt/static/file1.txt.\ntry_files will scan for files in the URI and raise a 404 error if it's not there.\nautoindex automatically generates an index page to check the contents of a directory.\nIPv4 is the common address with four numbers, like 127.0.0.1. \nIPv6 is longer, and it's intended as a replacement for IPv4. For \nexample, an IPv6 address can be expressed as  2001:0db8:0000:\n0000:0000:ff00:0042:7879. IPv4 addresses have already been \nexhausted, meaning that there are no new addresses available. \nIPv6 will in the long run provide enough to avoid this problem, \nthough IPv4 is still widely used, and probably will remain in use \nfor a long time yet.\nThis option is typically disabled in production servers, but it's very \nhandy to detect problems with static files while running in test \nmode.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1646,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "Web Server Structures\n[ 180 ]\nIt's important in production environments to serve static files directly from the web \nserver, instead of doing them further along the line with the Python worker. While \nthis is possible, and a common case when working in a development environment, \nit's very inefficient. The speed and memory usage will be much bigger, while a web \nserver is optimized to serve static files. Please always remember to serve static files in \nproduction through a web server.\nServing static content externally\nAn alternative is to use an external service to handle files, like AWS S3, that allows \nyou to serve static files. The files then will be under a different URL than the service, \nfor example:\n•\t\nThe service URL is https://example.com/index\n•\t\nThe static files are in https://mybucket.external-service/static/\nAll the references inside the service web pages, then, should point to the external \nservice endpoint.\nThis way of operating requires you to push the code to the external service as part of \nthe deployment. To allow for uninterrupted deployments, remember that the static \ncontent needs to be available before. Another important detail is to upload them with \na different path, so static files between deployments are not confused.\nThis is easy to do using different root paths. For example:\n1.\t Version v1 of the service is deployed. This is the starting point. The static \ncontent is served from https://mybucket.external-service/static/v1/.\nThe calls to the service, like https://example.com/index, return all their static \ncontent pointing at version v1.\n2.\t Once v2 of the service is ready, the first thing to do is to push it to the \nexternal service, so it's available in https://mybucket.external-service/\nstatic/v2/. Note that, at this point, no user is accessing /static/v2; the \nservice is still returning /static/v1.\nDeploy the new service. Once it is deployed, the users will start accessing /\nstatic/v2 when they call https://example.com/index.\nAs we've seen in previous chapters, the key for a seamless deployment is to perform \nactions in small increments, and each step must perform actions that are reversible \nand prepare the terrain so there's no moment when something that's required is not \nready.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "Chapter 6\n[ 181 ]\nThis approach can be used for big operations. In a JavaScript-heavy interface, \nlike a single-page application, changing the static files effectively can be a new \ndeployment. The underlying service API can remain the same but changing the \ndownloaded version for all JavaScript code and other static content, which in effect \nwill deploy a new version.\nThis structure makes both versions of the static content available at the same \ntime. This can also be used to make tests or release beta versions. As the service is \nreturning whether to use version A or B, this can be set dynamically.\nFor example, adding an optional parameter in any call to overwrite the returned \nversion:\n•\t\nCalling https://example.com/index returns the default version, for example, \nv2.\n•\t\nCalling https://example.com/index?overwrite_static=v3 returns the \nspecified version instead, like v3.\nOther options are returning v3 for specific users, like beta testers or internal staff. \nOnce v3 is deemed correct, it can be changed to be the new default with a small \nchange in the service.\nWe talked about single-page apps in Chapter 2.\nThis approach can be taken to the extreme to push any single \ncommit to the source control to the public S3 bucket, and then \ntest in any environment, including production. This can help to \ngenerate a very fast feedback loop where QA or product owners \ncan quickly see changes in their own browser, without requiring \nany deployment or special environment.\nDon't feel limited to a unique integer as the version number; it \ncan work as well with a random UUID or SHA of the content \ngenerated automatically. Web storage is quite cheap, so it would \nrequire a lot of versions with very big files to really start to worry \nabout cost. And old versions can be deleted periodically.\nWhile this approach can be very aggressive and not viable for all \napplications, for an application that requires many changes in a \nrich JavaScript interface or to make drastic changes to the look and \nfeel, it can be highly productive. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "Web Server Structures\n[ 182 ]\nThis external serving can be combined with CDN (content delivery network) \nsupport for a multiregional proxy. This will distribute the files around the world to \nprovide a copy of it closer to the user.\nUsing a CDN is very powerful for truly global audiences. They are especially \nuseful for serving data that requires low latency around the world. For example, \nbroadcasting near real-time video.\nThe data can be distributed internally between the different servers from the \ncompany providing the CDN service quite quickly, as they'll use dedicated networks \nbetween them instead of using an external network.\nIn any case, using an external service to store the static files will, obviously, remove \nthe need to configure the web server for them.\nReverse proxy\nLet's continue describing the web server configuration. After describing the static \nfiles, we need to define a connection to the backend, acting as a reverse proxy.\nA reverse proxy is a proxy server that can redirect a received request towards one or \nmore defined backends. In our example, the backend is the uWSGI process.\nThink of a CDN as an internal cache by the company providing \nthe service. For example, we have a service where their servers \nare located in Europe, but a user is accessing it from Japan. This \ncompany has servers in Japan that store a copy of the static content. \nThat means that the user can access the files with much lower \nlatency than if the request had to reach a server in Europe, more \nthan 8,000 kilometers away.\nVideo broadcast online is typically transferred as small video \nchunks of a few seconds in duration. An index file keeps track \nof what is the latest chunk generated, so clients can be kept up \nto date. This is the basis of the format HTTP Live Streaming, or \nHLS, very common as the transfer of data is done directly through \nHTTP.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1977,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "Chapter 6\n[ 183 ]\nThe web server will be able to communicate with the backend in multiple ways, \nallowing flexibility. This can use different protocols, like FastCGI, SCGI, straight \nHTTP for pure proxying, or, in our case, connecting directly to the uWSGI protocol. \nWe need to define it to connect through either a TCP socket or a UNIX socket. We \nwill use a UNIX socket.\nThe socket needs to be coordinated with the way uWSGI is configured. As we will \nsee later, the uWSGI process will create it:\n    location / {\n       proxy_set_header Host $host;\n       proxy_set_header X-Real-IP $remote_addr;\n       include uwsgi_params;\n        uwsgi_pass unix:///tmp/uwsgi.sock;\n    }\nFirst of all, the root of the server is at the / URL. It's important to make the static \ncontent before the reverse proxy, as the locations are checked in order. So any \nrequest for a /static request gets detected before checking for / and it's properly \ntreated.\nA reverse proxy works in a similar way as a load balancer, though \nload balancers can work with more protocols, while a reverse \nproxy is only capable of working with web requests. On top of \ndistributing requests across different servers, it can also add some \nfeatures like caching, security, SSL termination (receiving a request \nin HTTPS and connecting to other servers using HTTP), or, in this \nparticular case, receive a web request and transfer it to through a \nWSGI connection.\nTCP sockets are designed to allow communication between \ndifferent servers, while UNIX sockets are designed to communicate \nprocesses locally. UNIX sockets are a little bit lighter for \ncommunication inside the same host and they work like a file, \nallowing you to assign them permissions to control what process \ncan access what socket.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "Web Server Structures\n[ 184 ]\nThe core of the reverse proxy configuration is the uwsgi_pass clause. This specified \nwhere to redirect the requests. include uwgi_params will add a bunch of standard \nconfigurations to be passed to the next stage. \nExtra elements can be added as HTTP headers. They'll be added to the request, so \nthey are available further down the request. \n       proxy_set_header Host $host;\n       proxy_set_header X-Real-IP $remote_addr;\nIn this case, we are adding the Host header, with information about the requested \nhost. Note that the $host is an indication to nginx to fill the value with the host the \nrequest is addressed to. In the same way, the header X-Real-IP is added with the IP \naddress from the remote address.\nIn our configuration, we only use a single backend, as uWSGI will balance between \ndifferent workers. But, if necessary, multiple backends can be defined, even mixing \nUNIX and TCP sockets, defining a cluster.\nupstream uwsgibackends {\n  server unix:///tmp/uwsgi.sock;\n  server 192.168.1.117:8080;\n  server 10.0.0.6:8000;\n}\nLater, define the uwsgi_pass to use the cluster. The requests will be equally spread \nover the different backends.\nuwsgi_pass uwsgibackends;\nuwsgi_params is actually a defined file included by default in \nnginx config that adds a lot of uwsgi_param statements with \nelements like SERVER_NAME, REMOTE_ADDRESS, etc.\nMore uwsgi_param can be added if necessary, in a similar way to \nthe headers.\nSetting headers correctly to pass on is unappreciated work, but \ncan be critical to properly monitor problems. Setting headers may \nrequire doing so at different stages. As we will discuss later, a \nsingle request can pass through multiple proxies, and each of them \nneeds to adequately forward the headers.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "Chapter 6\n[ 185 ]\nLogging\nWe also need to track any possible error or access. There are two different logs that \nnginx (and other web servers) produces:\n•\t\nError log: The error log tracks possible problems from the web server itself, \nlike not being able to start, configuration problems, etc. \n•\t\nAccess log: The access log reports any request accessing the system. This is \nthe basic information about the system flowing. It can be used to find specific \nproblems like 502 errors when the backend cannot be connected, or, when \ntreated as aggregated, it can detect problems like an abnormal number of \nerror status codes (4xx or 5xx).\nBoth logs are critical information that needs to be adequately detected. Following \nthe Twelve-Factor App, we should treat them as streams of data. The easiest is to \nredirect them both to standard output.\n    access_log /dev/stdout;\n    error_log /dev/stdout;\nThis requires nginx to not start as a daemon process, or if it is, capture the standard \noutput properly.\nAnother option is to redirect the log into a centralized log facility, using the \nproper protocol. This directs all the logs into a centralized server that captures the \ninformation. In this example, we send it to a syslog host in syslog_host.\n    error_log syslog:server=syslog_host:514;\n    access_log syslog:server=syslog_host:514,tag=nginx;\nThis protocol allows you to include tags and extra information that can help separate \nthe origin of each log later.\nWe will talk in further detail about logs in Chapter 11.\nBeing able to distinguish the source of each log is critical and \nalways requires a bit of tweaking. Be sure to spend some time \nmaking the logs easy to search. It will greatly simplify the work \nwhen an error in production requires gathering information.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "Web Server Structures\n[ 186 ]\nAdvanced usages\nA web server is very powerful, and shouldn't be underestimated. Other than acting \npurely as a proxy, there are a lot of other features that can be enabled like returning \ncustom redirects, overwriting the proxy with a static page for maintenance windows, \nrewriting URLs to adjust changes, providing SSL termination (decrypt receiving \nHTTPS requests to pass them decrypted through regular HTTP, and encrypt the \nresult back), caching requests, splitting the requests based on percentages for A/B \ntesting, choosing a backend server based on geolocalization of the requester, etc.\nBe sure to read the documentation of nginx at http://nginx.org/en/docs/ to read \nall the possibilities.\nuWSGI\nThe next element of the chain is the uWSGI application. This application receives the \nrequests from nginx and redirects them into independent Python workers, in WSGI \nformat.\nuWSGI will also start and coordinate the different processes, handling the lifecycle \nfor each of them. The application works as an intermediary, starting a group of \nworkers receiving the requests.\nuWSGI is configured through a uwsgi.ini file. Let's see an example, available on \nGitHub at https://github.com/PacktPublishing/Python-Architecture-Patterns/\nblob/main/chapter_06_web_server/uwsgi_example.uni.\n[uwsgi]\nchdir=/root/directory\nwsgi-file = webapplication/wsgi.py\nmaster=True\nsocket=/tmp/uwsgi.sock\nvacuum=True\nprocesses=1\nWeb Server Gateway Interface (WSGI) is a Python standard to \ndeal with web requests. It's very popular and supported by a lot \nof software, both from the sending end (like nginx, but also other \nweb servers like Apache and GUnicorn) and from the receiving \nend (virtually every Python web framework, like Django, Flask, or \nPyramid).\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "Chapter 6\n[ 187 ]\nmax-requests=5000\n# Used to send commands to uWSGI\nmaster-fifo=/tmp/uwsgi-fifo\nThe first element defines what the working directory is. The application will be \nlaunched here, and other file references will work from here:\nchdir=/root/directory\nThen, we describe where the wsgi.py file is, which describes our application.\nThe WSGI application\nInside this file is the definition of the application function, which uWSGI can use to \naddress the internal Python code, in a controlled way. \nFor example:\ndef application(environ, start_response):\n    start_response('200 OK', [('Content-Type', 'text/plain')])\n    return [b'Body of the response\\n']\nThe first parameter is a dictionary with predefined variables that detail the request \n(like METHOD, PATH_INFO, CONTENT_TYPE, and so on) and parameters related to the \nprotocol or environment (for example, wsgi.version). \nThe second parameter, start_response, is a callable that allows you to set up the \nreturn status and any headers. \nThe function should return the body. Note how it's returned in byte stream format.\nThe difference between text streams (or strings) and byte streams \nwas one of the big differences introduced in Python 3. To \nsummarize it, byte streams are raw binary data, while text streams \ncontain meaning by interpreting that data through a particular \nencoding.\nThe differentiation between both can be a bit baffling sometimes, \nin particular since Python 3 makes the difference explicit, and that \nclashes with some previous lax practices, especially when dealing \nwith ASCII content that can be represented in the same way.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "Web Server Structures\n[ 188 ]\nThe function can also work as a generator and use the keyword yield instead of \nreturn when the returning body needs to be streamed.\nKeep in mind that text streams need to be encoded to be \ntransformed into byte streams, and byte streams need to be \ndecoded into text streams. Encoding is moving from the abstract \nrepresentation of text to the precise representation of binary.\nFor example, the Spanish word \"cañón\" contains two characters not \npresent in ASCII, ñ and ó. You can see how encoding them through \nUTF8 replaces them with specific binary elements described in \nUTF8: \n>>> 'cañón'.encode('utf-8')\nb'ca\\xc3\\xb1\\xc3\\xb3n' \n>>> b'ca\\xc3\\xb1\\xc3\\xb3n'.decode('utf-8')\n'cañón'\nAny function that uses yield is a generator in Python. This means \nthat when called, it returns an iterator object that returns elements \none by one, normally to be used in loops.\nThis is very useful for situations where each element of the loop \ntakes some time to process but can be returned without being \nrequired to calculate every single item, reducing latency and \nmemory usage, as not all elements need to be maintained in \nmemory.\n>>> def mygenerator():\n...   yield 1\n...   yield 2\n...   yield 3\n>>> for i in mygenerator():\n...   print(i)\n...\n1\n2\n3\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1374,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "Chapter 6\n[ 189 ]\nIn any case, the WSGI file is normally created by default by whatever framework is \nused. For example, a wsgi.py file created by Django will look like this.\nimport os\nfrom django.core.wsgi import get_wsgi_application\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"webapplication.\nsettings\")\napplication = get_wsgi_application()\nNote how the function get_wsgi_application will automatically set up the proper \napplication function, and connect it with the rest of the defined code – a great \nadvantage of using an existing framework!\nInteracting with the web server\nLet's continue with the uwsgi.ini configuration with the socket configuration:\nsocket=/tmp/uwsgi.sock\nvacuum=True\nThe socket parameter creates the UNIX socket for the web server to connect to. It \nwas discussed before in this chapter, when talking about the web server. This needs \nto be coordinated on both sides, to ensure they connect properly.\nThe vacuum option cleans up the socket when the server is closed.\nuWSGI also allows you to use a native HTTP socket, using \nthe option http-socket. For example, http-socket = \n0.0.0.0:8000 to serve all addresses on port 8000. You may use \nthis option if the web server is not on the same server and needs to \ncommunicate through the network.\nWhen possible, avoid exposing uWSGI directly publicly over the \ninternet. A web server will be safer and more efficient. It will also \nserve static content much more efficiently. If you really must skip \nthe web server, use the option http instead of http-socket, which \nincludes a certain level of protection.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1687,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "Web Server Structures\n[ 190 ]\nProcesses\nThe next parameters control the number of processes and how to control them:\nmaster=True\nprocesses=1\nThe master parameter creates a master process that ensures that the number of \nworkers is correct, restarting if not, and deals with the process lifecycle, among other \ntasks. It should always be enabled in production for smooth operation.\nThe processes parameter is very straightforward and describes how many Python \nworkers should be started. Received requests will be load balanced across them. \nThe way uWSGI generates new processes is through pre-forking. This means that \na single process gets started, and after the application is loaded (which may take a \nwhile), it's cloned through a fork process. This sensibly speeds up the startup time \nfor new processes, but at the same time, relays that the setup of the application can \nbe duplicated. \nChoosing the right number of processes is highly dependent on the application \nitself and the hardware that supports it. The hardware is important as a CPU with \nmultiple cores will be able to run more processes efficiently. The amount of IO vs \nCPU usage in the application will determine how many processes can be run by the \nCPU core.\nThis assumption, on rare occasions, may cause problems with \ncertain libraries that, for example, open file descriptors during \ninitializations that cannot be shared safely. If that's the case, the \nparameter lazy-apps will make each worker start from scratch, \nindependently. This is slower, but it creates more consistent \nresults.\nTheoretically, a process not using IO and purely crunching \nnumbers will use the whole core without wait periods, not \nallowing the core to switch to another process meanwhile. A \nprocess with high IO, with the core idle while waiting for results \nfrom the database and external services, will increase its efficiency \nby performing more context switches. This number should be \ntested to ensure the best results. A common starting point will \nbe two times the number of cores, but remember to monitor the \nsystem to tweak it and obtain the best results.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "Chapter 6\n[ 191 ]\nAn important detail about the created processes is that they deactivate the creation \nof new threads by default. This is an optimization choice. In the majority of web \napplications, there's no need to create independent threads inside each of the \nworkers, and that allows you to deactivate the Python GIL, speeding up the code. \nIf threads need to be used, the option enable-threads will enable them. \nProcess lifecycle\nDuring the time of operation, processes won't stay static. Any working web \napplication will need to reload with new code changes regularly. The next \nparameters are related to how processes are created and destroyed.\nmax-requests=5000\n# Used to send commands to uWSGI\nmaster-fifo=/tmp/uwsgi-fifo\nmax-requests specifies the number of requests to be processed by a single worker \nbefore being restarted. Once the worker gets to this number, uWSGI will destroy it \nand create another worker from scratch, following the usual process (fork by default, \nor using lazy-apps if configured).\nThis is useful to avoid problems with memory leaks or other sorts of stale problems, \nwhere the performance of a worker gets degraded over time. Recycling the workers \nis a protective measure that can be taken pre-emptively, so even if a problem is \npresent, it will be corrected before it causes any issues. \nThe Global Interpreter Lock or GIL is a mutex lock that only \nallows a single thread to have control of the Python process. \nThis means that, inside a single process, no two threads can run \nat the same time, something that multi-core CPU architecture \nmakes possible. Note that multiple threads may be waiting for IO \nresults while another runs, which is a usual situation in real-life \napplications. The GIL is typically held and released constantly, as \neach operation first holds the GIL and then releases it at the end.\nThe GIL is commonly blamed for inefficiencies in Python, though \nthe effect is only perceived in high-CPU multi-threaded operations \nin native Python (as opposed to using optimized libraries like \nNumPy), which are not as usual and are already slow to start with.\nThese interactions with the GIL are only wasteful if no threads will \nbe run, so that's why uWSGI deactivates it by default.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "Web Server Structures\n[ 192 ]\nRemember that, based on the Twelve-Factor App, web workers need to be able to be \nstopped and started at any time, so this recycling is painless.\nuWSGI will also recycle the worker when it's idle, after serving its 5,000th request, so \nit will be a controlled operation.\nWhen multiple workers are involved, if each of them will restart after their 5,000th \nrequest, a stampede problem can be created where one after another all the workers \nare recycled. Keep in mind that the load is distributed through the workers equally, \nso this count will be in sync across the multiple workers. While the expectation is \nthat, for example, with 16 workers, at least 15 of them will be available, in practice \nwe might find that all are being recycled at the same time.\nTo avoid this problem, use the max-requests-delta parameter. This parameter adds \na variable number for each worker. It will multiply the delta for the worker ID (a \nunique consecutive number for each worker starting from 1). So, configuring a delta \nof 200, each worker will have the following: \nWorker\nBase max-request\nDelta\nTotal requests to recycle\nWorker 1\n5,000\n1 * 200\n5,200\nWorker 2\n5,000\n2 * 200\n5,400\nWorker 3\n5,000\n3 * 200\n5,600\n…\nWorker 16\n5,000\n16 * 200\n8,200\nThis makes the recycling happen at different times, increasing the number of \nworkers available at the same time, as they won't restart simultaneously.\nKeep in mind this recycling may interfere with other operations. \nDepending on the startup time, it may take a few seconds or worse \n(especially if lazy-apps is used) to start the worker, potentially \ncreating a backlog of requests. uWSGI will queue the incoming \nrequests. In our example configuration, there's only a single \nworker defined in processes. With multiple workers this can be \nmitigated, as the rest of the workers will be able to handle the extra \nload.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "Chapter 6\n[ 193 ]\nThe master-fifo parameter creates a way to communicate with uWSGI and send \ncommands:\n# Used to send commands to uWSGI\nmaster-fifo=/tmp/uwsgi-fifo\nThis creates a UNIX socket in /tmp/uwsgi-fifo that can receive commands in the \nform of characters redirected to it. For example:\n# Generate a graceful reload\necho r >> /tmp/uwsgi-fifo\n# Graceful stop of the server\necho q >> /tmp/uwsgi-fifo\nThis method allows for better handling of situations than sending signals, as \nthere are more commands available and it allows for quite granular control of the \nprocesses and the whole uWSGI.\nFor example, sending Q will produce a direct shutdown of uWSGI, while q will \nproduce a graceful one. A graceful shutdown will start by stopping accepting new \nrequests in uWSGI, then waiting until any request in the internal uWSGI queue \nis being processed, and when a worker has finished its request, stopping it in an \norderly fashion. Finally, when all workers are done, stop the uWSGI master process.\nThis problem is of the same kind as what's called a cache stampede. \nThis is produced where multiple cache values are invalidated at \nthe same time, producing the regeneration of values at the same \ntime. Because the system expects to be running under some cache \nacceleration, suddenly having to recreate a significant portion \nsection of the cache may produce a serious performance problem, \nto the point of the complete collapse of the system.\nTo avoid this, avoid setting fixed times for the cache to expire, such \nas a certain hour of the clock. This can happen, for example, if a \nbackend gets updated with news for the day at midnight, making \nit tempting to expire the cache at this time. Instead, add an element \nto make the different keys expire at slightly different times to avoid \nthis problem. This can be achieved by adding a small random \namount of time to the expiry time for each of the keys, so they can \nreliably be refreshed at different times.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2072,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "Web Server Structures\n[ 194 ]\nThe graceful reload with r takes a similar approach, keeping the requests in the \ninternal queue and waiting until the workers are done to stop them and restart them. \nIt will also load any new configuration related to uWSGI itself. Note that, during \nthe time of the operation, the internal uWSGI listen queue may be filled up, causing \nproblems.\nIf the loading of processes is done through the fork process, after starting up the \nfirst one, the rest will be copies, so they will be loaded quite quickly. By comparison, \nusing lazy-apps may delay achieving full capacity as each individual worker will \nneed to be individually started from scratch. This can produce an extra load on the \nserver, depending on the number of workers and the startup procedure.\nReloading a single server under load may be complicated. Using multiple uWSGI \nservers simplifies the process. In this situation, reloads should happen at different \ntimes to allow you to distribute the load. \nA cluster-style approach can be taken in using multiple servers to perform this \ndance, creating copies of the uWSGI configuration in multiple servers and then \nrecycling them one at a time. While one is reloading, the others will be able to handle \nthe extra load. In extreme situations, an extra server can be used to produce extra \ncapacity during the reload.\nThe size of the listen queue can be tweaked with the listen \nparameter, but keep in mind that there's a limit set up by Linux \nthat you may need to change as well. Defaults are 100 for listen and \n128 for the Linux configuration.\nDo tests before changing those to big values, as churning through a \nbig backlog of tasks has its own problems.\nA possible alternative for lazy-apps is to use the c option, \nreloading the workers with chain reloading. This reloads each \nworker independently, waiting until a single worker is totally \nreloaded before moving to the next one. This procedure doesn't \nreload the uWSGI configuration but will do with code changes \nin the workers. It will take longer, but it will work at a controller \npace.\nThis is common in cloud environments where an extra server can \nbe used and then destroyed. In Docker situations, new containers \ncan be added to provide this extra capacity.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "Chapter 6\n[ 195 ]\nFor more information about the master-fifo and accepted commands, including \nhow to pause and resume the instance, and other exotic operations, check the uWSGI \ndocumentation at https://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html.\nPython worker\nThe core of the system is the Python WSGI worker. This worker receives the HTTP \nrequests from uWSGI after they're routed by the external web server, etc.\nThis is where the magic happens, and it is specific to the application. This is the \nelement that will see faster iteration than the rest of the links of the chain.\nEach framework will interact in a slightly different way with the requests, but in \ngeneral, they will follow similar patterns. We will use Django as an example.\nDjango MVT architecture\nDjango borrows heavily from the MVC structure but tweaks it a bit into what's called \nMVT (Model-View-Template): \n•\t\nThe Model remains the same, the representation of the data and interacting \nwith the storage.\nuWSGI is a very powerful application that has almost endless \npossibilities for configuration. Its documentation is overwhelming \nin the amount of detail it contains, but it's incredibly \ncomprehensive and insightful. You can learn a lot, not only \nabout uWSGI but also about how the whole web stack works. I \nhighly recommend going through slowly, but surely, to learn a \nlot. You can access the documentation at https://uwsgi-docs.\nreadthedocs.io/.\nWe won't discuss all aspects of Django or go into a deep dive of \nits features but will use a selection to look at some lessons that are \nuseful for other frameworks.\nThe Django project is really well documented. Seriously, it has \nalways been distinguished by its world-class documentation, \nsince the project started. You can read it here: http://www.\ndjangoproject.com.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1913,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "Web Server Structures\n[ 196 ]\n•\t\nThe View receives the HTTP request and processes it, interacting with the \ndifferent Models that may be required.\n•\t\nThe Template is a system to generate HTML files, from values passed on.\nThis changes Model-View-Controller a bit, though the result is similar.\nFigure 6.3: The Model-View-Controller\nThe Model works the same in both systems. The Django View acts as a combination \nof the View and the Controller, and the Template is a helping system for the View \ncomponent of the Django View.\nThe templating system is not strictly required to be used, as not every Django \ninterface requires an HTML page as a result. \nDjango is a powerful and comprehensive framework and has some assumptions \non how things are supposed to run, such as using the Django ORM or using its \ntemplating system. While doing so is \"swimming with the current,\" it's definitely \npossible to take other approaches and tailor any part of the system. This can involve \nelements like not using templates, using a different templating system, using a \ndifferent ORM library like SQLAlchemy, and adding extra libraries to connect to \ndifferent databases, including ones not supported natively by Django (like NoSQL \ndatabases). Do not let the constraints of the system limit you from achieving your \ngoals.\nWhile Django was designed to create HTML interfaces, there are \nways of creating other types of interfaces. In particular, for RESTful \ninterfaces, the Django REST framework (https://www.django-\nrest-framework.org) allows you to expand the functionality and \ngenerate self-documented RESTful interfaces easily.\nWe will look at the Django REST framework later in the chapter.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1786,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "Chapter 6\n[ 197 ]\nRouting a request towards a View\nDjango provides the tools to perform the proper routing from a particular URL to a \nspecific View.\nThis is done in the urls.py file. Let's see an example.\nfrom django.urls import path\nfrom views import first_view, second_view\nurlpatterns = [\n    path('example/', first_view)\n    path('example/<int:parameter>/<slug:other_parameter>', second_view)\n]\nThe required Views (that are typically declared as functions) are imported from \nwhatever module they are currently in into the file.\nThe urlpatterns list defines an ordered list of URL patterns that will be tested \nagainst an input URL.\nThe first path definition is very straightforward. If the URL is example/, it will call \nthe View first_view. \nThe second path definition contains definitions to capture parameters. It will \ntransform the defined parameters properly and pass them over to the view. For \nexample, the URL example/15/example-slug will create these parameters:\n•\t\nparameter=int(15)\n•\t\nother_parameter=str(\"example-slug\")\nThere are different types of parameters that can be configured. int is self-\nexplanatory, but slug is a limited string that will include only alphanumeric, _ \n(underscore), and – (dash) symbols, excluding characters like . or other symbols. \nDjango is opinionated in the way that it presents a lot of elements \nworking together with certain assumptions. They are tightly \nrelated to each other. If that's an impediment, for example, because \nyou need to use wildly different tools, a good alternative can be \nPyramid (https://trypyramid.com), a Python web framework \ndesigned to build your own combination of tools to ensure \nflexibility. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1780,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "Web Server Structures\n[ 198 ]\nAnother option is to generate the paths directly as regex. If you are familiar with \nthe regex format, this can be very powerful and allow a great deal of control. At the \nsame time, regexes can grow really complex and difficult to read and use. \nfrom django.urls import re_path\nurlpatterns = [\n    re_path('example/(?P<parameter>\\d+)/', view)\n]\nAn intermediate option is to define types to be sure that they match specific values, \nfor example, creating a type to match only months like Apr or Jun. If the type is \ndefined in this way, an incorrect pattern like Jen will return a 404 automatically. \nInternally, this will require writing a regex to match the proper string anyway, but \nafterwards, it can transform the value. For example, to transform the month Jun to \neither the number 1, normalize it as JUNE, or any other value that makes sense later. \nThe complexity of the regex will be abstracted by the type.\nKeep in mind that the patterns are checked in order. That means that, if a pattern \nmay fulfil two paths, it will select the first one. This may have unintended effects \nwhen a previous path \"hides\" the next one, so the least restrictive patterns should be \npositioned later.\nFor example:\nfrom django.urls import path\nurlpatterns = [\n    path('example/<str:parameter>/', first_view)\n    path('example/<int:parameter>/', second_view)\n]\nThere are more types available. There's also a str type that can be \ntoo broad. The character / is understood as special in URLs and it's \nalways excluded. This allows for easy separation of parameters. \nThe type slug should cover more typical use cases for parameters \ninside a URL.\nThis was the only option available previously in \nDjango. As you can see for the example, equivalent to \nexample/<int:parameter>/, the new path-defined URL patterns \nare easier to read and to deal with.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1971,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "Chapter 6\n[ 199 ]\nNo URL will ever get passed to second_view, as any parameter that is an integer will \nbe captured first. \nThe interesting stuff happens inside of the View. \nThe View\nThe View is the central element of Django. It receives the request information, plus \nany parameters from the URL, and processes it. The View normally will use the \ndifferent Models to compose the information, and finally returns a response.\nThe View is in charge of deciding if there's any change in behavior based on the \nrequest. Note that the routing towards the View only distinguishes between different \npaths, but other distinctions like HTTP method or parameters will need to be \ndifferentiated here.\nThis makes it a very common pattern to differentiate between POST and GET \nrequests to the same URL. A common usage in web pages is to make a form page \nto display the empty form, and then POST to the same URL. For example, in a form \nwith a single parameter, the structure will be similar to the following example: \ndef example_view(request):\n    # create an empty form\n    form_content = Form()\n    if request.method == 'POST':\n        # Obtain the value\n           value = request.POST['my-value']\n           if validate(value):\n               # Perform actions based on the value\n            do_stuff()\nThis kind of error is usually possible in most URL routers in web \nframeworks, as most of them are pattern-based. Keep an eye in \ncase it affects your code.\nThis is intended as pseudocode to not complicate it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1611,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "Web Server Structures\n[ 200 ]\n            content = 'Thanks for your answer'\n        else:\n            content = 'Sorry, this is incorrect' + form_content\n    elif request.method == 'GET':\n        content = form_content\n    return render(content)\n    \nWhile it's true that Django includes a form system that simplifies the validation and \nreporting of forms, this structure can grow legs and become tiresome. In particular, \nthe multiple nested if blocks are confusing.\nInstead of that, dividing the View with two different subfunctions may be clearer. \ndef display_form(form_content, message=''):\n    content = message + form_content\n    return content\ndef process_data(parameters, form_content):\n    # Obtain the value\n       if validate(parameters):\n           # Perform actions based on the value\n        do_stuff()\n        content = 'Thanks for your answer'\n    else:\n        message = 'Sorry, this is incorrect'\n        content = display_form(form_content , message)\n    return content\ndef example_view(request):\n    # create an empty form\nWe won't go into details with the form system in Django. It is quite \ncomplete and allows you to render rich HTML forms that will \nvalidate and show possible errors to the user. Read the Django \ndocumentation to know more.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "Chapter 6\n[ 201 ]\n    form_content = Form()\n    if request.method == 'POST':\n        content = process_data(request.POST, form_content)\n    elif request.method == 'GET':\n        content = display_form(form_content)\n    return render(content)\nThe challenge here is to preserve the fact that, when the parameters are incorrect, the \nform needs to be rendered again. By the principle of DRY (Don't Repeat Yourself), \nwe should try to locate that code in a single place. Here, in the display_form \nfunction. We allow some customization of the message to add some extra content, \nin case the data is incorrect.\nNote that the display_form function gets called both from example_view and also \ninside process_data.\nHttpRequest\nThe key element for passing information is the request parameter. This object's \ntype is HttpRequest, and contains all the information that the user is sending in the \nrequest.\nIts most important attributes are:\n•\t\nmethod, which contains the used HTTP method. \n•\t\nIf the method is GET, it will contain a GET attribute with a QueryDict (a \ndictionary subclass) containing all the query parameters in the request. For \nexample, a request such as:\n/example?param1=1&param2=text&param1=2\nIn a more complete example, the form will be tweaked to show \nthe specific errors. Django forms are able to do this automatically. \nThe process will be to create a form with the parameters from the \nrequest, validate it, and print it. It automatically will produce the \nproper error messages, based on the type of each of the fields, \nincluding custom types. Again, refer to Django's documentation for \nmore information.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "Web Server Structures\n[ 202 ]\nWill produce a request.GET value like this:\n<QueryDict: {'param1': ['1', '2'], 'param2': ['text']}>\nNote that the parameters are stored internally as a list of values, because \nquery parameters accept multiple parameters with the same key, though \nthat's not usually the case. They'll return a unique value when queried \nanyway:\n>>> request.GET['param1']\n2\n>>> request.GET['param2']\ntext\nAll the parameters are defined as strings, needing to be converted to other \ntypes if necessary.\n•\t\nIf the method is POST, an analogous POST attribute will be created. In this case, \nit will be filled first by the body of the request, to allow encoding form posts. \nIf the body is empty, it will fill the values with query parameters like the GET \ncase.\n•\t\ncontent_type with the MIME type of the request.\n•\t\nFILES, including data for any uploaded files in the request, for certain POST \nrequests.\n•\t\nheaders, a dictionary containing all the HTTP headers of the request and \nheaders. Another dictionary, META, contains extra information from headers \nthat may be introduced and are not necessarily HTTP-based, like SERVER_\nNAME. In general, it is better to obtain information from the headers attribute.\nThey'll be all reported in order, with the latest value being \nreturned. If you need to access all values, use the method getlist:\n>>> request.GET.getlist('param1')\n['1', '2']\nPOST multiple values will commonly be used in multiple \nselection forms.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1571,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "Chapter 6\n[ 203 ]\nThere are also some useful methods to retrieve information from the request, for \nexample:\n•\t\n.get_host() to obtain the name of the host. It will interpret the different \nheaders to determine the proper host, so it's more reliable than directly \nreading the HTTP_HOST header.\n•\t\n.build_absolute_uri(location) to generate a full URI, including the host, \nport, etc. This method is useful to create full references to return them.\nThese attributes and methods, combined with the parameters described in the \nrequest, allow you to retrieve all the relevant information necessary for processing \nthe request and call the required Models.\nHttpResponse\nThe HttpResponse class handles the information being returned by the View to the \nweb server. The return from a View function needs to be an HttpResponse object.\nfrom django.http import HttpResponse\ndef my_view(request):\n    return HttpResponse(content=\"example text\", status_code=200)\nThe response has a default status_code of 200 if it's not specified.\nIf the response needs to be written in several steps, it can be added through the \n.write() method.\nresponse = HttpResponse()\nresponse.write('First part of the body')\nresponse.write('Second part of the body')\nThe body can also be composed as an iterable.\nbody= ['Multiple ', 'data ', 'that ', 'will ', 'be ', 'composed']\nresponse = HttpResponse(content=body)\nAll responses from HttpResponse will be composed completely \nbefore being returned. It is possible to return responses in a \nstreaming way, meaning that the status code will be returned first \nand chunks of the body will be sent over time. To do that, there's \nanother class called StreamingHttpResponse that will work in \nthat way, and can be useful for sending big responses over time.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "Web Server Structures\n[ 204 ]\nInstead of using integers to define the status code, it's better to use the defined \nconstants available in Python, for example:\nfrom django.http import HttpResponse\nfrom http import HTTPStatus\ndef my_view(request):\n    return HttpResponse(content=\"example text\", status_code=HTTPStatus.\nOK)\nThis makes the usage of each status code more explicit and helps increase the \nreadability of the code, making them explicitly HTTPStatus objects.\nThe content parameter defines the body of the request. It can be described as a \nPython string, but it also accepts binary data, if the response is not plain text. If that's \nthe case, a content_type parameter should be added to adequately label the data \nwith the proper MIME type.\nHttpResponse(content=img_data, content_type=\"image/png\")\nHeaders can also be added to the response using the headers parameter.\nheaders = {\n    'Content-Type': 'application/pdf',\n    'Content-Disposition': 'attachment; filename=\"report.pdf\"',\n}\nresponse = HttpResponse(content=img_data, headers=header)\nYou can see all the status codes defined in Python here: https://\ndocs.python.org/3/library/http.html. Note the name is \ntheir standard HTTP status code name, as defined in several RFC \ndocuments, for example, 201 CREATED, 404 NOT FOUND, 502 BAD \nGATEWAY, etc.\nIt is very important that the returned Content-Type matches the \nformat of the body. This will make any other tool, like a browser, \nproperly interpret the content adequately.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1593,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "Chapter 6\n[ 205 ]\nHeaders are also stored in the response when it is accessed as a dictionary:\nresponse['Content-Disposition'] = 'attachment; filename=\"myreport.pdf\"'\ndel response['Content-Disposition']\nThere are specialized subclasses for common cases. Instead of using a generic \nHttpResponse, for JSON encoded requests, it's better to use JsonResponse, which will \ncorrectly fill the Content-Type and encode it:\nfrom django.http import JsonResponse\nresponse = JsonResponse({'example': 1, 'key': 'body'})\nIn the same style, the FileResponse allows you to download a file directly, providing \na file-like object and directly filling the headers and content type, including if it needs \nto be an attachment\nfrom django.http import FileResponse\nfile_object = open('report.pdf', 'rb')\nresponse = FileResponse(file_object, is_attachment=True)\nThe response can also be created by rendering a template. This is the usual way of \ndoing so for HTML interfaces, which was what Django was originally designed for. \nThe render function will automatically return an HttpResponse object. \nfrom django.shortcuts import render\ndef my_view(request):\n    ...\n    return render(request, 'mytemplate.html')\nMiddleware\nA key concept in WSGI requests is that they can be chained. This means that a \nrequest can go through different stages, wrapping a new request around the orinal \nat each stage, which allows you to add functionality.\nContent-Disposition can be used to label the response as an \nattachment that should be downloaded to the hard drive. \nAlso, we can set up the Content-Type header either manually \nthrough the headers parameter or through the content_type \nparameter directly.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1775,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "Web Server Structures\n[ 206 ]\nThis leads to the concept of middleware. Middleware improves the handling \nbetween systems by simplifying handling several aspects of the request, adding \nfunctionality, or simplifying their usage.\nA typical example of middleware is logging each received request in a standard \nmanner. The middleware will receive the request, produce a log, and hand the \nrequest to the next level.\nAnother example is managing whether the user is logged or not. There's a standard \nDjango middleware that will detect any session stored in cookies and will search in \nthe database for the associated user. It will then fill the request.user object with the \nproper user.\nAnother example, enabled by default in Django, checks the CSRF token on POST \nrequests. If the CSRF token is not present or it's incorrect, the request will be \nimmediately intercepted and it will return 403 FORBIDDEN, before accessing the View \ncode.\nMiddleware can access the request both when it's received and the response when \nit's ready, so they can work on either side or both sides in coordination:\n•\t\nLogging middleware that generates a log with the path and method of the \nreceived request can generate it before the request is sent to the View.\n•\t\nLogging middleware that also logs the status code needs to have the \ninformation of the status code, so it will need to do it once the View is \nfinished and the response is ready.\n•\t\nLogging middleware that logs the time it took to generate the request will \nneed to first register the time when the request was received, and what time \nit is when the response is ready, to log the difference. This requires code both \nbefore and after the View.\nMiddleware is a word that can refer to different concepts \ndepending on the context of its usage. When used in an HTTP \nserver environment, it typically refers to plugins that enhance or \nsimplify the handling of requests.\nWe introduced the idea of CSRF and tokens in Chapter 2.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2070,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "Chapter 6\n[ 207 ]\nMiddleware is defined in this way:\ndef example_middleware(get_response):\n    # The example_middleware wraps the actual middleware\n    def middleware(request):\n        # Any code to be executed before the view\n        # should be located here\n        response = get_response(request)\n        # Code to be executed after the view\n        # should be located here\n        return response\n    return middleware\nThe structure to return a function allows the initialization of chained elements. The \ninput get_reponse can be another middleware function or could be the final view. \nThis allows this kind of structure:\nchain = middleware_one(middleware_two(my_view))\nfinal_response = chain(request)\nThe order of the middleware is also important. For example, logging should happen \nbefore any middleware that can stop the request, as if done in reverse order, any \nrejected request (for example, not adding a proper CSRF) won't be logged.\nMiddleware can be easily added, either custom-made or by using third-party \noptions. There are a lot of packages that create their own middleware functions for \nuseful features in Django. When considering adding a new feature, spend some time \nsearching to see if there's something already available.\nGenerally, middleware functions have some recommendations \non where they should be located. Some are more sensitive to their \nposition than others. Check the documentation for each one. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1539,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "Web Server Structures\n[ 208 ]\nDjango REST framework\nWhile Django was designed originally to support HTML interfaces, its functionality \nhas been expanded, both as new features inside the Django project itself, as well as \nother external projects that enhance Django.\nOne of particular interest is Django REST framework. We will use it as an example of \nthe available possibilities.\nFor our example, we will implement some of the endpoints that we defined in \nChapter 2. We will use the following endpoints, to follow the whole lifecycle of a \nmicropost.\nEndpoint\nMethod\nAction\n/api/users/<username>/collection\nGET \nRetrieve all the \nmicroposts from \na user\n/api/users/<username>/collection\nPOST\nCreate a new \nmicropost for \nthe user\n/api/users/<username>/collection/<micropost_id>\nGET\nRetrieve a single \nmicropost\n/api/users/<username>/collection/<micropost_id>\nPUT, \nPATCH\nUpdate a \nmicropost\n/api/users/<username>/collection/<micropost_id>\nDELETE\nDelete a \nmicropost\nThe basic principle behind Django REST framework is to create different classes that \nencapsulate the exposed resources as URLs. \nThe extra concept is that objects will be transformed from an internal Model into an \nexternal JSON object and vice versa through a serializer. The serializer will handle the \ncreation and validate that the external data is correct.\nDjango REST framework is not only a popular and powerful \nmodule. It also uses a lot of conventions that are common across \nREST frameworks in multiple programming languages. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1609,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "Chapter 6\n[ 209 ]\nModels\nWe first need to introduce the models to store the information. We will use a Usr \nModel for the users and a Micropost Model.\nfrom django.db import models\nclass Usr(models.Model):\n    username = models.CharField(max_length=50)\nclass Micropost(models.Model):\n    user = models.ForeignKey(Usr, on_delete=models.CASCADE,\n                             related_name='owner')\n    text = models.CharField(max_length=300)\n    referenced = models.ForeignKey(Usr, null=True,\n                                   on_delete=models.CASCADE,\n                                   related_name='reference')\n    timestamp = models.DateTimeField(auto_now=True\nThe Usr model is very straightforward, only storing the username. The Micropost \nModel stores a string of text and the user that created the micropost. Optionally, it \ncan store a referenced user.\nA serializer can't only transform a Model object, but any kind of \ninternal Python class. You can use them to create \"virtual objects\" \nthat can pull information from multiple Models.\nA peculiarity of Django REST framework is that the serializer is the \nsame for input and output. In other frameworks, there are different \nmodules for the way in and out.\nNote that the relations have their own named back reference, \nreference and owner. They are created by default by Django so \nyou can search where a Usr is referenced, for example. \nNote also that the text allows for 300 characters, instead of the 255 \nthat we said in the API. This is to allow a bit of extra space in the \ndatabase. We will still protect against more characters later.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "Web Server Structures\n[ 210 ]\nURL routing\nWith this information, we create two different views, one for each URL that we need \nto create. They'll be called MicropostsListView and MicropostView. Let's take a look \nfirst at how the URLs are defined in the urls.py file:\nfrom django.urls import path\nfrom . import views\nurlpatterns = [\n    path('users/<username>/collection', views.MicropostsListView.as_\nview(),\n         name='user-collection'),\n    path('users/<username>/collection/<pk>', views.MicropostView.as_\nview(),\n         name='micropost-detail'),\n]\nNote that there are two URLs, that correspond to this definition:\n/api/users/<username>/collection\n/api/users/<username>/collection/<micropost_id>\nAnd each is mapped to the corresponding view.\nViews\nEach view inherits from the proper API endpoint, the collection one from \nListCreateAPIView, which defines the actions for LIST (GET) and CREATE (POST):\nfrom rest_framework.generics import ListCreateAPIView\nfrom .models import Micropost, Usr\nfrom .serializers import MicropostSerializer\nclass MicropostsListView(ListCreateAPIView):\n    serializer_class = MicropostSerializer\n    def get_queryset(self):\n        result = Micropost.objects.filter(\n             user__username=self.kwargs['username']\n        )\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "Chapter 6\n[ 211 ]\n        return result\n    def perform_create(self, serializer):\n        user = Usr.objects.get(username=self.kwargs['username'])\n        serializer.save(user=user)\nWe will check the serializer later. The class requires defining the queryset that it will \nuse to retrieve the information when the LIST part of the class is called. Because our \nURL includes the username, we need to identify it:\n    def get_queryset(self):\n        result = Micropost.objects.filter(\n             user__username=self.kwargs['username']\n        )\n        return result\nself.kwargs['username'] will retrieve the username defined in the URL.\nFor the CREATE part, we need to overwrite the perform_create method. This method \nreceives a serializer parameter that already contains the validated parameters.\nWe need to obtain the username and user from the same self.kwargs to be sure to \nadd it to the creation of the Micropost object.\n    def perform_create(self, serializer):\n        user = Usr.objects.get(username=self.kwargs['username'])\n        serializer.save(user=user)\nThe new object is created combining both the user and the rest of the data, added as \npart of the save method for the serializer.\nThe individual View follows a similar pattern, but there's no need to overwrite the \ncreation:\nfrom rest_framework.generics import ListCreateAPIView\nfrom .models import Micropost, Usr\nfrom .serializers import MicropostSerializer\nclass MicropostView(RetrieveUpdateDestroyAPIView):\n    serializer_class = MicropostSerializer\n    def get_queryset(self):\n        result = Micropost.objects.filter(\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1696,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "Web Server Structures\n[ 212 ]\n            user__username=self.kwargs['username']\n        )\n        return result\nIn this case, we allow more operations: RETRIEVE (GET), UPDATE (PUT and PATCH), and \nDESTROY (DELETE).\nSerializer\nThe serializer transforms from the Python object of the Model to the JSON result and \nthe other way around. The serializer is defined like this:\nfrom .models import Micropost, Usr\nfrom rest_framework import serializers\nclass MicropostSerializer(serializers.ModelSerializer):\n    href = MicropostHyperlink(source='*', read_only=True)\n    text = serializers.CharField(max_length=255)\n    referenced = serializers.SlugRelatedField(queryset=Usr.objects.all(),\n                                             slug_field='username',\n                                             allow_null=True)\n    user = serializers.CharField(source='user.username', read_only=True)\n    class Meta:\n        model = Micropost\n        fields = ['href', 'id', 'text', 'referenced', 'timestamp', \n'user']\nModelSerializer will automatically detect the fields in the model defined in the Meta \nsubclass. We specified the fields to be included in the fields section. Note that, apart \nfrom the ones that are directly translated, id and timestamp, we include others that \nwill change (user, text, referenced) and an extra one (href). The directly translated \nones are straightforward; we don't need to do anything there.\nThe text field is described again as a CharField, but this time, we limit the maximum \nnumber of characters. \nThe user field is also redescribed as a CharField, but using the source parameter we \ndefine it as the username of the referenced user. The field is defined as read_only.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "Chapter 6\n[ 213 ]\nreferenced is similar to it, but we need to define it as SlugRelatedField, so it \nunderstands that's a reference. A slug is a string that references the value. We define \nthat the slug_field is the username of the reference, and add the queryset to allow \nsearching for it.\nThe href field requires an extra defined class to create a proper URL reference. Let's \ntake a detailed look:\nfrom .models import Micropost, Usr\nfrom rest_framework import serializers\nfrom rest_framework.reverse import reverse\nclass MicropostHyperlink(serializers.HyperlinkedRelatedField):\n    view_name = 'micropost-detail'\n    def get_url(self, obj, view_name, request, format):\n        url_kwargs = {\n            'pk': obj.pk,\n            'username': obj.user.username,\n        }\n        result = reverse(view_name, kwargs=url_kwargs, request=request,\n                         format=format)\n        return result\nclass MicropostSerializer(serializers.ModelSerializer):\n    href = MicropostHyperlink(source='*', read_only=True)\n    ...\nview_name describes the URL that will be used. The reverse call transforms the \nparameters into the proper full URL. This is wrapped in the get_url method. This \nmethod receives mainly the obj parameter with the full object. This full object is \ndefined in the source='*' call to the MicropostHyperlink class in the serializer.\nThe combination of all these factors makes the interface work correctly. Django REST \nframework can also create an interface to help you visualize the whole interface and \nuse it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "Web Server Structures\n[ 214 ]\nFor example, a list will look like this:\nFigure 6.4: Microposts List\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 200,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "Chapter 6\n[ 215 ]\nAnd a micropost page will look like this, which allows you to test different actions \nlike PUT, PATCH, DELETE, and GET.\nFigure 6.5: Microposts page\nDjango REST framework is very powerful and can be used in \ndifferent ways to be sure that it behaves exactly as you expect. It \nhas its own quirks, and it tends to be a little temperamental with \nthe parameters until everything is configured just right. At the \nsame time, it allows you to customize the interface in every aspect. \nBe sure to read the documentation carefully. \nYou can find the whole documentation here: https://www.\ndjango-rest-framework.org/. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "Web Server Structures\n[ 216 ]\nExternal layers\nOn top of the web server, there is the possibility to continue the link by adding \nextra levels that work on the HTTP layer. This allows you to load balance between \nmultiple servers and increase the total throughput of the system. This can be chained \ninto multiple layers, if necessary.\nFigure 6.6: Chained load balancers\nThe route from the user to the edge of our system is handled by the internet, but once \nit reaches the edge load balancer, it directs the requests inside the system. The edge \nload balancer works as a gateway between the external networks and the controlled \nenvironment of our network.\nThe configuration of the network can greatly vary, and in lots of cases multiple load \nbalancers are not required, and the edge load balancer can handle multiple web \nservers directly. The capacity in this case is key, as a load balancer has a limit on the \nnumber of requests that it can take.\nThe edge load balancer is normally the only one that handles \nHTTPS connection, allowing the rest of the system to use only \nHTTP. This is convenient as HTTP requests are easier to cache and \nhandle. HTTPS requests are encoded end to end and cannot be \nproperly cached or analyzed. The internal traffic is protected from \nexternal access and should have robust policies to be sure that only \napproved engineers are able to access it and access logs to audit \naccesses. But at the same time, it can be easily debugged, and any \ntraffic problems can be solved much more easily.\nSome key load balancers can be set up as specialized hardware to \nensure that they have the capacity to handle the required number \nof requests.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1774,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "Chapter 6\n[ 217 ]\nThis multi-layered structure allows you to introduce caching at any point of the \nsystem. This can improve the performance of the system, though it needs to be \ntreated with care to be sure that it's adequate. After all, one of the most difficult \nproblems in software development is the proper handling of the cache and its \ninvalidation.\nSummary\nIn this chapter, we went into the details about how web servers work, and the \ndifferent layers that are involved.\nWe started by describing the fundamental details of the request-response and web \nserver architecture. Then, we moved on to describe a system with three layers, using \nnginx as the front web server and uWSGI to handle multiple Python workers that \nrun Django code.\nWe started with the web server itself, which allows you to serve HTTP, directly \nreturn the static content stored in files, and route it towards the next layer. We \nanalyzed the different configuration elements, including enabling header forwarding \nand logging.\nWe continued by describing how uWSGI works and how it's able to create and set up \ndifferent processes that interact through the WSGI protocol in Python. We described \nhow to set up the interaction with the previous level (the nginx web server) and the \nnext level (the Python code). We also described how the workers can be restarted in \nan orderly way, and how they can be automatically recycled periodically to mitigate \ncertain kinds of problems.\nWe described how Django works to define a web application, and how the requests \nand responses flow through the code, including how the middleware can be used \nto chain elements in the flow. We also introduced Django REST framework as a \nway to create RESTful APIs and show how our example introduced in Chapter 2 \ncan be implemented through the views and serializers provided by Django REST \nframework.\nFinally, we described how the structure can be extended by layers on top to be sure \nto distribute the load across multiple servers and scale the system.\nWe will next describe event-driven systems.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "Web Server Structures\n[ 218 ]\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 290,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "[ 219 ]\n7\nEvent-Driven Structures\nRequest-response is not the only software architecture that can be used in a system. \nThere can also be requests that don't require an immediate response. Perhaps there's \nno interest in a response, as the task can be done without the caller being required \nto wait, or perhaps it takes a long time and the caller doesn't want to be waiting for \nit. In any case, there's the option to, from the point of view of the caller, just send a \nmessage and proceed.\nThis message is called an event, and there are multiple uses for this kind of system. \nIn this chapter, we will introduce the concept, and we will describe in detail one \nof the most popular uses of it: creating asynchronous tasks that are executed in the \nbackground while the caller of the task continues uninterrupted.\nIn the chapter, we will describe the basics of asynchronous tasks, including the \ndetails of queueing systems and how to generate automatically scheduled tasks.\nWe will use Celery as an example of a popular task manager in Python that has \nmultiple capabilities. We will show specific examples of how to perform common \ntasks. We will also explore Celery Flower, a tool that creates a web interface to \nmonitor and control Celery and has an HTTP API that allows you to control that \ninterface, including sending new tasks to execute.\nIn this chapter, we'll cover the following topics:\n•\t\nSending events\n•\t\nAsynchronous tasks\n•\t\nSubdividing tasks\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1561,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 220 ]\n•\t\nScheduled tasks\n•\t\nQueue effects\n•\t\nCelery\nLet's start by describing the basics of event-driven systems.\nSending events\nEvent-driven structures are based on the fire-and-forget principle. Instead of sending \ndata and waiting until the other part returns a response, it just sends data and \ncontinues executing.\nThis makes it different from the request-response architecture that we saw in the \nprevious chapter. A request-response process will wait until an appropriate response \nis generated. Meanwhile, the execution of more code will stop, as the new data \nproduced by the external system is required to continue.\nIn an event-driven system, there's no response data, at least not in the same sense. \nInstead, an event containing the request will be sent, and the task will just continue. \nSome minimal information could be returned to ensure that the event can be \ntracked later.\nThe difference is that the task itself won't be done in the same moment, so getting \nback from generating the event will be very fast. The event, once generated, will \ntravel to a different system that will transmit it towards its destination.\nThis system is called a bus and works to make messages flow through the system. An \narchitecture can use a single bus that acts as a central place to send messages across \nsystems, or it can use multiple ones.\nEvent-driven systems can be implemented with request-response \nservers. This doesn't make them a pure request-response system. \nFor example, a RESTful API that creates an event and returns an \nevent ID. Any work is not done yet, and the only detail returned is \nan identifier to be able to check the status of any follow-up tasks.\nThis is not the only option, as this event ID may be produced \nlocally, or even not be produced at all.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "Chapter 7\n[ 221 ]\nEach of the events will be inserted into a queue. A queue is a logical FIFO system that \nwill transmit the events from the entry point to the defined next stage. At that point, \nanother module will receive the event and process it.\nThis new system is listening to the queue and extracts all the received events to \nprocess them. This worker can't communicate directly with the sender of the event \nthrough the same channel, but it can interact with other elements, like shared \ndatabases or exposed endpoints, and can even send more events into queues to \nfurther process the results.\nMultiple subscribers can tend the same queue, and they'll be extracting events \nin parallel. Multiple publishers can also produce events into the same queue. \nThe capacity of the queue will be described by the number of events that can \nbe processed, and enough subscribers should be provided so the queue can be \nprocessed quickly enough.\nTypical tools that can work as a bus are RabbitMQ, Redis, and Apache Kafka. While \nit is possible to use a tool \"as is,\" there are multiple libraries that will help you work \nwith these tools to create your own way of handling sending messages.\nAsynchronous tasks\nA simple event-driven system is one that allows you to execute asynchronous tasks. \nThe events produced by an event-driven system describe a particular task to execute. \nNormally, each task will require some time to execute, which makes it impractical to \nbe executed directly as part of the publisher code flow.\nIn general, it's advisable to use a single bus to communicate all \nthe systems. There are multiple tools that allow us to implement \nmultiple logical partitions, so the messages are routed to and from \nthe right destinations.\nThe systems at each end of the queue are called the publisher and \nthe subscriber.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 222 ]\nThe typical example is a web server that needs to respond to the user in a reasonable \ntime. Some HTTP timeouts can produce errors if an HTTP request takes too long, \nand generally it is not a great experience to respond in more than a second or two. \nThe solution is to send an event to handle this task, generate a task ID, and return the \ntask ID immediately. The event will be sent to a message queue that will deliver it to \na back-end system. The back-end system will then execute the task, which can take as \nlong as it needs to execute.\nMeanwhile, the task ID can be used to monitor the progress of the execution. The \nback-end task will update the status of the execution in shared storage, like a \ndatabase, so when it's completed, the web front-end can inform the user. This shared \nstorage can also store any produced results that may be interesting.\nFigure 7.1: The flow of an event\nBecause the status of the task is stored in a database that's accessible by the front-end \nweb server, the user can ask for the status of the task at any point by identifying it \nthrough the task ID.\nThese operations that take a long time may involve tasks like \nencoding video into a different resolution, analyzing images with \na complex algorithm, sending 1,000 emails to customers, deleting \na million registers in bulk, copying data from an external database \ninto a local one, generating reports, or pulling data from multiple \nsources. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "Chapter 7\n[ 223 ]\nFigure 7.2: Checking the progress of an async task with shared storage\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 190,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 224 ]\nThe back-end system can produce intermediate updates if necessary, showing when \n25% or 50% of the task has been completed. This will need to be stored in the same \nshared storage.\nThis process is a simplification, though. The queue is usually capable of returning \nwhether a task has been finished or not. The shared storage/database will be \nrequired only if the task is required to return some data. A database works fine for \nsmall results, but if big elements like documents are produced as part of the task, \nthis may not be a valid option and a different kind of storage may be required.\nA shared database is not the only way to be sure that the web server front-end is \ncapable of receiving information. The web server can expose an internal API that \nallows the back-end to send back information. This is, to all effects, the same as \nsending the data to a different external service. The back-end will need to access the \nAPI, configure it, and perhaps be authenticated. The API can be created exclusively \nfor the back-end or can be an API for general usage that also accepts the specific data \nthat the back-end system will produce.\nIn this case, all the information, task IDs, statuses, and results can remain inside the \nweb server's internal storage.\nFor example, if a task is to generate a report, the back-end will \nstore it in document storage like AWS S3 so it's available to be \ndownloaded by the user later.\nSharing access to a database between two different systems \ncan be difficult, as the database will need to be in sync for both \nsystems. We need to detach the systems so they can be deployed \nindependently and without breaking backward compatibility. \nAny change in the schema will require extra care to ensure that the \nsystem can perform at any point, without interruption. Exposing \nan API and keeping the database under the full control of the \nfront-end service is a good solution, but keep in mind that requests \noriginating from the back-end will compete with external requests, \nso we need enough capacity for both.\nRemember that the queue is likely to store the task ID and the \nstatus of the task. This may be replicated for convenience in the \ninternal storage.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2335,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "Chapter 7\n[ 225 ]\nFigure 7.3: Sending back information to the source service\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 178,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 226 ]\nRemember that this API doesn't have to be directed to the same front-end. It can \nalso call any other service, internal or external, generating a complex flow between \nelements. It even creates its own events that will be reintroduced into the queue to \nproduce other tasks.\nSubdividing tasks\nIt's entirely possible to generate more tasks from an initial one. This is done by \ncreating the right event inside a task and sending it to the right queue. \nThis allows a single task to distribute its load and parallelize its action. For example, \nif a task generates a report and sends it by email to a group of recipients, the task can \nfirst generate the report and then send the emails in parallel by creating new tasks \nthat will focus only on creating the emails and attaching the report.\nThis spreads the load over multiple workers, speeding up the process. Another \nadvantage is that individual tasks will be shorter, which makes them easier to \ncontrol, monitor, and operate.\nThe process can be repeated, if necessary, with subtasks creating their own subtasks. \nSome tasks may require creating huge amounts of information in the background, \nso subdividing them may make sense, but it will also increase the complexity of \nfollowing the flow of the code, so use this technique sparingly and only when it \ncreates a clear advantage.\nSome task managers may permit the creation of workflows where \ntasks are distributed, and their results are returned and combined. \nThis can be used in some cases, but in practice it is less useful than \nit initially appears, as it introduces extra waiting and we can end \nup with the task taking a longer time.\nBut easy wins are bulk tasks performing similar actions on \nmultiple elements without the need to combine the results, which \nare quite commonly encountered.\nKeep in mind, though, that this will make the initial task finish \nquickly, making the initial task's ID status a bad way to check \nwhether the whole operation has been completed. The initial task \nmay return the IDs of the new tasks if they need to be monitored.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "Chapter 7\n[ 227 ]\nScheduled tasks\nAsynchronous tasks don't need to be generated directly by a frontend and direct \naction by a user, but can also be set to run at specific times, through a schedule.\nSome examples of scheduled tasks include generating daily reports during night \nhours, updating information hourly via an external API, precaching values so they \nare quickly available later, generating a schedule for next week at the start of the \nweek, and sending reminder emails every hour.\nMost task queues will allow the generation of scheduled tasks, indicating it clearly in \ntheir definition, so they will be triggered automatically.\nSome scheduled tasks can be quite big, such as each night sending emails to \nthousands of recipients. It's very useful to divide a scheduled task, so a small \nscheduled task is triggered just to add all the individual tasks to the queue that will \nbe processed later. This distributes the load and allows the task to finish earlier, \nmaking full use of the system. \nIn the example of sending emails, a single task triggers every night, reading the \nconfiguration and creating a new task for each email found. Then the new tasks \nwill receive the email, compose the body by pulling from external information, \nand send it.\nQueue effects\nAn important element of asynchronous tasks is the effect that introducing a queue \nmay have. As we've seen, the background tasks are slow, meaning that any worker \nrunning them will be busy for some time. \nWe will see later in the chapter how to generate a scheduled task \nfor Celery.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1664,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 228 ]\nMeanwhile, more tasks can be introduced, which may mean that the queue starts \nbuilding up. \nFigure 7.4: Single queue\nOn the one hand, this can be a capacity problem. If the number of workers is not \nsufficient to handle the average number of tasks introduced in the queue, the queue \nwill build up until it reaches its limit, and new tasks will be rejected.\nBut typically, the load doesn't work like a constant influx of tasks. Instead, there are \ntimes when there are no tasks to execute, and other times when there's a sudden \nspike in the number of tasks to be executed, filling the queue. Also, there's a need \nto calculate the right number of workers to keep running to be sure that the waiting \nperiod for those spikes, where a task gets delayed because all the workers are busy, \nis not causing problems.\nCalculating the \"right\" amount of workers can be difficult, but with \na bit of trial and error a \"good enough\" number can be obtained. \nThere's a mathematical tool to deal with it, queueing theory, which \ncalculates it based on several parameters.\nIn any case, these days resources for each worker are cheap and it's \nnot imperative to generate the exact number of workers, as long as \nit's close enough so that any possible spike can be processed in a \nreasonable amount of time. \nYou can learn more about queueing theory at http://people.\nbrunel.ac.uk/~mastjjb/jeb/or/queue.html.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "Chapter 7\n[ 229 ]\nAn extra difficulty, as we saw with scheduled tasks, is that at a specific time, a \nconsiderable number of tasks can be triggered at the same time. This can saturate \nthe queue at a particular time, requiring perhaps an hour to digest all the tasks, for \nexample, creating daily reports, ingesting new updates in an external API every 4 \nhours, or aggregating data for the week. \nThis means that, for example, if 100 tasks to create background reports are added, \nthey will block a task to generate a report sent by a user, which will produce a bad \nexperience. The user will have to wait for far too long if they ask for the report a few \nminutes after the scheduled tasks were fired.\nA possible solution is to use multiple queues, with different workers pulling from \nthem. \nFigure 7.5: Priority and background queue\nThis makes those different tasks go to different workers, making it possible \nto reserve capacity for certain tasks to run uninterrupted. In our example, the \nbackground reports can go to their own dedicated workers, and the user reports \nhave their own workers as well. This, though, wastes capacity. If the background \nreports run only once a day, once the 100 tasks are processed, the workers will be \nidle for the rest of the day, even if there's a long queue in the worker serving the user \nreports.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 230 ]\nInstead of that, a mixed approach can be used.\nFigure 7.6: Regular worker pulling from multiple queues\nIn this case, the user report worker will continue with the same approach, but the \nbackground report worker will pull tasks from both queues. In this case, we limit \nthe capacity for background reports, but at the same time, we increase it for the user \nreport tasks when there's available capacity.\nWe reserve capacity for the user report tasks, which are priority, and make the rest of \nthe workers pull from all available tasks, including priority and non-priority tasks.\nTo be able to divide work into these two queues, the tasks need to be divided \ncarefully:\n•\t\nPriority tasks. They are started on behalf of the user. They are time sensitive. \nThey are fast to execute, so latency is important.\n•\t\nBackground tasks. Normally started by automated systems and scheduled \ntasks. They are less time sensitive. They can run for long periods, so higher \nlatency is easier to accept.\nThe balance between them should be maintained. If too many tasks are labeled as \npriority, the queue will be quickly filled, rendering it pointless.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "Chapter 7\n[ 231 ]\nThe number of priority workers can be tweaked based on the number and frequency \nof spikes and expected turnaround time. Only enough priority workers to cover \nregular traffic at the times where there are big spikes in background tasks are \nrequired, as long as those spikes are predictable.\nAn alternative is to generate a priority system based on specific priorities, like \nnumbers. That way, a task with priority 3 will be executed before a task with priority \n2, and that before a task with priority 1, and so on. The great advantage of having \npriorities is that the workers can be working all the time, without wasting any \ncapacity. \nBut this approach has some problems:\n•\t\nA lot of queue backends don't support it efficiently. To keep a queue sorted \nby priority costs more than just assigning tasks to a plain queue. In practice, \nit may not produce as good results as you expect, requiring many tweaks and \nadjustments.\n•\t\nIt means you need to deal with priority inflation. It's very easy for teams \nto start increasing the priority of tasks over time, especially if multiple \nteams are involved. The decision on what task should return first could get \ncomplicated and pressure can grow the priority numbers over time. \nWhile it can appear that a sorted queue is ideal, the simplicity of two levels (priority \nand background) makes it very easy to understand the system and generates easy \nexpectations when developing and creating new tasks. It's way easier to tweak and \nunderstand and will generate better results with less work.\nThere's always the temptation to generate multiple queues to set \nup different priorities and reserve capacity for each of them. This \nis normally not a good idea, as they will waste capacity. The most \nefficient system is one with a single queue, as all capacity will be \nalways used. There is a problem of priority, though, as it makes \nsome tasks take too long. More than two queues overcomplicates \nand risks wasting capacity where many workers are idle most of \nthe time, while other queues are filled. The simplicity of two queues \nhelps develop the discipline of deciding between only two options \nand makes it easy to understand why we want multiple queues.\nGood metrics are critical for monitoring and understanding the \nbehavior of the queue. We will talk more about metrics in Chapter \n13, Metrics.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 232 ]\nSingle code for all workers\nWhen having different workers pulling from different queues, the worker could have \ndifferent codebases, making one with priority tasks and another with background \ntasks.\nThis is generally not advisable, as it will differentiate the codebase and require \nmaintaining two code bases in parallel, with some problems:\n•\t\nIt's likely that some tasks or task parts will be either priority or background, \ndepending on what system or user triggers them. For example, reports that \ncan be either produced on the fly for a user, or daily as part of a batch process \nto finally send them by mail. The report generation should remain common, \nso any change is applied to both.\n•\t\nHandling two codebases instead of one is more inconvenient. A big part of \nthe general code is shared, so updates will need to be run independently.\n•\t\nA unique codebase can handle all kinds of tasks. That makes it possible \nto have a worker that handles both priority and background tasks. Two \ncodebases will require strict task separation, not using the extra capacity \navailable in the background workers to help with priority tasks.\nIt is better to use a single worker when building, and through the configuration \ndecide to receive messages from one queue or both. This simplifies the architecture \nfor local development and testing.\nNote that for this to work, it will require strict separation of tasks. \nMore about this a bit later.\nThis may not be adequate when the nature of the tasks may \ncreate conflicts. For example, if some of the tasks require big \ndependencies or specialized hardware (as could be the case with \nsome AI-related tasks) this may require that specific tasks run \nin dedicated workers, making it impractical for them to share \nthe same codebase. These cases are rare, and unless they are \nencountered, it's better to try to consolidate and use the same \nworker for all tasks.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "Chapter 7\n[ 233 ]\nCloud queues and workers\nThe main characteristic of cloud computing is that services can be started and \nstopped dynamically, allowing us to use only the resources required at a particular \nmoment. This allows the system to increase and decrease capacity quickly.\nIn cloud environments, it's possible that the number of workers extracting events \nfrom a queue can be modified. That alleviates the problems with resourcing that \nwe discussed above. Do we have a full queue? Increase the number of workers on \ndemand! Ideally, we could even spawn a single worker for each event that spawns a \ntask, making the system infinitely scalable.\nThis, obviously, is easier said than done, as there are some issues with trying to \ndynamically create workers on the spot:\n•\t\nThe start-up time can add significant time to the execution of the task, even to \nthe point of being longer than the execution time of the task itself. Depending \non how heavy the creation of a worker is, starting it can take a significant \namount of time.\nIn the traditional cloud setting, the lowest granularity required to start a new \nvirtual server, which is relatively heavy, takes at least a couple of minutes. \nWith newer tools, such as containers, this can be sped up sensibly, but the \nunderlying principle will remain, as at some point in time a new virtual \nserver will need to be spawned.\n•\t\nA single new virtual worker may be too big for a single worker, making it \ninefficient to spawn one for each task. Again, containerized solutions can \nhelp by making it easier to separate between creating a new container and \nrequiring spinning up a new virtual server in the cloud service.\n•\t\nAny cloud service should have limits. Each new worker created costs \nmoney and cloud services can get very expensive if scaled up without \ncontrol. Without certain control on the cost side of things, this can grow \nto be a problem due to high, unexpected costs. Normally this can happen \nby accident, with some explosion of workers due to some problem on the \nsystem, but there's also a security attack, called Cash Overflow, aimed at \nmaking a service run as expensively as possible to force the owner of the \nservice to stop it or even bankrupt them.\nBecause of these problems, normally a solution will need to work in sort of a batched \nway, allowing extra space to grow and generating extra virtual servers only when \nthey are required to reduce the queue. In the same way, when the extra capacity is \nnot required any more, it will be removed.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2630,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 234 ]\nThe process should be similar to this:\nFigure 7.7: Starting up a new server \nExtra care should be taken to be sure that all the workers located \nin the same virtual server are idle before stopping it. This is \ndone automatically by stopping the servers gracefully, so they'll \nfinish any remaining tasks, start no new ones, and finish when \neverything is done.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 494,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "Chapter 7\n[ 235 ]\nKnowing exactly when a new server should be spawned depends greatly on the \nrequirements for latency, traffic, and the speed of creating a new server (if the server \nstarts quickly, perhaps it can be less aggressive in scaling up).\nCelery\nCelery is the most popular task queue created in Python. It allows us to create new \ntasks easily and can handle the creation of the events that trigger new tasks.\nCelery requires to work to set up a broker, which will be used as a queue to handle \nthe messages.\nThe code that creates the message will add it to the broker, and the broker will pass \nit to one of the connected workers. When everything happens with Python code, \nwhere the celery package can be installed, it's simple to operate. We'll see later how \nto operate it in other cases.\nCelery can use multiple systems as brokers. The most popular are Redis and \nRabbitMQ. \nA good starting point is to create a new server each time the queue \nhas a number of tasks equal to or greater than the number of \nworkers in a single server. That triggers a new server that will be \nable to handle those tasks. If the creation is triggered with fewer \ntasks than that, it will create a server that is not quite filled. If the \nstart-up time is very long, this can be reduced to ensure that the \nnew server is up before there's a significant queue building up. But \nthis will require experimentation and testing for a specific system.\nIn Celery parlance, the broker is the message queue, while the \nbackend is reserved for interacting with a storage system to return \ninformation.\nIn our examples, we will use Redis as it can be used for the broker \nand the backend, and it's widely available in cloud systems. It's \nalso quite scalable and handles big loads easily.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1875,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 236 ]\nUsing a backend is optional, as tasks don't need to define a return value, and it's very \ncommon that asynchronous tasks don't directly return response data other than the \nstatus of the task. The key word here is \"directly\"; sometimes, a task will generate an \nexternal result that can be accessible, but not through the Celery system.\nSome examples of these values are reports that can be stored in other storage \nfacilities, emails sent during task processing, and pre-caching of values, where there \nis not a direct result, but there's new data generated and stored in other places.\nThe returning value needs also to be small enough that it can be stored in the system \nworking as the backend. Also, if strong persistence is used, it's recommended that a \ndatabase is used as the backend.\nWe will use the example present on GitHub: https://github.com/PacktPublishing/\nPython-Architecture-Patterns/tree/main/chapter_07_event_driven/celery_\nexample. We will use the example to create a task to retrieve, from an external API, \npending TO DO actions by some users, and generate an email to send as a reminder.\nLet's take a look at the code.\nConfiguring Celery\nThe code is divided into two files: celery_tasks.py, which describes the tasks, and \nstart_task.py, which connects with the queue and enqueues a task.\nAt the start of each, we need to configure the broker to use. In this case, we will use a \nRedis server running in the localhost:\nfrom celery import Celery\napp = Celery('tasks', broker='redis://localhost')\nAs a prerequisite, we need to set up a Redis server running in our expected \nlocalhost address. An easy way of doing so, if you have Docker installed, is to start \na container:\n$ docker run -d -p 6379:6379 redis\nRemember to install the required dependencies by running pip \ninstall -r requirements.txt.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "Chapter 7\n[ 237 ]\nThis starts the standard Redis container that will expose the service over the \nstandard port, 6379. That will connect automatically with the previous broker URL of \nredis://localhost.\nThis is all the configuration that's required, and it will allow both sides, the publisher \nand the subscriber, to connect to the queue.\nCelery worker\nWe will use https://jsonplaceholder.typicode.com/ to simulate calling an external \nAPI. This testing site exposes an accessible REST endpoint to retrieve some mock \ninformation. You can see their definition, but basically, we will access the /todos and \n/users endpoints. The /todos endpoint exposes actions stored by the users, so we \nwill query them to retrieve pending actions, and combine this with the information \nin the /users endpoint.\nThe celery_tasks.py worker defines a main task, obtain_info, and a secondary \ntask, send_email. The first one pulls the information from the API and decides what \nemails need to be sent. The second then sends the email. \nThe file starts with the configuration of the queue and imports:\nfrom celery import Celery\nimport requests\nfrom collections import defaultdict\napp = Celery('tasks', broker='redis://localhost')\nlogger = app.log.get_default_logger()\nBASE_URL = 'https://jsonplaceholder.typicode.com'\nThe logger definition permits the use of native Celery logs that will be streamed into \nthe Celery configuration for logs. By default, this is the standard output.\nLet's take a look at the obtain_info task. Note the @app.task that defines the function \nas a Celery task:\n@app.task\ndef obtain_info():\nThe sending of the email is just mocked to avoid complicating the \nsystem and needing to handle mocked email addresses. It's left as \nan exercise for the reader.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 238 ]\n    logger.info('Stating task')\n    users = {}\n    task_reminders = defaultdict(list)\n    # Call the /todos endpoint to retrieve all the tasks\n    response = requests.get(f'{BASE_URL}/todos')\n    for task in response.json():\n        # Skip completed tasks\n        if task['completed'] is True:\n            continue\n        # Retrieve user info. The info is cached to only ask\n        # once per user\n        user_id = task['userId']\n        if user_id not in users:\n            users[user_id] = obtain_user_info(user_id)\n        info = users[user_id]\n        # Append the task information to task_reminders, that\n        # aggregates them per user\n        task_data = (info, task)\n        task_reminders[user_id].append(task_data)\n    # The data is ready to process, create an email per\n    # each user\n    for user_id, reminders in task_reminders.items():\n        compose_email(reminders)\n    logger.info('End task')\nWe wrap the function with INFO logs to provide context to the task execution. \nFirst, it calls the /todos endpoint on this line, which then goes through each task \nindependently, skipping any completed task.\n    response = requests.get(f'{BASE_URL}/todos')\n    for task in response.json():\n        if task['completed'] is True:\n            continue\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1400,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "Chapter 7\n[ 239 ]\nThen, it checks the information for the user and puts it into the info variable. \nBecause this information can be used multiple times in the same loop, it is cached in \nthe users dictionary. Once the info is cached, it's not asked for again:\n        user_id = task['userId']\n        if user_id not in users:\n            users[user_id] = obtain_user_info(user_id)\n        info = users[user_id]\nThe individual task data is added to a list created to store all the tasks for a user. The \ntask_reminders dictionary is created as a defaultdict(list), meaning that the first \ntime a particular user_id is accessed, if it's not present, it will be initialized as an \nempty list, allowing a new element to be appended.\n        task_data = (info, task)\n        task_reminders[user_id].append(task_data)\nFinally, the stored elements in task_reminders are iterated to compose the resulting \nemail:\n    for user_id, reminders in task_reminders.items():\n        compose_email(reminders)\nTwo follow-up functions are called: obtain_user_info and compose_email.\nobtain_user_info retrieves the information directly from the /users/{user_id} \nendpoint and returns it:\ndef obtain_user_info(user_id):\n    logger.info(f'Retrieving info for user {user_id}')\n    response = requests.get(f'{BASE_URL}/users/{user_id}')\n    data = response.json()\n    logger.info(f'Info for user {user_id} retrieved')\n    return data\ncompose_email takes the information in the task list, which includes a group of user_\ninfo, task_info, extracts the title information for each task_info, then the email \nfrom the matched user_info, and then calls the send_email task:\ndef compose_email(remainders):\n    # remainders is a list of (user_info, task_info)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 240 ]\n    # Retrieve all the titles from each task_info\n    titles = [task['title'] for _, task in remainders]\n    # Obtain the user_info from the first element\n    # The user_info is repeated and the same on each element\n    user_info, _ = remainders[0]\n    email = user_info['email']\n    # Start the task send_email with the proper info\n    send_email.delay(email, titles)\nAs you can see, the send_email task includes a .delay call, which enqueues this task \nwith the appropriate parameters. send_email is another Celery task. It is very simple \nas we are just mocking the email delivery. It just logs its parameters:\n@app.task\ndef send_email(email, remainders):\n    logger.info(f'Send an email to {email}')\n    logger.info(f'Reminders {remainders}')\nTriggering tasks\nThe start_task.py script contains all the code to trigger the task. This is a simple \nscript that imports the task from the other file.\nfrom celery_tasks import obtain_info\nobtain_info.delay()\nNote that it inherits all the configuration from celery_tasks.py when doing the \nimport.\nImportantly, it calls the task with .delay(). This sends the task to the queue so the \nworker can pull it out and execute it.\nLet's see now how both files interact.\nNote that if you call the task directly with obtain_info(), you'll \nexecute the code directly, instead of submitting the task to the \nqueue.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "Chapter 7\n[ 241 ]\nConnecting the dots\nTo be able to set both parts, the publisher and the consumer, first start the worker \ncalling style:\n$ celery -A celery_tasks worker --loglevel=INFO -c 3\nThis starts the celery_tasks module (the celery_tasks.py file) with the -A \nparameter. It sets the log level to INFO and starts three workers with the -c 3 \nparameter. It will display a starting log similar to this one:\n$ celery -A celery_tasks worker --loglevel=INFO -c 3\n   v5.1.1 (sun-harmonics)\nmacOS-10.15.7-x86_64-i386-64bit 2021-06-22 20:14:09\n[config]\n.> app:         tasks:0x110b45760\n.> transport:   redis://localhost:6379//\n.> results:     disabled://\n.> concurrency: 3 (prefork)\n.> task events: OFF (enable -E to monitor tasks in this worker)\n[queues]\n.> celery           exchange=celery(direct) key=celery\n[tasks]\n  . celery_tasks.obtain_info\n  . celery_tasks.send_email\n[2021-06-22 20:14:09,613: INFO/MainProcess] Connected to redis://\nlocalhost:6379//\n[2021-06-22 20:14:09,628: INFO/MainProcess] mingle: searching for \nneighbors\n[2021-06-22 20:14:10,666: INFO/MainProcess] mingle: all alone\nNote: Some of the modules used, such as Celery, might not be \ncompatible with Windows systems. More information can be \nfound at https://docs.celeryproject.org/en/stable/faq.\nhtml#does-celery-support-windows.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1408,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 242 ]\nNote that it displays the two available tasks, obtain_info and send_email. In another \nwindow, we can send tasks calling the start_task.py script:\n$ python3 start_task.py\nThis will trigger the task in the Celery worker, producing logs (edited for clarity and \nbrevity). We will explain the logs in the next paragraphs.\n[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_\ninfo[5f6c9441-9dda-40df-b456-91100a92d42c] received\n[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task\n[2021-06-22 20:30:52,899: INFO/ForkPoolWorker-2] Retrieving info for \nuser 1\n...\n[2021-06-22 20:30:54,128: INFO/MainProcess] Task celery_tasks.send_\nemail[08b9ed75-0f33-48f8-8b55-1f917cfdeae8] received\n[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_\nemail[d1f6c6a0-a416-4565-b085-6b0a180cad37] received\n[2021-06-22 20:30:54,132: INFO/ForkPoolWorker-1] Send an email to \nSincere@april.biz\n[2021-06-22 20:30:54,134: INFO/ForkPoolWorker-1] Reminders ['delectus \naut autem', 'quis ut nam facilis et officia qui', 'fugiat veniam \nminus', 'laboriosam mollitia et enim quasi adipisci quia provident \nillum', 'qui ullam ratione quibusdam voluptatem quia omnis', 'illo \nexpedita consequatur quia in', 'molestiae perspiciatis ipsa', 'et \ndoloremque nulla', 'dolorum est consequatur ea mollitia in culpa']\n[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.\nsend_email[08b9ed75-0f33-48f8-8b55-1f917cfdeae8] succeeded in \n0.004046451000021989s: None\n[2021-06-22 20:30:54,137: INFO/ForkPoolWorker-3] Send an email to \nShanna@melissa.tv\n[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.\nobtain_info[5f6c9441-9dda-40df-b456-91100a92d42c] succeeded in \n1.5507660419999638s: None\n...\n[2021-06-22 20:30:54,141: INFO/ForkPoolWorker-3] Task celery_tasks.\nsend_email[d1f6c6a0-a416-4565-b085-6b0a180cad37] succeeded in \n0.004405897999959052s: None\n[2021-06-22 20:30:54,192: INFO/ForkPoolWorker-2] Task celery_tasks.\nsend_email[aff6dfc9-3e9d-4c2d-9aa0-9f91f2b35f87] succeeded in \n0.0012900159999844618s: None\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "Chapter 7\n[ 243 ]\nBecause we started three different workers, the logs are intertwined. Pay attention to \nthe first task, which corresponds to obtain_info. This task has been executed in the \nworker ForkPoolWorker-2 in our execution.\n[2021-06-22 20:30:52,627: INFO/MainProcess] Task celery_tasks.obtain_\ninfo[5f6c9441-9dda-40df-b456-91100a92d42c] received\n[2021-06-22 20:30:52,632: INFO/ForkPoolWorker-2] Stating task\n[2021-06-22 20:30:52,899: INFO/ForkPoolWorker-2] Retrieving info for \nuser 1\n...\n[2021-06-22 20:30:54,181: INFO/ForkPoolWorker-2] Task celery_tasks.\nobtain_info[5f6c9441-9dda-40df-b456-91100a92d42c] succeeded in \n1.5507660419999638s: None\nWhile this task is being executed, the send_email tasks are also being enqueued and \nexecuted by the other workers. \nFor example:\n[2021-06-22 20:30:54,133: INFO/MainProcess] Task celery_tasks.send_\nemail[d1f6c6a0-a416-4565-b085-6b0a180cad37] received\n[2021-06-22 20:30:54,132: INFO/ForkPoolWorker-1] Send an email to \nSincere@april.biz\n[2021-06-22 20:30:54,134: INFO/ForkPoolWorker-1] Reminders ['delectus \naut autem', 'quis ut nam facilis et officia qui', 'fugiat veniam \nminus', 'laboriosam mollitia et enim quasi adipisci quia provident \nillum', 'qui ullam ratione quibusdam voluptatem quia omnis', 'illo \nexpedita consequatur quia in', 'molestiae perspiciatis ipsa', 'et \ndoloremque nulla', 'dolorum est consequatur ea mollitia in culpa']\n[2021-06-22 20:30:54,135: INFO/ForkPoolWorker-1] Task celery_tasks.\nsend_email[08b9ed75-0f33-48f8-8b55-1f917cfdeae8] succeeded in \n0.004046451000021989s: None\nAt the end of the execution, there's a log showing the time it has taken, in seconds.\nIf only one worker is involved, the tasks will be run consecutively, \nmaking it easier to differentiate between tasks.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 244 ]\nWe can see how the send_email tasks start before the end of the obtain_info task, \nand that there are still send_email tasks running after the end of the obtain_info \ntask, showing how the tasks are running independently.\nScheduled tasks\nInside Celery, we can also generate tasks with a certain schedule, so they can be \ntriggered automatically at the proper time.\nTo do so, we need to define a task and a schedule. We defined them in the celery_\nscheduled_tasks.py file. Let's take a look:\nfrom celery import Celery\nfrom celery.schedules import crontab\napp = Celery('tasks', broker='redis://localhost')\nlogger = app.log.get_default_logger()\n@app.task\ndef scheduled_task(timing):\n    logger.info(f'Scheduled task executed {timing}')\napp.conf.beat_schedule = {\n    # Executes every 15 seconds\n    'every-15-seconds': {\n        'task': 'celery_scheduled_tasks.scheduled_task',\n        'schedule': 15,\n        'args': ('every 15 seconds',),\n    },\n    # Executes following crontab\n    'every-2-minutes': {\n        'task': 'celery_scheduled_tasks.scheduled_task',\n        'schedule': crontab(minute='*/2'),\n        'args': ('crontab every 2 minutes',),\n    },\n}\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "Chapter 7\n[ 245 ]\nThis file starts with the same configuration as the previous example, and we define a \nsmall, simple task that just displays when it is executed.\n@app.task\ndef scheduled_task(timing):\n    logger.info(f'Scheduled task executed {timing}')\nThe interesting bit comes later, as the schedule is configured in the app.conf.beat_\nschedule parameter. We created two entries.\napp.conf.beat_schedule = {\n    # Executes every 15 seconds\n    'every-15-seconds': {\n        'task': 'celery_scheduled_tasks.scheduled_task',\n        'schedule': 15,\n        'args': ('every 15 seconds',),\n    },\nThe first one defines an execution of the proper task every 15 seconds. The task needs \nto include the module name (celery_scheduled_tasks). The schedule parameter \nis defined in seconds. The args parameter contains any parameter to pass for the \nexecution. Note that it's defined as a list of parameters. In this case, we create a tuple \nwith a single entry, as there's only one argument.\nThe second entry defines the schedule instead as a crontab entry.\n    # Executes following crontab\n    'every-2-minutes': {\n        'task': 'celery_scheduled_tasks.scheduled_task',\n        'schedule': crontab(minute='*/2'),\n        'args': ('crontab every 2 minutes',),\n    },\nThis crontab object, which is passed as the schedule parameter, executes the task \nonce every two minutes. Crontab entries are very flexible and allow for a wide range \nof possible actions.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 246 ]\nSome examples are as follows:\nCrontab entry\nDescription\ncrontab()\nExecute every minute, the lowest possible \nresolution\ncrontab(minute=0)\nExecute every hour, at minute 0\ncrontab(minute=15)\nExecute hourly, at minute 15\ncrontab(hour=0, minute=0)\nExecute daily, at midnight (in your time \nzone)\ncrontab(hour=6, minute=30, day_of_\nweek='monday')\nExecute every Monday, at 6:30\ncrontab(hour='*/8', minute=0)\nExecute every hour divisible by 8 (0, 8, 16). \nThree times a day, at minute 0 in each case\ncrontab(day_of_month=1, hour=0, \nminute=0)\nExecute on the first of each month, at \nmidnight\ncrontab(minute='*/2')\nExecute every minute divisible by 2. Once \nevery two minutes\nThere are more options, including relating the time to solar times, like dawn and \ndusk, or custom schedulers, but most use cases will be perfectly fine either once \nevery X seconds or with a crontab definition.\nTo start the scheduler, we need to start a specific worker, the beat worker:\n$ celery -A celery_scheduled_tasks beat\ncelery beat v4.4.7 (cliffs) is starting.\n__    -    ... __   -        _\nLocalTime -> 2021-06-28 13:53:23\nConfiguration ->\n    . broker -> redis://localhost:6379//\n    . loader -> celery.loaders.app.AppLoader\n    . scheduler -> celery.beat.PersistentScheduler\n    . db -> celerybeat-schedule\n    . logfile -> [stderr]@%WARNING\n    . maxinterval -> 5.00 minutes (300s) \nYou can check the full documentation here: https://docs.\nceleryproject.org/en/stable/userguide/periodic-tasks.\nhtml#starting-the-scheduler.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "Chapter 7\n[ 247 ]\nWe start the celery_scheduled_tasks worker in the usual way.\n$ celery -A celery_scheduled_tasks worker --loglevel=INFO -c 3\nBut you can see that there's still no incoming tasks. We need to start celery beat, \nwhich is a specific worker that inserts the tasks in the queue:\n$ celery -A celery_scheduled_tasks beat\ncelery beat v4.4.7 (cliffs) is starting.\n__    -    ... __   -        _\nLocalTime -> 2021-06-28 15:13:06\nConfiguration ->\n    . broker -> redis://localhost:6379//\n    . loader -> celery.loaders.app.AppLoader\n    . scheduler -> celery.beat.PersistentScheduler\n    . db -> celerybeat-schedule\n    . logfile -> [stderr]@%WARNING\n    . maxinterval -> 5.00 minutes (300s)\nOnce celery beat is started, you'll start seeing the tasks being scheduled and \nexecuted as expected:\n[2021-06-28 15:13:06,504: INFO/MainProcess] Received task: celery_\nscheduled_tasks.scheduled_task[42ed6155-4978-4c39-b307-852561fdafa8]\n[2021-06-28 15:13:06,509: INFO/MainProcess] Received task: celery_\nscheduled_tasks.scheduled_task[517d38b0-f276-4c42-9738-80ca844b8e77]\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task \nexecuted crontab every 2 minutes\n[2021-06-28 15:13:06,511: INFO/ForkPoolWorker-2] Task celery_scheduled_\ntasks.scheduled_task[42ed6155-4978-4c39-b307-852561fdafa8] succeeded in \n0.0016690909999965697s: None\n[2021-06-28 15:13:06,512: INFO/ForkPoolWorker-1] Task celery_scheduled_\ntasks.scheduled_task[517d38b0-f276-4c42-9738-80ca844b8e77] succeeded in \n0.0014504210000154671s: None\n[2021-06-28 15:13:21,486: INFO/MainProcess] Received task: celery_\nscheduled_tasks.scheduled_task[4d77b138-283c-44c8-a8ce-9183cf0480a7]\n[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\n[2021-06-28 15:13:21,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\ntasks.scheduled_task[4d77b138-283c-44c8-a8ce-9183cf0480a7] succeeded in \n0.0005252540000242334s: None\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 248 ]\n[2021-06-28 15:13:36,486: INFO/MainProcess] Received task: celery_\nscheduled_tasks.scheduled_task[2eb2ee30-2bcd-45af-8ee2-437868be22e4]\n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\ntasks.scheduled_task[2eb2ee30-2bcd-45af-8ee2-437868be22e4] succeeded in \n0.000493534999975509s: None\n[2021-06-28 15:13:51,486: INFO/MainProcess] Received task: celery_\nscheduled_tasks.scheduled_task[c7c0616c-857a-4f7b-ae7a-dd967f9498fb]\n[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\n[2021-06-28 15:13:51,489: INFO/ForkPoolWorker-2] Task celery_scheduled_\ntasks.scheduled_task[c7c0616c-857a-4f7b-ae7a-dd967f9498fb] succeeded in \n0.0004461000000333115s: None\n[2021-06-28 15:14:00,004: INFO/MainProcess] Received task: celery_\nscheduled_tasks.scheduled_task[59f6a323-4d9f-4ac4-b831-39ca6b342296]\n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task \nexecuted crontab every 2 minutes\n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Task celery_scheduled_\ntasks.scheduled_task[59f6a323-4d9f-4ac4-b831-39ca6b342296] succeeded in \n0.0004902660000425385s: None\nYou can see that both kinds of tasks are scheduled accordingly. In this log, check the \ntimes and see that they are 15 seconds apart:\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\n[2021-06-28 15:13:21,488: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\n[2021-06-28 15:13:36,489: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\n[2021-06-28 15:13:51,488: INFO/ForkPoolWorker-2] Scheduled task \nexecuted every 15 seconds\nThe other task happens exactly every 2 minutes. Note that the first execution may \nnot be totally precise. In this case, the schedule was triggered in the later seconds of \n15:12 and still got executed later than that. In any case, it will be within the 1-minute \nresolution window of the crontab.\n[2021-06-28 15:13:06,510: INFO/ForkPoolWorker-1] Scheduled task \nexecuted crontab every 2 minutes\n[2021-06-28 15:14:00,006: INFO/ForkPoolWorker-2] Scheduled task \nexecuted crontab every 2 minutes\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "Chapter 7\n[ 249 ]\nWhen creating periodic tasks, keep in mind the different priorities, as we described \npreviously in the chapter.\nThis leads to the way of monitoring how the different tasks are being executed, in a \nbetter way than just by checking the logs.\nCelery Flower\nObtaining good monitoring in Celery is important if you want to understand the \nexecuted tasks and find and fix problems. A good tool for that is Flower, which \nenhances Celery by adding a real-time monitoring web page that allows you to \ncontrol Celery through the web page and through an HTTP API.\nIt's also very easy to set up and integrate with Celery. First, we need to be sure that \nthe flower package is installed. The package is included in the requirements.txt \nafter the previous step, but if it's not, you can install it independently using pip3.\n$ pip3 install flower\nOnce it is installed, you can start flower with the following command:\n$ celery --broker=redis://localhost flower -A celery_tasks  --port=5555\n[I 210624 19:23:01 command:135] Visit me at http://localhost:5555\n[I 210624 19:23:01 command:142] Broker: redis://localhost:6379//\n[I 210624 19:23:01 command:143] Registered tasks:\n    ['celery.accumulate',\n     'celery.backend_cleanup',\n     'celery.chain',\n     'celery.chord',\n     'celery.chord_unlock',\nIt is good practice to use a periodic task as a \"heartbeat\" to check \nthat the system is working correctly. This task can be used to \nmonitor that the tasks in the system are flowing as expected, with \nno big delays or problems.\nYou can check the whole documentation at https://flower.\nreadthedocs.io/en/latest/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1719,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 250 ]\n     'celery.chunks',\n     'celery.group',\n     'celery.map',\n     'celery.starmap',\n     'celery_tasks.obtain_info',\n     'celery_tasks.send_email']\n[I 210624 19:23:01 mixins:229] Connected to redis://localhost:6379//\nThe command is very similar to starting the Celery workers, but includes the \ndefinition of the broker using Redis, as we saw before, with --broker=redis://\nlocalhost, and specifying the port to expose, --port=5555.\nThe interface is exposed in http://localhost:5555.\nFigure 7.8: Celery Flower interface\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 655,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "Chapter 7\n[ 251 ]\nThe front page shows the different workers in the system. Note that it shows the \nnumber of active tasks, as well as processed tasks. In this case, we have 11 tasks \ncorresponding to a whole run of start_task.py. You can go to the Tasks tab to see \nthe details of each of the tasks executed, which looks like this:\nFigure 7.9: Tasks page\nYou can see information such as the input parameters, the state of the task, the name \nof the task, and how long it ran for.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 582,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 252 ]\nEach Celery process will appear independently, even if it's capable of running \nmultiple workers. You can check its parameters on the Worker page. See the Max \nconcurrency parameter.\nFigure 7.10: Worker page\nFrom here, you can also review and change the configuration of the number of \nworkers per Celery process, set rate limits, and more.\nFlower HTTP API\nA great addition from Flower is the HTTP API, which allows us to control Flower \nthrough HTTP calls. This enables the automatic control of the system and allows us \nto trigger the tasks directly with an HTTP request. This can be used to call the tasks \nin any programming language, and greatly increases the flexibility of Celery.\nThe URL to call a task asynchronously is the following:\nPOST /api/task/async-apply/{task}\nIt requires a POST, and the arguments of the call should be included in the body. For \nexample, make a call with curl:\n$ curl -X POST -d '{\"args\":[\"example@email.com\",[\"msg1\", \"msg2\"]]}' \nhttp://localhost:5555/api/task/async-apply/celery_tasks.send_email\n{\"task-id\": \"79258153-0bdf-4d67-882c-30405d9a36f0\"}\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1218,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "Chapter 7\n[ 253 ]\nThe task is executed in the worker:\n[2021-06-24 22:35:33,052: INFO/MainProcess] Received task: celery_\ntasks.send_email[79258153-0bdf-4d67-882c-30405d9a36f0]\n[2021-06-24 22:35:33,054: INFO/ForkPoolWorker-2] Send an email to \nexample@email.com\n[2021-06-24 22:35:33,055: INFO/ForkPoolWorker-2] Reminders ['msg1', \n'msg2']\n[2021-06-24 22:35:33,056: INFO/ForkPoolWorker-2] Task celery_tasks.\nsend_email[79258153-0bdf-4d67-882c-30405d9a36f0] succeeded in \n0.0021811629999999305s: None\nUsing the same API, the status of the task can be retrieved with a GET request:\nGET /api/task/info/{task_id}\nFor example:\n$ curl  http://localhost:5555/api/task/info/79258153-0bdf-4d67-882c-\n30405d9a36f0\n{\"uuid\": \"79258153-0bdf-4d67-882c-30405d9a36f0\", \"name\": \"celery_tasks.\nsend_email\", \"state\": \"SUCCESS\", \"received\": 1624571191.674537, \"sent\": \nnull, \"started\": 1624571191.676534, \"rejected\": null, \"succeeded\": \n1624571191.679662, \"failed\": null, \"retried\": null, \"revoked\": null, \n\"args\": \"['example@email.com', ['msg1', 'msg2']]\", \"kwargs\": \"{}\", \n\"eta\": null, \"expires\": null, \"retries\": 0, \"worker\": \"celery@Jaimes-\niMac-5K.local\", \"result\": \"None\", \"exception\": null, \"timestamp\": \n1624571191.679662, \"runtime\": 0.0007789200000161145, \"traceback\": \nnull, \"exchange\": null, \"routing_key\": null, \"clock\": 807, \"client\": \nnull, \"root\": \"79258153-0bdf-4d67-882c-30405d9a36f0\", \"root_id\": \n\"79258153-0bdf-4d67-882c-30405d9a36f0\", \"parent\": null, \"parent_id\": \nnull, \"children\": []}\nNote the state parameter, which here shows the task is finished successfully, but it \nwill return PENDING if it's not done yet.\nThis can be used to poll the status of the task until it's completed or it shows an error, \nas we described earlier in the chapter.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "Event-Driven Structures\n[ 254 ]\nSummary\nIn this chapter, we have seen what event-driven structures are. We started with \na general discussion about how events can be used to create different flows than \nthe traditional request-response structure. We talked about how the events are \nintroduced into queues to be transmitted to other systems. We introduced the idea of \na publisher and a subscriber to introduce or extract events from that queue.\nWe described how this structure could be used to act on asynchronous tasks: tasks \nthat run in the background and allow other elements of the interface to respond \nquickly. We described how dividing asynchronous tasks into smaller ones can help \nincrease throughput by taking advantage of having multiple subscribers that can \nexecute these smaller tasks. We described how tasks can be added automatically at \ncertain times to allow the execution of predetermined tasks periodically.\nAs the introduction of tasks can happen with great variability, we discussed some \nimportant details of how queues work, the different problems that we can encounter, \nand strategies to deal with them. We talked about how a simple strategy for a \nbackground queue and a priority queue works in most scenarios and warned about \novercomplicating it. We also explained that, in the same spirit, it's better to keep \nthe code synchronized among all workers, even in cases when the queues may be \ndifferent. We also briefly touched on the capabilities of cloud computing as applied \nto asynchronous workers.\nWe explained how to use Celery, a popular task manager, to create asynchronous \ntasks. We covered setting up the different elements, including the back-end broker, \nhow to define a proper worker, and how to generate tasks from a different service. \nWe included a section on how to create scheduled tasks in Celery as well.\nWe presented Celery Flower, a complement for Celery that includes a web interface \nwith which we can monitor and control Celery. It also includes an HTTP API that \nallows us to create tasks by sending HTTP requests, allowing any programming \nlanguage to interact with our Celery system.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2403,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "[ 255 ]\n8\nAdvanced Event-Driven \nStructures\nAs we saw in the previous chapter, event-driven architectures are quite flexible \nand capable of creating complex scenarios. In this chapter, we will see what are the \npossible event-driven structures that cover more advanced use cases and how to \ndeal with their complexities.\nWe will see how some common applications like logs and metrics can be thought of \nas event-driven systems and use them to generate control systems that will feedback \ninto the system producing the events.\nWe will also discuss, with an example, how to create complex pipelines where \ndifferent events are being produced and the system is coordinated. We will also \nmove to a more general overview, introducing the bus as a concept to interconnect \nall the event-driven components.\nWe will introduce some general ideas on further complex systems to describe some \nof the challenges that these kinds of big event-driven systems can produce, such \nas the need to use CQRS techniques to retrieve information that crosses multiple \nmodules. Finally, we will give some notes on how to test the system, paying \nattention to the different levels of tests.\nIn this chapter, we'll cover the following topics:\n•\t\nStreaming events\n•\t\nPipelines\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1354,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 256 ]\n•\t\nDefining a bus\n•\t\nMore complex systems\n•\t\nTesting event-driven systems\nWe will start by describing streams of events.\nStreaming events\nFor some purposes, it can be good to just produce events that capture information \nand store it for later access. This structure is typical for instrumentation, for example, \nwhere we create an event every time there's an error. This event will contain \ninformation about things such as where the error was generated, debugging details \nto be able to understand it, and so on. The event is then sent, and the application \ncontinues recovering from the error.\nThe same can be done for specific parts of the code. For example, to capture an \naccess time to a database, the timing and related data (like the specific query) can be \ncaptured and sent in an event.\nAll those events should be compiled into a location to allow them to be queried and \naggregated.\nWhile usually not thought of as event-driven processes, this is pretty much how \nlogs and metrics work. In the case of logs, the events are generally text strings that \nget fired whenever the code decides to create them. The logs are forwarded to a \ndestination that allows us to search them later. \nThese kinds of events are simple but can be very powerful by allowing us to discover \nwhat the program is executing in a live system.\nThis instrumentation may also be used to enable controls or alerts when certain \nconditions are matched. A typical example of this is to alert us if the number of \nerrors captured by logs crosses a certain threshold.\nLogs can be stored in different formats. It's also common to create \nthem in JSON to allow better searching.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1797,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "Chapter 8\n[ 257 ]\nFigure 8.1: Monitoring events flow\nThis can also be used to produce feedback systems, where the instrumentation \nmonitoring the system can be used to determine whether to change something in \nthe system itself. For example, capturing metrics to determine whether the system \nneeds to scale up or scale down and change the number of servers available based \non the amount of requests or other parameters.\nFigure 8.2: Feedback of scaling events\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 258 ]\nThis is not the only way a system can be monitored, though. This method of \noperation can also be used as a way of detecting quotas, for example, short-circuiting \nthe processing of incoming requests if a certain quota has been exceeded. \nFigure 8.3: Monitor to detect quotas and stop extra requests \nThis structure is different from the upfront approach of setting a module that \ncontrols the system, relying instead on acting only when the threshold is breached, \nmaking the calculations in the background. This can reduce the amount of processing \nrequired upfront.\nFor example, for a quota of a maximum number of requests per minute, the process \nwill be something like the following pseudocode:\ndef process_request(request):\n    # Search for the owner of the request\n    owner = request.owner\n    info = retrieve_owner_info_from_db(owner)\n    if check_quota_info(info):\n        return process_request(request)\n    else:\n        return 'Quota exceeded'\ncheck_quota_info will be different in both cases. The upfront approach requires \nmaintaining and storing information about the previous requests:\ndef check_quota_info(info):\n    current_minute = get_current_minute()\n if current_minute != info.minute:\n     # New minute, start the quota\n     info.requests = 0\n     info.minute = current_minute\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "Chapter 8\n[ 259 ]\n else:\n     info.requests += 1\n # Update the information\n info.save()\n if info.requests > info.quota:\n     # Quota exceeded\n     return False\n # Quota still valid\n return False\nIf the validation is done in an external system, based on the events generated, check_\nquota_info doesn't need to store the information, rather just checking whether the \nquota has been exceeded:\ndef check_quota_info(info): \n    # Generate the proper event for a new event\n    generate_event('request', info.owner)\n if info.quota_exceeded:\n     return False\n # Quota still valid\n return False\nThe whole check is performed in the backend monitoring system, based on the \ngenerated events, and then stored in the info. This detaches the logic for whether to \napply the quota from the check itself, decreasing the latency. The counterpart is that \nthe detection of the quota having been exceeded may be delayed, allowing some \nrequests to be processed even if they shouldn't be according to the quota.\nIdeally, the generated events should already be in use to monitor \nthe requests received. This operation can be very useful as it reuses \nevents generated for other uses, reducing the need to collect extra \ndata.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 260 ]\nAt the same time, the check can be more complex and doesn't need to be done as \neach new request comes along. For example, for an hourly quota when multiple \nrequests are received every second, perhaps a check every minute is good enough to \nensure the quota is respected. This can save a big deal of processing power compared \nto checking the conditions every time a request is received.\nWe will talk in more detail specifically about logs and metrics in Chapter 12, Logging, \nand Chapter 13, Metrics.\nPipelines\nThe flow of events doesn't have to be contained in a single system. The receiving \nend of the system can produce its own events, directed to other systems. Events will \ncascade into multiple systems, generating a process.\nThis is a similar situation to the one presented previously, but in this case it's a more \ndeliberate process aimed at creating specific data pipelines where the flow between \nsystems is triggered and processed.\nA possible example of this is a system to rescale videos into different sizes and \nformats. When a video is uploaded into the system, it needs to be converted into \nmultiple versions to be used in different situations. A thumbnail should also be \ncreated to display the first frame of the video before playing it. \nWe will do this in three steps. First, a queue will receive the event to start the \nprocessing. This will trigger two events in two different queues to process the resize \nand the thumbnail generation independently. This will be our pipeline.\nTo store the input and output data, given that they are videos and images, we \nrequire external storage. We will use AWS S3, or more precisely, a mock for S3.\nThis, of course, is highly dependent on the specific scales, \ncharacteristics, and requests involved in different systems. \nFor some systems, upfront could be a better choice, as it's \neasier to implement and doesn't require a monitoring system. \nAlways validate whether the options fit into your system before \nimplementing.\nAWS S3 is an object storage service provided by Amazon in the \ncloud, very popular for being both easy to use and very stable. We \nwill use a mock of S3 that will allow us to start a local service that \nbehaves like S3, which will simplify our example.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "Chapter 8\n[ 261 ]\nHere is a high-level diagram of the system:\nFigure 8.4: Video and image queue\nTo get started, we need to upload the source video to the mock S3 and start the task. \nWe will also require some way of checking the results. For that, two scripts will be \navailable.\nLet's start with the setup configuration.\nPreparation\nAs outlined above, we have two key prerequisites: a queue backend and the mock S3 \nstorage.\nFor the queue backend, we will use Redis again. Redis is very easy to configure for \nmultiple queues, and we'll see how later. To start the Redis queue, we will again use \nDocker to download and run the official image:\n$ docker run -d -p 6379:6379 redis\nThis starts a Redis container exposed on the standard port 6379. Note the -d option \nwill keep the container running in the background.\nThe code is available on GitHub at https://github.com/\nPacktPublishing/Python-Architecture-Patterns/tree/\nmain/chapter_08_advanced_event_driven.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 262 ]\nFor the mock S3 service, we will use the same approach, starting a container that \nstarts S3 Mock, a system that replicates the S3 API, but stores the files locally. This \nlets us avoid setting up a real S3 instance, which involves getting an AWS account, \npaying for our usage, and so on.\nTo start S3 Mock, we will also use Docker:\n$ docker run -d -p 9090:9090 -t adobe/s3mock\nThe container exposes the endpoint on port 9090. We will direct the S3 requests \ntoward this local port. We will use the videos bucket for storing all the data.\nWe will define three different Celery workers that will perform three different \ntasks: the base task, image task and video task. Each one will be pulling events from \ndifferent queues.\nWe will also use some third-party libraries. This includes Celery, as we saw in the \nprevious chapter, but also other libraries, like boto3, click, and MoviePy. All the \nrequired libraries are available in the requirements.txt file so they can be installed \nwith the following command:\n$ pip3 install -r requirements.txt\nS3 Mock is a great option for development testing for S3 \nstorage without using a real connection to S3. We will see later \nhow to connect to the mock with a standard module. The full \ndocumentation can be found at https://github.com/adobe/\nS3Mock.\nThis distinction of specific tasks for different workers is done \ndeliberately for explanation purposes. In this example, there's \nprobably not a good reason to make this distinction, as all the tasks \ncan run in the same worker, and new events can be reintroduced \nin the same queue, and this is recommended, as we saw in the \nprevious chapter. Sometimes, though, there are other conditions \nthat may require a change of approach.\nFor example, some of the tasks may require specific hardware \nfor AI processing, use way more RAM or CPU power making it \nimpractical to make all workers equal, or other reasons that will \nnecessitate separating the workers. Still, be sure that there's a \ngood reason to make the split. It will complicate the operation and \nperformance of the system.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "Chapter 8\n[ 263 ]\nLet's start with the first stage of the process, the base task that will redirect to the \nother two.\nBase task\nThe main task will receive a path that contains the image. It will then create two \ntasks for the processing of the video resizing and the extraction of the thumbnail.\nHere's the code for base_tasks.py:\nfrom celery import Celery\napp = Celery(broker='redis://localhost/0')\nimages_app = Celery(broker='redis://localhost/1')\nvideos_app = Celery(broker='redis://localhost/2')\nlogger = app.log.get_default_logger()\n@app.task\ndef process_file(path):\n    logger.info('Stating task')\n    logger.info('The file is a video, needs to extract thumbnail and '\n                'create resized version')\n    videos_app.send_task('video_tasks.process_video', [path])\n    images_app.send_task('image_tasks.process_video', [path])\n    logger.info('End task')\nNote that we are creating three different queues here:\napp = Celery(broker='redis://localhost/0')\nimages_app = Celery(broker='redis://localhost/1')\nvideos_app = Celery(broker='redis://localhost/2')\nRedis allows us to create different databases easily by referring to them with an \ninteger. So, we create database 0 for the base queue, database 1 for the images queue, \nand database 2 for the videos queue.\nWe generate events in these queues with the .send_task function. Note that on each \nqueue we send the proper task. We include the path as a parameter.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 264 ]\nWhen the task is triggered, it will enqueue the next tasks. Let's take a look at the \nimage task.\nImage task\nTo generate a thumbnail of the video, we need the help of two third-party modules:\n•\t\nboto3. This common library helps us connect to AWS services. In particular, \nwe will use it to download and upload to our own mocked S3 service.\n•\t\nMoviePy. This is a library for working with video. We will extract the first \nframe as an independent file using this library.\nBoth libraries are included in the requirements.txt file described earlier in the \nchapter and included in the GitHub repo. Let's take a look at image_tasks.py:\nfrom celery import Celery\nimport boto3\nimport moviepy.editor as mp\nimport tempfile\nMOCK_S3 = 'http://localhost:9090/'\nBUCKET = 'videos'\nvideos_app = Celery(broker='redis://localhost/1')\nNote that all parameters for the tasks are defined in the second \nparameter of .send_task. This requires that the parameter is a list \nof arguments. In this case, we only have a single parameter that \nneeds still to be described as a list with [path].\nYou can check the whole boto3 documentation at \nhttps://boto3.amazonaws.com/v1/documentation/\napi/latest/index.html. It can be used to control all \nAWS APIs.\nThe full MoviePy documentation is available at https://\nzulko.github.io/moviepy/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1451,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "Chapter 8\n[ 265 ]\nlogger = videos_app.log.get_default_logger()\n@videos_app.task\ndef process_video(path):\n    logger.info(f'Stating process video {path} for image thumbnail')\n    client = boto3.client('s3', endpoint_url=MOCK_S3)\n    # Download the file to a temp file\n    with tempfile.NamedTemporaryFile(suffix='.mp4') as tmp_file:\n        client.download_fileobj(BUCKET, path, tmp_file)\n        # Extract first frame with moviepy\n        video = mp.VideoFileClip(tmp_file.name)\n        with tempfile.NamedTemporaryFile(suffix='.png') as output_file:\n            video.save_frame(output_file.name)\n            client.upload_fileobj(output_file, BUCKET, path + '.png')\n    logger.info('Finish image thumbnails')\nNote that we define the Celery application with the correct database. We then \ndescribe the task. Let's divide it into different steps. We first download the source file \ndefined in path into a temporary file:\nclient = boto3.client('s3', endpoint_url=MOCK_S3)\n# Download the file to a temp file\nwith tempfile.NamedTemporaryFile(suffix='.mp4') as tmp_file:\n    client.download_fileobj(BUCKET, path, tmp_file)\nNote that we define the endpoint to connect with MOCK_S3, which is our S3 Mock \ncontainer, exposed on http://localhost:9090/ as we described before.\nRight after it we generate a temporary file to store the downloaded video. We define \nthat the suffix of the temporary file to be .mp4 so later VideoPy can detect properly \nthat the temporary file is a video.\nNote the next steps are all inside the with block defining the \ntemporary file. If it was defined outside of this block, the file would \nbe closed and not available.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1744,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 266 ]\nThe next step is to load the file in MoviePy and then extract the first frame into \nanother temporary file. This second temporary file has a suffix of .png to label it as \nan image:\nvideo = mp.VideoFileClip(tmp_file.name)\nwith tempfile.NamedTemporaryFile(suffix='.png') as output_file:\n    video.save_frame(output_file.name)\nFinally, the file is uploaded to S3 Mock, adding .png to the end of the original name:\nclient.upload_fileobj(output_file, BUCKET, path + '.png')\nOnce again, pay attention to the indentation to be sure that the temporary files are \navailable at the different stages.\nThe task to resize the video follows a similar pattern. Let's take a look.\nVideo task\nThe video Celery worker pulls from the video queue and performs similar steps to \nthe image task:\nfrom celery import Celery\nimport boto3\nimport moviepy.editor as mp\nimport tempfile\nMOCK_S3 = 'http://localhost:9090/'\nBUCKET = 'videos'\nSIZE = 720\nvideos_app = Celery(broker='redis://localhost/2')\nlogger = videos_app.log.get_default_logger()\n@videos_app.task\ndef process_video(path):\n    logger.info(f'Starting process video {path} for image resize')\n    client = boto3.client('s3', endpoint_url=MOCK_S3)\n    # Download the file to a temp file\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "Chapter 8\n[ 267 ]\n    with tempfile.NamedTemporaryFile(suffix='.mp4') as tmp_file:\n        client.download_fileobj(BUCKET, path, tmp_file)\n        # Resize with moviepy\n        video = mp.VideoFileClip(tmp_file.name)\n        video_resized = video.resize(height=SIZE)\n        with tempfile.NamedTemporaryFile(suffix='.mp4') as output_file:\n            video_resized.write_videofile(output_file.name)\n            client.upload_fileobj(output_file, BUCKET, path + \nf'x{SIZE}.mp4')\n    logger.info('Finish video resize')\nThe only difference from the image task is the resizing of the video to a height of 720 \npixels and uploading the result:\n# Resize with moviepy\nvideo = mp.VideoFileClip(tmp_file.name)\nvideo_resized = video.resize(height=SIZE)\nwith tempfile.NamedTemporaryFile(suffix='.mp4') as output_file:\n     video_resized.write_videofile(output_file.name)\nBut the general flow is very similar. Note that it's pulling from a different Redis \ndatabase, corresponding to the video queue.\nConnecting the tasks\nTo test the system, we need to start all the different elements. Each one is started in a \ndifferent terminal so we can see their different logs:\n    $ celery -A base_tasks worker --loglevel=INFO\n    $ celery -A video_tasks worker --loglevel=INFO\n    $ celery -A image_tasks worker --loglevel=INFO\nTo start the process, we need a video to be processed in the system. \nOne possibility to find good, free, videos is to use https://www.\npexels.com/, which has free stock content. For our example \nrun, we will download the 4K video with URL https://www.\npexels.com/video/waves-rushing-and-splashing-to-the-\nshore-1409899/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1731,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 268 ]\nWe will use the following script to upload the video to the S3 Mock storage and start \nthe task:\nimport click\nimport boto3\nfrom celery import Celery\ncelery_app = Celery(broker='redis://localhost/0')\n   MOCK_S3 = 'http://localhost:9090/'\nBUCKET = 'videos'\nSOURCE_VIDEO_PATH = '/source_video.mp4'\n@click.command()\n@click.argument('video_to_upload')\ndef main(video_to_upload):\n# Note the credentials are required by boto3, but we are using\n# a mock S3 that doesn't require them, so they can be fake\n    client = boto3.client('s3', endpoint_url=MOCK_S3,\n                          aws_access_key_id='FAKE_ACCESS_ID',\n                          aws_secret_access_key='FAKE_ACCESS_KEY')\n    # Create bucket if not set\n    client.create_bucket(Bucket=BUCKET)\n    # Upload the file\n    client.upload_file(video_to_upload, BUCKET, SOURCE_VIDEO_PATH)\n    # Trigger the\n    celery_app.send_task('base_tasks.process_file', [SOURCE_VIDEO_\nPATH])\nif __name__ == '__main__':\n    main()\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1111,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "Chapter 8\n[ 269 ]\nThe start of the script describes the Celery queue, the base queue, that will be the \nstart of the pipeline. We define several values related to the configuration, as we saw \nin the previous tasks. The only addition is SOURCE_VIDEO_PATH, which will host the \nvideo in S3 Mock.\nWe use the click library to generate an easy command-line interface (CLI). The \nfollowing lines generate a simple interface that requests the name of the video to \nupload as the parameter of the function.\n@click.command()\n@click.argument('video_to_upload')\ndef main(video_to_upload):\n           ….\nclick is a fantastic option to generate CLIs quickly. You can read more about it in its \ndocumentation here: https://click.palletsprojects.com/.\nThe content of the main function simply connects to our S3 Mock, creates the bucket \nif not set yet, uploads the file to SOURCE_VIDEO_PATH, and then sends the task to the \nqueue to start the process:\n    client = boto3.client('s3', endpoint_url=MOCK_S3)\n    # Create bucket if not set\n    client.create_bucket(Bucket=BUCKET)\n    # Upload the file\n    client.upload_file(video_to_upload, BUCKET, SOURCE_VIDEO_PATH)\n    # Trigger the\n    celery_app.send_task('base_tasks.process_file', [SOURCE_VIDEO_\nPATH])\nLet's run it and see the results.\nIn this script we use the same name to upload all files, overwriting \nit if the script is run again. Feel free to change this if it makes more \nsense to you to do it differently.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1558,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 270 ]\nRunning the task\nThe script can be run after adding the name of the video to upload. Remember that \nall the libraries in requirements.txt need to be installed:\n$ python3 upload_video_and_start.py source_video.mp4\nIt will take a bit of time to upload the file to S3 Mock. Once called, the first worker to \nreact is the base one. This worker will create two new tasks:\n[2021-07-08 20:37:57,219: INFO/MainProcess] Received task: base_tasks.\nprocess_file[8410980a-d443-4408-8f17-48e89f935325]\n[2021-07-08 20:37:57,309: INFO/ForkPoolWorker-2] Stating task\n[2021-07-08 20:37:57,660: INFO/ForkPoolWorker-2] The file is a video, \nneeds to extract thumbnail and create resized version\n[2021-07-08 20:37:58,163: INFO/ForkPoolWorker-2] End task\n[2021-07-08 20:37:58,163: INFO/ForkPoolWorker-2] Task base_tasks.\nprocess_file[8410980a-d443-4408-8f17-48e89f935325] succeeded in \n0.8547832089971052s: None\nThe other two will start soon after. The image worker will display new logs, starting \nthe image thumbnail creation:\n[2021-07-08 20:37:58,251: INFO/MainProcess] Received task: image_tasks.\nprocess_video[5960846f-f385-45ba-9f78-c8c5b6c37987]\n[2021-07-08 20:37:58,532: INFO/ForkPoolWorker-2] Stating process video \n/source_video.mp4 for image thumbnail\n[2021-07-08 20:38:41,055: INFO/ForkPoolWorker-2] Finish image \nthumbnails\n[2021-07-08 20:38:41,182: INFO/ForkPoolWorker-2] Task image_tasks.\nprocess_video[5960846f-f385-45ba-9f78-c8c5b6c37987] succeeded in \n42.650344008012326s: None\nThe video worker will take longer as it needs to resize the video:\n[2021-07-08 20:37:57,813: INFO/MainProcess] Received task: video_tasks.\nprocess_video[34085562-08d6-4b50-ac2c-73e991dbb58a]\n[2021-07-08 20:37:57,982: INFO/ForkPoolWorker-2] Starting process video \n/source_video.mp4 for image resize\n[2021-07-08 20:38:15,384: WARNING/ForkPoolWorker-2] Moviepy - Building \nvideo /var/folders/yx/k970yrd11hb4lmrq4rg5brq80000gn/T/tmp0deg6k8e.mp4.\n[2021-07-08 20:38:15,385: WARNING/ForkPoolWorker-2] Moviepy - Writing \nvideo /var/folders/yx/k970yrd11hb4lmrq4rg5brq80000gn/T/tmp0deg6k8e.mp4\n[2021-07-08 20:38:15,429: WARNING/ForkPoolWorker-2] t:   0%|          | \n0/528 [00:00<?, ?it/s, now=None]\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2308,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "Chapter 8\n[ 271 ]\n[2021-07-08 20:38:16,816: WARNING/ForkPoolWorker-2] t:   0%|          | \n2/528 [00:01<06:04,  1.44it/s, now=None]\n[2021-07-08 20:38:17,021: WARNING/ForkPoolWorker-2] t:   1%|          | \n3/528 [00:01<04:17,  2.04it/s, now=None]\n...\n[2021-07-08 20:39:49,400: WARNING/ForkPoolWorker-2] t:  99%|#########9| \n524/528 [01:33<00:00,  6.29it/s, now=None]\n[2021-07-08 20:39:49,570: WARNING/ForkPoolWorker-2] t:  99%|#########9| \n525/528 [01:34<00:00,  6.16it/s, now=None]\n[2021-07-08 20:39:49,874: WARNING/ForkPoolWorker-2] t: 100%|#########9| \n527/528 [01:34<00:00,  6.36it/s, now=None]\n[2021-07-08 20:39:50,027: WARNING/ForkPoolWorker-2] t: 100%|##########| \n528/528 [01:34<00:00,  6.42it/s, now=None]\n[2021-07-08 20:39:50,723: WARNING/ForkPoolWorker-2] Moviepy - Done !\n[2021-07-08 20:39:50,723: WARNING/ForkPoolWorker-2] Moviepy - video \nready /var/folders/yx/k970yrd11hb4lmrq4rg5brq80000gn/T/tmp0deg6k8e.mp4\n[2021-07-08 20:39:51,170: INFO/ForkPoolWorker-2] Finish video resize\n[2021-07-08 20:39:51,171: INFO/ForkPoolWorker-2] Task video_tasks.\nprocess_video[34085562-08d6-4b50-ac2c-73e991dbb58a] succeeded in \n113.18933968200872s: None\nTo retrieve the results, we will use the check_results.py script, which downloads \nthe contents of the S3 Mock storage:\nimport boto3\nMOCK_S3 = 'http://localhost:9090/'\nBUCKET = 'videos'\nclient = boto3.client('s3', endpoint_url=MOCK_S3)\nfor path in client.list_objects(Bucket=BUCKET)['Contents']:\n    print(f'file {path[\"Key\"]:25} size {path[\"Size\"]}')\n    filename = path['Key'][1:]\n    client.download_file(BUCKET, path['Key'], filename)\nBy running it, we download the files into the local directory:\n$ python3 check_results.py\nfile /source_video.mp4         size 56807332\nfile /source_video.mp4.png     size 6939007\nfile /source_video.mp4x720.mp4 size 8525077\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 272 ]\nYou can check the resulting files and confirm that they have been generated \ncorrectly. Note that source_video.mp4 will be the same as your input video.\nThis example demonstrates how to set up a relatively complex pipeline where \ndifferent queues and workers are triggered in a coordinated fashion. Note that while \nwe directly used Celery to send the tasks to the queues, we could also have used \nCelery Flower and an HTTP request to do this.\nDefining a bus\nWhile we talked about the queue backend system, this hasn't been truly expanded to \nthe concept of a bus. The term bus originates from the hardware buses that transmit \ndata between different components of a hardware system. This makes them a central, \nmultisource, and multidestination part of the system.\nA software bus is a generalization of this concept that allows us to interconnect \nseveral logical components.\nAs the bus is in charge of data transmission, that means that the sender doesn't need \nto know much other than the message to transmit and the queue to send it to. The \nbus itself will transmit to the destination or destinations.\nThe concept of a bus is closely related to that of the message broker. A message broker, \nthough, typically includes more capacities than a pure bus, such as being able to \ntransform messages along the way and use multiple protocols. Message brokers \ncan be very complex and allow a huge amount of customization and decoupling of \nservices. In general, most of the tools to support the usage of a bus will be labeled as \nmessage brokers, though some are more powerful than others.\nIn essence, a bus is a component specialized in the transmission \nof data. This is an ordered communication compared to the \nusual alternative of connecting directly to the services through a \nnetwork, without any intermediate component.\nThough we will use the term \"bus\", some of the capacities will be \nmore closely related to features such as routing messages, which \nshould require tools considered message brokers. Analyze the \nrequirements of your specific use cases and use a tool that can fulfil \nthem.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "Chapter 8\n[ 273 ]\nThe bus will be then defined as a central point where all the event-related \ncommunication will be directed to. This simplifies the configuration, as the events \ncan be routed to the proper destination without requiring a different endpoint.\nFigure 8.5: Message bussing\nInternally, though, the bus will contain different logical divisions that allow the \nproper routing of messages. These are the queues.\nIn our example before, we used Redis as a bus. Though the connection URL is a little \ndifferent, it can be refactored to make it a bit clearer:\n# Remember that database 0 is the base queue\nBASE_BROKER = 'redis://localhost/0'\nBase_app = Celery(broker=BROKER)\n# Refactor for base\nBROKER_ROOT = 'redis://localhost'\nThe routing can be complicated, if the bus allows for it, which is \nthe case here.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 274 ]\nBROKER_BASE_QUEUE = 0\nbase_app = Celery(broker=f'{BASE_BROKER}/{BROKER_BASE_QUEUE}') \n# To address the image queue\nBROKER_ROOT = 'redis://localhost'\nBROKER_IMAGE_QUEUE = 1\nimage_app = Celery(broker=f'{BASE_BROKER}/{BROKER_IMAGE_QUEUE}') \nThis central location makes the configuration of all the different services easy, both \nfor pushing events to the queues and pulling from them.\nMore complex systems\nMore complex systems can be created where the events pass through multiple stages \nand are even designed for easy plugin systems working from the same queue.\nThis can create complicated setups where the data flows through complex pipelines \nand is processed by independent modules. These kinds of scenarios are typically \nseen on instrumentation that aims to analyze and process big quantities of data to try \nand detect patterns and behaviors.\nImagine, for example, a system that makes bookings for a travel agency. There \nare a lot of searches and bookings requests that happen in the system, with \nassociated purchases such as car rentals, luggage bags, food, and so on. Each of \nthe actions produces a regular response (search, book, purchase, and so on), but an \nevent describing the action will be introduced into a queue to be processed in the \nbackground. Different modules will analyze user behavior with different objectives \nin mind.\nFor example, the following modules could be added to this system:\n•\t\nAggregate economic results by time, to obtain a global view of how the \nservice is working over time. This can involve details such as purchases per \nday, revenue, margins, and so on.\n•\t\nAnalyze the behavior of regular users. Follow users to discover their \npatterns. What are they searching for before booking? Are they using offers? \nHow often are they booking flights? How long is their average trip? Any \noutliers?\n•\t\nBe sure that there's enough inventory for purchases. Backorder any required \nelements, based on the items being purchased in the system. This includes \nalso scheduling enough food for flights, based on pre-purchases.\n•\t\nCollect information about preferred destinations, based on searches. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2271,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "Chapter 8\n[ 275 ]\n•\t\nTrigger alerts for things like full flights that could lead to scheduling more \nplanes for those days.\nThese modules are fundamentally about different things and present a different view \non the system. Some are more oriented toward the behavior of users and marketing, \nwhile others are more related to logistics. Depending on the size of the system, it \ncould be determined that the modules require a different, dedicated team to take \ncare of each of them independently.\nFigure 8.6: Bus from front end system to different modules\nNote that each system will likely have its own storage to allow it to store the \ninformation. This could also lead to the creation of their own APIs to access this \ninformation once collected.\nThe same event will be sent to the bus, and then the different services will receive it. \nTo be able to do so, you'll need to get a bus that accepts subscriptions from several \nsystems and delivers the same message to all subscribed systems.\nTo query the information, the system needs to query the \ndatabases of the modules where the data is stored. This can be \nan independent service, but it will likely be the same system's \nfront end, as it will typically contain all the external interface and \npermissions handling.\nThis makes it necessary for the front end system to access the \nstored information, either by directly accessing the database or by \nusing some API to access it. The front end system should model \naccess to the data, as we saw in Chapter 3, Data Modeling, and will \nvery likely require a model definition that abstracts the complex \naccess to the data.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 276 ]\nNote that the workers in this case can create more events to be introduced. For \nexample, any module will be able to create an alert, to which the alert system will be \nnotified. For example, if the inventory is too low, it may require a quick alert at the \nsame time it backorders, to be sure that action is taken quickly.\nFigure 8.7: Note that communication between the modules and the alerts also is done through the bus\nComplex event-driven systems can help you distribute the work between different \ncomponents. In this example, you can see how the immediate response (booking a \nflight) is completely independent of the further detailed analysis in the background \nthat can be used for longer-term planning. If all the components were added \nwhile the request was served, it could interfere with performance. The backend \ncomponents can be swapped and upgraded while the front end system is unaffected.\nTo properly implement this kind of system, the event needs to use a standard format \nthat's easy to adapt and extend, to ensure that any module that receives it can \nquickly scan through it and discard it if it's not necessary. \nA good idea is to use a simple JSON structure like the following:\n{\n  \"type\": string defining the event type,\n  \"data\": subevent content\n}\nThis pattern is called publish/subscribe or pub/sub. The consumers \nof the events need to subscribe to the topic, which is, in pub/\nsub parlance, is the equivalent of a queue. Most buses accept this \nsystem, though it may require some work to configure.\nFor example, there's a library to allow Celery to work under this \nsystem available at https://github.com/Mulugruntz/celery-\npubsub.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "Chapter 8\n[ 277 ]\nFor example, when a search is produced, an event like this will be created:\n{\n  \"type\": \"SEARCH\",\n  \"data\": {\n    \"from\": \"Dublin\",\n    \"to\": \"New York\",\n    \"depart_date\": 2021-12-31,\n    \"return_date\": null,\n    \"user\": null\n  }\n}\nThe type field makes easy to discard the event if it's not of interest to any module. \nFor example, the economic analysis module will discard any SEARCH event. Other \nmodules may require further processing. For example, the user behavior module \nwill analyze SEARCH events where the user field in the data is set.\nKeep in mind that an important element for event-driven systems is that the storage \nmay not be common to all. Perhaps each independent module has its own database. \nYou'll need to use the techniques for CQRS that we discussed in Chapter 3, Data \nModeling, to model data in these modules. In essence, you'll need to ask differently \nto read and to save new data, as writing new data requires the generation of events; \nand you'll need to model them as a business unit. What's more, the model may need \nto merge information from multiple modules in some cases. For example, if there's a \nquery in the system that requires obtaining some economic information for a user, it \nneeds to query both the user behavior module and the economic analysis module, \nwhile presenting the information as a unique model of EconomicInfoUser.\nThe flexible data structure will allow for new events to be generated, adding more \ninformation and allowing for controlled changes across the modules by enforcing the \nbackward compatibility of changes. Then the different teams can work in parallel, \nimproving the system without stepping on each other's toes too much.\nWhen information is frequently accessed, it may make sense \nto duplicate it in several places. This goes against the single \nresponsibility principle (that every feature should be the sole \nresponsibility of a single module), but the alternative is to create \ncomplicated methods of access to get information that's commonly \nused. Be careful when designing and dividing the system to avoid \nthese problems.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2220,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 278 ]\nBut ensuring that they behave correctly can be complicated, as there are multiple \nparts that interact with each other.\nTesting event-driven systems\nEvent-driven systems are very flexible and, in certain situations, can be incredibly \nuseful in detaching different elements. But this flexibility and detachment can make \nthem difficult to test to ensure that everything works as expected.\nIn general, unit tests are the fastest tests to generate, but the detached nature of \nevent-driven systems makes them not very useful to properly test the reception of \nevents. Sure, the events can be simulated, and the general behavior of receiving an \nevent can be tested. But the problem is: how can we ensure that the event has been \nproperly generated? And at the right moment?\nThe only option is to use integration tests to check the behavior of the system. But \nthese tests are more expensive to design and run.\nFor example, in our previous example, to test that a purchase of food correctly \ntriggers an alert, we need to:\n1.\t Generate a call to purchase a food item.\n2.\t Produce the appropriate event.\n3.\t Handle the event in the inventory control. The current inventory should be \nconfigured as low, which will produce an alert event.\n4.\t Handle the alert event properly.\nThere's always an endless debate about naming tests, what \nexactly a unit test is compared to an integration test, system \ntest, acceptance test, and so on. To avoid getting into too deep a \ndiscussion here, at it's not the objective of the book, we will use \nthe term unit test to describe tests that can only be run in a single \nmodule, and integration test to refer to those that require two or \nmore modules interacting with each other to be successful. Unit \ntests will mock any dependence, but integration tests will actually \ncall the dependence to be sure that the connection between \nmodules works correctly.\nThese two levels are significantly different in terms of the cost for \neach test written. Way more unit tests can be written and run than \nintegration tests in the same period of time.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2216,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "Chapter 8\n[ 279 ]\nAll these steps require configuration to be done in three different systems (the front-\nend system, the inventory control module, and the alert module), along with setting \nup the bus to connect them. Ideally, this test will require the system to be able to start \nup with an automation system to automate the tests. That requires every module \ninvolved to be automatable.\nAs we can see, this is a high bar in setting up and running tests, though it is still \nworth doing. To achieve a sane balance between integration and unit tests, we \nshould grow them and apply some strategy to be sure that we have reasonable \ncoverage for both.\nUnit tests are cheap, so every case should have healthy coverage by unit tests, where \nthe external modules are mocked. This includes cases such as different input formats, \ndifferent configurations, all flows, errors, and so on. Good unit tests should cover \nmost possibilities from an isolation point of view, mocking the input of data and any \nsent event.\nFor example, continuing the inventory control example, many unit tests can control \nthe following requisites, all by changing the input request:\n•\t\nPurchase of an element with high inventory.\n•\t\nPurchase of an element with low inventory. This should produce an alert \nevent.\n•\t\nPurchase of a non-existing element. This should generate an error.\n•\t\nEvent with invalid format. This should generate an error.\n•\t\nPurchase of an element with zero inventory. This should generate an alert \nevent.\n•\t\nMore cases, such as different kinds of purchases, formats, and so on.\nIntegration tests, on the other hand, should have only a few tests, mostly covering \nthe \"happy path\". The happy path means that a regular representative event is \nbeing sent and processed, but doesn't produce expected errors. The objective of \nan integration test is to confirm that all the parts are connecting and working as \nexpected. Given that integration tests are more expensive to run and operate, aim to \nimplement only the most important, and keep an eye out for any test that isn't worth \nmaintaining and can be pruned.\nWe described, in the above discussion on integration tests, a happy \npath scenario. The event triggers a handle in the inventory and \ngenerates an alert that's also handled. For integration tests, this \nis preferred over not generating an alert, as it stresses the system \nmore.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "Advanced Event-Driven Structures\n[ 280 ]\nThough it depends on the system, the ratio of unit to integration test should be \nheavily weighted toward unit tests, sometimes by 20 times or more (meaning 1 \nintegration test for 20 unit tests).\nSummary\nIn this chapter, we have seen more event-driven systems with a variety of advanced \nand complex architectures that can be designed. We have presented some of the \nflexibility and power that event-driven design can bring to a design, but also the \nchallenges attached to event-driven design.\nWe started by presenting common systems such as logs and metrics as event-driven \nsystems, as they are, and considered how looking at them in this way allows us to \ncreate alerting and feedback systems that can be used to control the source of the \nevents.\nWe also presented an example with Celery of a more complex pipeline, including the \nusage of multiple queues and shared storage to generate multiple coordinated tasks, \nsuch as resizing a video and extracting a thumbnail.\nWe presented the idea of a bus, a shared access point for all events in the system, and \nlooked at how we can generate more complex systems where events are delivered \nto multiple systems and cascade into complex actions. We also discussed the \nchallenges of solving these complex interactions, both in terms of requiring the use of \nCQRS techniques to model information that can be read after the write is generated \nthrough events, and the demands in terms of testing at different levels with unit and \nintegration tests.\nIn the next chapter, we will see the two main architectures for complex systems: \nmonolithic and microservices.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "[ 281 ]\n9\nMicroservices vs Monolith\nIn this chapter, we will present and comment on two of the most common \narchitectures for complex systems. Monolithic architecture creates a single block \nwhere the whole system is contained, and is simple to operate. Microservices \narchitecture, on the other hand, divides the system into smaller microservices that \ntalk to each other, aiming to allow different teams to take ownership of different \nelements, and helping big teams to work in parallel.\nWe will discuss when to choose each one, based on its different characteristics. \nWe will also go through the teamwork aspect of them, as they have different \nrequirements in terms of how the work needs to be structured.\nA common pattern is to migrate from an old monolithic architecture to a \nmicroservices one. We will talk about the stages involved in such a change.\nWe will also introduce Docker as a way of containerizing services, something \nvery useful when it comes to creating microservices, but that can also be applied \nto monoliths. We will containerize the web application presented in Chapter 5, The \nTwelve-Factor App Methodology.\nRemember that the architecture is not only related to tech, but to \na significant degree to how communication is structured! Refer \nto Chapter 1, Introduction to Software Architecture, for a further \ndiscussion of Conway's Law.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 282 ]\nFinally, we will briefly describe how to deploy and operate multiple containers using \nan orchestration tool, and describe the most popular one these days – Kubernetes.\nIn this chapter, we'll cover the following topics:\n•\t\nMonolithic architecture\n•\t\nThe microservices architecture\n•\t\nWhich architecture to choose\n•\t\nMoving from a monolith to microservices\n•\t\nContainerizing services\n•\t\nOrchestration and Kubernetes\nLet's start by talking in more depth about monolithic architecture.\nMonolithic architecture\nWhen a system is designed organically, the tendency is to generate a single unitary \nblock of software that contains the whole functionality of the system. \nThis is a logical progression. When a software system is designed, it starts small, \ntypically with a simple functionality. But, as the software is used, it grows in terms of \nits usage and starts getting requests for new functionality to complement the existing \nones. Unless there are sufficient resources and planning to structure the growth, \nthe path of least resistance will be to keep adding everything into the same code \nstructure, with little modularity.\nFigure 9.1: A monolithic application\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1301,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "Chapter 9\n[ 283 ]\nThis process ensures that all the code and functionality are tied together in a single \nblock, hence the name monolithic architecture.\nAlthough this kind of structure is quite common, in general, monolithic structures \nhave a better modularity and internal structure. Even if the software is composed \nof a single block, it can be divided logically into different parts, assigning different \nresponsibilities to different modules.\nThe defining characteristic of a monolith is that all the calls between modules are \nthrough internal APIs, within the same process. This affords the advantage of being \nvery flexible. The strategy for deploying a new version of the monolith is also easy. \nRestarting the process will ensure full deployment.\nThe version of the monolith is easy to know, as all the code is part of the same \nstructure. The code, if it's under source control, will all be under the same repo.\nThe microservices architecture\nThe microservices architecture was developed as an alternative to having a single \nblock containing all the code.\nAnd, by extension, software that follows this pattern is called a \nmonolith.\nFor example, in previous chapters we discussed the MVC \narchitecture. This is a monolithic architecture. The Models, Views, \nand Controllers are all under the same process, but there is a \ndefinitive structure in place that differentiates the responsibilities \nand functions.\nMonolithic architecture is not synonymous with a lack of structure.\nKeep in mind that a monolithic application can have multiple \ncopies running. For example, a monolithic web application can \nhave multiple copies of the same software running in parallel, with \na load balancer sending requests to all of them. A restart, in this \ncase, will be in multiple stages.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1888,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 284 ]\nA system following a microservices architecture is a collection of loosely coupled \nspecialized services that work in unison to provide a comprehensive service. Let's divide the \ndefinition up in order to be clearer:\n1.\t A collection of specialized services, meaning that there are different and \nwell-defined modules\n2.\t Loosely coupled, so each microservice can be independently deployed and \ndeveloped\n3.\t That work in unison. Each microservice needs to communicate with others\n4.\t To provide a comprehensive service, meaning that the whole system creates \na full system that has a clear motive and functionality\nCompared with a monolith, instead of grouping the whole software under the \nsame process, it uses multiple, separate functional parts (each microservice) that \ncommunicate through well-defined APIs. These elements can be in different \nprocesses and typically are moved out from different servers to allow proper scaling \nof the system.\nFigure 9.2: Note that not all microservices will be connected to the storage.  \nEach microservice may have its own individual storage\nThe defining characteristic is that the calls between different services are all through \nexternal APIs. These APIs act as a clear, defined barrier between functionalities. \nBecause of this, microservices architecture requires advanced planning and needs to \ndefine clearly the differences between components.\nIn particular, microservices architecture requires a good upfront \ndesign to be sure that the different elements connect together \ncorrectly, as any problem that is cross-service will be costly to work \nwith.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "Chapter 9\n[ 285 ]\nA system that follows the microservices architecture doesn't happen organically, but \nit's the result of a plan created beforehand and executed carefully. This architecture is \nnot typically started for systems from scratch, but instead, they are migrated from a \npreviously existing, successful, monolithic architecture.\nWhich architecture to choose\nThere's a tendency to think that a more evolved architecture, like the microservices \narchitecture, is better, but that's an oversimplification. Each one has its own set of \nstrengths and weaknesses.\nThe first one is the fact that almost every small application will start as a monolithic \napplication. This is because it is the most natural way to start a system. Everything is \nat hand, the number of modules is reduced, and it's an easy starting point.\nMicroservices, on the other hand, require the creation of a plan to divide the \nfunctionality carefully into different modules. This task may be complicated, as some \ndesigns may prove inadequate later on.\nThis requires quite a lot of work to be done beforehand, which requires an \ninvestment in the microservices architecture.\nThat said, as monoliths grow, they can start presenting problems just through the \nsheer size of the code. The main characteristic of a monolithic architecture is that all \nthe code is found together, and it can start presenting a lot of connections that can \ncause developers to be confused. Complexity can be reduced by good practices and \nconstant vigilance to ensure good internal structure, but that requires a lot of work \nin place by existing developers to enforce it. When dealing with a big and complex \nsystem, it may be easier to present clear and strict boundaries just by dividing \ndifferent areas into different processes.\nKeep in mind that no design can be totally future-proof. Any \nperfectly valid architectural decision may prove incorrect a year or \ntwo later when changes in the system require adjustments. While it \nis a good question to think about the future, trying to cover every \npossibility is futile. The proper balance between designing for the \ncurrent feature and designing for the future vision of the system is \na constant challenge in software architecture.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 286 ]\nThe modules can also require different specific knowledge, making it natural \nto assign different team members to different areas. To create a proper sense of \nownership of the modules, they can have different opinions in terms of code \nstandards, an adequate programming language for the job, ways of performing tasks, \nand so on; for example, a photosystem that has an interface for uploading photos \nand an AI system for categorizing them. While the first module will work as a web \nservice, the abilities required for training and handling an AI model to categorize the \ndata will be very different, making the module separation natural and productive. \nBoth of them in the same code base may generate problems by trying to work at the \nsame time.\nAnother problem of monolithic applications is the inefficient utilization of resources, \nas each deployment of the monolith carries over every copy of every module. For \nexample, the RAM required will be determined for the worst-case scenario across \nmultiple modules. When there are multiple copies of the monolith, that will waste \na lot of RAM preparing for worst-case scenarios that will likely be rare. Another \nexample is the fact that, if any module requires a connection to the database, a new \nconnection will be created, whether that's used or not.\nIn comparison, using microservices can adjust each service according to its own \nworst-case use case, and independently control the number of replicas for each. \nWhen viewed as a whole, that can lead to big resource saves in big deployments.\nFigure 9.3: Notice that using different microservices allows us to reduce RAM usage by dividing requests into \ndifferent microservices, while in a monolithic application, the worst-case scenario drives RAM utilization\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1906,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "Chapter 9\n[ 287 ]\nDeployments also work very differently between monoliths and microservices. As \nthe monolithic application needs to be deployed in a single go, every deployment is, \neffectively, a task for the whole team. If the team is small, creating a new deployment \nand ensuring that the new features are properly coordinated between modules and \nnot interfering incorrectly is not very complicated. However, as the teams grow \nbigger, this can present a serious challenge if the code is not strictly structured. In \nparticular, a bug in a small part of the system may bring down the whole system \ncompletely, as any critical error in the monolith affects the whole of the code.\nMonolith deployments require coordination between modules, meaning that \nthey need to work with each other, which normally leads to teams working \nclosely together until the feature is ready to be released, and require some sort of \nsupervision until the deployment is ready. This is noticeable when several teams are \nworking on the same code base, with competing goals, and this blurs the ownership \nand responsibility of deployments.\nBy comparison, different microservices are deployed independently. The API \nshould be stable and backward compatible with older releases, and that's one of the \nstrong requisites that need to be enforced. However, the boundaries are very clear, \nand in the event of a critical bug, the worst that can happen is that the particular \nmicroservice goes down, while other unrelated microservices continue unaffected.\nThis makes the system work in a \"degraded state,\" as compared to the \"all-or-none\" \napproach of the monolith. It limits the scope of a catastrophic failure.\nOf course, in both cases, solid testing techniques can be used to increase the quality \nof the software released.\nIn comparison with the monolith, microservices can be deployed independently, \nwithout coordinating closely with other services. This brings independence to the \nteams working on them and allows for faster, continuous deployments that require \nless central coordination.\nOf course, certain microservices may be more critical than others, \nmaking them worthy of extra attention and care regarding \ntheir stability. But, in that case, they can be defined as critical in \nadvance, with stricter stability rules enforced.\nThe keyword here is less coordination. Coordination is still \nrequired, but the objective of a microservices architecture is \nnecessarily that each microservice can be independently deployed \nand owned by a team, so the majority of changes can be dictated \nexclusively by the owner without requiring a process of warning \nother teams.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2761,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 288 ]\nMonolithic applications, because they communicate with other modules through \ninternal operations, mean that they typically can perform these operations much \nfaster than through the external APIs. This allows a very high level of interaction \nbetween modules without paying a significant performance price.\nThere is an overhead related to the usage of external APIs and communication \nthrough a network that can produce a noticeable delay, especially if there are too \nmany internal requests made to different microservices. Careful consideration is \nrequired to try to avoid repeating external calls and to limit the number of services \nthat can be contacted in a single task.\nAnother interesting advantage of microservices is the independence of technical \nrequirements. In a monolithic application, problems may arise as a result of requiring \ndifferent versions of libraries for different modules. For example, updating the \nversion of Python requires the whole code base to be prepared for that. These library \nupdates can be complicated as different modules may have different requirements, \nand one module can effectively mingle with another by requiring an upgrade of the \nversion of a certain library that's used by both.\nMicroservices, on the other hand, contain their own set of technical requirements, so \nthere's not this limitation. Because of the external APIs used, different microservices \ncan even be programmed in different programming languages. This allows the use of \nspecialized tools for different microservices, tailoring each one for each purpose and \nthereby avoiding conflicts.\nIn some cases, the usage of tools to abstract the contact with other \nmicroservices may produce extra calls that will be absolutely \nnecessary. For example, a task to process a document needs to \nobtain some user information, which requires calling a different \nmicroservice. The name is required at the start of the document, \nand the email at the end of it. A naïve implementation may \nproduce two requests to obtain the information instead of \nrequesting it all in a single go.\nJust because different microservices can be programmed in different \nlanguages doesn't mean that they should. Avoid the temptation \nof using too many programming languages in a microservices \narchitecture as this will complicate maintenance and make it \ndifficult for a member of a different team to be able to help, thereby \ncreating more isolated teams. \nHaving one or two default languages and frameworks available and \nthen allowing special justified cases is a sensible way to proceed.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2714,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "Chapter 9\n[ 289 ]\nAs we see, most of the characteristics of microservices make it more suited for a \nbigger operation, when the number of developers is high enough that they need to \nbe split into different teams and coordination needs to be more explicit. The high \nchange of pace in a big application also requires better ways to deploy and work \nindependently, in general.\nA small team can self-coordinate very well and will be able to work quickly and \nefficiently in a monolith.\nThis is not to say that a monolith can be very big. Some are. But, in a general sense, \nmicroservices architecture only makes sense if there are enough developers such that \ndifferent teams are working in the same system and are required to achieve a good \nlevel of independence between them.\nA side note about similar designs\nWhile the decision of monolith versus microservices is normally discussed in the \ncontext of web services, it's not exactly a new idea and it's not the only environment \nwhere there are similar ideas and structures.\nThe kernel of an OS can also be monolithic. In this case, a kernel structure is called \nmonolithic if it all operates within kernel space. A program running in kernel space \nin a computer can access the whole memory and hardware directly, something \nthat is critical for the usage of an OS, while at the same time, this is dangerous as \nit has big security and safety implications. Because the code in kernel space works \nso closely with the hardware, any failure here can result in the total failure of the \nsystem (a kernel panic). The alternative is to run in user space, which is the area \nwhere a program only has access to its own data, and has to interact explicitly with \nthe OS to retrieve information.\nFor example, a program in user space that wants to read from a file needs to \nmake a call to the OS, and the OS, in kernel space, will access the file, retrieve the \ninformation, and return it to the requested program, copying to a part of the memory \nwhere the program can access.\nThe idea of the monolithic kernel is that it can minimize this movement and context \nswitch between different kernel elements, such as libraries or hardware drivers.\nThe alternative to a monolithic kernel is called a microkernel. In a microkernel \nstructure, the kernel part is greatly reduced and elements such as filesystems, \nhardware drivers, and network stacks are executed in user space instead of in kernel \nspace. This requires these elements to communicate by passing messages through the \nmicrokernel, which is less efficient. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2660,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 290 ]\nAt the same time, it can improve the modularity and security of the elements, as any \ncrash in user space can be restarted easily.\nThere was a famous argument between Andrew S. Tanenbaum and Linus Torvalds \nabout what architecture is better, given that Linux was created as a monolithic \nkernel. In the long run, kernels have evolved toward hybrid models, where they take \naspects of both elements, incorporating microkernel ideas into existing monolithic \nkernels for flexibility.\nDiscovering and analyzing related architectural ideas can help to improve the tools \nat the disposal of a good architect and improve architectural understanding and \nknowledge.\nThe key factor – team communication\nA key element of the difference between microservices and monolithic architecture is \nthe difference in the communication structure that they support.\nIf the monolithic application has grown organically from a small project, as usually \nhappens, the internal structure can become messy, and requires developers with \nexperience in the system who can change and adapt it for any change. In bad cases, \nthe code can become very chaotic and be more and more complicated to work with.\nIncreasing the size of the development team becomes complicated, as each engineer \nrequires a lot of contextual information, and learning how to navigate the code is \ndifficult. The older teammates who have been around can help to train new team \nmembers, but they'll act as bottlenecks, and mentoring is a slow process that has \nlimits. Each new member of the team will require a significant amount of training \ntime until they can be productive in fixing bugs and adding new features.\nTeams also have a maximum natural size limit. Managing a team with too many \nmembers, without dividing it into smaller groups, is difficult.\nThe ideal size of a team depends on a lot of different factors, but \nbetween 5 and 9 is generally considered the ideal size to work \nefficiently.\nTeams that are bigger than that tend to self-organize into their own \nsmaller groups, losing focus as a unit and creating small information \nsilos where parts of the team are not aware of what's going on.\nTeams with fewer members create too much overhead in terms of \nmanagement and communication with other teams. They will be \nable to work faster with a slightly bigger size.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "Chapter 9\n[ 291 ]\nIf the growing size of the code requires it, this is the time to employ all the techniques \nthat we are describing in this book to generate more structure, architecting the \nsystem. This will involve defining modules with clear responsibilities and clear \nboundaries. This division allows the team to be divided into groups and allows them \nto work at creating ownership and explicit goals for each team.\nThis allows the teams to work in parallel without too much interference, so the extra \nmembers can increase the throughput in terms of features. As we commented before, \nclear boundaries will help in defining the work for each team.\nIn a monolith, however, these limitations are soft, as the whole system is accessible. \nSure, there is a certain discipline in terms of focusing on certain areas, and the \ntendency will be that one team will be able to access everything, and will tweak and \nbend internal APIs.\nWhen moving to a microservices architecture, the division of work becomes way \nmore explicit. The APIs between teams become hard limitations and there is a need \nfor more work upfront to communicate between teams. The trade-off is that teams \nare way more independent, as they can:\n•\t\nOwn the microservice completely without other teams coding in the same \ncode base \n•\t\nDeploy independently from other teams\nAs the code base will be smaller, new members of the team will be able to learn it \nquickly and be productive earlier. Because the external APIs to interact with other \nmicroservices will be explicitly defined, a higher level of abstraction will be applied, \nmaking it easier to interact.\nThis characteristic is not necessarily a bad thing, especially on a \nsmaller scale. This way of working with a small, focused team can \nproduce fantastic results, as they'll be able to adjust quickly all the \nrelated parts of the software. The drawback is that the members \nof the team need to be highly experienced and know their way \naround the software, which normally becomes more and more \ndifficult over time.\nNote this also means that different teams will know less about \nthe internals of other microservices compared with monolithic \napplications when there's at least a superficial knowledge of it. This \ncan create some friction when moving people from one team to \nanother.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2419,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 292 ]\nAs we saw in the first chapter, Conway's law is something to keep in mind when \nmaking architectural decisions that affect communication within the organization. \nLet's remember that this software law states that the structure of the software will \nreplicate the communication structure of the organization.\nA good example of Conway's law is the creation of DevOps practices. The older \nway of dividing work was to have different teams, one related to developing new \nfeatures, and another in charge of deploying and operating the software. The abilities \nrequired for each task are different, after all.\nThe risk of this structure is the \"I don't know what it is / I don't know where it runs\" \ndivision, which can cause the team responsible for developing new features to be \nunaware of bugs and problems associated with operating the software, while the \noperations team finds changes with little reaction time, and identifies bugs without \nunderstanding the inside operation of the software.\nThis division is still in place in many organizations, but the idea behind DevOps is \nthat the same team that develops the software is responsible for deploying it, thereby \ncreating a virtuous feedback loop where developers are aware of the complexities of \nthe deployment and can react and fix bugs in production and improve the operation \nof the software.\nNote that this normally involves creating a multi-functional team with people who \nboth understand operations and development, though they don't necessarily need to \nbe the same. Sometimes, an external team is responsible for creating a set of common \ntools for other teams to use in their operations.\nCommunication within the same team is different from the communication between \ndifferent teams. Communicating with other teams is always more difficult and \ncostlier. This is probably easy to say, but the implications of it for teamwork are big. \nExamples include the following:\nThis is a big change, and changing from the older structure to \nthe DevOps one involves mixing teams in a way that can be very \ndisruptive for the corporate culture. As we've tried to highlight \nhere, this involves people changes, which are slow and have a \nsignificant amount of pain associated with them. For example, \nthere may be a good operations culture where they share their \nknowledge and have fun together, and now they'll need to break \nup those teams and integrate them with new people.\nThis kind of process is difficult and should be planned carefully, \nunderstanding both its human and social scale.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "Chapter 9\n[ 293 ]\n•\t\nBecause APIs to be used externally from the team are going to be used by \nother engineers without the same level of expertise in the internals, it makes \nsense to make them generic and easy to use, as well as creating proper \ndocumentation.\n•\t\nIf a new design follows the structure of already existing teams, it will be \neasier to implement than the other way around. Architectural changes that lie \nbetween teams require organizational changes. Changing the structure of an \norganization is a long and painful process. Anyone who has been involved \nin a company reorganization can attest to that. These organizational changes \nwill be reflected in the software naturally, so ideally a plan will be generated \nto allow for it.\n•\t\nTwo teams working in the same service will create problems because each \nteam will try to pull it to their own goals. This is a situation that can happen \nwith some common libraries or with \"core\" microservices that are used by \nmultiple teams. Try to enforce clear owners for them to be sure that a single \nteam is in charge of any changes.\n•\t\nGiven that different physical locations and time zones naturally impose \ntheir own communication barrier, they normally are used to set up different \nteams, describing their own structured communication, like the API \ndefinition, between time zones.\nExplicit owners establish clarity about who is responsible \nfor changes and new features. Even if something is \nimplemented by someone else, the owner should be \nresponsible for approving it and giving direction and \nfeedback. They should also be prepared to have a long-\nterm vision and handle any technical debt.\nWorking remotely has increased significantly as a result \nof the COVID-19 crisis. This has also created the need \nto structure communication differently compared with \na team working together in the same room. This has \ndeveloped and improved communication skills, which can \nlead to better ways of organizing work. In any case, team \ndivision is not only a matter of being physically located \nin the same place but creating the bonds and structure to \nwork as a team.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 294 ]\nCommunication aspects of development are an important part of the work and \nshould not be underestimated. Keep in mind that changes to them are \"people \nchanges,\" which are more difficult to implement than tech changes.\nMoving from a monolith to microservices\nA usual case is the need to migrate from an existing monolithic architecture to a new \nmicroservices one.\nThe main reason for wanting to implement this change is the size of the system. \nAs we've discussed before, the main advantage of a microservice system is the \ncreation of multiple independent parts that can be developed in parallel, enabling \nthe development to be scaled and the pace increased by allowing more engineers to \nwork at the same time.\nThis is a move that makes sense if the monolith has grown to exceed a manageable \nsize and there are enough problems with releases, interfering features, and stepping \non each other's toes. But, at the same time, it's a very huge and painful transition to \nperform.\nChallenges for the migration\nWhile the final result may be much better than a monolithic application that shows \nits age, migrating to a new architecture is a big undertaking. We'll now look at some \nof the challenges and problems that we can expect in the process:\n•\t\nMigrating to microservices will require a huge amount of effort, actively \nchanging the way the organization operates, and will require a big upfront \ninvestment until it starts to pay off. The transition time will be painful and \nwill require compromises between the speed of migration and the regular \noperation of the service, as stopping the operation completely won't be an \noption. It will require a good deal of meetings and documentation to plan \nand communicate the migration to everyone. It needs to have active support \nat the executive level to ensure full commitment to get it done, with a clear \nunderstanding of why it is being done.\n•\t\nIt also requires a profound cultural change. As we've seen above, the key \nelement of microservices is the interaction between teams, which will \nchange significantly compared with the way of operating in a monolithic \narchitecture. This will likely involve changing teams and changing tools. \nTeams will have to be stricter in their usage and documentation of external \nAPIs. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2416,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "Chapter 9\n[ 295 ]\nThey'll need to be more formal in their interaction with other teams and \nprobably take attributions they didn't have before. In general, people don't \nlike change, and that could be responded to in the form of resistance by \nmembers of some teams. Be sure that these elements are taken into account.\n•\t\nAnother challenge is the training aspect. New tools will surely be used \n(we will cover Docker and Kubernetes later in this chapter), so some teams \nwill likely need to adapt to use them. Managing a cluster of services can be \ncomplicated to wrap one's head around, and it will likely involve different \ntools than the ones used previously. For example, local developers will likely \nbe very different. Learning how to operate and work with containers, if going \ndown that route, will take some time. This requires planning and the need to \nsupport team members until they are comfortable with the new system.\n•\t\nDividing the existing monolith into different services requires careful \nplanning. A bad division between services can make two services tightly \ncoupled, thereby not allowing independent deployment. This can result in a \nsituation where practically any change to one service will require a change \nin the other, even if, theoretically, this could be done independently. This \ncreates duplication of work, as routinely working on a single feature requires \nmultiple microservices to be changed and deployed. Microservices can be \nmutated later and boundaries redefined, but there's a high cost associated \nwith that. The same care should be taken later when adding new services.\n•\t\nThere's an overhead in creating microservices, as there is some work that \ngets replicated on each service. That overhead gets compensated for by \nallowing independent and parallel development. But, to take full advantage \nof that, you need numbers. A small development team of up to 10 people \ncan coordinate and handle a monolith very efficiently. It's only when the size \ngrows and independent teams are formed that migrating to microservices \nstarts to make sense. The bigger the company, the more it makes sense.\nA very clear example of this is the extra complexity for \ndebugging a request coming into the system, as it can \nbe jumping around different microservices. Previously, \nthis request was probably easier to track in the monolith. \nUnderstanding how a request moves and finding subtle \nbugs produced by that can be difficult. To be certain of \nfixing this, they will likely need to be replicated and fixed \nin local development, which, as we've seen, will entail the \nuse of different tools and systems.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2737,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 296 ]\n•\t\nA balance between allowing each team to make their own decisions and \nstandardize some common elements and decisions is necessary. If teams have \ntoo little direction, they'll keep reinventing the wheel over and over. They'll \nalso end up creating knowledge silos where the knowledge in a section of \nthe company is wholly non-transferable to another team, making it difficult \nto learn lessons collectively. Solid communication between teams is required \nto allow consensus and the reuse of common solutions. Allow controlled \nexperimentation, label it as such, and get the lessons learned across the board \nso that the other teams benefit. There will be tension between shared and \nreusable ideas and independent, multiple-implementation ideas.\n•\t\nFollowing the Agile principles, we know that working software is more \nimportant than extensive documentation. However, in microservices, it's \nimportant to maximize the usability of each individual microservice to \nreduce the amount of support between teams. That involves some degree of \ndocumentation. The best approach is to create self-documenting services.\n•\t\nAs we've discussed earlier, each call to a different microservice can increase \nthe delay of responses, as multiple layers will have to be involved. This can \nproduce latency problems, with external responses taking longer. They will \nalso be affected by the performance and capacity of the internal network \nconnecting the microservices.\nA move to microservices should be taken with care and by carefully analyzing its \npros and cons. It is possible that it will take years to complete the migration in a \nmature system. But for a big system, the resulting system will be much more agile \nand easy to change, allowing you to tackle technical debt effectively and to empower \ndevelopers to take full ownership and innovate, structuring communication and \ndelivering a high-quality, reliable service.\nA move in four acts\nThe migration from one architecture to another should be considered in four steps:\n1.\t Analyze the existing system carefully.\n2.\t Design to determine what the desired destination is.\nBe careful when introducing shared code across services. \nIf the code grows, it will make services dependent on \neach other. This can reduce the independence of the \nmicroservices.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2438,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "Chapter 9\n[ 297 ]\n3.\t Plan. Create a route to move, step by step, from the current system to the \nvision designed in the first stage.\n4.\t Execute the plan. This stage will need to be done slowly and deliberately, \nand at each step, the design and plan will need to be re-evaluated.\nLet's look at each of the steps in greater detail.\n1. Analyze\nThe very first step is to have a good understanding of our starting point with the \nexisting monolith. This may appear trivial, but the fact is that it is quite conceivable \nthat no particular person has a good understanding of all the details of the system. It \nmay require information gathering, compilation, and digging deep to understand the \nintricacies of the system.\nThe main objective of this phase should be to determine whether a change will \nactually be beneficial and get a preliminary idea of what microservices will result \nfrom the migration. Performing this migration is a big commitment, and it's always \na good idea to double-check that tangible benefits will result. Even if, at this stage, \nit won't be possible to estimate the effort required, it will start shaping the size of \nthe task.\nThe existing code can be described as legacy code. While a debate is \ncurrently taking place on exactly what code can be categorized as \nlegacy, the main property of it is code that is already in place and \ndoesn't follow the best and new practices that new code has.\nIn other words, legacy code is old code from some time ago and \nthat is very likely not up to date with current practices. However, \nlegacy code is critical, as it is in use and probably key for the day-\nto-day operations of the organization.\nThis analysis will benefit greatly from good metrics and actual data \nshowing the number of requests and interactions that are actually \nbeing produced in the system. This can be achieved through good \nmonitoring, and adding metrics and logs to the system to allow the \ncurrent behavior to be measured. This can lead to insights about \nwhat parts are commonly used, and, even better, parts that are \nalmost never used and can perhaps be deprecated and removed. \nMonitoring can continue to be used to ensure that the process is \ngoing according to plan.\nWe will discuss monitoring in more detail in Chapter 11, Package \nMangement, and Chapter 12, Logging.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2421,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 298 ]\nThis analysis can be almost instant if the system is already well-architected and \nproperly maintained, but may extend to months of meetings and digging into code if \nthe monolith is a mess of chaotic code. However, this stage will allow us to build on \nsolid foundations, knowing what the current system is.\n2. Design\nThe next stage of the process is to generate a vision in terms of what the system will \nlook like after breaking the monolith up into multiple microservices.\nEach microservice needs to be considered in isolation, and as part of the rest. Think \nin terms of what makes sense to separate. Some questions that may help you to \nstructure the design are as follows:\n•\t\nWhat microservices should be created? Can you describe each microservice \nwith a clear objective and area to control?\n•\t\nIs there any critical or core microservice that requires more attention \nor special requirements? For example, higher security or performance \nrequirements.\n•\t\nHow will the teams be structured to cover the microservices? Are there too \nmany for the team to support? If that's the case, can multiple requests or \nareas be joined as part of the same microservice?\n•\t\nWhat are the prerequisites of each microservice?\n•\t\nWhat new technologies will be introduced? Is any training required?\n•\t\nAre microservices independent? What are the dependencies between \nmicroservices? Is there any microservice that is accessed more than others?\n•\t\nCan microservices be deployed independently from each other? What's the \nprocess if a new change is introduced that requires a change in a dependent \ndependency?\n•\t\nWhat microservices are going to be exposed externally? What microservices \nare only exposed internally?\n•\t\nIs there any prerequisite in terms of required API limitations? For example, is \nthere any service that requires specific APIs, such as a SOAP connection?\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1999,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "Chapter 9\n[ 299 ]\nOther things that can be useful in informing the design can be to draw expected \nflow diagrams of requests that need to interact with multiple microservice, so as to \nanalyze the expected movement between services.\nSpecial care should be taken regarding whatever storage is decided for each \nmicroservice. In general, storage for one microservice should not be shared with \nanother, to isolate the data.\nThis has a very concrete application, that is, to not access a database or other kind \nof raw storage directly by two or more microservices. Instead, one microservice \nshould control the format and expose the data, and allow changes to the data by an \naccessible API.\nFor example, let's imagine that there are two microservices, one that controls reports \nand another that controls users. For certain reports, we may need to access the user \ninformation to pull, for example, the name and email of a user who generated a \nreport. We can break the microservice's responsibility by allowing the report service \nto access directly a database that contains user information.\nFigure 9.4: An example of incorrect usage, accessing the information directly from storage \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1286,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 300 ]\nInstead, the report service needs to access the user microservice through an API and \npull the data. That way, each microservice is responsible for its own storage and \nformat.\nFigure 9.5: This is the correct structure. Each microservice keeps its own independent storage.  \nThis way, any information is only shared through well-defined APIs\nAs we commented before, creating a flow diagram of some requests will help enforce \nthis separation and find possible points of improvement; for example, returning data \nfrom an API that is not required until later in the process.\nAt this stage, there's no need to design detailed APIs between microservices, but \nsome general ideas on what services handle what data and what the required flows \nbetween microservices are would be beneficial.\nWhile a prerequisite is not to mix storage, and to retain separation, \nyou can use the same backend service to provide support for \ndifferent microservices. The same database server can handle two \nor more logical databases that can store different information.\nGenerally, though, most microservices won't require their own \ndata to be stored and can work in a completely stateless way, \nrelying instead on other microservices to store the data.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1366,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "Chapter 9\n[ 301 ]\n3. Plan\nOnce the general areas are clear, it's time to get into more detail and start planning \nhow the system is going to be changed from the starting point to the end line.\nThe challenge here is to iteratively move into the new system while the system \nsimultaneously remains functional at all times. New features are likely being \nintroduced, but let's park that for the moment and talk only about the migration \nitself.\nTo be able to do so, we need to use what is known as the strangler pattern. This \npattern aims to gradually replace parts of the system with new ones until the entire \nprevious system is \"strangled\" and can be removed safely. This pattern gets applied \niteratively, slowly, migrating the functionality from the old system to the new one in \nsmall increments.\nFigure 9.6: The strangler pattern\nTo create new microservices, there are three possible strategies:\n•\t\nReplace the functionality with new code that substitutes the old code, \nfunctionally producing the same result. Externally, the code reacts exactly \nthe same to external requests, but internally, the implementation is new. \nThis strategy allows you to start from scratch and fix some of the oddities of \nthe old code. It can even be done in newer tools such as frameworks or even \nprogramming languages.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1409,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 302 ]\nAt the same time, this approach can be very time-consuming. If the legacy \nsystem is undocumented and/or untested, it can be difficult to guarantee the \nsame functionality. Also, if the functionality covered by this microservice is \nchanging quickly, it may enter a game of catch-up between the new system \nand the old one, where there's no time to replicate any new functionality.\n•\t\nDivide the functionality, copying and pasting code that exists in the monolith \ninto a new microservice structure. If the existing code is in good shape and \nstructured, this approach is relatively fast, only requiring some internal calls \nto be replaced with external API calls.\nThis process can also be made iterative by first starting with a single \nfunctionality migrated to the new microservice, and then, one by one, \nmoving the code until the functionality is completely migrated. At that point, \nit is safe to delete the code from the old system.\n•\t\nA combination of both divide and replace. Some parts of the same \nfunctionality can likely be copied directly, but for others, a new approach is \npreferred.\nThis will inform each microservice plan, although we will need to create a global \nview to determine which microservices to create in what order. \nHere are some useful points to think about to determine what the best course of \naction is:\n•\t\nWhat microservices need to be available first, taking into account \ndependencies that will be produced.\nThis approach makes the most sense where the legacy \nparts to be replicated are small and obsolete, like using a \ntech stack that is considered to be deprecated.\nIt may be necessary to include in the monolith new access \npoints to ensure that a new microservice can call back to \nobtain some information. \nIt's also possible that the monolith needs to be refactored \nto clarify elements and divide them into a structure that's \nmore in line with the new system. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "Chapter 9\n[ 303 ]\n•\t\nAn idea of what the biggest pain points are, and whether working on them \nis a priority. Pain points are the code or other elements that are changed \nfrequently and the current way of dealing with them in a monolith makes \nthem difficult. This can produce great benefits following migration.\n•\t\nWhat are the difficult points and the cans of worms? There will likely be \nsome. Acknowledge that they exist and minimize their impact on other \nservices. Note that they may be the same as the pain points, or they may \ndiffer. For example, old systems that are very stable are difficult points, but \nnot painful as per our definition, as they don't change.\n•\t\nNow for a couple of quick wins that will keep the momentum of the project \ngoing. Show the advantages to your teams and stakeholders quickly! This \nwill also allow everyone to understand the new mode of operation you want \nto move to and start working that way.\n•\t\nAn idea of the training that teams will require and what the new elements \nare that you want to introduce. Also, whether any skills are lacking in your \nteam – you may be planning to hire.\n•\t\nAny team changes and ownership of the new services. It's important to \nconsider feedback from the teams so that they can express their concerns \nregarding any oversights during the creation of the plan. Involve the team \nand value their feedback.\nOnce we have a plan on how we are going to proceed, it's time to do it.\n4. Execute\nFinally, we need to act on our plan to start the move from the outdated monolith to \nthe new wonderful land of microservices!\nThis will actually be the longest stage of the four, and arguably the most difficult. \nAs we said before, the objective is to keep the service running all throughout the \nprocess.\nThe key element for a successful transition is to maintain backward compatibility. \nThis means that the system keeps behaving like the monolithic system from an \nexternal point of view. That way, we can change the internals in terms of how the \nsystem works without affecting customers.\nIdeally, the new architecture will allow us to be faster, meaning the \nonly perceived change will be that the system is more responsive!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 304 ]\nThis is obviously easier said than done. Software development in a production \nenvironment has been referred to as starting an automobile race driving a Ford T \nand crossing the finishing line in a Ferrari, changing every single piece of it without \nstopping. Fortunately, software is so flexible that this is something we can even \ndiscuss.\nTo be able to make the change, from the monolith to the new microservice or \nmicroservices that handle the same functionality, the key tool is to use a load \nbalancer at the top level, right on the ingress of requests. This is especially useful if \nthe new microservice is directly replacing the requests. The load balancer can take \nthe intake of requests and redirect them to the proper service, in a controlled manner.\nThis can be used to migrate the requests from the monolith slowly to the new \nmicroservice that should receive this request. Keep in mind that the load balancer \ncan be configured by a different URL to direct the request to a different service, so \nit can use that small granularity to distribute the load properly across the different \nservices.\nThe process will look a little like this. First, the load balancer is directing all the \nrequests to the legacy monolith. Once the new microservice is deployed, the requests \ncan be load-balanced by introducing the new microservice. Initially, the balance \nshould only be forwarding a few requests to the new system, to be sure that the \nbehavior is the same.\nSlowly, over time, it can grow until all requests are migrated. For example, the first \nweek can only move 10% of the requests, the second week 30%, the third week 50%, \nand then 100% of all requests the week after.\nWe will assume that all incoming requests are HTTP requests. A \nload balancer can handle other kinds of requests, but HTTP is by \nfar the most common.\nThe migration period is 4 weeks. During that time, no new features \nand changes should be introduced as the interface needs to be \nstable between the legacy monolith and the new microservice. Be \nsure that all the parties involved are aware of the plan and each of \nthe steps.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "Chapter 9\n[ 305 ]\nAt that point, the handling of the requests in the legacy monolith is unused and can \nbe removed to cleanup if this makes sense.\nThis process is similar to the strangler pattern that we discussed before, but in this \ncase applied to individual requests. The load balancer will be an invaluable ally for \nimplementing the pattern in full form, extending this procedure in a greater mode, \nas we are adding more functionality and slowly migrating it to be certain that any \nproblem can be detected early and without affecting a large number of requests. \nExecution phases\nThe whole execution plan should consist of three phases:\n1.\t The pilot phase. Any plan will need to be tested with care. The pilot phase \nwill be when the plan is checked in terms of its feasibility and the tools \ntested. A single team should lead this effort, to be sure that they are focused \non it, and can learn and share quickly. Try to start on a couple of small \nservices and low-hanging fruit, so that the improvement is obvious for the \nteam. Good candidates are non-critical services, so if there's a problem, it \ndoesn't present a big impact. This phase will allow you to prepare for the \nmigration and to make adjustments and learn from inevitable mistakes.\n2.\t Consolidation phase. At this point, the basics of the migration are \nunderstood, but there's still a lot of code to migrate. The pilot team can \nthen start training other teams and spread the knowledge, so everyone \nunderstands how it should proceed. By this time, the basic infrastructure will \nbe in place, and hopefully the most obvious issues have been corrected or at \nleast there's a good understanding of how to deal with them.\nTo help with the spreading of knowledge, documenting standards will help \nteams to coordinate and depend less on asking the same questions over and \nover. Enforcing a list of prerequisites for a new microservice to be deployed \nand running in production will give clarity on what is required. Be sure \nalso to keep a feedback channel, so new teams can share their findings and \nimprove the process.\nThis phase will probably see some plan changes, as reality will overcome \nwhatever plan has been laid out in advance. Be sure to adapt and keep an eye \non the objective while navigating through the problems.\nAt this phase, the pace will be increased, as the uncertainty is being reduced \nas more and more code is migrated. At some point, creating and migrating a \nnew microservice will be routine for the team.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 306 ]\n3.\t Final phase. In this phase, the monolithic architecture has been split, and \nany new development is done in the microservices. There may still be some \nremains of the monolith that are regarded as unimportant or low priority. If \nthat's the case, the boundaries should be clear to contain the old way of doing \nthings.\nNow, teams can take full ownership of their microservices and start taking \nmore ambitious tasks, such as replacing a microservice completely by \ncreating an equivalent one in another programming language or changing \nthe architecture by merging or splitting microservices. This is the end stage \nwhere, from now on, you live in a microservices architecture. Be sure to \ncelebrate it with the team accordingly.\nThat's roughly the process. Of course, this may be a long and arduous process that \ncan span many months or even years. Be sure to keep a sustainable pace and a long-\nterm view on the objective to be able to continue until the goal is reached.\nContainerizing services\nThe traditional way of operating services is to use a server using a full OS, such \nas Linux, and then install on it all the required packages (for example, Python or \nPHP) and services (for example, nginx, uWSGI). The server acts as the unit, so each \nphysical machine needs to be independently maintained and managed. It also may \nnot be optimal from the point of view of hardware utilization.\nThis can be improved by replacing the physical server with virtual machines, so a \nsingle physical server can handle multiple VMs. This helps with hardware utilization \nand flexibility, but still requires each server to be managed as an independent \nphysical machine.\nContainers bring a different approach to this area. Instead of using a full-fledged \ncomputer (a server), with an installed OS, packages, and dependencies, and \nthen installing your software on top of that, which mutates more often than the \nunderlying system, it creates a package (the container image) that brings it all.\nMultiple tools help with this management, for example, \nconfiguration management tools such as Chef or Puppet. They can \nmanage multiple servers and guarantee that they have installed the \nproper versions and are running the proper services.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "Chapter 9\n[ 307 ]\nThe container has its own filesystem, including the OS, dependencies, packages, and \ncode, and is deployed as a whole. Instead of having a stable platform and running \nservices on top of them, containers run as a whole, self-containing everything \nrequired. The platform (host machine) is a thin layer that only needs to be able to \nrun the containers. Containers share the same kernel with the host, making them \nvery efficient to run, compared with VMs, which may require simulating the whole \nserver.\nThis allows, for example, different containers to be run in the same physical machine \nand have each container run a different OS, with different packages, and different \nversions of the code.\nThe most popular tool for building and running containers is Docker (https://www.\ndocker.com/). We will now examine how to operate with it.\nOnce installed, you should be able to check the version running and get something \nsimilar to the following:\n$ docker version\nClient:\n Cloud integration: 1.0.17\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.16.4\n Git commit:        f0df350\n Built:             Wed Jun  2 11:56:22 2021\n OS/Arch:           darwin/amd64\n Context:           desktop-linux\n Experimental:      true\nServer: Docker Engine - Community\n Engine:\nSometimes, containers are thought of as \"lightweight virtual \nmachines.\" This is not correct. Instead, think of them as a process \nwrapped in its own filesystem. This process is the main process of the \ncontainer, and when it finishes, the container stops running.\nTo install Docker, you can go to the documentation at https://\ndocs.docker.com/get-docker/ and follow the instructions. Use \nversion 20.10.7 or later.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 308 ]\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       b0f5bc3\n  Built:            Wed Jun  2 11:54:58 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0-rc95\n  GitCommit:        b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\nNow we need to build a container image that we can run.\nBuilding and running an image\nThe container image is the whole filesystem and instructions to run when it's started. \nTo start using containers we need to build the proper images that form the basis of \nthe system.\nAn image is created by applying a Dockerfile, a recipe that creates the image by \nexecuting different layers, one by one.\nLet's see a very simple Dockerfile. Create a file called sometext.txt containing \nsome small example text, and another file called Dockerfile.simple containing the \nfollowing text:\nFROM ubuntu \nRUN mkdir -p /opt/\nCOPY sometext.txt /opt/sometext.txt\nCMD cat /opt/sometext.txt\nThe first line, FROM, will start the image by using the Ubuntu image.\nRemember the description presented previously, that a container \nis a process surrounded by its own filesystem. Building the image \ncreates this filesystem.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "Chapter 9\n[ 309 ]\nOne of the main advantages of containers is the ability to use and share already \ncreated containers, either directly or as a starting point to enhance them. Nowadays, \nit is very common to create and push a container to Docker Hub to allow others to \nuse it directly. That's one of the great things about containers! They are very easy to \nshare and use.\nThe second line runs a command inside the container. In this case, it creates a new \nsubdirectory in /opt:\nRUN mkdir -p /opt/\nNext, we copy the current sometext.txt file inside, in the new subdirectory:\nCOPY sometext.txt /opt/sometext.txt\nFinally, we define the command to execute when the image is run:\nCMD cat /opt/sometext.txt\nTo build the image, we run the following command:\ndocker build -f <Dockerfile> --tag <tag name> <context>\nIn our case, we use the defined Dockerfile and example as a tag. The context is . \n(current directory), which defines the root point in terms of where to refer to all the \nCOPY commands:\n$ docker build -f Dockerfile.sample -–tag example .\n[+] Building 1.9s (8/8) FINISHED\n => [internal] load build definition from Dockerfile.sample                                              \n => => transferring dockerfile: 92B                                                                                   \n => [internal] load .dockerignore                                                                                     \n => => transferring context: 2B                                                                                       \nThere are many images that you can use as a starting point. You \nhave all the usual Linux distributions, such as Ubuntu, Debian, and \nFedora, but also images for full-fledged systems such as storage \nsystems (MySQL, PostgreSQL, and Redis) or images to work with \nspecific tools, such as Python, Node.js, or Ruby. Check Docker Hub \n(https://hub.docker.com) for all the available images.\nAn interesting starting point is to use the Alpine Linux distribution, \nwhich is designed to be small and focused on security. Check out \nhttps://www.alpinelinux.org for further information.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 310 ]\n => [internal] load metadata for docker.io/library/ubuntu:latest                           \n => [1/3] FROM docker.io/library/ubuntu@sha256:82becede498899ec668628e7\ncb0ad87b6e1c371cb8a1e597d83a47fac21d6af3       \n => [internal] load build context                                                                                     \n => => transferring context: 82B                                                                                      \n => CACHED [2/3] RUN mkdir -p /opt/                                                                                   \n => CACHED [3/3] COPY sometext.txt /opt/sometext.txt                                                                  \n => exporting to image                                                                                                \n => => exporting layers                                                                                               \n => => writing image sha256:e4a5342b531e68dfdb4d640f57165b704b1132cd18b\n5e2ba1220e2d800d066cb                          \nIf we list the available images, you will be able to see the example one:\n$ docker images\nREPOSITORY      TAG          IMAGE ID       CREATED         SIZE\nexample         latest       e4a5342b531e   2 hours ago     72.8MB\nubuntu          latest       1318b700e415   47 hours ago    72.8MB\nWe can now run the container, which will execute the cat command inside:\n$ docker run example\nSome example text\nThe container will stop the execution as the command finishes. You can see the \nstopped containers using the docker ps -a command, but a stopped container is \ngenerally not very interesting. \nWhile this way of running containers can be useful sometimes to compile binaries or \nother kinds of operations of a similar kind, normally, it's more common to create RUN \ncommands that are always running. In that case, it will run until stopped externally, \nas the command will run forever.\nA common exception to this is that the resulting filesystem is \nstored onto disk, so the stopped container may have interesting \nfiles generated as part of the command.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2235,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "Chapter 9\n[ 311 ]\nBuilding and running a web service\nA web service container is the most common type of microservice, as we have seen. \nTo be able to build and run one, we need to have the following parts:\n•\t\nProper infrastructure that runs the web service to a port in the container\n•\t\nOur code, which will run\nFollowing the usual architecture presented in previous chapters, we will use the \nfollowing tech stack:\n•\t\nOur code will be written in Python and use Django as the web framework\n•\t\nThe Python code will be executed through uWSGI\n•\t\nThe service will be exposed in port 8000 through an nginx web server\nLet's take a look at the different elements.\nThe code is structured in two main directories and one file:\n•\t\ndocker: This subdirectory contains the files related to the operation of Docker \nand other infrastructure.\n•\t\nsrc: The source code of the web service itself. The source code is the same as \nwe saw in Chapter 5, The Twelve-Factor App Methodology.\n•\t\nrequirements.txt: The file with the Python requirements for running the \nsource code.\nThe Dockerfile image is located in the ./docker subdirectory. We will follow it to \nexplain how the different parts connect:\nFROM ubuntu AS runtime-image\n# Install Python, uwsgi and nginx\nRUN apt-get update && apt-get install -y python3 nginx uwsgi uwsgi-\nplugin-python3\nThe code is available at https://github.com/PacktPublishing/\nPython-Architecture-Patterns/tree/main/chapter_09_\nmonolith_microservices/web_service.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1576,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 312 ]\nRUN apt-get install -y python3-pip\n# Add starting script and config\nRUN mkdir -p /opt/server\nADD ./docker/uwsgi.ini /opt/server\nADD ./docker/nginx.conf /etc/nginx/conf.d/default.conf\nADD ./docker/start_server.sh /opt/server\n# Add and install requirements\nADD requirements.txt /opt/server\nRUN pip3 install -r /opt/server/requirements.txt\n# Add the source code\nRUN mkdir -p /opt/code\nADD ./src/ /opt/code\nWORKDIR /opt/code\n# compile the static files\nRUN python3 manage.py collectstatic --noinput\nEXPOSE 8000\nCMD [\"/bin/sh\", \"/opt/server/start_server.sh\"]\nThe first part of the file starts the container from the standard Ubuntu Docker image \nand install the basic requirements: Python interpreter, nginx, uWSGI, and a couple of \ncomplementary packages – the uWSGI plugin to run python3 code and pip to be able \nto install Python packages:\nFROM ubuntu AS runtime-image\n# Install Python, uwsgi and nginx\nRUN apt-get update && apt-get install -y python3 nginx uwsgi uwsgi-\nplugin-python3\nRUN apt-get install -y python3-pip\nThe next stage is to add all the required scripts and config files to start the server and \nconfigure uWSGI and nginx. All these files are in the ./docker subdirectory and are \nstored inside the container in /opt/server (except for the nginx configuration that is \nstored in the default /etc/nginx subdirectory). \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "Chapter 9\n[ 313 ]\nWe ensure that the start script is executable:\n# Add starting script and config\nRUN mkdir -p /opt/server\nADD ./docker/uwsgi.ini /opt/server\nADD ./docker/nginx.conf /etc/nginx/conf.d/default.conf\nADD ./docker/start_server.sh /opt/server\nRUN chmod +x /opt/server/start_server.sh\nThe Python requirements are installed next. The requirements.txt file is added and \nthen installed through the pip3 command:\n# Add and install requirements\nADD requirements.txt /opt/server\nRUN pip3 install -r /opt/server/requirements.txt\nWe add the source code to /opt/code next. With the WORKDIR command, we execute \nany RUN command in that subdirectory and then run collectstatic with the Django \nmanage.py command to generate the static files in the proper subdirectory:\n# Add the source code\nRUN mkdir -p /opt/code\nADD ./src/ /opt/code\nWORKDIR /opt/code\n# compile the static files\nRUN python3 manage.py collectstatic --noinput\nFinally, we describe the exposed port (8000) and the CMD to run to start the container, \nthe start_server.sh script copied previously:\nEXPOSE 8000\nCMD [\"/bin/bash\", \"/opt/server/start_server.sh\"]\nSome Python packages may need certain packages to be installed \nin the container in the first stage to be sure that some tools are \navailable; for example, installing certain database connection \nmodules will require the proper client libraries to be installed.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1485,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 314 ]\nuWSGI configuration\nThe uWSGI configuration is very similar to the one presented in Chapter 5, The \nTwelve-Factor App Methodology:\n[uwsgi]\nplugins=python3\nchdir=/opt/code\nwsgi-file = microposts/wsgi.py\nmaster=True\nsocket=/tmp/uwsgi.sock\nvacuum=True\nprocesses=1\nmax-requests=5000\nuid=www-data\n# Used to send commands to uWSGI\nmaster-fifo=/tmp/uwsgi-fifo\nThe only difference is the need to include the plugins parameter to indicate that it \nruns the python3 plugin (this is because the Ubuntu-installed uwsgi package doesn't \nhave it activated by default). Also, we will run the process with the same user as \nnginx, to allow them to communicate through the /tmp/uwsgi.sock socket. This is \nadded with uid=www-data, with www-data being the default nginx user.\nnginx configuration\nThe nginx configuration is also very similar to the one presented in Chapter 5, The \nTwelve-Factor App Methodology:\nserver {\n    listen 8000 default_server;\n    listen [::]:8000 default_server;\n    root /opt/code/;\n    location /static/ {\n        autoindex on;\n        try_files $uri $uri/ =404;\n    }\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1215,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "Chapter 9\n[ 315 ]\n    location / {\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        uwsgi_pass unix:///tmp/uwsgi.sock;\n        include uwsgi_params;\n    }\n}\nThe only difference is the exposed port, which is 8000. Note that the root directory is \n/opt/code, making the static file directory /opt/code/static. This needs to be in sync \nwith the configuration from Django.\nStart script\nLet's take a look at the script that starts the service, start_script.sh:\n#!/bin/bash\n_term() {\n  # See details in the uwsgi.ini file and\n  # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html\n  # q means \"graceful stop\"\n  echo q > /tmp/uwsgi-fifo\n}\ntrap _term TERM\nnginx\nuwsgi --ini /opt/server/uwsgi.ini &\n# We need to wait to properly catch the signal, that's why uWSGI is \nstarted\n# in the background. $! is the PID of uWSGI\nwait $!\n# The container exits with code 143, which means \"exited because \nSIGTERM\"\n# 128 + 15 (SIGTERM)\n# http://www.tldp.org/LDP/abs/html/exitcodes.html\n# http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_12_02.html\necho \"Exiting, bye!\"\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1208,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 316 ]\nThe core of the start is at the center, in these lines, nginx:\nuwsgi --ini /opt/server/uwsgi.ini &\nwait $!\nThis starts both nginx and uwsgi, and waits until the uwsgi process is not running. In \nBash, $! is the PID of the last process (the uwsgi process).\nWhen Docker attempts to stop a container, it will first send a SIGTERM signal to \nthe container. That's why we create a trap command that captures this signal and \nexecutes the _term() function. This function sends a graceful stop command to the \nuwsgi queue, as we described in Chapter 5, The Twelve-Factor App Methodology, which \nends the process in a graceful manner:\n_term() {\n  echo q > /tmp/uwsgi-fifo\n}\ntrap _term TERM\nIf the initial SIGTERM signal is not successful, Docker will stop the container killing it \nfollowing a grace period, but that will risk having a non-graceful end for the process.\nBuilding and running\nWe can now build the image and run it. To build the image, we perform a similar \ncommand as before:\n$ docker build -f docker/Dockerfile --tag example .\n[+] Building 0.2s (19/19) FINISHED\n => [internal] load build definition from Dockerfile\n => => transferring dockerfile: 85B\n => [internal] load .dockerignore \n => => transferring context: 2B\n => [internal] load metadata for docker.io/library/ubuntu:latest\n => [ 1/14] FROM docker.io/library/ubuntu\n => [internal] load build context\n => => transferring context: 4.02kB\n => CACHED [ 2/14] RUN apt-get update && apt-get install -y python3 \nnginx uwsgi uwsgi-plugin-pytho  \n => CACHED [ 3/14] RUN apt-get install -y python3-pip\n => CACHED [ 4/14] RUN mkdir -p /opt/server \n => CACHED [ 5/14] ADD ./docker/uwsgi.ini /opt/server\n => CACHED [ 6/14] ADD ./docker/nginx.conf /etc/nginx/conf.d/default.\nconf\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "Chapter 9\n[ 317 ]\n => CACHED [ 7/14] ADD ./docker/start_server.sh /opt/server\n => CACHED [ 8/14] RUN chmod +x /opt/server/start_server.sh\n => CACHED [ 9/14] ADD requirements.txt /opt/server\n => CACHED [10/14] RUN pip3 install -r /opt/server/requirements.txt\n => CACHED [11/14] RUN mkdir -p /opt/code \n => CACHED [12/14] ADD ./src/ /opt/code\n => CACHED [13/14] WORKDIR /opt/code\n => CACHED [14/14] RUN python3 manage.py collectstatic --noinput\n => exporting to image\n => => exporting layers\n => => writing image sha256:7be9ae2ab0e16547480aef6d32a11c2ccaa3da4aa5e\nfbfddedb888681b8e10fa\n => => naming to docker.io/library/example\nTo run the service, start the container, mapping its port 8000 to a local port, for \nexample, local 8000:\n$ docker run -p 8000:8000 example\n[uWSGI] getting INI configuration from /opt/server/uwsgi.ini\n*** Starting uWSGI 2.0.18-debian (64bit) on [Sat Jul 31 20:07:20 2021] \n***\ncompiled with version: 10.0.1 20200405 (experimental) [master revision \n0be9efad938:fcb98e4978a:705510a708d3642c9c962beb663c476167e4e8a4] on 11 \nApril 2020 11:15:55\nos: Linux-5.10.25-linuxkit #1 SMP Tue Mar 23 09:27:39 UTC 2021\nnodename: b01ce0d2a335\nmachine: x86_64\nclock source: unix\npcre jit disabled\ndetected number of CPU cores: 2\ncurrent working directory: /opt/code\ndetected binary path: /usr/bin/uwsgi-core\nsetuid() to 33\nchdir() to /opt/code\nyour memory page size is 4096 bytes\ndetected max file descriptor number: 1048576\nlock engine: pthread robust mutexes\nthunder lock: disabled (you can enable it with --thunder-lock)\nuwsgi socket 0 bound to UNIX address /tmp/uwsgi.sock fd 3\nPython version: 3.8.10 (default, Jun  2 2021, 10:49:15)  [GCC 9.4.0]\n*** Python threads support is disabled. You can enable it with \n--enable-threads ***\nPython main interpreter initialized at 0x55a60f8c2a40\nyour server socket listen backlog is limited to 100 connections\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 318 ]\nyour mercy for graceful operations on workers is 60 seconds\nmapped 145840 bytes (142 KB) for 1 cores\n*** Operational MODE: single process ***\nWSGI app 0 (mountpoint='') ready in 1 seconds on interpreter \n0x55a60f8c2a40 pid: 11 (default app)\n*** uWSGI is running in multiple interpreter mode ***\nspawned uWSGI master process (pid: 11)\nspawned uWSGI worker 1 (pid: 13, cores: 1)\nAfter doing this, you can access your local address, http://localhost:8000, and \naccess the service; for example, accessing the URL http://localhost:8000/api/\nusers/jaime/collection:\nFigure 9.7: Microposts list\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "Chapter 9\n[ 319 ]\nYou'll see the access log in the screen where you started the container:\n[pid: 13|app: 0|req: 2/2] 172.17.0.1 () {42 vars in 769 bytes} [Sat Jul \n31 20:28:56 2021] GET /api/users/jaime/collection => generated 10375 \nbytes in 173 msecs (HTTP/1.1 200) 8 headers in 391 bytes (1 switches on \ncore 0)\nThe container can be stopped gracefully using the docker stop command. To do so, \nyou'll need to first discover the container ID using docker ps:\n$ docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED          \nSTATUS          PORTS                                       NAMES\nb01ce0d2a335   example   \"/bin/bash /opt/serv…\"   23 minutes ago   Up \n23 minutes   0.0.0.0:8000->8000/tcp, :::8000->8000/tcp   hardcore_chaum\n$ docker stop b01ce0d2a335\nb01ce0d2a335\nThe container log will show the details when capturing the SIGTERM signal sent by \nDocker and will then exit:\nCaught SIGTERM signal! Sending graceful stop to uWSGI through the \nmaster-fifo\nExiting, bye!\nTo be able to set this example, we made some conscious decisions to simplify the \noperation compared with a typical service.\nCaveats\nRemember to check Chapter 5, The Twelve-Factor App Methodology, to see the defined \nAPI and understand it better.\nThe DEBUG mode in the Django settings.py file is set to True, which allows us to see \nmore information when, for example, 404 or 500 errors are triggered. This parameter \nshould be disabled in production as it can give away critical information.\nThe STATIC_ROOT and STATIC_URL parameters need to be coordinated between Django \nand nginx to point to the same place. That way, the collectstatic command will \nstore the data in the same place where nginx will pick it up.\nThe most important detail is the use of a SQLite database instead of an internal one. \nThis database is stored in the src/db.sqlite3 file, in the filesystem of the container. \nThis means that if the container is stopped and restarted, any changes will be \ndestroyed. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 320 ]\nThe db.sqlite3 file in the GitHub repo contains some information that has been \nstored for convenience, two users, jaime and dana, each with a couple of microposts. \nThe API so far hasn't been defined in such a way to create new users, so it needs to \nrelay into creating them using Django tools or manipulating the SQL directly. These \nusers are added for demonstration purposes.\nIn general, this database usage is not well suited for production usage, requiring \nconnection to a database external to the container. This obviously requires an \navailable external database, which complicates the setup.\nNow that we know how to use containers, we can perhaps start another Docker \ncontainer with a database, such as MySQL, for a better configuration.\nIf we want to create more than one container and connect them, like a web server \nand a database that acts as a backend for storing the data, instead of starting all the \ncontainers individually, we can use orchestration tools.\nOrchestration and Kubernetes\nManaging multiple containers and connecting them is known as orchestrating them. \nMicroservices that are deployed in containers will have to orchestrate them to be \nsure that the multiple microservices are interconnecting.\nThis concept includes details such as discovering where the other containers \nare, dependencies between services, and generating multiple copies of the same \ncontainer.\nAs an exercise, create a script that seeds the database with \ninformation as part of the build process.\nA containerized database is not a great idea for production. In \ngeneral, containers are great for stateless services that change \noften, as they can be started and stopped easily. Databases tend to \nbe very stable and there are a lot of services that make provisions \nfor managed databases. The advantages that containers bring are \nsimply not relevant for a typical database.\nThat doesn't mean that there are usages out of production. It is a \ngreat option for local development, for example, as it allows the \ncreation of a replicable local environment easily.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2202,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "Chapter 9\n[ 321 ]\nThere are several tools that can perform orchestration, the two most common ones \nbeing docker-compose and Kubernetes.\ndocker-compose is part of the general offering by Docker. It works very well for small \ndeployments or local development. It defines a single YAML file that contains the \ndefinition of the different services, and the name that they can use. It can be used \nto replace a lot of docker build and docker run commands, as it can define all the \nparameters in the YAML file.\nKubernetes is aimed at bigger deployments and clusters and allows the generation \nof a full logical structure for containers to define how they connect to one another, \nthereby allowing abstraction to the underlying infrastructure.\nAny physical (or virtual) server configured in Kubernetes is called a node. All nodes \ndefine the cluster. Each node is handled by Kubernetes, and Kubernetes will create \na network between the nodes and assign the different containers to each of them, \nattending to the available space on them. This means that the number, location, or \nkind of node doesn't need to be handled by the services.\nInstead, the applications in the cluster are distributed in the logical layer. Several \nelements can be defined:\n•\t\nPod. A Pod is the minimal unit defined in Kubernetes, and it is defined as a \ngroup of containers that runs as a unit. Normally, Pods will consist of just \none container, but in some cases, they may comprise several. Everything in \nKubernetes runs in Pods.\n•\t\nDeployment. A collection of Pods. The Deployment will define the number \nof replicas that are needed, and create the proper number of Pods. Each Pod \nfor the same deployment can live in different nodes, but that's under the \ncontrol of Kubernetes.\nOrchestration tools are very powerful, as well as complex, and \nrequire that you become familiar with a lot of terms. To explain \nthem fully is beyond the scope of this book, but we will point \nto some and give a short introduction. Please refer to the linked \ndocumentation in the sections below for more information.\nYou can check the documentation for Docker Compose here: \nhttps://docs.docker.com/compose/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "Microservices vs Monolith\n[ 322 ]\nBecause the Deployment controls the number of Pods, if a Pod crashes, the \nDeployment will restart it. Also, the Deployment can be manipulated to \nchange the number, for example, by creating autoscalers. If the image to \nbe deployed in the Pods is changed, the Deployment will create new Pods \nwith the right image and remove the old ones accordingly, based on rolling \nupdates or other strategies.\n•\t\nService. A label that can be used to route requests to certain Pods, acting as \na DNS name. Normally, this will point to the Pods created for deployment. \nThis allows other Pods in the system to send requests to a known place. The \nrequests will be load-balanced between the different Pods.\n•\t\nIngress. External access to a service. This will map an incoming DNS to a \nservice. Ingresses allow applications to be exposed externally. An external \nrequest will go through the process of entering through an Ingress, being \ndirected to a Service, and then handled by a specific pod.\nSome components can be described in a Kubernetes cluster, such as ConfigMaps, \ndefining key-value pairs that can be used for configuration purposes; Volumes to \nshare storage across Pods; and Secrets to define secret values that can be injected \ninto Pods.\nKubernetes is a fantastic tool that can handle pretty big clusters with hundreds of \nnodes and thousands of Pods. It's also a complex tool that requires you to learn how \nit can be used and has a significant learning curve. It's pretty popular these days, and \nthere's plenty of documentation about it. The official documentation can be found \nhere: https://kubernetes.io/docs/home/.\nSummary\nIn this chapter, we described both the monolithic and microservices architectures. \nWe started by presenting the monolithic architecture and how it tends to be a \n\"default architecture,\" generated organically as an application is designed. Monoliths \nare created as unitary blocks that contain all the code within a single block.\nIn comparison, the microservices architecture divides the functionality of the whole \napplication into smaller parts so that they can be worked in parallel. For this strategy \nto work, it needs to define clear boundaries and document how to interconnect the \ndifferent services. Compared with the monolithic architecture, microservices aim \nto generate more structured code and control big code bases by dividing them into \nsmaller, more manageable systems.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "Chapter 9\n[ 323 ]\nWe discussed what the best architecture is and how to choose whether to design \na system as a monolith or as microservices. Each approach has its pros and cons, \nbut in general, systems start as monolithic and the move to divide the code base \ninto smaller microservices comes after the code base and the number of developers \nworking on it reaches a certain size.\nThe difference between the two architectures is not just technical. It largely involves \nhow developers working on the system need to communicate and divide the teams. \nWe discussed the different aspects to take into account, including the structure and \nsize of the teams.\nSince migrating from an old monolithic architecture to a new microservices one is \nsuch a common case, we talked about how to approach the work, analyze it, and \nperform it, using a four-stage roadmap: Analyze, Design, Plan, and Execute.\nWe then discussed how containerizing services (and, in particular, microservices) \ncan be helpful. We explored how to use Docker as a tool to containerize services and \nits multiple advantages and uses. We included an example of containerizing our \nexample web service, as described in Chapter 5, The Twelve-Factor App Methodology.\nFinally, we described briefly the usage of an orchestration tool to coordinate and \nintercommunicate between multiple containers, and the most popular, Kubernetes. \nWe then covered a brief introduction to Kubernetes.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\nYou can get more information about microservices and how \nto perform a migration from a monolithic architecture to a \nmicroservices one in the book Hands-On Docker for Microservices \nwith Python, from the author of this book, which expands on these \nconcepts and goes into greater depth.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "Designing is an important stage to have a plan of action, but really the meat of the \ndeveloping process is in the implementation.\nImplementing the general architecture design will require multiple smaller design \ndecisions about how the code needs to be structured and developed. It doesn't \nmatter how good the design is, the execution is critical and will validate or adjust the \nprepared plan.\nA solid implementation, then, requires developers to be skeptical about their own \ncoding abilities and code needs to be tested thoroughly before it can be considered \n\"done.\" This is a normal operation, and when done constantly, it produces good \ncascading effects, not only improving the quality of the code and reducing the \nnumber of problems but also increasing the capacity of the team to foresee weak \npoints and harden them to be sure that, once in operation, the software is reliable \nand works with as few problems as possible.\nWe will see how to approach testing, including the use of Test-Driven Design \n(TDD), a practice that puts testing at the center of the development process.\nSometimes some code aspects need to be shared multiple times to be reused. A \npowerful tool in the Python world is the easy creation and sharing of modules that \ncan be implemented. We will see how to structure, create, and maintain standard \nPython modules, including uploading them into PyPI, the standard Python \nrepository of third-party packages.\nThis section of the book includes the following chapters:\n•\t\nChapter 10, Testing and TDD, explaining different approaches to testing, the \nTest-Driven Design methodology, and tools to write tests easily\n•\t\nChapter 11, Package Management, describing how to structure code to be \nshared to use in different parts of the system or to share it with the broader \ncommunity\nPart III\nImplementation\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1937,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "[ 327 ]\n10\nTesting and TDD\nNo matter how good a developer is, they'll write code that doesn't always perform \ncorrectly. This is unavoidable, as no developer is perfect. But it's also because the \nexpected results are sometimes not the ones that one would think of while immersed \nin coding.\nDesigns rarely go as expected and there's always a discussion going back and forth \nwhile they are being implemented, until refining them and getting them correct.\nEveryone has a plan until they get punched in the mouth. – Mike Tyson\nWriting software is notoriously difficult because of its extreme plasticity, but at the \nsame time, we can use software to double-check that the code is doing what it is \nsupposed to do.\nWriting tests allows you to detect problems while the code is fresh and with some \nsane skepticism to verify that the expected results are the actual results. We will see \nduring the chapter how to write tests easily, as well as different strategies to write \ndifferent tests for capturing different kinds of problems.\nBe aware that, as with any other code, tests can have bugs as well.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "Testing and TDD\n[ 328 ]\nWe will describe how to work under TDD, a methodology that works by defining \nthe tests first, to ensure that the validation is as independent of the actual code \nimplementation as possible.\nWe will also show how to create tests in Python using common unit test frameworks, \nthe standard unittest module, and the more advanced and powerful pytest.\nIn this chapter, we'll cover the following topics:\n•\t\nTesting the code\n•\t\nDifferent levels of testing\n•\t\nTesting philosophy\n•\t\nTest-Driven Development\n•\t\nIntroduction to unit testing in Python\n•\t\nTesting external dependencies\n•\t\nAdvanced pytest\nLet's start with some basic concepts about testing.\nTesting the code\nThe first question when discussing testing the code is a simple one: What exactly do \nwe mean by testing the code?\nWhile there are multiple answers to that, in the broadest sense, the answer could be \n\"any procedure that probes the application to check that it works correctly before it reaches the \nfinal customers.\" In this sense, any formal or informal testing procedure will fulfil the \ndefinition.\nNote this chapter is a bit longer than others, mostly due to the need \nto present example code.\nThe most relaxed approach, which is sometimes seen in small \napplications with one or two developers, is to not create specific \ntests but to do informal \"full application runs\" checking that a \nnewly implemented feature works as expected.\nThis approach may work for small, simple applications, but the \nmain problem is ensuring that older features remain stable.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1650,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "Chapter 10\n[ 329 ]\nBut, for high-quality software that is big and complex enough, we need to be a bit \nmore careful about the testing. So, let's try to come up with a more precise definition \nof testing: Testing is any documented procedure, preferably automated, that, from a known \nsetup, checks the different elements of the application work correctly before it reaches the final \ncustomers.\nIf we check the differences with the previous definition, there are several key words. \nLet's check each of them to see the different details:\n•\t\nDocumented: Compared with the previous version, the aim should be that \nthe tests are documented. This allows you to reproduce them precisely if \nnecessary and allows you to compare them to discover blind spots.\nThere are multiple ways that a test can be documented, either by specifying a \nlist of steps to run and expected results or by creating code that runs the test. \nThe main idea is that a test can be analyzed, be run several times by different \npeople, be changed if necessary, and have a clear design and result.\n•\t\nPreferably automated: Tests should be able to be run automatically, with as \nlittle human intervention as possible. This allows you to trigger Continuous \nIntegration techniques to run many tests over and over, creating a \"safety \nnet\" that is able to catch unexpected errors as early as possible. We say \n\"preferably\" because perhaps some tests are impossible or very costly \nto totally automate. In any case, the objective should be to have the vast \nmajority of tests automated, to allow computers to do the heavy lifting and \nsave precious human time. There are also multiple software tools that allow \nyou to run tests, which can help.\n•\t\nFrom a known setup: To be able to run tests in isolation, we need to know \nwhat the status of the system should be before running the test. That ensures \nthat the result of a test will not create a certain state that could interfere with \nthe next test. Before and after a test, certain cleanup may be required.\nThis can make running tests in batches slower, compared with not worrying \nabout the initial or end status, but it will create a solid foundation to avoid \nproblems.\nAs a general rule, and especially in automated tests, the \norder in which the tests are executed should be irrelevant, \nto avoid cross-contamination. This is easier said than done, \nand in some cases, the order of tests can create problems. \nFor example, test A creates an entry that test B reads. If \ntest B is run in isolation, it will fail as it expects the entry \ncreated by A. These cases should be fixed, as they can \ngreatly complicate debugging. Also, being able to run tests \nindependently allows them to be parallelized.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "Testing and TDD\n[ 330 ]\n•\t\nDifferent elements of the application: Most tests should not address the \nwhole application, but smaller parts of it. We will talk more later about the \ndifferent levels of testing, but tests should be specific about what are they \ntesting and cover different elements, as tests covering more ground will be \ncostlier.\nA key element of testing is to have a good return on investment. Designing and \nrunning tests takes time, and that time needs to be well spent. Any test needs \nto be maintained, which should be worth it. Over the whole chapter, we will be \ncommenting on this important aspect of testing.\nThis general definition helps to start the discussion, but we can be more concrete \nabout the different tests defined by how much of the system is under test, during \neach test.\nThere's an important kind of testing that we are not covering \nwith this definition, which is called exploratory testing. These tests \nare typically run by QA engineers, who use the final application \nwithout a clear preconceived idea but try to pre-emptively find \nproblems. If the application has a customer-facing UI, this style of \ntesting can be invaluable in detecting inconsistencies and problems \nthat are not detected in the design phase.\nFor example, a good QA engineer will be able to say that the color \nof a button on page X is not the same as the button on page Y, or \nthat the button is not evident enough to perform an action, or that \nto perform a certain action there's a prerequisite that's not evident \nor possible with the new interface. Any user experience (UX) check \nwill probably fall into this category.\nBy its nature, this kind of testing cannot be \"designed\" or \n\"documented,\" as it ultimately comes down to interpretation and a \ngood eye to understand whether the application feels correct. Once a \nproblem is detected, then it can be documented to be avoided.\nWhile this is certainly useful and recommended, this style of \ntesting is more an art than an engineering practice and we won't be \ndiscussing it in detail.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "Chapter 10\n[ 331 ]\nDifferent levels of testing\nAs we described before, tests should cover different elements of the system. This \nmeans that a test can address a small or big part of the system (or the whole system), \ntrying to reduce its range of action.\nWhen testing a small part of the system, we reduce the complexity of the test and \nscope. We need to call only that small part of the system, and the setup is easier to \nstart with. In general, the smaller the element to test, the faster and easier it is to test it.\nWe will define three different levels or kinds of tests, from small to big scopes:\n•\t\nUnit tests, for tests that check only part of a service\n•\t\nIntegration tests, for tests that check a single service as a whole\n•\t\nSystem tests, for tests that check multiple services working together\nNames can actually vary quite a lot. In this book, we won't be very strict with \ndefinitions, instead defining soft limits and suggesting finding a balance that works \nfor your specific project. Don't be shy to take decisions on the proper level for each \ntest and define your own nomenclature, and always keep in mind how much effort it \ntakes to create tests to be sure that they are always worth it.\nLet's start describing each of the levels in more detail.\nUnit tests\nThe smallest kind of test is also the one where most effort is typically invested, the \nunit test. This kind of test checks the behavior of a small unit of code, not the whole \nsystem. This unit of code could be as small as a single function or test a single API \nendpoint, and so on.\nThe definition of the levels can be a little blurred. For example, \nintegration and unit tests can be defined side by side, and the \ndifference between them could be more academic in that case.\nAs we said above, there's a lot of debate on how big a unit test \nshould actually be, based on what the \"unit\" is and whether it is \nactually a unit. For example, in some cases, people will only call a \ntest a unit test if it involves a single function or class.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "Testing and TDD\n[ 332 ]\nBecause a unit test checks a small part of the functionality, it can be very easy to set \nup and quick to run. Therefore, making new unit tests is quick and can thoroughly \ntest the system, checking that the small individual pieces that make the whole system \nwork as expected.\nThe objective of unit tests is to check in depth the behavior of a defined feature of a \nservice. Any external requests or elements should be simulated, meaning that they \nare defined as part of the test. We will cover unit tests in more detail later in the \nchapter, as they are the key elements of the TDD approach.\nIntegration tests\nThe next level is the integration test. This is checking the whole behavior of a service \nor a couple of services.\nThe main goal of integration testing is to be sure that the different services or \ndifferent modules inside the same service can work with each other. While in unit \ntests, external requests are simulated, integration tests use the real service.\nIt's important to note that, commonly, different services will be developed \nby different developers or even different teams, and they can diverge in their \nunderstanding of how a particular API is implemented, even in the event of a well-\ndefined spec.\nThe setup in integration tests is more complex than in unit tests, as more elements \nneed to be properly set up. This makes integration tests slower and more expensive \nthan unit tests.\nIntegration tests are great to check that different services work in unison, but there \nare some limitations.\nIntegration tests are normally not as thorough as unit tests, focusing on checking \nbasic functionality and following a happy path. A happy path is a concept in testing \nmeaning that the test case should produce no errors or exceptions. \nThe simulation of external APIs may still be required. For example, \nsimulating an external payment provider for the tests. But, in \ngeneral, as many real services should be used for integration \ntests as possible, as the point of the test is to test that the different \nservices work together.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2183,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "Chapter 10\n[ 333 ]\nExpected errors and exceptions are normally tested in unit tests, since they are also \nelements that can fail. That doesn't mean that every single integration test should \nfollow a happy path; some integration errors may be worth checking, but in general, \na happy path tests the expected general behavior of the feature. They will compose \nthe bulk of the integration tests.\nSystem tests\nThe final level is the system level. System tests check that all the different services \nwork correctly together.\nA requirement for this kind of test is that there are actually multiple services in \nthe system. If not, they are not different from tests at the lower levels. The main \nobjective of these tests is to check that the different services can cooperate, and the \nconfiguration is correct.\nSystem tests are slow and difficult to implement. They require the whole system \nto be set up, with all the different services properly configured. Creating that \nenvironment can be complicated. Sometimes, it's so difficult that the only way of \nactually performing any system tests is to run them in the live environment.\nWhile this is not ideal, sometimes it is unavoidable and can help to improve \nconfidence after deployments, to ensure that the new code is working correctly. In \nthat case, given the constraints, only a minimum amount of tests should be run, as \nthe live environment is critical. The tests to run should also exercise the maximum \namount of common functionality and services to detect any critical problem as fast as \npossible. This set of tests is sometimes called acceptance tests or smoke tests. They may \nbe run manually, as a way of ensuring that everything looks correct.\nSmoke tests should be very clear, well documented, and designed carefully to cover \nthe most critical parts of the whole system. Ideally, they should also be read-only, so \nthey don't leave useless data after their execution.\nThe environment configuration is an important part of what \nthese tests check. That may make them important to run on each \nenvironment that is under test, including the live environment.\nOf course, smoke tests can be run not only on the live environment \nand can work as a way to ensure that other environments are \nworking correctly.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2371,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "Testing and TDD\n[ 334 ]\nTesting philosophy\nA key element of everything involved with testing is another question: Why test? \nWhat are we trying to achieve with it?\nAs we've seen, testing is a way of ensuring that the behavior of the code is the \nexpected one. The objective of testing is to detect possible problems (sometimes \ncalled defects) before the code is published and used by real users.\nA defect that goes undetected and gets deployed into a live system is pretty \nexpensive to repair. First of all, it needs to be detected. In a live application with a \nlot of activity, detecting a problem can be difficult (though we will talk about it in \nChapter 16, Ongoing Architecture), but even worse, it will normally be detected by a \nuser of the system using the application. It's possible that the user won't properly \ncommunicate the problem back, so the problem is still present, creating problems or \nlimiting activity. The detecting user might abandon the system, or at the very least \ntheir confidence in the system will decrease.\nAny reputational cost will be bad, but it can also be difficult to extract enough \ninformation from the user to know exactly what happened and how to fix it. This \nmakes the cycle between detecting the problem and fixing it long.\nAny testing system will improve the ability to fix defects earlier. Not only can we \ncreate a specific test that simulates exactly the same problem, but we can also create \na framework that executes tests regularly to have a clear approach to how to detect \nand fix problems.\nDifferent testing levels have different effects on this cost. In general, any problem \nthat can be detected at the unit test level is going to be cheaper to fix there, and the \ncost increases from there. Designing and running a unit test is easier and faster than \ndoing the same with an integration test, and an integration test is cheaper than a \nsystem test.\nThere's a subtle difference between defects and bugs. Bugs are a \nkind of defect where the software behaves in a way that it's not \nexpected to. For example, certain input produces an unexpected \nerror. Defects are more general. A defect could be that a button is \nnot visible enough, or that the logo on a page is not the correct one. \nIn general, tests are way better at detecting bugs than other defects, \nbut remember what we said about exploratory testing.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "Chapter 10\n[ 335 ]\nThe different test levels could be understood as different layers capturing possible \nproblems. Each layer will capture different problems if they appear. The closer to \nthe start of the process (design and unit tests while coding), the cheaper it is to create \na dense net that will detect and alert for problems. The cost of fixing a problem \nincreases the farther away it is from the controlled environment at the start of the \nprocess.\nFigure 10.1: The cost of fixing defects increases the later they get detected\nSome defects are impossible to detect at the unit test level, like the integration of \ndifferent parts. That's where the next level comes into play. As we've seen, the worst \nscenario is not detecting a problem and it affecting real users on the live system.\nBut having tests is not only a good way of capturing problems once. Because a test \ncan still remain, and be run on new code changes, it also creates a safety net while \ndeveloping to be sure that creating new code or modifying the code does not affect \nthe old functionality.\nThis is one of the best arguments for running tests automatically \nand constantly, as per Continuous Integration practices. The \ndeveloper can focus on the feature being developed, while the \nContinuous Integration tool will run every test, alerting early \nif there's a problem with some test. A problem with previously \nintroduced functionality that is failing is called a regression.\nRegression problems are quite common, so having good test \ncoverage is great to prevent them going undetected. Specific tests \ncovering previous functionality to ensure that it keeps running \nas expected can be introduced. These are regression tests, and \nsometimes they are added after we have detected a regression \nproblem.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1887,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "Testing and TDD\n[ 336 ]\nAnother benefit of having good tests that check the behavior of the system is that the \ncode itself can be changed heavily, knowing that the behavior will remain the same. \nThese changes can be made to restructure the code, clean it, and in general improve \nit. These changes are called refactoring the code, changing how the code is written \nwithout changing the expected behavior of it.\nNow, we should answer the question \"what is a good test?\" As we discussed, writing \na test is not free, there's an effort involved, and we need to be sure that it's worth it. \nHow can we create good ones?\nHow to design a great test\nDesigning good tests requires a certain mindset. The objective while designing the \ncode that covers certain functionality is to make the code fulfill that functionality \nwhile at the same time being efficient, writing clear code that could even be \ndescribed as elegant.\nThe objective of the test is to be sure that the functionality sticks to the expected \nbehavior, and that all the different problems that can arise produce results that make \nsense.\nNow, to be able to really put the functionality to the test, the mindset should be to \nstress the code as much as possible. For example, let's imagine a function divide(A, \nB), that divides two integers between -100 and 100: A between B.\nWhile approaching the test, we need to check what the limits are of this, trying \nto check that the function is performing properly with the expected behavior. For \nexample, the following tests could be created:\nAction\nExpected behavior\nComments\ndivide(10, 2)\nreturn 5\nBasic case\ndivide(-20, 4)\nreturn -5\nDivide one negative and one positive \ninteger\ndivide(-10, -5)\nreturn 2\nDivide two negative integers\ndivide(12, 2)\nreturn 5 \nNot exact division\ndivide(100, 50)\nreturn 2 \nMaximum value of A\ndivide(101, 50)\nProduce an input \nerror\nValue of A exceeding the maximum\ndivide(50, 100)\nreturn 0 \nMaximum value of B\ndivide(50, 101)\nProduce an input \nerror\nValue of B exceeding the maximum\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "Chapter 10\n[ 337 ]\ndivide(10, 0)\nProduce an \nexception\nDivide by zero\ndivide('10', 2)\nProduce an input \nerror\nInvalid format for parameter A\ndivide(10, '2')\nProduce an input \nerror\nInvalid format for parameter B\nNote how we are testing different possibilities:\n•\t\nThe usual behavior of all the parameters is correct, and the division works \ncorrectly. This includes both positive and negative numbers, exact division, \nand inexact division.\n•\t\nValues within the maximum and minimum values: We check that the \nmaximum values are hit and correct, and the next value is properly detected.\n•\t\nDivision by zero: A known limitation on functionality that should produce a \npredetermined response (exception).\n•\t\nWrong input format.\nWe can really create a lot of test cases for simple functionality! Note that all these \ncases can be expanded. For example, we can add divide(-100, 50) and divide(100, \n-50) cases. In those cases, the question is the same: are those tests adding better \ndetection of problems?\nThe proper balance between the number of tests and not having tests that cover \nfunctionality already checked by an existing test (for example, creating a big table \ndividing numbers with a lot of divisions) may depend greatly on the code under test \nand practices in your organization. Some critical areas may require more thorough \ntesting as a failure there could be more important.\nThe best test is the test that really stresses the code and ensures \nthat it's working as expected, trying very hard to cover the most \ndifficult use cases. Making the tests ask difficult questions of the \ncode under test is the best way of preparing your code for the real \naction. A system under load will see all kinds of combinations, so \nthe best preparation for that is to create tests that try as hard as \npossible to find problems, to be able to solve them before moving \nto the next phase.\nThis is analogous to football training, where a series of very \ndemanding exercises are presented to be sure that the trainee \nwill be able to perform later, during the match. Be sure that your \ntraining regime is hard enough to properly prepare for demanding \nmatches!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2258,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "Testing and TDD\n[ 338 ]\nNote that tests are done independently from the implementation of the code. A \ntest definition is done purely from an external view of the function to test, without \nrequiring knowing what's inside. This is called black-box testing. A heathy test suite \nalways starts with this approach.\nA critical ability to develop as a developer writing tests is to detach from the \nknowledge of the code itself and approach tests independently.\nIn some cases, this external approach won't be enough. If the developer knows \nthat there's some specific area where there could be problems, it may be good \nto complement it with tests that check functionality that is not apparent from an \nexternal point of view.\nFor example, a function that calculates a result based on some input may have an \ninternal point where the algorithm changes to calculate it using different models. \nThis information doesn't need to be known by the external user, but it will be good to \nadd a couple of checks that the transition works correctly.\nThis kind of testing is called white-box testing, in comparison to the black-box \napproach discussed early.\nFor example, any external API should test any input with care and \nbe really defensive about that, as external users may abuse external \nAPIs. For example, testing what happens when strings are input \nin integer fields, infinity or NaN (Not a Number) values are added, \npayload limits are exceeded, the maximum size of a list or page is \nexceeded, etc.\nBy comparison, interfaces that are mostly internal will require \nless testing, as the internal code is less likely to abuse the API. For \nexample, if the divide function is only internal, it might not be \nrequired to test that the input format is incorrect, just to check that \nthe limits are respected.\nTesting can be so detached that it may use independent people just \nto create the tests, like a QA team performing tests. Unfortunately, \nthis is not a possible approach for unit tests, which will likely be \ncreated by the same developers that write the code itself.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2169,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "Chapter 10\n[ 339 ]\nBlack-box testing tries to avoid a common problem where the same developer writes \nboth the code and the test and then checks that the interpretation of the feature \nimplemented in the code works as expected, instead of checking that it works as it \nshould when looking from an external endpoint. We will take a look later at TDD, \nwhich tries to ensure tests are created without the implementation in mind by \nwriting the tests before writing the code.\nStructuring tests\nIn terms of structure, especially for unit tests, a nice way to structure tests is using \nthe Arrange Act Assert (AAA) pattern.\nThis pattern means the test is in three different phases:\n•\t\nArrange: Prepare the environment for the tests. This includes all the setup to \nget the system right at the point before performing the next step, at a stable \nmoment. \n•\t\nAct: Perform the action that is the objective of the test.\n•\t\nAssert: Check that the result of the action is the expected one.\nThe test gets structured as a sentence like this:\nGIVEN (Arrange) an environment known, the ACTION (Act) produces the specified \nRESULT (Assert)\nIt's important to remember that, in a test suite, white-box tests \nshould always be secondary to black-box tests. The main objective \nis to test the functionality from an external perspective. White-box \ntesting may be a good addition, especially in some aspects, but it \nshould have a lower priority.\nDeveloping the ability to be able to create good black-box tests is \nimportant and should be transmitted to the team.\nThis pattern is also sometimes called GIVEN, WHEN, THEN as \neach step can be described in those terms.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1748,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "Testing and TDD\n[ 340 ]\nNote that this structure aims for all the tests to be independent, and for each to test a \nsingle thing.\nNote that this structure can be used whether the tests are executed through code or \nrun manually, though they'll be used more for automated tests. When running them \nmanually, the Arrange stage can take a long time to produce for each test, leading \nto a lot of time spent on that. Instead, manual tests are normally grouped together \nin the pattern that we describe above, executing a series of Act and Assert and using \nthe input in the previous stage as setup for the next. This creates a dependency in \nrequiring to run tests in a specific sequence, which is not great for unit test suites, but \nit can be better for smoke tests or other environments where the Arrange step is very \nexpensive.\nLet's see an example of code created with this structure. Imagine that we have a \nmethod that we want to test, called method_to_test. The method is part of a class \ncalled ClassToTest.\n def test_example():\n    # Arrange step\n    # Create the instance of the class to test\n    object_to_test = ClassToTest(paramA='some init param', \n                                 paramB='another init param')\n    # Act step\n    response = object_to_test.method_to_test(param='execution_param')\nA common different pattern is to group act steps in tests, testing \nmultiple functionalities in a single test. For example, test that \nwriting a value is correct and then check that the search for the \nvalue returns the proper value. This won't follow the AAA pattern. \nInstead, to follow the AAA pattern, two tests should be created, the \nfirst one to validate that the write works correctly and the second \nwhere the value is created as part of the setup in the Arrange step \nbefore doing the search.\nIn the same way, if the code to test is purely functional (meaning \nthat only the input parameters are the ones that determine its state, \nlike the divide example above), the Arrange step is not required.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2116,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "Chapter 10\n[ 341 ]\n    # Assert step\n    assert response == 'expected result'\nEach of the steps is very clearly defined. The first one prepares, in this case, an object \nin the class that we want to test. Note that we may need to add some parameters or \nsome preparation so the object is in a known starting point so the next steps work as \nexpected.\nThe Act step just generates the action that is under test. In this case, call the method_\nto_test method for the prepared object with the proper parameter.\nFinally, the Assert step is very straightforward and just checks the response is the \nexpected one.\nAnother common pattern that appears using the AAA pattern for tests is to create \ncommon functions for testing in Arrange steps. For example, creating a basic \nenvironment, which could require a complex setup, and then having multiple copies \nwhere the Act and Assert steps are different. This reduces the repetition of code.\nFor example:\ndef create_basic_environment():\n    object_to_test = ClassToTest(paramA='some init param', \n                                 paramB='another init param')\n    # This code may be much more complex and perhaps have\n    # 100 more lines of code, because the basic environment\n    # to test requires a lot of things to set up\n    return object_to_test\ndef test_exampleA():\n    # Arrange\n    object_to_test = create_basic_environment()\n    # Act\n    response = object_to_test.method_to_test(param='execution_param')\n    # Assert\n    assert response == 'expected result B'\nIn general, both the Act and Assert steps are simple to define and \nwrite. The Arrange step is where most of the effort of the test will \nnormally be.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "Testing and TDD\n[ 342 ]\ndef test_exampleB():\n    # Arrange\n    object_to_test = create_basic_environment()\n    # Act\n    response = object_to_test.method_to_test(param='execution_param')\n    # Assert\n    assert response == 'expected result B'\nWe will see later how we can structure multiple tests that are very similar to avoid \nrepetition, which is a problem when having big test suites. Having big test suites is \nimportant to create good test coverage, as we saw above.\nTest-Driven Development\nA very popular technique to approach programming is Test-Driven Development \nor TDD. TDD consists of putting tests at the center of the developing experience.\nThis builds on some of the ideas that we exposed earlier in the chapter, though \nworking on them with a more consistent view.\nThe TDD flow to develop software works as follows:\n1.\t New functionality is decided on to be added to the code.\n2.\t A new test is written to define the new functionality. Note that this is done \nbefore the code.\n3.\t The test suite is run to show that it's failing.\n4.\t The new functionality is then added to the main code, focusing on simplicity. \nOnly the required feature, without extra details, should be added.\nRepetition in tests is, up to a certain point, unavoidable and even \nhealthy to a certain degree. When changing the behavior of some \npart of the code because there are changes, the tests need to be \nchanged accordingly to accommodate the changes. This change \nhelps to weigh the size of the changes and avoid making big \nchanges lightly, as the tests will work as a reminder of the affected \nfunctionality.\nNonetheless, mindless repetition is not great, and we will see later \nsome options to reduce the amount of code to be repeated.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1834,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "Chapter 10\n[ 343 ]\n5.\t The test suite is run to show that the new test is working. This may need to \nbe done several times until the code is ready.\n6.\t The new functionality is ready! Now the code can be refactored to improve \nit, avoiding duplication, rearranging elements, grouping it with previously \nexisting code, etc.\nThe cycle can start again for any new functionality.\nAs you can see, TDD is based on three main ideas:\n•\t\nWrite the tests before writing the code: This prevents the problem of \ncreating a test that is too tightly coupled with the current implementation, \nforcing the developer to think about the test and the feature before jumping \ninto writing it. It also forces the developer to check that the test actually \nfails before the feature is written, being sure that a problem later on will be \ndetected. This is similar to the black box testing approach that we described \nearlier in the How to design a great test section.\n•\t\nRun the tests constantly: A critical part of the process is running the whole \ntest suite to check that all the functionality in the system is correct. This is \ndone over and over, every time that a new test is created, but also while \nthe functionality is being written. Running the tests is an essential part of \ndeveloping in TDD. This ensures that all functionality is always checked and \nthat the code works as expected at all times so any bug or discrepancy can be \nsolved quickly.\n•\t\nWork in very small increments: Focus on the task at hand, so each step \nbuilds and grows a test suite that is big and covers the whole functionality of \nthe code in depth.\nThis big test suite creates a safety net that allows you to perform refactors of the \ncode often, big and small, therefore improving the code constantly. Small increments \nmean small tests that are specific and need to be thought about before adding \nthe code.\nAn extension of this idea is a focus on writing only the code that's \nrequired for the task at hand and not more. This is sometimes \nreferred to as the YAGNI principle (You Ain't Gonna Need It). \nThe intention of this principle is to prevent overdesigning or \ncreating code for \"foreseeable requests in the future,\" which, in \npractice, have a high probability of never materializing and, even \nworse, makes the code more difficult to change in other directions. \nGiven that software development is notoriously difficult to plan in \nadvance, the emphasis should be on keeping things small and not \ngetting too far ahead of yourself.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2608,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "Testing and TDD\n[ 344 ]\nThese three ideas interact constantly during the development cycle, and it keeps the \ntests at the center of the development process, hence the name of the practice.\nAnother important advantage of TDD is that putting the focus so heavily on the tests \nmeans that how the code is going to be tested is thought about from the start, which \nhelps in designing code that's easily testable. Also, reducing the amount of code to \nwrite, focusing on it being strictly required to pass the test reduces the probability of \noverdesign. The requirement to create small tests and work in increments also tends \nto generate modular code, in small units that are combined together but are able to \nbe tested independently.\nThe general flow is to be constantly working with new failing tests, making them \npass and then refactoring, sometimes called the \"red/green/refactor\" pattern: red when \nthe test is failing and green when all tests are passing.\nRefactoring is a critical aspect of the TDD process. It is strongly encouraged, to \nconstantly improve the quality of the existing code. One of the best outcomes of this \nway of working is the generation of very extensive test suites that cover each detail \nof the code functionality, meaning that refactoring code can be done knowing that \nthere's a solid ground that is going to capture any problems introduced by changing \nthe code and adding bugs.\nImproving the code's readability, usability, and so on, by refactoring is known to \nhave a good impact in terms of improving the morale of developers and increasing \nthe pace at which changes can be introduced, as the code is kept in good shape.\nAnother important aspect of TDD is the requirement of speedy tests. As tests are \nalways running following TDD practices, the total execution time is quite important. \nThe time that it takes for each test should be considered carefully, as the growing size \nof the test suite will make it take longer to run.\nThere's a general threshold where focus gets lost, so running tests taking longer \nthan around 10 seconds will make them not \"part of the same operation,\" risking the \ndeveloper thinking about other stuff.\nIn general, and not only in TDD, allowing time to clean up old \ncode and improve it is critical to maintain a good pace for changes. \nOld code that is stale tends to be more and more difficult to work \nwith, and over time it will require way more effort to change it to \nmake more changes. Encouraging healthy habits to care about the \ncurrent state of the code and allowing time to perform maintenance \nimprovements is critical for the long-term sustainability of any \nsoftware system.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2763,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "Chapter 10\n[ 345 ]\nObviously, running the whole test suite in under 10 seconds will be extremely \ndifficult, especially as the number of tests grows. A full unit test suite for a complex \napplication can consist of 10,000 tests or more! In real life, there are multiple \nstrategies that can help alleviate this fact.\nThe whole test suite doesn't need to be run all the time. Instead, any test runner \nshould allow you to select a range of tests to run, allowing you to reduce the number \nof tests to run on each run while the feature is in development. This means running \nonly the tests that are relevant for the same module, for example. It can even mean \nrunning a single test, in certain cases, to speed up the result.\nAnyway, as the time taken to run tests is important in TDD, observing the duration \nof tests is important, and generating tests that can run quickly is key to being able \nto work in the TDD way. This is mainly achieved by creating tests that cover small \nportions of the code, and therefore the time to set up can be kept under control.\nIntroducing TDD into new teams\nIntroducing TDD practices in an organization can be tricky, as they change the way \nto perform actions that are quite basic, and go a bit against the usual way of working \n(writing tests after writing the code).\nWhen considering introducing TDD into a team, it's good to have an advocate that \ncan act as a point of contact for the rest of the team and solve the questions and \nproblems that may arise through creating tests.\nOf course, at some point, the whole test suite should be run. TDD is \nactually aligned with Continuous Integration, as it is also based on \nrunning tests, this time automatically once the code is checked out \ninto a repo. The combination of being able to run a few tests locally \nto ensure that things are working correctly while developing with \nthe whole test suite running in the background once the code is \ncommitted to the repo is great.\nTDD practices work best with unit tests. Integration and system \ntests may require a big setup that is not compatible with the speed \nand tight feedback loop required for TDD to work.\nFortunately, as we saw before, unit testing is where the bulk of \ntesting is typically focused on most projects.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2359,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "Testing and TDD\n[ 346 ]\nTDD is very popular in environments where pair programming is also common, \nso it's another possibility to have someone drive a session while training the other \ndevelopers and introducing the practice.\nIt may be challenging to apply TDD techniques with already existing code, as pre-\nexisting code can be difficult to test in this configuration, especially if the developers \nare new to the practice. TDD works great for new projects, though, as a test suite for \nnew code will be created at the same time as the code. A mixed approach of starting \na new module inside an existing project, so most code is new and can be designed \nusing TDD techniques, reduces the problem of dealing with legacy code.\nIf you want to see if TDD can be effective for new code, try to start small, using some \nsmall project with a small team to be sure that it's not too disruptive and that the \nprinciples can be properly digested and applied. There are some developers that \nreally love to use TDD principles, as it fits their personality and how they approach \nthe process of developing. Remember that this is not necessarily how everyone will \nfeel and that starting with these practices requires time, and perhaps it won't be \npossible to apply them 100% as the previous code might limit it.\nProblems and limitations\nTDD practices are very popular and widely followed in the industry, though they \nhave their limits. One is the problem of big tests that take too long to run. These tests \nmay be unavoidable in certain situations.\nAnother is the difficulty of fully taking this approach if it is not done from the \nbeginning, as parts of the code will already be written, and perhaps new tests should \nbe added, violating the rule of creating the tests before the code.\nAnother problem is designing new code while the features to be implemented are \nfluid and not fully defined. This requires experimentation, for example, to design a \nfunction to return a color that contrasts with an input color, for example, to present \na contrast color based on a theme selectable by the user. This function may require \ninspection to see if it \"looks right,\" which can require tweaking that's difficult to \nachieve with a preconfigured unit test.\nRemember, the key element of TDD is the mindset of forcing the \ndeveloper to think first about how a particular feature is going \nto be tested before starting to think about the implementation. \nThis mindset doesn't come naturally and needs to be trained and \npracticed.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "Chapter 10\n[ 347 ]\nNot a problem specifically with TDD, but something to be careful about is to \nremember to avoid dependencies between tests. This can happen with any test suite, \nbut given the focus on creating new tests, it's a likely problem if the team is starting \nwith TDD practices. Dependencies can be introduced by requiring tests to run in a \nparticular order, as the tests can contaminate the environment. This is normally not \ndone on purpose, but it's done inadvertently while writing multiple tests.\nIn any case, remember that TDD is not necessarily something that it's all or nothing, \nbut a set of ideas and practices that can help you design code that's well tested and \nhigh quality. Not every single test in the system needs to be designed using TDD, \nbut a lot of them can be.\nExample of the TDD process\nLet's imagine that we need to create a function that:\n•\t\nFor values lower than 0, returns zero\n•\t\nFor values greater than 10, returns 100\n•\t\nFor values between, it returns the power of two of the value. Note that for the \nedges, it returns the power of two of the input (0 for 0 and 100 for 10)\nTo write the code in full TDD fashion, we start with the smallest possible test. Let's \ncreate the smallest skeleton and the first test.\ndef parameter_tdd(value):\n    pass\nassert parameter_tdd(5) == 25\nWe run the test, and get an error with the test failing. Right now, we will use pure \nPython code, but later in the chapter, we'll see how to run tests more efficiently.\n$ python3 tdd_example.py\nTraceback (most recent call last):\n  File \".../tdd_example.py\", line 6, in <module>\nA typical effect on that will be that some tests fail if run \nindependently, as their dependencies are not run in that case.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1827,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "Testing and TDD\n[ 348 ]\n    assert parameter_tdd(5) == 25\nAssertionError\nThe implementation of the use case is quite straightforward.\ndef parameter_tdd(value):\n    return 25\nYes, we are actually returning a hardcoded value, but that's really all that is required \nto pass the first tests. Let's run the tests now and you'll see no errors.\n$ python3 tdd_example.py\nBut now we add tests for the lower edge. While these are two lines, they can be \nconsidered the same test, as they're checking that the edge is correct.\nassert parameter_tdd(-1) == 0\nassert parameter_tdd(0) == 0\nassert parameter_tdd(5) == 25\nLet's run the tests again.\n$ python3 tdd_example.py\nTraceback (most recent call last):\n  File \".../tdd_example.py\", line 6, in <module>\n    assert parameter_tdd(-1) == 0\nAssertionError\nWe need to add code to handle the lower edge.\ndef parameter_tdd(value):\n    if value <= 0:\n        return 0\n    return 25\nWhen running the test, we see that it's running the tests correctly. Let's add \nparameters now to handle the upper edge.\nassert parameter_tdd(-1) == 0\nassert parameter_tdd(0) == 0\nassert parameter_tdd(5) == 25\nassert parameter_tdd(10) == 100\nassert parameter_tdd(11) == 100\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1288,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "Chapter 10\n[ 349 ]\nThis triggers the corresponding error.\n$ python3 tdd_example.py\nTraceback (most recent call last):\n  File \"…/tdd_example.py\", line 12, in <module>\n    assert parameter_tdd(10) == 100\nAssertionError\nLet's add the higher edge.\ndef parameter_tdd(value):\n    if value <= 0:\n        return 0\n    if value >= 10:\n        return 100\n    return 25\nThis runs correctly. We are not confident that all the code is fine, and we really want \nto be sure that the intermediate section is correct, so we add another test.\nassert parameter_tdd(-1) == 0\nassert parameter_tdd(0) == 0\nassert parameter_tdd(5) == 25\nassert parameter_tdd(7) == 49\nassert parameter_tdd(10) == 100\nassert parameter_tdd(11) == 100\nAha! Now it shows an error, due to the initial hardcoding.\n$ python3 tdd_example.py\nTraceback (most recent call last):\n  File \"/…/tdd_example.py\", line 15, in <module>\n    assert parameter_tdd(7) == 49\nAssertionError\nSo let's fix it.\ndef parameter_tdd(value):\n    if value <= 0:\n        return 0\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "Testing and TDD\n[ 350 ]\n    if value >= 10:\n        return 100\n    return value ** 2\nThis runs all the tests correctly. Now, with the safety net of the tests, we think we can \nrefactor the code a little bit to clean it up.\ndef parameter_tdd(value):\n    if value < 0:\n        return 0\n    if value < 10:\n        return value ** 2\n    return 100\nWe can run the tests all through the process and be sure that the code is correct. The \nfinal result may be different based on what the team considers good code or what is \nmore explicit, but we have our test suite that will ensure that the tests are consistent, \nand the behavior is correct.\nThe function here is quite small, but this shows what the flow is when writing code \nin the TDD style.\nIntroduction to unit testing in Python\nThere are multiple ways to run tests in Python. One, as we have seen above, a bit \ncrude, is to execute code with multiple asserts. A common one is the standard library \nunittest.\nPython unittest\nunittest is a module included in the Python standard library. It is based on the \nconcept of creating a testing class that groups several testing methods. Let's write a \nnew file with the tests written in the proper format, called test_unittest_example.\npy.\nimport unittest\nfrom tdd_example import parameter_tdd\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "Chapter 10\n[ 351 ]\nclass TestTDDExample(unittest.TestCase):\n    def test_negative(self):\n        self.assertEqual(parameter_tdd(-1), 0)\n    def test_zero(self):\n        self.assertEqual(parameter_tdd(0), 0)\n    def test_five(self):\n        self.assertEqual(parameter_tdd(5), 25)\n    def test_seven(self):\n        # Note this test is incorrect\n        self.assertEqual(parameter_tdd(7), 0)\n    def test_ten(self):\n        self.assertEqual(parameter_tdd(10), 100)\n    def test_eleven(self):\n        self.assertEqual(parameter_tdd(11), 100)\nif __name__ == '__main__':\n    unittest.main()\nLet's analyze the different elements. The first ones are the imports on top.\nimport unittest\nfrom tdd_example import parameter_tdd\nWe import the unittest module and the function to test. The most important part \ncomes next, which defines the tests.\nclass TestTDDExample(unittest.TestCase):\n    def test_negative(self):\n        self.assertEqual(parameter_tdd(-1), 0)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "Testing and TDD\n[ 352 ]\nThe class TestTDDExample groups the different tests. Notice that it's inheriting \nfrom unittest.TestCase. Then, methods that start with test_ will produce the \nindependent tests. Here, we will show one. Internally, it calls the function and \ncompares the result with 0, using the self.assertEqual function.\nFinally, we add this code.\nif __name__ == '__main__':\n    unittest.main()\nThis runs the tests automatically if we run the file. So, let's run the file:\n$ python3 test_unittest_example.py\n...F..\n======================================================================\nFAIL: test_seven (__main__.TestTDDExample)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \".../unittest_example.py\", line 17, in test_seven\n    self.assertEqual(parameter_tdd(7), 0)\nAssertionError: 49 != 0\n----------------------------------------------------------------------\nRan 6 tests in 0.001s\nFAILED (failures=1)\nAs you can see, it has run all six tests, and shows any errors. Here, we can clearly see \nthe problem. If we need more detail, we can run with -v showing showing each of \nthe tests that are being run:\n$ python3 test_unittest_example.py -v\ntest_eleven (__main__.TestTDDExample) ... ok\ntest_five (__main__.TestTDDExample) ... ok\ntest_negative (__main__.TestTDDExample) ... ok\ntest_seven (__main__.TestTDDExample) ... FAIL\ntest_ten (__main__.TestTDDExample) ... ok\nNotice that test_seven is defined incorrectly. We do this to \nproduce an error when running it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1640,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "Chapter 10\n[ 353 ]\ntest_zero (__main__.TestTDDExample) ... ok\n======================================================================\nFAIL: test_seven (__main__.TestTDDExample)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \".../unittest_example.py\", line 17, in test_seven\n    self.assertEqual(parameter_tdd(7), 0)\nAssertionError: 49 != 0\n----------------------------------------------------------------------\nRan 6 tests in 0.001s\nFAILED (failures=1)\nYou can also run a single test or combination of them using the -k option, which \nsearches for matching tests.\n$ python3 test_unittest_example.py -v -k test_ten\ntest_ten (__main__.TestTDDExample) ... ok\n----------------------------------------------------------------------\nRan 1 test in 0.000s\nOK\nunittest is extremely popular and can accept a lot of options, and it's compatible \nwith virtually every framework in Python. It's also very flexible in terms of \nways of testing. For example, there are multiple methods to compare values, like \nassertNotEqual and assertGreater.\nIt also has setUp and tearDown methods to execute code before and after the \nexecution of each test in the class.\nThere's a specific assert function that works differently, which \nis assertRaises, used to detect when the code generates an \nexception. We will take a look at it later when testing mocking \nexternal calls.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1517,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "Testing and TDD\n[ 354 ]\nWhile unittest is probably the most popular test framework, it's not the most \npowerful one. Let's take a look at it.\nPytest\nPytest simplifies writing tests even further. One common complaint about unittest \nis that it forces you to set a lot of assertCompare calls that are not obvious. It also \nneeds to structure the tests, adding a bit of boilerplate code, like the test class. Other \nproblems are not as obvious, but when creating big test suites, the setup of different \ntests can start to get complicated.\nPytest instead simplifies the running and defining of tests, and captures all the \nrelevant information using standard assert statements that are easier to read and \nrecognize.\nBe sure to install pytest through pip in your environment.\n$ pip3 install pytest\nLet's see how to run the tests defined in the unittest, in the file test_pytest_\nexample.py.\nfrom tdd_example import parameter_tdd\ndef test_negative():\nBe sure to take a look at the official documentation: https://\ndocs.python.org/3/library/unittest.html.\nA common pattern is to create classes that inherit from other test \nclasses. Over time, that can grow legs of its own.\nIn this section, we will use pytest in the simplest way. Later in the \nchapter, we will cover more interesting cases.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1389,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "Chapter 10\n[ 355 ]\n    assert parameter_tdd(-1) == 0\ndef test_zero():\n    assert parameter_tdd(0) == 0\ndef test_five():\n    assert parameter_tdd(5) == 25\ndef test_seven():\n    # Note this test is deliberatly set to fail\n    assert parameter_tdd(7) == 0\ndef test_ten():\n    assert parameter_tdd(10) == 100\ndef test_eleven():\n    assert parameter_tdd(11) == 100\nIf you compare it with the equivalent code in test_unittest_example.py, the code \nis significantly leaner. When running it with pytest, it also shows more detailed, \ncolored information.\n$ pytest test_unittest_example.py\n================= test session starts =================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\ncollected 6 items\ntest_unittest_example.py ...F..                 [100%]\n====================== FAILURES =======================\n______________ TestTDDExample.test_seven ______________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\n    def test_seven(self):\n>       self.assertEqual(parameter_tdd(7), 0)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1135,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "Testing and TDD\n[ 356 ]\nE       AssertionError: 49 != 0\ntest_unittest_example.py:17: AssertionError\n=============== short test summary info ===============\nFAILED test_unittest_example.py::TestTDDExample::test_seven\n============= 1 failed, 5 passed in 0.10s =============\nAs with unittest, we can see more information with -v and run a selection of tests \nwith -k.\n$ pytest -v test_unittest_example.py\n========================= test session starts =========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncachedir: .pytest_cache\ncollected 6 items\ntest_unittest_example.py::TestTDDExample::test_eleven PASSED      [16%]\ntest_unittest_example.py::TestTDDExample::test_five PASSED        [33%]\ntest_unittest_example.py::TestTDDExample::test_negative PASSED    [50%]\ntest_unittest_example.py::TestTDDExample::test_seven FAILED       [66%]\ntest_unittest_example.py::TestTDDExample::test_ten PASSED         [83%]\ntest_unittest_example.py::TestTDDExample::test_zero PASSED        [100%]\n============================== FAILURES ===============================\n______________________ TestTDDExample.test_seven ______________________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\n    def test_seven(self):\n>       self.assertEqual(parameter_tdd(7), 0)\nE       AssertionError: 49 != 0\ntest_unittest_example.py:17: AssertionError\n======================= short test summary info =======================\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \nAssertionErr...\n===================== 1 failed, 5 passed in 0.08s =====================\n$ pytest test_pytest_example.py -v -k test_ten\n========================= test session starts =========================\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "Chapter 10\n[ 357 ]\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncachedir: .pytest_cache\ncollected 6 items / 5 deselected / 1 selected\ntest_pytest_example.py::test_ten PASSED                           [100%]\n=================== 1 passed, 5 deselected in 0.02s ===================\nAnd it's totally compatible with unittest defined tests, which allows you to combine \nboth styles or migrate them.\n$ pytest test_unittest_example.py\n========================= test session starts =========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\ncollected 6 items\ntest_unittest_example.py ...F..                                   [100%]\n============================== FAILURES ===============================\n______________________ TestTDDExample.test_seven ______________________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\n    def test_seven(self):\n>       self.assertEqual(parameter_tdd(7), 0)\nE       AssertionError: 49 != 0\ntest_unittest_example.py:17: AssertionError\n======================= short test summary info =======================\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \nAssertionErr...\n===================== 1 failed, 5 passed in 0.08s =====================\nAnother great feature of pytest is easy autodiscovery to find files that start with \ntest_ and run inside all the tests. If we try it, pointing at the current directory, we \ncan see it runs both test_unittest_example.py and test_pytest_example.py.\n$ pytest .\n========================= test session starts =========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\ncollected 12 items\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "Testing and TDD\n[ 358 ]\ntest_pytest_example.py ...F..                                    [50%]\ntest_unittest_example.py ...F..                                  [100%]\n============================== FAILURES ===============================\n_____________________________ test_seven ______________________________\n    def test_seven():\n        # Note this test is deliberatly set to fail\n>       assert parameter_tdd(7) == 0\nE       assert 49 == 0\nE        +  where 49 = parameter_tdd(7)\ntest_pytest_example.py:18: AssertionError\n______________________ TestTDDExample.test_seven ______________________\nself = <test_unittest_example.TestTDDExample testMethod=test_seven>\n    def test_seven(self):\n>       self.assertEqual(parameter_tdd(7), 0)\nE       AssertionError: 49 != 0\ntest_unittest_example.py:17: AssertionError\n======================= short test summary info =======================\nFAILED test_pytest_example.py::test_seven - assert 49 == 0\nFAILED test_unittest_example.py::TestTDDExample::test_seven - \nAssertionErr...\n==================== 2 failed, 10 passed in 0.23s =====================\nWe will continue talking about more features of pytest during the chapter, but first, \nwe need to go back to how to define tests when the code has dependencies.\nTesting external dependencies\nWhen building unit tests, we talked about how it's based around the concept of \nisolating a unit in the code to test it independently.\nThis isolation concept is key, as we want to focus on small sections of the code to \ncreate small, clear tests. Creating small tests also helps in keeping the tests fast.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1695,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "Chapter 10\n[ 359 ]\nIn our example above, we tested a purely functional function, parameter_tdd, that \nhad no dependencies. It was not using any external library or any other function. But \ninevitably, at some point, you'll need to test something that depends on something \nelse.\nThe question in this case is should the other component be part of the test or not?\nThis is not an easy question to answer. Some developers think that all unit tests \nshould be purely about a single function or method, and therefore, any dependency \nshould not be part of the test. But, on a more practical level, there are sometimes \npieces of code that form a unit that it's easier to test in conjunction than separately.\nFor example, think about a function that:\n•\t\nFor values lower than 0, returns zero.\n•\t\nFor values greater than 100, returns 10.\n•\t\nFor values between, it returns the square root of the value. Note that for the \nedges, it returns the square root of them (0 for 0 and 10 for 100).\nThis is very similar to the previous function, parameter_tdd, but this time we need \nthe help of an external library to produce the square root of a number. Let's take a \nlook at the code.\nIt's divided into two files. dependent.py contains the definition of the function.\nimport math\ndef parameter_dependent(value):\n    if value < 0:\n        return 0\n    if value <= 100:\n        return math.sqrt(value)\n    return 10\nThe code is pretty similar to the code in the parameter_tdd example. The module \nmath.sqrt returns the square root of a number.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "Testing and TDD\n[ 360 ]\nAnd the tests are in test_dependent.py.\nfrom dependent import parameter_dependent\ndef test_negative():\n    assert parameter_dependent(-1) == 0\ndef test_zero():\n    assert parameter_dependent(0) == 0\ndef test_twenty_five():\n    assert parameter_dependent(25) == 5\ndef test_hundred():\n    assert parameter_dependent(100) == 10\ndef test_hundred_and_one():\n    assert parameter_dependent(101) == 10\nIn this case, we are completely using the external library and testing it at the same \ntime that we are testing our code. For this simple example, this is a perfectly valid \noption, though that may not be the case for other cases.\nFor example, the external dependency could be making external HTTP calls that \nneed to be captured to prevent making them while running tests and to have control \nover the returned values, or other big pieces of functionality that should be tested in \nisolation.\nTo detach a function from its dependencies, there are two different approaches. We \nwill show them using parameter_dependent as a baseline.\nThe code is available in GitHub at https://github.com/\nPacktPublishing/Python-Architecture-Patterns/tree/\nmain/chapter_10_testing_and_tdd.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1293,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "Chapter 10\n[ 361 ]\nWe will see next how to mock the external calls.\nMocking\nMocking is a practice that internally replaces the dependencies, replacing them with \nfake calls, under the control of the test itself. This way, we can introduce a known \nresponse for any external dependency, and not call the actual code.\nTo be able to mock the code, in our test code, we need to prepare the mock as part of \nthe Arrange step. There are different libraries to mock calls, but the easiest is to use \nthe unittest.mock library included as part of the standard library.\nThe easiest usage of mock is to patch an external library:\nfrom unittest.mock import patch\nfrom dependent import parameter_dependent\n@patch('math.sqrt')\ndef test_twenty_five(mock_sqrt):\n    mock_sqrt.return_value = 5\n    assert parameter_dependent(25) == 5\n    mock_sqrt.assert_called_once_with(25)\nAgain, in this case, the tests work perfectly fine with the \ndependency included, as it's simple and doesn't produce side \neffects like external calls, etc.\nInternally, mocking is implemented using what is known as \nmonkey-patching, which is the dynamic replacement of existing \nlibraries with alternatives. While this can be achieved in different \nways in different programming languages, it's especially popular \nin dynamic languages like Python or Ruby. Monkey-patching can \nbe used for other purposes than testing, though it should be used \nwith care, as it can change the behavior of libraries and can be \nquite disconcerting for debugging.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "Testing and TDD\n[ 362 ]\nThe patch decorator intercepts the calls to the defined library, math.sqrt, and \nreplaces it with a mock object that passes to the function, here called mock_sqrt.\nThis object is a bit special. It basically allows any calls, accesses almost any method or \nattributes (except predefined ones), and keeps returning a mock object. This makes \nthe mock object something really flexible that will adapt to whatever code surrounds \nit. When necessary, the returning value can be set calling .return_value, as we show \nin the first line.\nWe are, in essence, saying that calls to mock_sqrt will return the value 5. So, we are \npreparing the output of the external call, so we can control it.\nFinally, we check that we called the mock mock_sqrt once, with the input (25) using \nthe method assert_called_once_with.\nIn essence, we are:\n•\t\nPreparing the mock so it replaces math.sqrt\n•\t\nSetting the value that it will return when called\n•\t\nChecking that the call works as expected\n•\t\nDouble-checking that the mock was called with the right value\nFor other tests, for example, we can check that the mock was not called, indicating \nthat the external dependence wasn't called.\n@patch('math.sqrt')\ndef test_hundred_and_one(mock_sqrt):\n    assert parameter_dependent(101) == 10\n    mock_sqrt.assert_not_called()\nThere are multiple assert functions that allow you to detect how the mock has been \nused. Some examples:\n•\t\nThe called attribute returning True or False based on whether the mock has \nbeen called or not, allowing you to write:\nassert mock_sqrt.called is True\n•\t\nThe call_count attribute returning the number of times a mock has been \ncalled.\n•\t\nThe assert_called_with() method to check the number of times that it has \nbeen called. It will raise an exception if the last call is not produced in the \nspecified way.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1936,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "Chapter 10\n[ 363 ]\n•\t\nThe assert_any_call() method to check whether any of the calls have been \nproduced in the specified way.\nWith that information, the full file for testing, test_dependent_mocked_test.py, will \nbe like this.\nfrom unittest.mock import patch\nfrom dependent import parameter_dependent\n@patch('math.sqrt')\ndef test_negative(mock_sqrt):\n    assert parameter_dependent(-1) == 0\n    mock_sqrt.assert_not_called()\n@patch('math.sqrt')\ndef test_zero(mock_sqrt):\n    mock_sqrt.return_value = 0\n    assert parameter_dependent(0) == 0\n    mock_sqrt.assert_called_once_with(0)\n@patch('math.sqrt')\ndef test_twenty_five(mock_sqrt):\n    mock_sqrt.return_value = 5\n    assert parameter_dependent(25) == 5\n    mock_sqrt.assert_called_with(25)\n@patch('math.sqrt')\ndef test_hundred(mock_sqrt):\n    mock_sqrt.return_value = 10\n    assert parameter_dependent(100) == 10\n    mock_sqrt.assert_called_with(100)\n@patch('math.sqrt')\ndef test_hundred_and_one(mock_sqrt):\n    assert parameter_dependent(101) == 10\n    mock_sqrt.assert_not_called()\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1139,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "Testing and TDD\n[ 364 ]\nIf the mock needs to return different values, you can define the side_effect attribute \nof the mock as a list or tuple. side_effect is similar to return_value, but it has a few \ndifferences, as we'll see.\n@patch('math.sqrt')\ndef test_multiple_returns_mock(mock_sqrt):\n    mock_sqrt.side_effect = (5, 10)\n    assert parameter_dependent(25) == 5\n    assert parameter_dependent(100) == 10\nside_effect can also be used to produce an exception, if needed.\nimport pytest\nfrom unittest.mock import patch\nfrom dependent import parameter_dependent\n@patch('math.sqrt')\ndef test_exception_raised_mock(mock_sqrt):\n    mock_sqrt.side_effect = ValueError('Error on the external library')\n    with pytest.raises(ValueError):\n        parameter_dependent(25)\nThe with section asserts that the expected Exception is raised in the block. If not, it \nshows an error.\nMocking is not the only way to handle dependencies for tests. We will see a different \napproach next.\nDependency injection\nWhile mocking replaces the dependency without the original code noticing, by \npatching it externally, dependency injection is a technique to make that dependency \nexplicit when calling the function under test, so it can be replaced with a testing \nsubstitute.\nIn unittest, checking a raised exception can be done with a \nsimilar with block.\nwith self.assertRaises(ValueError):\n    parameter_dependent(25)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "Chapter 10\n[ 365 ]\nIn essence, it's a way of designing the code that makes dependencies explicit by \nrequiring them as input parameters.\nLet's see how this changes the code under test.\ndef parameter_dependent(value, sqrt_func):\n    if value < 0:\n        return 0\n    if value <= 100:\n        return sqrt_func(value)\n    return 10\nNotice how now the sqrt function is an input parameter. \nIf we want to use the parameter_dependent function in a normal scenario, we will \nhave to produce the dependency, for example.\nimport math\ndef test_good_dependency():\n    assert parameter_dependent(25, math.sqrt) == 5\nAnd if we want to perform tests, we can do it by replacing the math.sqrt function \nwith a specific function, and then using it. For example:\ndef test_twenty_five():\n    def good_dependency(number):\n        return 5\n    assert parameter_dependent(25, good_dependency) == 5\nDependency injection, while useful for testing, is not only aimed \nat that. By adding the dependencies explicitly, it also reduces \nthe need for a function to know how to initialize a particular \ndependency, instead relying on the interface of the dependency. \nIt creates a separation between \"initializing\" a dependency (which \nshould be taken care of externally) and \"using\" it (which is the only \npart that the dependent code will do). This differentiation will \nbecome clearer later when we see an OOP example.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "Testing and TDD\n[ 366 ]\nWe can also provoke an error if calling the dependency to ensure that in some tests \nthe dependency is not used, for example.\ndef test_negative():\n    def bad_dependency(number):\n        raise Exception('Function called')\n    assert parameter_dependent(-1, bad_dependency) == 0\nNote how this approach is more explicit than mocking. The code to test becomes, in \nessence, totally functional as it doesn't have external dependencies.\nDependency injection in OOP\nDependency injection can also be used with OOP. In this case, we can start with code \nthat is like this.\nclass Writer:\n    def __init__(self):\n        self.path = settings.WRITER_PATH\n    def write(self, filename, data):\n        with open(self.path + filename, 'w') as fp:\n            fp.write(data)\nclass Model:\n    def __init__(self, data):\n        self.data = data\n        self.filename = settings.MODEL_FILE\n        self.writer = Writer()\n    def save(self):\n        self.writer.write(self.filename, self.data)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1100,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "Chapter 10\n[ 367 ]\nAs we can see, the settings class stores different elements that are required on \nwhere the data will be stored. The model receives some data and then saves it. The \ncode in operation will require minimal initialization.\n    model = Model('test')\n    model.save()\nThe model receives some data and then saves it. The code in operation requires \nminimal initialization, but at the same time, it's not explicit.\nTo use dependency injection principles, the code will need to be written in this way:\nclass WriterInjection:\n    def __init__(self, path):\n        self.path = path\n    def write(self, filename, data):\n        with open(self.path + filename, 'w') as fp:\n            fp.write(data)\nclass ModelInjection:\n    def __init__(self, data, filename, writer):\n        self.data = data\n        self.filename = filename\n        self.writer = writer\n    def save(self):\n        self.writer.write(self.filename, self.data)\nIn this case, every value that is a dependency is provided explicitly. In the definition \nof the code, the settings module is not present anywhere, but instead, that will \nbe specified when the class is instantiated. The code will now need to define the \nconfiguration directly.\n    writer = WriterInjection('./')\n    model = ModelInjection('test', 'model_injection.txt', writer)\n    model.save()\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1435,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "Testing and TDD\n[ 368 ]\nWe can compare how to test both cases, as seen in the file test_dependency_\ninjection_test.py. The first test is mocking, as we saw before, the write method of \nthe Writer class to assert that it has been called correctly.\n@patch('class_injection.Writer.write')\ndef test_model(mock_write):\n    model = Model('test_model')\n    model.save()\n    mock_write.assert_called_with('model.txt', 'test_model')\nCompared to that, the dependency injection example doesn't require a mock through \nmonkey-patching. It just creates its own Writer that simulates the interface.\ndef test_modelinjection():\n    EXPECTED_DATA = 'test_modelinjection'\n    EXPECTED_FILENAME = 'model_injection.txt'\n    class MockWriter:\n        def write(self, filename, data):\n            self.filename = filename\n            self.data = data\n    writer = MockWriter()\n    model = ModelInjection(EXPECTED_DATA, EXPECTED_FILENAME,\n                           writer)\n    model.save()\n    assert writer.data == EXPECTED_DATA\n    assert writer.filename == EXPECTED_FILENAME\nThis second style is more verbose, but it shows some of the differences when writing \ncode in this way:\n•\t\nNo monkey-patching mock is required. Monkey-patching can be quite \nfragile, as it's meddling with internal code that's not supposed to be exposed. \nWhile in testing this interference is not the same as doing it for regular code \nrunning, it's still something that can be messy and have unintended effects, \nespecially if the internal code changes in some unforeseen way. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "Chapter 10\n[ 369 ]\nKeep in mind that mocks will likely involve, at some point, relating to \nsecond-level dependencies, which can start having strange or complicated \neffects requiring you to spend time handling that extra complexity.\n•\t\nThe way of writing the code is different in itself. Code produced with \ndependency injection is, as we've seen, more modular and composed of \nsmaller elements. This tends to create smaller and more combinable modules \nthat play along together, with fewer unknown dependencies, as they are \nalways explicit.\n•\t\nBe careful, though, as this requires a certain amount of discipline and mental \nframing to produce truly loosely coupled modules. If this is not considered \nwhen designing the interfaces, the resulting code will instead be artificially \ndivided, resulting in tightly coupled code across different modules. \nDeveloping this discipline requires certain training; do not expect it to come \nnaturally to all developers.\n•\t\nThe code can sometimes be more difficult to debug, as the configuration \nwill be separated from the rest of the code, sometimes making it difficult \nto understand the flow of the code. The complexity can be produced at the \ninteraction of classes, which may be more difficult to understand and test. \nTypically, the upfront effort to develop code in this style is a bit greater as \nwell.\nDependency injection is a very popular technique in certain software circles and \nprogramming languages. Mocking is more difficult in less dynamic languages than \nPython, and also different programming languages have their own sets of ideas on \nhow to structure code. For example, dependency injection is very popular in Java, \nwhere there are specific tools to work in this style.\nAdvanced pytest\nWhile we've described the basic functionalities for pytest, we barely scratched the \nsurface in terms of the number of possibilities that it presents to help generate testing \ncode.\nWithout being exhaustive, we will see some useful possibilities of the tool.\nPytest is a big and comprehensive tool. It is worth learning how to \nuse it. Here, we will only scratch the surface. Be sure to check the \nofficial documentation at https://docs.pytest.org/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2303,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "Testing and TDD\n[ 370 ]\nGrouping tests\nSometimes it is useful to group tests together so they are related to specific things, \nlike modules, or to run them in unison. The simplest way of grouping tests together \nis to join them into a single class.\nFor example, going back to the test examples before, we could structure tests into \ntwo classes, as we see in test_group_classes.py.\nfrom tdd_example import parameter_tdd\nclass TestEdgesCases():\n    def test_negative(self):\n        assert parameter_tdd(-1) == 0\n    def test_zero(self):\n        assert parameter_tdd(0) == 0\n    def test_ten(self):\n        assert parameter_tdd(10) == 100\n    def test_eleven(self):\n        assert parameter_tdd(11) == 100\nclass TestRegularCases():\n    def test_five(self):\n        assert parameter_tdd(5) == 25\n    def test_seven(self):\n        assert parameter_tdd(7) == 49\nThis is an easy way to divide tests and allows you to run them independently:\n$ pytest -v test_group_classes.py\n======================== test session starts =========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncollected 6 items\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "Chapter 10\n[ 371 ]\ntest_group_classes.py::TestEdgesCases::test_negative PASSED      [16%]\ntest_group_classes.py::TestEdgesCases::test_zero PASSED          [33%]\ntest_group_classes.py::TestEdgesCases::test_ten PASSED           [50%]\ntest_group_classes.py::TestEdgesCases::test_eleven PASSED        [66%]\ntest_group_classes.py::TestRegularCases::test_five PASSED        [83%]\ntest_group_classes.py::TestRegularCases::test_seven PASSED       [100%]\n========================= 6 passed in 0.02s ==========================\n$ pytest -k TestRegularCases -v test_group_classes.py\n========================= test session starts ========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncollected 6 items / 4 deselected / 2 selected\ntest_group_classes.py::TestRegularCases::test_five PASSED        [50%]\ntest_group_classes.py::TestRegularCases::test_seven PASSED       [100%]\n================== 2 passed, 4 deselected in 0.02s ===================\n$ pytest -v test_group_classes.py::TestRegularCases\n========================= test session starts ========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncachedir: .pytest_cache\nrootdir: /Users/jaime/Dropbox/Packt/architecture_book/chapter_09_\ntesting_and_tdd/advanced_pytest\nplugins: celery-4.4.7\ncollected 2 items\ntest_group_classes.py::TestRegularCases::test_five PASSED        [50%]\ntest_group_classes.py::TestRegularCases::test_seven PASSED       [100%]\n========================== 2 passed in 0.02s =========================\nAnother possibility is to use markers. Markers are indicators that can be added \nthrough a decorator in the tests, for example, in test_markers.py.\nimport pytest\nfrom tdd_example import parameter_tdd\n@pytest.mark.edge\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1952,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "Testing and TDD\n[ 372 ]\ndef test_negative():\n    assert parameter_tdd(-1) == 0\n@pytest.mark.edge\ndef test_zero():\n    assert parameter_tdd(0) == 0\ndef test_five():\n    assert parameter_tdd(5) == 25\ndef test_seven():\n    assert parameter_tdd(7) == 49\n@pytest.mark.edge\ndef test_ten():\n    assert parameter_tdd(10) == 100\n@pytest.mark.edge\ndef test_eleven():\n    assert parameter_tdd(11) == 100 \nSee that we are defining a decorator, @pytest.mark.edge, on all the tests that checks \nthe edge of the values. \nIf we execute the tests, we can use the parameter -m to run only the ones with a \ncertain tag.\n $ pytest -m edge -v test_markers.py\n========================= test session starts ========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncollected 6 items / 2 deselected / 4 selected\ntest_markers.py::test_negative PASSED                            [25%]\ntest_markers.py::test_zero PASSED                                [50%]\ntest_markers.py::test_ten PASSED                                 [75%]\ntest_markers.py::test_eleven PASSED                              [100%]\n========================== warnings summary ==========================\ntest_markers.py:5\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1346,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "Chapter 10\n[ 373 ]\n  test_markers.py:5: PytestUnknownMarkWarning: Unknown pytest.mark.edge \n- is this a typo?  You can register custom marks to avoid this warning \n- for details, see https://docs.pytest.org/en/stable/mark.html\n    @pytest.mark.edge\ntest_markers.py:10\n...\n-- Docs: https://docs.pytest.org/en/stable/warnings.html\n============ 4 passed, 2 deselected, 4 warnings in 0.02s =============\nThe warning PytestUnknownMarkWarning: Unknown pytest.mark.edge is produced if \nthe marker edge is not registered. \nThis is very useful for finding typos, like accidentally writing egde or similar. To \navoid this warning, you'll need to add a pytest.ini config file with the definition of \nthe markers, like this.\n[pytest]\nmarkers =\n       edge: tests related to edges in intervals\nNow, running the tests shows no warning.\n$ pytest -m edge -v test_markers.py\n========================= test session starts =========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncachedir: .pytest_cache\nrootdir: /Users/jaime/Dropbox/Packt/architecture_book/chapter_09_\ntesting_and_tdd/advanced_pytest, configfile: pytest.ini\nplugins: celery-4.4.7\ncollected 6 items / 2 deselected / 4 selected\ntest_markers.py::test_negative PASSED                            [25%]\ntest_markers.py::test_zero PASSED                                [50%]\nBe aware that the GitHub code includes the pytest.ini code. \nYou won't see the warning if the pytest.ini file is present, for \nexample, if you clone the whole repo.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1668,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "Testing and TDD\n[ 374 ]\ntest_markers.py::test_ten PASSED                                 [75%]\ntest_markers.py::test_eleven PASSED                              [100%]\n=================== 4 passed, 2 deselected in 0.02s ===================\nNote that markers can be used across the full test suite, including multiple files. That \nallows for making markers to identify common patterns across the tests, for example, \ncreating a quick test suite with the most important tests to run with the marker \nbasic.\nThere are also some predefined markers with some built-in features. The most \ncommon ones are skip (which will skip the test) and xfail (which will reverse the \ntest, meaning that it expects it to fail).\nUsing fixtures\nThe use of fixtures is the preferred way to set up tests in pytest. A fixture, in essence, \nis a context created to set up a test.\nFixtures are used as input for the test functions, so they can be set up and create \nspecific environments for the test to be created.\nFor example, let's take a look at a simple function that counts the number of \noccurrences of a character in a string.\ndef count_characters(char_to_count, string_to_count):\n    number = 0\n    for char in string_to_count:\n        if char == char_to_count:\n            number += 1\n    return number\nThat's a pretty simple loop that iterates through the string and counts the matching \ncharacters.\nThis is equivalent to using the function .count() for a string, \nbut this is included to present a working function. It could be \nrefactored afterward!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1637,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "Chapter 10\n[ 375 ]\nA regular test to cover the functionalities could be as follows.\ndef test_counting():\n    assert count_characters('a', 'Barbara Ann') == 3\nPretty straightforward. Now let's see how we can define a fixture to define a setup, in \ncase we want to replicate it.\nimport pytest\n@pytest.fixture()\ndef prepare_string():\n    # Setup the values to return\n    prepared_string = 'Ba, ba, ba, Barbara Ann'\n    # Return the value\n    yield prepared_string\n    # Teardown any value\n    del prepared_string\nFirst of all, the fixture is decorated with pytest.fixture to mark it as such. A fixture \nis divided into three steps:\n•\t\nSetup: Here, we simply defined a string, but this will probably be the biggest \npart, where the values are prepared.\n•\t\nReturn the value: If we use the yield functionality, we will be able to go to \nthe next step; if not, the fixture will finish here.\n•\t\nTeardown and clean up values: Here, we simply delete the variable as an \nexample, though this will happen automatically later.\nLater, we will see a more complex fixture. Here, we are just \npresenting the concept.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1201,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "Testing and TDD\n[ 376 ]\nDefining the fixture this way will allow us to reuse it easily in different test functions, \njust using the name as the input parameter.\ndef test_counting_fixture(prepare_string):\n    assert count_characters('a', prepare_string) == 6\ndef test_counting_fixture2(prepare_string):\n    assert count_characters('r', prepare_string) == 2\nNote how the prepare_string parameter is automatically providing the value that \nwe defined with  yield. If we run the tests, we can see the effect. Even more, we can \nuse the parameter --setup-show to see the setup and tear down all of the fixtures.\n$ pytest -v test_fixtures.py -k counting_fixture --setup-show\n======================== test session starts ========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\nplugins: celery-4.4.7\ncollected 3 items / 1 deselected / 2 selected\ntest_fixtures.py::test_counting_fixture\n        SETUP    F prepare_string\n        test_fixtures.py::test_counting_fixture (fixtures used: \nprepare_string)PASSED\n        TEARDOWN F prepare_string\ntest_fixtures.py::test_counting_fixture2\n        SETUP    F prepare_string\n        test_fixtures.py::test_counting_fixture2 (fixtures used: \nprepare_string)PASSED\n        TEARDOWN F prepare_string\n=================== 2 passed, 1 deselected in 0.02s ===================\nThis fixture was very simple and did not do anything that couldn't be done defining \nthe string, but fixtures can be used to connect to a database or prepare files, taking \ninto account that they can clean them up at the end. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1715,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "Chapter 10\n[ 377 ]\nFor example, complicating the same example a bit, instead of counting from a string, \nit should count from a file, so the function needs to open a file, read it, and count the \ncharacters. The function will be like this.\ndef count_characters_from_file(char_to_count, file_to_count):\n    '''\n    Open a file and count the characters in the text contained\n    in the file\n    '''\n    number = 0\n    with open(file_to_count) as fp:\n        for line in fp:\n            for char in line:\n                if char == char_to_count:\n                    number += 1\n    return number\nThe fixture should then create a file, return it, and then remove it as part of the \nteardown. Let's take a look at it.\nimport os\nimport time\nimport pytest\n@pytest.fixture()\ndef prepare_file():\n    data = [\n        'Ba, ba, ba, Barbara Ann',\n        'Ba, ba, ba, Barbara Ann',\n        'Barbara Ann',\n        'take my hand',\n    ]\n    filename = f'./test_file_{time.time()}.txt'\n    # Setup the values to return\n    with open(filename, 'w') as fp:\n        for line in data:\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1168,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "Testing and TDD\n[ 378 ]\n            fp.write(line)\n    # Return the value\n    yield filename\n    # Delete the file as teardown\n    os.remove(filename)\nNote that in the filename, we define the name adding the timestamp when it's \ngenerated. This means that each of the files that will be generated by this fixture will \nbe unique.\n   filename = f'./test_file_{time.time()}.txt'\nThe file then gets created and the data is written.\n    with open(filename, 'w') as fp:\n        for line in data:\n            fp.write(line)\nThe name of the file, which, as we've seen, is unique, gets yielded. Finally, the file is \ndeleted in the teardown.\nThe tests are similar to the previous ones, as most of the complexity is stored in the \nfixture.\ndef test_counting_fixture(prepare_file):\n    assert count_characters_from_file('a', prepare_file) == 17\ndef test_counting_fixture2(prepare_file):\n    assert count_characters_from_file('r', prepare_file) == 6\nWhen running it, we see it works as expected, and we can check that the teardown \nstep deletes the testing files after each test.\n$ pytest -v test_fixtures2.py\n========================= test session starts =========================\nplatform darwin -- Python 3.9.5, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 \n-- /usr/local/opt/python@3.9/bin/python3.9\ncollected 2 items\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1406,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "Chapter 10\n[ 379 ]\ntest_fixtures2.py::test_counting_fixture PASSED                  [50%]\ntest_fixtures2.py::test_counting_fixture2 PASSED                 [100%]\n========================== 2 passed in 0.02s ==========================\nFixtures don't need to be defined in the same file. They can also be stored in a special \nfile called conftest.py, which will automatically be shared by pytest across all the \ntests.\nIn this chapter, we only scratched the surface in terms of the possibilities of pytest. It \nis a fantastic tool and one that I encourage you to learn about. It will pay off greatly \nto efficiently run tests and design them in the best possible way. Testing is a critical \npart of a project and it's one of the development stages where developers spend most \nof their time.\nSummary\nIn this chapter, we went through the whys and hows of tests to describe how a good \ntesting strategy is required to produce high-quality software and prevent problems \nonce the code is in use by customers.\nWe started by describing the general principles behind testing, how to make tests \nthat provide more value than their cost, and the different levels of testing to ensure \nthis. We saw the three main levels of tests, which we called unit tests (parts of a \nsingle component), system tests (the whole system), and integration tests in the \nmiddle (a whole component or several components, but not all).\nFixtures can also be combined, they can be set to be used \nautomatically, and there are already built-in fixtures to work with \ntemporal data and directories or capture output. There are also a \nlot of plugins for useful fixtures in PyPI, installable as third-party \nmodules, covering functionality like connecting to databases \nor interacting with other external resources. Be sure to check \nthe Pytest documentation and to search before implementing \nyour own fixture to see if you can leverage an already existing \nmodule: https://docs.pytest.org/en/latest/explanation/\nfixtures.html#about-fixtures.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "Testing and TDD\n[ 380 ]\nWe continued by describing different strategies to ensure that our tests are great \nones, and how to structure them using the Arrange-Act-Assert pattern, for ease of \nwriting and understanding them after they are written.\nLater, we described in detail the principles behind Test-Driven Development, a \ntechnique that puts tests at the center of development, which mandates writing \nthe tests before the code, working in small increments, and running the tests over \nand over to create a good test suite that protects against unexpected behavior. We \nalso analyzed the limits and caveats of working in a TDD fashion and provided an \nexample of what the flow looks like.\nWe continued by presenting ways of creating unit tests in Python, both using the \nstandard unittest module and by introducing the more powerful pytest. We also \npresented a section with advanced usage of pytest to show a bit of what this great \nthird-party module is capable of.\nWe described how to test external dependencies, something that is critically \nimportant when writing unit tests to isolate functionality. We also described how to \nmock dependencies and how to work under the dependency injection principles.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1473,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "[ 381 ]\n11\nPackage Management\nWhen working in complex systems, especially in microservices or similar \narchitectures, there is sometimes a need to share code so it's available at different, \nunconnected parts of the system. That's normally code that will help to abstract some \nfunctions, which can vary greatly, from security purposes (for example, calculating \na signature in a way that's understood by other systems that will have to verify it), \nto connecting to databases or external APIs, or even helping to monitor the system \nconsistently.\nInstead of reinventing the wheel each time, we can reuse the same code multiple \ntimes to be certain that it's properly tested and validated, and consistent throughout \nthe entire system. Some modules may be interesting to share not only across the \norganization but even outside it, creating a standard module others can take \nadvantage of.\nOthers have done that before, and a lot of common use cases, such as connecting to \nexisting databases, using network resources, accessing OS features, understanding \nfiles in all kinds of formats, calculating common algorithms and formulas, in all \nkinds of domains, creating and operating AI models, and a long list of other cases \nbesides, are available.\nTo enhance the sharing and utilization of all those abilities, modern programming \nlanguages have their own ways of creating and sharing packages, so the usefulness \nof the language multiplies greatly.\nIn this chapter, we will discuss the use of packages, mostly from a Python \nperspective, covering when and how to decide to create a package. We will explore \nthe different options available, from a simple structure to packages that include code \ncompiled so that it can be optimized for specific tasks.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "Package Management\n[ 382 ]\nIn this chapter, we'll cover the following topics:\n•\t\nThe creation of a new package\n•\t\nTrivial packaging in Python\n•\t\nThe Python packaging ecosystem\n•\t\nCreating a package\n•\t\nCython\n•\t\nPython package with binary code\n•\t\nUploading your package to PyPI\n•\t\nCreating your own private index\nLet's start by defining what code could be a candidate to create a package.\nThe creation of a new package\nIn any software, there will be snippets of code that could be shared across different \nparts of the code. When working with small, monolithic applications, this can be as \neasy as creating some internal modules or functions that can share functionality by \ncalling it directly.\nOver time, this common function or functions could be grouped together under a \nmodule to clarify that they are to be used across the application.\nThis will work fine up to a certain size. Some of the problems that can arise as the \ncode grows and becomes more complex are as follows:\n•\t\nCreate a more generic API to interact with the module, aimed at greater \nflexibility in terms of module utilization. This can involve creating a more \ndefensive style of programming, to be sure that the module is used as \nexpected and return proper errors.\nAvoid the temptation to use the name utils for a module with \ncode expected to be used in different positions. While this is very \ncommon, it is also not very descriptive and a bit lazy. How does \nsomeone know if a function is in the utils module? Instead of \nthat, try to use a descriptive name.\nIf it's not possible, divide it into submodules, so you can create \nsomething like utils.communication or utils.math to avoid \nthis effect.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1779,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "Chapter 11\n[ 383 ]\n•\t\nSpecific documentation needs to be provided for the module so that \ndevelopers who are not familiar with the module are able to use it.\n•\t\nOwnership of the module may need to be clarified and its own maintainers \nspecified. This can take the form of a stricter code review before changing the \ncode, with some developer or developers designated as the point of contact \nfor the module.\n•\t\nThe most critical one, the functionality of the module, is required to be \npresent in two or more independent services or code bases. If this happens, \ninstead of just copying/pasting the code across different code bases, it makes \nsense to create an independent module that can be imported. This could be \na deliberate option upfront, to standardize certain operations (for example, \nproduce and verify signed messages across multiple services) or it could be \nan afterthought following successful implementation of the functionality in \none code base that it could be handy to have in other services of the system. \nFor example, instrumenting the communication messages to you generates \na log. This log can be useful in other services, so, from the original service, it \ngets migrated to others.\nIn general, the module starts getting its own entity, and not only as a shared location \nfor incorporating code that is going to be shared. At that time, it starts to make sense \nto treat it as an independent library more than a module attached to a particular code \nbase.\nOnce the decision to create some code as an independent package has been taken, \nseveral aspects should be considered:\n•\t\nAs we've seen before, the most important is the ownership of the new \npackage. Packages exist in the boundaries between different teams and \ngroups, as they are used by different ones. Be sure to provide clear \nownership regarding any package to be sure that the team responsible \nfor it is reachable, both for any possible inquiries and for setting its own \nmaintenance.\n•\t\nAny new package will require time to develop new features and adjustments, \nespecially as the package is in use, probably stretching its limits as it's used \nin multiple services and in more ways. Be sure to take this into account \nand adjust the load of the team responsible accordingly. This will be very \ndependent on how mature the package is and how many new features are \nrequired.\n•\t\nIn the same way, be sure to budget time to maintain the package. Even \nif there are no new features, bugs will be detected and other general \nmaintenance, such as updating the dependencies on account of security fixes \nor compatibility with new OS versions, will need to be continued.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "Package Management\n[ 384 ]\nAll these elements should be taken into account. In general, it is advisable to create \nsome sort of roadmap where the team responsible can define what the objectives are \nand a time frame to achieve them.\nWe will focus on creating a new package in Python, but the basics are similar when \ncreating other packages in other languages.\nTrivial packaging in Python\nIn Python, it is easy to create a package to be imported by just adding a subdirectory \nto the code. While this is simple, it can be adequate initially, as the subdirectory can \nbe copied. For example, the code can be added directly to the source control system, \nor it can even be installed by compressing the code and uncompressing it in place.\nThe structure of the code for a module in Python can be worked out as a subdirectory \nwith a single entry point. For example, when creating a module called naive_package \nwith the following structure:\n└── naive_package\n    ├── __init__.py\n    ├── module.py\n    └── submodule\n        ├── __init__.py\n        └── submodule.py\nWe can see that the module contains a submodule, so let's start there. The submodule \ndirectory contains two files, the submodule.py file with the code, and an empty \n__init__.py file to allow the other file to be imported, as we will see later.\nThe bottom line is that a new package is a new project. You need to \ntreat it as such.\nThis is not a long-term solution, as it won't handle multiple \nversions, dependencies, and so on, but it can work in some cases as \na first step. At least initially, all the code to be packetized needs to \nbe stored in the same subdirectory.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "Chapter 11\n[ 385 ]\nThe content of submodule.py is this example function:\ndef subfunction():\n    return 'calling subfunction'\nThe top level is the module itself. We have the module.py file, which defines the \nsome_function function that calls the submodule:\nfrom .submodule.submodule import subfunction\ndef some_function():\n    result = subfunction()\n    return f'some function {result}'\nThe import line has a detail, a dot in the form of the submodule located in the same \ndirectory. This is specific syntax in Python 3 for being more precise when importing. \nWithout the dot, it will try to import from the library instead.\nThe rest of the function calls subfunction and combines the result to return a string \nof text.\nThe __init__.py file, in this case, is not empty, but instead, it imports the \nsome_function function:\nfrom .module import some_function\nNote again the relative import as indicated by the preceding dot. This allows having \nthe some_function function available as part of the top level of the naive_package \nmodule.\n__init__.py is a special Python file that indicates that the \ndirectory contains Python code and can be imported externally. It \nsymbolizes the directory itself, as we will see later.\nYou can learn more about relative imports in PEP-328, which \ndescribes it, here: https://www.python.org/dev/peps/pep-\n0328/. PEPs (Python Enhancement Proposals) are documents \ndescribing new features relating to the Python language or \ninformation related to the community. It is the official channel for \nproposing changes and advancing the language.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "Package Management\n[ 386 ]\nWe can now create a file to call the module. We'll write the call_naive_package.py \nfile, which needs to be at the same level as the native_package directory:\nfrom naive_package import some_function\nprint(some_function())\nThis file just calls the module-defined function and prints the result:\n$ python3 call_naive_package.py\nsome function calling subfunction\nThis method of handling a module to be shared is not recommended, but this small \nmodule can help us understand how to create a package and what the structure of a \nmodule is. The first step to detaching a module and creating an independent package \nwill be to create a single subdirectory that has a clear API, including clear entry \npoints to use it.\nBut to get a better solution, we will need to be able to create a full Python package \nfrom there. Let's take a look at what that means exactly.\nThe Python packaging ecosystem\nPython has a very active ecosystem of third-party open source packages that \ncovers a wide variety of topics and enables the power of any Python program to be \nenhanced. You can take advantage of installing them by using pip, which is installed \nautomatically for any new Python install.\nFor example, to install the package named requests, a package allowing the \ncompilation of easier and more powerful HTTP requests, the command is:\n$ pip3 install requests\npip searches in the Python Package Index automatically to see whether the package \nis available and if it is, it will download it and install it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1622,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "Chapter 11\n[ 387 ]\nWe will see more detailed usage on pip later in the chapter, but first, we need to \ndiscuss the main source where the packages are downloaded.\nPyPI\nThe Python Package Index (PyPI, normally pronounced as Pie-P-I, as opposed to Pie-\nPie) is the official source of packages in Python and can be checked at https://pypi.\norg:\nFigure 11.1: pypi.org main page\nNote that the pip command could take the form of pip3. This \ndepends on the installation of Python in your system. We will use \nthem indistinctly.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 621,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "Package Management\n[ 388 ]\nOn the PyPI web page, the search enables specific packages to be found along with \nuseful information, including available packages with partial matches. They can also \nbe filtered.\nFigure 11.2: Searching for packages\nOnce the individual package is specified, more information can be found regarding \nbrief documentation, links to the source and home page of the project, and other \nsimilar kinds of licenses or maintainers.\nThe home page and documentation page are very significant for \nbig packages, as they will include much more information about \nhow to use the package. Smaller packages will normally only \ninclude the documentation on this page, but it's always worth \nchecking their page for the source as it may link to a GitHub page \nwith details about bugs and the possibility of submitting patches or \nreports.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 951,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "Chapter 11\n[ 389 ]\nThe page for requests looks like this at the time of writing this book:\nFigure 11.3: Detailed info about a module\nSearching directly in PyPI can help locate some interesting modules, and in some \ncases, will be quite straightforward, such as finding a module to connect to a \ndatabase (for example, searching by the name of the database). This, though, \nnormally involves a significant amount of trial and error, as the name may not be \nindicative of how good a module will be for your use case.\nSpending some time on the internet searching for the best module for a use case is a \ngreat idea and it will improve the chances of finding the right package for your use \ncase.\nIn any case, given the number of available packages for Python, of varying quality \nand maturity, it's always worthwhile setting aside some time to research alternatives. \nA great source of knowledge in this case is StackOverflow \n(https://stackoverflow.com/), which contains a lot of \nquestions and answers that can be used to ascertain interesting \nmodules. A general Google search will also help.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "Package Management\n[ 390 ]\nPackages are not curated in any way by pypi.org, as it's publicly available to \nanyone to submit their packages, although malicious ones will be eliminated. How \npopular a package is will require more indirect methods, such as searching how \nmany downloads or searching through a searcher online to see whether other \nprojects are using it. Ultimately, it will require the performance of some Proof-of-\nConcept programs to analyze whether the candidate packages cover all the required \nfunctionalities.\nVirtual environments\nThe next element in the packaging chain is the creation of virtual environments to \nisolate the installation of modules.\nWhen dealing with installing packages, using the default environments in the system \nleads to the packages being installed there. This means that the general installation of \nthe Python interpreter will be affected by this.\nThis can lead to problems, as you may install packages that have side effects when \nusing the Python interpreter for other purposes, as dependencies in the packages \nmay interfere with each other.\nFor example, if the same machine has a Python program that requires the package1 \npackage and another Python program that requires package2, and they are both \nincompatible, that will create a conflict. Installing both package1 and package2 won't \nbe possible.\nThe solution to this problem is to create two different environments, so each package \nand its dependencies are stored independently – independently from each other, \nbut also independently from the system Python interpreter, so it won't affect any \npossible system activity that depends on the Python system interpreter.\nTo create a new virtual environment, you can use the standard module venv, \nincluded in all installations of Python 3 after 3.3:\n$ python3 -m venv venv\nNote that this can also happen through version incompatibility, \nespecially in the dependencies of the packages, or in the \ndependencies of dependencies. For example, package1 requires \ndependency version 5 to be installed, and package2 requires \ndependency version 6 or higher. They won't be able to run in \nconjunction with one another.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2268,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "Chapter 11\n[ 391 ]\nThis creates the venv subdirectory, which contains the virtual environment. The \nenvironment can be activated using the following source command:\n$ source ./venv/bin/activate\n(venv) $ which python\n./venv/bin/python\n(venv) $ which pip\n./venv/bin/python\nYou can see that the python interpreter and pip that get executed is the one located \nin the virtual environment, and not the system one, and also the indication in the \nprompt that the virtual environment, venv, is active.\nThe virtual environment also has its own library, so any installed packages will be \nstored here, and not in the system environment.\nOnce in the virtual environment, any call to pip will install the packages in the \nvirtual environment, so they are independent of any other environment. Each \nprogram can then be executed within its own virtual environment.\nWith a proper environment, we can use pip to install the different dependencies. \nPlease note that we have used a name for the created virtual \nenvironment, venv, which is the same as the name of the module. \nThat's not necessary. Virtual environments can be created with any \nname. Be sure to use a name that's descriptive in your use case.\nThe virtual environment can be deactivated by calling the \ndeactivate command. You can see that the (venv) indication \ndisappears.\nIn cases where the virtual environment cannot be activated directly \nthrough the command line and the command needs to be executed \ndirectly, for example, for cronjob cases, you can call the python \ninterpreter directly in the virtual environment by its full path, such \nas /path/to/venv/python/your_script.py.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1738,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "Package Management\n[ 392 ]\nPreparing an environment\nCreating a virtual environment is the first stage, but we need to install all \ndependencies for our software.\nTo be able to replicate the environment in all situations, the best is to create a \nrequirements file that defines all dependencies that should be installed. pip allows \nworking with a file, normally called requirements.txt, to install dependencies.\nThis is an excellent way of creating a replicable environment that can be started from \nscratch when necessary.\nFor example, let's take a look at the following requirements.txt file:\nrequests==2.26.0\npint==0.17\nThe file can be downloaded from GitHub at https://github.com/PacktPublishing/\nPython-Architecture-Patterns/blob/main/chapter_11_package_management/\nrequirements.txt.\nThe file can be installed in the virtual environment (remember to activate it) using \nthe following command:\n(venv) $ pip install -r requirements.txt\nAfter that, all the specified requirements will be installed in the environment.\nNote that the dependencies of your specified dependencies may not be totally \npinned down to specific versions. This is because the dependencies have their own \ndefinition, which can produce unknown upgrades on second-level dependencies \nwhen a new package is delivered.\nNote the format is package==version. This specifies the exact \nversion to use for the package, which is the recommended way \nof installing dependencies. That avoids the problem of using just \npackage, which will install the latest version, and which can lead \nto an upgrade that's not planned, which may break compatibility.\nOther options, such as package>=version, to specify a minimum \nversion are available. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1804,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "Chapter 11\n[ 393 ]\nTo avoid having that problem, you can create an initial installation with your first-\nlevel dependencies, and then obtain all the dependencies that have been installed \nwith the pip freeze command:\n(venv) $ pip freeze\ncertifi==2021.5.30\nchardet==3.0.4\ncharset-normalizer==2.0.4\nidna==2.10\npackaging==21.0\nPint==0.17\npyparsing==2.4.7\nrequests==2.26.0\nurllib3==1.26.6\nYou can use the output to update the requirements.txt directly, so the next \ninstallation will have all the second-level dependencies also pinned down.\nA note on containers\nWhen working in a container manner, the distinction between the system interpreter \nand the program interpreter is more diluted, as the container has its own OS \nwrapped, thereby enforcing a strong separation.\nIn the traditional way of deploying services, they are installed and run in the same \nserver, making it necessary to keep a separation between the interpreter due to the \nrestrictions that we talked about previously.\nBy using containers, we have already created a wrap around each of the services \ninto their own OS filesystem, which means that we can skip the creation of a virtual \nenvironment. The container acts as a virtual environment in this case, enforcing \nseparation between different containers.\nAs we've discussed previously in Chapter 8, Advanced Event-Driven Structures, \nwhen talking about containers, each container should serve only a single service, \ncoordinating different containers to generate different servers. That way, it \neliminates the case of having to share the same interpreter.\nNote that adding new requirements will require the same process \nto be generated, to install first, then run freeze, and then update \nthe requirements.txt file with the ouput.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1854,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "Package Management\n[ 394 ]\nThis means that we can ease some of the restrictions that we would normally impose \nin a traditional setting, and care just about one environment, being able to take less \ncare about polluting the system environment. There's only one environment, so \nwe can play with it more freely. If we need more services or environments, we can \nalways create more containers.\nPython packages\nA Python module ready to use is, in essence, a subdirectory with certain Python \ncode. This subdirectory gets installed in the proper library's subdirectoy, and the \ninterpreter searches in this subdirectory. The directory is called site-packages.\nTo distribute it, the subdirectory is packaged into two different files, either Egg files \nor Wheel files. Importantly, though, pip can only install Wheel files.\nEgg files are considered deprecated, as their format is older and it's basically a zipped \nfile containing some metadata. Wheel files have several advantages:\n•\t\nThey are better defined and allow for more use cases. There's a specific PEP, \nPEP-427 (https://www.python.org/dev/peps/pep-0427/), that defines the \nformat. Egg files were never officially defined.\n•\t\nThey can be defined to have better compatibility, allowing the creation \nof Wheel files that are compatible between different versions of Python, \nincluding Python 2 and Python 3.\n•\t\nWheel files can include already compiled binary code. Python allows the \ninclusion of libraries that are written in C, but these libraries need to target \nthe proper hardware architecture. In Egg files, the source files were included \nand compiled at install time, but that required the proper compilation tools \nand environment in the installation machine, and this could easily result in \ncompilation issues.\nThis subdirectory is available in the virtual environment, if you are \nusing one. You can check the following subdirectory: venv/lib/\npython3.9/site-packages/.\nSource packages can also be created. In this case, the file is a tar file \nthat contains all the code.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "Chapter 11\n[ 395 ]\n•\t\nInstead of that, Wheel files can be precompiled with binary files. The Wheel \nfile has better-defined compatibility based on hardware architecture and the \nOS, so the right Wheel file will be downloaded and installed, if available. This \nmakes the installation faster, as no compilation needs to be performed in \nthe installation, and removes the need for compilation tools available in the \ntarget machine. A Wheel file with a source file can also be created to allow its \ninstallation in machines not already precompiled, though in this case, it will \nrequire a compiler.\n•\t\nWheel files can be cryptographically signed, while Eggs don't support this \noption. That adds an extra layer to avoid compromised and modified \npackages.\nRight now, the standard for packaging in Python is Wheel files, and they should be \npreferred as a general rule. Egg files should be limited to older packages that haven't \nbeen upgraded to the new format.\nWe will see now how to create your own package.\nCreating a package\nEven if, in most cases, we will use third-party packages, at some point, it is possible \nthat you'll need to create your own package.\nTo do so, you need to create a setup.py file, which is the base of the package, \ndescribing what is inside it. Base package code will look like this:\npackage\n├── LICENSE\n├── README\n├── setup.py\n└── src\n    └─── <source code>\nEgg files can be installed with the older easy_install script, \nalthough this is no longer included in the latest versions of Python. \nCheck the documentation for setup tools on how to use easy_\ninstall: https://setuptools.readthedocs.io/en/latest/\ndeprecated/easy_install.html.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1765,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "Package Management\n[ 396 ]\nThe LICENSE and README files are not mandatory but are good to include for adding \ninformation about the package. The LICENSE file will be included automatically in the \npackage.\nThe README file is not included, but we will include its content in a full description of \nthe package as part of the build process, as we will see later.\nThe code of the process is the setup.py file. Let's take a look at an example:\nimport setuptools\nwith open('README') as readme:\n    description = readme.read()\nsetuptools.setup(\n    name='wheel-package',\n    version='0.0.1',\n    author='you',\n    author_email='me@you.com',\n    description='an example of a package',\n    url='http://site.com',\n    long_description=description,\n    classifiers=[\n        'Programming Language :: Python :: 3',\n        'Operating System :: OS Independent',\n        'License :: OSI Approved :: MIT License',\n    ],\n    package_dir={'': 'src'},\n    install_requires=[\n        'requests',\n    ],\n    packages=setuptools.find_packages(where='src'),\n    python_requires='>=3.9',\n)\nChoosing your own open source license can be difficult. You can \nuse the web (https://choosealicense.com/), which shows \ndifferent options and explains them. We will use the MIT license as \nan example.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1372,
      "extraction_method": "Direct"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "Chapter 11\n[ 397 ]\nThe setup.py file essentially contains the setuptools.setup function, which defines \nthe package. It defines the following:\n•\t\nname: The name of the package.\n•\t\nversion: The version of the package. It will be used when installing a \nparticular version or when ascertaining which is the latest version.\n•\t\nauthor and author_email: It is good to include these to receive any possible \nbug reports or requests.\n•\t\ndescription: A short description.\n•\t\nurl: The URL for the project.\n•\t\nlong_description: A longer description. Here, we are reading the README file, \nstoring the content in the description variable:\nwith open('README') as readme:\n    description = readme.read()\nAn important detail of setup.py is that it is dynamic, so we can use code to \ndetermine the values of any parameter.\n•\t\nclassifier: Categories for allowing packages to be categorized in different \nareas, such as the kinds of licenses and languages, or if the package is \nsupposed to work with a framework like Django. You can check the full list \nof classifiers at the following link: https://pypi.org/classifiers/.\n•\t\npackage_dir: The subdirectory where the code of the package is located. \nHere, we specify src. By default, it will use the same directory as setup.py, \nbut it's better to make the division so as to keep the code tidy.\n•\t\ninstall_requires: Any dependency that needs to be installed with your \npackage. Here, we are adding requests as an example. Note that any second-\norder dependencies (dependencies of requests) will be installed as well.\n•\t\npackages: Using the setuptools.find_packages function, include everything \nthat's in the src directory.\n•\t\npython_requires: Define what Python interpreters are compatible with the \npackage. In this case, we define it for Python 3.9 or higher.\nOnce the file is ready, you can run the setup.py script directly, for example, to check \nthat the data is correct:\n$ python setup.py check\nrunning check\nThis command will verify that the setup.py definition is correct and that no \nmandatory elements are missing.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "Package Management\n[ 398 ]\nDevelopment mode\nThe setup.py file can be used to install the package in develop mode. This installs the \npackage in the current environment in a linked way. This means that any changes \nto the code will be applied directly to the package after the interpreter is restarted, \nmaking it easy to change and work with tests. Remember to run it while inside the \nvirtual environment:\n(venv) $ python setup.py develop\nrunning develop\nrunning egg_info\nwriting src/wheel_package.egg-info/PKG-INFO\nwriting dependency_links to src/wheel_package.egg-info/dependency_\nlinks.txt\nwriting requirements to src/wheel_package.egg-info/requires.txt\nwriting top-level names to src/wheel_package.egg-info/top_level.txt\nreading manifest file 'src/wheel_package.egg-info/SOURCES.txt'\nadding license file 'LICENSE'\n...\nUsing venv/lib/python3.9/site-packages\nFinished processing dependencies for wheel-package==0.0.1\nThe developed version can be uninstalled easily to clean up the environment:\n(venv) $ python setup.py develop --uninstall\nrunning develop\nRemoving  /venv/lib/python3.9/site-packages/wheel-package.egg-link \n(link to src)\nRemoving wheel-package 0.0.1 from easy-install.pth file\nYou can read more about development mode in the official documentation here: \nhttps://setuptools.readthedocs.io/en/latest/userguide/development_mode.html.\nThis step installs the package directly in the current environment and can be used to \nrun tests and validate that the package is working as expected once installed. Once \nthis is done, we can prepare the package itself.\nPure Python package\nTo create a package, we first need to define what kind of package we want to create. \nAs we described before, we have three options: a source distribution, an Egg, or a \nWheel. Each one is defined by a different command in setup.py.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1926,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "Chapter 11\n[ 399 ]\nTo create a source distribution, we will use sdist (source distribution):\n$ python setup.py sdist\nrunning sdist\nrunning egg_info\nwriting src/wheel_package.egg-info/PKG-INFO\nwriting dependency_links to src/wheel_package.egg-info/dependency_\nlinks.txt\nwriting requirements to src/wheel_package.egg-info/requires.txt\nwriting top-level names to src/wheel_package.egg-info/top_level.txt\nreading manifest file 'src/wheel_package.egg-info/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'src/wheel_package.egg-info/SOURCES.txt'\nrunning check\ncreating wheel-package-0.0.1\ncreating wheel-package-0.0.1/src\ncreating wheel-package-0.0.1/src/submodule\ncreating wheel-package-0.0.1/src/wheel_package.egg-info\ncopying files to wheel-package-0.0.1...\ncopying LICENSE -> wheel-package-0.0.1\ncopying README.md -> wheel-package-0.0.1\ncopying setup.py -> wheel-package-0.0.1\ncopying src/submodule/__init__.py -> wheel-package-0.0.1/src/submodule\ncopying src/submodule/submodule.py -> wheel-package-0.0.1/src/submodule\ncopying src/wheel_package.egg-info/PKG-INFO -> wheel-package-0.0.1/src/\nwheel_package.egg-info\ncopying src/wheel_package.egg-info/SOURCES.txt -> wheel-package-0.0.1/\nsrc/wheel_package.egg-info\ncopying src/wheel_package.egg-info/dependency_links.txt -> wheel-\npackage-0.0.1/src/wheel_package.egg-info\ncopying src/wheel_package.egg-info/requires.txt -> wheel-package-0.0.1/\nsrc/wheel_package.egg-info\ncopying src/wheel_package.egg-info/top_level.txt -> wheel-\npackage-0.0.1/src/wheel_package.egg-info\nWriting wheel-package-0.0.1/setup.cfg\ncreating dist\nCreating tar archive\nremoving 'wheel-package-0.0.1' (and everything under it)\nThe dist package is available in the newly created dist subdirectory:\n$ ls dist\nwheel-package-0.0.1.tar.gz\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "Package Management\n[ 400 ]\nTo generate a proper Wheel package, we need to install the wheel module first:\n$ pip install wheel\nCollecting wheel\n  Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)\nInstalling collected packages: wheel\nSuccessfully installed wheel-0.37.0\nThis adds the bdist_wheel command to the available commands in setup.py, which \ngenerates a wheel:\n$ python setup.py bdist_wheel\nrunning bdist_wheel\nrunning build\nrunning build_py\ninstalling to build/bdist.macosx-11-x86_64/wheel\n...\nadding 'wheel_package-0.0.1.dist-info/LICENSE'\nadding 'wheel_package-0.0.1.dist-info/METADATA'\nadding 'wheel_package-0.0.1.dist-info/WHEEL'\nadding 'wheel_package-0.0.1.dist-info/top_level.txt'\nadding 'wheel_package-0.0.1.dist-info/RECORD'\nremoving build/bdist.macosx-11-x86_64/wheel\nAnd the wheel file is available, once more, in the dist subdirectory:\n$ ls dist\nwheel_package-0.0.1-py3-none-any.whl\nNote that it also includes Python version 3.\nAll these created packages can be installed directly with pip:\n$ pip install dist/wheel-package-0.0.1.tar.gz\nProcessing ./dist/wheel-package-0.0.1.tar.gz\n... \nWheel packages compatible with both Python 2 and Python 3 can \nbe used. These wheels are called Universal. That was useful while \ndoing the transition between both versions. Hopefully, by now, \nmost of the new code in Python is using version 3 and we don't \nhave to worry about that.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "Chapter 11\n[ 401 ]\nSuccessfully built wheel-package\nInstalling collected packages: wheel-package\nSuccessfully installed wheel-package-0.0.\n$ pip uninstall wheel-package\nFound existing installation: wheel-package 0.0.1\nUninstalling wheel-package-0.0.1:\n  Would remove:\n    venv/lib/python3.9/site-packages/submodule/*\n    venv/lib/python3.9/site-packages/wheel_package-0.0.1.dist-info/*\nProceed (Y/n)? y\n  Successfully uninstalled wheel-package-0.0.1\n$ pip install dist/wheel_package-0.0.1-py3-none-any.whl\nProcessing ./dist/wheel_package-0.0.1-py3-none-any.whl\nCollecting requests\n  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n...\nCollecting urllib3<1.27,>=1.21.1\n  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n...\nInstalling collected packages: wheel-package\nSuccessfully installed wheel-package-0.0.\nNote that the dependencies, in this case, requests, are installed automatically as well \nas any second-level dependency, for example, urllib3.\nThe power of the packaging is not only applicable to packages that contain only \nPython code. One of the most interesting features of wheels is the ability to generate \npre-compiled packages, which includes compiled code for a target system.\nTo be able to show that, we need to produce some Python module that contains code \nthat will be compiled. To do so, we need to take a small detour.\nCython\nPython is capable of creating C and C++ language extensions that are compiled \nand interact with the Python code. Python itself is written in C, so this is a natural \nextension.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1648,
      "extraction_method": "Direct"
    },
    {
      "page_number": 421,
      "chapter": null,
      "content": "Package Management\n[ 402 ]\nWhile Python has a lot of great features, pure speed when performing certain \noperations, such as numerical operations, is not its forte. This is where the C \nextensions come into their own as they enable low-level code to be accessed, which \ncan be optimized and run faster than Python. Don't underestimate the possibility of \ncreating a small, localized C extension that speeds up critical parts of the code.\nCreating a C extension, however, can be difficult. The interface between Python \nand C is not straightforward, and the memory management required in C may be \ndaunting unless you have significant experience of working with the C language.\nFortunately, there are some alternatives to make the task easier. A very good one is \nCython.\nCython is a tool that compiles Python code with some extensions in C, so writing a C \nextension is as simple as writing Python code. The code is annotated to describe the \nC types for variables, but other than that, it looks pretty similar. \nCython files are stored as .pyx files. Let's see an example, which will determine \nwhether a number is a prime number with the help of the wheel_package_compiled.\npyx file:\ndef check_if_prime(unsigned int number):\n    cdef int counter = 2\n    if number == 0:\n        return False\nIf you want to dive deep into the topic and create your own C/C++ \nextensions, you can start by reading the official documentation at \nhttps://docs.python.org/3/extending/index.html.\nOther options are available, such as creating extensions in Rust. \nYou can check how to do this in the following article: https://\ndevelopers.redhat.com/blog/2017/11/16/speed-python-\nusing-rust.\nA complete description of Cython and all its possibilities is beyond \nthe scope of this book. We present just a brief introduction. \nPlease check the complete documentation for more information: \nhttps://cython.org/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1989,
      "extraction_method": "Direct"
    },
    {
      "page_number": 422,
      "chapter": null,
      "content": "Chapter 11\n[ 403 ]\n    while counter < number:\n        if number % counter ==  0:\n            return False\n        counter += 1\n    return True\nThe code is checking whether a positive number is a prime number:\n•\t\nIt returns False if the input is zero.\n•\t\nIt tries to divide the number by a number from 2 to the number. If any \ndivision is exact, it returns False as the number is not a prime number.\n•\t\nIf no division is exact, or the number is lower than 2, it returns True.\nThe code is not exactly Pythonic, as it will be translated into C. It's more efficient to \navoid Python calls like range or similar. Don't be afraid to test different approaches \nto see what's faster to execute.\nOnce the pyx file is ready, it can be compiled and imported into Python, using \nCython. First, we need to install Cython:\n$ pip install cython\nCollecting cython\n  Using cached Cython-0.29.24-cp39-cp39-macosx_10_9_x86_64.whl (1.9 MB)\nInstalling collected packages: cython\nSuccessfully installed cython-0.29.24\nNow, using pyximport, we can import the module directly like a py file. Cython will \nautomatically compile it if necessary:\n>>> import pyximport\n>>> pyximport.install()\n(None, <pyximport.pyximport.PyxImporter object at 0x10684a190>)\n>>> import wheel_package_compiled\nThe code is not particularly good; it attempts too many divisions \nin general. It is just for the purpose of showing example code that \nmay make sense to be compiled and is not too complicated.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 423,
      "chapter": null,
      "content": "Package Management\n[ 404 ]\nvenv/lib/python3.9/site-packages/Cython/Compiler/Main.py:369: \nFutureWarning: Cython directive 'language_level' not set, using 2 for \nnow (Py2). This will change in a later release! File: wheel_package_\ncompiled.pyx\n  tree = Parsing.p_module(s, pxd, full_module_name)\n.pyxbld/temp.macosx-11-x86_64-3.9/pyrex/wheel_package_\ncompiled.c:1149:35: warning: comparison of integers of different signs: \n'int' and 'unsigned int' [-Wsign-compare]\n    __pyx_t_1 = ((__pyx_v_counter < __pyx_v_number) != 0);\n                  ~~~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~\n1 warning generated.\n>>> wheel_package_compiled.check_if_prime(5)\n   True\nYou can see that the compiler produces an error because there's a comparison \nbetween unsigned int and int (between counter and number). \nOnce the code is compiled, Cython creates both a wheel_package_compiled.c file, \nlocal to the directory, and the compiled .so file, which, by default, is stored in $HOME/ \n.pyxbld:\n$ ls ~/.pyxbld/lib.macosx-11-x86_64-3.9/\nwheel_package_compiled.cpython-39-darwin.so\nUsing pyximport is good for local development, but we can create a package that \ncompiles and packages it as part of the build process. \nThis has been deliberately left to clearly show when the \ncompilation takes place and that any compilation feedback, such as \nwarnings or errors, will be displayed.\nNote that this will be specific to your system. Here, we are showing \na module compiled for macOS.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1555,
      "extraction_method": "Direct"
    },
    {
      "page_number": 424,
      "chapter": null,
      "content": "Chapter 11\n[ 405 ]\nPython package with binary code\nWe will use the code we created using Cython to show how to build a package that \ncombines Python code with precompiled code. We will generate a Wheel file.\nWe create a package called wheel_package_compiled that extends the previous \nexample package, wheel_package, with the code presented to be compiled in Cython.\nThe structure of the package will be like this:\nwheel_package_compiled\n    ├── LICENSE\n    ├── README\n    ├── src\n    │   ├── __init__.py\n    │   ├── submodule\n    │   │   ├── __init__.py\n    │   │   └── submodule.py\n    │   ├── wheel_package.py\n    │   └── wheel_package_compiled.pyx\n    └── setup.py\nThis is the same as the package introduced previously, but with the addition of the \n.pyx file. The setup.py file needs to add some changes:\nimport setuptools\nfrom Cython.Build import cythonize\nfrom distutils.extension import Exteldnsion\nextensions = [\n    Extension(\"wheel_package_compiled\", [\"src/wheel_package_compiled.\npyx\"]),\n]\nThe code is available in GitHub at https://github.com/\nPacktPublishing/Python-Architecture-Patterns/tree/\nmain/chapter_11_package_management/wheel_package_\ncompiled.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1269,
      "extraction_method": "Direct"
    },
    {
      "page_number": 425,
      "chapter": null,
      "content": "Package Management\n[ 406 ]\nwith open('README') as readme:\n    description = readme.read()\nsetuptools.setup(\n    name='wheel-package-compiled',\n    version='0.0.1',\n    author='you',\n    author_email='me@you.com',\n    description='an example of a package',\n    url='http://site.com',\n    long_description=description,\n    classifiers=[\n        'Programming Language :: Python :: 3',\n        'Operating System :: OS Independent',\n        'License :: OSI Approved :: MIT License',\n    ],\n    package_dir={'': 'src'},\n    install_requires=[\n        'requests',\n    ],\n    ext_modules=cythonize(extensions),\n    packages=setuptools.find_packages(where='src'),\n    python_requires='>=3.9',\n)\nThe changes introduced, other than the name change for the package, are all related to \nthe new extension:\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nextensions = [\n    Extension(\"wheel_package_compiled\", [\"src/wheel_package_compiled.\npyx\"]),\n]\n...\next_modules=cythonize(extensions),\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 426,
      "chapter": null,
      "content": "Chapter 11\n[ 407 ]\nThe extension definition targets the name of the module to add, and the location \nof the source. With the cythonize function, we are indicating that we want to use \nCython to compile it.\nOnce this is configured, we can run the code to generate the wheel, calling setup.py:\n$ python setup.py bdist_wheel\nCompiling src/wheel_package_compiled.pyx because it changed.\n[1/1] Cythonizing src/wheel_package_compiled.pyx\n...\nrunning bdist_wheel\nrunning build\nrunning build_py\n...\ncreating 'dist/wheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_\nx86_64.whl' and adding 'build/bdist.macosx-11-x86_64/wheel' to it\nadding 'wheel_package_compiled.cpython-39-darwin.so'\nadding 'submodule/__init__.py'\nadding 'submodule/submodule.py'\nadding 'wheel_package_compiled-0.0.1.dist-info/LICENSE'\nadding 'wheel_package_compiled-0.0.1.dist-info/METADATA'\nadding 'wheel_package_compiled-0.0.1.dist-info/WHEEL'\nadding 'wheel_package_compiled-0.0.1.dist-info/top_level.txt'\nadding 'wheel_package_compiled-0.0.1.dist-info/RECORD'\nremoving build/bdist.macosx-11-x86_64/wheel\nThe compiled Wheel is available, as before, in the dist subdirectory.\n$ ls dist\nwheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\nCompared with the Wheel created previously, we can see that it adds the platform \nand hardware architecture (macOS 11 and x86 64 bits, which is the computer used to \ncompile it while writing the book). The cp39 part shows that it used the Python 3.9 \nABI (Application Binary Interface).\nExtension modules are modules compiled in C/C++. In this case, \nCython will run the intermediate steps to be sure that the proper \n.c file is the one being compiled.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1764,
      "extraction_method": "Direct"
    },
    {
      "page_number": 427,
      "chapter": null,
      "content": "Package Management\n[ 408 ]\nThe created Wheel is ready to use for the same architecture and system. The Wheel \npackage directly includes all the compiled code, so the package will install quickly, \nas only copying files is involved. Also, there will be no need to install compilation \ntools and dependencies.\nWhen working with packages that need to be installed in multiple architectures or \nsystems, you'll need to create an individual Wheel for each case and add the source \ndistribution file to allow other systems to work with it. \nBut, unless you are creating a general package to be submitted to PyPI, the package \nwill be for self-consumption, and normally you only need to create a Wheel file for \nyour specific use case.\nWhich leads to the same step. What if you want to share your module with the whole \nPython community?\nUploading your package to PyPI\nPyPI is open to accepting packages from any developer. We can create a new account \nand upload our packages to the official Python repo to allow any project to use it.\nTo help with testing and to be sure that we can verify the process, there's a testing \nsite called TestPyPI at https://test.pypi.org/ that can be used to perform tests and \nto upload your package first.\nOne of the great characteristics of open source projects, like Python \nand its ecosystem, is the ability to use code that is gracefully shared \nby other developers. While not mandatory, it is always good \nto give back and to share code that could be of interest to other \ndevelopers to increase the usefulness of the Python library. \nBe a good participant in the Python ecosystem and share code that \ncould be useful to others.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 428,
      "chapter": null,
      "content": "Chapter 11\n[ 409 ]\nFigure 11.4: TestPyPI main page\nThe site is the same as the production one but indicates with a banner that it's the \ntesting site.\nYou can register a new user at https://test.pypi.org/account/register/. After \nthat, you'll need to create a new API token to allow the package to be uploaded.\nRemember to verify your email. Without a verified email, you \nwon't be able to create an API token.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 512,
      "extraction_method": "Direct"
    },
    {
      "page_number": 429,
      "chapter": null,
      "content": "Package Management\n[ 410 ]\nIf there's a problem with the API token or you lose it, you can always delete it and \nstart again.\nFigure 11.5: You'll need to grant the full scope to upload a new package\nCreate a new token and copy it to a safe place. The token (which starts with pypi-) \nwill only be displayed once for safety reasons, so be careful with it.\nThe next step is to install the twine package, which simplifies the process of \nuploading. Be sure to install it in our virtual environment:\n(venv) $ pip install twine\nCollecting twine\n  Downloading twine-3.4.2-py3-none-any.whl (34 kB)\n...\nThe token replaces the login and password when uploading a \npackage. We will see later how to use it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 430,
      "chapter": null,
      "content": "Chapter 11\n[ 411 ]\nInstalling collected packages: zipp, webencodings, six, Pygments, \nimportlib-metadata, docutils, bleach, tqdm, rfc3986, requests-toolbelt, \nreadme-renderer, pkginfo, keyring, colorama, twine\nSuccessfully installed Pygments-2.10.0 bleach-4.1.0 colorama-0.4.4 \ndocutils-0.17.1 importlib-metadata-4.8.1 keyring-23.2.0 pkginfo-1.7.1 \nreadme-renderer-29.0 requests-toolbelt-0.9.1 rfc3986-1.5.0 six-1.16.0 \ntqdm-4.62.2 twine-3.4.2 webencodings-0.5.1 zipp-3.5.0\nNow we can upload the packages created in the dist subdirectory. \nWe have now built the compiled Wheel and the source distribution:\n(venv) $ ls dist\nwheel-package-compiled-0.0.1.tar.gz\nwheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\nLet's upload the packages. We need to indicate that we want to upload to the testpy \nrepo. We will use __token__ as the username and the full token (including the pypi- \nprefix) as the password:\n(venv) $ python -m twine upload --repository testpypi dist/*\nUploading distributions to https://test.pypi.org/legacy/\nEnter your username: __token__\nEnter your password:\nUploading wheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\n100%|██████████████████████████████████████████████████████████████████\n█████| 12.6k/12.6k [00:01<00:00, 7.41kB/s]\nUploading wheel-package-compiled-0.0.1.tar.gz\n100%|██████████████████████████████████████████████████████████████████\n█████| 24.0k/24.0k [00:00<00:00, 24.6kB/s]\nView at:\nhttps://test.pypi.org/project/wheel-package-compiled/0.0.1/\nFor our example, we will use the same package created previously, \nbut keep in mind that trying to reupload it may not work, as there \nmay already be a package called that in TestPyPI. TestPyPI is \nnot permanent, and regularly deletes packages, but the example \nuploaded as part of the writing process of the book could still be \nthere. To do your tests, create your own package with a unique \nname.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 431,
      "chapter": null,
      "content": "Package Management\n[ 412 ]\nThe package is now uploaded! We can check the page on the TestPyPI website.\nFigure 11.6: Main page for the package\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 432,
      "chapter": null,
      "content": "Chapter 11\n[ 413 ]\nYou can verify the uploaded files by clicking Download files:\nFigure 11.7: Verifying the uploaded files\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 433,
      "chapter": null,
      "content": "Package Management\n[ 414 ]\nYou can also access the files through the search function:\nFigure 11.8: The package available in search\nYou can now download the package directly through pip, but you need to indicate \nthat the index to use is the TestPyPI one. To ensure a clean installation, create a new \nvirtual environment as follows:\n$ python3 -m venv venv2\n$ source ./venv2/bin/activate\n(venv2) $ pip install --index-url https://test.pypi.org/simple/ wheel-\npackage-compiled\nLooking in indexes: https://test.pypi.org/simple/\nCollecting wheel-package-compiled\n  Downloading https://test-files.pythonhosted.org/packages/87/c3/88129\n8cdc8eb6ad23456784c80d585b5872581d6ceda6da3dfe3bdcaa7ed/wheel_package_\ncompiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl (9.6 kB)\nCollecting requests\n  Downloading https://test-files.pythonhosted.org/packages/6d/00/8ed\n1b6ea43b10bfe28d08e6af29fd6aa5d8dab5e45ead9394a6268a2d2ec/requests-\n2.5.4.1-py2.py3-none-any.whl (468 kB)\n     |████████████████████████████████| 468 kB 634 kB/s\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1110,
      "extraction_method": "Direct"
    },
    {
      "page_number": 434,
      "chapter": null,
      "content": "Chapter 11\n[ 415 ]\nInstalling collected packages: requests, wheel-package-compiled\nSuccessfully installed requests-2.5.4.1 wheel-package-compiled-0.0.1\nNote that the version downloaded is the Wheel one, as it is the right target for the \ncompiled version. It also correctly downloads the specified requests dependency.\nYou can now test the package through the Python interpreter:\n(venv2) $ python\nPython 3.9.6 (default, Jun 29 2021, 05:25:02)\n[Clang 12.0.5 (clang-1205.0.22.9)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import wheel_package_compiled\n>>> wheel_package_compiled.check_if_prime(5)\nTrue\nThe package is now installed and ready to use. The next step is to upload this \npackage to production PyPI instead of TestPyPI. This is totally analogous to the \nprocess that we've seen here, creating an account in PyPI and proceeding from there. \nBut, what if the objective of the package is not to create a publicly available package? \nIt is possible that we need to create our own index with our packages.\nCreating your own private index\nSometimes, you'll need to have your own private index, so you can serve your own \npackages without opening them to the full internet, for internal packages that need \nto be used across the company, but where it doesn't make sense to upload them to \nthe public PyPI.\nYou can create your own private index that can be used to share those packages and \ninstall them by calling to that index.\nTo serve the packages, we need to run a PyPI server locally. There are several options \nin terms of available servers that can be used, but an easy option is pypiserver \n(https://github.com/pypiserver/pypiserver).\npypiserver can be installed in several ways; we will see how to \nrun it locally, but to serve it correctly, you'll need to install it in a \nway that's available in your network. Check the documentation to \nsee several options, but a good option is to use the official Docker \nimage available.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 435,
      "chapter": null,
      "content": "Package Management\n[ 416 ]\nTo run pypiserver, first, install the package using pip and create a directory for \nstoring the packages:\n$ pip install pypiserver\nCollecting pypiserver\n  Downloading pypiserver-1.4.2-py2.py3-none-any.whl (77 kB)\n     |████████████████████████████████| 77 kB 905 kB/s\nInstalling collected packages: pypiserver\nSuccessfully installed pypiserver-1.4.2\n$ mkdir ./package-library\nStart the server. We use the parameter -p 8080 to serve it in that port, the directory \nto store the packages, and -P . -a . to facilitate the uploading of packages without \nauthentication:\n$ pypi-server -P . -a . -p 8080 ./package-library\nOpen a browser and check http://localhost:8080.\nFigure 11.9: Local pypi server\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 823,
      "extraction_method": "Direct"
    },
    {
      "page_number": 436,
      "chapter": null,
      "content": "Chapter 11\n[ 417 ]\nYou can check the available packages in this index by going to http://\nlocalhost:8080/simple/.\nFigure 11.10: Empty index so far\nWe now need to upload the packages, using twine again, but pointing to our private \nURL. As we are able to upload with no authentication, we can enter an empty \nusername and password:\n$ python -m twine upload --repository-url http://localhost:8080 dist/*\nUploading distributions to http://localhost:8080\nEnter your username:\nEnter your password:\nUploading wheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\n100%|████████████████████████████████| 12.6k/12.6k [00:00<00:00, \n843kB/s]\nUploading wheel-package-compiled-0.0.1.tar.gz\n100%|████████████████████████████████| 24.0k/24.0k [00:00<00:00, \n2.18MB/s]\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 863,
      "extraction_method": "Direct"
    },
    {
      "page_number": 437,
      "chapter": null,
      "content": "Package Management\n[ 418 ]\nThe index is now showing the package available.\nFigure 11.11: Showing the package uploaded\nFigure 11.12: All the uploaded files for the package\nThe files are also uploaded to the package-library directory:\n$ ls package-library\nwheel-package-compiled-0.0.1.tar.gz\nwheel_package_compiled-0.0.1-cp39-cp39-macosx_11_0_x86_64.whl\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 438,
      "chapter": null,
      "content": "Chapter 11\n[ 419 ]\nAny file added to package-library will also be served, allowing packages to be \nadded by moving them to the directory, although that could be complicated once the \nserver is deployed properly to the packages over the network.\nThe package can now be downloaded and installed, pointing to your private index \nusing the –index-url parameter:\n$ pip install --index-url http://localhost:8080 wheel-package-compiled\nLooking in indexes: http://localhost:8080\nCollecting wheel-package-compiled\n  Downloading http://localhost:8080/packages/wheel_package_compiled-\n0.0.1-cp39-cp39-macosx_11_0_x86_64.whl (9.6 kB)\n…\nSuccessfully installed certifi-2021.5.30 charset-normalizer-2.0.4 idna-\n3.2 requests-2.26.0 urllib3-1.26.6 wheel-package-compiled-0.0.1\n$ python\nPython 3.9.6 (default, Jun 29 2021, 05:25:02)\n[Clang 12.0.5 (clang-1205.0.22.9)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import wheel_package_compiled\n>>> wheel_package_compiled.check_if_prime(5)\nTrue\nThis tests that the module can be imported and executed after installation.\nSummary\nIn this chapter, we described when it's a good idea to create a standard package and \nthe caveats and requirements that we should add to be sure that we are taking a \ngood decision. In essence, creating a new package is creating a new project, and we \nshould give the proper ownership, documentation, and so on, as expected of other \nprojects in the organization.\nWe described the simplest possible package in Python just by structuring code, but \nwithout creating a proper package. This acts as a baseline on how the code should be \nstructured later.\nWe continued describing what the current packaging environment is and what are \nthe different elements that are part of it, like PyPI, which is the official source for \npublicly available packages, and how to create virtual environments to not cross-\ncontaminate different environments when requiring different dependencies. We also \ndescribed the Wheel package, which will be the kind of package that we will create \nlater.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 439,
      "chapter": null,
      "content": "Package Management\n[ 420 ]\nNext, we described how to create such a package, creating a setup.py file. We \ndescribed how to install it in development mode to be able to do tests and how to \nbuild and get the package ready.\nWe took a small detour to explain how to generate code to be compiled with \nCython, an easy way to create Python extensions writing in Python code with some \nextensions, to generate C code automatically.\nWe used Cython code to show how to generate a compiled Wheel, allowing the \ndistribution of already precompiled code without needing to be compiled on \ninstallation.\nWe showed how to upload packages to PyPI to distribute publicly (showing how to \nupload to TestPyPI, allowing the upload of packages to be tested) and described how \nto create your own individual index so that you can distribute your own packages \nprivately.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\nThere are some alternatives to creating packages instead of using \nthe standard setup.py file. You can take a look at the Poetry \npackage (https://python-poetry.org/) to see how to manage \npackages in a more integrated way, especially if the package has \nmany dependencies.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 440,
      "chapter": null,
      "content": "Our work with an architecture is not finished when a system is up and running. A \nworking application requires ongoing maintenance and effort to keep it running \nsuccessfully.\nSystems will be in a maintenance phase during the longest part of their life cycles. \nThis phase is where we add features, detect and fix defects, and analyze the system's \nbehavior to prevent problems. \nTo be able to do that successfully, we need to have tools to cover two basic elements:\n•\t\nObservability: This is the capability of knowing what's going on in a live \nsystem. Low-observability systems are difficult or even impossible to \nunderstand, which makes it difficult to know if there are problems or work \nout the cause of those problems. In high-observability systems, it's easy \nto infer the internal state and the events flowing inside the system, which \nallows for easy detection of the critical structures where problems are being \ngenerated. \nThe main tools for observing systems are logs and metrics, which are used in \nconjunction to allow us to understand the system and analyze its behavior.\nObservability is a property of the system itself. Typically, monitoring is the \naction of obtaining information about the current or past state of the system. \nIt's all a bit of a naming debate, but technically, you monitor the system to \ncollect the observable parts of it.\nPart IV\nOngoing \noperations\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 441,
      "chapter": null,
      "content": "•\t\nAnalysis: To detect problems in more controlled situations, we have two \nimportant tools, debugging and profiling. The first is a staple of the \ndevelopment process, working step by step through code to understand how \na piece of code works and ascertain why it's doing what it is doing. Profiling \nis instrumenting the code to show how it works and, specifically, determine \nwhich parts take the most time to execute, to allow you to act on them and \nimprove their performance. \nThese two tools work complementarily with one another and allow us to fix \nand improve different kinds of problems after they've been detected.\nIn this section, we will also talk about the challenge of making changes while the \nsystem is in operation. The only constant in software is change, and balancing \nexisting systems with new functionalities is a critical ability. Part of this task is to \ncoordinate between different teams so they are aware of the implications of their \nchanges and can work as a single unit.\nThis section comprises of the following chapters:\n•\t\nLogging\n•\t\nMetrics\n•\t\nProfiling\n•\t\nDebugging\n•\t\nOngoing architecture\nWe will start by understanding how to use logs for monitoring.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1289,
      "extraction_method": "Direct"
    },
    {
      "page_number": 442,
      "chapter": null,
      "content": "[ 423 ]\n12\nLogging\nOne of the basic elements of monitoring and observability is logs. Logs allow us to \ndetect actions that are happening in a running system. That information can be used \nto analyze the behavior of the system, especially any errors or bugs that may arise, \ngiving us useful insight into what is actually going on.\nUsing logs correctly is deceptively difficult, though. It's easy to collect too much or \ntoo little information, or to log the wrong information. In this chapter, we will see \nsome of the key elements of what to collect, and the general strategy to follow to \nensure that logs are used to their best effect.\nIn this chapter, we'll cover the following topics:\n•\t\nLog basics\n•\t\nProducing logs in Python\n•\t\nDetecting problems through logs\n•\t\nLog strategies\n•\t\nAdding logs while developing\n•\t\nLog limitations\nLet's start with the basic principles of logging.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 443,
      "chapter": null,
      "content": "Logging\n[ 424 ]\nLog basics\nLogs are basically messages produced by the system as it runs. These messages are \nproduced by specific pieces of code as they are executed, allowing us to track actions \nhappening in the code.\nLogs can be completely generic, like \"Function X is called\" or can include some context \nof the specifics of the execution, like \"Function X is called with parameter Y.\"\nNormally, logs are generated as plaintext messages. While there are other options, \npure plaintext is very easy to deal with, can be read easily, is flexible in its format, \nand can be searched with pure text tools like grep. These tools are normally very fast \nand most developers and sysadmins know how to use them.\nAs well as the main message text, each log contains some metadata about what \nsystem produced the log, what time the log was created, and so on. If the log is in \ntext format, this is normally attached to the start of the line.\nAnother important metadata value is the severity of the log. This allows us to \ncategorize the different logs by their relative importance. The standard severity \nlevels, in order of less to more important, are DEBUG, INFO, WARNING, and ERROR. \nIt's important to categorize the logs with their proper severity and filter out \nunimportant messages to focus on the more important ones. Each logging facility can \nbe configured to only produce logs at one severity level or more.\nA standard and consistent log format helps you with searching, \nfiltering, and sorting the messages. Ensure that you use consistent \nformats across your different systems.\nThe CRITICAL level is less used, but it's useful to show catastrophic \nerrors.\nIt's possible to add custom log levels instead of the predefined \nones. This is generally a bad idea and should be avoided in \nmost cases, as the log levels are well understood by all tools and \nengineers. We will describe later in this chapter how to define a \nstrategy per level to make the best of each level.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2079,
      "extraction_method": "Direct"
    },
    {
      "page_number": 444,
      "chapter": null,
      "content": "Chapter 12\n[ 425 ]\nIn a system serving requests, either as request-response or asynchronously, most \nof the logs will be generated as part of dealing with a request, which will produce \nseveral logs indicating what the request is doing. Because more than one request will \nnormally be undergoing processing at once, the logs will be generated intermixed. \nFor example, consider the following logs:\nSept 16 20:42:04.130 10.1.0.34 INFO web: REQUEST GET /login\nSept 16 20:42:04.170 10.1.0.37 INFO api: REQUEST GET /api/login\nSept 16 20:42:04.250 10.1.0.37 INFO api: REQUEST TIME 80 ms\nSept 16 20:42:04.270 10.1.0.37 INFO api: REQUEST STATUS 200\nSept 16 20:42:04.360 10.1.0.34 INFO web: REQUEST TIME 230 ms\nSept 16 20:42:04.370 10.1.0.34 INFO web: REQUEST STATUS 200\nThe preceding logs show two different services, as indicated by the different IP \naddresses (10.1.0.34 and 10.1.0.37) and the two different service types (web and \napi). Though this can be enough to separate the requests, it's a good idea to create a \nsingle request ID to be able to group the requests in the following way:\nSept 16 20:42:04.130 10.1.0.34 INFO web: [4246953f8] REQUEST GET /login\nSept 16 20:42:04.170 10.1.0.37 INFO api: [fea9f04f3] REQUEST GET /api/\nlogin\nSept 16 20:42:04.250 10.1.0.37 INFO api: [fea9f04f3] REQUEST TIME 80 ms\nSept 16 20:42:04.270 10.1.0.37 INFO api: [fea9f04f3] REQUEST STATUS 200\nSept 16 20:42:04.360 10.1.0.34 INFO web: [4246953f8] REQUEST TIME 230 \nms\nSept 16 20:42:04.370 10.1.0.34 INFO web: [4246953f8] REQUEST STATUS 200\nAs we saw in Chapter 5, The Twelve-Factor App Methodology, in the Twelve-Factor \nApp methodology, logs should be treated as an event stream. This means that the \napplication itself should not be concerned with the storage and treatment of logs. \nInstead, the logs should be directed to stdout. From there, while developing the \napplication, the developer can extract the information while it's running.\nIn microservices environments, requests will flow from one service \nto the other, so it's a good idea to create a request ID that's shared \nacross services so the full cross-service flow can be understood. \nTo do that, the request ID needs to be created by the first service \nand then transmitted to the next, typically as a header in an HTTP \nrequest.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2384,
      "extraction_method": "Direct"
    },
    {
      "page_number": 445,
      "chapter": null,
      "content": "Logging\n[ 426 ]\nIn production environments, stdout should be captured so that other tools can use it \nand then routed, annexing any different sources into a single stream, and then stored \nor indexed for later consulting. These tools should be configured in the production \nenvironment, and not in the app itself. \nPossible tools for this rerouting include alternatives like Fluentd (https://github.\ncom/fluent/fluentd) or even the old favorite combination of a direct to logger Linux \ncommand to create system logs and then sending those logs to a configured rsyslog \n(https://www.rsyslog.com/) server that can forward and aggregate them.\nNo matter how we collect logs, a typical system will produce a lot of them, and \nthey need to be stored somewhere. While each individual log is small, aggregating \nthousands of them uses a significant amount of space. Any log system should be \nconfigured to have a policy on how much data it should accept to avoid growing \nindefinitely. In general, a retention policy based on time (such as keeping logs from \nthe last 15 days) is the best approach, as it will be easy to understand. Finding the \nbalance between how far back in the past you need to be able look and the amount of \nspace the system uses is important.\nGenerating log entries is easy, as we will see in the next section, Producing logs in \nPython.\nProducing logs in Python\nPython includes a standard module to produce logs. This module is easy to use, with \na very flexible configuration, but it can be confusing if you don't understand the way \nit operates.\nA basic program to create logs looks like this. This is available as basic_logging.\npy on GitHub at https://github.com/PacktPublishing/Python-Architecture-\nPatterns/tree/main/chapter_12_logging:\nBe sure to check the retention policy when enabling any new log \nservice, be it local or cloud-based, to make sure it's compatible \nwith your defined retention period. You won't be able to analyze \nanything that happened before the time window. Double-\ncheck that the rate of log creation is as expected and that space \nconsumption is not making the effective time window in which \nyou can collect logs smaller. You don't want to find out that you \nunexpectedly went over quota while you were tracking a bug.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2370,
      "extraction_method": "Direct"
    },
    {
      "page_number": 446,
      "chapter": null,
      "content": "Chapter 12\n[ 427 ]\nimport logging\n# Generate two logs with different severity levels\nlogging.warning('This is a warning message')\nlogging.info('This is an info message')\nThe .warning and .info methods create logs with the corresponding severity \nmessage. The message is a text string. \nWhen executed, it shows the following:\n$ python3 basic_logging.py\nWARNING:root:This is a warning message\nThe logs are, by default, routed to stdout, which is what we want, but it is \nconfigured not to display INFO logs. The format of the logs is also the default, which \ndoesn't include a timestamp.\nTo add all this information, we need to understand the three basic elements used for \nlogging in Python:\n•\t\nA formatter, which describes how the full log is going to be presented, \nattaching metadata like the timestamp or the severity.\n•\t\nA handler, which decides how the logs are propagated. It sets the format of \nthe logs through the formatter, as defined above.\n•\t\nA logger, which produces the logs. It has one or more handlers that describe \nhow the logs are propagated.\nWith this information, we can configure the logs to specify all the details we want:\nimport sys\nimport logging\n# Define the format\nFORMAT = '%(asctime)s.%(msecs)dZ:APP:%(name)s:%(levelname)s:%(message)\ns'\nformatter = logging.Formatter(FORMAT, datefmt=\"%Y-%m-%dT%H:%M:%S\")\n# Create a handler that sends the logs to stdout\nhandler = logging.StreamHandler(stream=sys.stdout)\nhandler.setFormatter(formatter)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 447,
      "chapter": null,
      "content": "Logging\n[ 428 ]\n# Create a logger with name 'mylogger', adding the handler and setting\n# the level to INFO\nlogger = logging.getLogger('mylogger')\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n# Generate three logs\nlogger.warning('This is a warning message')\nlogger.info('This is an info message')\nlogger.debug('This is a debug message, not to be displayed')\nWe define the three elements in the same order that we saw before. First the \nformatter, then the handler, which sets the formatter, and finally the logger, which \nadds the handler.\nThe formatter has the following format:\nFORMAT = '%(asctime)s.%(msecs)dZ:APP:%(name)s:%(levelname)s:%(message)\ns'\nformatter = logging.Formatter(FORMAT, datefmt=\"%Y-%m-%dT%H:%M:%S\")\nFORMAT is composed in Python % format, which is an old way to describe strings. Most \nelements are described as %(name)s, where the final s character means string format. \nHere's a description of each element:\n•\t\nasctime sets the timestamp in a human-readable format. We describe \nit in the datefmt argument to follow the ISO 8601 format. We also add \nthe milliseconds next and a Z to get the timestamp in full ISO 8601 form. \n%(msecs)d with a d at the end means that we print the value as an integer. \nThis is to limit the value to milliseconds and not show any extra resolution, \nwhich is available as a fractional value.\n•\t\nname is the name of the logger, as we will describe later. We add also APP to \ndifferentiate between different applications.\n•\t\nlevelname is the severity of the log, such as INFO, WARNING, or ERROR.\n•\t\nmessage, finally, is the log message.\nOnce we have defined the formatter, we can move to the handler:\nhandler = logging.StreamHandler(stream=sys.stdout)\nhandler.setFormatter(formatter)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1847,
      "extraction_method": "Direct"
    },
    {
      "page_number": 448,
      "chapter": null,
      "content": "Chapter 12\n[ 429 ]\nThe handler is a StreamHandler, and we set the destination of the stream to be sys.\nstdout, which is the Python-defined variable that points to stdout.\nOnce the handler is defined, we can create the logger:\nlogger = logging.getLogger('mylogger')\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\nThe first thing to do is to create a name for the logger, which here we define as \nmylogger. This allows us to divide the logs of the application into subsections. We \nappend the handler using .addHandler.\nFinally, we define the level to log as INFO using the .setLevel method. This will \ndisplay all logs of the level INFO and higher, while those lower won't be.\nIf we run the file, we see the whole configuration coming together:\n$ python3 configured_logging.py\n2021-09-18T23:15:24.563Z:APP:mylogger:WARNING:This is a warning message\n2021-09-18T23:15:24.563Z:APP:mylogger:INFO:This is an info message\nWe can see that:\n•\t\nThe time is defined in ISO 8601 format as 2021-09-18T23:15:24.563Z. This is \na combination of the asctime and msec parameters.\n•\t\nThe APP and mylogger parameters allow us to filter by application and \nsubmodule.\n•\t\nThe severity is displayed. Note that there's a DEBUG message that isn't \ndisplayed, as the minimum level configured is INFO.\nThe logging module in Python is capable of high levels of configuration. Check \nthe official documentation for more information at https://docs.python.org/3/\nlibrary/logging.html.\nThere are more handlers available, like FileHandler to send the \nlogs to a file, SysLogHandler to send logs to a syslog destination, \nand even more advanced cases like TimeRotatingFileHandler, \nwhich rotates the logs based on time, meaning it stores the last \ndefined time, and archives older versions. You can see more \ninformation of all available handlers in the documentation at \nhttps://docs.python.org/3/howto/logging.html#useful-\nhandlers.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2013,
      "extraction_method": "Direct"
    },
    {
      "page_number": 449,
      "chapter": null,
      "content": "Logging\n[ 430 ]\nDetecting problems through logs\nFor any problem in a running system, there are two kind of errors that can occur: \nexpected and unexpected. In this section, we will see the differences between them in \nterms of logs and how we handle them.\nDetecting expected errors\nExpected errors are errors that are detected explicitly by creating an ERROR log in the \ncode. For example, the following code produces an ERROR log when the accessed URL \nreturns a status code different from 200 OK:\nimport logging\nimport requests\nURL = 'https://httpbin.org/status/500'\nresponse = requests.get(URL)\nstatus_code = response.status_code\nif status_code != 200:\n    logging.error(f'Error accessing {URL} status code {status_code}')\nThis code, when executed, triggers an ERROR log:\n$ python3 expected_error.py\nERROR:root:Error accessing https://httpbin.org/status/500 status code \n500\nThis is a common pattern to access an external URL and validate that it has been \naccessed correctly. The block where the log is generated could perform some \nremediation or a retry, among other things.\nThis is an example of an expected error. We planned in advance for something that \nwe didn't want to happen, but understood that there's a possibility of it happening. \nBy planning in advance, the code is ready to process the error and capture it \nadequately.\nHere, we use the https://httpbin.org service, a simple HTTP \nrequest and response service that can be used to test code. In \nparticular, the https://httpbin.org/status/<code> endpoint \nreturns the specified status code, making it easy to generate errors.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1697,
      "extraction_method": "Direct"
    },
    {
      "page_number": 450,
      "chapter": null,
      "content": "Chapter 12\n[ 431 ]\nIn this case, we can describe the situation clearly enough, and provide context to \nunderstand what is happening. The problem is obvious, even if the solution may not \nbe.\nThese kinds of errors are relatively easy to deal with since they describe foreseen \nproblems. \nFor example, the site may be unavailable, there could be an authentication problem, \nor perhaps the base URL is misconfigured.\nOther examples of this type of error include connections to databases and data being \nstored in a deprecated format. \nCapturing unexpected errors\nBut expected errors are not the only ones that can occur. Unfortunately, any running \nsystem will surprise you with all kinds of unexpected behavior that will break the \ncode in creative ways. Unexpected errors in Python are normally produced by \nan exception being raised at some point in the code when that exception won't be \ncaptured.\nFor example, imagine that when making a small change to some code, we introduce \na typo:\nimport logging\nimport requests\nURL = 'https://httpbin.org/status/500'\nlogging.info(f'GET {URL}')\nresponse = requests.ge(URL)\nstatus_code = response.status_code\nif status_code != 200:\n    logging.error(f'Error accessing {URL} status code {status_code}')\nKeep in mind that in some cases, it's possible for the code to \ndeal with a certain situation without failing, but for it still to be \nconsidered an error. For example, maybe you want to detect if an \nold authentication system is still in use by someone. This method \nof adding ERROR or WARNING logs when deprecated actions are \ndetected can enable you to take actions to remedy the situation.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 451,
      "chapter": null,
      "content": "Logging\n[ 432 ]\nNote that in line 8, we introduced a typo:\nresponse = requests.ge(URL)\nThe correct .get call has been replaced by .ge. When we run it, it produces the \nfollowing error:\n$ python3 unexpected_error.py\nTraceback (most recent call last):\n  File \"./unexpected_error.py\", line 8, in <module>\n    response = requests.ge(URL)\nAttributeError: module 'requests' has no attribute 'ge'\nBy default in Python, it will show the error and stack trace in the stdout. When the \ncode is executed as part of a web server, this is sometimes enough to send these \nmessages as ERROR logs, depending on how the configuration is set up.\nIf you need to create a script that needs to be running endlessly and is protected \nagainst any unexpected errors, be sure to use a try..except block as it's generic, so \nany possible exception will be captured and handled.\nFor example, let's adjust the code to make a request every few seconds. The code is \navailable in GitHub at https://github.com/PacktPublishing/Python-Architecture-\nPatterns/tree/main/chapter_12_logging:\nimport logging\nimport requests\nfrom time import sleep\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nAny web server will capture and route these messages properly \ntoward the logs and generate a proper 500 status code, indicating \nthat there has been an unexpected error. The server will still be \navailable for the next request.\nAny Python exception that's properly captured with a specific \nexcept block can be considered an expected error. Some of them \nmay require ERROR messages to be generated, but others may be \ncaptured and handled without requiring such information.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 452,
      "chapter": null,
      "content": "Chapter 12\n[ 433 ]\nwhile True:\n    \n    try:\n        sleep(3)\n        logging.info('--- New request ---')\n    \n        URL = 'https://httpbin.org/status/500'\n        logging.info(f'GET {URL}')\n        response = requests.ge(URL)\n        scode = response.status_code\n        if scode != 200:\n            logger.error(f'Error accessing {URL} status code {scode}')\n    except Exception as err:\n        logger.exception(f'ERROR {err}')\nThe key element is the following endless loop:\nwhile True:\n    try:\n        code\n    except Exception as err:\n        logger.exception(f'ERROR {err}')\nThe try..except block is inside the loop, so even if there's an error, the loop will be \nuninterrupted. If there's any error, except Exception will capture it, no matter what \nthe exception is.\nTo be sure that not only is the error logged, but also the full stack trace, we log it \nusing .exception instead of .error. This extends the information over a single text \nmessage while logging it with ERROR severity.\nWhen we run the command, we get these logs. Be sure to stop it by pressing Ctrl + C:\n$ python3 protected_errors.py\nINFO:root:--- New request ---\nINFO:root:GET https://httpbin.org/status/500\nERROR:root:ERROR module 'requests' has no attribute 'ge'\nThis is sometimes referred to as Pokemon exception handling, as in \n\"Gotta catch 'em all.\" This should be restricted to a kind of \"last-\nresort safety net.\" In general, not being precise with the exceptions \nto be captured is a bad idea, as you can hide errors by handling \nthem incorrectly. Errors should never pass silently.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 453,
      "chapter": null,
      "content": "Logging\n[ 434 ]\nTraceback (most recent call last):\n  File \"./protected_errors.py\", line 18, in <module>\n    response = requests.ge(URL)\nAttributeError: module 'requests' has no attribute 'ge'\nINFO:root:--- New request ---\nINFO:root:GET https://httpbin.org/status/500\nERROR:root:ERROR module 'requests' has no attribute 'ge'\nTraceback (most recent call last):\n  File \"./protected_errors.py\", line 18, in <module>\n    response = requests.ge(URL)\nAttributeError: module 'requests' has no attribute 'ge'\n^C\n...\nKeyboardInterrupt\nAs you can see, the logs include Traceback, which allows us to detect a specific \nproblem by adding information about where the exception was produced.\nAny unexpected error should be logged as ERROR. Ideally, they should also be \nanalyzed and the code changed to bugfix them or at least transform them into \nexpected errors. Sometimes this is not feasible due to other pressing issues or a low \noccurrence of the problem, but some strategy should be implemented to make sure \nthere's consistency in the handling of bugs.\nSometimes, unexpected errors will present themselves with enough information \nabout what the problem is, which could be related to an external problem like a \nnetwork issue or a database problem. The solution may be located outside the realm \nof the service itself.\nLog strategies\nA common problem when dealing with logs is deciding on the appropriate severity \nfor each of the individual services. Is this message a WARNING or an ERROR? Should this \nstatement be added as an INFO message or not?\nA great tool to handle unexpected errors is Sentry (https://\nsentry.io/). This tool creates a trigger for each error on a lot \nof common platforms, including Python Django, Ruby on Rails, \nNode, JavaScript, C#, iOS, and Android. It aggregates the errors \ndetected and allows us to work with them more strategically, \nwhich is sometimes difficult when just having access to the logs.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2027,
      "extraction_method": "Direct"
    },
    {
      "page_number": 454,
      "chapter": null,
      "content": "Chapter 12\n[ 435 ]\nMost of the log severity descriptions have definitions, such as the program shows a \npotentially harmful situation or the application highlights the progress of the request. These \nare vague definitions and difficult to act on in a real-life situation. Instead of using \nthese vague definitions, try to define each level in relationship with any follow-up \naction that should be taken if the issue is encoutered. This helps to clarify to the \ndevelopers what to do when a given error log is found. For example: \"Do I want to be \ninformed each and every time this situation happens?\"\nThe following table shows some examples of the different severity levels and what \naction could be taken:\nLog level\nAction to take\nComments\nDEBUG\nNone.\nNot tracked. Only useful \nwhile developing.\nINFO\nNone.\nINFO logs show generic \ninformation about the flow \nof the actions in the app to \nhelp track systems.\nWARNING\nTrack the number of logs. \nAlert on raising levels.\nWARNING logs track errors \nthat are automatically fixed, \nlike retries to connect to an \nexternal service, or fixable \nformat errors in a database. \nA sudden increase may \nrequire investigation.\nERROR\nTrack the number of logs. \nAlert on raising levels. \nReview all errors.\nERROR logs track errors \nthat can't be recovered. A \nsudden increase may require \nimmediate action. All of \nthem should be periodically \nreviewed to fix common \noccurrences and mitigate \nthem, perhaps moving them \nto WARNING level.\nCRITICAL\nImmediate response.\nCRITICAL logs indicate a \ncatastrophic failure in the \napplication. A single one \nindicates the system is not \nworking at all and can't \nrecover.\nThis sets clear expectations on how to respond. Note this is an example, and you \nmay need to make tweaks and adjustments to adapt this to the needs of your specific \norganization.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 455,
      "chapter": null,
      "content": "Logging\n[ 436 ]\nThe hierarchy of different severities is very clear, and in our example, there's \nan acceptance that there'll be a certain number of ERROR logs generated. For the \ndevelopment team's sanity, not everything needs to be fixed immediately, but a \ncertain order and prioritization should be enforced.\nWARNING logs are indications that something may not be working as smoothly as \nexpected, but things are under control, unless there's a sudden increase in the \nnumber of logs of this kind. INFO logs are just there to give context in the event of a \nproblem, but can be ignored otherwise.\nIn production situations, ERROR logs will typically be categorized \nfrom \"we're doomed\" to \"meh.\" Development teams should \nactively either fix \"meh\" logs or stop the issue from being logged \nto remove noise from the monitoring tools. That may include \nlowering the level of logs if they aren't worth checking. You want \nas few ERROR logs as possible, so all of them can be meaningful. \nRemember that ERROR logs will include unexpected errors that \ntypically require a fix to either resolve the issue completely, or \nexplicitly capture it and reduce its severity if it is not important. \nThis follow-up is definitely a challenge as applications grow, as the \nnumber of ERROR logs will increase significantly. It requires time \nto be spent on proactive maintenance. If this is not taken seriously \nand it is too often dropped for other tasks, it will compromise the \nreliability of the application in the medium term.\nA common mistake is to generate ERROR logs in actions where there \nare incorrect input parameters, such as in web requests when a \n400 BAD REQUEST status code is returned. Some developers will \nargue that a customer sending a malformed request is an error. But \nthere's nothing that the developer team should do if the request is \nproperly detected and returned. It's business as usual, and the only \naction may be to return a meaningful message to the requester so \nthey can fix their request. \nIf this behavior persists in certain critical requests, like repeatedly \nsending a bad password, a WARNING log can be created. There's no \npoint in creating an ERROR log when the application is behaving as \nexpected. \nIn web applications, as a rule of thumb, ERROR logs should only \nbe created when the status code is one of the 50X variants (like \n500, 502, and 503). Remember that the 40X errors mean that the \nsender has a problem, while 50X means that the application has the \nproblem, and it's your team's responsibility to fix it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2652,
      "extraction_method": "Direct"
    },
    {
      "page_number": 456,
      "chapter": null,
      "content": "Chapter 12\n[ 437 ]\nWith common and shared definitions of log levels across the team, all engineers \nwill have a shared understanding of error severity that will help shape meaningful \nactions toward improving the code.\nAllow time for tweaking and adjusting any definition. It's also likely that you'll have \nto deal with logs created before the definition, which can require work. One of the \nbiggest challenges in legacy systems is creating a proper logging system to categorize \nproblems, as they'll likely be very noisy, making it difficult to distinguish the real \nproblems from annoyances and even non-problems.\nAdding logs while developing\nAny test runner will capture logs and display it as part of the trace while running \ntests.\nThis is a good opportunity to check that the expected logs are being generated while \nthe feature is still in development phase, especially if it's done in a TDD process \nwhere the failing tests and errors are produced routinely as part of the process, as \nwe saw in Chapter 10, Testing and TDD. Any test that checks an error should also add \na corresponding log and, while developing the feature, check that they are being \nproduced.\nWhile developing, DEBUG logs can be used to add extra information about the code \nflow that would be excessive for production. In development, this extra information \ncan help fill in the gaps between INFO logs and help developers to solidify the habit \nof adding logs. A DEBUG log may be promoted to INFO if, during tests, it's found to be \nuseful in tracking problems in production.\npytest, which we introduced in Chapter 10, Testing and TDD, will \ndisplay logs as part of the result of a failing test.\nYou can explicitly add to the test a check to validate that the log is \nbeing generated by using a tool like pytest-catchlog (https://\npypi.org/project/pytest-catchlog/).\nTypically, though, we just take a bit of care and incorporate the \npractice of checking while using TDD practices as part of the initial \ncheck that the test is failing. However, be sure that the developers \nunderstand why it's useful to have logs while developing to make \nthe habit stick.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2241,
      "extraction_method": "Direct"
    },
    {
      "page_number": 457,
      "chapter": null,
      "content": "Logging\n[ 438 ]\nAdditionally, for special occasions, DEBUG logs can be enabled in production in \ncontrolled cases to track certain problems that are difficult to understand. Note that \nthis has big implications on the number of generated logs, which can lead to storage \nproblems. Be very cautious here.\nIt's always a good idea to double-check that the logs are being properly captured and \navailable in different environments. All the configuration to ensure that the logs are \nproperly captured may take a bit of time, so it's better to do this beforehand. This \ninvolves capturing unexpected errors and other logs in production and checking that \nall the plumbing is done correctly. The alternative is to discover that it's not working \ncorrectly only after stumbling into a real problem.\nLog limitations\nLogs are very useful to understand what's happening in a running system, but they \nhave certain limitations that are important to understand:\n•\t\nLogs are only as good as their messages. A good, descriptive message is critical \nin making logs useful. Reviewing the log messages with a critical eye, \nand correcting them when needed, is important to save precious time on \nproduction problems.\n•\t\nHave an appropriate number of logs. Too many logs can confuse a flow, and \ntoo few may not include enough information to allow us to understand the \nproblem. Large numbers of logs also create problems with storage. \n•\t\nLogs should work as an indication of the context of the problem, but likely won't \npinpoint it. Trying to generate specific logs that fully explain a bug will be an \nimpossible task. Instead, focus on showing the general flow and surrounding \ncontext of the action, so it can be replicated locally and debugged. For \nexample, for a request, make sure to log both the request and its parameters \nso the situation can be replicated.\nBe sensible about the messages displayed in INFO and higher \nseverity logs. In terms of information that's displayed, avoid \nsensitive data such as passwords, secret keys, credit card numbers, \nand personal information.\nKeep an eye in production for any size limitations and how quickly \nlogs are generated. Systems may experience a log explosion in \nsituations when new features are generated, if the number of \nrequests grows, or if the number of workers in the system is \nincreased. These three situations can be produced when systems \nundergo growth.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 458,
      "chapter": null,
      "content": "Chapter 12\n[ 439 ]\n•\t\nLogs allow us to follow the execution of a single instance. When grouped together \nusing a request ID or similar, logs can be grouped by execution, allowing us \nto follow the flow of a request or task. However, logs don't directly display \naggregated information. Logs answer the question, \"what happened in this \ntask?\", but not \"what is going on in the system?\" For that kind of info, it's better \nto use metrics.\n•\t\nLogs only work retrospectively. When a problem in a task is detected, logs \ncan only show information that was prepared in advance. That's why it's \nimportant to analyze critically and refine the information, removing logs that \nare not useful and adding others with relevant contextual information to help \nreplicate the problem.\nLogs are a fantastic tool, but they need to be maintained to ensure that they can \nbe used to detect bugs and problems and allow us to take action as efficiently as \npossible.\nSummary\nIn this chapter, we started by presenting the basic elements of logs. We defined \nhow logs contain messages plus some metadata like a timestamp, and considered \nthe different severity levels. We also described the need to define request IDs to \ngroup logs related to the same task. We also discussed how, in the Twelve-Factor \nApp methodology, logs should be sent to stdout to detach log generation from \nthe process of handling and routing them to the proper destination to allow the \ncollection of all logs in the system.\nWe then showed how to produce logs in Python using the standard logging module, \ndescribing the three key elements of the logger, the handler, and the formatter. \nNext, we showed the two different errors that can be produced in a system: expected, \nunderstood as errors that were foreseen as possible and are handled; and unexpected, \nmeaning those that were not foreseen and occurred out of our control. We then went \nthrough the different strategies and cases for these.\nWe described the different severities and how to generate a strategy for what actions \nshould be taken when a log of a certain severity is detected, instead of categorizing \nthe logs in terms of \"how critical they are\", which ends up generating vague \nguidelines and not being very useful.\nThere are tools available to create metrics based on logs. \nWe will talk more about metrics in Chapter 13, Metrics.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 459,
      "chapter": null,
      "content": "Logging\n[ 440 ]\nWe discussed several habits to improve the usefulness logs by including them in \ndevelopment in a TDD workflow. This allows developers to consider the information \npresented in logs while writing tests and producing errors, which presents the \nperfect opportunity to ensure that the logs generated work correctly.\nFinally, we discussed the limitations of logs and how we can deal with them. \nIn the next chapter, we will look at how to work with aggregated information to find \nout the general state of the system through the usage of metrics.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 820,
      "extraction_method": "Direct"
    },
    {
      "page_number": 460,
      "chapter": null,
      "content": "[ 441 ]\n13\nMetrics\nAs well as logging, the other key element of observability is metrics. Metrics allow \nyou to see the general state of the system and observe trends and situations that are \nmostly caused by multiple, perhaps even many, tasks being executed at the same \ntime.\nWhen monitoring a live system, typically metrics are the main focus, as they allow \nyou to see at a glance whether everything appears to be working correctly. Normally \nwith metrics, it is possible to detect if a system is struggling, for example, for a \nsudden increase in the number of incoming requests, but also to foresee problems \nby showing trends, like a small but constant increase in the number of requests. This \nallows you to act preemptively, without waiting until a problem is serious.\nGenerating a good metric system to monitor the life of a system is invaluable to be \nable to react quickly when problems arise. Metrics can also be used as a base for \nautomatic alerts that can help warn about certain conditions taking place, typically \nsomething to investigate or correct.\nIn this chapter, we'll cover the following topics:\n•\t\nMetrics versus logs\n•\t\nGenerating metrics with Prometheus\nDuring this chapter, we will mostly use examples of web services, \nlike request metrics. Do not feel restricted by them; you can \ngenerate metrics in all kinds of services!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 461,
      "chapter": null,
      "content": "Metrics\n[ 442 ]\n•\t\nQuerying Prometheus\n•\t\nProactively working with metrics\n•\t\nAlerting\nFirst, we will take a look at metrics compared with the other main tool for \nobservability, logs.\nMetrics versus logs\nAs we saw in the previous chapter, logs are text messages produced as code is \nexecuted. They are good at giving visibility on each of the specific tasks that the \nsystem is performing, but they generate a huge amount of data, which is difficult to \ndigest in bulk. Instead, only small groups of logs are able to be analyzed at any given \ntime.\nBut sometimes the important information is not a specific request, but to understand \nthe behavior of the system as a whole. Is the load of the system growing compared to \nyesterday's? How many errors are we returning? Is the time it takes to process tasks \nincreasing? Or decreasing?\nAll those questions are impossible to answer with logs, as they require a broader \nview, at a higher level. To be able to achieve that, the data needs to be aggregated to \nbe able to understand the system as a whole.\nThe information to store in metrics is also different. While each recorded log is a text \nmessage, each produced metric is a number. These numbers will later be statistically \nprocessed to aggregate the information. \nNormally, the logs analyzed will all be related to a single task. \nWe saw in the previous chapter how to use a request ID for that. \nBut on certain occasions, it may be necessary to check all logs \nhappening in a particular time window to see crossing effects, like \na problem in one server that affects all tasks during certain times.\nWe will talk later in the chapter about the different kinds of \nnumbers that can be produced as a metric.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 462,
      "chapter": null,
      "content": "Chapter 13\n[ 443 ]\nThe difference between the amount of information produced in each record means \nthat metrics are much more lightweight compared with logs. To further reduce the \namount of data stored, the data is aggregated automatically. \nMetrics should capture and analyze information related to performance, such as the \naverage time to process a task. That allows you to detect possible bottlenecks and \nact quickly in order to improve the performance of the system. This is easier to do \nin an aggregated way, as information for a single task, like generated logs, may not \ncapture enough information to see the big picture. An important outcome of this is \nto be able to see trends and detect problems before they grow too big, remediating \nthem early. Compared to this, logs are mostly used after the fact and are difficult to \nuse as a way to take preventive action.\nKinds of metrics\nThere are different kinds of metrics that can be produced. This can be different \ndepending on the specific tool used to generate the metrics, but in general, there are \na few that are common in most systems, like the following:\n•\t\nCounter: A trigger is generated each time something happens. This will be \ncounted and aggregated as a total; for example, in a web service, the number \nof requests or the number of generated errors. Counters are useful for \nunderstanding how many times a certain action happens in the system.\n•\t\nGauge: A single number across the system. A gauge number can go up or \ndown, but the last value overwrites the previous, as it stores the general state \nof the system; for example, the number of elements in a queue, or the number \nof existing workers in the system.\n•\t\nMeasure: Events that have a numeric value associated with them. These \nnumbers can be averaged, summed, or aggregated in a certain way. \nCompared with gauges, the difference is that previous measures are still \nindependent; for example, when we emit a metric with a request time in \nmilliseconds and request size in bytes. \nThe resolution of metrics may depend on the tool and set \nconfiguration. Keep in mind that a higher resolution will require \nmore resources to store all the data. A typical resolution is one \nminute, which is small enough to present detailed information \nunless you have a very active system that routinely receives 10 \ntasks per second or more.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 463,
      "chapter": null,
      "content": "Metrics\n[ 444 ]\nMeasures can also work as counters, since each emitted event is, in essence, a \ncounter. For example, tracking the request time will also count the number of \nrequests, as it will be generated once per request. Tools will normally create \nthe associated counter automatically for every measure.\nDefining which metric is adequate for the specific value to measure is important. In \nmost cases, they'll be measures, to allow storing a value produced by events. Counters \nare normally evident (they are measures without values), while gauges are normally \nthe ones that are less obvious and can present more of a challenge on when to use \nthem.\nMetrics can also be derived from other metrics to generate new ones. For example, \nwe can divide the number of requests that return an error code by the total number \nof requests to produce an error percentage. Such derived metrics can help you \nunderstand information in a meaningful way.\nThere are also two kinds of metric systems, depending on how the metrics are \nproduced:\n•\t\nEvery time there's a metric produced, an event gets pushed toward the metrics \ncollector\n•\t\nEach system maintains its own metrics internally, which are periodically \npulled from the metrics collector\nEach system has its own pros and cons. Pushing events produces higher traffic and \nactivity, as every individual event is sent immediately, which can cause bottlenecks \nand delays. Pulling events will only sample the information, and produce lower-\nresolution data, as it can miss what happened between samples, but it's more stable \nas the number of requests is not increasing with the number of events.\nWe will use some examples with Prometheus, a metrics system that uses the pulling \napproach. The most used exponent of the push approach is Graphite.\nBoth approaches are used, but the current trend is moving toward \npulling systems. They reduce the amount of maintenance that is \nrequired for pushing systems and are easier to scale.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2079,
      "extraction_method": "Direct"
    },
    {
      "page_number": 464,
      "chapter": null,
      "content": "Chapter 13\n[ 445 ]\nGenerating metrics with Prometheus\nPrometheus is a popular metrics system that is well supported and easy to use. We \nwill use it as an example during the chapter to show how to collect metrics and how \nit interconnects with other tools to display metrics.\nAs we saw before, Prometheus uses the pulling approach to metrics generation. That \nmeans that any system that produces metrics will run its own internal Prometheus \nclient that keeps track of metrics. \nFor web services, this can be added as an extra endpoint that serves the metrics. This \nis the approach taken by the django-prometheus module, which will automatically \ncollect a lot of common metrics for a Django web service.\nPreparing the environment\nWe need to set up the environment to be sure to install all the required packages and \ndependencies of the code. \nLet's start by creating a new virtual environment, as introduced in Chapter 11, Package \nManagement, to be sure to create our own isolated sandbox to install packages:\n$ python3 -m venv venv\n$ source venv/bin/activate\nWe can now install the prepared list of requirements, stored in requirements.txt. \nThis contains the Django and Django REST framework modules, as seen in Chapter 6, \nWeb Server Structures, but also the Prometheus dependency:\n(venv) $ cat requirements.txt\ndjango\ndjango-rest-framework\ndjango-prometheus\n(venv) $ pip install -r requirements.txt\nCollecting Django\nWe will build up from the Django application code presented \nin Chapter 6, Web Server Structures, to present a working \napplication. Check the code in GitHub at https://github.com/\nPacktPublishing/Python-Architecture-Patterns/tree/\nmain/chapter_13_metrics/microposts.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1794,
      "extraction_method": "Direct"
    },
    {
      "page_number": 465,
      "chapter": null,
      "content": "Metrics\n[ 446 ]\n  Downloading Django-3.2.7-py3-none-any.whl (7.9 MB)\n     |████████████████████████████████| 7.9 MB 5.7 MB/s\n...\nInstalling collected packages: djangorestframework, django-rest-\nframework\n    Running setup.py install for django-rest-framework ... done\nSuccessfully installed django-rest-framework-0.1.0 \ndjangorestframework-3.12.4\nTo start the server, go to the micropost subdirectory and run the runserver \ncommand:\n(venv) $ python3 manage.py runserver 0.0.0.0:8000\nWatching for file changes with StatReloader\nPerforming system checks...\nSystem check identified no issues (0 silenced).\nOctober 01, 2021 - 23:24:26\nDjango version 3.2.7, using settings 'microposts.settings'\nStarting development server at http://0.0.0.0:8000/\nQuit the server with CONTROL-C.\nThe application is now accessible in the root address: http://localhost:8000, for \nexample, http://localhost:8000/api/users/jaime/collection.\nIf you remember from Chapter 3, Data Modeling, we added some initial data, so you \ncan access the URLs http://localhost:8000/api/users/jaime/collection and \nhttp://localhost:8000/api/users/dana/collection to see some data. \nNote that we started the server at address 0.0.0.0. This opens \nDjango to serve any IP address, and not only requests coming from \nlocalhost. This is an important detail that will be clarified later.\nNote also that the root address will return a 404 error, as no \nendpoint is defined there.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 466,
      "chapter": null,
      "content": "Chapter 13\n[ 447 ]\nFigure 13.1: Accessing an available URL in the application\nAccess these pages a couple of times to produce metrics that we can later access.\nConfiguring Django Prometheus\nThe configuration of the django-prometheus module is done in the microposts/\nsettings.py file, where we need to do two things.\nFirst, add the django-prometheus application to the installed app list which enables \nthe module:\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 656,
      "extraction_method": "Direct"
    },
    {
      "page_number": 467,
      "chapter": null,
      "content": "Metrics\n[ 448 ]\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'django_prometheus',\n    'rest_framework',\n    'api',\n]\nWe also need to include the proper middlewares to track requests. We need to put \none middleware at the start of the request process and another at the end, to be sure \nto capture and measure the whole process:\nMIDDLEWARE = [\n    'django_prometheus.middleware.PrometheusBeforeMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n    'django_prometheus.middleware.PrometheusAfterMiddleware',\n]\nCheck the position of django.prometheus.middleware.PrometheusBeforeMiddleware \nand django_prometheus.middleware.PrometheusAfterMiddleware.\nWith this configuration, the Prometheus collection is now enabled. But we also need \na way to access them. Remember, an important element for the Prometheus system is \nthat each application serves its own metric collection. \nIn this case, we can add an endpoint to the file microposts/url.py, which handles the \ntop-level URLs for the system:\nfrom django.contrib import admin\nfrom django.urls import include, path\nurlpatterns = [\nWe also changed the ALLOWED_HOSTS value to be '*' and allow \nrequests from any hostname. This detail will be explained a bit \nlater.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 468,
      "chapter": null,
      "content": "Chapter 13\n[ 449 ]\n    path('', include('django_prometheus.urls')),\n    path('api/', include('api.urls')),\n    path('admin/', admin.site.urls),\n]\nThe path('', include('django_prometheus.urls')) line sets up a /metrics URL \nthat we can now access.\nChecking the metrics\nThe main URL root shows that there's a new endpoint – /metrics:\nFigure 13.2: This page appears because the DEBUG mode is active.  \nRemember to deactivate it before deploying in production\nWhen accessing the /metrics endpoint, it shows all the collected metrics. Note that \nthere are a lot of metrics that are collected. This is all in text format, and it's expected \nto be collected by a Prometheus metric server.\nBe sure to access a few times the endpoints http://\nlocalhost:8000/api/users/jaime/collection and http://\nlocalhost:8000/api/users/dana/collection to produce some \nmetrics. You can check how some metrics, like django_http_\nrequests_total_by_view_transport_method_total{metho\nd=\"GET\",transport=\"http\",view=\"user-collection\"}, are \nincreasing.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 469,
      "chapter": null,
      "content": "Metrics\n[ 450 ]\nFigure 13.3: The raw Prometheus metrics, as collected by the application\nThe next step is to start a Prometheus server that can pull the info and display it.\nStarting a Prometheus server\nThe Prometheus server will pull periodically for metrics to all the configured \napplications that are collecting their metrics. These elements are called targets by \nPrometheus.\nThe easiest way to start a Prometheus server is to start the official Docker image.\nWe need to start the server, but before that, we need to set up the configuration in \nthe prometheus.yml file. You can check the example on GitHub: https://github.com/\nPacktPublishing/Python-Architecture-Patterns/blob/main/chapter_13_metrics/\nprometheus.yml:\nWe introduced Docker in Chapter 9, Microservices vs Monolith. Refer \nto that chapter for more information.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 470,
      "chapter": null,
      "content": "Chapter 13\n[ 451 ]\n# my global config\nglobal:\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. \nDefault is every 1 minute.\n  # scrape_timeout is set to the global default (10s).\nscrape_configs:\n  # The job name is added as a label `job=<job_name>` to any timeseries \nscraped from this config.\n  - job_name: \"prometheus\"\n    # metrics_path defaults to '/metrics'\n    # scheme defaults to 'http'.\n    static_configs:\n      # The target needs to point to your local IP address\n      # 192.168.1.196 IS AN EXAMPLE THAT WON'T WORK IN YOUR SYSTEM\n      - targets: [\"192.168.1.196:8000\"]\nThe config file has two main sections. The first with global indicates how often to \nscrape (to read information from the targets) and other general configuration values.\nThe second, scrape_config, describes what to scrape from, and the main parameter \nis targets. Here, we need to configure all our targets. This one in particular needs to \nbe described by its external IP, which will be the IP from your computer. \nThis address cannot be localhost, as inside the Prometheus Docker container it will \nresolve as the same container, which is not what you want. You'll need to find out \nyour local IP address. \nThis is to ensure that the Prometheus server can access the Django application that's \nrunning locally. As you remember, we opened the access allowing connections from \nany hostname with the option 0.0.0.0 when starting the server and allowing all \nhosts in the config parameter ALLOWED_HOSTS.\nIf you don't know how to find it through ipconfig or ifconfig, \nyou can check out this article on ways to find it: https://\nlifehacker.com/how-to-find-your-local-and-external-ip-\naddress-5833108. Remember that it's your local address, not the \nexternal one.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1865,
      "extraction_method": "Direct"
    },
    {
      "page_number": 471,
      "chapter": null,
      "content": "Metrics\n[ 452 ]\nDouble-check that you can access the metrics in the local IP.\nFigure 13.4: Note the IP used to access; remember that you should use your own local one\nWith all this information, you are now ready to start the Prometheus server in \nDocker, using your own config file. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 385,
      "extraction_method": "Direct"
    },
    {
      "page_number": 472,
      "chapter": null,
      "content": "Chapter 13\n[ 453 ]\nPlease note that this command requires you to find the full path to the prometheus.\nyml file. If you are in the same directory, you can address it as $(pwd)/prometheus.\nyml.\nFor this, run the following docker command, adding the whole path to the config file \nto share it with the new container:\n$ docker run -p 9090:9090  -v /full/path/to/file/prometheus.yml:/etc/\nprometheus/prometheus.yml prom/prometheus\nlevel=info ts=2021-10-02T15:24:17.228Z caller=main.go:400 msg=\"No \ntime or size retention was set so using the default time retention\" \nduration=15d\nlevel=info ts=2021-10-02T15:24:17.228Z caller=main.go:438 msg=\"Starting \nPrometheus\" version=\"(version=2.30.2, branch=HEAD, revision=b30db03f356\n51888e34ac101a06e25d27d15b476)\"\n... \nlevel=info ts=2021-10-02T15:24:17.266Z caller=main.go:794 msg=\"Server \nis ready to receive web requests.\"\nThe docker command is structured in the following way:\n•\t\n-p 9090:9090 maps the local 9090 port to the 9090 port inside the container\n•\t\n-v /full/path/to/file/prometheus.yml:/etc/prometheus/prometheus.\nyml mounts the local file (remember to add the full path or use $(pwd)/\nprometheus.yml) in the expected configuration route for Prometheus\n•\t\ndocker run prom/Prometheus is the command to run the prom/Prometheus \nimage, which is the official Prometheus image\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1425,
      "extraction_method": "Direct"
    },
    {
      "page_number": 473,
      "chapter": null,
      "content": "Metrics\n[ 454 ]\nAfter the Prometheus server is up and running, the server is accessible at http://\nlocalhost:9090.\nFigure 13.5: The empty graph Prometheus page\nFrom here, we can start querying the system.\nQuerying Prometheus\nPrometheus has its own query system, called PromQL, and ways of operating with \nmetrics that, while powerful, can be a little confusing at the beginning. Part of it is its \npull approach to metrics.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 474,
      "chapter": null,
      "content": "Chapter 13\n[ 455 ]\nFor example, requesting one useful metric, like django_http_requests_latency_\nseconds_by_view_method_count, will display how many times each view has been \ncalled for each method.\nFigure 13.6: Notice how the prometheus-django-metrics view is called more often, as it is called  \nautomatically by Prometheus once every 15 seconds to scrape the results\nThis is presented as an accumulated value that grows over time. This is not very \nuseful, as it's difficult to make sense of what exactly it means.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 475,
      "chapter": null,
      "content": "Metrics\n[ 456 ]\nInstead, the value is more likely to be presented as a rate, representing how many \nrequests have been detected per second. For example, with a resolution of 1 minute, \nrate(django_http_requests_latency_seconds_by_view_method_count[1m]) shows \nthe following graph instead:\nFigure 13.7: Note that the different methods and views are displayed as different lines\nAs you can see, there's a constant number of requests from prometheus-django-\nmetrics, which is Prometheus requesting the metrics information. This happens once \nevery 15 seconds, or approximately 0.066 times per second.\nIn the graph, there's also another spike of the user-collection method happening at \n15:55, at the time where we manually generated some requests to the service. As you \ncan see, the resolution is per minute, as described in the rate.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 934,
      "extraction_method": "Direct"
    },
    {
      "page_number": 476,
      "chapter": null,
      "content": "Chapter 13\n[ 457 ]\nIf we want to aggregate all of this in a single graph, we can use the sum operator, \nspecifying what we want to aggregate from. To sum all GET requests, for example, \nwith:\nsum(rate(django_http_requests_latency_seconds_by_view_method_\ncount[1m])) by (method)\nThis produces this other graph: \nFigure 13.8: Note the bottom value is based on the baseline created by the calls to prometheus-django-metrics\nTo plot times instead, the metric to use is the django_http_requests_latency_\nseconds_by_view_method_bucket one. The bucket metrics are generated in a way \nthat can be combined with the histogram_quantile function to display a particular \nquantile, which is useful for giving a proper feeling of times.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 825,
      "extraction_method": "Direct"
    },
    {
      "page_number": 477,
      "chapter": null,
      "content": "Metrics\n[ 458 ]\nTo plot the 0.95 quantile over a period of 5 minutes, the following query can be \ngenerated:\nhistogram_quantile(0.95, rate(django_http_requests_latency_seconds_by_\nview_method_bucket[5m]))\nWhen you run it, you should receive the following:\nFigure 13.9: Note how the metrics collection is much faster than the user-collection requests\nFor example, quantile 0.95 means that the time is the highest of \n95% of the requests. This is more useful than creating averages as \nthey can get skewed by high numbers. Instead, you can draw the \nquantile 0.50 (the maximum time it takes for half of the requests), \nthe quantile 0.90 (the maximum time for most of the requests), and \nquantile 0.99 for the very top time it takes to return a request. This \nallows you to get a better picture, as it's different from the situation \nof growing quantile 0.50 (most requests take longer to return) with \ngrowing quantile 0.99 (some slow queries are getting worse).\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1062,
      "extraction_method": "Direct"
    },
    {
      "page_number": 478,
      "chapter": null,
      "content": "Chapter 13\n[ 459 ]\nTo plot times instead, the metric to use is the django_http_requests_latency_\nseconds_by_view_method_bucket one. The bucket metrics are generated in a way \nthat can be combined with the histogram_quantile function to display a particular \nquantile, which is useful for giving a proper feeling of times.\nMetrics can also be filtered to display only specific labels, and a good number of \nfunctions to multiply, divide, add, create averages, and all kinds of operations are \navailable.\nThe interface has autocompleted, which can help you find certain metrics.\nCheck the Prometheus documentation about queries to find out more: https://\nprometheus.io/docs/prometheus/latest/querying/basics/.\nProactively working with metrics\nAs we've seen, metrics show an aggregated point of view for the status of the whole \ncluster. They allow you to detect trending problems, but it's difficult to pinpoint a \nsingle spurious error.\nThis shouldn't stop us from considering them as a critical tool for successful \nmonitoring because they can tell whether the whole system is healthy. In some \ncompanies, the most critical metrics are on permanent display on screens so the \noperations team can see them and react quickly to any sudden problem.\nPrometheus queries can be a bit long and complicated when trying \nto display the result of several metrics, such as the percentage of \nsuccessful requests over the total. Be sure to test that the result is \nwhat you expect it to be and allocate time to tweak the queries later \nto keep improving them.\nPrometheus is normally paired with Grafana. Grafana is an open \nsource, interactive visualization tool that can be connected with \nPrometheus to create rich dashboards. This leverages the collection \nof metrics and helps visualize the state of the system in a much \nmore understandable way. Describing how to use Grafana is out \nof scope for this book, but using it to display metrics is highly \nrecommended: https://grafana.com/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2080,
      "extraction_method": "Direct"
    },
    {
      "page_number": 479,
      "chapter": null,
      "content": "Metrics\n[ 460 ]\nFinding the proper balance of what metrics are the key ones for a service is not as \nstraightforward as it seems, and it will require time and experience, perhaps even \ntrial and error. \nThere are, though, four metrics for online services that are considered always \nimportant. They are:\n•\t\nLatency: How many milliseconds it takes for the system to respond to a \nrequest. Depending on the service, sometimes seconds can be used instead. \nIn my experience, milliseconds are typically the adequate time scale, as most \nweb applications take between 50 ms and 1 second to respond, depending on \nthe request. Requests taking longer than 1 second are typically rarer, though \nthere are always some, depending on the system.\n•\t\nTraffic: The number of requests flowing through the system per unit of time, \nfor example, the number of requests per minute.\n•\t\nErrors: The percentage of requests received that return an error.\n•\t\nSaturation: Describing whether the capacity of the cluster has enough \nheadroom. This includes elements as available hard drive space, memory, \nand so on. For example, there's 15% available RAM in the system.\nThese metrics can be found in the Google SRE book as the four golden signals and are \nrecognized as the most important high-level elements for successful monitoring.\nAlerting\nWhen problems are detected through the metrics, an automatic alert should be \ntriggered. Prometheus has an alert system that will raise when a defined metric \nfulfills the defined alert.\nThe main tool to check saturation is the multiple default \nexporters available to collect most of the hardware information \nautomatically, like memory, CPU, and hard drive space. When \nusing a cloud provider, normally they expose their own set of \nrelated metrics, like CloudWatch in AWS.\nCheck out the Prometheus documentation on alerting for more \ninformation: https://prometheus.io/docs/alerting/latest/\noverview/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2026,
      "extraction_method": "Direct"
    },
    {
      "page_number": 480,
      "chapter": null,
      "content": "Chapter 13\n[ 461 ]\nNormally, alerts will be configured when the value of metrics is crossing some \nthreshold. For example, the number of errors is higher than X, or the time to return a \nrequest is too high.\nThe built-in Alertmanager can alert in some ways, like sending an email, but it can \nalso be connected to other tools to perform more complex actions. For example, \nconnecting to an integrated incident solution like Opsgenie (https://www.atlassian.\ncom/software/opsgenie) allows you to create alert flows, such as sending emails and \nSMS, calls.\nAlerting, as with metrics, is an ongoing process. Some key thresholds won't be \nevident at the start of the system, and only experience will allow you to discover \nthem. In the same way, it's very likely that some alerts are created that don't require \nactive monitoring, and should be disconnected to ensure that the alerts in the system \nare on point and have a high signal-to-noise ratio.\nSummary\nIn this chapter, we described what metrics are and how they compare with logs. We \ndescribed how metrics are useful to analyze the general state of the system, while \nlogs describe specific tasks, being more difficult to describe the aggregated situation.\nWe enumerated different kinds of metrics that can be produced and described \nPrometheus, a common metrics system that uses the pull approach on how to \ncapture metrics.\nAn alert could also be that some element is too low; for example, \nif the number of requests in a system falls to zero, that could be an \nindication that the system is down.\nWhile alerts can be generated directly from metrics, there are \ntools that allow you also to generate alerts from logs directly. For \nexample, Sentry (https://sentry.io/) will aggregate errors \nbased on logs and can set up thresholds to escalate toward more \nactive alerts, like sending emails. \nAnother alternative is to derivate metrics from logs using external \nlogging systems. This allows you, for example, to create a counter \nbased on the number of ERROR logs, or more complicated metrics. \nThese systems, once more, allow you to trigger alerts based on \nthese derived metrics.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 481,
      "chapter": null,
      "content": "Metrics\n[ 462 ]\nWe set an example of how to generate metrics automatically in Django by installing \nand configuring the django-prometheus module, and how to start a Prometheus \nserver that scrapes the generated metrics.\nNext, we described how to query metrics in Prometheus, introducing PromQL, and \nshowed some common examples of how to display metrics, plot rate to see clearly \nhow the metrics are changing over time, and how to use the histogram_quantile \nfunction to work with times.\nWe also showed in the chapter how to work proactively to detect common problems \nas soon as possible and what the four golden signals are, as described by Google. \nFinally, we introduced alerts as a way to be notified when metrics are out of a normal \nmargin. Using alerts is a smart way to be notified without having to manually look at \nmetrics.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\nKeep in mind that you can also generate your own custom \nmetrics, not having to only rely on the ones in an external module. \nCheck the Prometheus client to see how, for example, for Python: \nhttps://github.com/prometheus/client_python.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1334,
      "extraction_method": "Direct"
    },
    {
      "page_number": 482,
      "chapter": null,
      "content": "[ 463 ]\n14\nProfiling\nIt is quite common that written code doesn't behave perfectly after being tested with \nreal data. Other than bugs, we can find the problem that the performance of the code \nis not adequate. Perhaps some requests are taking too much time, or perhaps the \nusage of memory is too high.\nIn those cases, it's difficult to know exactly what the key elements are, that are taking \nthe most time or memory. While it's possible to try to follow the logic, normally once \nthe code is released, the bottlenecks will be at points that are almost impossible to \nknow beforehand.\nTo get information on what exactly is going on and follow the code flow, we can use \nprofilers to dynamically analyze the code and better understand how the code is \nexecuted, in particular, where most time is spent. This can lead to adjustments and \nimprovements affecting the most significant elements of the code, driven by data, \ninstead of vague speculation.\nIn this chapter, we'll cover the following topics:\n•\t\nProfiling basics\n•\t\nTypes of profilers\n•\t\nProfiling code for time\n•\t\nPartial profiling\n•\t\nMemory profiling\nFirst, we will take a look at the basic principles of profiling.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 483,
      "chapter": null,
      "content": "Profiling\n[ 464 ]\nProfiling basics\nProfiling is a dynamic analysis that instruments code to understand how it runs. \nThis information is extracted and compiled in a way that can be used to get a better \nknowledge of a particular behavior based on a real case, as the code is running as \nusual. This information can be used to improve the code.\nThe normal application of profiling is to improve the performance of the code under \nanalysis. By understanding how it executes in practice, it sheds light on the dynamics \nof the code modules and parts that could be causing problems. Then, actions can be \ntaken in those specific areas.\nPerformance can be understood in two ways: either time performance (how long \ncode takes to execute) or memory performance (how much memory the code takes to \nexecute). Both can be bottlenecks. Some code may take too long to execute or use a \nlot of memory, which may limit the hardware where it's executed.\nA common case in software development is that you don't really know what your \ncode is going to do until it gets executed. Clauses to cover corner cases that appear \nrare may execute much more than expected, and software works differently when \nthere are big arrays, as some algorithms may not be adequate. \nCertain static analysis tools, as opposed to dynamic, can provide \ninsight into aspects of the code. For example, they can be used \nto detect if certain code is dead code, meaning it's not called \nanywhere in the whole code. Or, they can detect some bugs, like \nthe usage of variables that haven't been defined before, like when \nhaving a typo. But they don't work with the specifics of code that's \nactually being run. Profiling will bring specific data based on the \nuse case instrumented and will return much more information on \nthe flow of the code.\nWe will focus more on time performance in this chapter, as it is \ntypically a bigger problem, but we will also explain how to use a \nmemory profiler.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 484,
      "chapter": null,
      "content": "Chapter 14\n[ 465 ]\nThe problem is that doing that analysis before having the system running is \nincredibly difficult, and at most times, futile, as the problematic pieces of code will \nvery likely be completely unexpected.\nProgrammers waste enormous amounts of time thinking about, or worrying about, \nthe speed of noncritical parts of their programs, and these attempts at efficiency \nactually have a strong negative impact when debugging and maintenance are \nconsidered. We should forget about small efficiencies, say about 97% of the time: \npremature optimization is the root of all evil. Yet we should not pass up our \nopportunities in that critical 3%.\nDonald Knuth – Structured Programing with GOTO Statements - 1974.\nProfiling gives us the ideal tool to not prematurely optimize, but to optimize \naccording to real, tangible data. The idea is that you cannot optimize what you \ncannot measure. The profiler measures so it can be acted upon.\nProfiling can be achieved in different ways, each with its pros and cons.\nTypes of profilers\nThere are two main kinds of time profilers:\n•\t\nDeterministic profilers, through a process of tracing. A deterministic profiler \ninstruments the code and records each individual command. This makes \ndeterministic profilers very detailed, as they can follow up the code on each \nstep, but at the same time, the code is executed slower than without the \ninstrumentation.\nThe famous quote above is sometimes reduced to \"premature \noptimization is the root of all evil,\" which is a bit reductionist \nand doesn't carry the nuance. Sometimes it's important to design \nelements with care and it's possible to plan in advance. As good as \nprofiling (or other techniques) may be, they can only go so far. But \nit's important to understand, on most occasions, it's better to take \nthe simple approach, as performance will be good enough, and it \nwill be possible to improve it later in the few cases when it's not.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2046,
      "extraction_method": "Direct"
    },
    {
      "page_number": 485,
      "chapter": null,
      "content": "Profiling\n[ 466 ]\n•\t\nDeterministic profilers are not great to execute continuously. Instead, they \ncan be activated in specific situations, like while running specific tests offline, \nto find out problems.\n•\t\nStatistical profiles, through sampling. This kind of profiler, instead of \ninstrumenting the code and detecting each operation, awakes at certain \nintervals and takes a sample of the current code execution stack. If this \nprocess is done for long enough, it captures the general execution of the \nprogram. \nWhile they don't give as detailed information as deterministic profiles, \nstatistical profilers are much more lightweight and don't consume many \nresources. They can be enabled to constantly monitor live systems without \ninterfering with their performance.\nStatistical profilers only make sense on systems that are under relative \nload, as in a system that is not stressed, they'll show that most time is spent \nwaiting.\nStatistical profilers can be internal, if the sampling is done directly on the \ninterpreter, or even external if it's a different program that is taking the \nsamples. An external profiler has the advantage that, even if there's any \nproblem with the sampling process, it won't interfere with the program being \nsampled.\nBoth profilers can be seen as complementary. Statistical profilers are good tools for \nunderstanding the most-visited parts of the code and where the system, aggregated, \nis spending time. They live in the live system, where the real case usages determine \nthe behavior of the system.\nThe deterministic profilers are tools for analyzing specific use cases in the petri dish \nof the developer's laptop, where a specific task that is having some problem can be \ndissected and analyzed carefully, to be improved.\nTaking a sample of the stack is similar to taking a picture. \nImagine a train or subway hall where people are moving \nacross to go from one platform to another. Sampling is \nanalogous to taking pictures at periodic intervals, for \nexample, once every 5 minutes. Sure, it's not possible \nto get exactly who comes from one platform and goes \nto another, but after a whole day, it will provide good \nenough information on how many people have been \naround and what platforms are the most popular.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 486,
      "chapter": null,
      "content": "Chapter 14\n[ 467 ]\nTypically, code will present hotspots, slow parts of it that get executed often. Finding \nthe specific parts to focus attention on and then act on them is a great way to \nimprove the overall speed.\nThese hotspots can be revealed by profiling, either by checking the global hotspots \nusing a statistical profiler or the specific hotspots for a task with a deterministic \nprofiler. The first will display the specific parts of the code that are most used in \ngeneral, which allows us to understand the pieces that get hit more often and take \nthe most time in aggregate. The deterministic profiler can show, for a specific task, \nhow long it takes for each line of code, and determine what are the slow elements.\nAnother kind of profiler is the memory profiler. A memory profiler records when \nmemory is increased and decreased, tracking the usage of memory. Profiling \nmemory is typically used to find out memory leaks, which are rare for a Python \nprogram, but they can happen.\nPython has a garbage collector that releases memory automatically when an object \nis not referenced anymore. This happens without having to take any action, so \ncompared with programs with manual memory assignment, like C/C++, the \nmemory management is easier to handle. The garbage collection mechanism used \nfor Python is called reference counting, and it frees memory immediately once a \nmemory object is not used by anyone, as compared with other kinds of garbage \ncollectors that wait.\nIn the case of Python, memory leaks can be created by three main use cases, from \nmore likely to least:\n•\t\nSome objects are still referenced, even if they are not used anymore. This can \ntypically happen if there are long-lived objects that keep small elements in \nbig elements, like lists of dictionaries when they are added and not removed.\nIn some respects, statistical profilers are analogous to metrics \nand deterministic profilers to logs. One displays the aggregated \nelements and the other the specific elements. Deterministic \nprofilers, contrary to logs, are not ideal tools for using in live \nsystems without care, though.\nWe won't look at statistical profilers as they require systems that \nare under load and they are difficult to create in a test that's fit for \nthe scope of this book. You can check py-spy (https://pypi.\norg/project/py-spy/) or pyinstrument (https://pypi.org/\nproject/pyinstrument/).\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 487,
      "chapter": null,
      "content": "Profiling\n[ 468 ]\n•\t\nAn internal C extension is not managing the memory correctly. This may \nrequire further investigation with specific C profiling tools, which is out of \nscope for this book.\n•\t\nComplex reference cycles. A reference cycle is a group of objects that \nreference each other, e.g. object A references B and object B references \nA. While Python has algorithms to detect them and release the memory \nnonetheless, there's the small possibility that the garbage collector is disabled \nor any other bug problem. You can see more information on the Python \ngarbage collector here: https://docs.python.org/3/library/gc.html.\nThe most likely situation for extra usage of memory is an algorithm that uses a lot of \nmemory, and detecting when the memory is allocated can be achieved with the help \nof a memory profiler.\nLet's introduce some code and profile it.\nProfiling code for time\nWe will start by creating a short program that will calculate and display all prime \nnumbers up to a particular number. Prime numbers are numbers that are only \ndivisible by themselves and one.\nWe will start by taking a naïve approach first:\ndef check_if_prime(number):\n    result = True\n    for i in range(2, number):\n        if number % i == 0:\n            result = False\n    return result\nThis code will take every number from 2 to the number under test (without \nincluding it), and check whether the number is divisible. If at any point it is divisible, \nthe number is not a prime number.\nMemory profiling is typically more complicated and takes more \neffort than time profiling.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1676,
      "extraction_method": "Direct"
    },
    {
      "page_number": 488,
      "chapter": null,
      "content": "Chapter 14\n[ 469 ]\nTo calculate all the way from 1 to 5,000, to verify that we are not making any \nmistakes, we will include the first prime numbers lower than 100 and compare \nthem. This is on GitHub, available as primes_1.py at https://github.com/\nPacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_\nprofiling/primes_1.py.\nPRIMES = [1, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, \n53,\n          59, 61, 67, 71, 73, 79, 83, 89, 97]\nNUM_PRIMES_UP_TO = 5000\ndef check_if_prime(number):\n    result = True\n    for i in range(2, number):\n        if number % i == 0:\n            result = False\n    return result\nif __name__ == '__main__':\n    # Calculate primes from 1 to NUM_PRIMES_UP_TO\n    primes = [number for number in range(1, NUM_PRIMES_UP_TO)\n              if check_if_prime(number)]\n    # Compare the first primers to verify the process is correct\n    assert primes[:len(PRIMES)] == PRIMES\n    print('Primes')\n    print('------')\n    for prime in primes:\n        print(prime)\n    print('------')\nThe calculation of prime numbers is performed by creating a list of all numbers \n(from 1 to NUM_PRIMES_UP_TO) and verifying each of them. Only values that return \nTrue will be kept:\n    # Calculate primes from 1 to NUM_PRIMES_UP_TO\n    primes = [number for number in range(1, NUM_PRIMES_UP_TO)\n              if check_if_prime(number)]\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1463,
      "extraction_method": "Direct"
    },
    {
      "page_number": 489,
      "chapter": null,
      "content": "Profiling\n[ 470 ]\nThe next line asserts that the first prime numbers are the same as the ones defined in \nthe PRIMES list, which is a hardcoded list of the first primes lower than 100.\nassert primes[:len(PRIMES)] == PRIMES\nThe primes are finally printed. Let's execute the program, timing its execution:\n$ time python3 primes_1.py\nPrimes\n------\n1\n2\n3\n5\n7\n11\n13\n17\n19\n… \n4969\n4973\n4987\n4993\n4999\n------\nReal      0m0.875s\nUser      0m0.751s\nsys 0m0.035s\nFrom here, we will start analyzing the code to see what is going on internally and see \nif we can improve it.\nUsing the built-in cProfile module\nThe easiest, faster way of profiling a module is to directly use the included cProfile \nmodule in Python. This module is part of the standard library and can be called as \npart of the external call, like this:\n$ time python3 -m cProfile primes_1.py\nPrimes\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 955,
      "extraction_method": "Direct"
    },
    {
      "page_number": 490,
      "chapter": null,
      "content": "Chapter 14\n[ 471 ]\n------\n1\n2\n3\n5\n...\n4993\n4999\n------\n         5677 function calls in 0.760 seconds\n   Ordered by: standard name\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.002    0.002    0.757    0.757 primes_1.\npy:19(<listcomp>)\n        1    0.000    0.000    0.760    0.760 primes_1.py:2(<module>)\n     4999    0.754    0.000    0.754    0.000 primes_1.py:7(check_if_\nprime)\n        1    0.000    0.000    0.760    0.760 {built-in method \nbuiltins.exec}\n        1    0.000    0.000    0.000    0.000 {built-in method \nbuiltins.len}\n      673    0.004    0.000    0.004    0.000 {built-in method \nbuiltins.print}\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_\nlsprof.Profiler' objects}\nReal      0m0.895s\nUser      0m0.764s\nsys 0m0.032s\nNote this called the script normally, but also presented the profile analysis. The table \nshows:\n•\t\nncalls: Number of times each element has been called\n•\t\ntottime: Total time spent on each element, not including sub calls\n•\t\npercall: Time per call on each element (not including sub calls)\n•\t\ncumtime: Cumulative time – the total time spent on each element, including \nsubcalls\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 491,
      "chapter": null,
      "content": "Profiling\n[ 472 ]\n•\t\npercall: Time per call on an element, including subcalls\n•\t\nfilename:lineno: Each of the elements under analysis\nIn this case, the time is clearly seen to be spent in the check_if_prime function, which \nis called 4,999 times, and it takes the practical totality of the time (744 milliseconds \ncompared with a total of 762).\nWhile this text table can be enough for simple scripts like this one, the output can be \npresented as a file and then displayed with other tools:\n$ time python3 -m cProfile -o primes1.prof  primes_1.py\n$ ls primes1.prof\nprimes1.prof\nNow we need to install the visualizer SnakeViz, installing it through pip:\n$ pip3 install snakeviz\nFinally, open the file with snakeviz, which will open a browser with the information:\n$ snakeviz primes1.prof\nsnakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit\nhttp://127.0.0.1:8080/snakeviz/%2FUsers%2Fjaime%2FDropbox%2FPackt%2Farc\nhitecture_book%2Fchapter_13_profiling%2Fprimes1.prof\nWhile not easy to see here due to the fact that it's a small script, \ncProfile increases the time it takes to execute the code. There's an \nequivalent module called profile that's a direct replacement but \nimplemented in pure Python, as opposed to a C extension. Please \ngenerally use cProfile as it's faster, but profile can be useful at \ncertain moments, like when trying to extend the functionality.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 492,
      "chapter": null,
      "content": "Chapter 14\n[ 473 ]\nFigure 14.1: Graphical representation of the profiling information. The full page is too big to fit  \nhere and has been cropped purposefully to show some of the info.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 493,
      "chapter": null,
      "content": "Profiling\n[ 474 ]\nThis graph is interactive, and we can click and hover on different elements to get \nmore information:\nFigure 14.2: Checking the information about check_if_prime. The full page is too big to  \nfit here and has been cropped purposefully to show some of the info.\nWe can confirm here that the bulk of the time is spent on check_if_prime, but we \ndon't get information about what's inside it.\nThis is because cProfile only has function granularity. You'll see how long each \nfunction call takes, but not a lower resolution. For this specifically simple function, \nthis may not be enough.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 494,
      "chapter": null,
      "content": "Chapter 14\n[ 475 ]\nWe will see how to use a profiler that has a higher resolution, analyzing each line \nof code.\nLine profiler\nTo analyze the check_if_prime function, we need to first install the module line_\nprofiler\n$ pip3 install line_profiler\nAfter it's installed, we will make a small change in the code, and save it as primes_2.\npy. We will add the decorator @profile for the check_if_prime function, to indicate \nto the line profiler to look into it.\nThe code will be like this (the rest will be unaffected). You can check the whole \nfile on GitHub at https://github.com/PacktPublishing/Python-Architecture-\nPatterns/blob/main/chapter_14_profiling/primes_2.py.\n@profile\ndef check_if_prime(number):\n    result = True\n    for i in range(2, number):\n        if number % i == 0:\n            result = False\n    return result\nDo not underestimate this tool. The code example presented is \npurposefully simple to avoid spending too much time explaining \nits use. Most of the time, localizing the function that's taking most \nof the time is good enough to visually inspect it and discover \nwhat's taking too long. Keep in mind that, in most practical \nsituations, the time spent will be on external calls like DB accesses, \nremote requests, etc.\nKeep in mind that you should only profile sections of the code \nwhere you want to know more in this way. If all the code was \nprofiled in this way, it would take a lot of time to analyze.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 495,
      "chapter": null,
      "content": "Profiling\n[ 476 ]\nExecute the code now with kernprof, which will be installed after the installation of \nline_profiler.\n$ time kernprof -l primes_2.py\nPrimes\n------\n1\n2\n3\n5\n…\n4987\n4993\n4999\n------\nWrote profile results to primes_2.py.lprof\nReal      0m12.139s\nUser      0m11.999s\nsys 0m0.098s\nNote the execution took noticeably longer – 12 seconds compared with subsecond \nexecution without the profiler enabled. Now we can take a look at the results with \nthis command:\n$ python3 -m line_profiler primes_2.py.lprof\nTimer unit: 1e-06 s\nTotal time: 6.91213 s\nFile: primes_2.py\nFunction: check_if_prime at line 7\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     7                                           @profile\n     8                                           def check_if_\nprime(number):\n     9      4999       1504.0      0.3      0.0      result = True\n    10\n    11  12492502    3151770.0      0.3     45.6      for i in range(2, \nnumber):\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 496,
      "chapter": null,
      "content": "Chapter 14\n[ 477 ]\n    12  12487503    3749127.0      0.3     54.2          if number % i \n== 0:\n    13     33359       8302.0      0.2      0.1              result = \nFalse\n    14\n    15      4999       1428.0      0.3      0.0      return result\nHere, we can start analyzing the specifics of the algorithm used. The main problem \nseems to be that we are doing a lot of comparisons. Both lines 11 and 12 are being \ncalled too many times, though the time per hit is short. We need to find a way to \nreduce the number of times they're being called.\nThe first one is easy. Once we find a False result, we don't need to wait anymore; \nwe can return directly, instead of continuing with the loop. The code will be like this \n(stored in primes_3.py, available at https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/blob/main/chapter_14_profiling/primes_3.py):\n@profile\ndef check_if_prime(number):\n    for i in range(2, number):\n        if number % i == 0:\n            return False\n    return True\nLet's take a look at the profiler result.\n$ time kernprof -l primes_3.py\n... \nReal      0m2.117s\nUser      0m1.713s\nsys       0m0.116s\n$ python3 -m line_profiler primes_3.py.lprof\nTimer unit: 1e-06 s\nTotal time: 0.863039 s\nFile: primes_3.py\nFunction: check_if_prime at line 7\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1382,
      "extraction_method": "Direct"
    },
    {
      "page_number": 497,
      "chapter": null,
      "content": "Profiling\n[ 478 ]\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     7                                           @profile\n     8                                           def check_if_\nprime(number):\n     9\n    10   1564538     388011.0      0.2     45.0      for i in range(2, \nnumber):\n    11   1563868     473788.0      0.3     54.9          if number % i \n== 0:\n    12      4329       1078.0      0.2      0.1              return \nFalse\n    13\n    14       670        162.0      0.2      0.0      return True\nWe see how time has gone down by a big factor (2 seconds compared with the 12 \nseconds before, as measured by time) and we see the great reduction in time spent \non comparisons (3,749,127 microseconds before, and then 473,788 microseconds), \nmainly due to the fact there are 10 times fewer comparisons, 1,563,868 compared \nwith 12,487,503.\nWe can also improve and further reduce the number of comparisons by limiting the \nsize of the loop.\nRight now, the loop will try to divide the source number between all the numbers up \nto itself. For example, for 19, we try these numbers (as 19 is a prime number, it's not \ndivisible by any except for itself).\nDivide 19 between\n[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nTrying all these numbers is not necessary. At least, we can skip half of them, as no \nnumber will be divisible by a number higher than half itself. For example, 19 divided \nby 10 or higher is less than 2.\nDivide 19 between\n[2, 3, 4, 5, 6, 7, 8, 9, 10]\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1680,
      "extraction_method": "Direct"
    },
    {
      "page_number": 498,
      "chapter": null,
      "content": "Chapter 14\n[ 479 ]\nFurthermore, any factor of a number will be lower than its square root. This can be \nexplained as follows: If a number is the factor of two or more numbers, the highest \nthey may be is the square root of the whole number. So we check only the numbers \nup to the square root (rounded down):\nDivide 19 between\n[2, 3, 4]\nBut we can reduce it even further. We only need to check the odd numbers after 2, as \nany even number will be divisible by 2. So, in this case, we even reduce it further.\nDivide 19 between\n[2, 3]\nTo apply all of this, we need to tweak the code again and store it in primes_4.py, \navailable on GitHub at https://github.com/PacktPublishing/Python-Architecture-\nPatterns/blob/main/chapter_14_profiling/primes_4.py:\ndef check_if_prime(number):\n    if number % 2 == 0 and number != 2:\n        return False\n    for i in range(3, math.floor(math.sqrt(number)) + 1, 2):\n        if number % i == 0:\n            return False\n    return True\nThe code always checks for divisibility by 2, unless the number is 2. This is to keep \nreturning 2 correctly as a prime.\nThen, we create a range of numbers that starts from 3 (we already tested 2) and \ncontinue until the square root of the number. We use the math module to perform \nthe action and to floor the number to the nearest lower integer. The range function \nrequires a +1 of this number, as it doesn't include the defined number. Finally, the \nrange step on 2 integers at  time so that all the numbers are odd, since we started \nwith 3.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 499,
      "chapter": null,
      "content": "Profiling\n[ 480 ]\nFor example, to test a number like 1,000, this is the equivalent code.\n>>> import math\n>>> math.sqrt(1000)\n31.622776601683793\n>>> math.floor(math.sqrt(1000))\n31\n>>> list(range(3, 31 + 1, 2))\n[3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\nNote that 31 is returned as we added the +1.\nLet's profile the code again.\n$ time kernprof -l primes_4.py\nPrimes\n------\n1\n2\n3\n5\n…\n4973\n4987\n4993\n4999\n------\nWrote profile results to primes_4.py.lprof\nReal      0m0.477s\nUser      0m0.353s\nsys       0m0.094s\nWe see another big increase in performance. Let's see the line profile.\n$ python3 -m line_profiler primes_4.py.lprof\nTimer unit: 1e-06 s\nTotal time: 0.018276 s\nFile: primes_4.py\nFunction: check_if_prime at line 8\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 837,
      "extraction_method": "Direct"
    },
    {
      "page_number": 500,
      "chapter": null,
      "content": "Chapter 14\n[ 481 ]\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     8                                           @profile\n     9                                           def check_if_\nprime(number):\n    10\n    11      4999       1924.0      0.4     10.5      if number % 2 == 0 \nand number != 2:\n    12      2498        654.0      0.3      3.6          return False\n    13\n    14     22228       7558.0      0.3     41.4      for i in range(3, \nmath.floor(math.sqrt(number)) + 1, 2):\n    15     21558       7476.0      0.3     40.9          if number % i \n== 0:\n    16      1831        506.0      0.3      2.8              return \nFalse\n    17\n    18       670        158.0      0.2      0.9      return True\nWe've reduced the number of loop iterations drastically to 22,228, from 1.5 million in \nprimes_3.py and over 12 million in primes_2.py, when we started the line profiling. \nThat's some serious improvement!\nThe line approach should be used only for small sections. In general, we've seen how \ncProfile can be more useful, as it's easier to run and gives information. \nPrevious sections have assumed that we are able to run the whole script and then \nreceive the results, but that may not be correct. Let's take a look at how to profile in \nsections of the program, for example, when a request is received.\nPartial profiling \nIn many scenarios, profilers will be useful in environments where the system is in \noperation and we cannot wait until the process finishes before obtaining profiling \ninformation. Typical scenarios are web requests.\nYou can try to do the test to increase NUM_PRIMES_UP_TO in \nprimes_2.py and primes_4.py and compare them. The change \nwill be clearly perceptible.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 501,
      "chapter": null,
      "content": "Profiling\n[ 482 ]\nIf we want to analyze a particular web request, we may need to start a web server, \nproduce a single request, and stop the process to obtain the result. This doesn't work \nas well as you may think due to some problems that we will see.\nBut first, let's create some code to explain this situation.\nExample web server returning prime numbers\nWe will use the final version of the function check_if_prime and create a web service \nthat returns all the primes up to the number specified in the path of the request. The \ncode will be the following, and it's fully available in the server.py file on GitHub at \nhttps://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/\nchapter_14_profiling/server.py.\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nimport math\ndef check_if_prime(number):\n    if number % 2 == 0 and number != 2:\n        return False\n    for i in range(3, math.floor(math.sqrt(number)) + 1, 2):\n        if number % i == 0:\n            return False\n    return True\ndef prime_numbers_up_to(up_to):\n    primes = [number for number in range(1, up_to + 1)\n              if check_if_prime(number)]\n    return primes\ndef extract_param(path):\n    '''\n    Extract the parameter and transform into\n    a positive integer. If the parameter is\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 502,
      "chapter": null,
      "content": "Chapter 14\n[ 483 ]\n    not valid, return None\n    '''\n    raw_param = path.replace('/', '')\n    # Try to convert in number\n    try:\n        param = int(raw_param)\n    except ValueError:\n        return None\n    # Check that it's positive\n    if param < 0:\n        return None\n    return param\ndef get_result(path):\n    param = extract_param(path)\n    if param is None:\n        return 'Invalid parameter, please add an integer'\n    return prime_numbers_up_to(param)\nclass MyServer(BaseHTTPRequestHandler):\n    def do_GET(self):\n        result = get_result(self.path)\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.end_headers()\n        return_template = '''\n            <html>\n                <head><title>Example</title></head>\n                <body>\n                    <p>Add a positive integer number in the path to \ndisplay\n                    all primes up to that number</p>\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1033,
      "extraction_method": "Direct"
    },
    {
      "page_number": 503,
      "chapter": null,
      "content": "Profiling\n[ 484 ]\n                    <p>Result {result}</p>\n                </body>\n            </html>\n        '''\n        body = bytes(return_template.format(result=result), 'utf-8')\n        self.wfile.write(body)\nif __name__ == '__main__':\n    HOST = 'localhost'\n    PORT = 8000\n    web_server = HTTPServer((HOST, PORT), MyServer)\n    print(f'Server available at http://{HOST}:{PORT}')\n    print('Use CTR+C to stop it')\n    # Capture gracefully the end of the server by KeyboardInterrupt\n    try:\n        web_server.serve_forever()\n    except KeyboardInterrupt:\n        pass\n    web_server.server_close()\n    print(\"Server stopped.\")\nThe code is better understood if you start from the end. The final block creates a \nweb server using the base HTTPServer definition in the Python module http.server. \nPreviously, we created the class MyServer, which defines what to do if there's a GET \nrequest in the do_GET method.\nThe do_GET method returns an HTML response with the result calculated by get_\nresult. It adds all the required headers and formats the body in HTML.\nThe interesting bits of the process happen in the next functions.\nget_result is the root one. It first calls extract_param to get a number, up to which to \ncalculate the threshold number for us to calculate primes up to. If correct, then that's \npassed to prime_numbers_up_to.\ndef get_result(path):\n    param = extract_param(path)\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 504,
      "chapter": null,
      "content": "Chapter 14\n[ 485 ]\n    if param is None:\n        return 'Invalid parameter, please add an integer'\n    return prime_numbers_up_to(param)\nThe function extract_params will extract a number from the URL path. It first \nremoves any / character, and then tries to convert it into an integer and checks the \ninteger is positive. For any errors, it returns None.\ndef extract_param(path):\n    '''\n    Extract the parameter and transform into\n    a positive integer. If the parameter is\n    not valid, return None\n    '''\n    raw_param = path.replace('/', '')\n    # Try to convert in number\n    try:\n        param = int(raw_param)\n    except ValueError:\n        return None\n    # Check that it's positive\n    if param < 0:\n        return None\n    return param\nThe function prime_numbers_up_to, finally, calculates the prime numbers up to the \nnumber passed. This is similar to the code that we saw earlier in the chapter. \ndef prime_numbers_up_to(up_to):\n    primes = [number for number in range(1, up_to + 1)\n              if check_if_prime(number)]\n    return primes\nFinally, check_if_prime, which we covered extensively earlier in the chapter, is the \nsame as it was at primes_4.py.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1278,
      "extraction_method": "Direct"
    },
    {
      "page_number": 505,
      "chapter": null,
      "content": "Profiling\n[ 486 ]\nThe process can be started with:\n$ python3 server.py\nServer available at http://localhost:8000\nUse CTR+C to stop it\nAnd then tested by going to http://localhost:8000/500 to try to get prime numbers \nup to 500.\n \nFigure 14.3: The interface displaying all primes up to 500\nAs you can see, we have an understandable output. Let's move on to profiling the \nprocess we used to get it.\nProfiling the whole process\nWe can profile the whole process by starting it under cProfile and then \ncapturing its output with. We start it like this, make a single request to http://\nlocalhost:8000/500, and check the results.\n$ python3 -m cProfile -o server.prof server.py\nServer available at http://localhost:8000\nUse CTR+C to stop it\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 506,
      "chapter": null,
      "content": "Chapter 14\n[ 487 ]\n127.0.0.1 - - [10/Oct/2021 14:05:34] \"GET /500 HTTP/1.1\" 200 -\n127.0.0.1 - - [10/Oct/2021 14:05:34] \"GET /favicon.ico HTTP/1.1\" 200 -\n^CServer stopped.\nWe have stored the results in the file server.prof. This file can then be analyzed as \nbefore, using snakeviz.\n$ snakeviz server.prof\nsnakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit\nWhich displays the following diagram:\nFigure 14.4: Diagram of the full profile. The full page is too big to fit here and has been  \ncropped purposefully to show some of the info.\nAs you can see, the diagram shows that for the vast majority of the test duration, the \ncode was waiting for a new request, and internally doing a poll action. This is part of \nthe server code and not our code.\nTo find the code that we care about, we can manually search in the long list below \nfor get_result, which is the root of the interesting bits of our code. Be sure to select \nCutoff: None to display all the functions.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1081,
      "extraction_method": "Direct"
    },
    {
      "page_number": 507,
      "chapter": null,
      "content": "Profiling\n[ 488 ]\nOnce selected, the diagram will display from there onward. Be sure to scroll up to \nsee the new diagram.\nFigure 14.5: The diagram showing from get_result. The full page is too big to fit  \nhere and has been cropped purposefully to show some of the info.\nHere, you can see more of the general structure of the code execution. You can see \nthat most of the time is spent on the multiple check_if_prime calls, which comprise \nthe bulk of prime_numbers_up_to and the list comprehension included in it, and very \nlittle time is spent on extract_params.\nBut this approach has some problems:\n•\t\nFirst of all, we need to go a full cycle between starting and stopping a \nprocess. This is cumbersome to do for requests.\n•\t\nEverything that happens in the cycle is included. That adds noise to the \nanalysis. Fortunately, we knew that the interesting part was in get_result, \nbut that may not be evident. This case also uses a minimal structure but \nadding that in the case of a complex framework like Django can lead to a lot \nof \t\n.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 508,
      "chapter": null,
      "content": "Chapter 14\n[ 489 ]\n•\t\nIf we process two different requests, they will be added into the same file, \nagain mixing the results.\nThese problems can be solved by applying the profiler to only the part that is of \ninterest and producing a new file for each request.\nGenerating a profile file per request\nTo be able to generate a different file with information per individual request, \nwe need to create a decorator for easy access. This will profile and produce an \nindependent file.\nIn the file server_profile_by_request.py, we get the same code as in server.py, but \nadding the following decorator.\nfrom functools import wraps\nimport cProfile\nfrom time import time\ndef profile_this(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        prof = cProfile.Profile()\n        retval = prof.runcall(func, *args, **kwargs)\n        filename = f'profile-{time()}.prof'\n        prof.dump_stats(filename)\n        return retval\n    return wrapper\nThe decorator defines a wrapper function that replaces the original function. We use \nthe wraps decorator to keep the original name and docstring.\nThis is just a standard decorator process. A decorator function in \nPython is one that returns a function that then replaces the original \none. As you can see, the original function func is still called inside \nthe wrapper that replaces it, but it adds extra functionality.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 509,
      "chapter": null,
      "content": "Profiling\n[ 490 ]\nInside, we start a profiler and run the function under it using the runcall function. \nThis line is the core of it – using the profiler generated, we run the original function \nfunc with its parameters and store its returned value.\nretval = prof.runcall(func, *args, **kwargs)\nAfter that, we generate a new file that includes the current time and dump the stats \nin it with the .dump_stats call.\nWe also decorate the get_result function, so we start our profiling there.\n@profile_this\ndef get_result(path):\n    param = extract_param(path)\n    if param is None:\n        return 'Invalid parameter, please add an integer'\n    return prime_numbers_up_to(param)\nThe full code is available in the file server_profile_by_request.py, available on \nGitHub at https://github.com/PacktPublishing/Python-Architecture-Patterns/\nblob/main/chapter_14_profiling/server_profile_by_request.py.\nLet's start the server now and make some calls through the browser, one to http://\nlocalhost:8000/500 and another to http://localhost:8000/800.\n$ python3 server_profile_by_request.py\nServer available at http://localhost:8000\nUse CTR+C to stop it\n127.0.0.1 - - [10/Oct/2021 17:09:57] \"GET /500 HTTP/1.1\" 200 -\n127.0.0.1 - - [10/Oct/2021 17:10:00] \"GET /800 HTTP/1.1\" 200 -\nWe can see how new files are created:\n$ ls profile-*\nprofile-1633882197.634005.prof \nprofile-1633882200.226291.prof\nThese files can be displayed using snakeviz:\n$ snakeviz profile-1633882197.634005.prof\nsnakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1638,
      "extraction_method": "Direct"
    },
    {
      "page_number": 510,
      "chapter": null,
      "content": "Chapter 14\n[ 491 ]\nFigure 14.6: The profile information of a single request. The full page is too big to fit  \nhere and has been cropped purposefully to show some of the info.\nEach file contains only the information from get_result onwards, which gets \ninformation only up to a point. Even more so, each file displays information only for \na specific request, so it can be profiled individually, with a high level of detail.\nThe code can be adapted to adapt the filename more specifically to include details \nlike call parameters, which can be useful. Another interesting possible adaptation is \nto create a random sample, so only 1 in X calls produces profiled code. This can help \nreduce the overhead of profiling and allow you to completely profile some requests.\nNext, we'll see how to perform memory profiling.\nThis is different from a statistical profiler, as it will still completely \nprofile some requests, instead of detecting what's going on at a \nparticular time. This can help follow the flow of what happens for \nparticular requests.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1148,
      "extraction_method": "Direct"
    },
    {
      "page_number": 511,
      "chapter": null,
      "content": "Profiling\n[ 492 ]\nMemory profiling\nSometimes, applications use too much memory. The worst-case scenario is that \nthey use more and more memory as time goes by, normally due to what's called a \nmemory leak, maintaining memory that is no longer used, due to some mistake in \nthe coding. Other problems can also include the fact that the usage of memory may \nbe improved, as it's a limited resource.\nTo profile memory and analyze what the objects are that use the memory, we need \nfirst to create some example code. We will generate enough Leonardo numbers.\nLeonardo numbers are numbers that follow a sequence defined as the following:\n•\t\nThe first Leonardo number is one\n•\t\nThe second Leonardo number is also one\n•\t\nAny other Leonardo number is the two previous Leonardo numbers plus one\nLeonardo numbers are similar to Fibonacci numbers. They are actually related to \nthem. We use them instead of Fibonacci to show more variety. Numbers are fun!\nWe present the first 35 Leonardo numbers by creating a recursive function and store \nit in leonardo_1.py, available on GitHub at https://github.com/PacktPublishing/\nPython-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_1.py.\ndef leonardo(number):\n    if number in (0, 1):\n        return 1\n    return leonardo(number - 1) + leonardo(number - 2) + 1\nNUMBER = 35\nfor i in range(NUMBER + 1):\n    print('leonardo[{}] = {}'.format(i, leonardo(i)))\nYou can run the code and see it takes progressively longer.\n$ time python3 leonardo_1.py\nleonardo[0] = 1\nleonardo[1] = 1\nleonardo[2] = 3\nleonardo[3] = 5\nleonardo[4] = 9\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1672,
      "extraction_method": "Direct"
    },
    {
      "page_number": 512,
      "chapter": null,
      "content": "Chapter 14\n[ 493 ]\nleonardo[5] = 15\n...\nleonardo[30] = 2692537\nleonardo[31] = 4356617\nleonardo[32] = 7049155\nleonardo[33] = 11405773\nleonardo[34] = 18454929\nleonardo[35] = 29860703\nreal      0m9.454s\nuser      0m8.844s\nsys 0m0.183s\nTo speed up the process, we see that it's possible to use memorization techniques, \nwhich means to store the results and use them instead of calculating them all the \ntime.\nWe change the code like this, creating the leonardo_2.py file (available on GitHub at \nhttps://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/\nchapter_14_profiling/leonardo_2.py).\nCACHE = {}\ndef leonardo(number):\n    if number in (0, 1):\n        return 1\n    if number not in CACHE:\n        result = leonardo(number - 1) + leonardo(number - 2) + 1\n        CACHE[number] = result\n    return CACHE[number]\nNUMBER = 35000\nfor i in range(NUMBER + 1):\n    print(f'leonardo[{i}] = {leonardo(i)}')\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 513,
      "chapter": null,
      "content": "Profiling\n[ 494 ]\nThis uses a global dictionary, CACHE, to store all Leonardo numbers, speeding up the \nprocess. Note that we increased the number of numbers to calculate from 35 to 35000, \na thousand times more. The process runs quite quickly.\n$ time python3 leonardo_2.py\nleonardo[0] = 1\nleonardo[1] = 1\nleonardo[2] = 3\nleonardo[3] = 5\nleonardo[4] = 9\nleonardo[5] = 15\n...\nleonardo[35000] = ...\nreal      0m15.973s\nuser      0m8.309s\nsys       0m1.064s\nLet's take a look now at memory usage.\nUsing memory_profiler\nNow that we have our application storing information, let's use a profiler to show \nwhere the memory is stored.\nWe need to install the package memory_profiler. This package is similar to line_\nprofiler.\n$ pip install memory_profiler\nWe can now add a @profile decorator in the leonardo function (stored in \nleonardo_2p.py, on GitHub at https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/blob/main/chapter_14_profiling/leonardo_2p.py), and run \nit using the memory_profiler module. You'll notice that it runs slower this time, but \nafter the usual result, it displays a table.\n$ time python3 -m memory_profiler leonardo_2p.py\n...\nFilename: leonardo_2p.py\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1284,
      "extraction_method": "Direct"
    },
    {
      "page_number": 514,
      "chapter": null,
      "content": "Chapter 14\n[ 495 ]\nLine #    Mem usage    Increment  Occurences   Line Contents\n============================================================\n     5  104.277 MiB   97.082 MiB      104999   @profile\n     6                                         def leonardo(number):\n     7\n     8  104.277 MiB    0.000 MiB      104999       if number in (0, 1):\n     9   38.332 MiB    0.000 MiB           5           return 1\n    10\n    11  104.277 MiB    0.000 MiB      104994       if number not in \nCACHE:\n    12  104.277 MiB    5.281 MiB       34999           result = \nleonardo(number - 1) + leonardo(number - 2) + 1\n    13  104.277 MiB    1.914 MiB       34999           CACHE[number] = \nresult\n    14\n    15  104.277 MiB    0.000 MiB      104994       return CACHE[number]\nReal      0m47.725s\nUser      0m25.188s\nsys 0m10.372s\nThis table shows first the memory usage, and the increment or decrement, as well as \nhow many times each line appears.\nYou can see the following:\n•\t\nLine 9 gets executed only a few times. When it does, the amount of memory \nis around 38 MiB, which will be the minimum memory used by the program.\n•\t\nThe total memory used is almost 105 MiB.\n•\t\nThe whole memory increase is localized in lines 12 and 13, when we create \na new Leonardo number and when we store it in the CACHE dictionary. Note \nhow we are never releasing memory here.\nWe don't really need to keep all the previous Leonardo numbers in memory at all \ntimes, and we can try a different approach to keep only a few.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 515,
      "chapter": null,
      "content": "Profiling\n[ 496 ]\nMemory optimization\nWe create the file leonardo_3.py with the following code, available on GitHub at \nhttps://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/\nchapter_14_profiling/leonardo_3.py:\nCACHE = {}\n@profile\ndef leonardo(number):\n    if number in (0, 1):\n        return 1\n    if number not in CACHE:\n        result = leonardo(number - 1) + leonardo(number - 2) + 1\n        CACHE[number] = result\n    ret_value = CACHE[number]\n    MAX_SIZE = 5\n    while len(CACHE) > MAX_SIZE:\n        # Maximum size allowed,\n        # delete the first value, which will be the oldest\n        key = list(CACHE.keys())[0]\n        del CACHE[key]\n    return ret_value\nNUMBER = 35000\nfor i in range(NUMBER + 1):\n    print(f'leonardo[{i}] = {leonardo(i)}')\nNote we keep the @profile decorator to run the memory profiler again. Most of the \ncode is the same, but we added the following extra block:\n    MAX_SIZE = 5\n    while len(CACHE) > MAX_SIZE:\n        # Maximum size allowed,\n        # delete the first value, which will be the oldest\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1160,
      "extraction_method": "Direct"
    },
    {
      "page_number": 516,
      "chapter": null,
      "content": "Chapter 14\n[ 497 ]\n        key = list(CACHE.keys())[0]\n        del CACHE[key]\nThis code will keep the number of elements in the CACHE dictionary within a limit. \nWhen the limit is reached, it will remove the first element returned by CACHE.keys(), \nwhich will be the oldest.\nThe dictionary won't be able to grow. Let's now try to run it and see the results of the \nprofiling.\n$ time python3 -m memory_profiler leonardo_3.py\n...\nFilename: leonardo_3.py\nLine #    Mem usage    Increment  Occurences   Line Contents\n============================================================\n     5   38.441 MiB   38.434 MiB      104999   @profile\n     6                                         def leonardo(number):\n     7\n     8   38.441 MiB    0.000 MiB      104999       if number in (0, 1):\n     9   38.367 MiB    0.000 MiB           5           return 1\n    10\n    11   38.441 MiB    0.000 MiB      104994       if number not in \nCACHE:\n    12   38.441 MiB    0.008 MiB       34999           result = \nleonardo(number - 1) + leonardo(number - 2) + 1\n    13   38.441 MiB    0.000 MiB       34999           CACHE[number] = \nresult\n    14\n    15   38.441 MiB    0.000 MiB      104994       ret_value = \nCACHE[number]\n    16\n    17   38.441 MiB    0.000 MiB      104994       MAX_SIZE = 5\n    18   38.441 MiB    0.000 MiB      139988       while len(CACHE) > \nMAX_SIZE:\nSince Python 3.6, all Python dictionaries are ordered, so they'll \nreturn their keys in the order they have been input previously. We \ntake advantage of that for this. Note we need to convert the result \nfrom CACHE.keys() (a dict_keys object) to a list to allow getting \nthe first element.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1745,
      "extraction_method": "Direct"
    },
    {
      "page_number": 517,
      "chapter": null,
      "content": "Profiling\n[ 498 ]\n    19                                                 # Maximum size \nallowed,\n    20                                                 # delete the \nfirst value, which will be the oldest\n    21   38.441 MiB    0.000 MiB       34994           key = \nlist(CACHE.keys())[0]\n    22   38.441 MiB    0.000 MiB       34994           del CACHE[key]\n    23\n    24   38.441 MiB    0.000 MiB      104994       return ret_value\nIn this case, we see how the memory remains stable at around the 38 MiB, that we see \nis the minimum. In this case, note how there are no increments or decrements. Really \nwhat happens here is that increments and decrements are too small to be noticed. \nBecause they cancel each other, the report is close to zero.\nThe memory-profiler module is also able to perform more actions, including \nshowing the usage of memory based on time and plotting it, so you can see memory \nincreasing or decreasing over time. Take a look at its full documentation at https://\npypi.org/project/memory-profiler/.\nSummary\nIn this chapter, we described what profiling is and when it's useful to apply it. We \ndescribed that profiling is a dynamic tool that allows you to understand how code \nruns. This information is useful in understanding the flow in a practice situation \nand being able to optimize the code with that information. Code can be optimized \nnormally to execute faster, but other alternatives are open, like using fewer resources \n(normally memory), reducing external accesses, etc.\nWe described the main types of profilers: deterministic profilers, statistical profilers, \nand memory profilers. The first two are mostly oriented toward improving the \nperformance of code and memory profilers analyze the memory used by the code in \nexecution. Deterministic profilers instrument the code to detail the flow of the code \nas it's executed. Statistical profilers sample the code at periodic times to provide a \ngeneral view of the parts of the code that are executed more often.\nWe then showed how to profile the code using deterministic profilers, presenting \nan example. We analyzed it first with the built-in module cProfile, which gives a \nfunction resolution. We saw how to use graphical tools to show the results. To dig \ndeeper, we used the third-party module line-profiler, which goes through each \nof the code lines. Once the flow of the code is understood, it is optimized to greatly \nreduce its execution time.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2549,
      "extraction_method": "Direct"
    },
    {
      "page_number": 518,
      "chapter": null,
      "content": "Chapter 14\n[ 499 ]\nThe next step was to see how to profile a process intended to keep running, like a \nweb server. We showed the problems with trying to profile the whole application \nin these cases and described how we can profile each individual request instead \nfor clarity.\nFinally, we also presented an example to profile memory and see how it's used by \nusing the module memory-profiler.\nIn the next chapter, we will learn more details about how to find and fix problems in \ncode, including in complex situations, through debugging techniques.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\nThese techniques are also applicable to other situations like \nconditional profiling, profiling in only certain situations, like at \ncertain times or one of each 100 requests.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 519,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 520,
      "chapter": null,
      "content": "[ 501 ]\n15\nDebugging\nGenerally speaking, the cycle for debugging problems has the following steps:\n1.\t Detecting the problem. A new problem or defect is discovered\n2.\t Analyzing and assigning priority to this problem, to be sure that we spend \ntime on meaningful problems and focus on the most important ones\n3.\t Investigating what exactly causes the problem. Ideally, this should end with \na way of replicating the problem in a local environment\n4.\t Replicating the problem locally, and getting into the specific details on why it \nhappens\n5.\t Fixing the problem\nAs you can see, the general strategy is to first locate and understand the problem, so \nwe can then properly debug and fix it.\nIn this chapter, we'll cover the following topics to see effective techniques on how to \nwork through all those phases:\n•\t\nDetecting and processing defects\n•\t\nInvestigation in production\n•\t\nUnderstanding the problem in production\n•\t\nLocal debugging\n•\t\nPython introspection tools\n•\t\nDebugging with logs\n•\t\nDebugging with breakpoints\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 521,
      "chapter": null,
      "content": "Debugging\n[ 502 ]\nLet's take a look at the very first step when dealing with defects.\nDetecting and processing defects\nThe first step is actually detecting the problem. This can sound a bit silly, but it's a \nquite crucial stage.\nDetecting problems can be done in different ways, and some may be more evident \nthan others. Normally, once the code is in production, defects will be detected by a \nuser, either internally (best case) or externally (worst case), or through monitoring.\nBased on how problems are detected, we can categorize them into different \nseverities, for example:\n•\t\nCatastrophic problems that are completely stopping the operation. These \nbugs mean that nothing, not even non-related tasks in the same system, \nworks\n•\t\nCritical problems that stop the execution of some tasks, but not others\n•\t\nSerious problems that will stop or cause problems with certain tasks, but \nonly in some circumstances. For example, a parameter is not checked and \nproduces an exception, or some combination produces a task so slow that it \nproduces a timeout\n•\t\nMild problems, which include tasks containing errors or inaccuracies. For \nexample, a task produces an empty result in certain circumstances, or a \nproblem in the UI that doesn't allow calling a functionality\n•\t\nCosmetic or minor problems like typos and similar\nWhile we will mainly use the term \"bug\" to describe any defect, \nremember that it may include details like bad performance or \nunexpected behavior that may not be properly categorized as a \n\"bug.\" The proper tool to fix the problem could be different, but the \ndetection is normally done in a similar way.\nKeep in mind that monitoring will only be able to capture obvious, \nand typically serious, errors.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 522,
      "chapter": null,
      "content": "Chapter 15\n[ 503 ]\nBecause every development team is limited, there will always be too many bugs \nand having the proper approach on what to pay attention to and what to fix first is \ncritical. Normally bugs in the first group will obviously be quite pressing to fix and \nwill require an immediate all-hands reaction. But categorization and prioritization \nare important. \nHaving a clear signal on what things to look for next will help developers have \na clear view and be efficient by spending time on important problems and not \nwhatever is the latest. Teams themselves can perform some triage of problems, but \nit's good to add some context.\nKeep in mind that usually, you need to both correct bugs and implement new \nfeatures, and each of these tasks can distract from the other. \nFixing bugs is important, not only for the resulting quality of the service, as any user \nwill find working with a buggy service very frustrating. But it's also important for \nthe development team, as working with a low-quality service is also frustrating for \ndevelopers. \nAny detected problem, except the catastrophic ones, where context is irrelevant, \nshould capture the context surrounding the steps that were required to produce the \nerror. The objective of this is to be able to reproduce the error.\nWhen a problem can be replicated, you're halfway to the solution. The problem can \nbe ideally replicated into a test, so it can be tested over and over until the problem is \nunderstood and fixed. In the best situations, this test can be a unit test, if the problem \naffects a single system and all the conditions are understood and can be replicated. If \nthe problem affects more than one system, it may be necessary to create integration \ntests.\nA proper balance needs to be struck between bug fixing and \nintroducing new features. Also remember to allocate time for the \ncorresponding new bugs introduced for new features. A feature is \nnot ready when released, it's ready when its bugs are fixed.\nReproducing the error is a critical element of fixing it. The worst-\ncase scenario is that a bug is intermittent or appears to happen \nat random times. More digging will be required in order to \nunderstand why it is happening when it's happening.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2340,
      "extraction_method": "Direct"
    },
    {
      "page_number": 523,
      "chapter": null,
      "content": "Debugging\n[ 504 ]\nOnce a problem is categorized and replicable, the investigation can proceed to \nunderstand why.\nVisually inspecting the code and trying to reason where problems and bugs are is \nnormally not good enough. Even very simple code will surprise you in terms of how \nit's executed. Being able to analyze how, in a particular case, the code is executing \nwith precision is critical for analyzing and fixing problems that are found.\nInvestigation in production\nOnce we are aware that we have a problem in production, we need to understand \nwhat is happening and what the key elements that produce it are.\nThe most important tools when analyzing why a particular problem is produced are \nthe observability tools. That's why it is important to do preparation work in advance \nto be sure to be able to find problems when required.\nWe talked in previous chapters about logs and metrics. When debugging, metrics \nare normally not relevant, except to show the relative importance of a bug. Checking \nan increase in returned errors can be important to detect that there's an error, but \ndetecting what error will require more precise information.\nDo not underestimate metrics, though. They can help quickly determine what \nspecific component is failing or if there's any relationship with other elements, for \nexample, if there's a single server that's producing errors, or if it has run out of \nmemory or hard drive space. \nA common problem during an investigation is to find out what \nthe specific circumstances are that are provoking the problem, \nfor example, data that's set up in a particular way in production \nand that triggers some issue. Finding exactly what is causing the \nproblem can be complicated in this environment. We will talk later \nin the chapter about finding a problem in production.\nIt's very important to remark on the importance of being able to \nreplicate a problem. If that's the case, tests can be done to produce \nthe error and follow the consequences.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2087,
      "extraction_method": "Direct"
    },
    {
      "page_number": 524,
      "chapter": null,
      "content": "Chapter 15\n[ 505 ]\nBut in any case, logs will generally be more useful in determining which part of the \ncode is behaving badly. As we saw in Chapter 12, Logging, we can describe error logs \nas detecting two kinds of problems:\n•\t\nExpected errors. In this case, we did the work of debugging the error \nbeforehand and knowing what happened should be easy. Examples of this \ncan be an external request that returns an error, a database that cannot be \nconnected to, etc.\nMost of these errors will be related to external services (from the point of \nview of the one raising the error) that are misbehaving. This could indicate \na network problem, misconfiguration, or problems in other services. It is \nnot rare that errors propagate through the system as an error may provoke \na cascading failure. Typically, though, the origin will be an unexpected \nerror and the rest will be expected ones, as they'll receive the error from an \nexternal source.\n•\t\nUnexpected errors. The sign of these errors are logs indicating that \nsomething has gone wrong, and in most modern programming languages, a \nstack trace of some sort in the logs detailing the line of code when the error \nwas produced.\nThe system should provide the proper handling for the task. For example, a \nweb server will return a 500 error, and a task management system may retry \nthe task after some delay. This may lead to the error being propagated, as we \nsaw before.\nFor example, a problematic server can produce apparently random \nerrors, if the external requests are directed to different servers, \nand the failure is related to a combination of a specific request \naddressed to a specific server.\nBy default, any kind of framework that executes tasks, \nlike a web framework or task management system, will \nproduce an error, but keep the system stable. This means \nthat only the task producing the error will be interrupted \nand any new task will be handled from scratch.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2035,
      "extraction_method": "Direct"
    },
    {
      "page_number": 525,
      "chapter": null,
      "content": "Debugging\n[ 506 ]\nIn any of the two cases, the main tool to detect what the problem was will be logs. \nEither the logs show a known problem that is captured and properly labeled, or the \nlogs show a stack trace that should indicate what specific part of the code is showing \nthe error.\nFinding the element and part of the code that is the source of the error is important \nfor understanding the problem and for debugging the specific problem. This is \nparticularly important in microservices architectures, as they'll have multiple \nindependent elements.\nKeep in mind that sometimes it is not possible to totally avoid errors. For example, \nif there's an external dependency calling an external API and it has a problem, this \nmay trigger internal errors. These can be mitigated, failing gracefully, or generating \na state of \"service not available.\" But the root of the error may not be possible to \nfix totally. \nWe can have these cases be notified to us, but they won't require further short-term \naction.\nIn other cases, when the error is not immediately obvious and further investigation \nneeds to be done, it will require some debugging.\nUnderstanding the problem in production\nThe challenge in complex systems is the fact that detecting problems becomes \nexponentially more complicated. As multiple layers and modules are added and \ninteract with each other, bugs become potentially more subtle and more complex.\nWe talked about microservices and monolithic architectures in \nChapter 9, Microservices vs Monolith. Monoliths are easier to deal \nwith in terms of bugs, as all the code is handled on the same site, \nbut anyway they'll become more and more complex as they grow.\nMitigating external dependencies may require creating \nredundancy, even using different suppliers so as not to be \ndependent on a single point of failure, though this may not be \nrealistic, as it can be extremely costly.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2004,
      "extraction_method": "Direct"
    },
    {
      "page_number": 526,
      "chapter": null,
      "content": "Chapter 15\n[ 507 ]\nThe objective in this step, though, should be to analyze enough of the problem in \nproduction to be able to replicate it in a local environment, where the smaller scale \nof the environment will make it easier and less invasive to probe and make changes. \nOnce enough information has been collected, it's better to leave any production \nenvironment alone and focus on the specifics of the problem. \nSometimes, general logging enabled is enough to determine exactly what the bug \nis or how to replicate it locally. In those cases, it may be necessary to research the \ncircumstances that trigger the problem.\nLogging a request ID\nOne of the problems when analyzing a large number of logs is correlating them. To \nproperly group logs that are related to each other, we could filter by the host that \ngenerates them and select a short window of time, but even that may not be good \nenough as two or more different tasks can be running at the same time. We need a \nunique identifier per task or request that can trace all logs coming from the same \nsource. We will call this identifier a request ID, as they are added automatically in \nmany frameworks. This sometimes is called a task ID in task managers.\nAs we saw before, microservice architectures can be especially \ndifficult to debug. The interaction between different microservices \ncan produce complex interactions that can produce subtle \nproblems in the integration of its different parts. This integration \ncan be difficult to test in integration tests, or perhaps the source of \nthe problem is in a blind spot of the integration tests.\nBut monoliths can also have problems as their parts grow more \ncomplex. Difficult bugs may be produced due to the interaction \nof specific production data that interacts in an unexpected way. A \nbig advantage of monolithic systems is that the tests will cover the \nwhole system, making it easier to replicate with unit or integration \ntests.\nRemember that having a replicable bug is more than half the battle! \nOnce the problem can be categorized as a replicable set of steps \nlocally, a test can be created to produce it over and over and debug \nin a controlled environment.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 527,
      "chapter": null,
      "content": "Debugging\n[ 508 ]\nIn cases where multiple services are involved, like in microservice architectures, it \nis very important to keep a common request ID that can work to trace the different \nrequests between different services. That allows you to follow and correlate different \nlogs in the system from different services that have the same origin. \nThe following diagram shows the flow between a frontend and two backend services \nthat are called internally. Note that the X-Request-ID header is set by the frontend \nand it's forwarded to service A, which then forwards it toward service B.\nFigure 15.1: Request ID across multiple services\nBecause all of them share the same request ID, logs can be filtered by that \ninformation to obtain all the information about a single task.\nTo achieve this, we can use the module django_log_request_id to create a request ID \nin Django applications.\nWe show some code in GitHub at https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/tree/main/chapter_15_debug following the example across \nthe book. This requires creating a virtual environment and installing the package, \nalongside the rest of the requirements.\n$ python3 -m venv ./venv\n$ source ./venv/bin/activate\n(venv) $ pip install -r requirements.txt\nYou can see the whole documentation here: https://github.\ncom/dabapps/django-log-request-id/.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 528,
      "chapter": null,
      "content": "Chapter 15\n[ 509 ]\nThe code has been changed to include some extra logs in the microposts/api/views.\npy file (as seen at https://github.com/PacktPublishing/Python-Architecture-\nPatterns/blob/main/chapter_15_debug/microposts/api/views.py):\nfrom rest_framework.generics import ListCreateAPIView\nfrom rest_framework.generics import RetrieveUpdateDestroyAPIView\nfrom .models import Micropost, Usr\nfrom .serializers import MicropostSerializer\nimport logging\nlogger = logging.getLogger(__name__)\nclass MicropostsListView(ListCreateAPIView):\n    serializer_class = MicropostSerializer\n    def get_queryset(self):\n        logger.info('Getting queryset')\n        result = Micropost.objects.filter(user__username=self.\nkwargs['username'])\n        logger.info(f'Querysert ready {result}')\n        return result\n    def perform_create(self, serializer):\n        user = Usr.objects.get(username=self.kwargs['username'])\n        serializer.save(user=user)\nclass MicropostView(RetrieveUpdateDestroyAPIView):\n    serializer_class = MicropostSerializer\n    def get_queryset(self):\n        logger.info('Getting queryset for single element')\n        result = Micropost.objects.filter(user__username=self.\nkwargs['username'])\n        logger.info(f'Queryset ready {result}')\n        return result\nNote how this is now adding some logs when accessing the list collections page and \nthe individual micropost page. We will use the example URL /api/users/jaime/\ncollection/5.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 529,
      "chapter": null,
      "content": "Debugging\n[ 510 ]\nTo enable the usage of the request ID, we need to properly set up the configuration \nin microposts/settings.py (https://github.com/PacktPublishing/Python-\nArchitecture-Patterns/blob/main/chapter_15_debug/microposts/microposts/\nsettings.py):\nLOG_REQUEST_ID_HEADER = \"HTTP_X_REQUEST_ID\"\nGENERATE_REQUEST_ID_IF_NOT_IN_HEADER = True\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'filters': {\n        'request_id': {\n            '()': 'log_request_id.filters.RequestIDFilter'\n        }\n    },\n    'formatters': {\n        'standard': {\n            'format': '%(levelname)-8s [%(asctime)s] [%(request_id)s] \n%(name)s: %(message)s'\n        },\n    },\n    'handlers': {\n        'console': {\n            'level': 'INFO',\n            'class': 'logging.StreamHandler',\n            'filters': ['request_id'],\n            'formatter': 'standard',\n        },\n    },\n    'root': {\n        'handlers': ['console'],\n        'level': 'INFO',\n    },\n}\nThe LOGGING dictionary is a characteristic in Django that describes how to log. \nfilters adds extra information, in this case, our request_id, formatter describes the \nspecific format to use (note that we add request_id as a parameter, which will be \npresented in brackets). \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1353,
      "extraction_method": "Direct"
    },
    {
      "page_number": 530,
      "chapter": null,
      "content": "Chapter 15\n[ 511 ]\nhandlers describes what happens to each message, joining filters and formatter \nwith information about levels to display and where to send the info. In this case, \nStreamHandler will send the log to stdout. We set all the logs on the root level to use \nthis handler.\nThe lines,\nLOG_REQUEST_ID_HEADER = \"HTTP_X_REQUEST_ID\"\nGENERATE_REQUEST_ID_IF_NOT_IN_HEADER = True\nstate that a new Request ID parameter should be created if not found as a header in \nthe input and that the name of the header will be X-Request-ID.\nOnce all of this is configured, we can run a test starting the server with: \n(venv) $ python3 manage.py runserver\nWatching for file changes with StatReloader\n2021-10-23 16:11:16,694 INFO     [none] django.utils.autoreload: \nWatching for file changes with StatReloader\nPerforming system checks...\nSystem check identified no issues (0 silenced).\nOctober 23, 2021 - 16:11:16\nDjango version 3.2.8, using settings 'microposts.settings'\nStarting development server at http://127.0.0.1:8000/\nQuit the server with CONTROL-C\nOn another screen, make a call to the test URL with curl:\n(venv) $ curl http://localhost:8000/api/users/jaime/collection/5\n{\"href\":\"http://localhost:8000/api/users/jaime/\ncollection/5\",\"id\":5,\"text\":\"A referenced micropost\",\"referenced\":\"dana\n\",\"timestamp\":\"2021-06-10T21:15:27.511837Z\",\"user\":\"jaime\"}\nCheck the Django documentation for more information: https://\ndocs.djangoproject.com/en/3.2/topics/logging/. Logging \nin Django may take a bit of experience in setting all the parameters \ncorrectly. Take your time when configuring it.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 531,
      "chapter": null,
      "content": "Debugging\n[ 512 ]\nAt the same time, you'll see the logs on the server screen:\n2021-10-23 16:12:47,969 INFO     [66e9f8f1b43140338ddc3ef569b8e845] \napi.views: Getting queryset for single element\n2021-10-23 16:12:47,971 INFO     [66e9f8f1b43140338ddc3ef569b8e845] \napi.views: Queryset ready <QuerySet [<Micropost: Micropost object (1)>, \n<Micropost: Micropost object (2)>, <Micropost: Micropost object (5)>]>\n[23/Oct/2021 16:12:47] \"GET /api/users/jaime/collection/5 HTTP/1.1\" 200 \n177\nWhich, as you can see, added a new request ID element, 66e9f8f1b43140338ddc3ef56\n9b8e845 in this case.\nBut the request ID can also be created by calling with the proper header. Let's try \nagain, making another curl request and the -H parameter to add a header.\n$ curl -H \"X-Request-ID:1A2B3C\" http://localhost:8000/api/users/jaime/\ncollection/5\n{\"href\":\"http://localhost:8000/api/users/jaime/\ncollection/5\",\"id\":5,\"text\":\"A referenced micropost\",\"referenced\":\"dana\n\",\"timestamp\":\"2021-06-10T21:15:27.511837Z\",\"user\":\"jaime\"}\nYou can check the logs in the server again:\n2021-10-23 16:14:41,122 INFO     [1A2B3C] api.views: Getting queryset \nfor single element\n2021-10-23 16:14:41,124 INFO     [1A2B3C] api.views: Queryset ready \n<QuerySet [<Micropost: Micropost object (1)>, <Micropost: Micropost \nobject (2)>, <Micropost: Micropost object (5)>]>\n[23/Oct/2021 16:14:41] \"GET /api/users/jaime/collection/5 HTTP/1.1\" 200 \n177\nThis shows that the request ID has been set by the value in the header.\nThe request ID can be passed over other services by using the Session included in \nthe same module, which acts as a Session in the requests module.\nfrom log_request_id.session import Session\nsession = Session()\nsession.get('http://nextservice/url')\nThis will set the proper header in the request, passing through it to the next step of \nthe chain, like service A or service B.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1957,
      "extraction_method": "Direct"
    },
    {
      "page_number": 532,
      "chapter": null,
      "content": "Chapter 15\n[ 513 ]\nAnalyzing data\nIf the default logs are not enough to understand the problem, the next stage in those \ncases is understanding the data related to the problem. Typically, the data storage \nmay be inspected to follow up on the related data for the task, to see if there's any \nindication about that.\nAnalyzing the stored data may require performing ad hoc manual queries to \ndatabases or other kinds of data storage to find out if the related data is consistent or \nif there is any combination of parameters that's not expected.\nIf investigating the data is not enough to be able to understand the problem, it may \nbe necessary to increase the information on the logs.\nBe sure to check the django-log-request-id documentation.\nThis step may be complicated by either missing data or data \nrestrictions that make it difficult or impossible to obtain the data. \nSometimes only a few people in the organization can access \nthe required data, which may delay the investigation. Another \npossibility is that the data is impossible to retrieve. For example, \ndata policies may not store the data, or the data may be encrypted. \nThis is a regular occurrence in cases involving Personally \nIdentifiable Information (PII), passwords, or similar data.\nRemember that the objective is to capture information from \nproduction to be able to understand and replicate the problem \nindependently. \nIn some cases, when investigating a problem in production, it is \npossible that changing the data manually will fix the issue. This \ncould be necessary in some emergency situations, but the objective \nstill needs to be to understand why this inconsistent situation of \nthe data has been possible or how the service should be changed \nto allow you to deal with this data situation. Then the code can be \nchanged accordingly to ensure that the problem doesn't happen in \nthe future.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 533,
      "chapter": null,
      "content": "Debugging\n[ 514 ]\nIncreasing logging\nIf the regular logs and an investigation of the data don't bear fruit, it may be \nnecessary to increase the level of logging with special logs, following the problem.\nThis is a last-resort method, because it has two main problems:\n•\t\nAny change in the logs needs to be deployed, which makes it costly and \nexpensive to run. \n•\t\nThe number of logs in the system will be incremented, which will require \nmore space to store them. Depending on the number of requests in the \nsystem, this can create pressure on the logging system.\nThese extra logs should always be short term, and should be reverted as soon as \npossible.\nWhile enabling an extra level of logging, like setting logs to DEBUG level, is technically \npossible, this will probably increase the logs too much, and will make it difficult to \nknow what the key ones are in the massive amount of logs. With some DEBUG logs, \nspecifics of the area under investigation can be temporally promoted to INFO or \nhigher to make sure that they are properly logged.\nBe extra careful with information that's logged temporally. Confidential information \nlike PII should not be logged. Instead, try to log surrounding information that can \nhelp find out the problem.\nFor example, if there's a suspicion that some unexpected character may be producing \na problem with the algorithm to check the password, instead of logging the \npassword, some code can be added to detect whether there's an invalid character.\nFor example, assuming there's a problem with a password or secret that has an \nemoji, we could extract only non-ASCII characters to find out if this is the problem, \nlike this:\n>>> password = 'secret password \n'\n>>> bad_characters = [c for c in password if not c.isascii()]\n>>> bad_characters\n['\n']\nThe value in bad_characters can be then logged, as it won't contain the full \npassword. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1978,
      "extraction_method": "Direct"
    },
    {
      "page_number": 534,
      "chapter": null,
      "content": "Chapter 15\n[ 515 ]\nAdding temporal logs is cumbersome, as it typically will involve several \ndeployments until finding out the problem. It's always important to keep the number \nof logs to a minimum, cleaning up the useless ones as quickly as possible, and \nremember to remove them completely after the work is done.\nRemember that the work is just to be able to reproduce the problem locally, so you \ncan more efficiently investigate and fix the problem locally. Sometimes the problem \nmay be deemed obvious after some temporal log, but, good TDD practice, as we saw \nin Chapter 10, Testing and TDD, tests displaying and then fixing the bug.\nOnce we can detect the problem locally, it is time to go to the next step.\nLocal debugging\nDebugging locally means exposing and fixing a problem once we have a local \nreproduction.\nThe basic steps of debugging are reproducing the problem, knowing what the \ncurrent, incorrect result is, and knowing what the correct result should be. With that \ninformation, we can start debugging.\nTaking a step back, any debugging process follows the following process:\n1.\t You realize there's a problem\n2.\t You understand what the correct behavior should be\n3.\t You investigate and discover why the current system behaves incorrectly\n4.\t You fix the problem\nNote that this assumption is probably easier to test quickly and \nwithout any secret data with a unit test. This is just an example.\nA great way of creating the reproduction of the problem is with a \ntest, if that's possible. As we saw in Chapter 10, Testing and TDD, \nthis is the basis of TDD. Create a test that fails and then change \nthe code to make it pass. This approach is very usable when fixing \nbugs.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 535,
      "chapter": null,
      "content": "Debugging\n[ 516 ]\nKeeping this process in mind is also useful from a local debugging perspective, \nthough at this point, it is likely that steps 1 and 2 are already sorted out. In the vast \nmajority of cases, the difficult step is 3, as we've been seeing throughout the chapter.\nTo understand, once presented with the code, why the code is behaving as it is, a \nmethod similar to the scientific method can be used to systematize the approach:\n1.\t Measure and observe the code\n2.\t Produce a hypothesis on why a certain result is being produced\n3.\t Validate or disprove the hypothesis by either analyzing the produced state, if \npossible, or creating a specific \"experiment\" (some specific code, like a test) to \nforce it to be produced\n4.\t Use the resulting information to iterate the process until the source of the \nproblem is totally understood\nNote that this process doesn't necessarily need to be applied to the whole problem. \nIt can be focused on the specific parts of the code that can influence the problem. For \nexample, is this setting activated in this case? Is this loop in the code being accessed? \nIs the value calculated lower than a threshold, which will later send us down a \ndifferent code path? \nAll those answers will increase the knowledge of why the code is behaving in the \nway that it's behaving.\nDebugging is a skill. Some people may say it's an art. In any case, it can be improved \nover time, as more time gets invested in it. Practice plays an important role in \ndeveloping the kind of intuition that involves knowing when to take a deeper look \ninto some areas over others to identify the promising areas where the code may be \nfailing. \nThere are some general ideas that can be very helpful when approaching debugging:\n•\t\nDivide and conquer. Take small steps and isolate areas of the code so \nit's possible to simplify the code and make it digestible. As important as \nunderstanding when there's a problem in the code is detecting when there \nisn't so we can set our focus on the relevant bits. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2127,
      "extraction_method": "Direct"
    },
    {
      "page_number": 536,
      "chapter": null,
      "content": "Chapter 15\n[ 517 ]\n•\t\nMove backward from the visible error. It's quite common that the source \nof a problem is not where an error is raised or obvious, but instead the \nerror was produced earlier. A good approach is to move backward from \nthe position where the problem is obvious and then validate the flow. This \nallows you to ignore all code that comes after the problem, and have a clear \npath of analysis.\n•\t\nYou can make an assumption, as long as you can then prove that this \nassumption is correct. Code is complex, and you won't be able to keep the \nwhole codebase in your head. Instead, focus needs to be carefully moved \nacross different parts, making assumptions about what the rest is returning.\nProperly eliminating everything can be arduous, but removing proven \nassumptions from the mind will reduce the amount of code to analyze and \nverify. \nBut those assumptions need to be validated to really prove that they are \ncorrect, or we risk the chance of making a wrong assumption. It's very easy \nto fall into bad assumptions and think that the problem is in a particular part \nof the code when it really is in another.\nThough the whole range of techniques and possibilities of debugging is there, and \ncertainly sometimes bugs can be convoluted and difficult to detect and fix, most bugs \nare typically easy to understand and fix. Perhaps they are a typo, an off-by-one error, \nor a type error that needs to be checked. \nEdward J. Gauss described this method in what he called \nthe \"wolf fence algorithm\" in a 1982 article:\nThere's one wolf in Alaska; how do you find it? First build a \nfence down the middle of the state, wait for the wolf to howl, \ndetermine which side of the fence it is on. Repeat process on that \nside only, until you get to the point where you can see the wolf.\nAs Sherlock Holmes once said:\nWhen you have eliminated the impossible, whatever remains, \nhowever improbable, must be the truth.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 537,
      "chapter": null,
      "content": "Debugging\n[ 518 ]\nBefore we move on to specific techniques, we need to understand the tools in Python \nhelp in our investigation.\nPython introspection tools \nAs Python is a dynamic language, it's very flexible and allows you to perform actions \non its objects to discover their properties or types.\nThis is called introspection, and allows you to inspect elements without having too \nmuch context about the objects to be inspected. This can be performed at runtime, so \nit can be used while debugging to discover the attributes and methods of any object.\nThe main starting point is the type function. The type function simply returns the \nclass of an object. For example:\n>>> my_object = {'example': True}\n>>> type(my_object)\n<class 'dict'>\n>>> another_object = {'example'}\n>>> type(another_object)\n<class 'set'>\nThis can be used to double-check that an object is of the expected type. \nA typical example error is to have a problem because a variable can be either an \nobject or None. In that case, it's possible that a mistake handling the variable makes it \nnecessary to double-check that the type is the expected one.\nWhile type is useful in debugging environments, avoid using it directly in your code. \nFor example, avoid comparing defaults of None, True, and False with their types, as \nthey are created as singletons. That means there's a single instance of each of these \nobjects, so every time that we need to verify if an object is None, it's better to make an \nidentity comparison, like this:\n>>> object = None\n>>> object is None\nTrue\nKeeping the code simple helps a lot in later debugging problems. \nSimple code is easy to understand and debug.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 538,
      "chapter": null,
      "content": "Chapter 15\n[ 519 ]\nIdentity comparisons can prevent the kind of problem where the usage of None or \nFalse can't be distinguished in an if block.\n>>> object = False\n>>> if not object:\n...     print('Check valid')\n...\nCheck valid\n>>> object = None\n>>> if not object:\n...     print('Check valid')\n...\nCheck valid\nInstead, only checking against the identity comparison will allow you to detect only \nthe value of None properly.\n>>> object = False\n>>> if object is None:\n...     print('object is None')\n...\n>>> object = None\n>>> if object is None:\n...     print('object is None')\n...\nobject is None\nThe same can be used for Boolean values.\n>>> bool('Testing') is True\nTrue\nFor other cases, there's the isinstance function, which can be used to find if a \nparticular object is an instance of a particular class:\n>>> class A:\n...     pass\n...\n>>> a = A()\n>>> isinstance(a, A)\nTrue\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 975,
      "extraction_method": "Direct"
    },
    {
      "page_number": 539,
      "chapter": null,
      "content": "Debugging\n[ 520 ]\nThis is better than making comparisons with type, because it is aware of any \ninheritance that may have been produced. For example, in the following code we see \nhow an object from a class that inherits from another will return that it's an instance \nof either, while the type function will only return one.\n>>> class A:\n...     pass\n...\n>>> class B(A):\n...     pass\n...\n>>> b = B()\n>>> isinstance(b, B)\nTrue\n>>> isinstance(b, A)\nTrue\n>>> type(b)\n<class '__main__.B'>\nThe most useful function for introspection, though, is dir. dir allows you to see all \nthe methods and attributes in an object, and it's particularly useful when analyzing \nobjects from a not-clear origin, or where the interface is not clear.\n>>> d = {}\n>>> dir(d)\n['__class__', '__class_getitem__', '__contains__', '__delattr__', '__\ndelitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__\ngetattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__\ninit_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', \n'__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__\nrepr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__\nsizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', \n'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', \n'values']\nObtaining the whole attributes can be a bit too much in certain situations, so the \nreturned values can filter out the double-underscore ones to reduce the amount of \nnoise and be able to detect attributes that can give some clue about the object usage \nmore easily.\n>>> [attr for attr in dir(d) if not attr.startswith('__')]\n['clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', \n'setdefault', 'update', 'values']\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 540,
      "chapter": null,
      "content": "Chapter 15\n[ 521 ]\nAnother interesting function is help, which displays the help from objects. This is \nparticularly helpful for methods:\n>>> help(d.pop)\nHelp on built-in function pop:\npop(...) method of builtins.dict instance\n    D.pop(k[,d]) -> v, remove specified key and return the \ncorresponding value.\n    If key is not found, default is returned if given, otherwise \nKeyError is raised\nThis function displays the defined docstring from the object.\n>>> class C:\n...     '''\n...     This is an example docstring\n...     '''\n...     pass\n...\n>>> c = C()\n>>> help(c)\nHelp on C in module __main__ object:\nclass C(builtins.object)\n |  This is an example docstring\n |\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables (if defined)\n |\n |  __weakref__\n |      list of weak references to the object (if defined)\nAll these methods can help you navigate code that's new or under analysis without \nbeing an expert, and avoid many checks with code that can be hard to search \nthrough.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 541,
      "chapter": null,
      "content": "Debugging\n[ 522 ]\nUsing these tools is good, but let's see how we can understand the behavior of the \ncode.\nDebugging with logs\nA simple yet effective way of detecting what's going on and how the code is being \nexecuted is adding comments that are displayed either containing statements like \nstarting the loop here or including values of variables like Value of A = X. By \nstrategically locating these kinds of outputs, the developer can understand the flow \nof the program.\nThe simplest form of this approach is print debugging. It consists of adding print \nstatements to be able to watch the output from them, normally while executing the \ncode locally in a test or similar.\nObviously, these print statements need to be removed after the process has \nbeen finished. One of the main complaints about this technique is precisely this, \nthat there's a chance that some print statements intended for debugging are not \nremoved, and it's a common mistake.\nThis can be refined, though, by instead of directly using print statements, using logs \ninstead, as we introduced in Chapter 12, Logging.\nAdding sensible docstrings is a great help not only for keeping \nthe code well commented and adding context for developers \nworking in the code, but also in case of debugging in parts \nwhere the function or object is used. You can learn more about \ndocstrings in the PEP 257 document: https://www.python.\norg/dev/peps/pep-0257/.\nWe touched on this earlier in this chapter as well as in Chapter 10, \nTesting and TDD. \nPrint debugging can be considered a bit controversial to some \npeople. It has been around for a long time, and it's considered a \ncrude way of debugging. In any case, it can be very quick and \nflexible and can fit some debug cases very well, as we will see.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 542,
      "chapter": null,
      "content": "Chapter 15\n[ 523 ]\nThe advantage of this method is that it can be done quickly and it can also be used to \nexplore logs that can then be promoted to permanent ones, once adapted.\nAnother important advantage is that tests can be run very quickly, as adding more \nlogs is a simple operation, and logs won't interfere with the execution of code. This \nmakes it a good combination to use with TDD practices.\nThe fact that the logs won't interfere with the code and code can be running \nunaffected can make some difficult bugs based on concurrency easier to debug, \nas interrupting the flow of the operation in those cases will affect the behavior of \nthe bug.\nWhile debugging through logs can be quite convenient, it requires certain knowledge \nof where and what logs to set to obtain the relevant information. Anything not \nlogged won't be visible in the next run. This knowledge can come through a \ndiscovery process and take time to pinpoint the relevant information that will lead to \nfixing the bug.\nAnother problem is that new logs are new code, and they can create problems if \nthere are errors introduced like bad assumptions or typos. This will normally be easy \nto fix, but can be an annoyance and require a new run.\nRemember that all introspection tools that we talked about before in the chapter are \navailable.\nIdeally, these logs will be DEBUG logs, which will only be displayed \nwhen running tests, but won't be produced in a production \nenvironment. \nWhile logs can be added and not produced later, it's good practice \nanyway to remove any spurious logs after fixing the bug. Logs \ncan accumulate and there will be an excessive amount of them \nunless they are periodically taken care of. It can be difficult to find \ninformation in a big wall of text.\nConcurrent bugs can be quite complicated. They are produced \nwhen two independent threads interact in an unexpected way. \nBecause of the uncertain nature of what one thread will start and \nstop or when an action from one thread will affect the other, they \nnormally require extensive logs to try to capture the specifics of \nthat problem.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2204,
      "extraction_method": "Direct"
    },
    {
      "page_number": 543,
      "chapter": null,
      "content": "Debugging\n[ 524 ]\nDebugging with breakpoints\nIn other situations, it's better to stop the execution of the code and take a look at the \ncurrent status. Given that Python is a dynamic language, it means that, if we stop the \nexecution of the script and enter the interpreter, we can run any kind of code and see \nits results.\nThis is exactly what is done through the usage of the breakpoint function.\nWhen the interpreter finds a breakpoint call, it stops and opens an interactive \ninterpreter. From this interactive interpreter, the current status of the code can be \nexamined and any investigation can take place, simply executing the code. This \nmakes it possible to understand interactively what the code is doing.\nLet's take a look at some code and analyze how it runs. The code can be found on \nGitHub at https://github.com/PacktPublishing/Python-Architecture-Patterns/\nblob/main/chapter_15_debug/debug.py and it's the following:\ndef valid(candidate):\n    if candidate <= 1:\n        return False\n    lower = candidate - 1\n    while lower > 1:\n        if candidate / lower == candidate // lower:\n            return False\n    return True\nassert not valid(1)\nbreakpoint is a relatively new addition to Python, available since \nPython 3.7. Previously, it was necessary to import the module pdb, \ntypically in this way in a single line:\nimport pdb; pdb.set_trace()\nOther than the ease of usage, breakpoint has some other \nadvantages that we will see.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 544,
      "chapter": null,
      "content": "Chapter 15\n[ 525 ]\nassert valid(3)\nassert not valid(15)\nassert not valid(18)\nassert not valid(50)\nassert valid(53)\nPerhaps you are able to understand what the code does, but let's take a look at it \ninteractively. You can check first that all the assert statements at the end are correct.\n$ python3 debug.py\nBut we now introduce a breakpoint call before line 9, right at the start of the while \nloop.\n    while lower > 1:\n        breakpoint()\n        if candidate / lower == candidate // lower:\n            return False\nExecute the program again and it now stops at that line and presents an interactive \nprompt:\n$ python3 debug.py\n> ./debug.py(10)valid()\n-> if candidate / lower == candidate // lower:\n(Pdb)\nCheck the value of candidate and both operations.\n(Pdb) candidate\n3\n(Pdb) candidate / lower\n1.5\n(Pdb) candidate // lower\n1\nThis line is checking whether dividing candidate by lower produces an exact integer, \nas in that case both operations will return the same. Execute the next line by hitting n, \nfrom the command n(ext), and check that the loop ends and it returns True:\n(Pdb) n\n> ./debug.py(13)valid()\n-> lower -= 1\n(Pdb) n\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1239,
      "extraction_method": "Direct"
    },
    {
      "page_number": 545,
      "chapter": null,
      "content": "Debugging\n[ 526 ]\n> ./debug.py(8)valid()\n-> while lower > 1:\n(Pdb) n\n> ./debug.py(15)valid()\n-> return True\n(Pdb) n\n--Return--\n> ./debug.py(15)valid()->True\n-> return True\nContinue the execution until a new breakpoint is found using the command c, from \nc(ontinue). Note this happens on the next call to valid, which has an input of 15.\n(Pdb) c\n> ./debug.py(10)valid()\n-> if candidate / lower == candidate // lower:\n(Pdb) candidate\n15\nYou can also use the command l(ist) to display the surrounding code.\n(Pdb) l\n  5\n  6  \t\n    lower = candidate - 1\n  7\n  8  \t\n    while lower > 1:\n  9  \t\n        breakpoint()\n 10  ->\t\n        if candidate / lower == candidate // lower:\n 11  \t\n            return False\n 12\n 13  \t\n        lower -= 1\n 14\n 15  \t\n    return True\nContinue freely investigating the code. When you are finished, run q(uit) to exit.\n(Pdb) q\nbdb.BdbQuit\nAfter analyzing the code carefully, you probably know what it does. It checks \nwhether a number is prime or not by checking if it's divisible by any number lower \nthan the number itself.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 546,
      "chapter": null,
      "content": "Chapter 15\n[ 527 ]\nAnother two useful debug commands are s(tep), to get into a function call, and \nr(eturn), to execute the code until the current function returns its execution.\nbreakpoint can also be customized to call other debuggers, not only pdb. There are \nother debuggers for Python that include more contextual information or with more \nadvanced usages, like ipdb (https://pypi.org/project/ipdb/). To use them, you \nneed to set the PYTHONBREAKPOINT environment variable with the endpoint for the \ndebugger, after installing the debugger.\n$ pip3 install ipdb\n…\n$ PYTHONBREAKPOINT=IPython.core.debugger.set_trace python3 debug.py\n> ./debug.py(10)valid()\n      8     while lower > 1:\n      9         breakpoint()\n---> 10         if candidate / lower == candidate // lower:\n     11             return False\n     12\nipdb>\nThere are multiple debuggers that can be used, including support from IDEs like \nVisual Studio or PyCharm. Here are examples of two other debuggers:\n•\t\npudb (https://github.com/inducer/pudb): Has a console-based graphical \ninterface and more context around the code and variables\n•\t\nremote-pdb (https://github.com/ionelmc/python-remote-pdb): Allows you \nto debug remotely, connecting to a TCP socket. This allows you to debug a \nprogram running in a different machine or trigger the debugger in a situation \nwhere there's no good access to the stdout of the process, for example, \nbecause it's running in the background\nWe investigated similar code and improvements in Chapter 14, \nProfiling. This is, needless to say, not the most efficient way of \nsetting code to check this, but it has been added as an example and \nfor teaching purposes.\nThis environment variable can be set to 0 to skip any breakpoint, \neffectively deactivating the debug process: PYTHONBREAKPOINT=0. \nThis can be used as a failsafe to avoid being interrupted by \nbreakpoint statements that haven't been properly removed, or to \nquickly run the code without interruptions.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 547,
      "chapter": null,
      "content": "Debugging\n[ 528 ]\nUsing a debugger properly is a skill that requires time to learn. Be sure to try \ndifferent options and get comfortable with them. Debugging will also be used while \nrunning tests, as we described in Chapter 10, Testing and TDD.\nSummary\nIn this chapter, we described the general process of detecting and fixing problems. \nWhen working in complex systems, there's the challenge of properly detecting \nand categorizing the different reports to be sure that they are prioritized. It's very \nimportant to be able to reliably reproduce the problem in order to show all the \nconditions and context that are producing the issue.\nOnce a problem is deemed important, there needs to be an investigation into why \nthis problem is happening. This can be on the running code, and use the available \ntools in production to see if it can be understood why the problem occurs. The \nobjective of this investigation is to be able to replicate the problem locally. \nMost issues will be easy to reproduce locally and move forward, but we also \ndescribed some tools in case it remains a mystery why the issue is being produced. \nAs the main tool to understand the behavior of the code in production is logs, \nwe talked about creating a request ID that can help us to trace the different calls \nand relate logs from different systems. We also described how the data in the \nenvironment may have the key to why the problem is occurring there. If it is \nnecessary, the number of logs may need to be increased to extract information from \nproduction, though this should be reserved for very elusive bugs.\nWe then moved on to how to debug locally, after replicating the problem, ideally, \nas we saw in Chapter 10, Testing and TDD, in the form of a unit test. We gave some \ngeneral ideas to help with debugging, though it must be said that debugging is a skill \nthat needs to be practiced.\nDebugging can be learned and improved, so it's an area where \nmore experienced developers can help their junior counterparts. \nBe sure to create a team where it is encouraged to help with \ndebugging when required in difficult cases. Two pairs of eyes see \nmore than one!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2253,
      "extraction_method": "Direct"
    },
    {
      "page_number": 548,
      "chapter": null,
      "content": "Chapter 15\n[ 529 ]\nWe introduced some of the tools that help with debugging in Python, which make \nuse of the possibilities that Python presents for introspection. As Python is a dynamic \nlanguage, there are a lot of possibilities, as it's able to execute any code, including all \nthe introspection capabilities.\nWe then talked about how to create logs to debug, which is an improved version \nof using print statements, and, when done in a systematic way, can help to create \nbetter logs in the long run. Finally, we moved on to debugging using the breakpoint \nfunction call, which stops the execution of the program and allows you to inspect \nand understand the status at that point, as well as continuing with the flow.\nIn the next chapter, we will talk about the challenges of working in the architecture \nof a system when it's running and needs to be evolved.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1124,
      "extraction_method": "Direct"
    },
    {
      "page_number": 549,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 550,
      "chapter": null,
      "content": "[ 531 ]\n16\nOngoing Architecture\nJust as software itself is never truly complete, software architecture is never a \nfinished piece of work. There are always changes, adjustments, and tweaks that need \nto be performed in order to improve the system: adding new features; improving \nperformance; fixing security problems. While good architecture requires us to \nunderstand deeply how to design a system, the reality of the ongoing process is \nmore about making changes and improvements.\nWe will talk in this chapter about some of those aspects, as well as dealing with \nsome of the techniques and ideas around making changes in a real working system, \nkeeping in mind that the process can always be improved further by reflecting on \nhow the process is performed and following some guidelines to ensure that the \nsystem can be changed continuously while at the same time maintaining service \nto customers.\nIn this chapter, we'll cover the following topics:\n•\t\nAdjusting the architecture\n•\t\nScheduled downtime\n•\t\nIncidents\n•\t\nLoad testing\n•\t\nVersioning\n•\t\nBackward compatibility\n•\t\nFeature flags\n•\t\nTeamwork aspects of changes\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1224,
      "extraction_method": "Direct"
    },
    {
      "page_number": 551,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 532 ]\nLet's start by taking a look at why to make changes in the architecture of a system.\nAdjusting the architecture\nWhile for most of this book we've been talking about system design, which is the \nbasic function of an architect, it is most likely that the bulk of their day-to-day job \nwill be more focused on redesigns. \nThis is always an endless task, as working software systems are always under \nrevision and expansion. Some of the reasons why it may be necessary to adjust the \narchitecture of a system are as follows:\n•\t\nTo provide certain features or characteristics previously not available – for \nexample, adding an event-driven system to run asynchronous tasks, allowing \nus to avoid the request-response pattern that was previously all that was \navailable.\n•\t\nBecause there are bottlenecks or limitations with the current architecture. For \nexample, only a single database is present in the system and there's a limit on \nthe number of queries that can run.\n•\t\nAs systems grow, it may be necessary to divide parts to allow better control \nover them – for example, dividing a monolith into microservices, as we saw \nin Chapter 8, Advanced Event-Driven Structures.\n•\t\nTo increase the security of the system – for example, removing or encoding \nstored information that might be sensitive, like emails addresses and other \npersonally identifiable information (PII).\n•\t\nBig API changes, like introducing a new version of an API either internally or \nexternally. For example, adding a new endpoint that works better for other \ninternal systems to perform some action, where the calling services should be \nmigrated.\n•\t\nChanges in the storage system, including all the different ideas that we \ndiscussed in Chapter 3, Data Modeling when talking about distributed \ndatabases. This could also include adding or replacing existing storage \nsystems.\n•\t\nTo adapt technologies that are obsolete. This can happen in legacy systems \nthat have a critical component that is no longer supported, or a fundamental \nsecurity problem. For example, replacing an old module with another \nthat is capable of using new security processes because the old one is not \nmaintained anymore and relies on old encryption methods.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2336,
      "extraction_method": "Direct"
    },
    {
      "page_number": 552,
      "chapter": null,
      "content": "Chapter 16\n[ 533 ]\n•\t\nRewrites using new languages or technology. This can be done to consolidate \ntechnologies if at some point a system was created using a different \nlanguage, and, after a while, it is decided to bring it in line with the most \nused language to allow better maintenance. This scenario is typical in \norganizations that experienced growth, and at some point, a team decided \nto use their favorite language to create a service. After some time, this may \ncause problems by complicating maintenance as expertise in this language \nmay be lacking. This can be even worse if the original developer has left the \norganization. It could be better to adjust or rewrite the service by integrating \nit into an existing one or replace it with an equivalent one in the preferred \nlanguage.\n•\t\nOther kinds of technical debt – for example, refactors that can clean the code \nand make it more readable, or to allow for changing names of components to \nbe more precise, among other things.\nThese are just some examples, but the truth is that all systems require constant \nupdating and adjusting, as software is rarely a finished task.\nThe challenge is not only to design these changes to achieve the expected results, but \nalso to move from the starting point to the destination with minimal interruption to \nthe system. These days the expectation is that online systems are only very rarely \ninterrupted, setting a high bar for any change.\nTo achieve this, changes need to be taken in small steps, taking extra care to ensure \nthat the system is available at all points.\nScheduled downtime\nWhile ideally there should be no interruption in the system as a result of the \nchanges made, sometimes it's simply not possible to perform big changes without \ninterrupting the system.\nWhen and whether it's sensible to have downtime may depend \ngreatly depending on the system. For example, in its first years \nof operation, the popular website Stack Overflow (https://\nstackoverflow.com/) had frequent downtime, initially even \nevery day, where the webpage returned a \"down for maintenance\" \npage during the morning hours in Europe. That changed \neventually, and now it's rare to see that kind of message. \nBut that was acceptable in the early stages of the project as the bulk \nof their users used the site in line with North American hours and \nit was (and still is) a free website.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 553,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 534 ]\nScheduling downtime is always an option, but it's a costly one, so it needs to be \ndesigned in a way that minimizes the impact on the operations. If the system is an \nestablished 24x7 service that's critical for customers, or produces income for the \nbusiness while up (like a store, for example), any downtime will have a pretty hefty \nprice tag.\nIn other cases, like a small new service with very little traffic, customers will either be \nmore understanding or there'll even be a good chance that they will be unaffected. \nScheduled downtime should be communicated beforehand to affected customers. \nThis communication can take multiple forms, and will greatly depend on the kind of \nservice. For example, a public web store may announce downtime with a banner on \ntheir page during the week informing that it won't be available on Sunday morning, \nbut scheduling downtime for a banking operation may require months of advance \nnotice and negotiation over when is the best time.\nIf possible, is a good practice to define maintenance windows to properly set clear \nexpectations about times when the service will or might have a high risk of some sort \nof interruption.\nMaintenance window\nMaintenance windows are periods where it is communicated beforehand that \nmaintenance might happen. The idea is to guarantee the stability of the system \noutside of maintenance windows while allocating clear times where maintenance \nmight happen.\nA maintenance window could perhaps be at weekends or nights in the most active \ntimezone for the system. During the busiest hours of activity the service remains \nuninterrupted, and maintenance is only carried over when it can't wait, like when \npreventing or fixing a critical incident.\nMaintenance windows are different than scheduled downtime. While in some cases \nit will happen, not every maintenance window needs to involve downtime – there is \nsimply the possibility that it might happen.\nNot every maintenance window needs to be defined equally – some may be safer \nthan others and capable of doing more extensive maintenance. For example, \nweekends may be reserved for scheduled downtime, but nights during the working \nweek may see regular deployments.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2329,
      "extraction_method": "Direct"
    },
    {
      "page_number": 554,
      "chapter": null,
      "content": "Chapter 16\n[ 535 ]\nIt's important to communicate maintenance windows in advance, for example \ndesigning a table like the following:\nDays\nTime\nType of \nmaintenance \nwindow\nRisk\nComments\nMonday to \nThursday\n08:00 – 12:00 UTC\nRegular \nmaintenance\nLow risk\nRegular deployments \nconsidered low risk. No \nimpact to service.\nSaturday\n08:00 – 18:00 UTC\nSerious \nmaintenance\nHigh risk\nAdjustments considered \nrisky. While the \nexpectation is that the \nservice will be fully \navailable, there is a \nchance that it will be \ninterrupted at some \npoint during the \nwindow.\nSaturday\n08:00 – 18:00 UTC\nNotified \nScheduled \ndowntime\nService \nunavailable\nOne month's notice \ngiven. Essential \nmaintenance that \nrequires the service to be \nunavailable.\nAn important detail about maintenance windows is that they should be big enough \nto allow ample time for the maintenance to be done. Be sure to be generous with \ntime, as it's better to set expectations with a large maintenance window that can \nbe used safely for any eventuality, rather than a short one that often needs to be \nextended.\nWhile scheduled downtime and maintenance windows will help frame the times \nwhere the service is active and what times are riskier for the user, it's still possible \nthat some problem arises and causes a problem in the system.\nIncidents\nUnfortunately, at some point in its life, the system won't behave as it should. It will \nproduce an error so important that it needs to be taken care of immediately.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1578,
      "extraction_method": "Direct"
    },
    {
      "page_number": 555,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 536 ]\nAn incident is defined as a problem that disrupts the service so much that it requires \nan emergency response. \nDuring incidents, using all monitoring tools available is critical to find the problem \nas soon as possible and be able to correct it. Reaction times should be as fast as \npossible while keeping the risk of corrective actions as low as possible. A balance \nneeds to be struck here, and depending on the nature of the incident, riskier actions \ncan be taken, for example when the system is completely down, as recovering the \nsystem will be more important.\nRecovery during incidents will normally be limited by two factors:\n•\t\nHow good the monitoring tools are at detecting and understanding problems\n•\t\nHow fast a change can be introduced in the system, related to how quick it is \nto change a parameter or to deploy new code\nThe first of the above points is the understand part and the second is the solve part \n(though it may be necessary to make changes to get a better understanding of the \nproblem, as we saw in Chapter 14, Profiling). \nThis is why these two elements, the observability and the time required to make \na change, are so important. In normal situations, taking a long time to deploy or \nto make a change is normally just a minor annoyance, but in a critical situation, it \ncould hinder the fixes that can help the health of the system to recover. \nThis doesn't necessarily mean that the full service is totally \ninterrupted – it could be a noticeable degradation of the external \nservice, or even a problem in one internal service that reduces the \nquality of service overall. For example, if an asynchronous task \nhandler is failing 50% of the time, external customers may only see \nthat their tasks take longer, but that is probably important enough \nto take corrective action.\nWe cover both of these aspects in the book, with the observability \ntools examined in Chapter 11, Package Management, and Chapter \n12, Logging. We also may need to use the techniques described in \nChapter 14, Profiling.\nIntroducing changes to the system is tightly related to the \nContinuous Integration (CI) techniques that we discussed \nin Chapter 4, The Data Layer. A fast CI pipeline can make a big \ndifference in how long it takes new code to be ready to deploy.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2409,
      "extraction_method": "Direct"
    },
    {
      "page_number": 556,
      "chapter": null,
      "content": "Chapter 16\n[ 537 ]\nThe reaction to an incident is a complicated process that requires flexibility and \nimprovisation, which improve with experience. But there needs to be as well a \ncontinuous process of improving the uptime of the system and understanding the \nweakest part of the system, to avoid the problems or minimize them.\nPostmortem analysis\nPostmortem analysis, also called a post-incident review, is an analysis done after a \nproblem has impacted the service. Its objective is to understand what failed, why, \nand take corrective measures to ensure that the problem doesn't happen again, or at \nleast that it has a reduced impact.\nTypically, a postmortem starts with the people involved in the correction of the \nproblem filling in a template form. Having a template predefined helps to shape the \ndiscussion and focus on the remediation to carry out.\nThe basic template should start with all the main details of what happened, followed \nby why it happened, and finally, the most important part: what are the next actions \nto correct the problem?\nFor example, a simple template could be the following:\nIncident report\n1.\t Summary. A brief description of what happened.\nExample: The service went down between 08:30 and 9:45 UTC on the 5th of \nNovember. \n2.\t Impact. Describe the impact of the problem. What was the external problem? \nHow external users were affected?\nExample: All user requests were returning 500 errors. \nThere are plenty of postmortem templates available online that you \ncan search through to see if there's a particular one that you like, \nor just to get ideas. As with any other part of the process, it should \nbe improved and refined as it goes along. Remember to create and \ntweak your own template.\nRemember that a postmortem analysis happens after the incident \nis over. While it could be good to take some notes while is \nhappening, the focus during an incident is to fix it first. Focus on \nthe most important thing first.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 557,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 538 ]\n3.\t Detection. A description of how it was detected initially. Could it have been \ndetected earlier?\nExample: The monitoring system alerted about the problem at 8:35 UTC, after \n5 minutes of 100% error requests.\n4.\t Response. Actions taken to correct the problem.\nExample: John cleaned the disk space in the database server and restarted the \ndatabase.\n5.\t Timeline. A timeline of events to understand how the incident developed \nand how long each phase took.\nExample:\n8:30 Start of the problem.\n8:35 An alert in the monitoring system was triggered. John started looking \ninto the problem.\n8:37 It is detected that the database is unresponsive and cannot be restarted.\n9:05 After investigation, John discovered that the database disk was full.\n9:30 The logs in the database server had filled up the server disk space, \ncausing the database server to crash.\n9:40 Old logs are removed from the server, freeing disk space. The database \nis restarted.\n9:45 Service is restored.\n6.\t Root cause. A description of the identified root cause of the problem that, if \nfixed, will completely remove this problem.\nDetecting the root cause is not necessarily easy, as \nsometimes a chain of events will be involved. To help find \nthe root cause, you can use the five whys technique. Start \ndescribing the impact and ask why it happened. Then ask \nwhy this happened, and so on. Keep iterating until you \nhave asked \"why?\" five times, and the resulting one will be \nthe root cause. Don't take this to mean that you must ask \n\"why?\" exactly five times, but keep going until you can get \na solid answer.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 558,
      "chapter": null,
      "content": "Chapter 16\n[ 539 ]\nTake into account that the investigation can go further than the steps taken \nto recover the service during the incident, where a quick fix may have been \nenough to get out of the woods.\nExample: \nThe server returned errors. Why?\nBecause the database had crashed. Why?\nBecause the database server ran out of space. Why?\nBecause the space was fully filled with logs. Why?\nBecause the log space on the disk was not limited and could grow indefinitely.\n7.\t Lessons learned. Things that could be improved in the process, as well as \nany other element that went well and could be useful to know, like the usage \nof a certain tool or metric that was useful when analyzing the problem. \nExample:\nThe amount of disk space that logs use should be limited in all cases.\nThe disk space itself is not being monitored or alerted before it completely \nruns out.\nThe alerting system is too slow and requires a high level of errors before \nalerting.\n8.\t Next actions. The most important part of the process. Describe what actions \nshould be performed to eliminate or, if that's not possible, mitigate the \nproblem. Be sure that these actions have clear owners and are followed up.\nNot only should the root cause be addressed, but also any possible \nimprovements detected in the lessons learned part.\nExample:\nAction: Enable log rotation to limit the amount of space that logs can take up \nin all servers, starting with the database. Assigned to the operations team.\nIf there's a ticketing system, these actions should be \ntransformed into tickets and be prioritized accordingly to \nbe sure that the proper team implements them. \n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 559,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 540 ]\nAction: Monitor and alert on the disk space to raise an alert if the disk space \nhas less than 20% of the total available space, to allow faster reactions. \nAssigned to the operations team.\nAction: Tweak the error alert to change it to alert when there's only one \nminute of 30% or more requests returning errors. Assigned to the operations \nteam.\nNote that the template doesn't have to be filled out in one go. Typically, the template \nwill be filled in as much as possible, and a postmortem meeting will be held, when \nthe incident can be analyzed and the template totally filled in, including the Next \naction part, which, again, is the most important part of the analysis.\nIn recent years, an equivalent process to try to foresee problems has been put in \nplace, especially before an important event.\nPremortem analysis\nThe premortem analysis is an exercise to try to analyze what could go wrong before \nan important event. The event could be some milestone, launch event, or something \nsimilar that is expected to significantly change the conditions of the system.\nFor example, there could be a marketing campaign launch that is expected to double \nor triple the amount of traffic that had previously been normal.\nThe premortem analysis is the reverse of a postmortem. You set your mindset in the \nfuture and ask: What went wrong? What is the worst-case scenario? From there, you \nverify your assumptions about your system and prepare for them.\nKeep in mind that it's crucial that postmortem processes are \nfocused on improving the system and not on assigning blame for \nthe problem. The objective of the process is to detect weak spots \nand to try to make sure that problems are not repeated.\nThe word \"premortem\" is quite a funny neologism that comes from \nthe usage of \"postmortem\" as a way to refer to an analysis done after \nthe fact, making an analogy with an autopsy. Though hopefully, \nnothing is dead yet!.\nIt can also be called a preparation analysis.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2097,
      "extraction_method": "Direct"
    },
    {
      "page_number": 560,
      "chapter": null,
      "content": "Chapter 16\n[ 541 ]\nConsider an analysis for the above example of tripling the amount of traffic on the \nsystem. Can we simulate the conditions to verify that our system is ready for it? \nWhich elements of the system do we think are less robust?\nAll that can lead to planning for the different scenarios and running tests to ensure \nthat the system will be ready for the event.\nWhen doing any premortem analysis, be sure to have enough time to perform the \nnecessary actions and tests to prepare the system. As usual, actions will have to be \nprioritised to be sure that time is well spent. But keep in mind that this preparation \ncan be an endless task, and as time will be limited, it needs to be focused on the \nmost important or sensitive parts of the system. Be sure to use as many data-driven \nactions as possible and focus the analysis on real data and not hunches.\nLoad testing\nA key element of preparation in these cases is load testing. \nLoad testing is creating a simulated load that goes to an increased level of traffic. It \ncan be done in an explorative way, i.e., let's find out what the limits of our system are; \nor in a confirmative way, i.e., let's double-check that we can reach this level of traffic.\nLoad testing is typically done not in production environments, but in staging ones, \nreplicating the configuration and hardware in production, though it is normal to \ncreate a final load test verifying that the configuration in the production environment \nis the correct one.\nThe basic element of a load test is to simulate a typical user performing actions on \nthe system. For example, a typical user can log in, check a few pages, add some \ninformation, and then log out. We can replicate this behavior using automated tools \nthat work on our external interface.\nAn interesting part of load testing analysis in cloud environments \nis to ensure that any autoscaling in the system works correctly, so \nit provisions more hardware automatically when receiving greater \nload, and deletes it when it's not necessary. Caution is required \nhere, as a full load test to the maximum capacity of the cluster can \nbe expensive each time it's run.\nA good way of using these tools is reusing any kind of automated \ntesting that can be created, and using it as well as the basis for the \nsimulation. This makes the integration or system test framework \nthe unit to enable load testing.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2496,
      "extraction_method": "Direct"
    },
    {
      "page_number": 561,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 542 ]\nThen, we can multiply that unit simulating the behavior for a single user multiple \ntimes to simulate the effect of N users, producing enough load to test our system. \nIf necessary, or to perform tweaks, logs can be analyzed to generate an adequate \nprofile of the typical interfaces exercised by the users. Remember to relay in data \nwhen possible. Load tests, though, are sometimes needed when there is no solid \ndata, as they are done typically when new features are introduced, so estimations \nhave to be used.\nKeep in mind that creating the load can also suffer from its own bottlenecks. To \nmultiply the simulations, it may be necessary to use multiple servers and ensure that \nthe network is capable of supporting the traffic.\nMultiplying the simulation can be done directly by starting the process multiple \ntimes. This procedure, though simple, is quite effective and can be controlled with \nsimple scripts. It also has the flexibility that the simulation can be any kind of \nprocess, including readjusted system tests using any existing software. This speeds \nup the preparation of the load test and builds trust that the simulation is accurate, as \nit reuses existing software that has been tested previously.\nFor simplicity, it's better to use a single simulation that works as \na combination of typical behaviors of users instead of trying to \ngenerate multiple smaller simulations trying to replicate different \nusers.\nAs we said before, the usage of some system test that exercises the \nmain parts of the system works very well in these cases, once you \ndouble-check that the behavior is compatible with the typical case \nin the system.\nRemember to monitor the results of each simulation, and errors \nin particular. This will help detect possible problems. Load tests \nalso exercise the monitoring of the system, so it's a good exercise in \ndetecting weak points and improving on them.\nThe more intensive load tests are, the more problems they'll be able \nto capture. Then we can avoid those problems once real traffic is in \nplay.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 562,
      "chapter": null,
      "content": "Chapter 16\n[ 543 ]\nLoad tests should also be aimed at creating some headroom in the production cluster \nso they verify that the load is always under control, even in cases when it's growing, \ninstead of finding bottlenecks during regular operations, which may produce \nincidents.\nVersioning\nWhen making changes to any service, a system needs to be in place to track the \ndifferent changes. That way, we can understand what gets deployed when and what \nhas changed from last week.\nVersioning means assigning a unique code version to each service or system. It makes \nit easy to understand what software has been deployed and track down what has \nbeen changed from one version to another. \nIt is also possible to use specific tools aimed at common use cases \nlike HTTP interfaces, for example, Locust (https://locust.\nio/). This tool allows us to create a web session, simulating a user \naccessing the system. The great advantages of Locust are that it \nalready has a reporting system embedded and can be scaled with \nminimal preparation. However, it requires the creation of a new \nsession explicitly for the load test and is only capable of working \nwith web interfaces.\nThis information is really powerful when you're facing an incident. \nOne of the riskiest moments in a system is when there's a new \ndeployment, as new code can create new problems. It's not unusual \nthat an incident is produced due to the release of a new version.\nVersion numbers are normally assigned in the source control \nsystem at specific points to precisely track the code at that \nparticular point. The point of having a defined version is to have a \nprecise definition of the code under that unique version number. A \nversion number that is applicable to multiple iterations of the code \nis useless.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1881,
      "extraction_method": "Direct"
    },
    {
      "page_number": 563,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 544 ]\nVersion numbers are about communicating the differences in code when talking \nabout different snapshots of the same project. Their main objective is to communicate \nand allow us to understand how software evolves, not only within the team, but \nexternally as well.\nTraditionally, versions were highly related to packaged software and different \nversions of the software that were sold in boxes, making them marketing versions. \nWhen the internal version was required, a build number was used, which was a \nconsecutive number based on the number of times the software had been compiled.\nVersions can not only be applied to whole software, but also to elements of it, as \nAPI version, library versions, etc. In the same way, different versions can be used \neffectively for the same software, such as for creating an internal version for the \ntechnical team but an external version for marketing purposes.\nIn modern software, where the releases are frequent and the version needs to change \noften, this simple method is not adequate, and instead different version schemas are \ncreated. The most common is semantic versioning.\nSemantic versioning uses two or three numbers, separated by dots. An optional v \nprefix can be added to clarify that it refers to a version:\nvX.Y.Z\nThe first number (X) is called the major version. The second (Y) is the minor version, \nand the last number (Z) is the patch version. These numbers are increased as new \nversions are generated:\n•\t\nAn increase in the major version indicates that the software is not compatible \nwith previously existing software. \n•\t\nAn increase in the minor version means that this version contains new \nfeatures, but they don't break compatibility with older versions.\nFor example, some software could be sold as Awesome Software \nv4, have an API v2, and internally be described as build number \nv4.356.\nWe talked about semantic versioning in Chapter 2, API Design, but \nthe topic is important enough to be repeated. Note that the same \nconcept can be used both for APIs and code releases.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2175,
      "extraction_method": "Direct"
    },
    {
      "page_number": 564,
      "chapter": null,
      "content": "Chapter 16\n[ 545 ]\n•\t\nFinally, an increase of the patch version only covers bugfixes and other \nimprovements like security patches. It fixes problems, but doesn't change the \ncompatibility of the system.\nA good example of this kind of versioning is the Python interpreter itself:\n•\t\nPython 3 was an increase in the major version, and as such, code from \nPython 2 required changes to be run under Python 3\n•\t\nPython 3.9 introduced new features compared with Python 3.8, for example, \nthe new union operators for dictionaries\n•\t\nPython 3.9.7 adds bugfixes and improvements over the previous patch \nversion\nSemantic versioning is very popular and it's particularly useful when dealing with \nAPIs and with libraries that are going to be used externally. It provides a clear \nexpectation, from just the version number, on what to expect from a new change, and \nallows clarity at the time of adding new features.\nThis kind of versioning, though, may be too restrictive for certain projects, and in \nparticular, for internal interfaces. As it operates with small iterations that maintain \ncompatibility along the way, only deprecating features after they are old, it works \nmore like a window that is always evolving. Therefore, it's difficult to introduce a \nmeaningful specific version.\nWhen working with internal APIs, especially with microservices or internal libraries \nthat change very often and are consumed by other parts of the organization, it is \nbetter to relax the rules and, while using something similar to semantic versioning, \njust using it as a general tool to increase version numbers in a consistent manner to \nprovide an understanding of how the code changes, but without necessarily having \nto force changes in major or minor versions.\nKeep in mind that increasing a major version number can also \nmark changes that would ordinarily appear in minor version \nupdates, too. A change in the major version number will likely \nbring new features as well as major overhauls.\nFor example, the Linux kernel decided to move away from \nsemantic versioning for this reason, deciding that instead new \nmajor versions will be small and not change things, and won't \ncarry any particular meaning: http://lkml.iu.edu/hypermail/\nlinux/kernel/1804.1/06654.html.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2361,
      "extraction_method": "Direct"
    },
    {
      "page_number": 565,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 546 ]\nWhen communicating through external APIs, though, version numbers do not only \ncarry a technical meaning, but also a marketing one. Using semantic versioning gives \na strong assurance of the capacities of the API.\nKeep in mind that it can be possible to create a general version of a whole system, \neven if internally its different components have their own independent versions. In \ncases like online services, though, that can be tricky or pointless. Instead, the focus \nshould be on maintaining backward compatibility.\nBackward compatibility\nThe key aspect of changing architecture in a running system is the necessity of \nalways keeping backward compatibility in its interfaces and APIs.\nBackward compatibility means that systems keep their old interfaces working as \nexpected, so any calling system won't be affected by the change. This allows them to \nbe upgraded at any point, without interrupting the service.\nAs versioning is so important, a good idea is to allow services to \nself-report their version number via a specific endpoint like /api/\nversion or another easily accessed way to be sure that it's clear \nand can be checked by other dependant services.\nWe also talked about backward compatibility in regard to \ndatabases changes in Chapter 3, Data Modeling. Here we will talk \nabout interfaces, but it follows the same ideas.\nKeep in mind that backward compatibility needs to apply \nexternally, as customers rely on a stable working interface, but \nalso internally where multiple services interact with each other. If \nthe system is complex and has multiple parts, the APIs connecting \nthem should be backward compatible. This is particularly \nimportant in microservices architectures to allow the independent \ndeployment of microservices.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1886,
      "extraction_method": "Direct"
    },
    {
      "page_number": 566,
      "chapter": null,
      "content": "Chapter 16\n[ 547 ]\nThis concept is quite simple, but it has implications on how changes need to be \ndesigned and implemented:\n•\t\nChanges should always be additive. That means that they add options, and \ndon't remove them. This makes any existing calls to the system keep using \nthe existing features and options and doesn't disrupt them.\n•\t\nRemoving options should be done with extreme care, and only after \nverifying that they are not used anymore. To be able to detect that, we need \nto adjust the monitoring so we have real data that can clearly provide solid \ndata to allow us to determine this.\n•\t\nEven additive changes in externally accessible APIs are difficult. External \ncustomers tend to remember the API as it is, so it can be difficult to change \nthe format of existing calls, even if it's just adding a new field.\nThis depends on the format used. Adding a new field in a JSON object \nis safer than changing a SOAP definition, which needs to be defined \nbeforehand. This is one of the reasons why JSON is so popular – because it's \nflexible in the definition of the objects returned.\nNonetheless, for external APIs it could be safer to add new endpoints if \nnecessary. API changes are normally done in stages, creating a new version \nof the API and trying to encourage customers to change to the new and better \nAPI. These migrations can be long and arduous, as external users will require \nclear advantages to be persuaded to adopt the change on their end.\nWith external interfaces, it may be almost impossible \nto remove any option or endpoint, especially on APIs. \nCustomers don't want to change their existing systems to \nadjust to any changes unless there's a good reason, and \neven in that case it will take a lot of work to adequately \ncommunicate it. We will talk later in this chapter about \nthis situation.\nWeb interfaces allow greater flexibility for changes as they \nare used manually by humans.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2021,
      "extraction_method": "Direct"
    },
    {
      "page_number": 567,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 548 ]\n•\t\nExisting tests, both unit and integration tests, are the best way to ensure that \nthe API is backward compatible. In essence, any new feature should pass \nthe tests without a problem, as the old behavior won't change. Good test \ncoverage of the API functionality is the best way to maintain compatibility.\nIntroducing changes in external interfaces is more complicated and normally \nrequires the definition of stricter APIs and a slower pace of change. Internal \ninterfaces allow greater flexibility, as their changes can be communicated across the \norganization in an incremental way that will allow adaptation without interrupting \nthe service at any point.\nIncremental changes\nIncremental changes to the system, slowing mutating and adjusting the APIs, can \nbe released in sequence with multiple services involved. But the changes need to be \napplied in sequence and keep backward compatibility in mind.\nFor example, let's say that we have two services: service A generates an interface \ndisplaying students taking exams, and calls service B to obtain the list of examinees. \nThis is done by calling an internal endpoint:\nGET /examinees (v1)\n[\n    {\n         \"examinee_id\": <student id>,\n         \"name\": <name of the examinee>\n    }, …\n]\nA good example of how painful a change in APIs can \nbe is the migration from Python 2 to Python 3. Python 3 \nhas been available since 2008, but took a long time to get \nany kind of traction, because programs written in Python \n2 needed to be changed. The migration has been quite \nlengthy, even to the point that the last Python 2 interpreter \n(Python 2.7) was supported for ten years, from its first \nrelease in 2010 until 2020. Even with that long process, \nthere's still code in legacy systems working with Python \n2. This shows the difficulty of moving from one API to \nanother if no backward compatibility is respected.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2000,
      "extraction_method": "Direct"
    },
    {
      "page_number": 568,
      "chapter": null,
      "content": "Chapter 16\n[ 549 ]\nThere's a new feature that needs to be introduced in service A that requires extra \ninformation from the examinees, and requires us to know the number of times that \neach examinee has attempted a particular exam to sort them adequately by that \nparameter. With the current information, that's impossible, but service B can be \ntweaked to return that information.\nTo do so, the API needs to be extended, so it returns that information:\nGET /examinees (v2)\n[\n    {\n         \"examinee_id\": <student id>,\n         \"name\": <name of the examinee>,\n         \"exam_tries\", <num tries>\n    }, …\n]\nOnly after this change is properly done and deployed can service A use it. This \nprocess happens in the following stages:\n1.\t Initial stage.\n2.\t Deployment of service B with /examinees (v2). Note how service A will just \nignore the extra field and keep working normally.\n3.\t Deployment of service A reading and using the new parameter exam_tries.\nAll of the steps are stable. The service works without a problem throughout each \none, so there's detachment between the different services.\nThis detachment is important because if there's a problem with a \ndeployment, it can be reversed and only affects a single service, \nquickly reverting to the previous stable situation until the issue \ncan be fixed. The worst situation is to have two changes in services \nthat need to happen at the same time, as a failure in one will affect \nthe other and reversing the situation may not be easy. Even worse, \nthe problem could be in the interaction between them, and in that \nsituation it won't be clear which one is responsible, because it could \nbe both. It is important to keep to small individual steps where \neach step is solid and reliable.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1844,
      "extraction_method": "Direct"
    },
    {
      "page_number": 569,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 550 ]\nThis way of operating allows us to implement greater changes, for example, \nrenaming a field. Let's say that we don't like the examinee_id field and want to \nchange it for a more appropriate student_id. The process will go like this:\n1.\t Update the returned object to include a new field called student_id, \nreplicating the previous value in service B:\nGET /examinees (v3)\n[\n    {\n         \"examinee_id\": <student id>,\n         \"student_id\": <student id>,\n         \"name\": <name of the examinee>,\n         \"exam_tries\", <num tries>\n    }, …\n]\n2.\t Update and deploy service A to use student_id instead of examinee_id. \n3.\t Do the same in other services that possibly call service B. \n4.\t Remove the old field from service B and deploy the service:\nGET /examinees (v3)\n[\n    {\n         \"examinee_id\": <student id>,\n         \"student_id\": <student id>,\n         \"name\": <name of the examinee>,\n         \"exam_tries\", <num tries>\n    }, …\n]\n5.\t Remove the old field from service B and deploy the service.\nUse monitoring tools and logs to verify this!\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1177,
      "extraction_method": "Direct"
    },
    {
      "page_number": 570,
      "chapter": null,
      "content": "Chapter 16\n[ 551 ]\nThis illustrates how we can deploy changes without interrupting the service in \nterms of what is being deployed. But, how can we ensure that the services are always \navailable while deploying a new version?\nDeploying without interruption\nTo allow continuous releases without service interruption, we need to take the \nbackward-compatible changes and deploy them while the service is still responding. \nTo do so, the best ally is the load balancer.\nThe process of a successful smooth deployment requires several instances of the \nservice to be updated, as follows:\nThis step is technically optional, though it would be good \nfor maintenance reasons to remove cruft from the API. But \nthe reality of the day-to-day work means that it's likely \nthat it will stay there, just not being accessed anymore. A \ngood balance needs to be found between the convenience \nof leaving it be and maintaining a clean and updated API.\nWe talked about load balancers in Chapter 5, The Twelve-Factor App \nMethodology, and Chapter 8, Advanced Event-Driven Structures. They \nare really useful!\nWe are going to assume that we are using cloud instances or \ncontainers that can be created and destroyed easily. Keep in mind \nthat you can treat them as workers under nginx or any other kind \nof web server acting as a load balancer inside a single server. This \nis how the nginx reload command works.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1495,
      "extraction_method": "Direct"
    },
    {
      "page_number": 571,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 552 ]\n1.\t This is the initial stage, where all the instances have version 1 of the service to \nbe updated:\nFigure 16.1: Starting point\n2.\t A new instance with service 2 is created. Note that it's not yet been added to \nthe load balancer.\nFigure 16.2: New server created\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 394,
      "extraction_method": "Direct"
    },
    {
      "page_number": 572,
      "chapter": null,
      "content": "Chapter 16\n[ 553 ]\n3.\t The new version is added to the load balancer. Right now, the requests \ncan be directed to version 1 or version 2. If we followed the principles of \nbackward compatibility, though, this should not cause any problems.\nFigure 16.3: New server included in the load balancer\n4.\t To keep the number of instances constant, an old instance needs to be \nremoved. A careful approach here means starting by disabling the old \ninstance in the load balancer, so no new requests will be addressed. After the \nservice finishes all the already-ongoing requests (remember, no new requests \nwill be sent to this instance), the instance is effectively disabled and can be \nremoved totally from the load balancer.\nFigure 16.4: Removal of an old server from the load balancer\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 880,
      "extraction_method": "Direct"
    },
    {
      "page_number": 573,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 554 ]\n5.\t The old instance can be destroyed/recycled.\nFigure 16.5: Old server has been totally removed\n6.\t The process can be repeated until all instances are at version 2.\nFigure 16.6: Final stage with all new servers\nThere are tools that allow us to do this process automatically. For example, \nKubernetes will perform this automatically when rolling out changes to containers. \nWe also saw that web services like nginx or Apache will do as well. But the same \nprocess can also be applied manually or through developing custom tools when an \nunusual use case demands it.\nFeature flags\nThe idea of feature flags is to hide functionality that is still not ready to be released \nunder a configuration change. Following the principles of small increments and \nquick iteration makes it impossible to create big changes, like a new user interface.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 574,
      "chapter": null,
      "content": "Chapter 16\n[ 555 ]\nTo complicate things further, these big changes will likely happen in parallel with \nothers. There's no chance of delaying the whole release process for 6 months or more \nuntil the new user interface is working correctly. \nCreating a separate branch that's long-lived is also not a great solution, as merging \nthis branch becomes a nightmare. Long-living branches are complex to manage and \nalways difficult to work with.\nA better solution is to create a configuration parameter that activates or deactivates \nthis feature. The feature can then be tested in a particular environment, while all the \ndevelopment continues at the same pace. \nThat means that other changes, like bug fixes or performance improvements, are \nstill happening and being deployed. And the work done on the big new feature is \nmerged into the main branch as often as usual. This means that the developed parts \nof the big new feature are also being released to the production environment, but \nthey are not active yet.\nThe feature will be then developed in small increments until it's ready for release. \nThe final step is to simply enable it through a configuration change.\nThis technique allows us to grow in confidence and release big features without \nsacrificing small incremental approaches to it.\nTeamwork aspects of changes\nSoftware architecture is not only about technology, but a part of it is highly \ndependent on communication and human aspects. \nTests need to ensure that both options – the feature active and \ndeactivated – work correctly, but working in small increments \nmakes this relatively easy.\nNote that the feature may be active for certain users or \nenvironments. This is how beta features are tested: they rely \non some users being able to access the feature before it is fully \nreleased. The test users could be internal to the organization \ninitially, like QA teams, managers, product owners, etc., so they \ncan provide feedback on the feature, but using production data.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 575,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 556 ]\nThe process of implementing changes in a system has some human elements \naffecting teamwork that need to be taken into consideration. \nSome examples:\n•\t\nKeep in mind that the work of a software architect typically lies in managing \ncommunication with multiple teams, which requires care and soft skills in \nboth actively listening to teams and explaining or even negotiating design \nchanges. Depending on the size of the organization, that could be challenging \nas different teams may have wildly different cultures.\n•\t\nThe pace and acceptance of technical changes in an organization are \ntightly related to the organization's culture (or subcultures). Changes in \norganizations' ways of working typically occur much more slowly, although \norganizations that can quickly change technologies tend to be faster in \nadjusting to organization-wide changes.\n•\t\nIn the same way, technology changes require support and training, even if \nit's purely within the organization. When requiring some big technology \nchange, be sure to have a point of contact where the team can go to resolve \ndoubts and questions. \nA lot of the questions can be solved by explaining why that change is \nrequired and working from there.\n•\t\nRemember when we talked about Conway's Law of software architecture in \nChapter 1, Introduction to software architecture, about how the communication \nstructure and architectural structure are related. A change in one will likely \naffect the other, which means that big enough architectural changes will lead \nto organizational restructuring, which has its own challenges.\n•\t\nAt the same time, changes may have winners and losers in the affected teams. \nOne engineer could feel threatened because they won't be able to use their \nfavorite programming language. In the same way, their partner will be \nexcited because now the opportunity to use their favorite piece of tech is \namazing.\nThis problem can be particularly poignant in team shuffling when people \nare moving around or when creating new teams. An important factor in  the \npace of development is to have an efficient team and making changes to \nteams has an impact on their communication and effectiveness. This impact \nneeds to be analyzed and taken into consideration.\n•\t\nMaintenance needs to be introduced routinely as part of the day-to-day \noperations of the organization. Regular maintenance should include all \nsecurity updates, but also tasks like upgrading OS versions, dependencies, \netc.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2601,
      "extraction_method": "Direct"
    },
    {
      "page_number": 576,
      "chapter": null,
      "content": "Chapter 16\n[ 557 ]\nA general plan to deal with this kind of routine maintenance will provide \nclarity and clear expectations. For example: the OS version will be upgraded \nwithin three to six months of a new LTS version being released. This produces \npredictability, gives clear objectives to follow, and produces continuous \nimprovement of the system. \nIn the same way, automatic tools that detect security vulnerabilities make it \neasy for the team to know when it's time to upgrade dependencies either in \nthe code or in the underlying system.\n•\t\nIn the same way, the repayment of technical debt needs to be introduced \nas a habit to be sure that the system is healthy. Technical debt is typically \ndetected by the teams themselves, as they'll have the best understanding \nof it, and is manifested with a progressively slower pace of code changes. \nIf technical debt is not addressed, it will become more and more complicated \nto work with, making the development process more difficult and risking \nburnout by developers. Be sure to budget time to tackle it before it gets out \nof control.\nAs a general consideration, just keep in mind that changes in architecture need to be \ncarried out by members of the team, and that information needs to be communicated \nand executed correctly. As with any other task where communication is an important \ncomponent, this presents its own challenges and problems, as communicating with \npeople, especially with several people, is arguably one of the most difficult tasks \nin software development. Any software architecture designer needs to be aware \nof this and allocate enough time to be sure to, on one hand, communicate the plan \nadequately, and on the other, receive feedback and adjust accordingly to get the \nbest results.\nSummary\nIn this chapter, we described the different aspects and challenges of keeping a system \nrunning while developing and changing it, including its architecture.\nWe started by describing different ways that architecture can require adjustments \nand changes. We then moved on to talk about how to manage changes, including \nthe option of having some designated time where the system won't be available, \nand introduced the concept of maintenance windows to clearly communicate \nexpectations of stability and change.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2392,
      "extraction_method": "Direct"
    },
    {
      "page_number": 577,
      "chapter": null,
      "content": "Ongoing Architecture\n[ 558 ]\nWe next went over the different incidents that can happen when problems arise, \nand the system struggles. We went over the necessary continuous process of \nimprovement and reflection after an incident of this kind happens, and also looked \nat preparation processes that can be used before a significant event where the risk \nincreases, for example, because of a marketing push expected to increase the load of \nthe system.\nTo deal with this, we next introduced load testing and how it can be used to verify \nthe system's capacity for accepting a defined load, making sure that it's ready \nto support the expected traffic. We talked as well about the necessity of creating \na versioning system that clearly communicates what version of the software is \ncurrently deployed. \nNext, we talked about the critical aspect of backward compatibility and how \nis crucial in ensuring small, fast increments that are the key to continuous \nimprovement and advancement. We also talked about how feature flags can help \nmix this process of releasing bigger features that need to be activated as a whole.\nFinally, we described different aspects of how changes in a system and architecture \ncan affect human collaboration and communication and how that needs to be taken \ninto account while performing changes to the system, in particular changes that \nmay affect the structure of the teams, which, as we've seen, will tend to replicate the \nstructure of the software.\nJoin our book's Discord space\nJoin the book's Discord workspace for a monthly Ask me Anything session with the author: \nhttps://packt.link/PythonArchitechture\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1742,
      "extraction_method": "Direct"
    },
    {
      "page_number": 578,
      "chapter": null,
      "content": "[ 559 ]\npackt.com\nSubscribe to our online digital library for full access to over 7,000 books and videos, \nas well as industry leading tools to help you plan your personal development and \nadvance your career. For more information, please visit our website.\nWhy subscribe?\n•\t\nSpend less time learning and more time coding with practical eBooks and \nVideos from over 4,000 industry professionals\n•\t\nImprove your learning with Skill Plans built especially for you\n•\t\nGet a free eBook or video every month\n•\t\nFully searchable for easy access to vital information\n•\t\nCopy and paste, print, and bookmark content\nAt www.packt.com, you can also read a collection of free technical articles, sign up for \na range of free newsletters, and receive exclusive discounts and offers on Packt books \nand eBooks.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 898,
      "extraction_method": "Direct"
    },
    {
      "page_number": 579,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 580,
      "chapter": null,
      "content": "[ 561 ]\nOther Books \nYou May Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nLearn Python Programming, Third Edition\nFabrizio Romano\nHeinrich Kruger\nISBN: 9781801815093\n•\t\nGet Python up and running on Windows, Mac, and Linux\n•\t\nWrite elegant, reusable, and efficient code in any situation\n•\t\nAvoid common pitfalls like duplication, complicated design, and over-\nengineering\n•\t\nUnderstand when to use the functional or object-oriented approach to \nprogramming\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 597,
      "extraction_method": "Direct"
    },
    {
      "page_number": 581,
      "chapter": null,
      "content": "[ 562 ]\nOther Books You May Enjoy\n•\t\nBuild a simple API with FastAPI and program GUI applications with Tkinter\n•\t\nGet an initial overview of more complex topics such as data persistence and \ncryptography\n•\t\nFetch, clean, and manipulate data, making efficient use of Python's built-in \ndata structures\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 402,
      "extraction_method": "Direct"
    },
    {
      "page_number": 582,
      "chapter": null,
      "content": "[ 563 ]\nOther Books You May Enjoy\nPython GUI Programming with Tkinter, Second Edition\nAlan D. Moore\nISBN: 9781801815925\n•\t\nProduce well-organized, functional, and responsive GUI applications\n•\t\nExtend the functionality of existing widgets using classes and OOP\n•\t\nPlan wisely for the expansion of your app using MVC and version control\n•\t\nMake sure your app works as intended through widget validation and unit \ntesting\n•\t\nUse tools and processes to analyze and respond to user requests\n•\t\nBecome familiar with technologies used in workplace applications, including \nSQL, HTTP, Matplotlib, threading, and CSV\n•\t\nUse PostgreSQL authentication to ensure data security for your application\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 583,
      "chapter": null,
      "content": "[ 564 ]\nOther Books You May Enjoy\nPython Object-Oriented Programming, Fourth Edition\nSteven F. Lott\nDusty Phillips\nISBN: 9781801077262\n•\t\nImplement objects in Python by creating classes and defining methods\n•\t\nExtend class functionality using inheritance\n•\t\nUse exceptions to handle unusual situations cleanly\n•\t\nUnderstand when to use object-oriented features, and more importantly, \nwhen not to use them\n•\t\nDiscover several widely used design patterns and how they are implemented \nin Python\n•\t\nUncover the simplicity of unit and integration testing and understand why \nthey are so important\n•\t\nLearn to statically type check your dynamic code\n•\t\nUnderstand concurrency with asyncio and how it speeds up programs\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 816,
      "extraction_method": "Direct"
    },
    {
      "page_number": 584,
      "chapter": null,
      "content": "[ 565 ]\nOther Books You May Enjoy\nPackt is searching for authors like you\nIf you're interested in becoming an author for Packt, please visit authors.packtpub.\ncom and apply today. We have worked with thousands of developers and tech \nprofessionals, just like you, to help them share their insight with the global tech \ncommunity. You can make a general application, apply for a specific hot topic that \nwe are recruiting an author for, or submit your own idea.\nShare your thoughts\nNow you've finished Python Architecture Patterns, we'd love to hear your thoughts! If \nyou purchased the book from Amazon, please click here to go straight to the \nAmazon review page for this book and share your feedback or leave a review on the \nsite that you purchased it from.\nYour review is important to us and the tech community and will help us make sure \nwe're delivering excellent quality content.\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 585,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 586,
      "chapter": null,
      "content": "[ 567 ]\nIndex\nA\nabstractions  18-20\nlimitations  22\nusing  21\naccess log  185\naction abstractions  23, 24\nadvanced Pytest  369\nfixtures, using  374-379\ntests, grouping  370-374\nURL  369\nalerting  460, 461\nreference link  460\nalpine Linux distribution\nURL  309\nanalysis  422\nApache\nURL  177\nApache Cassandra\nURL  82\nApache Kafka  221\nAPI design  17\nAPI-first approach  61\napplication programming interface (API)\nabout  17\ndesigning  63, 64\ndesign, reviewing  73\nendpoints  65-72\nimplementing  73\nversioning  51\nArangoDB\nURL  83\nArrange Act Assert (AAA) pattern  339\nphases  339\nAsynchronous JavaScript And XML (AJAX)  61\nasynchronous tasks  221-226\natomicity  85\nauthenticating\nversus authorizing  47\nauthentication  44\nAuthorization Code grant  48\nauthorizing\nversus authenticating  47\nautomation  150\nAWS S3  153\nreference link  153\nB\nbackend  55, 57\nbackward compatibility  121, 130, 546-548\ndeploying, without interruption  551-554\nincremental changes  548-550\nbcrypt function  12\nBoto3 documentation\nreference link  264\nboto3 library  264\nbuilt-in cProfile module\nusing  470-474\nbus  220\ndefining  272-274\nC\ncardinality  107, 109\ncatastrophic problems  502\nCelery  235, 236\nconfiguring  236\ndots, connecting  241-243\nHTTP API  252, 253\nscheduled tasks  244-249\ntask, triggering  240\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1388,
      "extraction_method": "Direct"
    },
    {
      "page_number": 587,
      "chapter": null,
      "content": "[ 568 ]\nCelery Flower  249-252\nCelery worker  237-240\nCircleCI\nURL  149\nCloud Bigtable\nreference link  82\ncode profiling, for time  468- 470\nbuilt-in cProfile module, using  470-474\nline profiler, using  475-481\ncommand-line interface (CLI)  269\nCommand Query Responsibility Segregation \n(CQRS)  127, 129\ncomplex systems  274-277\nmodules  274\nconfiguration  153-155\nconfiguration parameters\nfeature configuration  154\noperational configuration  154\nconsistency  85\ncontainer image  308\nbuilding  308, 309\nrunning  310\ncontainerized Twelve-Factor app  169, 171\ncontainers  306\ncontent delivery network (CDN)  182\nContinuous Integration (CI)  148-150\nConway's Law  7\nCreate Retrieve Update Delete (CRUD)  26\ncritical problems  502\ncryptographical hash  11\nCython  402-404\nURL  402\nD\ndata\nencapsulating  122-126\nDatabase Administrator (DBA)  76\ndatabase management system (DBMS)  76\ndatabase migrations  130\ndatabases\nabout  5\nchanging, without interruption  132-135\nnon-relational databases  79\nrelational databases  77-79\nsmall databases  83, 84\ntypes  76, 77\ndatabase transactions  85, 86\ndata indexing  105-107\ndata migrations  136\ndb.sqlite3 file  320\ndebugging  422\nwith breakpoints  524-527\nwith logs  522, 523\ndeclarative  139\ndefects\ndetecting  502, 503\nprocessing  503\ndenormalization  103, 105\ndependency injection  364-366\nin OOP  366-369\ndeployment  321\ndeterministic profilers  465\ndevelopment mode  398\nreference link  398\ndistributed relational databases  87\nDjango  132\nrequest, routing to View  197, 198\nDjango framework\nURL  115\nDjango migrations\nreference link  132\nDjango MVT architecture  195, 196\nDjango project\nURL  195\nDjango Prometheus\nconfiguring  447-449\nDjango REST framework  208\nmodels  209\nserializer  212-215\nURL  196\nURL routing  210\nviews  210, 211\nDocker\nURL  307\nDocker Compose\nURL  321\nDocker Hub\nURL  309\ndocstrings, PEP 257 document\nreference link  522\nDocument Object Model (DOM)  60\ndocument stores  81, 82\nDomain-Driven Design  112-114\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 588,
      "chapter": null,
      "content": "[ 569 ]\nDomain Model  122\nDomain-Specific Languages (DSL)  79\ndurability  86\ndynamic page  59-61\ndynamic queries  121\nE\nedge load balancer  216\nElasticsearch\nreference link  82\nELK Stack\nreference link  168\nenvironment variables  159\nerror log  185\nevent-driven systems  220, 278\ntesting  278-280\nevents\nsending  220, 221\nstreaming  256-260\neventual consistency  87\nexecution phases, for migration from  \nmonolith to microservices\nconsolidation  305\nfinal  306\npilot  305\nexpected errors  505\ndetecting  430, 431\nexternal dependencies\nmocking  361-364\ntesting  358-361\nexternal layers  216\nexternal versioning\nversus internal versioning  51, 52\nF\nfeature configuration  154\nfeature flags  554, 555\nfixture\nsteps  375\nusing  374-379\nFlower tool\nreference link  249\nFluentd\nreference link  426\nforeign key  78, 98, 99\nformatter  427\nfrontend  55, 57\ncommon technologies  56\nfull stack engineer  56\nfull table scan  106\nG\nGitHub\nURL  149\nGitLab\nURL  149\nGlobal Interpreter Lock (GIL)  191\nGrafana\nURL  459\ngraph databases  83\nH\nhandler  427\nreference link  429\nhappy path  279\nheaders list\nreference link  30\nhorizontal scalability  151\nhotspots\nglobal hotspots  467\nspecific hotspots  467\nHTML interfaces  58\nauthenticating  44-46\nHTTP API  252\nHttpRequest  201\nattributes  201-203\nHttpResponse  203-205\nHybrid approach  63\nI\nincident  535, 536\ningress  322\nin-process communication  6, 7\nintegration tests  278, 332, 333\ninternal versioning\nversus external versioning  51, 52\nintrospection tools  518-521\nipdb\nreference link  527\nisolation  86\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1644,
      "extraction_method": "Direct"
    },
    {
      "page_number": 589,
      "chapter": null,
      "content": "[ 570 ]\nJ\nJWT token\nelements  49\nK\nkernel space  289\nkey-value stores  80\nKubernetes  321, 322\nURL  322\nL\nLAMP architecture  7\nleaky abstraction\nleaking  22\nlegacy databases\ndealing with  139\nleonardo_1.py\nreference link  492\nleonardo_2.py file\nreference link  493\nLeonardo numbers  492\nline profiler\nusing  475-481\nload testing  541, 542\nlocal debugging  515-517\nlogger  427\nLoggly\nURL  168\nlogs  185, 421-426\naccess log  185\nadding  437, 438\nerror log  185\nlimitations  438, 439\nproblems, detecting through  430\nproducing, in Python  426-429\nstrategies  434-437\nversus metrics  442, 443\nM\nmaster-fifo\nreference link  195\nmemory leaks\nuse cases  467\nmemory profiler  467, 468, 492-494\nmemory optimization  496, 498\nmemory_profiler package, using  494, 495\nmemory-profiler module\nreference link  498\nmessage broker  272\nmetrics  421, 444\nchecking  449, 450\ncounter  443\nerrors  460\ngauge  443\nlatency  460\nmeasure  444\nsaturation  460\ntraffic  460\nversus logs  442, 443\nworking with  459\nmetrics, generating with Prometheus  445\nenvironment setup  445-447\nmicrokernel  289\nMicropostsListView  210\nMicropostView  210\nmicroservices architecture  281-285\ncommunication structure  291-293\nversus monolithic architecture  285-288\nmiddleware\ndefining  205, 207\nmigration  131\nmigration, from monolith to  \nmicroservices  294\nanalyze  297\nchallenges  294-296\ndesign  298-300\nexecute  303, 304\nplan  301-303\nmild problems  502\nminor problems  502\nmixed sharding  93-95\nmocking  361-364\nModel layer  112\nModel View Controller (MVC)  57, 58\nModel-View-Template (MVT)  195\nMongoDB\nURL  82\nmonolithic architecture  7, 281-283\ncommunication structure  290, 291\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 590,
      "chapter": null,
      "content": "[ 571 ]\nMoviePy documentation\nreference link  264\nMoviePy library  264\nMozilla SameSite Cookie\nreference link  45\nN\nnatural key  78\nNeo4j\nURL  83\nnginx\nURL  177\nnginx configuration  314\nnodes  321\nnon-relational databases  77-79\nchanges, without enforcing schema  137, 138\ndocument stores  81, 82\ngraph databases  83\nkey-value stores  80\nwide-column stores  82\nNoSQL  79\nO\nOAuth  47\nObject-Oriented Programming  \n(OOP)  24, 112-114\nObject-Relational Mapping (ORM)  114\ndatabase, detaching from code  116\nindependence, from SQL  116, 117\nissues related to composing SQL,  \nremoving  118-122\nusing  114-116\nobservability  421\nOOP programming\ndependency injection  366-369\nOpen API\nURL  40\noperational configuration  154\nOpsgenie\nURL  461\norchestration  321\nORM\nschema, syncing, to   141, 142\nP\npackage\ncreating  382-384, 395-397\ninstalling, in development mode  398\nuploading, to PyPI  408-415\npartial profiling  481\nprofile file per request, generating  489-491\nweb server returning prime numbers,  \ncreating  482-486\nwhole process, profiling  486-488\nPeewee\nreference link  115\nPEP-328 imports\nreference link  385\nperformance\nmemory performance  464\ntime performance  464\nPersonally Identifiable Information (PII)  513\nphilosophy\ntest, designing  336-339\ntesting  334, 336\ntest, structuring  339-342\npipelines  260, 261\nbase task  263\nimage task  264, 265\npreparation  261, 262\ntask, connecting  267-269\ntask, running  270-272\nvideo task  266, 267\npod  321\nPony\nURL  115\nPostman\nURL  39\npostmortem analysis  537-540\npremortem analysis  540\npreparation analysis  540\nprimary key  77\nprimary/replica  88-90\nprimes_1.py\nreference link  469\nprint debugging  522\nprivate index\ncreating  415-419\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1791,
      "extraction_method": "Direct"
    },
    {
      "page_number": 591,
      "chapter": null,
      "content": "[ 572 ]\nproduction investigation  504, 506\ndata, analyzing  513\nlogging, increasing  514, 515\nrequest ID, logging  507-512\nprofilers\ndeterministic profilers  465\nmemory profiler  467\nstatistical profilers  466\nprofiling  422\nbasics  464, 465\nPrometheus\nmetrics, generating  445\nquerying  454-459\nPrometheus documentation, on queries\nreference link  459\nPrometheus server\nstarting  450-453\npublisher  221\npublish/subscribe (pub/sub)  276\npudb\nreference link  527\npure sharding  92, 93\npyinstrument\nreference link  467\npyjwt\nreference link  50\npypiserver  415\nPyramid\nURL  197\npy-spy\nreference link  467\nPytest  354-358\npytest-catchlog\nreference link  437\nPython\nintrospection tools  518-521\nlogs, producing  426-429\ntrivial packaging  384-386\nunit testing  350\nPython Enhancement Proposals (PEPs)  385\nPython garbage collector\nreference link  468\nPython package  394, 395\ndefining  398-401\nwith binary code  405-408\nPython Package Index (PyPI)  387-390\npackage, uploading to  408-415\nPython packaging ecosystem  386\nPython Package Index (PyPI)  387-390\nvirtual environment  390, 391\nvirtual environment, creating  392, 393\nPython status codes\nreference link  204\nPython WSGI worker  195\nQ\nqueue  221\nqueue effects  227-231\ncloud queue  233-235\ncloud worker  233-235\nqueueing theory\nreference link  228\nqueue tasks\nbackground tasks  230\npriority tasks  230\nsingle code  232\nR\nRabbitMQ  221\nRedis  81, 221\nreference counting  467\nrelational databases  77-79\nRelational schema changes  131\nremote-pdb\nreference link  527\nRemote Procedure Call (RPC)  17\nreplication lag  88, 89\nRepositories  125\nRepository pattern  116\nRepresentational State Transfer (REST)  25\nrequest ID\nlogging  507-512\nrequest-response architecture  174-176\nresources abstractions  23, 24\nRESTful interfaces  25\nauthenticating  46-49\ndefining  26-28\nheaders and status  29\nOpen API specification, using  40-44\npagination  35-37\nresource, designing  32, 33\nresources and parameters  34, 35\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 2057,
      "extraction_method": "Direct"
    },
    {
      "page_number": 592,
      "chapter": null,
      "content": "[ 573 ]\nRESTful API process, designing  37-40\nRESTful rules  25\nreverse proxy  182-184\nworking  183\nRiak  81\nrsyslog\nreference link  426\nS\nsalt  11\nscalability  148-153\nscalability, types\nhorizontal scalability  151\nvertical scalability  151\nscheduled downtime  533, 534\nmaintenance window  534, 535\nscheduled tasks  227\nschema\ndetecting, from database  139, 141\nsyncing, to ORM  141, 142\nschema design  97-101\nschema normalization  101, 102\nself-encoded tokens  49-51\nsemantic versioning  52-54, 544\nSentry\nURL  434, 461\nserious problems  502\nserver_profile_by_request.py\nreference link  490\nserver.py file\nreference link  482\nservice  322\nservices\ncontainerizing  306, 307\nsharding  90, 91\nadvantages  96\ndisadvantages  96\nshard key  90\nshared database  224\nsimple versioning  54\nsingle code  232\nSingle Object Access Protocol (SOAP)  17\nsingle-page application  61, 62\nSingle-Responsibility principle  5, 6\nSingle Sign-On (SSO)  46\nsite-packages  394\nsmall databases  77, 83, 84\nsoftware architecture  2, 532, 533\nconsiderations  3\ndivision, into smaller units  4-6\neffects  7-9\nsecurity aspects  11, 12\nteamwork aspects  556, 557\nSplunk\nURL  168\nSQLAlchemy  139\nURL  115, 140\nSQL injection attack  118\nStack Overflow\nURL  533\nstatic files\nserving  180\nstatistical profilers  466\nstatus codes  30, 31\nstored procedures  141\nstrangler pattern  301\nStructured Query Language (SQL)  78\nsubdividing tasks  226\nsubscriber  221\nsurrogate key  78\nSwagger\nabout  40\nURL  40\nsystem structure\ndefining  2, 3\nsystem tests  333\nT\ntable sharding  95\nTest-Driven Development (TDD)  342-345\nideas  343\ninto teams  345, 346\nproblems and limitations  346\nprocess, example  347-350\ntesting, levels  331\nintegration test  332, 333\nsystem test  333\nunit tests  331\nTestPyPI\nURL  408\ntraditional HTML interfaces  58, 59\ntransaction  85\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1919,
      "extraction_method": "Direct"
    },
    {
      "page_number": 593,
      "chapter": null,
      "content": "[ 574 ]\nTravisCI\nURL  149\ntrivial packaging\nin Python  384-386\nTwelve-Factor App  148\nfactors  156\nTwelve-Factor App, groups\nbuild once, run multiple times  157, 158\ndependencies and configuration  159-163\nmonitoring and admin  166-169\nscalability  163-166\nTwelve-Factor App methodology  147\nU\nunexpected errors  505\ncapturing  431-434\nuniform interface\nprerequisites  25\nUniform Resource Identifiers (URIs)  26\nUnit of Work class  125\nUnit of Work pattern  122-126\nunit testing\nin Python  350\nunit tests  278, 331\nunittest module  350-353\nuWSGI application  186, 187\ninteracting, with web server  189\nprocesses  190, 191\nprocess lifecycle  191-195\nreference link  195\nuWSGI configuration  314\nV\nversioning  543-546\nneed for  51\nvertical scalability  151\nView  199-201\nHttpRequest  201-203\nHttpResponse  203-205\nvirtual environment  390, 391\ncontainer, using  393, 394\ncreating  392, 393\nW\nweb application for microblogging  9\nfunctional elements functional elements  10\nweb architecture  177\nweb server  5\nabout  177-179\nadvanced usages  186\nlogs  185\nreverse proxy  182-184\nstatic files, serving  180-182\nWeb Server Gateway Interface (WSGI)  186\nweb service\nbuilding  316\nrunning  317-319\nstart script  315, 316\nweb service container  311\nbuilding  311\nrunning  312, 313\nweb worker  5\nwheel package  400\nwheel_package_compiled package  405\nwide-column databases  82\nwolf fence algorithm  517\nWSGI application  187-189\nY\nYou Ain't Gonna Need It (YAGNI)  343\n EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 1560,
      "extraction_method": "Direct"
    },
    {
      "page_number": 594,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    },
    {
      "page_number": 595,
      "chapter": null,
      "content": " EBSCOhost - printed on 2/9/2023 8:47 AM via . All use subject to https://www.ebsco.com/terms-of-use\n",
      "content_length": 101,
      "extraction_method": "Direct"
    }
  ]
}