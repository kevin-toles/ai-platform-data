{
  "metadata": {
    "title": "Serverless Architectures with AWS - Mohit Gupta",
    "author": "Gupta, Mohit;",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 156,
    "conversion_date": "2025-12-19T17:44:56.136463",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Serverless Architectures with AWS - Mohit Gupta.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-9)",
      "start_page": 2,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "The Serverless Model\n\nBenefits of the Serverless Model\n\nIntroduction to AWS\n\nAWS Serverless Ecosystem\n\nAWS Lambda\n\nSupported Languages\n\nExercise 1: Running Your First Lambda Function\n\nActivity 1: Creating a New Lambda Function that Finds the Square Root of the Average of Two Input Numbers\n\nLimits of AWS Lambda\n\nAWS Lambda Pricing Overview\n\nLambda Free Tier\n\nActivity 2: Calculating the Total Lambda Cost\n\nAdditional Costs\n\nSummary\n\nWorking with the AWS Serverless Platform\n\nIntroduction\n\nAmazon S3\n\nKey Characteristics of Amazon S3\n\nDeploying a Static Website\n\nExercise 2: Setting up a Static Website with an S3 Bucket Using a Domain Name in Route 53\n\nEnabling Versioning on S3 Bucket\n\nS3 and Lambda Integration\n\nExercise 3: Writing a Lambda Function to Read a Text File\n\nAPI Gateway\n\nWhat is API Gateway?\n\nAPI Gateway Concepts\n\nExercise 4: Creating a REST API and Integrating It with Lambda\n\nOther Native Services\n\nAmazon SNS\n\nAmazon SQS\n\nDynamoDB\n\nDynamoDB Streams\n\nDynamoDB Streams Integration with Lambda\n\nExercise 5: Creating an SNS topic and Subscribing to It\n\nExercise 6: SNS Integration with S3 and Lambda\n\nActivity 3: Setting Up a Mechanism to Get an Email Alert When an Object Is Uploaded into an S3 Bucket\n\nSummary\n\nBuilding and Deploying a Media Application\n\nIntroduction\n\nDesigning a Media Web Application – from Traditional to Serverless\n\nBuilding a Simple Serverless Media Web Application\n\nExercise 7: Building the Role to Use with an API\n\nExercise 8: Creating an API to Push to / Get from an S3 Bucket\n\nExercise 9: Building the Image Processing System\n\nDeployment Options in the Serverless Architecture\n\nActivity 4: Creating an API to Delete the S3 Bucket\n\nSummary\n\nServerless Amazon Athena and the AWS Glue Data Catalog\n\nIntroduction\n\nAmazon Athena\n\nDatabases and Tables\n\nExercise 10: Creating a New Database and Table Using Amazon Athena\n\nAWS Glue\n\nExercise 11: Using AWS Glue to Build a Metadata Repository\n\nActivity 5: Building an AWS Glue Catalog for a CSV-Formatted Dataset and Analyzing the Data Using AWS Athena\n\nSummary\n\nReal-Time Data Insights Using Amazon Kinesis\n\nIntroduction\n\nAmazon Kinesis\n\nBenefits of Amazon Kinesis\n\nAmazon Kinesis Data Streams\n\nHow Kinesis Data Streams Work\n\nExercise 12: Creating a Sample Kinesis Stream\n\nAmazon Kinesis Firehose\n\nExercise 13: Creating a Sample Kinesis Data Firehose Delivery Stream\n\nActivity 6: Performing Data Transformations for Incoming Data\n\nAmazon Kinesis Data Analytics\n\nExercise 14: Setting Up an Amazon Kinesis Analytics Application\n\nActivity 7: Adding Reference Data to the Application and Creating an Output, and Joining Real-Time Data with the Reference Data\n\nSummary\n\nAppendix\n\nTo my children, Aarya and Naisha.\n\nOceanofPDF.com\n\nPreface\n\nAbout\n\nThis section briefly introduces the author and reviewer, the coverage of this book, the technical skills you'll need to get started,\n\nand the hardware and software required to complete all of the included activities and exercises.\n\nAbout the Book\n\nServerless Architectures with AWS begins with an introduction to the serverless model and helps you get started with AWS and\n\nAWS Lambda. You'll also get to grips with other capabilities of the AWS serverless platform and see how AWS supports\n\nenterprise-grade serverless applications with and without Lambda.\n\nThis book will guide you through deploying your first serverless project and exploring the capabilities of Amazon Athena, an\n\ninteractive query service that makes it easy to analyze data in Amazon Simple Storage Service (Amazon S3) using standard SQL.\n\nYou'll also learn about AWS Glue, a fully managed extract, transfer, and load (ETL) service that makes categorizing data easy and\n\ncost-effective. You'll study how Amazon Kinesis makes it possible to unleash the potential of real-time data insights and analytics\n\nwith capabilities such as Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics. Last but not least, you'll be\n\nequipped to combine Amazon Kinesis capabilities with AWS Lambda to create lightweight serverless architectures.\n\nBy the end of the book, you will be ready to create and run your first serverless application that takes advantage of the high\n\navailability, security, performance, and scalability of AWS.\n\nAbout the Author and Reviewer\n\nMohit Gupta is a solutions architect, focused on cloud technologies and Big Data analytics. He has more than 12 years of\n\nexperience in IT and has worked on AWS and Azure technologies since 2012. He has helped customers design, build, migrate, and\n\nmanage their workloads and applications on various cloud-based products, including AWS and Azure. He received his B.Tech in\n\nComputer Science from Kurukshetra University in 2005. Additionally, he holds many industry-leading IT certifications. You can\n\nreach him on LinkedIn at mogupta84 or follow his twitter handle @mogupta.\n\nAmandeep Singh works as a distinguished Engineer with Pitney Bowes India Pvt Ltd. He has extensive development experience\n\nof more than 13 years in product companies like Pitney Bowes and Dell R&D center. His current role involves designing cloud\n\nbased distributed solutions at enterprise scale. He is a AWS certified Solutions Architect, and helps Pitney Bowes migrate large\n\nmonolith platform to AWS Cloud in the form of simpler and smarter microservices. He is strong believer of new age DevOps\n\nprinciples and microservices patterns. He can be reached on LinkedIn at bhatiaamandeep.\n\nObjectives\n\nExplore AWS services for supporting a serverless environment\n\nSet up AWS services to make applications scalable and highly available\n\nDeploy a static website with a serverless architecture\n\nBuild your first serverless web application\n\nStudy the changes in a deployed serverless web application\n\nApply best practices to ensure overall security, availability, and reliability\n\nAudience\n\nServerless Architectures with AWS is for you if you want to develop serverless applications and have some prior coding\n\nexperience. Though no prior experience of AWS is needed, basic knowledge of Java or Node.js will be an advantage.\n\nApproach\n\nServerless Architectures with AWS takes a hands-on approach to learning how to design and deploy serverless architectures. It\n\ncontains multiple activities that use real-life business scenarios for you to practice and apply your new skills in a highly relevant\n\ncontext.\n\nHardware Requirements\n\nFor an optimal student experience, we recommend the following hardware configuration:\n\nProcessor: Intel Core i5 or equivalent\n\nMemory: 4 GB RAM\n\nStorage: 35 GB available space\n\nSoftware Requirements\n\nYou'll also need the following software installed in advance:\n\nOperating system: Windows 7 or above\n\nAWS Free Tier account\n\nNetwork access on ports 22 and 80\n\nConventions\n\nCode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and\n\nTwitter handles are shown as follows: \"You can also copy this code from the s3_with_lambda.js file.\"\n\nA block of code is set as follows:\n\nvar AWS = require('aws-sdk');\n\nvar s3 = new AWS.S3();\n\nNew terms and important words are shown in bold. Words that you see on the screen, for example, in menus or dialog boxes,\n\nappear in the text like this: \"Click on Next and follow the instructions to create the bucket.\"\n\nAdditional Resources\n\nThe code bundle for this book is also hosted on GitHub at https://github.com/TrainingByPackt/Serverless-Architectures-with-\n\nAWS.\n\nWe also have other code bundles from our rich catalog of books and videos available at https://github.com/PacktPublishing/.\n\nCheck them out!\n\nOceanofPDF.com\n\nAWS, Lambda, and Serverless Applications\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain the serverless model\n\nDescribe the different AWS serverless services present in the AWS ecosystem\n\nCreate and execute an AWS Lambda function\n\nThis chapter teaches you the basics of serverless architectures, focusing on AWS.\n\nIntroduction\n\nImagine that a critical application in your company is having performance issues. This application is available to customers 24-7, and\n\nduring business hours, the CPU and memory utilization reaches 100%. This is resulting in an increased response time for customers.\n\nAround 10 years ago, a good migration plan to solve this issue would involve the procurement and deployment of new hardware resources\n\nfor both the application and its databases, the installation of all required software and application code, performing all functional and\n\nperformance quality analysis work, and finally, migrating the application. The cost of this would run into millions. However, nowadays,\n\nthis issue can be resolved with new technologies that offer different approaches to customers – going Serverless is definitely one of them.\n\nIn this chapter, we'll start with an explanation of the serverless model, and get started with AWS and Lambda, the building blocks of a\n\nserverless applications on AWS. Finally, you'll learn how to create and run Lambda functions.\n\nThe Serverless Model\n\nTo understand the serverless model, let's first understand how we build traditional applications such as mobile applications and web\n\napplications. Figure 1.1 shows a traditional on-premises architecture, where you would take care of every layer of application\n\ndevelopment and the deployment process, starting with setting up hardware, software installation, setting up a database, networking,\n\nmiddleware configuration, and storage setup. Moreover, you would need a staff of engineers to set up and maintain this kind of on-\n\npremises setup, making it very time-consuming and costly. Moreover, the life cycle of these servers was no longer than 5-6 years, which\n\nmeant that you would end up upgrading your infrastructure every few years.\n\nThe work wouldn't end there, as you would have to perform regular server maintenance, including setting up server reboot cycles and\n\nrunning regular patch updates. And despite doing all the groundwork and making sure that the system ran fine, the system would actually\n\nfail and cause application downtime. The following diagram shows a traditional on-premises architecture:",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-17)",
      "start_page": 10,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "Figure 1.1 : Diagram of traditional on-premises architecture\n\nThe serverless model changes this paradigm completely, as it abstracts all the complexity attached with provisioning and managing data\n\ncenters, servers, and software. Let's understand it in more detail.\n\nThe serverless model refers to applications in which server management and operational tasks are completely hidden from end users, such\n\nas developers. In the serverless model, developers are dedicated specifically to business code and the application itself, and they do not\n\nneed to care about the servers where the application will be executed or run from, or about the performance of those servers, or any\n\nrestrictions on them. The serverless model is scalable and is actually very flexible. With the serverless model, you focus on things that are\n\nmore important to you, which is most probably solving business problems. The serverless model allows you to focus on your application\n\narchitecture without you needing to think about servers.\n\nSometimes, the term \"serverless\" can be confusing. Serverless does not mean that you don't need any servers at all, but that you are not\n\ndoing the work of provisioning servers, managing software, and installing patches. The term \"the serverless model\" just means that it is\n\nsomeone else's servers. Serverless architectures, if implemented properly, can provide great advantages in terms of lowering costs and\n\nproviding operational excellence, thus improving overall productivity. However, you have to be careful when dealing with the challenges\n\nimposed by serverless frameworks. You need to make sure that your application doesn't have issues with performance, resource\n\nbottlenecks, or security.\n\nFigure 1.2 shows the different services that are part of the serverless model. Here, we have different services for doing different kinds of\n\nwork. We have the API Gateway service, a fully managed REST interface, which helps to create, publish, maintain, monitor, and secure\n\nAPIs. Then, we have the AWS Lambda service that executes the application code and does all the computation work. Once computation is\n\ndone, data gets stored in the DynamoDB database, which is again a fully managed service that provides a fast and scalable database\n\nmanagement system. We also have the S3 storage service, where you can store all your data in raw formats that can be used later for data\n\nanalytics. The following diagram talks about the serverless model:\n\nFigure 1.2 : The serverless model (using AWS services)\n\nServerless models have become quite popular in recent times, and many big organizations have moved their complete infrastructure to\n\nserverless architectures and have been running them successfully, getting better performance a much a lower cost. Many serverless\n\nframeworks have been designed, making it easier to build, test, and deploy serverless applications. However, our focus in this book will be\n\non serverless solutions built on Amazon Web Services (AWS). Amazon Web Services is a subsidiary of Amazon.com that provides on-\n\ndemand cloud service platforms.\n\nBenefits of the Serverless Model\n\nThere are a number of benefits to using a serverless model:\n\nNo server management: Provisioning and managing servers is a complex task and it can take anything from days to months to\n\nprovision and test new servers before you can start using them. If not done properly, and with a specific timeline, it can become a\n\npotential obstacle for the release of your software onto the market. Serverless models provide great relief here by masking all the\n\nsystem engineering work from the project development team.\n\nHigh availability and fault tolerance: Serverless applications have built-in architecture that supports high availability (HA). So,\n\nyou don't need to worry about the implementation of these capabilities. For example, AWS uses the concept of regions and\n\navailability zones to maintain the high availability of all AWS services. An availability zone is an isolated location inside a region\n\nand you can develop your application in such a way that, if one of the availability zones goes down, your application will continue\n\nto run from another availability zone.\n\nScalability: We all want our applications to be successful, but we need to make sure that we are ready when there is an absolute\n\nneed for scaling. Obviously, we don't want to spawn very big servers in the beginning (since this can escalate costs quickly), but we\n\nwant to do it as and when the need occurs. With serverless models, you can scale your applications very easily. Serverless models\n\nrun under the limits defined by you, so you can easily expand those limits in the future. You can adjust the computing power, the\n\nmemory, or IO needs of your application with just a few clicks, and you can do that within minutes. This will help you to control\n\ncosts as well.\n\nDeveloper productivity: In a serverless model, your serverless vendor takes all the pain of setting up hardware, networking, and\n\ninstalling and managing software. Developers need to focus only on implementing the business logic and don't need to worry about\n\nunderlying system engineering work, resulting in higher developer productivity.\n\nNo idle capacity: With serverless models, you don't need to provision computing and storage capacity in advance. And you can\n\nscale up and down based on your application requirements. For example, if you have an e-commerce site, then you might need\n\nhigher capacity during festive seasons than other days. So, you can just scale up resources for that period only.\n\nMoreover, today's serverless models, such as AWS, work on the \"pay as you go\" model, meaning that you don't pay for any capacity\n\nthat you don't use. This way, you don't pay anything when your servers are idle, which helps to control costs.\n\nFaster time to market: With serverless models, you can start building software applications in minutes, as the infrastructure is\n\nready to be used at any time. You can scale up or down underlying hardware in a few clicks. This saves you time with system\n\nengineering work and helps to launch applications much more quickly. This is one of the key factors for companies adopting the\n\nserverless model.\n\nDeploy in minutes: Today's serverless models simplify deployment by doing all the heavy lifting work and eliminating the need for\n\nmanaging any underlying infrastructure. These services follow DevOps practices.\n\nIntroduction to AWS\n\nAWS is a highly available, reliable, and scalable cloud service platform offered by Amazon that provides a broad set of infrastructure\n\nservices. These services are delivered on an \"on-demand\" basis and are available in seconds. AWS was one of the first platforms to offer\n\nthe \"pay-as-you-go\" pricing model, where there is no upfront expense. Rather, payment is made based on the usage of different AWS\n\nservices. The AWS model provides users with compute, storage, and throughput as needed.\n\nThe AWS platform was first conceptualized in 2002 and Simple Queue Service (SQS) was the first AWS service, which was launched in\n\n2004. However, the AWS concept has been reformulated over the years, and the AWS platform was officially relaunched in 2006,\n\ncombining the three initial service offerings of Amazon S3 (Simple Storage Service): cloud storage, SQS, and EC2 (Elastic Compute\n\nCloud). Over the years, AWS has become a platform for virtually every use case. From databases to deployment tools, from directories to\n\ncontent delivery, from networking to compute services, there are currently more than 100 different services available with AWS. More\n\nadvanced features, such as machine learning, encryption, and big data are being developed at a rapid pace. Over the years, the AWS\n\nplatform of products and services has become very popular with top enterprise customers. As per current estimates, over 1 million\n\ncustomers trust AWS for their IT infrastructure needs.\n\nAWS Serverless Ecosystem\n\nWe will take a quick tour of the AWS serverless ecosystem and briefly talk about the different services that are available. These services\n\nwill be discussed in detail in future chapters.\n\nFigure 1.4 shows the AWS serverless ecosystem, which is comprised of eight different AWS services:\n\nLambda: AWS Lambda is a compute service that runs code in response to different events, such as in-app activity, website clicks,\n\nor outputs from connected devices, and automatically manages the compute resources required by the code. Lambda is a core\n\ncomponent of the serverless environment and integrates with different AWS services to do the work that's required.\n\nSimple Storage Service (S3): Amazon S3 is a storage service that you can use to store and retrieve any amount of information, at\n\nany time, from anywhere on the web. AWS S3 is a highly available and fault-tolerant storage service.\n\nSimple Queue Service (SQS): Amazon SQS is a distributed message queuing service that supports message communication\n\nbetween computers over the internet. SQS enables an application to submit a message to a queue, which another application can\n\nthen pick up at a later time.\n\nSimple Notification Service (SNS): Amazon SNS is a notification service that coordinates the delivery of messages to subscribers.\n\nIt works as a publish/subscribe (pub/sub) form of asynchronous communication.\n\nDynamoDB: Amazon DynamoDB is a NoSQL database service.\n\nAmazon Kinesis: Amazon Kinesis is a real-time, fully managed, and scalable service.\n\nStep Functions: AWS Step Functions make it easy to coordinate components of distributed applications. Suppose you want to start\n\nrunning one component of your application after another one has completed successfully, or you want to run two components in\n\nparallel. You can easily coordinate these workflows using Step Functions. This saves you the time and effort required to build such\n\nworkflows yourself and helps you to focus on business logic more.\n\nAthena: Amazon Athena is an interactive serverless query service that makes it easy to use standard SQL to analyze data in\n\nAmazon S3. It allows you to quickly query structured, unstructured, and semi-structured data that's stored in S3. With Athena, you\n\ndon't need to load any datasets locally or write any complex ETL (extract, transform, and load), as it provides the capability to\n\nread data directly from S3. We will learn more about AWS Athena in Chapter 4, Serverless Amazon Athena and the AWS Glue Data\n\nCatalog.\n\nHere's a diagram of the AWS serverless ecosystem:\n\nFigure 1.3 : The AWS serverless ecosystem Ecosystem\n\nAWS Lambda\n\nAWS Lambda is a serverless computing platform that you can use to execute your code to build on-demand, smaller applications. It is a\n\ncompute service that runs your backend code without you being involved in the provisioning or managing of any servers in the\n\nbackground.\n\nThe Lambda service scales automatically based on your usage and it has inbuilt fault-tolerance and high availability, so you don't need to\n\nworry about the implementation of HA or DR (disaster recovery) with AWS Lambda. You are only responsible for managing your code,\n\nso you can focus on the business logic and get your work done.\n\nOnce you upload your code to Lambda, the services handles all the capacity, scaling, patching, and infrastructure to run your code and\n\nprovides performance visibility by publishing real-time metrics and logs to Amazon CloudWatch. You select the amount of memory\n\nallocation for your function (between 128 MB and 3 GB). Based on the amount of memory allocation, CPU and network resources are\n\nallocated to your function. You could also say that AWS Lambda is a function in code that allows stateless execution to be triggered by\n\nevents. This also means that you cannot log in to actual compute instances or customize any underlying hardware.\n\nWith Lambda, you only pay for the time that your code is running. Once execution is completed, the Lambda service goes into idle mode\n\nand you don't pay for any idle time. AWS Lambda follows a very fine-grained pricing model, where you are charged for compute time in\n\n100 ms increments. It also comes with a Free Tier, with which you can use Lambda for free until you reach a certain cap on the number of\n\nrequests. We will study AWS Lambda pricing in more detail in a later section.\n\nAWS Lambda is a great tool for triggering code in the cloud that functions based upon events. However, we need to remember that AWS\n\nLambda (in itself) is stateless, meaning that your code should run as you develop it in a stateless manner. However, if required, a database\n\nsuch as DynamoDB can be used. Over the years, AWS Lambda has become very popular for multiple serverless use cases, such as web\n\napplications, data processing, IoT devices, voice-based applications, and infrastructure management.\n\nSupported Languages\n\nLambda is stateless and serverless. You should develop your code so that it runs in a stateless manner. If you want to use other third-party\n\nservices or libraries, AWS allows you to zip up those folders and libraries and give them to Lambda in a ZIP file, which in turn enables\n\nother supportive languages that you would like to use.\n\nAWS Lambda supports code written in the following six languages:\n\nNode.js (JavaScript)\n\nPython\n\nJava (Java 8 compatible)\n\nC# (.NET Core)\n\nGo\n\nPowerShell\n\nNote\n\nAWS Lambda could change the list of supported languages at any time. Check the AWS website for the latest information.\n\nExercise 1: Running Your First Lambda Function\n\nIn this exercise, we'll create a Lambda function, specify the memory and timeout settings, and execute the function. We will create a basic\n\nLambda function to generate a random number between 1 and 10.\n\nHere are the steps for completion:\n\n1. Open a browser and log in to the AWS console by going to this URL: https://aws.amazon.com/console/.\n\nFigure 1.4 : The AWS console\n\n2. Click on Services at the top-left of the page. Either look for Lambda in the listed services or type Lambda in the search box, and\n\nclick on the Lambda service in the search result:\n\nFigure 1.5 : AWS services\n\n3. Click on Create a function to create your first Lambda function on the AWS Lambda page:\n\nFigure 1.6 : The Get started window\n\n4. On the Create function page, select Author from scratch:\n\nFigure 1.7 : The Create function page\n\n5. In the Author from scratch window, fill in the following details:\n\nName: Enter myFirstLambdaFunction.\n\nRuntime: Choose Node.js 6.10. The Runtime window dropdown shows the list of languages that are supported by AWS Lambda,\n\nand you can author your Lambda function code in any of the listed options. For this exercise, we will author our code in Node.js.\n\nRole: Choose Create new role from one or more template. In this section, you specify an IAM role.\n\nRole name: Enter lambda_basic_execution.\n\nPolicy templates: Select Simple Microservice permissions:\n\nFigure 1.8 : The Author from scratch window\n\n6. Now, click on Create function. You should see the message shown in the following screenshot:\n\nFigure 1.9 : Output showing Lambda function creation\n\nSo, you have created your first Lambda function, but we have yet to change its code and configuration based on our requirements.\n\nSo, let's move on.\n\n7. Go to the Function code section:\n\nFigure 1.10 : The Function code window\n\n8. Use the Edit code inline option to write a simple random number generator function.\n\n9. The following is the required code for our sample Lambda function. We have declared two variables: minnum and maxnum. Then,\n\nwe are using the random() method of the Math class to generate a random number. Finally, we call \"callback(null, generatednumber)\". If an error occurs, null will be returned to the caller; otherwise, the value of the generatednumber\n\nvariable will be passed as an output:\n\n//TODO implement\n\nlet minnum = 0;\n\nlet maxnum = 10;\n\nlet generatednumber = Math.floor(Math.random() * maxnum) + minnum\n\ncallback(null, generatednumber);\n\n10. In the Basic settings window, write myLambdaFunction_settings in the Description field, select 128 MB in the Memory\n\nfield, and have 3 sec in the Timeout field:",
      "page_number": 10
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 18-28)",
      "start_page": 18,
      "end_page": 28,
      "detection_method": "topic_boundary",
      "content": "Figure 1.11 : The Basic settings window\n\n11. That's it. Click on the Save button in the top-right corner of the screen. Congratulations! You have just created your first Lambda\n\nfunction:\n\nFigure 1.12 : Output of the Lambda function created\n\n12. Now, to run and test your function, you need to create a test event. This allows you to set up event data to be passed to your\n\nfunction. Click on the dropdown next to Select a test event in the top-right corner of the screen and select Configure test event:\n\nFigure 1.13 : Lambda function Test window\n\n13. When the popup appears, click on Create new test event and give it a name. Click on Create and the test event gets created:\n\nFigure 1.14 : The Configure test event window\n\n14. Click on the Test button next to test events and you should see the following window upon successful execution of the event:\n\nFigure 1.15 : The Test execution window\n\n15. Expand the Details tab and more details about the function execution appear, such as actual duration, billed duration, actual\n\nmemory used, and configured memory:\n\nFigure 1.16 : The Details tab\n\nYou don't need to manage any underlying infrastructure, such as EC2 instances or Auto Scaling groups. You only have to provide your\n\ncode and let Lambda do the rest of the magic.\n\nActivity 1: Creating a New Lambda Function that Finds the Square Root of the Average of Two Input Numbers\n\nCreate a new Lambda function that finds the square root of the average of two input numbers. For example, the two numbers provided are\n\n10 and 40. Their average is 25 and the square root of 25 is 5, so your result should be 5. This is a basic Lambda function that can be\n\nwritten using simple math functions.\n\nHere are the steps for completion:\n\n1. Follow the exercise that we just completed before this activity.\n\n2. Go to the AWS Lambda service and create a new function.\n\n3. Provide the function name, runtime, and role, as discussed in the previous exercise.\n\n4. Under the section on Function code, write the code to find the square root of the average of two input numbers. Once done, save\n\nyour code.\n\n5. Create the test event and try to test the function by executing it.\n\n6. Execute the function.\n\nNote\n\nThe solution for this activity can be found on page 152.\n\nLimits of AWS Lambda\n\nAWS Lambda imposes certain limits in terms of resource levels, according to your account level. Some notable limits imposed by AWS\n\nLambda are as follows:\n\nMemory Allocation: You can allocate memory to your Lambda function with a minimum value of 128 MB and a maximum of\n\n3,008 MB. Based on memory allocation, CPU and network resources are allocated to the Lambda function. So, if your Lambda\n\nfunction is resource-intensive, then you might like to allocate more memory to it. Needless to say, the cost of a Lambda function\n\nvaries according to the amount of memory allocated to the function.\n\nExecution Time: Currently, the Lambda service caps the maximum execution time of your Lambda function at 15 minutes. If your\n\nfunction does not get completed by this time, it will be automatically be timed out.\n\nConcurrent Executions: The Lambda service allows up to 1000 total concurrent executions across all functions within a given\n\nregion. Depending on your usage, you may want to set the concurrent execution limit for your functions, otherwise the overall costs\n\nmay escalate very soon.\n\nNote\n\nIf you want to learn more about other limits of Lambda functions, go to\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/limits.html#limits-list.\n\nAWS Lambda Pricing Overview\n\nAWS Lambda is a serverless compute service and you only pay for what you use, not for any idle time. There is a Free Tier associated\n\nwith Lambda pricing. We will discuss the Lambda Free Tier in the next section.\n\nTo understand the AWS billing model for Lambda, you first need to understand the concept of GB-s.\n\n1 GB-s is 1 Gigabyte of memory used per second. So, if your code uses 5 GB in 2 minutes, and then 3 GB in 3 minutes, the accumulated\n\nmemory usage would be 5*120 + 3*180 = 1140 GB seconds.\n\nNote\n\nThe prices for the AWS services discussed in this section and in this book are current at the time of writing, as AWS prices may change at\n\nany time. For the latest prices, please check the AWS website.\n\nLambda pricing depends on the following two factors:\n\nTotal Request Count: This is the total number of times the Lambda function has been invoked to start executing in response to an\n\nevent notification or invoke call. As part of the Free Tier, the first 1 million requests per month are free. There is a charge of $0.20\n\nfor 1 million requests beyond the limits of the Free Tier.\n\nTotal Execution Time: This is the time taken from the start of your Lambda function execution until it either returns a value or\n\nterminates, rounded up to the nearest 100 ms. The price for execution time varies with the amount of memory allocated to your\n\nfunction. If you want to understand how the cost of total execution time varies with the total amount of memory allocated to the\n\nLambda function, go to https://aws.amazon.com/lambda/pricing:\n\nFigure 1.17 : Lambda pricing\n\nLambda Free Tier\n\nAs part of the Lambda Free Tier, you can make 1 million free requests per month. You can have 400,000 GB-seconds of compute time per\n\nmonth. Since function duration costs vary with the allocated memory size, the memory size you choose for your Lambda functions\n\ndetermines how long they can run in the Free Tier.\n\nNote\n\nThe Lambda Free Tier gets adjusted against monthly charges, and the Free Tier does not automatically expire at the end of your 12-month\n\nAWS Free Tier term, but is available to both existing and new AWS customers indefinitely.\n\nActivity 2: Calculating the Total Lambda Cost\n\nWe have a Lambda function that has 512 MB of memory allocated to it and there were 20 million calls for that function in a month, with\n\neach function call lasting 1 second. Calculate the total Lambda cost.\n\nHere's how we calculate the cost:\n\n1. Note the monthly compute price and compute time provided by the Free Tier.\n\n2. Calculate the total compute time in seconds.\n\n3. Calculate the total compute time in GB-s.\n\n4. Calculate the monthly billable compute in GB- s. Here's the formula:\n\nMonthly billable compute (GB- s) = Total compute – Free Tier compute\n\n5. Calculate the monthly compute charges in dollars. Here's the formula:\n\nMonthly compute charges = Monthly billable compute (GB-s) * Monthly compute price\n\n6. Calculate the monthly billable requests. Here's the formula:\n\nMonthly billable requests = Total requests – Free Tier requests\n\n7. Calculate the monthly request charges. Here's the formula:\n\nMonthly request charges = Monthly billable requests * Monthly request price\n\n8. Calculate the total cost. Here's the formula:\n\nMonthly compute charge + Monthly request charges\n\nNote\n\nThe solution for this activity can be found on page 153.\n\nAdditional Costs\n\nWhile estimating Lambda costs, you must be aware of additional costs. You will incur costs as part of Lambda integration with other AWS\n\nservices such as DynamoDB or S3. For example, if you are using the Lambda function to read data from an S3 bucket and write output\n\ndata into DynamoDB tables, you will incur additional charges for read from S3 and writing provisioned throughput to DynamoDB. We\n\nwill study more about S3 and DynamoDB in Chapter 2, Working with the AWS Serverless Platform.\n\nIn summary, it may not seem like running Lambda functions costs a lot of money, but millions of requests and multiple functions per\n\nmonth tend to escalate the overall cost.\n\nSummary\n\nIn this chapter, we focused on understanding the serverless model and getting started with AWS and Lambda, the first building block of a\n\nserverless application on AWS. We looked at the main advantages and disadvantages of the serverless model and its use cases. We\n\nexplained the serverless model, and worked with AWS serverless services. We also created and executed the AWS Lambda function.\n\nIn the next chapter, we'll look at the capabilities of the AWS Serverless Platform and how AWS supports enterprise-grade serverless\n\napplications, with and without Lambda. From Compute to API Gateway and from storage to databases, the chapter will cover the fully\n\nmanaged services that can be used to build and run serverless applications on AWS.\n\nOceanofPDF.com\n\nWorking with the AWS Serverless Platform\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain Amazon S3 and serverless deployments\n\nUse API Gateway and integrate it with AWS Lambda\n\nWork with fully managed services such as SNS, SQS, and DynamoDB\n\nThis chapter teaches you how to build and run serverless applications with AWS.\n\nIntroduction\n\nIn the previous chapter, we focused on understanding the serverless model and getting started with AWS and Lambda, the first building\n\nblocks of a serverless application on AWS. You also learned about how the serverless model differs from traditional product development.\n\nIn this chapter, we will learn about other AWS capabilities such as S3, SNS, and SQS. You can start by asking students about different\n\nAWS serverless technologies that the students have heard about or have had the chance to work with. Talk to them briefly about different\n\nAWS services such as S3 storage, API Gateway, SNS, SQS, and DynamoDB services. We will discuss them in detail in this chapter.\n\nAmazon S3\n\nAmazon Simple Storage Service or S3 is nothing but a cloud storage platform that lets you store and retrieve any amount of data\n\nanywhere. Amazon S3 provides unmatched durability, scalability, and availability so that you can store your data in one of the most secure\n\nways. This storage service is accessible via simple web interfaces, which can either be REST or SOAP. Amazon S3 is one of the most\n\nsupported platforms, so either you can use S3 as a standalone service or you can integrate it with other AWS services.\n\nAmazon S3 is an object storage unit that stores data as objects within resources called \"buckets\". Buckets are containers for your objects\n\nand serve multiple purposes. Buckets let you organize Amazon namespaces at the highest level and also play a key role in access control.\n\nYou can store any amount of objects within a bucket, while your object size can vary from 1 byte to 5 terabytes. You can perform read,\n\nwrite, and delete operations on your objects in the buckets.\n\nObjects in S3 consist of metadata and data. Data is the content that you want to store in the object. Within a bucket, an object is uniquely\n\nidentified by a key and a version ID. The key is the name of the object.\n\nWhen you add a new object in S3, a version ID is generated and assigned to the object. Versioning allows you to maintain multiple\n\nversions of an object. Versioning in S3 needs to be enabled before you can use it.\n\nNote\n\nIf versioning is disabled and you try to copy the object with the same name (key), it will overwrite the existing object.\n\nA combination of bucket, key, and version ID allows you to uniquely identify each object in Amazon S3.\n\nFor example, if your bucket name is aws-serverless and the object name is CreateS3Object.csv, the following would be the\n\nfully qualified path of an object in S3:\n\nFigure 2.1: Fully qualified URL to access the aws-serverless bucket that has an object\n\ncalled CreateS3Object.csv\n\nKey Characteristics of Amazon S3\n\nNow, let's understand some of the key characteristics of using the Amazon S3 service:\n\nDurability and high availability: Amazon S3 provides durable infrastructure to store your data and promises a durability of Eleven\n\n9s (99.999999999%). The Amazon S3 service is available in multiple regions around the world. Amazon S3 provides geographic\n\nredundancy within each region since your data gets copied automatically to at least three different availability zone locations within\n\na region. Also, you have the option to replicate your data across regions. As we saw earlier, you can maintain multiple versions of\n\nyour data as well, which can be used for recovery purposes later.\n\nIn the following diagram, you can see that when the S3 bucket in source-region-A goes down, route 53 is redirected to the\n\nreplicated copy in source-region-B:\n\nFigure 2.2: Amazon S3 Geographic Redundancy\n\nNote\n\nGeographic redundancy enables the replication of your data and stores this backup data in a separate physical location. You can always\n\nget your data back from this backup physical location just in case the main site fails.\n\nScalability: Amazon S3 is a highly scalable service as it can scale up or scale down easily based on your business needs. Suppose,\n\ntoday, that you have an urgent need to run analytics on 500 GB of data and before you do analytics, you have to bring that data into\n\nthe AWS ecosystem. Don't worry, as you can just create a new bucket and start uploading your data into it. All of the scalability\n\nwork happens behind the scenes, without any impact on your business.\n\nSecurity: In Amazon S3, you can enable server-side encryption, which encrypts your data automatically while it is getting written\n\non the S3 bucket. Data decryption happens by itself when someone wants to read the data. Amazon S3 also supports data transfer\n\nover SSL, and you can also configure bucket policies to manage object permissions and control access to your data using AWS\n\nIdentity and Access Management (IAM). We will look at permissions in more detail in a later part of this chapter.\n\nNote\n\nSince it is server-side encryption, there is no user interference required. Hence, when a user tries to read the data, the server\n\ndecrypts the data automatically.\n\nIntegration: You can use Amazon S3 as a standalone service to store data or you can integrate it with other AWS services such as\n\nLambda, Kinesis, and DynamoDB. We will look at some of these AWS services and their integration as part of our exercises in a\n\nlater part of this chapter.\n\nLow cost: Like other AWS serverless services, Amazon S3 works on a pay-as-you-go model. This means that there are no upfront\n\npayments and you pay based on your usage. Since it is a serverless offering, you don't need to manage any underlying hardware or\n\nnetwork resources. Therefore, there is no need to buy and manage expensive hardware. This helps to keep costs low with Amazon\n\nS3.\n\nAccess via APIs: You can use the REST API to make requests to Amazon S3 endpoints.\n\nDeploying a Static Website\n\nWith Amazon S3, you can host your entire static website at a low cost, while leveraging a highly available and scalable hosting solution to\n\nmeet varied traffic demands.\n\nExercise 2: Setting up a Static Website with an S3 Bucket Using a Domain Name in Route 53\n\nIn this exercise, we'll look at doing the following:\n\nCreating an S3 bucket and providing required permissions\n\nUploading a file onto an S3 bucket, which will be used to set the default page of your website\n\nConfiguring your S3 bucket\n\nSo, let's get started. Here are the steps to perform this exercise:\n\n1. Log in to your AWS account using your credentials.\n\n2. Click on the dropdown next to Services on top-left side and type S3:\n\nFigure 2.3: Searching Amazon S3 services via the dropdown option\n\n3. The Amazon S3 page will open. Click on Create Bucket:\n\nFigure 2.4: Creating an Amazon S3 bucket\n\n4. The Create bucket dialog box will open. You need to provide the following information:\n\nBucket Name: Enter a unique bucket name. For this book, we've used www.aws-serverless.tk since we will host a website\n\nusing our S3 bucket. As per AWS guidelines, a bucket name must be unique across all existing bucket names in Amazon S3. So,\n\nyou need to choose your individual bucket names.\n\nRegion: Click on the dropdown next to Region and select the region where you want to create the bucket. We will go with the\n\ndefault region, US-East (N. Virginia).\n\nIf you want to copy these settings from any other bucket and want to apply them to the new bucket, you can click on the dropdown\n\nnext to Copy settings from an existing bucket. We will configure the settings for this bucket here, so we will leave this option\n\nblank:\n\nFigure 2.5: The Create bucket menu: Name and region section\n\n5. Click on Next. We will be taken to the Properties window. Here, we can set the following properties of the S3 bucket:\n\nVersioning\n\nServer access logging\n\nTags\n\nObject-level logging\n\nDefault encryption",
      "page_number": 18
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 29-37)",
      "start_page": 29,
      "end_page": 37,
      "detection_method": "topic_boundary",
      "content": "Figure 2.6: The Create bucket menu: Set properties section\n\nFor this exercise, go with the default properties and click on the Next button.\n\n6. The next window is Set permissions. Here, we grant read and write permissions for this bucket to other AWS users and manage\n\npublic permissions as well. We can see in the following screenshot that the owner of the bucket has both read and write permissions\n\nby default. If you want to give permission for this bucket to any other AWS account as well, you can click on Add Account:\n\nFigure 2.7: The Create bucket menu: Set permissions option\n\n7. Keep all of the checkboxes unchecked. We'll host a website using this S3 bucket.\n\n8. Keep Manage system permissions with the default settings and click on the Next button to go to the Review screen. Here, you can\n\nreview all of the settings for your S3 bucket. If you want to change anything, click on the Edit button and change it. Alternatively,\n\nclick on Create Bucket and your bucket will be created:\n\nFigure 2.8: The Create bucket menu: Review section\n\n9. Click on the newly created bucket name and click on the second tab, Properties, and enable Static website hosting:\n\nFigure 2.9: Enabling the Static website hosting option under the Properties section\n\n10. Select the Use this bucket to host a website option. Enter the name of the index document. This document will be used to display\n\nthe home page of your website. You can also add an error.html file, which will be used to display the page in case of any error.\n\nWe aren't adding an error.html file for this exercise. You can also set redirection rules to redirect requests for an object to another object in the same bucket or to an external URL. Click on the Save button to save it:\n\nFigure 2.10: The Static website hosting menu\n\nNote\n\nAt the top, note the Endpoint information. This will be the URL to access your website. In this case, it is http://www.aws-\n\nserverless.com.s3-website-us-east-1.amazonaws.com/.\n\n11. Next, click on the Overview tab.\n\n12. In the Overview tab, click on Upload. Click on Add files. Upload the index.html page (found in the Chapter02 folder of the\n\ncode bundle) as an object into our S3 bucket. Now, click on the Next button:\n\nFigure 2.11: Uploading the Index.html file to the Amazon S3 bucket\n\nNote\n\nThe index.html file is a simple HTML file that contains basic tags, which are for demonstration purposes only.\n\n13. Under Manage Public Permissions, select Grant public read access to this object(s). Keep the rest of the settings as they are.\n\n14. Click on Next. Keep all of the properties to their default values on the Set properties screen. On the next screen, review the object\n\nproperties and click on the Upload button.\n\nCongratulations! You have just deployed your website using the Amazon S3 bucket.\n\n15. Go to a browser on your machine and go to the endpoint that we noted in step 10. You should see the home page (index.html)\n\ndisplayed on your screen:\n\nFigure 2.12: Viewing the uploaded Index.html file on the browser\n\nWe have successfully deployed our S3 bucket as a static website. There are different use case scenarios for S3 services, such as media\n\nhosting, backup and storage, application hosting, software, and data delivery.\n\nEnabling Versioning on S3 Bucket\n\nNow, we'll look at enabling versioning on an S3 bucket. Here are the steps to do so:\n\n1. Log in to your AWS account.\n\n2. In the S3 bucket name list, choose the name of the bucket that you want to enable versioning for.\n\n3. Select Properties.\n\n4. Select Versioning.\n\n5. Choose Enable versioning or Suspend versioning and then click on Save.\n\nS3 and Lambda Integration\n\nYour Lambda function can be called using Amazon S3. Here, the event data is passed as a parameter. This integration enables you to write\n\nLambda functions that process Amazon S3 events, for example, when a new S3 bucket gets created and you want to take an action. You\n\ncan write a Lambda function and invoke it based on the activity from Amazon S3:\n\nFigure 2.13: Demonstrating the integration of AWS S3 with AWS Lambda\n\nExercise 3: Writing a Lambda Function to Read a Text File\n\nIn this exercise, we will demonstrate AWS S3 integration with the AWS Lambda service. We will create an S3 bucket and load a text file.\n\nThen, we will write a Lambda function to read that text file. You will see an enhancement for this demonstration later in this chapter when\n\nwe integrate it further with the API Gateway service to show the output of that text file as an API response.\n\nHere are the steps to perform this exercise:\n\n1. Go to the AWS services console and open the S3 dashboard. Click on Create bucket and provide a bucket name. Let's call it\n\nlambda-s3-demo. Note that your bucket name must be unique:\n\nFigure 2.14: Creating an S3 bucket named lambda-s3-demo\n\n2. Click on Next and follow the instructions to create the bucket. Set all of the settings as default. Since we will write the Lambda\n\nfunction using the same account, we don't need to provide any explicit permission to this bucket.\n\n3. Create a file in your local disk and add the content Welcome to Lambda and S3 integration demo Class!! in the\n\nfile. Save it as sample.txt.\n\n4. Drag and drop this file into the Upload window to upload it to the newly created S3 bucket.\n\n5. Click on Upload:\n\nFigure 2.15: Uploading a sample text file to the newly created S3 bucket\n\nNote\n\nObserve the contents of this file's text message: Welcome to Lambda and S3 integration demo Class!!.\n\n6. Go to the AWS service portal, search for Lambda, and open the Lambda dashboard. Click on Create function and provide the\n\nfollowing details:\n\nProvide the name of the Lambda function. Let's name it read_from_s3.\n\nChoose the runtime as Node.js 6.10.\n\nChoose the Create a new role from one or more templates option. Provide the role name as read_from_s3_role.\n\nUnder policy templates, choose Amazon S3 object read-only permissions.\n\n7. Click on Create function.\n\n8. Once the Lambda function has been created, jump to the Function code section and replace the contents of the index.js file with the following code and save it. You can also copy this code from the s3_with_lambda.js file. In this script, we are creating two variables, src_bkt and src_key, which will contain the name of the S3 bucket and the name of the file that was\n\nuploaded to the bucket. Then, we will retrieve that file as an object from the S3 bucket using s3.getObject and return the\n\ncontents of the file as an output of the Lambda function:\n\nvar AWS = require('aws-sdk');\n\nvar s3 = new AWS.S3();\n\nexports.handler = function(event, context, callback) {\n\n// Create variables the bucket & key for the uploaded S3 object\n\nvar src_bkt = 'lambdas3demo';\n\nvar src_key = 'sample.txt';\n\n// Retrieve the object\n\ns3.getObject({\n\nBucket: src_bkt,\n\nKey: src_key\n\n}, function(err, data) {\n\nif (err) {\n\nconsole.log(err, err.stack);\n\ncallback(err);\n\n}\n\nelse {\n\nconsole.log('\\n\\n' + data.Body.toString()+'\\n');\n\ncallback(null, data.Body.toString());\n\n}\n\n});\n\n};\n\nNote that the default output of the data will be in binary format, so we are using the toString function to convert that binary output to a string:\n\nFigure 2.16: Illustrating the use of the toString() function\n\n9. Click on the Save button to save the Lambda function.",
      "page_number": 29
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 38-45)",
      "start_page": 38,
      "end_page": 45,
      "detection_method": "topic_boundary",
      "content": "10. Test the function now. But, before you can test it, you will have to configure test events, like we have done in earlier exercises.\n\nOnce a test event is configured, click on Test to execute the Lambda function.\n\nOnce the function has been executed, you should see the highlighted message Welcome to Lambda and S3 integration demo Class !!,\n\nas provided in the following screenshot. This message was the content of the sample.txt file that we uploaded into our S3 bucket in step 3:\n\nFigure 2.17: Demonstrating the Lambda function's execution\n\nNow, we have completed our discussion about S3 integration with a Lambda function.\n\nAPI Gateway\n\nAPI development is a complex process, and is a process that is constantly changing. As part of API development, there are many inherent\n\ncomplex tasks, such as managing multiple API versions, implementation of access and authorization, managing underlying servers, and\n\ndoing operational work. All of this makes API development more challenging and impactful on an organization's ability to deliver\n\nsoftware in a timely, reliable, and repeatable way.\n\nAmazon API Gateway is a service from Amazon that takes care of all API development-related issues (discussed previously) and enables\n\nyou to make your API development process more robust and reliable. Let's look into this in more detail now.\n\nWhat is API Gateway?\n\nAmazon API Gateway is a fully managed service that focuses on creating, publishing, maintaining, monitoring, and securing APIs. Using\n\nAPI Gateway, you can create an API that acts as a single point of integration for external applications while you implement business logic\n\nand other required functionality at the backend using other AWS services.\n\nWith API Gateway, you can define your REST APIs with a few clicks in an easy-to-use GUI environment. You can also define API\n\nendpoints, their associated resources and methods, manage authentication and authorization for API consumers, manage incoming traffic\n\nto your backend systems, maintain multiple versions of the same API, and perform operational monitoring of API metrics as well. You can\n\nalso leverage the managed cache layer, where the API Gateway service stores API responses, resulting in faster response times.\n\nThe following are the major benefits of using API Gateway. We have seen similar benefits of using other AWS services, such as Lambda\n\nand S3:\n\nScalability\n\nOperational monitoring\n\nPay-as-you-go model\n\nSecurity\n\nIntegration with other AWS services\n\nAPI Gateway Concepts\n\nLet's understand certain concepts of the API Gateway and how they work. This will help you build a better understanding on how the API\n\nGateway works:\n\nAPI endpoints: An API endpoint is one end of the communication, a location from where the API can access all the required\n\nresources.\n\nIntegration requests: The integration request specifies how your frontend will communicate with the backend system. Also,\n\nrequests may need to be transformed based on the type of backend system running. Possible integration types are Lambda, AWS\n\nservice, HTTP, and Mock.\n\nIntegration response: After the backend system processes the requests, API Gateway consumes it. Here, you specify how the\n\nerrors/response codes received from backend systems are mapped to the ones defined in API gateway.\n\nMethod request: The method request is a contract between the user (public interface) and the frontend system on what will be the\n\nrequest mode. This includes the API authorization and HTTP definitions.\n\nMethod response: Similar to the API method request, you can specify the method response. Here, you can specify the supported\n\nHTTP status codes and header information.\n\nExercise 4: Creating a REST API and Integrating It with Lambda\n\nNow, we will look at a demo of API Gateway and explore its different features. Along with this demo, we will also create a simple REST\n\nAPI using API Gateway and integrate it with a Lambda function. We will extend our earlier exercise on S3 integration with Lambda and\n\ncreate a REST API to show the contents of \"sample.txt\" as API response. This API will be integrated with Lambda to execute the\n\nfunction, and a GET method will be defined to capture the contents of the file and show it as the API response:\n\nFigure 2.18: Illustrating the various feature integrations of API Gateway with Lambda\n\nfunctions\n\nHere are the steps to perform this exercise:\n\n1. Open a browser and log in to the AWS console: https://aws.amazon.com/console/.\n\n2. Click on the dropdown next to Services or type API Gateway in the search box and click on the service:\n\nFigure 2.19: Searching for API Gateway from the Services section\n\n3. On the API Gateway dashboard, if you're visiting the page for the first time, click on Get Started. Otherwise, you will see the\n\nfollowing Create New API screen:\n\nFigure 2.20: The Create new API page\n\nHere, you have three options for choose from:\n\nNew API\n\nImport from Swagger\n\nExample API\n\n4. Select New API and provide the following details:\n\nAPI name: Enter read_from_S3_api\n\nDescription: Enter sample API\n\nEndpoint Type: Choose Regional and click on Create API.\n\nFigure 2.21: Creating a new API with the specified details\n\n5. On the next page, click on Actions. You will see some options listed as Resources and Methods. A resource works as a building\n\nblock for any RESTful API and helps in abstracting the information. Methods define the kind of operation to be carried out on the\n\nresources. A resource has a set of methods that operate on it, such as GET, POST, and PUT.\n\nWe haven't created any resources yet as part of this exercise, so the AWS console will only have the root resource and no other\n\nresources.\n\n6. Now, create a resource. On the resource dashboard, provide the resource name and click on Create Resource from the dropdown of\n\nAction.\n\n7. Type read_file_from_s3 in the Resource Name field and click on Create Resource:\n\nFigure 2.22: Creating a resource with the provided information\n\n8. Create a method to access the information. Select that resource and then click on Actions to create a method. Choose GET from the\n\navailable methods and click on ✓ to confirm the GET method type:\n\nFigure 2.23: Creating a method to access the available information\n\n9. Now, choose Lambda Function as the integration type:\n\nFigure 2.24: Selecting Lambda Function as an integration type\n\n10. Once you click on Save, you will get following warning. Here, AWS is asking you to provide API Gateway permission to invoke\n\nthe Lambda function:\n\nFigure 2.25: Warning notification to enable API Gateway's permission\n\n11. Click on the OK button. The following screen will appear, which shows the workflow of the API. The following are the steps taken\n\nby the API:\n\nYour API will invoke the Lambda function.\n\nThe Lambda function gets executed and sends the response back to the API.\n\nThe API receives the response and publishes it:\n\nFigure 2.26: Illustrating the workflow of an API\n\n12. Now, it's time to deploy the API. Click on the Actions dropdown and select Deploy API:\n\nFigure 2.27: The Deploy API menu\n\n13. Create a new deployment stage. Let's call it prod. Then, click on Deploy to deploy your API:\n\nFigure 2.28: Creating a new deployment stage named prod\n\n14. Once the API has been deployed, you should see the following screen. This screen has a few advanced settings so that you can\n\nconfigure your API. Let's skip this:\n\nFigure 2.29: The menu options of the deployed API\n\n15. Click on prod to open the submenu and select the GET method that you created for the API. Invoke the API URL that appears on\n\nthe screen. You can access this link to access your API:\n\nFigure 2.30: Invoking the API URL\n\nThis is what will appear on your screen:\n\nFigure 2.31: Illustrating the web page of the invoked URL\n\nGreat! You have just integrated the API Gateway with Lambda and S3.\n\nOther Native Services\n\nWe'll now turn our focus to other native services. We'll begin with Amazon SNS and then move on to Amazon SQS.\n\nAmazon SNS\n\nAmazon Simple Notification Services (SNS) is the cloud-based notification service that's provided by AWS that enables the delivery of\n\nmessages to the recipients or to the devices. SNS uses the publisher/subscriber model for the delivery of messages. Recipients can either\n\nsubscribe to one or more \"topics\" within SNS or can be subscribed by the owner of a particular topic. AWS SNS supports message\n\ndeliveries over multiple transport protocols.\n\nAWS SNS is very easy to set up and can scale very well depending on the number of messages. Using SNS, you can send messages to a\n\nlarge number of subscribers, especially mobile devices. For example, let's say you have set up the monitoring for one of your RDS\n\ninstances in AWS, and once the CPU goes beyond 80%, you want to send an alert in the form of an email. You can set up an SNS service\n\nto achieve this notification goal:\n\nFigure 2.32: Establishing the alert mechanism using the SNS services",
      "page_number": 38
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 46-53)",
      "start_page": 46,
      "end_page": 53,
      "detection_method": "topic_boundary",
      "content": "You can set up AWS SNS using the AWS Management Console, AWS command-line interface, or using the AWS SDK. You can use\n\nAmazon SNS to broadcast messages to other AWS services such as AWS Lambda, Amazon SQS, and to HTTP endpoints, email, or SMS\n\nas well.\n\nLet's quickly understand the basic components, along with their functions, of Amazon SNS:\n\nTopic: A topic is a communication channel that is used to publish messages. Alternatively, you can subscribe to a topic to start\n\nreceiving messages. It provides a communication endpoint for publishers and subscribers to talk to each other.\n\nPublication of messages: Amazon SNS allows you to publish messages that are then delivered to all the endpoints that have been\n\nconfigured as subscribers for a particular topic.\n\nHere are some of the applications of Amazon SNS:\n\nSubscription to messages: Using SNS, you can subscribe to a particular topic and start receiving all the messages that get\n\npublished to that particular topic.\n\nEndpoints: With Amazon SNS, you publish messages to the endpoints, which can be different applications based on your needs.\n\nYou can have an HTTP endpoint, or you can deliver your messages to other AWS services (as endpoints) such as SQS and Lambda.\n\nUsing SNS, you can configure emails or mobile SMS as possible endpoints as well. Please note that the mobile SMS facility is\n\navailable in limited countries. Please check the Amazon SNS documentation for more details.\n\nAmazon SQS\n\nIn a simple message queue service, we have applications playing the roles of producers and consumers. The applications, known as\n\nproducers, create messages and deliver them to the queues. Then, there is another application, called the consumer, which connects to the\n\nqueue and receives the messages. Amazon SQL is a managed service adaptation of such message queue services.\n\nAmazon Simple Queue Service (SQS) is a fully managed messaging queue service that enables applications to communicate by sending\n\nmessages to each other:\n\nFigure 2.33: Enabling Amazon SQS for better communication between applications\n\nAmazon SQS provides a secure, reliable way to set up message queues. Currently, Amazon SQS supports two types of message queues:\n\nStandard queues: Standard queues can support close to unlimited throughput, that is, an unlimited number of transactions per\n\nsecond. These queues don't enforce the ordering of messages, which means that messages may be delivered in a different order than\n\nthey were originally sent. Also, standard queues work on the at-least-once model, in which messages are delivered at least once,\n\nbut they may be delivered more than once as well. Therefore, you need to have a mechanism in place to handle message\n\nduplication. You should use standard queues, whose throughput is more important than the order of requests.\n\nFIFO queues: FIFO queues work on the First-In-First-Out message delivery model, wherein the ordering of messages is\n\nmaintained. Messages are received in the same order in which they were sent. Due to ordering and other limitations, FIFO queues\n\ndon't have the same throughput as what's provided by standard queues. Note that FIFO queues are available in limited AWS regions.\n\nPlease check the AWS website for more details. You should use FIFO queues when the order of messages is important.\n\nNote\n\nThere is a limit on the number of messages supported by FIFO queues.\n\nDead Letter (DL) queues: DL queues are queues that can receive messages that can't be processed successfully. You can configure\n\na dead letter queue as a target for all unprocessed messages from other queues.\n\nJust like Amazon SNS, you can also set up the AWS SQS service using the AWS Management Console, AWS command-line interface, or\n\nusing the AWS SDK.\n\nDynamoDB\n\nAmazon DynamoDB is a NoSQL database service that is fully managed. Here, you won't have to face the operative and scaling challenges\n\nof a distributed database. Like other serverless AWS services, with DynamoDB, you don't have to worry about hardware provisioning\n\nsetup, configuration data replication, or cluster scaling.\n\nDynamoDB uses the concept of partition keys to spread data across partitions for scalability, so it's important to choose an attribute with a\n\nwide range of values and that is likely to have evenly distributed access patterns.\n\nWith DynamoDB, you pay only for the resources you provision. There is no minimum fee or upfront payment required to use\n\nDynamoDB. The pricing of DynamoDB depends on the provisioned throughput capacity.\n\nThroughput Capacity\n\nIn DynamoDB, when you plan to provision a table, how do you know the throughput capacity required to get optimal performance out of\n\nyour application?\n\nThe amount of capacity that you provision depends on how many reads you are trying to execute per second, and also how many write\n\noperations you are trying to do per second. Also, you need to understand the concept of strong and eventual consistency. Based on your\n\nsettings, DynamoDB will reserve and allocate enough Amazon resources to keep low response times and partition data over enough\n\nservers to meet the required capacity to keep the application's read and write requirements.\n\nNote\n\nEventual consistency is a type of consistency where there is no guarantee that what you are reading is the latest updated data. Strong\n\nconsistency is another type of consistency where you always read the most recent version of the data. Eventual consistent operations\n\nconsume half of the capacity of strongly consistent operations.\n\nNow, let's look at some important terms:\n\nRead capacity: How many items you expect to read per second. You also have to specify the item size of your request. Two\n\nkilobyte items consume twice the throughput of one kilobyte items.\n\nWrite capacity: How many items you expect to write per second.\n\nNote\n\nYou are charged for reserving these resources, even if you don't load any data into DynamoDB. You can always change the\n\nprovisioned read and write values later.\n\nDynamoDB Streams\n\nDynamoDB Streams is a service that helps you capture table activity for DynamoDB tables. These streams provide an ordered sequence of\n\nitem-level modifications in a DynamoDB table and store the information for up to 24 hours. You can combine DynamoDB Streams with\n\nother AWS services to solve different kinds of problems, such as audit logs, data replication, and more. DynamoDB Streams ensure the\n\nfollowing two things:\n\nNo duplicity of stream records, which ensures that each stream record will only appear once\n\nThe ordered sequence of streams is maintained, which means that stream records appear in the same sequence as the modifications\n\nto the table\n\nAWS maintains separate endpoints for DynamoDB and DynamoDB Streams. To work with database tables and indexes, your application\n\nmust access a DynamoDB endpoint. To read and process DynamoDB Stream records, your application must access a DynamoDB Streams\n\nendpoint in the same region.\n\nDynamoDB Streams Integration with Lambda\n\nAmazon DynamoDB is integrated with AWS Lambda. This enables you to create triggers that can respond to events automatically in\n\nDynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.\n\nIntegration with Lambda allows you to perform many different actions with DynamoDB Streams, such as storing data modifications on S3\n\nor sending notifications using AWS services such as SNS.\n\nExercise 5: Creating an SNS topic and Subscribing to It\n\nIn this exercise, we'll create an SNS topic and subscribe to it. So, let's get started:\n\n1. Go to AWS services and type SNS in the search box. Once you click on Simple Notification Service (SNS), the following screen\n\nwill appear. Click on Get started, which will take you to the SNS dashboard:\n\nFigure 2.34: Creating a new SNS service\n\n2. Click on Topics on the left menu and click on Create new topic:\n\nFigure 2.35: Creating a new topic from the Topics section\n\n3. Provide the Topic name as TestSNS and the Display name as TestSNS, and click on Create topic. The Topic name and\n\nDisplay name can be different as well:\n\nFigure 2.36: Providing a Topic and Display name for the topic\n\n4. Once the topic has been created successfully, the following screen appears. This screen has the name of the topic and the topic's\n\nARN.\n\nNote\n\nARN stands for Amazon Resource Name, and it is used to identify a particular resource in AWS.\n\nFigure 2.37: Summary page of the newly created topic\n\nNote that if you need to reference a particular AWS resource in any other AWS service, you do so using the ARN.\n\nWe have successfully created a topic. Let's go ahead and create a subscription for this topic. We will set up an email notification as\n\npart of the subscription creation so that whenever something gets published to the topic, we will get an email notification.\n\n5. Click on Subscriptions on the left menu and then click on Create subscription:\n\nFigure 2.38: Creating a subscription for the SNS service\n\n6. Provide the ARN for the topic that we created in step 4. Click on the dropdown next to Protocol and choose Email. Provide an\n\nemail address as a value for the endpoint. Then, click on Create subscription:\n\nFigure 2.39: Providing details to create a new subscription\n\n7. Once the subscription has been created successfully, you should see the following screenshot. Note that the current status of the\n\nsubscription is PendingConfirmation:\n\nFigure 2.40: The summary of the newly created subscription\n\n8. Check your emails. You should have received an email notification from Amazon to confirm the subscription. Click on Confirm\n\nSubscription:\n\nFigure 2.41: Verifying the subscription from the registered email address\n\nOnce the subscription is confirmed, you should see the following screenshot:\n\nFigure 2.42: The Subscription confirmed message\n\n9. Now, go back to the Subscription page and you will notice that PendingConfirmation is gone. Click on the refresh button if you\n\nstill see PendingConfirmation. It should now be gone:\n\nFigure 2.43: Summary of the confirmed ARN subscription\n\nSo, you have successfully created an SNS topic and have successfully subscribed to that topic as well. Whenever anything gets published\n\nto this topic, you will get an email notification.\n\nExercise 6: SNS Integration with S3 and Lambda\n\nIn this exercise, we will see create a Lambda function and integrate it with SNS to send email notifications:\n\nFigure 2.44: Integrating a Lambda function with SNS to enable an email subscription\n\nHere are the steps to perform this exercise:\n\n1. Go to the AWS service console and type Lambda in the search box. Then, open the Lambda management page.\n\n2. Click on Create function and continue with the current selection, that is, Author from scratch:\n\nFigure 2.45: Creating a Lambda function from scratch\n\n3. Now, provide the following details:\n\nName: Write lambda_with_sns.\n\nRuntime: Keep it as Node.js.\n\nRole: Select Create role from template from the dropdown. Here, we are creating a Lambda function to send an SNS notification.\n\nRole name: Provide the role name as LambdaSNSRole.\n\nPolicy templates: Choose SNS publish policy:\n\nFigure 2.46: The menu options to create a Lambda function from scratch\n\n4. Now, click on Create function. Once the function has been created successfully, you should see the following message:\n\nFigure 2.47: The function created notification\n\n5. Let's jump to the function's code section. Go to the Git project and copy and paste the code in the code section of this page:\n\nFigure 2.48: Adding code from the Git project to the code section of the function",
      "page_number": 46
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 54-63)",
      "start_page": 54,
      "end_page": 63,
      "detection_method": "topic_boundary",
      "content": "The following is an explanation of the main parts of the code:\n\nsns.publish: The publish action is used to send a message to an Amazon SNS topic. In our case, we have an email subscription\n\non the topic, we are trying to publish onto. Therefore, a successful publishing here will result in an email notification.\n\nMessage: The message you want to send to the topic. This message text will be delivered to the subscriber.\n\nTopicArn: The topic you want to publish to. Here, we are publishing to the \"TestSNS\" topic, which we created in our previous exercise. So, copy and paste the ARN of the topic that we created in the earlier exercise here.\n\n6. Click on the Save button on the top right corner. Now, we are ready to test the code.\n\n7. Click on the Test button. You need to configure the test event. Let's create a test event with the name TestEvent and click on the\n\nSave button:\n\nFigure 2.49: Creating a test event named TestEvent\n\n8. Click on the Test button now, and you should see the following screen:\n\nFigure 2.50: The test execution was successful notification\n\n9. Expand the execution result. Here, you can find more details about the function executions. Here, you can review the duration of the\n\nfunction's execution, the resources that have been configured, billed duration, and max memory used:\n\nFigure 2.51: Summary of test execution\n\n10. Review the execution results under the Function code section as well:\n\nFigure 2.52: Reviewing the test execution results under the function code\n\nAs we can see, the following message in the execution results is Message sent successfully. This confirms that the Lambda code was\n\nsuccessful in sending a notification to the SNS topic.\n\nTime to check your email account, which was configured as part of the subscriber in the preview exercise. You should see the following\n\nAWS notification message:\n\nFigure 2.53: Sample email from the SNS service named TestSNS\n\nThis concludes our exercise on the simple integration of Lambda with Amazon SNS.\n\nActivity 3: Setting Up a Mechanism to Get an Email Alert When an Object Is Uploaded into an S3 Bucket\n\nIn the last exercise, we showcased lambda integration with Amazon SNS. As part of the exercise, whenever our lambda function was\n\nexecuted, we got an email alert generated by SNS service.\n\nNow, we will extend that exercise to perform an activity here.\n\nLet's assume that you are processing certain events and whenever there is an error with processing of a particular event, you move the\n\nproblematic event into a S3 bucket so you can process them separately. Also, you want to be notified via an email whenever any such an\n\nevent arrives in the S3 bucket.\n\nSo, we will do an activity to create a new S3 bucket and set up a mechanism that enables you to get an email alert whenever a new object\n\nis uploaded into this S3 bucket. When a new object is added to the S3 bucket, it will trigger the Lambda function created in the earlier\n\nexercise which will send the required email alert using SNS service.\n\nHere are the steps for completion:\n\n1. Go to AWS S3 service and click on Create bucket.\n\n2. Provide details such as name and region.\n\n3. Select the appropriate permissions.\n\n4. Go to the Lambda function created in the earlier exercise. Add S3 as a trigger under Lambda configuration section.\n\n5. Add the required details related to S3 bucket configuration, mainly the bucket name.\n\n6. Click on Add to add that S3 bucket as a trigger to execute Lambda function.\n\n7. Click on Save to save the changes to the Lambda function.\n\n8. Now, try to upload a new sample file to the S3 bucket. You should see an email alert in your mailbox.\n\nNote\n\nThe solution for this activity can be found on page 154.\n\nSummary\n\nIn this chapter, we looked at Amazon S3 and serverless deployments. We worked with API Gateway and its integration with AWS. We\n\ndelved into fully managed services such as SNS, SQS, and DynamoDB. Finally, we integrated SNS with S3 and Lambda.\n\nIn the next chapter, we'll build an API Gateway that we covered in this chapter. A comparison with a traditional on-premises web\n\napplication will be done as we replace traditional servers with serverless tools while making the application scalable, highly available, and\n\nperformant.\n\nOceanofPDF.com\n\nBuilding and Deploying a Media Application\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain the challenges of a traditional web application and making a traditional application serverless\n\nBuild an API Gateway API and upload binary data using it\n\nWork with media processing using AWS Lambda\n\nExplain image processing using AWS Rekognition\n\nThis chapter teaches you how to deploy your first serverless project step by step, by building a simple serverless application that uploads\n\nand processes media files.\n\nIntroduction\n\nEnterprises can face tremendous pressure and challenges when building and scaling even simple media-based applications. The\n\nconventional ways of building applications require enterprises to invest a lot of time and money up front, so that even simple application\n\ndevelopment can become a big project for companies.\n\nWhen it comes to building media-processing applications, which are generally very resource intensive, the situation gets even worse.\n\nIn this chapter, we are going to look at the challenges around building such applications and how cloud native development has changed\n\nthe way applications are built and delivered to customers.\n\nDesigning a Media Web Application – from Traditional to Serverless\n\nBuilding media web applications in the traditional way follows a certain path. This is displayed in the following diagram:\n\nFigure 3.1: Traditional way of building media applications\n\nHowever, in serverless application development, you don't manage the infrastructure but depend upon cloud providers for it. You have to\n\ndevelop your application to be independently deployable as microservices. During serverless development, you might want to break your\n\nbig monolithic application into smaller independent business units.\n\nSuch a serverless development brings many important patterns as well as development methodologies to be considered. Also, cloud\n\nproviders provide many managed services at every stage of the software development life cycle to help you build faster with out-of-the-\n\nbox monitoring/visibility in your serverless infrastructure.\n\nIn the next section of this chapter, we will take a look at the steps we need to follow if a media application has to be built in serverless\n\nmode. You will see we don't really need to talk to our IT department to raise any infrastructure requests and wait on them for weeks or\n\nmonths. Infrastructure is available with you within minutes from cloud providers.\n\nBuilding a Simple Serverless Media Web Application\n\nYou have realized by now that by using traditional structures, there is a lot of time-consuming technical administration.\n\nIn the cloud era, this is not the case. Cloud providers take care of all the infrastructure, as well as scaling and reliability, and the other\n\nneeds of your application, so you can focus on the business logic. This not only helps you focus on the right things, but also helps you\n\nreduce your time to market drastically.\n\nTo depict this, let's do a quick demo of our use case and look at how we can implement it in the AWS Cloud.\n\nWe'll deploy our web application locally. Clients will use this application to upload images to AWS. (Figure 3.2 depicts what we want to\n\nachieve in this tutorial.) Clients will call the APIs to upload images. These APIs will be hosted in an API Gateway and expose endpoints to\n\nupload images to S3. Once the image is uploaded to S3, an event will be triggered by S3 automatically that will launch a Lambda\n\nfunction. This Lambda function will read the image and process it using the AWS Rekognition service to find data inside the image. All\n\nthe infrastructure required for this is managed by AWS automatically.\n\nAuto scaling and reliability come out of the box by deploying the application on AWS Cloud's global infrastructure:\n\nFigure 3.2: Demonstrating working mechanism of serverless media web application\n\nExercise 7: Building the Role to Use with an API\n\nIn the following demo, we are using the AWS web console to build the role and assign it to the API.\n\nBefore you start creating the API, you need to create a proper role to assign to the API when it is created. This role should have access to\n\ncreate/read/update/delete S3 buckets and APIGatewayInvokeFullAccess. This role should also have\n\napigateway.amazonaws.com added to its trusted entities so that API Gateway can acquire this role.\n\nHere are the steps to perform this exercise:\n\n1. Search for IAM in the AWS console and open the Identity and Access Management window.\n\n2. Click on Roles to view existing roles.\n\n3. Click on Create role and select AWS service under Select type of trusted entity.\n\n4. Select API Gateway and click on Next: Permissions:\n\nFigure 3.3: Creating role window\n\n5. Click on Next: Review without changing anything.\n\n6. Name the role and give a description, as shown in the following screenshot. Name it api-s3-invoke-demo:\n\nFigure 3.4: Providing the role information in the Review section\n\nYour role is created. Let's add the required policy to it to work with S3.\n\n7. Click on the newly created role to go to its Summary page. On the Summary page of that role, click on Attach Policy:\n\nFigure 3.5: Summary page of newly created page\n\n8. On the policies page, search and add two policies: AmazonS3FullAccess and AmazonAPIGatewayInvokeFullAccess.\n\n9. After attaching the policies, the final role summary should be as follows:\n\nFigure 3.6: Summary page view with newly attached policies\n\nExercise 8: Creating an API to Push to / Get from an S3 Bucket\n\nIn this exercise, we will create an API that will interact with the AWS S3 service.\n\nWe will push files to S3 and also create the GET method in the API to fetch the contents of the S3 bucket. All this will be serverless, meaning we are not going to provision any EC2 instances, but use AWS's managed serverless infrastructure.\n\nHere are the steps to perform this exercise:\n\n1. In the Amazon API Gateway section of the AWS console, click on the Create API button:\n\nFigure 3.7: Creating a new API from the APIs section\n\n2. Select the New API radio and add the following details:\n\nAPI Name: image-demo\n\nDescription: this is a demo api for images\n\nEndpoint Type: Regional\n\nFigure 3.8: Creating a new API with specified details\n\n3. Click on Actions and select Create Resource to create a child resource named image and set it as a path variable under the\n\nresource path:",
      "page_number": 54
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 64-72)",
      "start_page": 64,
      "end_page": 72,
      "detection_method": "topic_boundary",
      "content": "Figure 3.9: Creating resource for newly created API\n\nMake sure you add { } in Resource Path.\n\n4. Create another child resource of image child and name it file:\n\nFigure 3.10: Creating another resource for the API\n\n5. Now that the resource has been created, you need to create methods for your API. Click on \"/{image}\" and from Actions, select\n\nCreate Method:\n\nFigure 3.11: Creating methods for the API\n\n6. Then, select GET in the setup and click on the tick mark:\n\nFigure 3.12: Selecting the GET method from the dropdown list\n\n7. Select the integration type as AWS Service and fill in the details for the GET method, as shown next. Also, select Use patch\n\noverride under the Action type, and fill in the details as {bucket} in the execution role. Mention the ARN of the role that was\n\ncreated. Then, click on Save:\n\nFigure 3.13: Selecting options to set up the GET method\n\n8. Click on Save. You should see what is shown in the following screenshot for the GET method:\n\nFigure 3.14: The Method Execution window of the GET method\n\nYou can see four sections in the preceding screenshot:\n\nMethod Request\n\nIntegration Request\n\nIntegration Response\n\nMethod Response\n\n9. Go back to Method Execution and click on Method Request, then add the Content-Type to the HTTP Request Headers section:\n\nFigure 3.15: HTTP Request Headers section\n\nNow, you need to map the path variable in the Method Request to the Integration Request. This is required because we want to\n\nsend the data coming in to the API request to the backend system. Method Request represents the incoming data request and\n\nIntegration Request represents the underlying request that is sent to the system actually doing the work. In this case, that system is\n\nS3.\n\n10. Click on Integration Request and scroll to URL Path Parameters. Click on Add Path to add following.\n\nName: bucket\n\nMapped from: method.request.path.image\n\n11. In Integration Request in the HTTP headers section, add two headers:\n\nx-amz-acl = 'authenticated-read'\n\nContent-Type = method.request.header.Content-Type\n\nNote\n\nx-amz-acl is required to tell S3 that this is the authenticated request. The user needs to be provided read access to the bucket.\n\nYour URL path parameters and HTTP headers for the Integration Request should look as follows now:\n\nFigure 3.16: HTTP Headers and URL Path Parameters section\n\n12. Repeat steps 5 to 11 to create the PUT method. Instead of selecting GET in step 6, you have to select PUT. We will use this method\n\nto create a new bucket.\n\nYour API should now look like this:\n\nFigure 3.17: Method Execution window of the PUT method\n\n13. Next, create the API to upload the image into the specified bucket. Click on /{file} and then select Create Method from the Action\n\ndropdown. Select the PUT method and configure it as shown in the following screenshot. Make note of the path override. It should\n\nbe set to {bucket}/{object}. Role ARN should be same as in previous steps. Click on Save.\n\nFigure 3.18: The Setup window of the PUT method\n\n14. In the Method Request, add the HTTP Header Content-type.\n\n15. Click on Integration Request, and add in URL Path Parameters, the mapping of the bucket and object as shown here, and click\n\non the tick mark:\n\nbucket = method.request.path.image\n\nobject = method.request.path.file\n\n16. Also, add the Content-Type header mapping to method.request.header.Content-Type as done earlier for other\n\nmethods.\n\nFigure 3.19: Method Request window of the PUT method\n\nOne more thing we need to do is to configure the API to accept binary image content.\n\n17. Now, go to Settings from the left navigation panel to configure the API to accept binary image content:\n\nFigure 3.20: Settings options to accept binary image content\n\n18. Add image/png under Binary Media Types and click on Save Changes:\n\nFigure 3.21: Add the Binary Media Type option under the Binary Media Types section\n\nAll changes are done. We are now ready to deploy our API.\n\n19. Click on Deploy API from the Actions dropdown:\n\nFigure 3.22: Deploying the API by clicking on the Deploy API option from the Actions\n\ndropdown list\n\n20. Enter the stage details and description, and click on Deploy:\n\nFigure 3.23: Deploying the API after providing all the details\n\nAll changes are done. We are now ready to deploy our API.\n\n21. Your API is deployed on the dev stage. Note the Invoke URL:\n\nFigure 3.24: Invoke URL of the newly deployed API\n\n22. You can use any API client, such as SoapUI or Postman, for testing your API. We'll use the ReadyAPI tool as it has robust support.\n\nNote\n\nYou can download a 14-day free trial at this link: https://smartbear.com/product/ready-api/free-trial/ (you have to enter details of\n\nyourself for the download to start).\n\n23. Now, create a bucket. In SoapUI, create a new project for a PUT request and enter the invoke URL copied earlier. Enter the bucket\n\nname in the path after /dev:\n\nFigure 3.25: Creating a new project for the PUT request\n\n24. Click on OK. Click on Continue to describe your API:\n\nFigure 3.26: Selecting appropriate options for the project\n\n25. Specify the bucket name after dev/ in Resource. In the following screenshot, mohit-1128-2099 is the bucket name. Change\n\nthe Media Type to application/xml:\n\nFigure 3.27: Specifying the bucket name\n\nIn our exercise, we are creating the S3 bucket in us-east-1. So, we will keep request body as blank. However, if you want to create\n\nthe bucket in some other region (us-west-, 1 in our, example below), you have to set following text in the request body and hit send.\n\nYou should get HTTP 200 Response status and your bucket should be created in S3:\n\n<CreateBucketConfiguration>\n\n<LocationConstraint>us-west-1</LocationConstraint>\n\n</CreateBucketConfiguration>\n\nNow the S3 bucket should get created. Go to the AWS S3 service and check the existence of the bucket. You can also do a GET API\n\ncall to check the existence of the bucket along with its contents.",
      "page_number": 64
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 73-80)",
      "start_page": 73,
      "end_page": 80,
      "detection_method": "topic_boundary",
      "content": "26. Now, make another call to our API to upload an image. Update the path in the Resource textbox of ReadyAPI to include the\n\nfilename that you want to upload in the S3 bucket. You need to attach the file and set the media-type to image/png. Go to the Attachments tab at the bottom of the request, and attach any PNG image. Click No on the Cache request dialog box.\n\n27. Click on Send and you should be able to get back 200 OK response. Go back to the AWS S3 service and you should see the newly\n\ncreated bucket now:\n\nFigure 3.28: Output showing the newly created bucket\n\nSo far, we have created an API with the GET and PUT methods that is accepting requests from the user and uploading images to S3. Note\n\nthat we haven't had to spawn a server so far for building the entire working service.\n\nExercise 9: Building the Image Processing System\n\nYou have just created the API. Now, you need to create the backend Lambda function that gets triggered every time an image is uploaded.\n\nThis function will be responsible for analyzing the image and detecting the labels inside the image, such as objects. It will call the\n\nRekognition API and feed the image into it, which will then analyze the image and return data.\n\nThe returned data will be pushed to a topic in SNS. SNS is AWS Simple Notification Service and works on the pub/sub mechanism. We\n\nwill then subscribe our email address to that SNS so any messages that are sent to the topic get delivered to our email address also.\n\nThe final functionality will be that when a user uploads an image using the API to the S3 bucket, our infrastructure analyzes it and emails\n\nus the data found in the image.\n\nWe are going to create this infrastructure step by step, as follows:\n\n1. Create the IAM role. As we created the role for API Gateway, we need to follow similar steps; however, we have to select Lambda\n\non the first screen under Choose the service that will use this role, and the permissions should be AWSLambdaFullAccess,\n\nAmazonRekognitionFullAccess, and AmazonSNSFullAccess. No need to add any tags for now. This is how your role should look after creation:\n\nFigure 3.29: Summary of the newly created ARN role\n\n2. Use the S3 bucket created with our API.\n\nOne more thing before creating the Lambda function is to create an SNS and subscribe your email to it. This SNS will be used by\n\nthe Lambda to publish image analysis data. Once published, the SNS will send that message to your subscribed email.\n\n3. Go to the SNS console and click on Create Topic to publish the extracted text:\n\nFigure 3.30: Create new topic window\n\n4. Click on Create subscription:\n\nFigure 3.31: Topic details window\n\n5. Create the subscription for the topic. Choose Email under Protocol and provide an email address under Endpoint. There will be an\n\nemail sent your email account. You have to confirm the subscription:\n\nFigure 3.32: Create subscription window\n\n6. Keep a note of the Topic ARN as it will be required in the Lambda code.\n\nWe have created an API Gateway API, S3 bucket, SNS Topic, and email subscription. Now, we have to create a Lambda function.\n\n7. In the Lambda console, click on Create function. Make sure that you have the same region selected as the one in which you have\n\ncreated the S3 bucket. Select Author from scratch.\n\n8. In the next wizard, fill the name of the Lambda, choose an existing role, and choose the S3 bucket name that you created:\n\nFigure 3.33: Screenshot of the Using blueprint rekognition-python window\n\n9. Scroll to the end and hit Create function.\n\n10. In the configuration section of the Lambda, go to the function code and replace it with the following code. In the code, we create the\n\nSDK object, AWS Rekognition client, and AWS SNS client. We then handle the incoming Lambda request. We create the bucket\n\nand image names. We call the detectLabels function to get all the labels using the AWS Rekognition service. Create the\n\nmessage to post and publish to the SNS. The detectLabels function is used to make the call to the Rekognition service using the bucket name:\n\nvar A = require('aws-sdk');\n\nvar rek = new A.Rekognition();\n\nvar sns = new A.SNS();\n\nAWS.config.update({region: 'us-east-1'});\n\nexports.handler = (event, context, callback) => {\n\nconsole.log('Hello, this is nodejs!');\n\n// Get the object from the event\n\nvar bucket = event['Records'][0]['s3']['bucket']['name'];\n\nvar imageName = event['Records'][0]['s3']['object']['key'];\n\ndetectLabels(bucket, imageName)\n\n.then(function(response){\n\nvar params = {\n\nMessage: JSON.stringify(response['Labels']), /* required */\n\nSubject: imageName,\n\nTopicArn: 'arn:aws:sns:us-east-1:XXXXXXXXXXXX:extract-image-labels-sns'\n\n};\n\nsns.publish(params, function(err, data) {\n\nif (err) console.log(err, err.stack); // an error occurred\n\nelse console.log(data); // successful response\n\n});\n\n});\n\ncallback(null, 'Hello from Lambda');\n\n};\n\nfunction detectLabels(bucket, key) {\n\nlet params = {\n\nImage: {\n\nS3Object: {\n\nBucket: bucket,\n\nName: key\n\n}\n\n}\n\n};\n\nreturn rekognition.detectLabels(params).promise();\n\n}\n\n11. Another thing to note is in the Lambda configuration in the S3 section. Make sure the trigger is enabled. If it is not, toggle the\n\nbutton to enable it:\n\nFigure 3.34: The Configure triggers window\n\nFigure 3.35: Enabling the S3 trigger for the Lambda function\n\nNote\n\nMake sure the S3 bucket is in the same region as the Lambda, or it won't be able to trigger the Lambda.\n\nThis concludes the creation of all the infrastructure.\n\nNow, when you call the API to upload any image, you should see an email in your inbox with content similar to this:\n\nFigure 3.36: Sample email after uploading an image\n\nDeployment Options in the Serverless Architecture\n\nWe have seen how we can create a serverless application using the AWS console. This is not the only way to achieve it. In the cloud\n\nworld, infrastructure automation is a key aspect of any deployment. Cloud providers have built strong frameworks around their services\n\nthat can be used to script out the entire infrastructure. AWS provides APIs, SDKs, and a CLI that can be consumed in various ways to\n\nprovision infrastructure automatically.\n\nIn general, there are three additional ways we can achieve the previous functionality without using the AWS console:\n\nAWS CLI: AWS provides a command-line interface for working with AWS services. It is built on top of an AWS Python SDK\n\ncalled boto. You just need to install Python on your Mac, Windows, or Linux machine and then install the AWS CLI.\n\nOnce installed, you can run the following command in your Terminal or command line to check it is properly installed:\n\n$ aws --version\n\naws-cli/1.11.96 Python/2.7.10 Darwin/16.7.0 botocore/1.8.2\n\nAWS Code SDKs: AWS provides many SDKs that can be used directly in your favorite programing language for working with\n\nAWS services. As of today, these are the programing languages AWS supports:\n\n.NET\n\nJava\n\nC++\n\nJavaScript\n\nPython\n\nRuby\n\nGo\n\nNode.js\n\nPHP\n\nServerless Framework: This is one option that is becoming more popular by the day. It is a command-line tool that can be used to\n\nbuild and deploy serverless cloud services. It can be used not only with AWS, but also with many other major cloud providers, such\n\nas Azure, Google Cloud Platform (GCP), and IBM Cloud.\n\nIt is built in JavaScript and requires Node.js v6.5.0 or later to be installed. For deployment, you provide a YAML-based file,\n\nserverless.yml, to the CLI. It internally translates all the content of YAML into an AWS CloudFormation template and uses it to\n\nprovision the infrastructure.\n\nIt is again a very powerful tool for working with serverless AWS-managed services.\n\nLike the AWS CLI, it can also be very nicely integrated into a CI/CD process in an enterprise to achieve automation.\n\nActivity 4: Creating an API to Delete the S3 Bucket\n\nCreate an API to delete the S3 bucket that we just created in the preceding exercise. In this activity, you need to expose an API that will\n\naccept the bucket name and will delete the S3 bucket.\n\nHere are the steps to complete the activity:\n\n1. Go to the AWS API Gateway console and in the API created in this chapter, create a Delete API.\n\n2. Configure the incoming headers and path parameters properly in the Method Request and Integration Request sections.\n\n3. Change the authorization of the Delete method from NONE to AWS_IAM.\n\n4. Click on the Deploy API.\n\n5. Test the Delete method using the Test Tool (Ready API).\n\nYou should see the bucket getting deleted in the AWS S3 console.\n\nNote\n\nThe solution for this activity can be found on page 157.\n\nSummary\n\nIn this chapter, you have seen the challenges of traditional web application development and how serverless development can address\n\nthem. You also learned how to work with API Gateway and expose a REST-based API with it. You integrated AWS S3 with API Gateway\n\nand created and read a bucket using PUT and GET APIs. We then created Lambda functions. We worked with AWS Rekognition in the\n\nevent-based architecture to analyze images at runtime and identify important data inside them.\n\nIn the next chapter, we'll explore the capabilities of AWS Athena. We'll also work with AWS Glue, and learn how to populate the AWS\n\nGlue Data Catalog.\n\nOceanofPDF.com",
      "page_number": 73
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 81-88)",
      "start_page": 81,
      "end_page": 88,
      "detection_method": "topic_boundary",
      "content": "Serverless Amazon Athena and the AWS Glue Data Catalog\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain serverless AWS Athena capabilities, as well as its storage and querying concepts\n\nAccess Amazon Athena and its different use cases\n\nCreate databases and tables in Athena\n\nExplain AWS Glue and its benefits\n\nWork with data catalogs and populate the AWS Glue Data Catalog\n\nThis chapter delves into the capabilities of AWS Athena. You'll also work with AWS Glue, and learn how to populate the AWS Glue Data\n\nCatalog.\n\nIntroduction\n\nConsider a situation where you're just about to leave for the day from the office, and at that very moment your boss asks you to run a\n\nreport on a new, complex dataset. You're asked to finish this report before you leave for the day.\n\nIn the past, completing such a report would've taken hours. You would have to first analyze the data, create a schema, and then dump the\n\ndata before you could execute queries to create the required report.\n\nNow, with the AWS Glue and Athena services, you can get such reports done very quickly and leave for the day on time.\n\nIn the previous chapter, we saw how serverless application development can address the challenges of traditional application development.\n\nIn this chapter, we'll explore the capabilities of AWS Athena. We'll also work with AWS Glue, and learn how to populate the AWS Glue\n\nData Catalog.\n\nAmazon Athena\n\nIn simple terms, Amazon Athena is nothing but an interactive query service that is serverless. It makes use of standard SQL to analyze\n\ndata in Amazon S3. It allows you to quickly query structured, unstructured, and semi-structured data that is stored in S3. With Athena, you\n\ndon't need to load any datasets locally or write any complex ETLs (Extracts, Transforms, and Loads) as it provides the capability to\n\nread data directly from S3.\n\nNote\n\nETL is a popular concept from the data warehouse world, where three separate functions are used to prepare data for data analysis. The\n\nterm extract refers to data extraction from the source dataset, transform refers to data transformation (if required), and load refers to data\n\nloading in the final tables, which will be used for data analysis.\n\nThe Amazon Athena service uses Presto technology. Presto is a distributed SQL query engine that is open source. Presto provides a SQL-\n\nlike dialect for querying data and is designed to provide fast performance for running interactive analytic queries. The size of the data\n\nsources doesn't matter. The AWS management console, Athena API, Athena CLI, or a simple JDBC connection can be used to access\n\nAmazon Athena.\n\nAthena is a serverless offering, meaning that you don't need to set up or manage any underlying data servers. What you must do is set up a\n\nconnection to your data in Amazon S3. Then, you must define the schema. Once you've done that, you can start querying with the help of\n\nthe query editor in the AWS management console. You can use ANSI SQL queries with Amazon Athena, including the support of joins\n\nand functions, to query data in S3. Therefore, it's easy for anyone with basic skills in SQL to analyze large-scale datasets quickly.\n\nAmazon Athena supports multiple data formats such as CSV, JSON, and Parquet. With Athena, you can query encrypted data (be it\n\nencrypted on the server side or client side). Amazon Athena also gives you the option to encrypt your result sets by integrating with KMS\n\n(Key Management Service):\n\nFigure 4.1: Ad hoc analysis using Amazon Athena\n\nNote\n\nAmazon Athena is priced per query. You will be charged for the data scanned per query. As you might've noticed, data can be stored in\n\ndifferent formats on Amazon S3. Therefore, you can use different formats to store data in a compressed form, resulting in lower amounts of\n\ndata being scanned by your query. You can partition your data or convert your data to columnar storage formats to read only the columns\n\nthat are required to process the data. Amazon Athena costs start from $5/TB of data scanned.\n\nAWS provides native integration of Athena with the AWS Glue Data Catalog. The AWS Glue Data Catalog provides you with a metadata\n\nstore that is persistent for your data in Amazon S3. You can, thereby, create tables and query data in Athena. We will study this concept in\n\nmore detail later in this chapter, along with an exercise.\n\nHere are some of the use cases of Amazon Athena:\n\nAd hoc analysis\n\nAnalyzing server logs\n\nUnderstanding unstructured data – works well with complex data types such as arrays or maps\n\nQuick reporting\n\nHere are some of the tools that are used to access Amazon Athena:\n\nAWS Management Console\n\nA JDBC or ODBC connection\n\nAPI\n\nCommand-line interface\n\nDatabases and Tables\n\nAmazon Athena allows you to create databases and tables. A database is basically a grouping of tables. By default, we have a sampledb database that has been created in Athena, but you can create a new database and start creating your tables underneath it if you like. Also,\n\nyou will find a table called elb_logs under sampledb:\n\nFigure 4.2: The Database page\n\nExercise 10: Creating a New Database and Table Using Amazon Athena\n\nLet's go ahead and create a new database and table so that we can understand Athena, alongside a quick demo, in more detail. Athena\n\nworks on data that's stored in S3. Before going to Athena, let's create an S3 bucket and upload the sample dataset that's provided. In this\n\nexercise, we will use the flavors_of_cacao.csv dataset that was provided with this book:\n\n1. Go to AWS services and search for Athena.\n\nYou should be able to see the following window:\n\nFigure 4.3: Query Editor\n\n2. Now, click on Create table and choose Manually.\n\nNote\n\nWe will look into the Automatically (AWS Glue crawler) option in the next part of this chapter.\n\n3. Provide the required details, such as the database name, table name, and S3 bucket details. You need to provide the S3 bucket's\n\ndetails in the same place where you have the data stored that you want to analyze via Athena.\n\n4. Click on the Next button.\n\nYou can create the table under the default (already selected) database as well:\n\nFigure 4.4: Add table\n\nNote\n\nThe table name needs to be unique here, so choose a different table name than what you see here. Before creating the table, I\n\nuploaded the flavors_of_cacao.csv dataset into my S3 bucket, that is, s3://aws-athena-demo-0606/. However, you\n\ncan make any changes in the underlying dataset post table creation, as long as you don't change the underlying schema. We will\n\nlook at this in more detail.\n\n5. Click on Next, and you will see the different data formats that you can access using AWS Athena.\n\n6. Choose CSV and click on Next:\n\nFigure 4.5: Data Format\n\n7. On the next screen, define the columns, along with their data types. Either you can define all of the columns one by one, or you can\n\nclick on the Bulk add columns option to add all of the columns' details into one place.\n\nWe have nine columns in our datasets, so we will add the following information, along with their data types:\n\nCompany string,Bean_Origin string,REF int,Review_Date int,Cocoa_Percent\n\nstring,Company_Location string,Rating decimal,Bean_Type string,Broad_Bean_Origin string\n\nNote that the header is removed from the data file. The column details can be understood from the following screenshot:\n\nFigure 4.6: Bulk add columns\n\n8. Once you click on the Add button, you will note that all of the columns, along with their data types, are displayed. You can make\n\nany changes (as required) here as well:\n\nFigure 4.7: The Columns section\n\n9. Click on Next. You can configure partitions on this screen. Partitions allow you to create logical groups of information together,\n\nwhich helps with the faster retrieval of information. Partitions are generally recommended for larger datasets.\n\nHowever, since our dataset is quite small, we will skip configuring partitions for now:\n\nFigure 4.8: Partitions\n\n10. Click on Create Table and Athena will run the create table statement. Your table will be created. Now, you should see\n\nfollowing screen:\n\nFigure 4.9: Run query\n\nHere, we can note that our database, Athena_demo, has been created, and a new table, flavours_of_cocoa, has been created as well. You can see the table definition on the right of the screen.\n\nNote\n\nIf you don't want to go through GUI for table creation, you can write the table Data Definition Language (DDL) directly into the\n\nquery window and create the table. You can also use the ALTER and DROP TABLE commands to modify and drop the existing\n\ntables, respectively.\n\n11. Click on the Save as button and provide a name for the query:",
      "page_number": 81
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 89-96)",
      "start_page": 89,
      "end_page": 96,
      "detection_method": "topic_boundary",
      "content": "Figure 4.10: Choosing a name window\n\nNow, the table has been created, and you can run SQL statements to view the table data.\n\n12. In the following screenshot, you will see the 10 rows that have been selected from the table:\n\nFigure 4.11: Selecting 10 rows\n\n13. Write SQL functions as well to analyze the data from different perspectives. Here, we will list the total products of a company that\n\nhave a rating greater than 4 by performing a group by operation on the company and getting the required count:\n\nselect company, count(*) cnt from flavours_of_cocoa\n\nwhere rating > 4\n\ngroup by company\n\norder by cnt desc\n\nlimit 10;\n\n14. Execute the query.\n\nYou'll see the following output in the result pane:\n\nFigure 4.12: Output window\n\nSince Athena is based on hive SQL and Presto, you can use many of the Hive functions with Athena.\n\nNote\n\nFor the complete documentation on supported SQL queries, functions, and operators, go to\n\nhttps://docs.aws.amazon.com/athena/latest/ug/functions-operators-reference-section.html.\n\nWith this, we have completed our exercise on AWS Athena. As we stated earlier in this chapter, Athena is a wonderful query service that\n\nsimplifies analyzing data from Amazon S3 directly.\n\nBe aware that you get charged on the amount of data being analyzed for your query. In the preceding query, you can see the highlighted\n\npart for the amount of data being scanned by the query. However, if you don't apply the proper filters, you can end up scanning\n\nunnecessarily huge amounts of data, which eventually escalates the overall costs.\n\nAWS Glue\n\nAWS Glue is a serverless, cloud-optimized, and fully managed ETL service that provides automatic schema inference for your structured\n\nand semi-structured datasets. AWS Glue helps you understand your data, suggests transformations, and generates ETL scripts so that you\n\ndon't need to do any ETL development.\n\nYou can also set up AWS Glue for running your ETL jobs, automatically provisioning and scaling the resources needed to complete them.\n\nYou can point AWS Glue to your data that's stored on different AWS services such as S3, RDS, and Redshift. It finds out what your data\n\nis. It stores the related metadata, such as schemas and table definitions, in the AWS Glue Data Catalog.\n\nOnce your data is cataloged, you can start using it for different kinds of data analysis. For executing data transformations and data loading\n\nprocesses, AWS Glue generates code.\n\nFirst, let's understand the major components of AWS Glue, which might be new to the students:\n\nAWS Glue Data Catalog: A data catalog is used to organize your data. Generally, glue crawlers populate data catalogs, but you can\n\nuse DDL statements as well to populate it. You can bring in metadata information from multiple data sources such as Amazon S3,\n\nRedshift, or RDS instances, and create a single data catalog for all of them. Now, all of the metadata is in one place and is\n\nsearchable. The Glue catalog is basically a replacement for Hive Metastore.\n\nNote\n\nA data catalog is mainly comprised of metadata information (definitions) related to database objects such as tables, views,\n\nprocedures, indexes, and synonyms. Almost all databases in the market today have data catalogs populated in the form of\n\ninformation schema. Data catalogs help users to understand and consume data for their analysis. It is a very popular concept in the\n\nbig data world.\n\nAWS Glue Crawlers: Crawlers are primarily used to connect with different data sources, discover the schema, and partition and\n\nstore associated metadata into the data catalogs. Crawlers detect schema changes and version updates, and keep the data catalog in\n\nsync. They also detect if data is partitioned in the underlying tables.\n\nCrawlers have data classifiers written to infer the schemas for several popular data formats such as relational data stores, JSON, and\n\nParquet format. You can also write a custom data classifier for a custom file format (using Grok pattern) that Glue doesn't recognize\n\nand associates it with the crawler. You can write multiple classifiers and, once your data is classified, Glue will ignore subsequent\n\ndata classifiers.\n\nYou can run Glue crawlers on an ad hoc basis or on a particular schedule. Moreover, with Glue crawlers being serverless, you only pay\n\nwhen they are in use.\n\nThe following diagram depicts the complete workflow for AWS Glue:\n\nFigure 4.13: AWS Glue\n\nIn the preceding diagram, we have multiple data sources such as Amazon S3, Redshift, and RDS instances, which are connected by AWS\n\nGlue crawlers to read and populate the AWS Glue data catalogs. Also, you can use Amazon Athena or AWS Redshift Spectrum to access\n\nAWS Glue data catalogs for the purpose of data analysis.\n\nExercise 11: Using AWS Glue to Build a Metadata Repository\n\nLet's look at an example of how AWS Glue automatically identifies data formats and schemas and then builds a metadata repository,\n\nthereby eliminating the need to manually define and maintain schemas. We will use the same chocolate-barratings dataset that\n\nwe used previously for our Glue exercise:\n\n1. Log in to the AWS Management Console and go to AWS Glue service.\n\n2. Go to Crawlers and click on Add crawler to open the Add crawler screen. Let's name the crawler chocolate_ratings:\n\nFigure 4.14: Crawler information\n\n3. Click on Next. Here, you can specify the Amazon S3 path where your dataset is located. We can either use the S3 picker\n\n(highlighted in yellow) for this or just paste in the S3 path:\n\nFigure 4.15: Adding a data store\n\n4. Click on Next. If you have data across multiple S3 buckets or other data sources such as RDS and Redshift, you can add them on\n\nthis screen. We will only go with a single S3 source for this demonstration for now:\n\nFigure 4.16: Adding another data store\n\n5. On the next screen, define an IAM role for the crawler. This role provides the crawler with the required permissions to access the\n\ndifferent data stores. Click on Next:\n\nFigure 4.17: Choosing an IAM role\n\n6. Now, set up the schedule for the crawler. We can either run this crawler on demand or on schedule. If we automatically schedule a\n\ncrawler, it helps us to identify any changes to the underlying data and keeps the data catalog up to date. This automatic update of the\n\ndata catalog is very helpful for datasets that change on frequently. We will run it on demand for now:\n\nFigure 4.18: Creating a schedule for the crawler\n\n7. Here, you can either select an existing database to keep the data catalog or create a new one. We will create a new database called\n\nglue-demo for this demonstration. Also, if you want to add a prefix to all of the tables that have been created by crawler for easy identification, you can add the prefix here. We will skip this:\n\nFigure 4.19: Adding a database\n\n8. Also, as we discussed earlier in this chapter, crawlers can handle changes to the schemas to ensure that table metadata is always in\n\nsync with the underlying data. As you can see in the following screenshot, the default settings allow crawlers to modify the\n\ncatalogue schemas if the underlying data is updated or deleted. We can disable it as well, based on our needs:\n\nFigure 4.20: Configuring the crawler's output\n\n9. Click on Next to review the crawler specifications and click on Finish to create the crawler:\n\nFigure 4.21: Review all of the steps\n\n10. Now that the crawler has been created, let's go ahead and run it. Once the crawler has completed running, you will notice in the\n\nfollowing screenshot that one new table has been added in the data catalog. This table is a metadata representation of data and\n\npoints to the location where the data is physically located:\n\nFigure 4.22: Adding the crawler\n\n11. Go to Tables and click the aws_athena_demo_0606 table to take a look at the schema that has been populated by the crawlers:\n\nFigure 4.23: Editing the table\n\nYou can also change the data type of any column as required here. Also, you can view the partitions associated with the table.\n\nSince our table is defined in the catalog, we can query it using Amazon Redshift Spectrum or Amazon Athena. Both products allow you to\n\nquery data directly from S3. We already looked at how to query it using Amazon Athena earlier in this chapter. The only difference will be\n\nthat the database name will be different this time. Please go ahead and try it yourself.\n\nNow, we have seen how AWS Glue makes it easy to crawl data and maintain metadata information in the data catalog. Although there are\n\nmultiple other ways to populate your catalog with tables such as manually defining the table, importing from an external Hive Metastore,\n\nor running Hive DDL queries to create the catalog, AWS Glue provides an easy to use method to create and maintain data catalogs on\n\na regular basis. This concludes our discussion on AWS Glue.\n\nActivity 5: Building an AWS Glue Catalog for a CSV-Formatted Dataset and Analyzing the Data Using AWS Athena\n\nImagine you are a data analyst. You have been provided with a dataset that contains inventory-to-sales ratios for each month since 1992.\n\nThese ratios can be better explained as follows:\n\nRatio = Number of months of inventory/Sales for a month\n\nConsidering this, a ratio of 3.5 means that the business has an inventory that will cover three and a half months of sales. You have been\n\nasked to quickly review the data. You have to prepare a report to get a count of the months for the last 10 years when the inventories to\n\nsales ratio was < 1.25. For example, if the ratio was low four times in the month of January since 1992, then January 4 should be the\n\nresult.\n\nA CSV formatted dataset called total business-inventories-to-sales ratio has been provided with this book. This dataset is derived from\n\nanother dataset that's available on the Kaggle website (https://www.kaggle.com/census/total-business-inventories-and-sales-data). This\n\ndataset has two columns:\n\nObserved_Data: Date when the observation was made\n\nObserved_Value: Inventories to sales ratios\n\nHere are the steps to complete this activity:\n\n1. Create an AWS Glue crawler and build a catalog for this dataset. Verify that the data types are reflected correctly.\n\n2. Go to AWS Athena and create a new schema and table for the data, which you cataloged in step 1.\n\n3. Once you have the data exposed in Athena, you can start building your reports.\n\n4. Write a query to filter the data, where the inventories to sales ratios (observed_values) was less than 1.25, and group the\n\noutput by month. Then, you will have the reports ready to share.\n\nNote\n\nThe solution for this activity can be found on page 158.\n\nSummary",
      "page_number": 89
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 97-106)",
      "start_page": 97,
      "end_page": 106,
      "detection_method": "topic_boundary",
      "content": "In this chapter, we learned about serverless AWS Athena capabilities, its storage, and its querying concepts. We also discussed different\n\nuse cases for AWS Athena. Later, we learned about AWS Glue and its benefits. We looked at what data catalogs are, their uses, and how to\n\npopulate the AWS Glue Data Catalog. In the end, we leveraged the catalog that we created via AWS Glue in AWS Athena to access the\n\nunderlying data and analyze it.\n\nIn the next chapter, we'll focus on the capabilities of Amazon Athena and AWS Glue.\n\nOceanofPDF.com\n\nReal-Time Data Insights Using Amazon Kinesis\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain the concept of real-time data streams\n\nCreate a Kinesis stream using Amazon Kinesis Data Streams\n\nUse Amazon Kinesis Data Firehose to create a delivery stream\n\nSet up an analytics application and process data using Amazon Kinesis Data Analytics\n\nThis chapter shows you how to unleash the potential of real-time data insights and analytics using Amazon Kinesis. You'll also combine\n\nAmazon Kinesis capabilities with AWS Lambda to create lightweight, serverless architectures.\n\nIntroduction\n\nWe live in a world surrounded by data. Whether you are using a mobile app, playing a game, browsing a social networking website, or\n\nbuying your favorite accessory from an online store, companies have set up different services to collect, store, and analyze high\n\nthroughput information to stay up to date on customer's choices and behaviors. These types of setups, in general, require complex software\n\nand infrastructures that can be expensive to provision and manage.\n\nMany of us have worked on aggregating data from different sources to accomplish reporting requirements, and most of us can attest that\n\nthis whole data crunching process is often very demanding. However, a more painful trend has been that as soon as the results of this data\n\nare found, the information is out of date again. Technology has drastically changed over the last decade, which has resulted in real-time\n\ndata being a necessity to stay relevant for today's businesses. Moreover, real-time data helps organizations improve on operational\n\nefficiency and many other metrics.\n\nWe also need to be aware of the diminishing value of data. As time goes on, the value of old data continues to decrease, which makes\n\nrecent data very valuable; hence, the need for real-time analysis increases even further.\n\nIn this chapter, we'll look at how Amazon Kinesis makes it possible to unleash the potential of real-time data insights and analytics, by\n\noffering capabilities such as Kinesis Video Streams, Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics.\n\nAmazon Kinesis\n\nAmazon Kinesis is a distributed data streaming platform for collecting and storing data streams from hundreds of thousands of producers.\n\nAmazon Kinesis makes it easy to set up high capacity pipes that can be used to collect and analyze your data in real time. You can process\n\nincoming feeds at any scale, enabling you to respond to different use cases, such as customer spending alerts and click stream analysis.\n\nAmazon Kinesis enables you to provide curated feeds to customers on a real-time basis rather than performing batch processing on large,\n\ntext-based log files later on. You can just send each event to Kinesis and have it analyzed right away to find patterns and exceptions, and\n\nkeep an eye on all of your operational details. This will allow you to take decisive action instantly.\n\nBenefits of Amazon Kinesis\n\nLike other AWS serverless services, Amazon Kinesis has several benefits. Most of the benefits have already been discussed in terms of\n\nother services, so I will restrain myself from going into the details. However, here is the list of the benefits of using Amazon Kinesis'\n\nservices:\n\nEasy administration\n\nLow cost\n\nSecurity\n\nPay as you go capabilities\n\nDurability\n\nHigh scalability\n\nChoice of framework\n\nReplayability\n\nContinuous processing\n\nHighly concurrent processing\n\nAmazon Kinesis provides different capabilities, depending on different use cases. We will now look at three of the major (and most\n\nimportant) capabilities in detail.\n\nAmazon Kinesis Data Streams\n\nAmazon Kinesis Data Streams is a managed service that makes it easy for you to collect and process real-time streaming data. Kinesis\n\nData Streams enable you to leverage streaming data to power real-time dashboards so that you can look at critical information about your\n\nbusiness and make quick decisions. The Kinesis Data Stream can scale easily from megabytes to terabytes per hour, and from thousands to\n\nmillions of records per second.\n\nYou can use Kinesis Data Streams in typical scenarios, such as real-time data streaming and analytics, real-time dashboards, and log\n\nanalysis, among many other use cases. You can also use Kinesis Data Streams to bring streaming data as input into other AWS services\n\nsuch as S3, Amazon Redshift, EMR, and AWS Lambda.\n\nHow Kinesis Data Streams Work\n\nKinesis Data Streams are made up of one or more shards. What is a shard? A shard is a uniquely identified sequence of data records in a\n\nstream, providing a fixed unit of capacity. Each shard can ingest data of up to a maximum of 1 MB per second and up to 1,000 records per\n\nsecond, while emitting up to a maximum of 2 MB per second. We can simply increase or decrease the number of shards allocated to your\n\nstream in case of changes in input data. The total capacity of the stream is the sum of the capacities of its shards.\n\nBy default, Kinesis Data Streams keep your data for up to 24 hours, which enables you to replay this data during that window (in case\n\nthat's required). You can also increase this retention period to up to 7 days if there is a need to keep the data for longer periods. However,\n\nyou will incur additional charges for extended windows of data retention.\n\nA producer in Kinesis is any application that puts data into the Kinesis Data Streams, and a consumer consumes data from the data\n\nstream.\n\nThe following diagram illustrates the simple functionality of Kinesis Data Streams. Here, we are capturing real-time streaming events\n\nfrom a data source, such as website logs to Amazon Kinesis Streams, and then providing it as input to another AWS Lambda service for\n\ninterpretation. Then, we are showcasing the results on a PowerBI dashboard, or any other dashboard tool:\n\nFigure 5.1: An image showing the simple functionality of Kinesis data streams\n\nExercise 12: Creating a Sample Kinesis Stream\n\nLet's go to the AWS console and create a sample Kinesis stream, which will then be integrated with Lambda to move the real-time data\n\ninto DynamoDB. Whenever an event is published in the Kinesis stream, it will trigger the associated Lambda function, which will then\n\ndeliver that event to the DynamoDB database.\n\nThe following is a high-level diagram showcasing the data flow of our exercise. There are many real-world scenarios that can be\n\naccomplished using this architecture:\n\nFigure 5.2: An image showing the data flow of the architecture\n\nSuppose you run an e-commerce company and want to contact customers that put items in to their shopping carts but don't buy them. You\n\ncan build a Kinesis stream and redirect your application to send information related to failed orders to that Kinesis stream, which can then\n\nbe processed using Lambda and stored in a DynamoDB database. Now, your customer care team can look into the data to get the\n\ninformation related to failed orders in real time, and then contact the customers.\n\nHere are the steps to perform this exercise:\n\n1. Go to AWS services and search for Kinesis. Once it has been selected, you will be redirected to the Kinesis dashboard. Here, you\n\ncan view the services that have been created for all four different flavors of Amazon Kinesis:\n\nFigure 5.3: A screenshot of the Amazon Kinesis dashboard\n\nNote\n\nOur focus for this exercise is Kinesis Data Streams. We will look at other Kinesis services later in this chapter.\n\n2. Go to Data Streams and click on Create Kinesis stream:\n\nFigure 5.4: A screenshot showing how to create Kinesis streams\n\n3. Provide the name of the Kinesis stream. Let's name it kinesis-to-dynamodb. Also, provide the estimated number of shards\n\nthat you will need to handle the data. As we discussed earlier, read and write capacities are calculated based on the number of\n\nconfigured shards. Since we are creating it for demonstration purposes, let's put its value as 1.\n\nYou will notice that the values against write and read get changed based on the number being provided against the number of\n\nshards. Once you are done, click on Create Kinesis Stream:\n\nFigure 5.5: A screenshot showing the process of naming the Kinesis stream and\n\nestimating the number of shards\n\n4. Once the stream has been created, you will notice the status of the stream as Active. Now, you are ready to use this stream for your\n\nincoming data:\n\nFigure 5.6: A screenshot showing the status of the stream after creation\n\nSo, we have created a Kinesis data stream and we will integrate it now with DynamoDB using an AWS Lambda function.\n\n5. Let's go ahead and create a new table in DynamoDB that will store the data coming from the Kinesis Data Stream. Go to AWS\n\nservices and search for DynamoDB. Then, click on Create table:\n\nFigure 5.7: A screenshot showing how to create a new table\n\n6. Name your table sample-table and specify the createdate column partition key. Click on Create. This will create the\n\nrequired destination table for you:\n\nFigure 5.8: A screenshot showing the creation of the destination table\n\n7. In the AWS Lambda service, write a Lambda function to fetch the records from the Kinesis data stream and store them in\n\nDynamoDB.\n\n8. Click on Create function under Lambda service. Click on Blueprints and search for kinesis-process-record. Click on kinesis-\n\nprocess-record template:\n\nFigure 5.9: A screenshot showing the creation of the function under the Lambda service\n\n9. Give a name to the Lambda function. Create a new role, which will allow Lambda to insert records into the DynamoDB database.\n\nTake a look at the following screenshot to find out which policies you need to attach to the role:\n\nFigure 5.10: A screenshot showing the creation of a new role for the function\n\n10. Provide the required details about the kinesis stream. You can set up the appropriate value of the batch size, depending on the flow\n\nof messages. For now, we will keep the default value. Once you are done, click on create function:\n\nFigure 5.11: A screenshot of setting the appropriate value for the batch size\n\n11. When you create a Lambda function from a blueprint, you need to create the function first, before changing any code.\n\n12. Go to the section function code and replace the nodeJS code with the one provided in the kinesis-lambda-dynamodb-\n\nintegration.js file.\n\nWe are populating two columns in this code. The first one is the createdate column, which was also defined as PK in our DynamoDB table definition earlier, in step 7. We are using the default value for this column. The second column is the ASCII\n\nconversion for the base64 data, which is coming in as a part of the Kinesis data stream. We are storing both values as data in our\n\nDynamoDB table, sample-table. Then, we are using the putItem method of the AWS.DynamoDBclient class to store the data in a DynamoDB table:\n\nFigure 5.12: A screenshot of the code that's used to populate the two columns\n\n13. Go ahead and save the code. To execute it, we need to create a Kinesis test event that will trigger the Lambda function and store the\n\nevent data in the DynamoDB database. Click on Configure test event, provide a name (for example, KinesisTestEvent), and click on Create.\n\n14. Once the test event is created, go ahead and execute the lambda function. Your lambda function should get executed successfully.\n\nExecute it couple of times and you should start seeing data into your table in DynamoDB database.\n\nFigure 5.13: A screenshot showing the execution of the Lambda function that we created\n\nearlier\n\nThis concludes our exercise on Amazon Kinesis data events and their integration with the DynamoDB database, via the AWS Lambda\n\nservice.\n\nAmazon Kinesis Firehose\n\nLet's suppose you're working with stock market data and you want to run minute-by-minute analytics on the market stocks (instead of\n\nwaiting until the end of the day). You will have to create dynamic dashboards such as top performing stocks, and update your investment\n\nmodels as soon as new data arrives.\n\nTraditionally, you could achieve this by building the backend infrastructure, setting up the data collection, and then processing the data.\n\nBut it can be really hard to provision and manage a fleet of servers to buffer and batch the data arriving from thousands of sources\n\nsimultaneously. Imagine that one of those servers goes down or something goes wrong in the data stream; you could actually end up losing\n\ndata.\n\nAmazon Kinesis Firehose makes it easy for you to capture and deliver real-time streaming data reliably to Amazon S3, Amazon Redshift,\n\nor Amazon Elasticsearch Service. Using Amazon Firehose, you can respond to data in near real time, enabling you to deliver powerful\n\ninteractive experiences and new item recommendations, and do real-time alert management for critical applications.\n\nAmazon Firehose scales automatically as volume and throughput varies and it takes care of stream management, including batching,\n\ncompressing, encrypting, and loading the data into different target data stores supported by Amazon Firehose. As with other AWS services,\n\nthere is no minimum fee or setup cost required, so you only pay for the data being sent by you by adjusting streaming data quickly and\n\nautomating administration tasks.\n\nAmazon Firehose allows you to focus on your application and deliver great real-time user experiences rather than being stuck with the\n\nprovisioning and management of a backend setup:",
      "page_number": 97
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 107-116)",
      "start_page": 107,
      "end_page": 116,
      "detection_method": "topic_boundary",
      "content": "Figure 5.14: A diagram showing the functionalities of Amazon Kinesis Data Firehose\n\nExercise 13: Creating a Sample Kinesis Data Firehose Delivery Stream\n\nIn this exercise, we'll go to the AWS console and create a sample Kinesis Data Firehose delivery stream. As part of this exercise, we will\n\ndeliver data to an S3 bucket:\n\n1. On Amazon Kinesis Dashboard, go to Data Firehose and click on Create delivery stream:\n\nFigure 5.15: A screenshot showing how to create a Firehose delivery stream\n\n2. Provide the delivery stream's name. Let's call it Kinesis-firehose_to_S3. Now, there are two options here to specify the\n\nsource of data. The first one is Direct PUT, which you can use as a source if you want to send data directly from applications, such\n\nas IOT, CloudWatch logs, or any other AWS application. The second one is the Kinesis Stream, which you can use if you have data\n\ncoming via a regular Kinesis stream. Let's take Direct PUT as the source for this exercise. We will discuss using a Kinesis stream\n\nas a data source in a later part of this chapter:\n\nFigure 5.16: A screenshot showing how to specify the source of data\n\nClick Next to go to Step 2: Process records.\n\n3. On this page, you can transform the records as required. As we discussed earlier in this chapter, Firehose allows you to do ETL\n\nwith streaming data. To do the transformations, write a Lambda function. Let's skip this option for now:\n\nFigure 5.17: A screenshot showing the options for Record transformation\n\nJust click on Next to move to Step 3: Choose destination.\n\n4. Kinesis Firehose also allows you to convert data formats on the go (for example, Parquet to JSON). You can write a Lambda\n\nfunction to easily achieve this. Let's skip this option for now, and click on Next to move to Step 4: Configure settings.\n\n5. On this page, you need to select the destination of your streaming data. As we discussed earlier, you can send your data to different\n\ndestinations, such as S3, Redshift, or the Elasticsearch service. For this demo, we will choose Amazon S3 as the destination.\n\nSpecify the S3 bucket details, such as where you want to save the data. Here, you can specify an existing bucket or create a new\n\none. Leave the prefix blank. Once you are done, click on Next to move to Step 4: Configure settings:\n\nFigure 5.18: A screenshot showing how to create a new bucket or provide details about\n\nan existing one\n\n6. Here, you can configure the buffer conditions, encryption, and compression settings. Buffer settings enable Firehose to buffer the\n\nrecords before they get delivered to S3. Let's set the buffer size as 1 MB and the buffer interval as 60 seconds. When either of these\n\ntwo conditions are met, the records will be moved to the destination.\n\nNote that you can specify the buffer interval to be between 60 and 900 seconds:\n\nFigure 5.19: A screenshot showing the configuration of the buffer conditions,\n\nencryption, and compression settings\n\nLet's keep encryption, compression, and error logging disabled, for now.\n\n7. Also, you need to specify the role that will be used to deliver the data to S3. We will go ahead and create a new role now:\n\nFigure 5.20: A screenshot showing how to specify the role that will be used to deliver the\n\ndata to S3\n\n8. At this point, we need to create a new role, so we will open a separate AWS window and search for Roles. Click on Create role.\n\nWe will go back to proceed from step 6 once the role has been created (step 12):\n\nFigure 5.21: A screenshot showing the creation of a new role\n\n9. Select AWS service under trusted entity and choose Kinesis from the list of services that will use this role. Once you select\n\nKinesis, Kinesis Firehose will appear as the possible use case. Click on Permissions:\n\nFigure 5.22: A screenshot showing the selection of the trusted entity type and service for\n\nthe role\n\n10. Attach the Permission policy now. Search for S3 and attach the AmazonS3FullAccess policy with the role, and click on Review:\n\nFigure 5.23: A screenshot showing the attachment of the permission policy\n\n11. Click on Review. Provide a name for the role, and click on Create role:\n\nFigure 5.24: A screenshot showing how to add the role name and description\n\n12. Now, the role has been created, so let's put in the required information on the screen from step 6:\n\nFigure 5.25: A screenshot showing the fulfillment of details like the IAM Role and\n\npolicy name\n\n13. Click on Review to verify the settings for Kinesis Firehose:\n\nFigure 5.26: A screenshot showing the verification of settings for Kinesis Firehose\n\n14. Click on Create delivery stream, and your Firehose delivery stream should be created successfully:\n\nFigure 5.27: A screenshot showing the successful creation of the delivery stream\n\n15. Let's try to ingest some sample data into our delivery stream and verify whether it reaches the destination.\n\nClick on the delivery stream to go to the details page for that stream. Under Test with demo data, click on Start sending demo\n\ndata. This will start ingesting test data into the Firehose delivery stream:",
      "page_number": 107
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 117-125)",
      "start_page": 117,
      "end_page": 125,
      "detection_method": "topic_boundary",
      "content": "Figure 5.28: A screenshot showing the details of a particular stream\n\n16. Once data ingestion has started, you should see the following message:\n\nFigure 5.29: A screenshot showing the confirmation about demo data being sent to\n\nthe delivery stream\n\nYou will have to wait for a few seconds (20 seconds) for the data to be ingested. Once data ingestion is done, you can click on Stop\n\nsending demo data.\n\n17. Now, it is time to verify whether the data has been delivered successfully to S3 or not. Go to the S3 location that we configured\n\nearlier to receive the data, and you should see the data there:\n\nFigure 5.30: A screenshot showing the data has been successfully delivered\n\nNote that there might be some delay for data to appear in S3, depending on your buffer settings.\n\nThis concludes our demo of Amazon Kinesis Firehose delivery streams.\n\nActivity 6: Performing Data Transformations for Incoming Data\n\nIn the last exercise, we worked on a Kinesis Firehose demo that was integrated with Lambda to move real-time data into S3. You may\n\nhave noticed a Lambda function in the architectural diagram, but we didn't use it in our exercise. There was a data transformation section\n\n(step 3) in the last exercise that we kept disabled.\n\nNow, as part of this activity, we will perform data transformation for incoming data (from Firehose) by using a Lambda function, and then\n\nstore that transformed data in the S3 bucket. With data transformation, we can solve many real-world business problems. We are going to\n\ncreate a Kinesis Firehose data stream, transform the data using a Lambda function, and then finally store it in S3. The following are some\n\nexamples of this:\n\nData format conversion, such as from JSON to CSV, or vice versa\n\nAdding identifiers\n\nData Curation and Filtering\n\nData enhancements, like the addition of date or time\n\nHere are the steps to perform this activity:\n\n1. Start by creating a Kinesis Firehose data stream, and follow the steps that we followed in the last exercise.\n\n2. We disabled data transformation using Lambda in the last exercise. This time, enable the Transform source records with AWS\n\nLambda option.\n\n3. Once it has been enabled, create a Lambda function to do the data transformation on incoming data.\n\n4. There are already some sample functions provided by Amazon. So, for the sake of simplicity, pick one of them, as well. Try out\n\nGeneral Firehose Processing. You can read more about it on the AWS website, if required.\n\n5. Once the Lambda function has been created, ensure that it has the required privileges.\n\n6. Keep the rest of the settings as is.\n\n7. Now, configure an Amazon S3 bucket as the Firehose destination, like we did in the ast exercise.\n\n8. Send the test data from the Test with demo data section by clicking on Start sending demo data:\n\nFigure 5.31: The Test with demo data window\n\n9. Go to the S3 location that was configured earlier to receive the data, and you should see the data file. Upon downloading this data\n\nfile and opening it with Notepad, you should see the data in the CSV format, as shown here:\n\nFigure 5.32: Screenshot showing data in the CSV format\n\nNote\n\nThe solution for this activity can be found on page 161.\n\nAmazon Kinesis Data Analytics\n\nYou are now able to consume real-time streaming data using Amazon Kinesis and Kinesis Firehose, and move it to a particular\n\ndestination. How can you make this incoming data useful for your analysis? How can you make it possible to analyze the data in real time\n\nand perform actionable insights?\n\nAmazon Kinesis Data Analytics is a fully managed service that allows you to interact with real-time streaming data, using SQL. This can\n\nbe used to run standard queries, so that we can analyze the data and send processed information to different business intelligence tools and\n\nvisualize it.\n\nA common use case for the Kinesis Data Analytics application is time series analytics, which refers to extracting meaningful information\n\nfrom data, using time as a key factor. This type of information is useful in many scenarios, such as when you want to continuously check\n\nthe top performing stocks every minute and send that information to your data warehouse to feed your live dashboard, or calculate the\n\nnumber of customers visiting your website every ten minutes and send that data to S3. These time windows of 1 minute and 10 minutes,\n\nrespectively, move forward in time continuously as new data arrives, thus computing new results.\n\nDifferent kinds of time intervals are used, depending on different use cases. Common types of time intervals include sliding and tumbling\n\nwindows. Sharing different windows intervals is out of the scope of this book, but the students are encouraged to look online for more\n\ninformation.\n\nThe following diagram illustrates a sample workflow for Amazon Kinesis Analytics:\n\nFigure 5.33: An image showing the workflow of Amazon Kinesis Analytics\n\nYou can configure the Amazon Kinesis Data Analytics application to run your queries continuously. As with other serverless AWS\n\nservices, you only pay for the resources that your queries consume with Amazon Kinesis Data Analytics. There is no upfront investment or\n\nsetup fee.\n\nExercise 14: Setting Up an Amazon Kinesis Analytics Application\n\nIn the AWS console, set up the Amazon Kinesis Analytics application. We will also look at the interactive SQL editor, which allows you to\n\neasily develop and test real-time streaming analytics using SQL, and also provides SQL templates that you can use to easily implement\n\nthis functionality by simply adding SQL from the templates.\n\nUsing a demo stream of stock exchange data that comes with Amazon Kinesis Analytics, we will count the number of trades for each\n\nstock ticker and generate a periodic report every few seconds. You will notice that the report is progressing through time, generating the\n\ntime series analytics where the latest results are emitted every few seconds, based on the chosen time window for this periodic report.\n\nThe steps are as follows:\n\n1. Go to the Data Analytics tab in the Amazon Kinesis dashboard and click on the Create application button to open the Create\n\napplication form. Provide the application's name. Let's call it kinesis-data-analytics, and click on Create application.\n\nYou can leave the Description blank:\n\nFigure 5.34: A screenshot showing the creation of the Kinesis Analytics application\n\n2. Once the data analytics application has been created successfully, you should get the following message on the screen:\n\nFigure 5.35: A screenshot showing the success message, stating that the Kinesis\n\nAnalytics application was created successfully\n\n3. Now, you need to connect this application with the source of the streaming data so that our analytics application starts getting data.\n\nClick on Connect Streaming data.\n\n4. You can choose either an existing Kinesis stream or a Kinesis Firehose delivery stream. Alternatively, you can configure a new\n\nstream as well. We will configure a new stream here, so let's select Configure a new stream:\n\nFigure 5.36: A screenshot showing how to connect the application with the streaming\n\ndata source\n\n5. Click on Create a demo stream. This will create a new stream and populate it with sample stock ticker data:\n\nFigure 5.37: A screenshot showing the creation of the demo stream\n\n6. As you can see in the following screenshot, new demo stream creation involves the following steps:\n\nCreating an IAM role, creating and setting up a new Kinesis stream, populating the new stream with data, and finally, auto-\n\ndiscovering the schema and date formats:\n\nFigure 5.38: A screenshot showing the status of different processes while the demo\n\nstream is being created\n\n7. Once the setup for the demo stream is complete, it gets selected as a source for the Kinesis data stream. The name of the stream in\n\nthis example is SOURCE_SQL_STREAM_001. It takes you back to choosing the streaming data source, with the newly created\n\nstream selected:\n\nFigure 5.39: A screenshot displaying the name of the created stream, and its details\n\n8. Also, you will notice the sample of the data being generated by the Kinesis data stream. Please note that this schema has been auto-\n\ndiscovered by the Kinesis data analytics application. If you see any issues with the sample data or want to fix it, you can edit it or\n\nretry schema discovery.\n\nWe will keep the other options disabled for now and move on:",
      "page_number": 117
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 126-137)",
      "start_page": 126,
      "end_page": 137,
      "detection_method": "topic_boundary",
      "content": "Figure 5.40: A screenshot displaying a sample of the data generated by the stream\n\n9. Click on Save and continue and you should be redirected to the Kinesis data analytics application page. Now, the Kinesis data\n\nstream setup has been completed, and we can start configuring other settings for our data analytics application:\n\nNote\n\nYou have the option to connect reference data with the real-time streaming data. Reference data can be any of your static data or\n\noutput from other analytics, which can enrich data analytics. It can be either in JSON or CSV data format, and each data analytics\n\napplication can be attached with only one piece of reference data. We will not attach any reference data for now.\n\nFigure 5.41: A screenshot displaying the READY status of the Kinesis Data Analytics\n\napplication\n\n10. Now, we will go ahead and set up real-time analytics. This will enable us to write SQL queries or use an SQL from many templates\n\nthat are available with it. Click on Go to SQL editor under Real time analytics.\n\nClick on Yes, start application in the pop-up window:\n\nFigure 5.42: A screenshot showing the dialog box to start an application\n\nNow, we are in the SQL editor. Here, we can see the sample data from earlier that we configured in the Kinesis Data Stream. We\n\nwill also notice a SQL editor, where we can write SQL queries.\n\n11. You can also add SQL from templates. For our demo, we will pick on SQL from the template and fetch the real-time results:\n\nFigure 5.43: A screenshot showing the SQL editor used for writing SQL queries\n\n12. Click on Add SQL from templates and choose the second query from the left, which aggregates data in a tumbling time\n\nwindow.\n\nYou will see the SQL query on the right-hand side. Click on Add this query to the editor:\n\nFigure 5.44: A screenshot showing the list of SQL queries that are generated when the\n\nAggregate function in a tumbling time window is selected\n\n13. If you see any issue with the sample data, you can click on Actions to take the appropriate step:\n\nFigure 5.45: A screenshot showing a list of different actions that can be used in case of\n\nissues with sample data\n\n14. Once your query appears in the SQL editor, click on Save and run SQL:\n\nFigure 5.46: A screenshot showing the options to save and run SQL\n\n15. Once SQL is executed against the stream data, you will start to see results, as shown in the following screenshot:\n\nFigure 5.47: A screenshot showing real-time analytics once SQL is executed\n\n16. Now, the Kinesis data analytics application is running this SQL against live streaming data every 10 seconds because that is the\n\nwindow that's specified in the SQL query. You will notice a change in the results in the following screenshot as compared to our last\n\nscreenshot. This is because the results were refreshed while the screenshots were being taken:\n\nFigure 5.48: A screenshot showing a list of data that changes every 10 seconds\n\nSo, you have accomplished the task of querying the streaming data in real time, using simple standard SQL statements.\n\n17. Next, configure the destination of your real-time analysis. You can send this analysis to a Kinesis stream or a Kinesis Firehose\n\ndelivery stream, and publish it on your BI dashboards. Alternatively, you can store them in Redshift or DynamoDB using the\n\nFirehose delivery stream. Go to the Destination tab and click on Connect to a destination:\n\nFigure 5.49: A screenshot showing the Destination tab, where an application can be\n\nconnected to any stream\n\nAfter clicking on Destination, you should see the following screenshot:\n\nFigure 5.50: A screenshot showing the different suggested destinations once the\n\nDestination tab has been selected\n\n18. Choose an existing Kinesis stream, and choose DESTINATION_SQL_STREAM for the in-application stream name; click on\n\nSave and continue.\n\nNow, you have completed the setup for the Kinesis data analytics application.\n\n19. You can review the settings for Source, Real-time analytics, and Destination on the application dashboard, as shown in the\n\nfollowing screenshot. Note that at this point, your data analytics application is running real-time analytics using SQL statements on\n\nreal-time data ingestion, which is happening via a Kinesis stream, and sending the query output to another Kinesis stream:\n\nFigure 5.51: A screenshot showing the settings for the source, real-time analytics, and\n\ndestination for the application\n\n20. Once you have collected the required information, you can click on Actions to stop the data analytics application (and later, to start\n\nit again, as required):\n\nFigure 5.52: A screenshot showing the status of the application once we have stopped\n\nrunning it\n\nThis concludes our exercise on the Kinesis Data Analytics application.\n\nIn our last exercise, we created a Kinesis Data Analytics stream, where we could analyze data in real time. This is very useful when you\n\nwant to understand the impact of certain data changes in real time, and make decisions for further changes. It has many real-word\n\napplications as well, such as in dynamic pricing on e-commerce websites, where you want to adjust the pricing based on the product\n\ndemand in real time.\n\nSometimes, there can be a requirement to join this real-time analysis with some reference data to create patterns within the data.\n\nAlternatively, you may just want to further enhance your real-time data with some static information to make better sense of your data.\n\nActivity 7: Adding Reference Data to the Application and Creating an Output, and Joining Real-Time Data with the Reference Data\n\nEarlier in this chapter, we saw that the Kinesis Data Analytics application provides capabilities to add reference data into existing real-\n\ntime data. In the next activity, we will enhance our test stock ticker data (that was produced natively by Kinesis Data Streams) by joining\n\nit with static data. Currently, our data contains abbreviations for company names, and we will join it with our static dataset to publish full\n\ncompany names in the query output.\n\nNote\n\nThere is a reference data file named ka-reference-data.json, which is provided in the code section. This is a JSON-formatted\n\nsample file. You can use either CSV or JSON as the format of the reference data.\n\nHere are the steps to complete this activity:\n\n1. Make sure that you have Kinesis data analytics in working condition, and that you are able to do real-time analysis, like we\n\naccomplished in the last exercise.\n\n2. Create an S3 bucket and upload the ka-reference-data.json file into the bucket.\n\n3. Go to the Kinesis data analytics application and add the reference data. Provide the bucket, S3 object, and table details, and\n\npopulate the schema using schema discovery.\n\n4. Make sure that the IAM role is configured properly.\n\n5. Now, you should have the real-time streaming data and reference data available in the Kinesis Data Analytics application.\n\n6. Go to the SQL prompt and write the SQL statement to join the real-time streaming data with the reference data and out the company\n\ndetails whose names are provided in the reference file.\n\n7. You should be able to see the output with both the ticker symbol and the company name as an output in real time, and it should get\n\nrefreshed every few minutes.\n\nNote\n\nThe solution for this activity can be found on page 165.\n\nSummary\n\nIn this chapter, we focused on the concept of real-time data streams. We learned about the key concepts and use cases for Amazon Kinesis\n\nData Streams, Amazon Kinesis Data Firehose, and Amazon Kinesis Data Analytics. We also looked at examples of how these real-time\n\ndata streams integrate with each other and help us build real-world use cases.\n\nIn this book, we embarked on an example-driven journey of building serverless applications on AWS, applications that do not require the\n\ndevelopers to provision, scale, or manage any underlying servers. We started with an overview of traditional application deployments and\n\nchallenges associated with it and how those challenges resulted in the evolution of serverless applications. With serverless introduced, we\n\nlooked at the AWS Cloud computing platform, and focused on Lambda, the main building block of serverless models on AWS.\n\nLater, we looked at other capabilities of the AWS serverless platform, such as S3 storage, API Gateway, SNS notifications, SQS queues,\n\nAWS Glue, AWS Athena, and Kinesis applications. Using an event-driven approach, we studied the main benefits of having a serverless\n\narchitecture, and how it can be leveraged to build enterprise-level solutions. Hopefully, you have enjoyed this book and are ready to create\n\nand run your serverless applications, which will take advantage of the high availability, security, performance, and scalability of AWS. So,\n\nfocus on your product instead of worrying about managing and operating the servers to run it.\n\nOceanofPDF.com\n\nAppendix\n\nAbout\n\nThis section is included to assist the students to perform the activities in the book. It includes detailed steps that are to be performed by the\n\nstudents to achieve the objectives of the activities.",
      "page_number": 126
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 138-145)",
      "start_page": 138,
      "end_page": 145,
      "detection_method": "topic_boundary",
      "content": "Chapter 1: AWS, Lambda, and Serverless Applications\n\nSolution for Activity 1: Creating a New Lambda Function That Finds the Square Root of the Average of Two Input Numbers\n\n1. Click on Create a function to create your first Lambda function on the AWS Lambda page.\n\n2. On the Create function page, select Author from scratch.\n\n3. In the Author from scratch window, fill in the following details:\n\nName: Enter myFirstLambdaFunction.\n\nRuntime: Choose Node.js 6.10. The Runtime window dropdown shows the list of languages that are supported by AWS Lambda\n\nand you can author your Lambda function code in any of the listed options. For this activity, we will author our code in Node.js.\n\nRole: Choose Create new role from template(s). In this section, you specify an IAM role.\n\nRole name: Enter lambda_basic_execution.\n\nPolicy templates: Select Simple Microservice permissions.\n\n4. Now click on Create function.\n\n5. Go to the Function code section.\n\n6. Use the Edit code inline option, and enter this code:\n\nexports.handler = (event, context, callback) => {\n\n// TODO\n\nlet first_num = 10;\n\nlet second_num = 40;\n\nlet avgNumber = (first_num+second_num)/2\n\nlet sqrtNum = Math.sqrt(avgNumber)\n\ncallback(null, sqrtNum);\n\n};\n\n7. Click on the dropdown next to Select a test event in the top-right corner of the screen and select Configure test event.\n\n8. When the popup appears, click on Create new test event and give it a name. Click on Create and the test event gets created.\n\n9. Click on the Test button next to test events and you should see the following window upon successful execution of the event:\n\nFigure 1.18: Test successful window\n\nSolution for Activity 2: Calculating the Total Lambda Cost\n\n1. Note the monthly compute price and compute time provided by the Free Tier.\n\nThe monthly compute price is $0.00001667 per GB-s and the Free Tier provides 400,000 GB-s.\n\n2. Calculate the total compute time in seconds.\n\nTotal compute (seconds) = 20M * (1s) = 20,000,000 seconds\n\n3. Calculate the total compute time in GB-s.\n\nTotal compute (GB-s) = 20,000,000 * 512MB/1024 = 10,000,000 GB-s\n\n4. Calculate the monthly billable compute in GB- s. Here's the formula:\n\nMonthly billable compute (GB- s) = Total compute – Free tier compute\n\n= 10,00,000 GB-s – 400,000 Free Tier GB-s\n\n= 9,600,000 GB-s\n\n5. Calculate the monthly compute charges in dollars. Here's the formula:\n\nMonthly compute charges = Monthly billable compute (GB-s) * Monthly compute price\n\n= 9,600,000 * $0.00001667\n\n= $160.02\n\n6. Calculate the monthly billable requests. Here's the formula:\n\nMonthly billable requests = Total requests – Free tier requests\n\n= 20M requests – 1M Free Tier requests\n\n= 19M Monthly billable requests\n\n7. Calculate the monthly request charges. Here's the formula:\n\nMonthly request charges = Monthly billable requests * Monthly request price\n\n= 19M * $0.2/M\n\n= $3.8\n\n8. Calculate the total cost. Here's the formula:\n\nTotal cost = Monthly compute charge + Monthly request charges\n\n= $160.02 + $3.8\n\n= $163.82\n\nChapter 2: Working with the AWS Serverless Platform\n\nSolution for Activity 3: Setting up a Mechanism to Get an Email Alert When An Object is Uploaded into an S3 Bucket\n\n1. Go to the AWS S3 service and click on Create bucket.\n\n2. Provide a Bucket name and select the region where the S3 bucket will be created. Click on Next. Note that the bucket name can't be\n\nduplicated.\n\n3. If you want to change any configuration, you can do it here. Click on Next.\n\n4. Now, you can change the settings related to the security of the S3 bucket. If you want to allow public access to the S3 bucket, you\n\ncan uncheck the options here. Click on Next.\n\n5. Review all of the configuration settings. If you want to change anything, you can go back. Alternatively, click on Finish and your\n\nbucket should be created successfully.\n\n6. Go to the Lambda function that we created in the earlier exercise. Add S3 as a trigger under the Lambda configuration section:\n\nFigure 2.54: Lambda configuration window\n\n7. Click on Configuration required and add the required details related to S3 bucket configuration, mainly the bucket name. Keep\n\nthe rest of the settings as default:\n\nFigure 2.55: Configure triggers window\n\n8. Click on Add to add that S3 bucket as a trigger to execute the Lambda function:\n\nFigure 2.56: Window showing S3 bucket being added as a trigger\n\n9. Click on Save to save the changes to the Lambda function:\n\nFigure 2.57: Window showing S3 bucket getting saved\n\n10. Also, the email message will have changed in the Lambda code to reflect our activity. See line # 8 in the following screenshot. You\n\ncan customize it based on your needs:\n\nFigure 2.58: Window showing code of index.js\n\n11. Now, upload a new sample file to the S3 bucket. You should see an email alert in your mailbox.\n\n12. Go back to the Amazon S3 service, click on the bucket name, and click on the Upload button.\n\n13. Click on Add files and select the file that you want to load into the S3 bucket. Click on Next.\n\n14. Set the file level permissions. Click on Next.\n\n15. Select the storage class. You can continue with the default option. Click on Next.\n\n16. Review the configuration and click on Upload.\n\n17. The file should be uploaded successfully:\n\nFigure 2.59: Overview section of Amazon S3\n\n18. Once the file has been uploaded, go to your mailbox and you should see an email alert:\n\nFigure 2.60: Output showing a new object being uploaded to the S3 bucket\n\nThis concludes our activity.\n\nChapter 3: Building and Deploying a Media Application\n\nSolution for Activity 4: Creating an API to Delete the S3 Bucket\n\n1. Go to the AWS API Gateway console and in the API created in this chapter, create a Delete API.\n\n2. Configure the incoming headers and path parameters properly in the Method Request and Integration Request sections.\n\nYour API configuration should look similar to the following screenshot:\n\nFigure 3.37: The DELETE method execution window\n\n3. Remember to change the authorization of the Delete method from NONE to AWS_IAM.\n\n4. Click on the Deploy API.\n\n5. Test the Delete method using the Test Tool (Ready API). Set content-type as application/xml:\n\nFigure 3.38: Output showing the bucket getting deleted\n\nYou should see the bucket getting deleted in the AWS S3 console.\n\nChapter 4: Serverless Amazon Athena and the AWS Glue Data Catalog\n\nSolution for Activity 5: Building a AWS Glue catalog for a CSV- Formatted Dataset and Analyzing the Data Using AWS Athena\n\n1. Log in to your AWS account.\n\n2. Upload the data file total-business-inventories-to-sales-ratio.csv (provided with this book) into a S3 bucket.\n\nMake sure that the required permissions are in place:\n\nFigure 4.24: Uploading the data file\n\n3. Go to the AWS Glue service.\n\n4. Select Crawlers and click on Add Crawler.\n\n5. Provide the crawler name and click on Next.\n\n6. Provide the path of the S3 bucket, where the file was uploaded in step 2. Click on Next.\n\n7. Click on Next, as we don't want to add another data store.\n\n8. Choose an existing IAM role that was created in Exercise 11: Using AWS Glue to Build a Metadata Repository. Alternatively, you\n\ncan create a new one. Click on Next.\n\n9. Let's keep it as Run on demand and click on Next.\n\n10. Either you can create a new database here or click on the dropdown to select an existing one. Click on Next.\n\n11. Review the settings and click on Finish. You have successfully created the crawler.\n\n12. Now, go ahead and run the crawler.\n\n13. Once the run of the crawler is completed, you will see a new table being created under the schema that you chose in step 10:\n\nFigure 4.25: The new table after the crawler run was completed\n\n14. Go to tables, and you should see the newly created table, inventory_sales_ratio. Note that the table name is derived from\n\nthe bucket name.\n\n15. Go to the AWS Athena service. You should see a new table name under the database that was selected in step 10.\n\n16. Click on new query and write the following query to get the expected output:\n\nselect month(try(date_parse(observed_date, '%m/%d/%Y'))) a, count(*) from\n\ninventory_sales_ratio\n\nwhere observed_value < 1.25 group by month(try(date_parse(observed_date, '%m/%d/%Y')))\n\norder by a ;\n\n17. When the query gets executed, you should see the expected output:",
      "page_number": 138
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 146-154)",
      "start_page": 146,
      "end_page": 154,
      "detection_method": "topic_boundary",
      "content": "Figure 4.26: The output after the query has run\n\n18. Looking at the output, we have a total of 8 months since 1992 where the inventories to sales ratios was < 1.25. We also have the\n\nmonth level count as well.\n\nWe have successfully completed the activity.\n\nChapter 5: Real-Time Data Insights Using Amazon Kinesis\n\nSolution for Activity 6: Performing Data Transformations for Incoming Data\n\n1. Start by creating a Kinesis Firehose Data Stream and follow the steps that we completed in the last exercise.\n\n2. We disabled data transformation using Lambda in the last exercise. This time, enable the Transform source records with AWS\n\nLambda option.\n\n3. Once enabled, create a Lambda function to do the data transformation for incoming data:\n\nFigure 5.54: The Transform source records with AWS Lambda window\n\n4. There are already some sample functions that have been provided by Amazon. You can click on Create New and it will open up the\n\nlist of transformation functions provided by AWS. Let's choose General Firehose Processing:\n\nFigure 5.55: The Choose Lambda blueprint window\n\n5. This opens up the Lambda function window. Here, you need to provide the name of the function, along with the IAM role\n\ninformation:\n\nFigure 5.56: The Basic information window\n\n6. Edit the code inline and replace the existing code with the code provided in the json2csv_transform.js file, under the code\n\nsection. Keep the rest of the settings as is:\n\nFigure 5.57: Window showing code of index.js\n\n7. Once the Lambda function has been created, go back to the Firehose screen and configure the rest of the settings, such as the\n\nAmazon S3 bucket, which will work the same as the Firehose destination that we configured in the last exercise:\n\nFigure 5.58: The Convert record format window\n\n8. Also, once the Lambda function has been created, update the IAM role in the Firehose configuration to reflect the required access\n\nfor the Lambda function:\n\nFigure 5.59: The Test with demo data window\n\n9. Everything else remains the same as in the last exercise.\n\n10. Send the test data from the Test with demo data section by clicking on Start sending demo data:\n\nFigure 5.60: Window showing the Start sending demo data button\n\n11. Go to the S3 location that we configured earlier to receive the data and you should see the data file, as shown here:\n\nFigure 5.61: Window showing the data file added successfully\n\n12. Upon downloading this data file and opening it with Notepad, you should see the data in CSV format, as shown here:\n\nFigure 5.62: Screenshot showing data in the CSV format\n\nSolution for Activity 7: Adding Reference Data to the Application and Creating an Output, Joining Real-Time Data with the Reference Data\n\n1. Ensure that you have Kinesis Data Analytics in working condition and that you are able to do real-time analysis, like we\n\naccomplished in the last exercise:\n\nFigure 5.63: The kinesis-data-analytics page\n\n2. Create a S3 bucket and upload the ka-reference-data.json file into the bucket:\n\nFigure 5.64: Screenshot showing the ka-reference-data.json file added to the S3 bucket\n\n3. Go to the Kinesis Data Analytics application page and click on Connect reference data. Provide the bucket, S3 object, and table\n\ndetails, and populate the schema using schema discovery:\n\nFigure 5.65: The Connect reference data source page\n\nYou will notice in the preceding screenshot that the Kinesis application will create the IAM role with required access.\n\nSchema discovery will detect the schema for the reference data file and show you the sample data:\n\nFigure 5.66: The Schema section\n\n4. Click on Save and close button. You will have successfully added the referenced data:\n\nFigure 5.67: Page showing the referenced data added successfully\n\nNow, you should have the real-time streaming data and reference data available in the Kinesis Data Analytics application. The\n\nfollowing screenshot is showing real-time streaming data: The following image is showing the added reference data:\n\nFigure 5.68: The Real-time analytics section\n\nFigure 5.69: The Source data section\n\n5. Go to the SQL prompt and write the SQL statement to join real-time streaming data with the reference data, and out the company\n\ndetails whose names are provided in the reference file.\n\n6. Run the following query in the SQL prompt. In this query, we are joining (left join) SOURCE_SQL_STREAM_001 with the\n\nka_reference_data dataset and filtering where company name is not null:\n\nCREATE STREAM \"KINESIS_SQL_STREAM\" (ticker_symbol VARCHAR(14), \"Company_Name\"\n\nvarchar(30), sector VARCHAR(22), change DOUBLE, price DOUBLE);\n\nCREATE PUMP \"STREAM_PUMP\" AS INSERT INTO \"KINESIS_SQL_STREAM\"\n\nSELECT STREAM ticker_symbol, \"kar\".\"Company\", sector, change, price\n\nFROM \"SOURCE_SQL_STREAM_001\" LEFT JOIN \"ka_reference_data\" as \"kar\"\n\nON \"SOURCE_SQL_STREAM_001\".ticker_symbol = \"kar\".\"Ticker\"\n\nwhere \"kar\".\"Company\" is not null ;\n\nNote\n\nYou can use the inner join while removing the where clause to achieve the same results.",
      "page_number": 146
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 155-156)",
      "start_page": 155,
      "end_page": 156,
      "detection_method": "topic_boundary",
      "content": "Figure 5.70: The result page for real-time analytics\n\n7. You should be able to see the output with both the ticker symbol and company name as output in real-time. It should get refreshed\n\nevery few minutes:\n\nFigure 5.71: Output showing both the ticker symbol and company name\n\nThis concludes our activity on adding reference data and using it to perform real-time data analytics on Amazon Kinesis Data Analytics.\n\nOceanofPDF.com",
      "page_number": 155
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Serverless Architectures with AWS\n\nCopyright © 2018 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any\n\nmeans, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or\n\nreviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the\n\ninformation contained in this book is sold without warranty, either express or implied. Neither the author, nor Packt Publishing,\n\nand its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this\n\nbook.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this\n\nbook by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n\nAuthor: Mohit Gupta\n\nReviewer: Amandeep Singh\n\nManaging Editor: Edwin Moses\n\nAcquisitions Editor: Aditya Date\n\nProduction Editor: Nitesh Thakur\n\nEditorial Board: David Barnes, Ewan Buckingham, Simon Cox, Manasa Kumar, Alex Mazonowicz, Douglas Paterson, Dominic\n\nPereira, Shiny Poojary, Saman Siddiqui, Erol Staveley, Ankita Thakur, and Mohita Vyas.\n\nFirst Published: December 2018\n\nProduction Reference: 1211218\n\nPublished by Packt Publishing Ltd.\n\nLivery Place, 35 Livery Street\n\nBirmingham B3 2PB, UK\n\nISBN 978-1-78980-502-4\n\nTable of Contents\n\nPreface\n\nAWS, Lambda, and Serverless Applications\n\nIntroduction",
      "content_length": 1593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "The Serverless Model\n\nBenefits of the Serverless Model\n\nIntroduction to AWS\n\nAWS Serverless Ecosystem\n\nAWS Lambda\n\nSupported Languages\n\nExercise 1: Running Your First Lambda Function\n\nActivity 1: Creating a New Lambda Function that Finds the Square Root of the Average of Two Input Numbers\n\nLimits of AWS Lambda\n\nAWS Lambda Pricing Overview\n\nLambda Free Tier\n\nActivity 2: Calculating the Total Lambda Cost\n\nAdditional Costs\n\nSummary\n\nWorking with the AWS Serverless Platform\n\nIntroduction\n\nAmazon S3\n\nKey Characteristics of Amazon S3\n\nDeploying a Static Website",
      "content_length": 561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Exercise 2: Setting up a Static Website with an S3 Bucket Using a Domain Name in Route 53\n\nEnabling Versioning on S3 Bucket\n\nS3 and Lambda Integration\n\nExercise 3: Writing a Lambda Function to Read a Text File\n\nAPI Gateway\n\nWhat is API Gateway?\n\nAPI Gateway Concepts\n\nExercise 4: Creating a REST API and Integrating It with Lambda\n\nOther Native Services\n\nAmazon SNS\n\nAmazon SQS\n\nDynamoDB\n\nDynamoDB Streams\n\nDynamoDB Streams Integration with Lambda\n\nExercise 5: Creating an SNS topic and Subscribing to It\n\nExercise 6: SNS Integration with S3 and Lambda\n\nActivity 3: Setting Up a Mechanism to Get an Email Alert When an Object Is Uploaded into an S3 Bucket\n\nSummary",
      "content_length": 664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Building and Deploying a Media Application\n\nIntroduction\n\nDesigning a Media Web Application – from Traditional to Serverless\n\nBuilding a Simple Serverless Media Web Application\n\nExercise 7: Building the Role to Use with an API\n\nExercise 8: Creating an API to Push to / Get from an S3 Bucket\n\nExercise 9: Building the Image Processing System\n\nDeployment Options in the Serverless Architecture\n\nActivity 4: Creating an API to Delete the S3 Bucket\n\nSummary\n\nServerless Amazon Athena and the AWS Glue Data Catalog\n\nIntroduction\n\nAmazon Athena\n\nDatabases and Tables\n\nExercise 10: Creating a New Database and Table Using Amazon Athena\n\nAWS Glue\n\nExercise 11: Using AWS Glue to Build a Metadata Repository\n\nActivity 5: Building an AWS Glue Catalog for a CSV-Formatted Dataset and Analyzing the Data Using AWS Athena",
      "content_length": 808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Summary\n\nReal-Time Data Insights Using Amazon Kinesis\n\nIntroduction\n\nAmazon Kinesis\n\nBenefits of Amazon Kinesis\n\nAmazon Kinesis Data Streams\n\nHow Kinesis Data Streams Work\n\nExercise 12: Creating a Sample Kinesis Stream\n\nAmazon Kinesis Firehose\n\nExercise 13: Creating a Sample Kinesis Data Firehose Delivery Stream\n\nActivity 6: Performing Data Transformations for Incoming Data\n\nAmazon Kinesis Data Analytics\n\nExercise 14: Setting Up an Amazon Kinesis Analytics Application\n\nActivity 7: Adding Reference Data to the Application and Creating an Output, and Joining Real-Time Data with the Reference Data\n\nSummary\n\nAppendix\n\nTo my children, Aarya and Naisha.\n\nOceanofPDF.com",
      "content_length": 671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Preface\n\nAbout\n\nThis section briefly introduces the author and reviewer, the coverage of this book, the technical skills you'll need to get started,\n\nand the hardware and software required to complete all of the included activities and exercises.\n\nAbout the Book\n\nServerless Architectures with AWS begins with an introduction to the serverless model and helps you get started with AWS and\n\nAWS Lambda. You'll also get to grips with other capabilities of the AWS serverless platform and see how AWS supports\n\nenterprise-grade serverless applications with and without Lambda.\n\nThis book will guide you through deploying your first serverless project and exploring the capabilities of Amazon Athena, an\n\ninteractive query service that makes it easy to analyze data in Amazon Simple Storage Service (Amazon S3) using standard SQL.\n\nYou'll also learn about AWS Glue, a fully managed extract, transfer, and load (ETL) service that makes categorizing data easy and\n\ncost-effective. You'll study how Amazon Kinesis makes it possible to unleash the potential of real-time data insights and analytics\n\nwith capabilities such as Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics. Last but not least, you'll be\n\nequipped to combine Amazon Kinesis capabilities with AWS Lambda to create lightweight serverless architectures.\n\nBy the end of the book, you will be ready to create and run your first serverless application that takes advantage of the high\n\navailability, security, performance, and scalability of AWS.\n\nAbout the Author and Reviewer\n\nMohit Gupta is a solutions architect, focused on cloud technologies and Big Data analytics. He has more than 12 years of\n\nexperience in IT and has worked on AWS and Azure technologies since 2012. He has helped customers design, build, migrate, and\n\nmanage their workloads and applications on various cloud-based products, including AWS and Azure. He received his B.Tech in\n\nComputer Science from Kurukshetra University in 2005. Additionally, he holds many industry-leading IT certifications. You can\n\nreach him on LinkedIn at mogupta84 or follow his twitter handle @mogupta.\n\nAmandeep Singh works as a distinguished Engineer with Pitney Bowes India Pvt Ltd. He has extensive development experience\n\nof more than 13 years in product companies like Pitney Bowes and Dell R&D center. His current role involves designing cloud\n\nbased distributed solutions at enterprise scale. He is a AWS certified Solutions Architect, and helps Pitney Bowes migrate large\n\nmonolith platform to AWS Cloud in the form of simpler and smarter microservices. He is strong believer of new age DevOps\n\nprinciples and microservices patterns. He can be reached on LinkedIn at bhatiaamandeep.\n\nObjectives\n\nExplore AWS services for supporting a serverless environment\n\nSet up AWS services to make applications scalable and highly available\n\nDeploy a static website with a serverless architecture\n\nBuild your first serverless web application",
      "content_length": 2966,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Study the changes in a deployed serverless web application\n\nApply best practices to ensure overall security, availability, and reliability\n\nAudience\n\nServerless Architectures with AWS is for you if you want to develop serverless applications and have some prior coding\n\nexperience. Though no prior experience of AWS is needed, basic knowledge of Java or Node.js will be an advantage.\n\nApproach\n\nServerless Architectures with AWS takes a hands-on approach to learning how to design and deploy serverless architectures. It\n\ncontains multiple activities that use real-life business scenarios for you to practice and apply your new skills in a highly relevant\n\ncontext.\n\nHardware Requirements\n\nFor an optimal student experience, we recommend the following hardware configuration:\n\nProcessor: Intel Core i5 or equivalent\n\nMemory: 4 GB RAM\n\nStorage: 35 GB available space\n\nSoftware Requirements\n\nYou'll also need the following software installed in advance:\n\nOperating system: Windows 7 or above\n\nAWS Free Tier account\n\nNetwork access on ports 22 and 80\n\nConventions\n\nCode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and\n\nTwitter handles are shown as follows: \"You can also copy this code from the s3_with_lambda.js file.\"\n\nA block of code is set as follows:\n\nvar AWS = require('aws-sdk');\n\nvar s3 = new AWS.S3();\n\nNew terms and important words are shown in bold. Words that you see on the screen, for example, in menus or dialog boxes,\n\nappear in the text like this: \"Click on Next and follow the instructions to create the bucket.\"\n\nAdditional Resources",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "The code bundle for this book is also hosted on GitHub at https://github.com/TrainingByPackt/Serverless-Architectures-with-\n\nAWS.\n\nWe also have other code bundles from our rich catalog of books and videos available at https://github.com/PacktPublishing/.\n\nCheck them out!\n\nOceanofPDF.com",
      "content_length": 287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "AWS, Lambda, and Serverless Applications\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain the serverless model\n\nDescribe the different AWS serverless services present in the AWS ecosystem\n\nCreate and execute an AWS Lambda function\n\nThis chapter teaches you the basics of serverless architectures, focusing on AWS.\n\nIntroduction\n\nImagine that a critical application in your company is having performance issues. This application is available to customers 24-7, and\n\nduring business hours, the CPU and memory utilization reaches 100%. This is resulting in an increased response time for customers.\n\nAround 10 years ago, a good migration plan to solve this issue would involve the procurement and deployment of new hardware resources\n\nfor both the application and its databases, the installation of all required software and application code, performing all functional and\n\nperformance quality analysis work, and finally, migrating the application. The cost of this would run into millions. However, nowadays,\n\nthis issue can be resolved with new technologies that offer different approaches to customers – going Serverless is definitely one of them.\n\nIn this chapter, we'll start with an explanation of the serverless model, and get started with AWS and Lambda, the building blocks of a\n\nserverless applications on AWS. Finally, you'll learn how to create and run Lambda functions.\n\nThe Serverless Model\n\nTo understand the serverless model, let's first understand how we build traditional applications such as mobile applications and web\n\napplications. Figure 1.1 shows a traditional on-premises architecture, where you would take care of every layer of application\n\ndevelopment and the deployment process, starting with setting up hardware, software installation, setting up a database, networking,\n\nmiddleware configuration, and storage setup. Moreover, you would need a staff of engineers to set up and maintain this kind of on-\n\npremises setup, making it very time-consuming and costly. Moreover, the life cycle of these servers was no longer than 5-6 years, which\n\nmeant that you would end up upgrading your infrastructure every few years.\n\nThe work wouldn't end there, as you would have to perform regular server maintenance, including setting up server reboot cycles and\n\nrunning regular patch updates. And despite doing all the groundwork and making sure that the system ran fine, the system would actually\n\nfail and cause application downtime. The following diagram shows a traditional on-premises architecture:",
      "content_length": 2549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Figure 1.1 : Diagram of traditional on-premises architecture\n\nThe serverless model changes this paradigm completely, as it abstracts all the complexity attached with provisioning and managing data\n\ncenters, servers, and software. Let's understand it in more detail.\n\nThe serverless model refers to applications in which server management and operational tasks are completely hidden from end users, such\n\nas developers. In the serverless model, developers are dedicated specifically to business code and the application itself, and they do not\n\nneed to care about the servers where the application will be executed or run from, or about the performance of those servers, or any\n\nrestrictions on them. The serverless model is scalable and is actually very flexible. With the serverless model, you focus on things that are\n\nmore important to you, which is most probably solving business problems. The serverless model allows you to focus on your application\n\narchitecture without you needing to think about servers.\n\nSometimes, the term \"serverless\" can be confusing. Serverless does not mean that you don't need any servers at all, but that you are not\n\ndoing the work of provisioning servers, managing software, and installing patches. The term \"the serverless model\" just means that it is\n\nsomeone else's servers. Serverless architectures, if implemented properly, can provide great advantages in terms of lowering costs and\n\nproviding operational excellence, thus improving overall productivity. However, you have to be careful when dealing with the challenges\n\nimposed by serverless frameworks. You need to make sure that your application doesn't have issues with performance, resource\n\nbottlenecks, or security.\n\nFigure 1.2 shows the different services that are part of the serverless model. Here, we have different services for doing different kinds of\n\nwork. We have the API Gateway service, a fully managed REST interface, which helps to create, publish, maintain, monitor, and secure\n\nAPIs. Then, we have the AWS Lambda service that executes the application code and does all the computation work. Once computation is\n\ndone, data gets stored in the DynamoDB database, which is again a fully managed service that provides a fast and scalable database\n\nmanagement system. We also have the S3 storage service, where you can store all your data in raw formats that can be used later for data\n\nanalytics. The following diagram talks about the serverless model:",
      "content_length": 2462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Figure 1.2 : The serverless model (using AWS services)\n\nServerless models have become quite popular in recent times, and many big organizations have moved their complete infrastructure to\n\nserverless architectures and have been running them successfully, getting better performance a much a lower cost. Many serverless\n\nframeworks have been designed, making it easier to build, test, and deploy serverless applications. However, our focus in this book will be\n\non serverless solutions built on Amazon Web Services (AWS). Amazon Web Services is a subsidiary of Amazon.com that provides on-\n\ndemand cloud service platforms.\n\nBenefits of the Serverless Model\n\nThere are a number of benefits to using a serverless model:\n\nNo server management: Provisioning and managing servers is a complex task and it can take anything from days to months to\n\nprovision and test new servers before you can start using them. If not done properly, and with a specific timeline, it can become a\n\npotential obstacle for the release of your software onto the market. Serverless models provide great relief here by masking all the\n\nsystem engineering work from the project development team.\n\nHigh availability and fault tolerance: Serverless applications have built-in architecture that supports high availability (HA). So,\n\nyou don't need to worry about the implementation of these capabilities. For example, AWS uses the concept of regions and\n\navailability zones to maintain the high availability of all AWS services. An availability zone is an isolated location inside a region\n\nand you can develop your application in such a way that, if one of the availability zones goes down, your application will continue\n\nto run from another availability zone.\n\nScalability: We all want our applications to be successful, but we need to make sure that we are ready when there is an absolute\n\nneed for scaling. Obviously, we don't want to spawn very big servers in the beginning (since this can escalate costs quickly), but we\n\nwant to do it as and when the need occurs. With serverless models, you can scale your applications very easily. Serverless models\n\nrun under the limits defined by you, so you can easily expand those limits in the future. You can adjust the computing power, the\n\nmemory, or IO needs of your application with just a few clicks, and you can do that within minutes. This will help you to control\n\ncosts as well.\n\nDeveloper productivity: In a serverless model, your serverless vendor takes all the pain of setting up hardware, networking, and\n\ninstalling and managing software. Developers need to focus only on implementing the business logic and don't need to worry about\n\nunderlying system engineering work, resulting in higher developer productivity.",
      "content_length": 2744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "No idle capacity: With serverless models, you don't need to provision computing and storage capacity in advance. And you can\n\nscale up and down based on your application requirements. For example, if you have an e-commerce site, then you might need\n\nhigher capacity during festive seasons than other days. So, you can just scale up resources for that period only.\n\nMoreover, today's serverless models, such as AWS, work on the \"pay as you go\" model, meaning that you don't pay for any capacity\n\nthat you don't use. This way, you don't pay anything when your servers are idle, which helps to control costs.\n\nFaster time to market: With serverless models, you can start building software applications in minutes, as the infrastructure is\n\nready to be used at any time. You can scale up or down underlying hardware in a few clicks. This saves you time with system\n\nengineering work and helps to launch applications much more quickly. This is one of the key factors for companies adopting the\n\nserverless model.\n\nDeploy in minutes: Today's serverless models simplify deployment by doing all the heavy lifting work and eliminating the need for\n\nmanaging any underlying infrastructure. These services follow DevOps practices.\n\nIntroduction to AWS\n\nAWS is a highly available, reliable, and scalable cloud service platform offered by Amazon that provides a broad set of infrastructure\n\nservices. These services are delivered on an \"on-demand\" basis and are available in seconds. AWS was one of the first platforms to offer\n\nthe \"pay-as-you-go\" pricing model, where there is no upfront expense. Rather, payment is made based on the usage of different AWS\n\nservices. The AWS model provides users with compute, storage, and throughput as needed.\n\nThe AWS platform was first conceptualized in 2002 and Simple Queue Service (SQS) was the first AWS service, which was launched in\n\n2004. However, the AWS concept has been reformulated over the years, and the AWS platform was officially relaunched in 2006,\n\ncombining the three initial service offerings of Amazon S3 (Simple Storage Service): cloud storage, SQS, and EC2 (Elastic Compute\n\nCloud). Over the years, AWS has become a platform for virtually every use case. From databases to deployment tools, from directories to\n\ncontent delivery, from networking to compute services, there are currently more than 100 different services available with AWS. More\n\nadvanced features, such as machine learning, encryption, and big data are being developed at a rapid pace. Over the years, the AWS\n\nplatform of products and services has become very popular with top enterprise customers. As per current estimates, over 1 million\n\ncustomers trust AWS for their IT infrastructure needs.\n\nAWS Serverless Ecosystem\n\nWe will take a quick tour of the AWS serverless ecosystem and briefly talk about the different services that are available. These services\n\nwill be discussed in detail in future chapters.\n\nFigure 1.4 shows the AWS serverless ecosystem, which is comprised of eight different AWS services:\n\nLambda: AWS Lambda is a compute service that runs code in response to different events, such as in-app activity, website clicks,\n\nor outputs from connected devices, and automatically manages the compute resources required by the code. Lambda is a core\n\ncomponent of the serverless environment and integrates with different AWS services to do the work that's required.\n\nSimple Storage Service (S3): Amazon S3 is a storage service that you can use to store and retrieve any amount of information, at\n\nany time, from anywhere on the web. AWS S3 is a highly available and fault-tolerant storage service.\n\nSimple Queue Service (SQS): Amazon SQS is a distributed message queuing service that supports message communication\n\nbetween computers over the internet. SQS enables an application to submit a message to a queue, which another application can\n\nthen pick up at a later time.\n\nSimple Notification Service (SNS): Amazon SNS is a notification service that coordinates the delivery of messages to subscribers.\n\nIt works as a publish/subscribe (pub/sub) form of asynchronous communication.",
      "content_length": 4113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "DynamoDB: Amazon DynamoDB is a NoSQL database service.\n\nAmazon Kinesis: Amazon Kinesis is a real-time, fully managed, and scalable service.\n\nStep Functions: AWS Step Functions make it easy to coordinate components of distributed applications. Suppose you want to start\n\nrunning one component of your application after another one has completed successfully, or you want to run two components in\n\nparallel. You can easily coordinate these workflows using Step Functions. This saves you the time and effort required to build such\n\nworkflows yourself and helps you to focus on business logic more.\n\nAthena: Amazon Athena is an interactive serverless query service that makes it easy to use standard SQL to analyze data in\n\nAmazon S3. It allows you to quickly query structured, unstructured, and semi-structured data that's stored in S3. With Athena, you\n\ndon't need to load any datasets locally or write any complex ETL (extract, transform, and load), as it provides the capability to\n\nread data directly from S3. We will learn more about AWS Athena in Chapter 4, Serverless Amazon Athena and the AWS Glue Data\n\nCatalog.\n\nHere's a diagram of the AWS serverless ecosystem:\n\nFigure 1.3 : The AWS serverless ecosystem Ecosystem\n\nAWS Lambda\n\nAWS Lambda is a serverless computing platform that you can use to execute your code to build on-demand, smaller applications. It is a\n\ncompute service that runs your backend code without you being involved in the provisioning or managing of any servers in the\n\nbackground.\n\nThe Lambda service scales automatically based on your usage and it has inbuilt fault-tolerance and high availability, so you don't need to\n\nworry about the implementation of HA or DR (disaster recovery) with AWS Lambda. You are only responsible for managing your code,\n\nso you can focus on the business logic and get your work done.\n\nOnce you upload your code to Lambda, the services handles all the capacity, scaling, patching, and infrastructure to run your code and\n\nprovides performance visibility by publishing real-time metrics and logs to Amazon CloudWatch. You select the amount of memory",
      "content_length": 2105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "allocation for your function (between 128 MB and 3 GB). Based on the amount of memory allocation, CPU and network resources are\n\nallocated to your function. You could also say that AWS Lambda is a function in code that allows stateless execution to be triggered by\n\nevents. This also means that you cannot log in to actual compute instances or customize any underlying hardware.\n\nWith Lambda, you only pay for the time that your code is running. Once execution is completed, the Lambda service goes into idle mode\n\nand you don't pay for any idle time. AWS Lambda follows a very fine-grained pricing model, where you are charged for compute time in\n\n100 ms increments. It also comes with a Free Tier, with which you can use Lambda for free until you reach a certain cap on the number of\n\nrequests. We will study AWS Lambda pricing in more detail in a later section.\n\nAWS Lambda is a great tool for triggering code in the cloud that functions based upon events. However, we need to remember that AWS\n\nLambda (in itself) is stateless, meaning that your code should run as you develop it in a stateless manner. However, if required, a database\n\nsuch as DynamoDB can be used. Over the years, AWS Lambda has become very popular for multiple serverless use cases, such as web\n\napplications, data processing, IoT devices, voice-based applications, and infrastructure management.\n\nSupported Languages\n\nLambda is stateless and serverless. You should develop your code so that it runs in a stateless manner. If you want to use other third-party\n\nservices or libraries, AWS allows you to zip up those folders and libraries and give them to Lambda in a ZIP file, which in turn enables\n\nother supportive languages that you would like to use.\n\nAWS Lambda supports code written in the following six languages:\n\nNode.js (JavaScript)\n\nPython\n\nJava (Java 8 compatible)\n\nC# (.NET Core)\n\nGo\n\nPowerShell\n\nNote\n\nAWS Lambda could change the list of supported languages at any time. Check the AWS website for the latest information.\n\nExercise 1: Running Your First Lambda Function\n\nIn this exercise, we'll create a Lambda function, specify the memory and timeout settings, and execute the function. We will create a basic\n\nLambda function to generate a random number between 1 and 10.\n\nHere are the steps for completion:\n\n1. Open a browser and log in to the AWS console by going to this URL: https://aws.amazon.com/console/.\n\nFigure 1.4 : The AWS console",
      "content_length": 2429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "2. Click on Services at the top-left of the page. Either look for Lambda in the listed services or type Lambda in the search box, and\n\nclick on the Lambda service in the search result:\n\nFigure 1.5 : AWS services\n\n3. Click on Create a function to create your first Lambda function on the AWS Lambda page:\n\nFigure 1.6 : The Get started window\n\n4. On the Create function page, select Author from scratch:\n\nFigure 1.7 : The Create function page\n\n5. In the Author from scratch window, fill in the following details:\n\nName: Enter myFirstLambdaFunction.\n\nRuntime: Choose Node.js 6.10. The Runtime window dropdown shows the list of languages that are supported by AWS Lambda,\n\nand you can author your Lambda function code in any of the listed options. For this exercise, we will author our code in Node.js.\n\nRole: Choose Create new role from one or more template. In this section, you specify an IAM role.",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Role name: Enter lambda_basic_execution.\n\nPolicy templates: Select Simple Microservice permissions:\n\nFigure 1.8 : The Author from scratch window\n\n6. Now, click on Create function. You should see the message shown in the following screenshot:\n\nFigure 1.9 : Output showing Lambda function creation\n\nSo, you have created your first Lambda function, but we have yet to change its code and configuration based on our requirements.\n\nSo, let's move on.\n\n7. Go to the Function code section:",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Figure 1.10 : The Function code window\n\n8. Use the Edit code inline option to write a simple random number generator function.\n\n9. The following is the required code for our sample Lambda function. We have declared two variables: minnum and maxnum. Then,\n\nwe are using the random() method of the Math class to generate a random number. Finally, we call \"callback(null, generatednumber)\". If an error occurs, null will be returned to the caller; otherwise, the value of the generatednumber\n\nvariable will be passed as an output:\n\n//TODO implement\n\nlet minnum = 0;\n\nlet maxnum = 10;\n\nlet generatednumber = Math.floor(Math.random() * maxnum) + minnum\n\ncallback(null, generatednumber);\n\n10. In the Basic settings window, write myLambdaFunction_settings in the Description field, select 128 MB in the Memory\n\nfield, and have 3 sec in the Timeout field:",
      "content_length": 847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Figure 1.11 : The Basic settings window\n\n11. That's it. Click on the Save button in the top-right corner of the screen. Congratulations! You have just created your first Lambda\n\nfunction:\n\nFigure 1.12 : Output of the Lambda function created\n\n12. Now, to run and test your function, you need to create a test event. This allows you to set up event data to be passed to your\n\nfunction. Click on the dropdown next to Select a test event in the top-right corner of the screen and select Configure test event:\n\nFigure 1.13 : Lambda function Test window\n\n13. When the popup appears, click on Create new test event and give it a name. Click on Create and the test event gets created:",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Figure 1.14 : The Configure test event window\n\n14. Click on the Test button next to test events and you should see the following window upon successful execution of the event:\n\nFigure 1.15 : The Test execution window\n\n15. Expand the Details tab and more details about the function execution appear, such as actual duration, billed duration, actual\n\nmemory used, and configured memory:\n\nFigure 1.16 : The Details tab\n\nYou don't need to manage any underlying infrastructure, such as EC2 instances or Auto Scaling groups. You only have to provide your\n\ncode and let Lambda do the rest of the magic.",
      "content_length": 595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Activity 1: Creating a New Lambda Function that Finds the Square Root of the Average of Two Input Numbers\n\nCreate a new Lambda function that finds the square root of the average of two input numbers. For example, the two numbers provided are\n\n10 and 40. Their average is 25 and the square root of 25 is 5, so your result should be 5. This is a basic Lambda function that can be\n\nwritten using simple math functions.\n\nHere are the steps for completion:\n\n1. Follow the exercise that we just completed before this activity.\n\n2. Go to the AWS Lambda service and create a new function.\n\n3. Provide the function name, runtime, and role, as discussed in the previous exercise.\n\n4. Under the section on Function code, write the code to find the square root of the average of two input numbers. Once done, save\n\nyour code.\n\n5. Create the test event and try to test the function by executing it.\n\n6. Execute the function.\n\nNote\n\nThe solution for this activity can be found on page 152.\n\nLimits of AWS Lambda\n\nAWS Lambda imposes certain limits in terms of resource levels, according to your account level. Some notable limits imposed by AWS\n\nLambda are as follows:\n\nMemory Allocation: You can allocate memory to your Lambda function with a minimum value of 128 MB and a maximum of\n\n3,008 MB. Based on memory allocation, CPU and network resources are allocated to the Lambda function. So, if your Lambda\n\nfunction is resource-intensive, then you might like to allocate more memory to it. Needless to say, the cost of a Lambda function\n\nvaries according to the amount of memory allocated to the function.\n\nExecution Time: Currently, the Lambda service caps the maximum execution time of your Lambda function at 15 minutes. If your\n\nfunction does not get completed by this time, it will be automatically be timed out.\n\nConcurrent Executions: The Lambda service allows up to 1000 total concurrent executions across all functions within a given\n\nregion. Depending on your usage, you may want to set the concurrent execution limit for your functions, otherwise the overall costs\n\nmay escalate very soon.\n\nNote\n\nIf you want to learn more about other limits of Lambda functions, go to\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/limits.html#limits-list.\n\nAWS Lambda Pricing Overview\n\nAWS Lambda is a serverless compute service and you only pay for what you use, not for any idle time. There is a Free Tier associated\n\nwith Lambda pricing. We will discuss the Lambda Free Tier in the next section.",
      "content_length": 2479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "To understand the AWS billing model for Lambda, you first need to understand the concept of GB-s.\n\n1 GB-s is 1 Gigabyte of memory used per second. So, if your code uses 5 GB in 2 minutes, and then 3 GB in 3 minutes, the accumulated\n\nmemory usage would be 5*120 + 3*180 = 1140 GB seconds.\n\nNote\n\nThe prices for the AWS services discussed in this section and in this book are current at the time of writing, as AWS prices may change at\n\nany time. For the latest prices, please check the AWS website.\n\nLambda pricing depends on the following two factors:\n\nTotal Request Count: This is the total number of times the Lambda function has been invoked to start executing in response to an\n\nevent notification or invoke call. As part of the Free Tier, the first 1 million requests per month are free. There is a charge of $0.20\n\nfor 1 million requests beyond the limits of the Free Tier.\n\nTotal Execution Time: This is the time taken from the start of your Lambda function execution until it either returns a value or\n\nterminates, rounded up to the nearest 100 ms. The price for execution time varies with the amount of memory allocated to your\n\nfunction. If you want to understand how the cost of total execution time varies with the total amount of memory allocated to the\n\nLambda function, go to https://aws.amazon.com/lambda/pricing:\n\nFigure 1.17 : Lambda pricing\n\nLambda Free Tier\n\nAs part of the Lambda Free Tier, you can make 1 million free requests per month. You can have 400,000 GB-seconds of compute time per\n\nmonth. Since function duration costs vary with the allocated memory size, the memory size you choose for your Lambda functions\n\ndetermines how long they can run in the Free Tier.\n\nNote\n\nThe Lambda Free Tier gets adjusted against monthly charges, and the Free Tier does not automatically expire at the end of your 12-month\n\nAWS Free Tier term, but is available to both existing and new AWS customers indefinitely.",
      "content_length": 1925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "Activity 2: Calculating the Total Lambda Cost\n\nWe have a Lambda function that has 512 MB of memory allocated to it and there were 20 million calls for that function in a month, with\n\neach function call lasting 1 second. Calculate the total Lambda cost.\n\nHere's how we calculate the cost:\n\n1. Note the monthly compute price and compute time provided by the Free Tier.\n\n2. Calculate the total compute time in seconds.\n\n3. Calculate the total compute time in GB-s.\n\n4. Calculate the monthly billable compute in GB- s. Here's the formula:\n\nMonthly billable compute (GB- s) = Total compute – Free Tier compute\n\n5. Calculate the monthly compute charges in dollars. Here's the formula:\n\nMonthly compute charges = Monthly billable compute (GB-s) * Monthly compute price\n\n6. Calculate the monthly billable requests. Here's the formula:\n\nMonthly billable requests = Total requests – Free Tier requests\n\n7. Calculate the monthly request charges. Here's the formula:\n\nMonthly request charges = Monthly billable requests * Monthly request price\n\n8. Calculate the total cost. Here's the formula:\n\nMonthly compute charge + Monthly request charges\n\nNote\n\nThe solution for this activity can be found on page 153.\n\nAdditional Costs\n\nWhile estimating Lambda costs, you must be aware of additional costs. You will incur costs as part of Lambda integration with other AWS\n\nservices such as DynamoDB or S3. For example, if you are using the Lambda function to read data from an S3 bucket and write output\n\ndata into DynamoDB tables, you will incur additional charges for read from S3 and writing provisioned throughput to DynamoDB. We\n\nwill study more about S3 and DynamoDB in Chapter 2, Working with the AWS Serverless Platform.\n\nIn summary, it may not seem like running Lambda functions costs a lot of money, but millions of requests and multiple functions per\n\nmonth tend to escalate the overall cost.\n\nSummary\n\nIn this chapter, we focused on understanding the serverless model and getting started with AWS and Lambda, the first building block of a\n\nserverless application on AWS. We looked at the main advantages and disadvantages of the serverless model and its use cases. We\n\nexplained the serverless model, and worked with AWS serverless services. We also created and executed the AWS Lambda function.",
      "content_length": 2286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "In the next chapter, we'll look at the capabilities of the AWS Serverless Platform and how AWS supports enterprise-grade serverless\n\napplications, with and without Lambda. From Compute to API Gateway and from storage to databases, the chapter will cover the fully\n\nmanaged services that can be used to build and run serverless applications on AWS.\n\nOceanofPDF.com",
      "content_length": 363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Working with the AWS Serverless Platform\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain Amazon S3 and serverless deployments\n\nUse API Gateway and integrate it with AWS Lambda\n\nWork with fully managed services such as SNS, SQS, and DynamoDB\n\nThis chapter teaches you how to build and run serverless applications with AWS.\n\nIntroduction\n\nIn the previous chapter, we focused on understanding the serverless model and getting started with AWS and Lambda, the first building\n\nblocks of a serverless application on AWS. You also learned about how the serverless model differs from traditional product development.\n\nIn this chapter, we will learn about other AWS capabilities such as S3, SNS, and SQS. You can start by asking students about different\n\nAWS serverless technologies that the students have heard about or have had the chance to work with. Talk to them briefly about different\n\nAWS services such as S3 storage, API Gateway, SNS, SQS, and DynamoDB services. We will discuss them in detail in this chapter.\n\nAmazon S3\n\nAmazon Simple Storage Service or S3 is nothing but a cloud storage platform that lets you store and retrieve any amount of data\n\nanywhere. Amazon S3 provides unmatched durability, scalability, and availability so that you can store your data in one of the most secure\n\nways. This storage service is accessible via simple web interfaces, which can either be REST or SOAP. Amazon S3 is one of the most\n\nsupported platforms, so either you can use S3 as a standalone service or you can integrate it with other AWS services.\n\nAmazon S3 is an object storage unit that stores data as objects within resources called \"buckets\". Buckets are containers for your objects\n\nand serve multiple purposes. Buckets let you organize Amazon namespaces at the highest level and also play a key role in access control.\n\nYou can store any amount of objects within a bucket, while your object size can vary from 1 byte to 5 terabytes. You can perform read,\n\nwrite, and delete operations on your objects in the buckets.\n\nObjects in S3 consist of metadata and data. Data is the content that you want to store in the object. Within a bucket, an object is uniquely\n\nidentified by a key and a version ID. The key is the name of the object.\n\nWhen you add a new object in S3, a version ID is generated and assigned to the object. Versioning allows you to maintain multiple\n\nversions of an object. Versioning in S3 needs to be enabled before you can use it.\n\nNote\n\nIf versioning is disabled and you try to copy the object with the same name (key), it will overwrite the existing object.\n\nA combination of bucket, key, and version ID allows you to uniquely identify each object in Amazon S3.\n\nFor example, if your bucket name is aws-serverless and the object name is CreateS3Object.csv, the following would be the\n\nfully qualified path of an object in S3:",
      "content_length": 2878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Figure 2.1: Fully qualified URL to access the aws-serverless bucket that has an object\n\ncalled CreateS3Object.csv\n\nKey Characteristics of Amazon S3\n\nNow, let's understand some of the key characteristics of using the Amazon S3 service:\n\nDurability and high availability: Amazon S3 provides durable infrastructure to store your data and promises a durability of Eleven\n\n9s (99.999999999%). The Amazon S3 service is available in multiple regions around the world. Amazon S3 provides geographic\n\nredundancy within each region since your data gets copied automatically to at least three different availability zone locations within\n\na region. Also, you have the option to replicate your data across regions. As we saw earlier, you can maintain multiple versions of\n\nyour data as well, which can be used for recovery purposes later.\n\nIn the following diagram, you can see that when the S3 bucket in source-region-A goes down, route 53 is redirected to the\n\nreplicated copy in source-region-B:\n\nFigure 2.2: Amazon S3 Geographic Redundancy",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Note\n\nGeographic redundancy enables the replication of your data and stores this backup data in a separate physical location. You can always\n\nget your data back from this backup physical location just in case the main site fails.\n\nScalability: Amazon S3 is a highly scalable service as it can scale up or scale down easily based on your business needs. Suppose,\n\ntoday, that you have an urgent need to run analytics on 500 GB of data and before you do analytics, you have to bring that data into\n\nthe AWS ecosystem. Don't worry, as you can just create a new bucket and start uploading your data into it. All of the scalability\n\nwork happens behind the scenes, without any impact on your business.\n\nSecurity: In Amazon S3, you can enable server-side encryption, which encrypts your data automatically while it is getting written\n\non the S3 bucket. Data decryption happens by itself when someone wants to read the data. Amazon S3 also supports data transfer\n\nover SSL, and you can also configure bucket policies to manage object permissions and control access to your data using AWS\n\nIdentity and Access Management (IAM). We will look at permissions in more detail in a later part of this chapter.\n\nNote\n\nSince it is server-side encryption, there is no user interference required. Hence, when a user tries to read the data, the server\n\ndecrypts the data automatically.\n\nIntegration: You can use Amazon S3 as a standalone service to store data or you can integrate it with other AWS services such as\n\nLambda, Kinesis, and DynamoDB. We will look at some of these AWS services and their integration as part of our exercises in a\n\nlater part of this chapter.\n\nLow cost: Like other AWS serverless services, Amazon S3 works on a pay-as-you-go model. This means that there are no upfront\n\npayments and you pay based on your usage. Since it is a serverless offering, you don't need to manage any underlying hardware or\n\nnetwork resources. Therefore, there is no need to buy and manage expensive hardware. This helps to keep costs low with Amazon\n\nS3.\n\nAccess via APIs: You can use the REST API to make requests to Amazon S3 endpoints.\n\nDeploying a Static Website\n\nWith Amazon S3, you can host your entire static website at a low cost, while leveraging a highly available and scalable hosting solution to\n\nmeet varied traffic demands.\n\nExercise 2: Setting up a Static Website with an S3 Bucket Using a Domain Name in Route 53\n\nIn this exercise, we'll look at doing the following:\n\nCreating an S3 bucket and providing required permissions\n\nUploading a file onto an S3 bucket, which will be used to set the default page of your website\n\nConfiguring your S3 bucket\n\nSo, let's get started. Here are the steps to perform this exercise:\n\n1. Log in to your AWS account using your credentials.\n\n2. Click on the dropdown next to Services on top-left side and type S3:",
      "content_length": 2847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Figure 2.3: Searching Amazon S3 services via the dropdown option\n\n3. The Amazon S3 page will open. Click on Create Bucket:\n\nFigure 2.4: Creating an Amazon S3 bucket\n\n4. The Create bucket dialog box will open. You need to provide the following information:\n\nBucket Name: Enter a unique bucket name. For this book, we've used www.aws-serverless.tk since we will host a website\n\nusing our S3 bucket. As per AWS guidelines, a bucket name must be unique across all existing bucket names in Amazon S3. So,\n\nyou need to choose your individual bucket names.\n\nRegion: Click on the dropdown next to Region and select the region where you want to create the bucket. We will go with the\n\ndefault region, US-East (N. Virginia).\n\nIf you want to copy these settings from any other bucket and want to apply them to the new bucket, you can click on the dropdown\n\nnext to Copy settings from an existing bucket. We will configure the settings for this bucket here, so we will leave this option\n\nblank:",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "Figure 2.5: The Create bucket menu: Name and region section\n\n5. Click on Next. We will be taken to the Properties window. Here, we can set the following properties of the S3 bucket:\n\nVersioning\n\nServer access logging\n\nTags\n\nObject-level logging\n\nDefault encryption",
      "content_length": 264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Figure 2.6: The Create bucket menu: Set properties section\n\nFor this exercise, go with the default properties and click on the Next button.\n\n6. The next window is Set permissions. Here, we grant read and write permissions for this bucket to other AWS users and manage\n\npublic permissions as well. We can see in the following screenshot that the owner of the bucket has both read and write permissions\n\nby default. If you want to give permission for this bucket to any other AWS account as well, you can click on Add Account:",
      "content_length": 524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Figure 2.7: The Create bucket menu: Set permissions option\n\n7. Keep all of the checkboxes unchecked. We'll host a website using this S3 bucket.\n\n8. Keep Manage system permissions with the default settings and click on the Next button to go to the Review screen. Here, you can\n\nreview all of the settings for your S3 bucket. If you want to change anything, click on the Edit button and change it. Alternatively,\n\nclick on Create Bucket and your bucket will be created:",
      "content_length": 467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Figure 2.8: The Create bucket menu: Review section\n\n9. Click on the newly created bucket name and click on the second tab, Properties, and enable Static website hosting:\n\nFigure 2.9: Enabling the Static website hosting option under the Properties section\n\n10. Select the Use this bucket to host a website option. Enter the name of the index document. This document will be used to display\n\nthe home page of your website. You can also add an error.html file, which will be used to display the page in case of any error.\n\nWe aren't adding an error.html file for this exercise. You can also set redirection rules to redirect requests for an object to another object in the same bucket or to an external URL. Click on the Save button to save it:",
      "content_length": 741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Figure 2.10: The Static website hosting menu\n\nNote\n\nAt the top, note the Endpoint information. This will be the URL to access your website. In this case, it is http://www.aws-\n\nserverless.com.s3-website-us-east-1.amazonaws.com/.\n\n11. Next, click on the Overview tab.\n\n12. In the Overview tab, click on Upload. Click on Add files. Upload the index.html page (found in the Chapter02 folder of the\n\ncode bundle) as an object into our S3 bucket. Now, click on the Next button:",
      "content_length": 472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Figure 2.11: Uploading the Index.html file to the Amazon S3 bucket\n\nNote\n\nThe index.html file is a simple HTML file that contains basic tags, which are for demonstration purposes only.\n\n13. Under Manage Public Permissions, select Grant public read access to this object(s). Keep the rest of the settings as they are.\n\n14. Click on Next. Keep all of the properties to their default values on the Set properties screen. On the next screen, review the object\n\nproperties and click on the Upload button.\n\nCongratulations! You have just deployed your website using the Amazon S3 bucket.\n\n15. Go to a browser on your machine and go to the endpoint that we noted in step 10. You should see the home page (index.html)\n\ndisplayed on your screen:",
      "content_length": 736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Figure 2.12: Viewing the uploaded Index.html file on the browser\n\nWe have successfully deployed our S3 bucket as a static website. There are different use case scenarios for S3 services, such as media\n\nhosting, backup and storage, application hosting, software, and data delivery.\n\nEnabling Versioning on S3 Bucket\n\nNow, we'll look at enabling versioning on an S3 bucket. Here are the steps to do so:\n\n1. Log in to your AWS account.\n\n2. In the S3 bucket name list, choose the name of the bucket that you want to enable versioning for.\n\n3. Select Properties.\n\n4. Select Versioning.\n\n5. Choose Enable versioning or Suspend versioning and then click on Save.\n\nS3 and Lambda Integration\n\nYour Lambda function can be called using Amazon S3. Here, the event data is passed as a parameter. This integration enables you to write\n\nLambda functions that process Amazon S3 events, for example, when a new S3 bucket gets created and you want to take an action. You\n\ncan write a Lambda function and invoke it based on the activity from Amazon S3:\n\nFigure 2.13: Demonstrating the integration of AWS S3 with AWS Lambda\n\nExercise 3: Writing a Lambda Function to Read a Text File\n\nIn this exercise, we will demonstrate AWS S3 integration with the AWS Lambda service. We will create an S3 bucket and load a text file.\n\nThen, we will write a Lambda function to read that text file. You will see an enhancement for this demonstration later in this chapter when\n\nwe integrate it further with the API Gateway service to show the output of that text file as an API response.\n\nHere are the steps to perform this exercise:\n\n1. Go to the AWS services console and open the S3 dashboard. Click on Create bucket and provide a bucket name. Let's call it\n\nlambda-s3-demo. Note that your bucket name must be unique:",
      "content_length": 1783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Figure 2.14: Creating an S3 bucket named lambda-s3-demo\n\n2. Click on Next and follow the instructions to create the bucket. Set all of the settings as default. Since we will write the Lambda\n\nfunction using the same account, we don't need to provide any explicit permission to this bucket.\n\n3. Create a file in your local disk and add the content Welcome to Lambda and S3 integration demo Class!! in the\n\nfile. Save it as sample.txt.\n\n4. Drag and drop this file into the Upload window to upload it to the newly created S3 bucket.\n\n5. Click on Upload:",
      "content_length": 550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Figure 2.15: Uploading a sample text file to the newly created S3 bucket\n\nNote\n\nObserve the contents of this file's text message: Welcome to Lambda and S3 integration demo Class!!.\n\n6. Go to the AWS service portal, search for Lambda, and open the Lambda dashboard. Click on Create function and provide the\n\nfollowing details:\n\nProvide the name of the Lambda function. Let's name it read_from_s3.\n\nChoose the runtime as Node.js 6.10.\n\nChoose the Create a new role from one or more templates option. Provide the role name as read_from_s3_role.\n\nUnder policy templates, choose Amazon S3 object read-only permissions.\n\n7. Click on Create function.\n\n8. Once the Lambda function has been created, jump to the Function code section and replace the contents of the index.js file with the following code and save it. You can also copy this code from the s3_with_lambda.js file. In this script, we are creating two variables, src_bkt and src_key, which will contain the name of the S3 bucket and the name of the file that was",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "uploaded to the bucket. Then, we will retrieve that file as an object from the S3 bucket using s3.getObject and return the\n\ncontents of the file as an output of the Lambda function:\n\nvar AWS = require('aws-sdk');\n\nvar s3 = new AWS.S3();\n\nexports.handler = function(event, context, callback) {\n\n// Create variables the bucket & key for the uploaded S3 object\n\nvar src_bkt = 'lambdas3demo';\n\nvar src_key = 'sample.txt';\n\n// Retrieve the object\n\ns3.getObject({\n\nBucket: src_bkt,\n\nKey: src_key\n\n}, function(err, data) {\n\nif (err) {\n\nconsole.log(err, err.stack);\n\ncallback(err);\n\n}\n\nelse {\n\nconsole.log('\\n\\n' + data.Body.toString()+'\\n');\n\ncallback(null, data.Body.toString());\n\n}\n\n});\n\n};\n\nNote that the default output of the data will be in binary format, so we are using the toString function to convert that binary output to a string:\n\nFigure 2.16: Illustrating the use of the toString() function\n\n9. Click on the Save button to save the Lambda function.",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "10. Test the function now. But, before you can test it, you will have to configure test events, like we have done in earlier exercises.\n\nOnce a test event is configured, click on Test to execute the Lambda function.\n\nOnce the function has been executed, you should see the highlighted message Welcome to Lambda and S3 integration demo Class !!,\n\nas provided in the following screenshot. This message was the content of the sample.txt file that we uploaded into our S3 bucket in step 3:\n\nFigure 2.17: Demonstrating the Lambda function's execution\n\nNow, we have completed our discussion about S3 integration with a Lambda function.\n\nAPI Gateway\n\nAPI development is a complex process, and is a process that is constantly changing. As part of API development, there are many inherent\n\ncomplex tasks, such as managing multiple API versions, implementation of access and authorization, managing underlying servers, and\n\ndoing operational work. All of this makes API development more challenging and impactful on an organization's ability to deliver\n\nsoftware in a timely, reliable, and repeatable way.\n\nAmazon API Gateway is a service from Amazon that takes care of all API development-related issues (discussed previously) and enables\n\nyou to make your API development process more robust and reliable. Let's look into this in more detail now.\n\nWhat is API Gateway?\n\nAmazon API Gateway is a fully managed service that focuses on creating, publishing, maintaining, monitoring, and securing APIs. Using\n\nAPI Gateway, you can create an API that acts as a single point of integration for external applications while you implement business logic\n\nand other required functionality at the backend using other AWS services.\n\nWith API Gateway, you can define your REST APIs with a few clicks in an easy-to-use GUI environment. You can also define API\n\nendpoints, their associated resources and methods, manage authentication and authorization for API consumers, manage incoming traffic\n\nto your backend systems, maintain multiple versions of the same API, and perform operational monitoring of API metrics as well. You can\n\nalso leverage the managed cache layer, where the API Gateway service stores API responses, resulting in faster response times.\n\nThe following are the major benefits of using API Gateway. We have seen similar benefits of using other AWS services, such as Lambda\n\nand S3:\n\nScalability",
      "content_length": 2392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Operational monitoring\n\nPay-as-you-go model\n\nSecurity\n\nIntegration with other AWS services\n\nAPI Gateway Concepts\n\nLet's understand certain concepts of the API Gateway and how they work. This will help you build a better understanding on how the API\n\nGateway works:\n\nAPI endpoints: An API endpoint is one end of the communication, a location from where the API can access all the required\n\nresources.\n\nIntegration requests: The integration request specifies how your frontend will communicate with the backend system. Also,\n\nrequests may need to be transformed based on the type of backend system running. Possible integration types are Lambda, AWS\n\nservice, HTTP, and Mock.\n\nIntegration response: After the backend system processes the requests, API Gateway consumes it. Here, you specify how the\n\nerrors/response codes received from backend systems are mapped to the ones defined in API gateway.\n\nMethod request: The method request is a contract between the user (public interface) and the frontend system on what will be the\n\nrequest mode. This includes the API authorization and HTTP definitions.\n\nMethod response: Similar to the API method request, you can specify the method response. Here, you can specify the supported\n\nHTTP status codes and header information.\n\nExercise 4: Creating a REST API and Integrating It with Lambda\n\nNow, we will look at a demo of API Gateway and explore its different features. Along with this demo, we will also create a simple REST\n\nAPI using API Gateway and integrate it with a Lambda function. We will extend our earlier exercise on S3 integration with Lambda and\n\ncreate a REST API to show the contents of \"sample.txt\" as API response. This API will be integrated with Lambda to execute the\n\nfunction, and a GET method will be defined to capture the contents of the file and show it as the API response:\n\nFigure 2.18: Illustrating the various feature integrations of API Gateway with Lambda\n\nfunctions",
      "content_length": 1941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Here are the steps to perform this exercise:\n\n1. Open a browser and log in to the AWS console: https://aws.amazon.com/console/.\n\n2. Click on the dropdown next to Services or type API Gateway in the search box and click on the service:\n\nFigure 2.19: Searching for API Gateway from the Services section\n\n3. On the API Gateway dashboard, if you're visiting the page for the first time, click on Get Started. Otherwise, you will see the\n\nfollowing Create New API screen:\n\nFigure 2.20: The Create new API page\n\nHere, you have three options for choose from:\n\nNew API",
      "content_length": 560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Import from Swagger\n\nExample API\n\n4. Select New API and provide the following details:\n\nAPI name: Enter read_from_S3_api\n\nDescription: Enter sample API\n\nEndpoint Type: Choose Regional and click on Create API.\n\nFigure 2.21: Creating a new API with the specified details\n\n5. On the next page, click on Actions. You will see some options listed as Resources and Methods. A resource works as a building\n\nblock for any RESTful API and helps in abstracting the information. Methods define the kind of operation to be carried out on the\n\nresources. A resource has a set of methods that operate on it, such as GET, POST, and PUT.\n\nWe haven't created any resources yet as part of this exercise, so the AWS console will only have the root resource and no other\n\nresources.\n\n6. Now, create a resource. On the resource dashboard, provide the resource name and click on Create Resource from the dropdown of\n\nAction.\n\n7. Type read_file_from_s3 in the Resource Name field and click on Create Resource:",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Figure 2.22: Creating a resource with the provided information\n\n8. Create a method to access the information. Select that resource and then click on Actions to create a method. Choose GET from the\n\navailable methods and click on ✓ to confirm the GET method type:\n\nFigure 2.23: Creating a method to access the available information\n\n9. Now, choose Lambda Function as the integration type:\n\nFigure 2.24: Selecting Lambda Function as an integration type\n\n10. Once you click on Save, you will get following warning. Here, AWS is asking you to provide API Gateway permission to invoke\n\nthe Lambda function:\n\nFigure 2.25: Warning notification to enable API Gateway's permission\n\n11. Click on the OK button. The following screen will appear, which shows the workflow of the API. The following are the steps taken\n\nby the API:\n\nYour API will invoke the Lambda function.\n\nThe Lambda function gets executed and sends the response back to the API.",
      "content_length": 936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "The API receives the response and publishes it:\n\nFigure 2.26: Illustrating the workflow of an API\n\n12. Now, it's time to deploy the API. Click on the Actions dropdown and select Deploy API:\n\nFigure 2.27: The Deploy API menu\n\n13. Create a new deployment stage. Let's call it prod. Then, click on Deploy to deploy your API:",
      "content_length": 321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "Figure 2.28: Creating a new deployment stage named prod\n\n14. Once the API has been deployed, you should see the following screen. This screen has a few advanced settings so that you can\n\nconfigure your API. Let's skip this:\n\nFigure 2.29: The menu options of the deployed API\n\n15. Click on prod to open the submenu and select the GET method that you created for the API. Invoke the API URL that appears on\n\nthe screen. You can access this link to access your API:",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Figure 2.30: Invoking the API URL\n\nThis is what will appear on your screen:\n\nFigure 2.31: Illustrating the web page of the invoked URL\n\nGreat! You have just integrated the API Gateway with Lambda and S3.\n\nOther Native Services\n\nWe'll now turn our focus to other native services. We'll begin with Amazon SNS and then move on to Amazon SQS.\n\nAmazon SNS\n\nAmazon Simple Notification Services (SNS) is the cloud-based notification service that's provided by AWS that enables the delivery of\n\nmessages to the recipients or to the devices. SNS uses the publisher/subscriber model for the delivery of messages. Recipients can either\n\nsubscribe to one or more \"topics\" within SNS or can be subscribed by the owner of a particular topic. AWS SNS supports message\n\ndeliveries over multiple transport protocols.\n\nAWS SNS is very easy to set up and can scale very well depending on the number of messages. Using SNS, you can send messages to a\n\nlarge number of subscribers, especially mobile devices. For example, let's say you have set up the monitoring for one of your RDS\n\ninstances in AWS, and once the CPU goes beyond 80%, you want to send an alert in the form of an email. You can set up an SNS service\n\nto achieve this notification goal:\n\nFigure 2.32: Establishing the alert mechanism using the SNS services",
      "content_length": 1301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "You can set up AWS SNS using the AWS Management Console, AWS command-line interface, or using the AWS SDK. You can use\n\nAmazon SNS to broadcast messages to other AWS services such as AWS Lambda, Amazon SQS, and to HTTP endpoints, email, or SMS\n\nas well.\n\nLet's quickly understand the basic components, along with their functions, of Amazon SNS:\n\nTopic: A topic is a communication channel that is used to publish messages. Alternatively, you can subscribe to a topic to start\n\nreceiving messages. It provides a communication endpoint for publishers and subscribers to talk to each other.\n\nPublication of messages: Amazon SNS allows you to publish messages that are then delivered to all the endpoints that have been\n\nconfigured as subscribers for a particular topic.\n\nHere are some of the applications of Amazon SNS:\n\nSubscription to messages: Using SNS, you can subscribe to a particular topic and start receiving all the messages that get\n\npublished to that particular topic.\n\nEndpoints: With Amazon SNS, you publish messages to the endpoints, which can be different applications based on your needs.\n\nYou can have an HTTP endpoint, or you can deliver your messages to other AWS services (as endpoints) such as SQS and Lambda.\n\nUsing SNS, you can configure emails or mobile SMS as possible endpoints as well. Please note that the mobile SMS facility is\n\navailable in limited countries. Please check the Amazon SNS documentation for more details.\n\nAmazon SQS\n\nIn a simple message queue service, we have applications playing the roles of producers and consumers. The applications, known as\n\nproducers, create messages and deliver them to the queues. Then, there is another application, called the consumer, which connects to the\n\nqueue and receives the messages. Amazon SQL is a managed service adaptation of such message queue services.\n\nAmazon Simple Queue Service (SQS) is a fully managed messaging queue service that enables applications to communicate by sending\n\nmessages to each other:\n\nFigure 2.33: Enabling Amazon SQS for better communication between applications\n\nAmazon SQS provides a secure, reliable way to set up message queues. Currently, Amazon SQS supports two types of message queues:\n\nStandard queues: Standard queues can support close to unlimited throughput, that is, an unlimited number of transactions per\n\nsecond. These queues don't enforce the ordering of messages, which means that messages may be delivered in a different order than\n\nthey were originally sent. Also, standard queues work on the at-least-once model, in which messages are delivered at least once,\n\nbut they may be delivered more than once as well. Therefore, you need to have a mechanism in place to handle message\n\nduplication. You should use standard queues, whose throughput is more important than the order of requests.\n\nFIFO queues: FIFO queues work on the First-In-First-Out message delivery model, wherein the ordering of messages is\n\nmaintained. Messages are received in the same order in which they were sent. Due to ordering and other limitations, FIFO queues",
      "content_length": 3061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "don't have the same throughput as what's provided by standard queues. Note that FIFO queues are available in limited AWS regions.\n\nPlease check the AWS website for more details. You should use FIFO queues when the order of messages is important.\n\nNote\n\nThere is a limit on the number of messages supported by FIFO queues.\n\nDead Letter (DL) queues: DL queues are queues that can receive messages that can't be processed successfully. You can configure\n\na dead letter queue as a target for all unprocessed messages from other queues.\n\nJust like Amazon SNS, you can also set up the AWS SQS service using the AWS Management Console, AWS command-line interface, or\n\nusing the AWS SDK.\n\nDynamoDB\n\nAmazon DynamoDB is a NoSQL database service that is fully managed. Here, you won't have to face the operative and scaling challenges\n\nof a distributed database. Like other serverless AWS services, with DynamoDB, you don't have to worry about hardware provisioning\n\nsetup, configuration data replication, or cluster scaling.\n\nDynamoDB uses the concept of partition keys to spread data across partitions for scalability, so it's important to choose an attribute with a\n\nwide range of values and that is likely to have evenly distributed access patterns.\n\nWith DynamoDB, you pay only for the resources you provision. There is no minimum fee or upfront payment required to use\n\nDynamoDB. The pricing of DynamoDB depends on the provisioned throughput capacity.\n\nThroughput Capacity\n\nIn DynamoDB, when you plan to provision a table, how do you know the throughput capacity required to get optimal performance out of\n\nyour application?\n\nThe amount of capacity that you provision depends on how many reads you are trying to execute per second, and also how many write\n\noperations you are trying to do per second. Also, you need to understand the concept of strong and eventual consistency. Based on your\n\nsettings, DynamoDB will reserve and allocate enough Amazon resources to keep low response times and partition data over enough\n\nservers to meet the required capacity to keep the application's read and write requirements.\n\nNote\n\nEventual consistency is a type of consistency where there is no guarantee that what you are reading is the latest updated data. Strong\n\nconsistency is another type of consistency where you always read the most recent version of the data. Eventual consistent operations\n\nconsume half of the capacity of strongly consistent operations.\n\nNow, let's look at some important terms:\n\nRead capacity: How many items you expect to read per second. You also have to specify the item size of your request. Two\n\nkilobyte items consume twice the throughput of one kilobyte items.\n\nWrite capacity: How many items you expect to write per second.\n\nNote\n\nYou are charged for reserving these resources, even if you don't load any data into DynamoDB. You can always change the\n\nprovisioned read and write values later.",
      "content_length": 2914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "DynamoDB Streams\n\nDynamoDB Streams is a service that helps you capture table activity for DynamoDB tables. These streams provide an ordered sequence of\n\nitem-level modifications in a DynamoDB table and store the information for up to 24 hours. You can combine DynamoDB Streams with\n\nother AWS services to solve different kinds of problems, such as audit logs, data replication, and more. DynamoDB Streams ensure the\n\nfollowing two things:\n\nNo duplicity of stream records, which ensures that each stream record will only appear once\n\nThe ordered sequence of streams is maintained, which means that stream records appear in the same sequence as the modifications\n\nto the table\n\nAWS maintains separate endpoints for DynamoDB and DynamoDB Streams. To work with database tables and indexes, your application\n\nmust access a DynamoDB endpoint. To read and process DynamoDB Stream records, your application must access a DynamoDB Streams\n\nendpoint in the same region.\n\nDynamoDB Streams Integration with Lambda\n\nAmazon DynamoDB is integrated with AWS Lambda. This enables you to create triggers that can respond to events automatically in\n\nDynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.\n\nIntegration with Lambda allows you to perform many different actions with DynamoDB Streams, such as storing data modifications on S3\n\nor sending notifications using AWS services such as SNS.\n\nExercise 5: Creating an SNS topic and Subscribing to It\n\nIn this exercise, we'll create an SNS topic and subscribe to it. So, let's get started:\n\n1. Go to AWS services and type SNS in the search box. Once you click on Simple Notification Service (SNS), the following screen\n\nwill appear. Click on Get started, which will take you to the SNS dashboard:\n\nFigure 2.34: Creating a new SNS service\n\n2. Click on Topics on the left menu and click on Create new topic:",
      "content_length": 1897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Figure 2.35: Creating a new topic from the Topics section\n\n3. Provide the Topic name as TestSNS and the Display name as TestSNS, and click on Create topic. The Topic name and\n\nDisplay name can be different as well:\n\nFigure 2.36: Providing a Topic and Display name for the topic\n\n4. Once the topic has been created successfully, the following screen appears. This screen has the name of the topic and the topic's\n\nARN.\n\nNote\n\nARN stands for Amazon Resource Name, and it is used to identify a particular resource in AWS.",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Figure 2.37: Summary page of the newly created topic\n\nNote that if you need to reference a particular AWS resource in any other AWS service, you do so using the ARN.\n\nWe have successfully created a topic. Let's go ahead and create a subscription for this topic. We will set up an email notification as\n\npart of the subscription creation so that whenever something gets published to the topic, we will get an email notification.\n\n5. Click on Subscriptions on the left menu and then click on Create subscription:\n\nFigure 2.38: Creating a subscription for the SNS service\n\n6. Provide the ARN for the topic that we created in step 4. Click on the dropdown next to Protocol and choose Email. Provide an\n\nemail address as a value for the endpoint. Then, click on Create subscription:\n\nFigure 2.39: Providing details to create a new subscription\n\n7. Once the subscription has been created successfully, you should see the following screenshot. Note that the current status of the\n\nsubscription is PendingConfirmation:\n\nFigure 2.40: The summary of the newly created subscription",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "8. Check your emails. You should have received an email notification from Amazon to confirm the subscription. Click on Confirm\n\nSubscription:\n\nFigure 2.41: Verifying the subscription from the registered email address\n\nOnce the subscription is confirmed, you should see the following screenshot:\n\nFigure 2.42: The Subscription confirmed message\n\n9. Now, go back to the Subscription page and you will notice that PendingConfirmation is gone. Click on the refresh button if you\n\nstill see PendingConfirmation. It should now be gone:\n\nFigure 2.43: Summary of the confirmed ARN subscription\n\nSo, you have successfully created an SNS topic and have successfully subscribed to that topic as well. Whenever anything gets published\n\nto this topic, you will get an email notification.\n\nExercise 6: SNS Integration with S3 and Lambda\n\nIn this exercise, we will see create a Lambda function and integrate it with SNS to send email notifications:",
      "content_length": 933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Figure 2.44: Integrating a Lambda function with SNS to enable an email subscription\n\nHere are the steps to perform this exercise:\n\n1. Go to the AWS service console and type Lambda in the search box. Then, open the Lambda management page.\n\n2. Click on Create function and continue with the current selection, that is, Author from scratch:\n\nFigure 2.45: Creating a Lambda function from scratch\n\n3. Now, provide the following details:\n\nName: Write lambda_with_sns.\n\nRuntime: Keep it as Node.js.\n\nRole: Select Create role from template from the dropdown. Here, we are creating a Lambda function to send an SNS notification.\n\nRole name: Provide the role name as LambdaSNSRole.\n\nPolicy templates: Choose SNS publish policy:",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Figure 2.46: The menu options to create a Lambda function from scratch\n\n4. Now, click on Create function. Once the function has been created successfully, you should see the following message:\n\nFigure 2.47: The function created notification\n\n5. Let's jump to the function's code section. Go to the Git project and copy and paste the code in the code section of this page:\n\nFigure 2.48: Adding code from the Git project to the code section of the function",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "The following is an explanation of the main parts of the code:\n\nsns.publish: The publish action is used to send a message to an Amazon SNS topic. In our case, we have an email subscription\n\non the topic, we are trying to publish onto. Therefore, a successful publishing here will result in an email notification.\n\nMessage: The message you want to send to the topic. This message text will be delivered to the subscriber.\n\nTopicArn: The topic you want to publish to. Here, we are publishing to the \"TestSNS\" topic, which we created in our previous exercise. So, copy and paste the ARN of the topic that we created in the earlier exercise here.\n\n6. Click on the Save button on the top right corner. Now, we are ready to test the code.\n\n7. Click on the Test button. You need to configure the test event. Let's create a test event with the name TestEvent and click on the\n\nSave button:\n\nFigure 2.49: Creating a test event named TestEvent\n\n8. Click on the Test button now, and you should see the following screen:\n\nFigure 2.50: The test execution was successful notification\n\n9. Expand the execution result. Here, you can find more details about the function executions. Here, you can review the duration of the\n\nfunction's execution, the resources that have been configured, billed duration, and max memory used:",
      "content_length": 1308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Figure 2.51: Summary of test execution\n\n10. Review the execution results under the Function code section as well:\n\nFigure 2.52: Reviewing the test execution results under the function code\n\nAs we can see, the following message in the execution results is Message sent successfully. This confirms that the Lambda code was\n\nsuccessful in sending a notification to the SNS topic.\n\nTime to check your email account, which was configured as part of the subscriber in the preview exercise. You should see the following\n\nAWS notification message:",
      "content_length": 539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Figure 2.53: Sample email from the SNS service named TestSNS\n\nThis concludes our exercise on the simple integration of Lambda with Amazon SNS.\n\nActivity 3: Setting Up a Mechanism to Get an Email Alert When an Object Is Uploaded into an S3 Bucket\n\nIn the last exercise, we showcased lambda integration with Amazon SNS. As part of the exercise, whenever our lambda function was\n\nexecuted, we got an email alert generated by SNS service.\n\nNow, we will extend that exercise to perform an activity here.\n\nLet's assume that you are processing certain events and whenever there is an error with processing of a particular event, you move the\n\nproblematic event into a S3 bucket so you can process them separately. Also, you want to be notified via an email whenever any such an\n\nevent arrives in the S3 bucket.\n\nSo, we will do an activity to create a new S3 bucket and set up a mechanism that enables you to get an email alert whenever a new object\n\nis uploaded into this S3 bucket. When a new object is added to the S3 bucket, it will trigger the Lambda function created in the earlier\n\nexercise which will send the required email alert using SNS service.\n\nHere are the steps for completion:\n\n1. Go to AWS S3 service and click on Create bucket.\n\n2. Provide details such as name and region.\n\n3. Select the appropriate permissions.\n\n4. Go to the Lambda function created in the earlier exercise. Add S3 as a trigger under Lambda configuration section.\n\n5. Add the required details related to S3 bucket configuration, mainly the bucket name.\n\n6. Click on Add to add that S3 bucket as a trigger to execute Lambda function.\n\n7. Click on Save to save the changes to the Lambda function.\n\n8. Now, try to upload a new sample file to the S3 bucket. You should see an email alert in your mailbox.\n\nNote\n\nThe solution for this activity can be found on page 154.\n\nSummary",
      "content_length": 1852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "In this chapter, we looked at Amazon S3 and serverless deployments. We worked with API Gateway and its integration with AWS. We\n\ndelved into fully managed services such as SNS, SQS, and DynamoDB. Finally, we integrated SNS with S3 and Lambda.\n\nIn the next chapter, we'll build an API Gateway that we covered in this chapter. A comparison with a traditional on-premises web\n\napplication will be done as we replace traditional servers with serverless tools while making the application scalable, highly available, and\n\nperformant.\n\nOceanofPDF.com",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Building and Deploying a Media Application\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain the challenges of a traditional web application and making a traditional application serverless\n\nBuild an API Gateway API and upload binary data using it\n\nWork with media processing using AWS Lambda\n\nExplain image processing using AWS Rekognition\n\nThis chapter teaches you how to deploy your first serverless project step by step, by building a simple serverless application that uploads\n\nand processes media files.\n\nIntroduction\n\nEnterprises can face tremendous pressure and challenges when building and scaling even simple media-based applications. The\n\nconventional ways of building applications require enterprises to invest a lot of time and money up front, so that even simple application\n\ndevelopment can become a big project for companies.\n\nWhen it comes to building media-processing applications, which are generally very resource intensive, the situation gets even worse.\n\nIn this chapter, we are going to look at the challenges around building such applications and how cloud native development has changed\n\nthe way applications are built and delivered to customers.\n\nDesigning a Media Web Application – from Traditional to Serverless\n\nBuilding media web applications in the traditional way follows a certain path. This is displayed in the following diagram:",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Figure 3.1: Traditional way of building media applications\n\nHowever, in serverless application development, you don't manage the infrastructure but depend upon cloud providers for it. You have to\n\ndevelop your application to be independently deployable as microservices. During serverless development, you might want to break your\n\nbig monolithic application into smaller independent business units.\n\nSuch a serverless development brings many important patterns as well as development methodologies to be considered. Also, cloud\n\nproviders provide many managed services at every stage of the software development life cycle to help you build faster with out-of-the-\n\nbox monitoring/visibility in your serverless infrastructure.\n\nIn the next section of this chapter, we will take a look at the steps we need to follow if a media application has to be built in serverless\n\nmode. You will see we don't really need to talk to our IT department to raise any infrastructure requests and wait on them for weeks or\n\nmonths. Infrastructure is available with you within minutes from cloud providers.\n\nBuilding a Simple Serverless Media Web Application\n\nYou have realized by now that by using traditional structures, there is a lot of time-consuming technical administration.\n\nIn the cloud era, this is not the case. Cloud providers take care of all the infrastructure, as well as scaling and reliability, and the other\n\nneeds of your application, so you can focus on the business logic. This not only helps you focus on the right things, but also helps you\n\nreduce your time to market drastically.\n\nTo depict this, let's do a quick demo of our use case and look at how we can implement it in the AWS Cloud.\n\nWe'll deploy our web application locally. Clients will use this application to upload images to AWS. (Figure 3.2 depicts what we want to\n\nachieve in this tutorial.) Clients will call the APIs to upload images. These APIs will be hosted in an API Gateway and expose endpoints to\n\nupload images to S3. Once the image is uploaded to S3, an event will be triggered by S3 automatically that will launch a Lambda\n\nfunction. This Lambda function will read the image and process it using the AWS Rekognition service to find data inside the image. All\n\nthe infrastructure required for this is managed by AWS automatically.\n\nAuto scaling and reliability come out of the box by deploying the application on AWS Cloud's global infrastructure:",
      "content_length": 2428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Figure 3.2: Demonstrating working mechanism of serverless media web application\n\nExercise 7: Building the Role to Use with an API\n\nIn the following demo, we are using the AWS web console to build the role and assign it to the API.\n\nBefore you start creating the API, you need to create a proper role to assign to the API when it is created. This role should have access to\n\ncreate/read/update/delete S3 buckets and APIGatewayInvokeFullAccess. This role should also have\n\napigateway.amazonaws.com added to its trusted entities so that API Gateway can acquire this role.\n\nHere are the steps to perform this exercise:\n\n1. Search for IAM in the AWS console and open the Identity and Access Management window.\n\n2. Click on Roles to view existing roles.\n\n3. Click on Create role and select AWS service under Select type of trusted entity.\n\n4. Select API Gateway and click on Next: Permissions:",
      "content_length": 887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Figure 3.3: Creating role window\n\n5. Click on Next: Review without changing anything.\n\n6. Name the role and give a description, as shown in the following screenshot. Name it api-s3-invoke-demo:\n\nFigure 3.4: Providing the role information in the Review section\n\nYour role is created. Let's add the required policy to it to work with S3.\n\n7. Click on the newly created role to go to its Summary page. On the Summary page of that role, click on Attach Policy:",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Figure 3.5: Summary page of newly created page\n\n8. On the policies page, search and add two policies: AmazonS3FullAccess and AmazonAPIGatewayInvokeFullAccess.\n\n9. After attaching the policies, the final role summary should be as follows:\n\nFigure 3.6: Summary page view with newly attached policies\n\nExercise 8: Creating an API to Push to / Get from an S3 Bucket\n\nIn this exercise, we will create an API that will interact with the AWS S3 service.\n\nWe will push files to S3 and also create the GET method in the API to fetch the contents of the S3 bucket. All this will be serverless, meaning we are not going to provision any EC2 instances, but use AWS's managed serverless infrastructure.",
      "content_length": 689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Here are the steps to perform this exercise:\n\n1. In the Amazon API Gateway section of the AWS console, click on the Create API button:\n\nFigure 3.7: Creating a new API from the APIs section\n\n2. Select the New API radio and add the following details:\n\nAPI Name: image-demo\n\nDescription: this is a demo api for images\n\nEndpoint Type: Regional\n\nFigure 3.8: Creating a new API with specified details\n\n3. Click on Actions and select Create Resource to create a child resource named image and set it as a path variable under the\n\nresource path:",
      "content_length": 537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Figure 3.9: Creating resource for newly created API\n\nMake sure you add { } in Resource Path.\n\n4. Create another child resource of image child and name it file:\n\nFigure 3.10: Creating another resource for the API\n\n5. Now that the resource has been created, you need to create methods for your API. Click on \"/{image}\" and from Actions, select\n\nCreate Method:\n\nFigure 3.11: Creating methods for the API",
      "content_length": 400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "6. Then, select GET in the setup and click on the tick mark:\n\nFigure 3.12: Selecting the GET method from the dropdown list\n\n7. Select the integration type as AWS Service and fill in the details for the GET method, as shown next. Also, select Use patch\n\noverride under the Action type, and fill in the details as {bucket} in the execution role. Mention the ARN of the role that was\n\ncreated. Then, click on Save:\n\nFigure 3.13: Selecting options to set up the GET method\n\n8. Click on Save. You should see what is shown in the following screenshot for the GET method:",
      "content_length": 564,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Figure 3.14: The Method Execution window of the GET method\n\nYou can see four sections in the preceding screenshot:\n\nMethod Request\n\nIntegration Request\n\nIntegration Response\n\nMethod Response\n\n9. Go back to Method Execution and click on Method Request, then add the Content-Type to the HTTP Request Headers section:\n\nFigure 3.15: HTTP Request Headers section\n\nNow, you need to map the path variable in the Method Request to the Integration Request. This is required because we want to\n\nsend the data coming in to the API request to the backend system. Method Request represents the incoming data request and\n\nIntegration Request represents the underlying request that is sent to the system actually doing the work. In this case, that system is\n\nS3.\n\n10. Click on Integration Request and scroll to URL Path Parameters. Click on Add Path to add following.\n\nName: bucket\n\nMapped from: method.request.path.image\n\n11. In Integration Request in the HTTP headers section, add two headers:\n\nx-amz-acl = 'authenticated-read'\n\nContent-Type = method.request.header.Content-Type",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Note\n\nx-amz-acl is required to tell S3 that this is the authenticated request. The user needs to be provided read access to the bucket.\n\nYour URL path parameters and HTTP headers for the Integration Request should look as follows now:\n\nFigure 3.16: HTTP Headers and URL Path Parameters section\n\n12. Repeat steps 5 to 11 to create the PUT method. Instead of selecting GET in step 6, you have to select PUT. We will use this method\n\nto create a new bucket.\n\nYour API should now look like this:\n\nFigure 3.17: Method Execution window of the PUT method\n\n13. Next, create the API to upload the image into the specified bucket. Click on /{file} and then select Create Method from the Action\n\ndropdown. Select the PUT method and configure it as shown in the following screenshot. Make note of the path override. It should\n\nbe set to {bucket}/{object}. Role ARN should be same as in previous steps. Click on Save.",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Figure 3.18: The Setup window of the PUT method\n\n14. In the Method Request, add the HTTP Header Content-type.\n\n15. Click on Integration Request, and add in URL Path Parameters, the mapping of the bucket and object as shown here, and click\n\non the tick mark:\n\nbucket = method.request.path.image\n\nobject = method.request.path.file\n\n16. Also, add the Content-Type header mapping to method.request.header.Content-Type as done earlier for other\n\nmethods.\n\nFigure 3.19: Method Request window of the PUT method\n\nOne more thing we need to do is to configure the API to accept binary image content.\n\n17. Now, go to Settings from the left navigation panel to configure the API to accept binary image content:",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Figure 3.20: Settings options to accept binary image content\n\n18. Add image/png under Binary Media Types and click on Save Changes:\n\nFigure 3.21: Add the Binary Media Type option under the Binary Media Types section\n\nAll changes are done. We are now ready to deploy our API.\n\n19. Click on Deploy API from the Actions dropdown:",
      "content_length": 326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Figure 3.22: Deploying the API by clicking on the Deploy API option from the Actions\n\ndropdown list\n\n20. Enter the stage details and description, and click on Deploy:\n\nFigure 3.23: Deploying the API after providing all the details\n\nAll changes are done. We are now ready to deploy our API.\n\n21. Your API is deployed on the dev stage. Note the Invoke URL:",
      "content_length": 354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Figure 3.24: Invoke URL of the newly deployed API\n\n22. You can use any API client, such as SoapUI or Postman, for testing your API. We'll use the ReadyAPI tool as it has robust support.\n\nNote\n\nYou can download a 14-day free trial at this link: https://smartbear.com/product/ready-api/free-trial/ (you have to enter details of\n\nyourself for the download to start).\n\n23. Now, create a bucket. In SoapUI, create a new project for a PUT request and enter the invoke URL copied earlier. Enter the bucket\n\nname in the path after /dev:\n\nFigure 3.25: Creating a new project for the PUT request\n\n24. Click on OK. Click on Continue to describe your API:",
      "content_length": 643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Figure 3.26: Selecting appropriate options for the project\n\n25. Specify the bucket name after dev/ in Resource. In the following screenshot, mohit-1128-2099 is the bucket name. Change\n\nthe Media Type to application/xml:\n\nFigure 3.27: Specifying the bucket name\n\nIn our exercise, we are creating the S3 bucket in us-east-1. So, we will keep request body as blank. However, if you want to create\n\nthe bucket in some other region (us-west-, 1 in our, example below), you have to set following text in the request body and hit send.\n\nYou should get HTTP 200 Response status and your bucket should be created in S3:\n\n<CreateBucketConfiguration>\n\n<LocationConstraint>us-west-1</LocationConstraint>\n\n</CreateBucketConfiguration>\n\nNow the S3 bucket should get created. Go to the AWS S3 service and check the existence of the bucket. You can also do a GET API\n\ncall to check the existence of the bucket along with its contents.",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "26. Now, make another call to our API to upload an image. Update the path in the Resource textbox of ReadyAPI to include the\n\nfilename that you want to upload in the S3 bucket. You need to attach the file and set the media-type to image/png. Go to the Attachments tab at the bottom of the request, and attach any PNG image. Click No on the Cache request dialog box.\n\n27. Click on Send and you should be able to get back 200 OK response. Go back to the AWS S3 service and you should see the newly\n\ncreated bucket now:\n\nFigure 3.28: Output showing the newly created bucket\n\nSo far, we have created an API with the GET and PUT methods that is accepting requests from the user and uploading images to S3. Note\n\nthat we haven't had to spawn a server so far for building the entire working service.\n\nExercise 9: Building the Image Processing System\n\nYou have just created the API. Now, you need to create the backend Lambda function that gets triggered every time an image is uploaded.\n\nThis function will be responsible for analyzing the image and detecting the labels inside the image, such as objects. It will call the\n\nRekognition API and feed the image into it, which will then analyze the image and return data.\n\nThe returned data will be pushed to a topic in SNS. SNS is AWS Simple Notification Service and works on the pub/sub mechanism. We\n\nwill then subscribe our email address to that SNS so any messages that are sent to the topic get delivered to our email address also.\n\nThe final functionality will be that when a user uploads an image using the API to the S3 bucket, our infrastructure analyzes it and emails\n\nus the data found in the image.\n\nWe are going to create this infrastructure step by step, as follows:\n\n1. Create the IAM role. As we created the role for API Gateway, we need to follow similar steps; however, we have to select Lambda\n\non the first screen under Choose the service that will use this role, and the permissions should be AWSLambdaFullAccess,\n\nAmazonRekognitionFullAccess, and AmazonSNSFullAccess. No need to add any tags for now. This is how your role should look after creation:",
      "content_length": 2113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Figure 3.29: Summary of the newly created ARN role\n\n2. Use the S3 bucket created with our API.\n\nOne more thing before creating the Lambda function is to create an SNS and subscribe your email to it. This SNS will be used by\n\nthe Lambda to publish image analysis data. Once published, the SNS will send that message to your subscribed email.\n\n3. Go to the SNS console and click on Create Topic to publish the extracted text:\n\nFigure 3.30: Create new topic window\n\n4. Click on Create subscription:",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Figure 3.31: Topic details window\n\n5. Create the subscription for the topic. Choose Email under Protocol and provide an email address under Endpoint. There will be an\n\nemail sent your email account. You have to confirm the subscription:\n\nFigure 3.32: Create subscription window\n\n6. Keep a note of the Topic ARN as it will be required in the Lambda code.\n\nWe have created an API Gateway API, S3 bucket, SNS Topic, and email subscription. Now, we have to create a Lambda function.\n\n7. In the Lambda console, click on Create function. Make sure that you have the same region selected as the one in which you have\n\ncreated the S3 bucket. Select Author from scratch.\n\n8. In the next wizard, fill the name of the Lambda, choose an existing role, and choose the S3 bucket name that you created:",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Figure 3.33: Screenshot of the Using blueprint rekognition-python window\n\n9. Scroll to the end and hit Create function.\n\n10. In the configuration section of the Lambda, go to the function code and replace it with the following code. In the code, we create the\n\nSDK object, AWS Rekognition client, and AWS SNS client. We then handle the incoming Lambda request. We create the bucket\n\nand image names. We call the detectLabels function to get all the labels using the AWS Rekognition service. Create the\n\nmessage to post and publish to the SNS. The detectLabels function is used to make the call to the Rekognition service using the bucket name:\n\nvar A = require('aws-sdk');\n\nvar rek = new A.Rekognition();\n\nvar sns = new A.SNS();\n\nAWS.config.update({region: 'us-east-1'});\n\nexports.handler = (event, context, callback) => {\n\nconsole.log('Hello, this is nodejs!');\n\n// Get the object from the event\n\nvar bucket = event['Records'][0]['s3']['bucket']['name'];\n\nvar imageName = event['Records'][0]['s3']['object']['key'];\n\ndetectLabels(bucket, imageName)\n\n.then(function(response){\n\nvar params = {\n\nMessage: JSON.stringify(response['Labels']), /* required */\n\nSubject: imageName,\n\nTopicArn: 'arn:aws:sns:us-east-1:XXXXXXXXXXXX:extract-image-labels-sns'",
      "content_length": 1247,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "};\n\nsns.publish(params, function(err, data) {\n\nif (err) console.log(err, err.stack); // an error occurred\n\nelse console.log(data); // successful response\n\n});\n\n});\n\ncallback(null, 'Hello from Lambda');\n\n};\n\nfunction detectLabels(bucket, key) {\n\nlet params = {\n\nImage: {\n\nS3Object: {\n\nBucket: bucket,\n\nName: key\n\n}\n\n}\n\n};\n\nreturn rekognition.detectLabels(params).promise();\n\n}\n\n11. Another thing to note is in the Lambda configuration in the S3 section. Make sure the trigger is enabled. If it is not, toggle the\n\nbutton to enable it:\n\nFigure 3.34: The Configure triggers window",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Figure 3.35: Enabling the S3 trigger for the Lambda function\n\nNote\n\nMake sure the S3 bucket is in the same region as the Lambda, or it won't be able to trigger the Lambda.\n\nThis concludes the creation of all the infrastructure.\n\nNow, when you call the API to upload any image, you should see an email in your inbox with content similar to this:\n\nFigure 3.36: Sample email after uploading an image\n\nDeployment Options in the Serverless Architecture\n\nWe have seen how we can create a serverless application using the AWS console. This is not the only way to achieve it. In the cloud\n\nworld, infrastructure automation is a key aspect of any deployment. Cloud providers have built strong frameworks around their services\n\nthat can be used to script out the entire infrastructure. AWS provides APIs, SDKs, and a CLI that can be consumed in various ways to\n\nprovision infrastructure automatically.\n\nIn general, there are three additional ways we can achieve the previous functionality without using the AWS console:\n\nAWS CLI: AWS provides a command-line interface for working with AWS services. It is built on top of an AWS Python SDK\n\ncalled boto. You just need to install Python on your Mac, Windows, or Linux machine and then install the AWS CLI.\n\nOnce installed, you can run the following command in your Terminal or command line to check it is properly installed:\n\n$ aws --version\n\naws-cli/1.11.96 Python/2.7.10 Darwin/16.7.0 botocore/1.8.2\n\nAWS Code SDKs: AWS provides many SDKs that can be used directly in your favorite programing language for working with\n\nAWS services. As of today, these are the programing languages AWS supports:\n\n.NET\n\nJava\n\nC++\n\nJavaScript",
      "content_length": 1664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Python\n\nRuby\n\nGo\n\nNode.js\n\nPHP\n\nServerless Framework: This is one option that is becoming more popular by the day. It is a command-line tool that can be used to\n\nbuild and deploy serverless cloud services. It can be used not only with AWS, but also with many other major cloud providers, such\n\nas Azure, Google Cloud Platform (GCP), and IBM Cloud.\n\nIt is built in JavaScript and requires Node.js v6.5.0 or later to be installed. For deployment, you provide a YAML-based file,\n\nserverless.yml, to the CLI. It internally translates all the content of YAML into an AWS CloudFormation template and uses it to\n\nprovision the infrastructure.\n\nIt is again a very powerful tool for working with serverless AWS-managed services.\n\nLike the AWS CLI, it can also be very nicely integrated into a CI/CD process in an enterprise to achieve automation.\n\nActivity 4: Creating an API to Delete the S3 Bucket\n\nCreate an API to delete the S3 bucket that we just created in the preceding exercise. In this activity, you need to expose an API that will\n\naccept the bucket name and will delete the S3 bucket.\n\nHere are the steps to complete the activity:\n\n1. Go to the AWS API Gateway console and in the API created in this chapter, create a Delete API.\n\n2. Configure the incoming headers and path parameters properly in the Method Request and Integration Request sections.\n\n3. Change the authorization of the Delete method from NONE to AWS_IAM.\n\n4. Click on the Deploy API.\n\n5. Test the Delete method using the Test Tool (Ready API).\n\nYou should see the bucket getting deleted in the AWS S3 console.\n\nNote\n\nThe solution for this activity can be found on page 157.\n\nSummary\n\nIn this chapter, you have seen the challenges of traditional web application development and how serverless development can address\n\nthem. You also learned how to work with API Gateway and expose a REST-based API with it. You integrated AWS S3 with API Gateway\n\nand created and read a bucket using PUT and GET APIs. We then created Lambda functions. We worked with AWS Rekognition in the\n\nevent-based architecture to analyze images at runtime and identify important data inside them.\n\nIn the next chapter, we'll explore the capabilities of AWS Athena. We'll also work with AWS Glue, and learn how to populate the AWS\n\nGlue Data Catalog.",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Serverless Amazon Athena and the AWS Glue Data Catalog\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain serverless AWS Athena capabilities, as well as its storage and querying concepts\n\nAccess Amazon Athena and its different use cases\n\nCreate databases and tables in Athena\n\nExplain AWS Glue and its benefits\n\nWork with data catalogs and populate the AWS Glue Data Catalog\n\nThis chapter delves into the capabilities of AWS Athena. You'll also work with AWS Glue, and learn how to populate the AWS Glue Data\n\nCatalog.\n\nIntroduction\n\nConsider a situation where you're just about to leave for the day from the office, and at that very moment your boss asks you to run a\n\nreport on a new, complex dataset. You're asked to finish this report before you leave for the day.\n\nIn the past, completing such a report would've taken hours. You would have to first analyze the data, create a schema, and then dump the\n\ndata before you could execute queries to create the required report.\n\nNow, with the AWS Glue and Athena services, you can get such reports done very quickly and leave for the day on time.\n\nIn the previous chapter, we saw how serverless application development can address the challenges of traditional application development.\n\nIn this chapter, we'll explore the capabilities of AWS Athena. We'll also work with AWS Glue, and learn how to populate the AWS Glue\n\nData Catalog.\n\nAmazon Athena\n\nIn simple terms, Amazon Athena is nothing but an interactive query service that is serverless. It makes use of standard SQL to analyze\n\ndata in Amazon S3. It allows you to quickly query structured, unstructured, and semi-structured data that is stored in S3. With Athena, you\n\ndon't need to load any datasets locally or write any complex ETLs (Extracts, Transforms, and Loads) as it provides the capability to\n\nread data directly from S3.\n\nNote\n\nETL is a popular concept from the data warehouse world, where three separate functions are used to prepare data for data analysis. The\n\nterm extract refers to data extraction from the source dataset, transform refers to data transformation (if required), and load refers to data\n\nloading in the final tables, which will be used for data analysis.\n\nThe Amazon Athena service uses Presto technology. Presto is a distributed SQL query engine that is open source. Presto provides a SQL-\n\nlike dialect for querying data and is designed to provide fast performance for running interactive analytic queries. The size of the data\n\nsources doesn't matter. The AWS management console, Athena API, Athena CLI, or a simple JDBC connection can be used to access\n\nAmazon Athena.",
      "content_length": 2639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Athena is a serverless offering, meaning that you don't need to set up or manage any underlying data servers. What you must do is set up a\n\nconnection to your data in Amazon S3. Then, you must define the schema. Once you've done that, you can start querying with the help of\n\nthe query editor in the AWS management console. You can use ANSI SQL queries with Amazon Athena, including the support of joins\n\nand functions, to query data in S3. Therefore, it's easy for anyone with basic skills in SQL to analyze large-scale datasets quickly.\n\nAmazon Athena supports multiple data formats such as CSV, JSON, and Parquet. With Athena, you can query encrypted data (be it\n\nencrypted on the server side or client side). Amazon Athena also gives you the option to encrypt your result sets by integrating with KMS\n\n(Key Management Service):\n\nFigure 4.1: Ad hoc analysis using Amazon Athena\n\nNote\n\nAmazon Athena is priced per query. You will be charged for the data scanned per query. As you might've noticed, data can be stored in\n\ndifferent formats on Amazon S3. Therefore, you can use different formats to store data in a compressed form, resulting in lower amounts of\n\ndata being scanned by your query. You can partition your data or convert your data to columnar storage formats to read only the columns\n\nthat are required to process the data. Amazon Athena costs start from $5/TB of data scanned.\n\nAWS provides native integration of Athena with the AWS Glue Data Catalog. The AWS Glue Data Catalog provides you with a metadata\n\nstore that is persistent for your data in Amazon S3. You can, thereby, create tables and query data in Athena. We will study this concept in\n\nmore detail later in this chapter, along with an exercise.\n\nHere are some of the use cases of Amazon Athena:\n\nAd hoc analysis\n\nAnalyzing server logs\n\nUnderstanding unstructured data – works well with complex data types such as arrays or maps\n\nQuick reporting\n\nHere are some of the tools that are used to access Amazon Athena:",
      "content_length": 1991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "AWS Management Console\n\nA JDBC or ODBC connection\n\nAPI\n\nCommand-line interface\n\nDatabases and Tables\n\nAmazon Athena allows you to create databases and tables. A database is basically a grouping of tables. By default, we have a sampledb database that has been created in Athena, but you can create a new database and start creating your tables underneath it if you like. Also,\n\nyou will find a table called elb_logs under sampledb:\n\nFigure 4.2: The Database page\n\nExercise 10: Creating a New Database and Table Using Amazon Athena\n\nLet's go ahead and create a new database and table so that we can understand Athena, alongside a quick demo, in more detail. Athena\n\nworks on data that's stored in S3. Before going to Athena, let's create an S3 bucket and upload the sample dataset that's provided. In this\n\nexercise, we will use the flavors_of_cacao.csv dataset that was provided with this book:\n\n1. Go to AWS services and search for Athena.\n\nYou should be able to see the following window:",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "Figure 4.3: Query Editor\n\n2. Now, click on Create table and choose Manually.\n\nNote\n\nWe will look into the Automatically (AWS Glue crawler) option in the next part of this chapter.\n\n3. Provide the required details, such as the database name, table name, and S3 bucket details. You need to provide the S3 bucket's\n\ndetails in the same place where you have the data stored that you want to analyze via Athena.\n\n4. Click on the Next button.\n\nYou can create the table under the default (already selected) database as well:\n\nFigure 4.4: Add table",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Note\n\nThe table name needs to be unique here, so choose a different table name than what you see here. Before creating the table, I\n\nuploaded the flavors_of_cacao.csv dataset into my S3 bucket, that is, s3://aws-athena-demo-0606/. However, you\n\ncan make any changes in the underlying dataset post table creation, as long as you don't change the underlying schema. We will\n\nlook at this in more detail.\n\n5. Click on Next, and you will see the different data formats that you can access using AWS Athena.\n\n6. Choose CSV and click on Next:\n\nFigure 4.5: Data Format\n\n7. On the next screen, define the columns, along with their data types. Either you can define all of the columns one by one, or you can\n\nclick on the Bulk add columns option to add all of the columns' details into one place.\n\nWe have nine columns in our datasets, so we will add the following information, along with their data types:\n\nCompany string,Bean_Origin string,REF int,Review_Date int,Cocoa_Percent\n\nstring,Company_Location string,Rating decimal,Bean_Type string,Broad_Bean_Origin string\n\nNote that the header is removed from the data file. The column details can be understood from the following screenshot:",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Figure 4.6: Bulk add columns\n\n8. Once you click on the Add button, you will note that all of the columns, along with their data types, are displayed. You can make\n\nany changes (as required) here as well:",
      "content_length": 203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Figure 4.7: The Columns section\n\n9. Click on Next. You can configure partitions on this screen. Partitions allow you to create logical groups of information together,\n\nwhich helps with the faster retrieval of information. Partitions are generally recommended for larger datasets.\n\nHowever, since our dataset is quite small, we will skip configuring partitions for now:",
      "content_length": 368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Figure 4.8: Partitions\n\n10. Click on Create Table and Athena will run the create table statement. Your table will be created. Now, you should see\n\nfollowing screen:\n\nFigure 4.9: Run query\n\nHere, we can note that our database, Athena_demo, has been created, and a new table, flavours_of_cocoa, has been created as well. You can see the table definition on the right of the screen.\n\nNote\n\nIf you don't want to go through GUI for table creation, you can write the table Data Definition Language (DDL) directly into the\n\nquery window and create the table. You can also use the ALTER and DROP TABLE commands to modify and drop the existing\n\ntables, respectively.\n\n11. Click on the Save as button and provide a name for the query:",
      "content_length": 724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Figure 4.10: Choosing a name window\n\nNow, the table has been created, and you can run SQL statements to view the table data.\n\n12. In the following screenshot, you will see the 10 rows that have been selected from the table:\n\nFigure 4.11: Selecting 10 rows\n\n13. Write SQL functions as well to analyze the data from different perspectives. Here, we will list the total products of a company that\n\nhave a rating greater than 4 by performing a group by operation on the company and getting the required count:\n\nselect company, count(*) cnt from flavours_of_cocoa\n\nwhere rating > 4\n\ngroup by company\n\norder by cnt desc\n\nlimit 10;",
      "content_length": 624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "14. Execute the query.\n\nYou'll see the following output in the result pane:\n\nFigure 4.12: Output window\n\nSince Athena is based on hive SQL and Presto, you can use many of the Hive functions with Athena.\n\nNote\n\nFor the complete documentation on supported SQL queries, functions, and operators, go to\n\nhttps://docs.aws.amazon.com/athena/latest/ug/functions-operators-reference-section.html.\n\nWith this, we have completed our exercise on AWS Athena. As we stated earlier in this chapter, Athena is a wonderful query service that\n\nsimplifies analyzing data from Amazon S3 directly.\n\nBe aware that you get charged on the amount of data being analyzed for your query. In the preceding query, you can see the highlighted\n\npart for the amount of data being scanned by the query. However, if you don't apply the proper filters, you can end up scanning\n\nunnecessarily huge amounts of data, which eventually escalates the overall costs.\n\nAWS Glue\n\nAWS Glue is a serverless, cloud-optimized, and fully managed ETL service that provides automatic schema inference for your structured\n\nand semi-structured datasets. AWS Glue helps you understand your data, suggests transformations, and generates ETL scripts so that you\n\ndon't need to do any ETL development.\n\nYou can also set up AWS Glue for running your ETL jobs, automatically provisioning and scaling the resources needed to complete them.\n\nYou can point AWS Glue to your data that's stored on different AWS services such as S3, RDS, and Redshift. It finds out what your data\n\nis. It stores the related metadata, such as schemas and table definitions, in the AWS Glue Data Catalog.\n\nOnce your data is cataloged, you can start using it for different kinds of data analysis. For executing data transformations and data loading\n\nprocesses, AWS Glue generates code.",
      "content_length": 1802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "First, let's understand the major components of AWS Glue, which might be new to the students:\n\nAWS Glue Data Catalog: A data catalog is used to organize your data. Generally, glue crawlers populate data catalogs, but you can\n\nuse DDL statements as well to populate it. You can bring in metadata information from multiple data sources such as Amazon S3,\n\nRedshift, or RDS instances, and create a single data catalog for all of them. Now, all of the metadata is in one place and is\n\nsearchable. The Glue catalog is basically a replacement for Hive Metastore.\n\nNote\n\nA data catalog is mainly comprised of metadata information (definitions) related to database objects such as tables, views,\n\nprocedures, indexes, and synonyms. Almost all databases in the market today have data catalogs populated in the form of\n\ninformation schema. Data catalogs help users to understand and consume data for their analysis. It is a very popular concept in the\n\nbig data world.\n\nAWS Glue Crawlers: Crawlers are primarily used to connect with different data sources, discover the schema, and partition and\n\nstore associated metadata into the data catalogs. Crawlers detect schema changes and version updates, and keep the data catalog in\n\nsync. They also detect if data is partitioned in the underlying tables.\n\nCrawlers have data classifiers written to infer the schemas for several popular data formats such as relational data stores, JSON, and\n\nParquet format. You can also write a custom data classifier for a custom file format (using Grok pattern) that Glue doesn't recognize\n\nand associates it with the crawler. You can write multiple classifiers and, once your data is classified, Glue will ignore subsequent\n\ndata classifiers.\n\nYou can run Glue crawlers on an ad hoc basis or on a particular schedule. Moreover, with Glue crawlers being serverless, you only pay\n\nwhen they are in use.\n\nThe following diagram depicts the complete workflow for AWS Glue:",
      "content_length": 1940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Figure 4.13: AWS Glue\n\nIn the preceding diagram, we have multiple data sources such as Amazon S3, Redshift, and RDS instances, which are connected by AWS\n\nGlue crawlers to read and populate the AWS Glue data catalogs. Also, you can use Amazon Athena or AWS Redshift Spectrum to access\n\nAWS Glue data catalogs for the purpose of data analysis.\n\nExercise 11: Using AWS Glue to Build a Metadata Repository\n\nLet's look at an example of how AWS Glue automatically identifies data formats and schemas and then builds a metadata repository,\n\nthereby eliminating the need to manually define and maintain schemas. We will use the same chocolate-barratings dataset that\n\nwe used previously for our Glue exercise:\n\n1. Log in to the AWS Management Console and go to AWS Glue service.\n\n2. Go to Crawlers and click on Add crawler to open the Add crawler screen. Let's name the crawler chocolate_ratings:\n\nFigure 4.14: Crawler information\n\n3. Click on Next. Here, you can specify the Amazon S3 path where your dataset is located. We can either use the S3 picker\n\n(highlighted in yellow) for this or just paste in the S3 path:\n\nFigure 4.15: Adding a data store\n\n4. Click on Next. If you have data across multiple S3 buckets or other data sources such as RDS and Redshift, you can add them on\n\nthis screen. We will only go with a single S3 source for this demonstration for now:",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Figure 4.16: Adding another data store\n\n5. On the next screen, define an IAM role for the crawler. This role provides the crawler with the required permissions to access the\n\ndifferent data stores. Click on Next:\n\nFigure 4.17: Choosing an IAM role\n\n6. Now, set up the schedule for the crawler. We can either run this crawler on demand or on schedule. If we automatically schedule a\n\ncrawler, it helps us to identify any changes to the underlying data and keeps the data catalog up to date. This automatic update of the\n\ndata catalog is very helpful for datasets that change on frequently. We will run it on demand for now:\n\nFigure 4.18: Creating a schedule for the crawler\n\n7. Here, you can either select an existing database to keep the data catalog or create a new one. We will create a new database called\n\nglue-demo for this demonstration. Also, if you want to add a prefix to all of the tables that have been created by crawler for easy identification, you can add the prefix here. We will skip this:",
      "content_length": 1005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "Figure 4.19: Adding a database\n\n8. Also, as we discussed earlier in this chapter, crawlers can handle changes to the schemas to ensure that table metadata is always in\n\nsync with the underlying data. As you can see in the following screenshot, the default settings allow crawlers to modify the\n\ncatalogue schemas if the underlying data is updated or deleted. We can disable it as well, based on our needs:\n\nFigure 4.20: Configuring the crawler's output\n\n9. Click on Next to review the crawler specifications and click on Finish to create the crawler:",
      "content_length": 550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Figure 4.21: Review all of the steps\n\n10. Now that the crawler has been created, let's go ahead and run it. Once the crawler has completed running, you will notice in the\n\nfollowing screenshot that one new table has been added in the data catalog. This table is a metadata representation of data and\n\npoints to the location where the data is physically located:\n\nFigure 4.22: Adding the crawler\n\n11. Go to Tables and click the aws_athena_demo_0606 table to take a look at the schema that has been populated by the crawlers:",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Figure 4.23: Editing the table\n\nYou can also change the data type of any column as required here. Also, you can view the partitions associated with the table.\n\nSince our table is defined in the catalog, we can query it using Amazon Redshift Spectrum or Amazon Athena. Both products allow you to\n\nquery data directly from S3. We already looked at how to query it using Amazon Athena earlier in this chapter. The only difference will be\n\nthat the database name will be different this time. Please go ahead and try it yourself.\n\nNow, we have seen how AWS Glue makes it easy to crawl data and maintain metadata information in the data catalog. Although there are\n\nmultiple other ways to populate your catalog with tables such as manually defining the table, importing from an external Hive Metastore,\n\nor running Hive DDL queries to create the catalog, AWS Glue provides an easy to use method to create and maintain data catalogs on\n\na regular basis. This concludes our discussion on AWS Glue.\n\nActivity 5: Building an AWS Glue Catalog for a CSV-Formatted Dataset and Analyzing the Data Using AWS Athena\n\nImagine you are a data analyst. You have been provided with a dataset that contains inventory-to-sales ratios for each month since 1992.\n\nThese ratios can be better explained as follows:\n\nRatio = Number of months of inventory/Sales for a month\n\nConsidering this, a ratio of 3.5 means that the business has an inventory that will cover three and a half months of sales. You have been\n\nasked to quickly review the data. You have to prepare a report to get a count of the months for the last 10 years when the inventories to\n\nsales ratio was < 1.25. For example, if the ratio was low four times in the month of January since 1992, then January 4 should be the\n\nresult.\n\nA CSV formatted dataset called total business-inventories-to-sales ratio has been provided with this book. This dataset is derived from\n\nanother dataset that's available on the Kaggle website (https://www.kaggle.com/census/total-business-inventories-and-sales-data). This\n\ndataset has two columns:\n\nObserved_Data: Date when the observation was made\n\nObserved_Value: Inventories to sales ratios\n\nHere are the steps to complete this activity:\n\n1. Create an AWS Glue crawler and build a catalog for this dataset. Verify that the data types are reflected correctly.\n\n2. Go to AWS Athena and create a new schema and table for the data, which you cataloged in step 1.\n\n3. Once you have the data exposed in Athena, you can start building your reports.\n\n4. Write a query to filter the data, where the inventories to sales ratios (observed_values) was less than 1.25, and group the\n\noutput by month. Then, you will have the reports ready to share.\n\nNote\n\nThe solution for this activity can be found on page 158.\n\nSummary",
      "content_length": 2779,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "In this chapter, we learned about serverless AWS Athena capabilities, its storage, and its querying concepts. We also discussed different\n\nuse cases for AWS Athena. Later, we learned about AWS Glue and its benefits. We looked at what data catalogs are, their uses, and how to\n\npopulate the AWS Glue Data Catalog. In the end, we leveraged the catalog that we created via AWS Glue in AWS Athena to access the\n\nunderlying data and analyze it.\n\nIn the next chapter, we'll focus on the capabilities of Amazon Athena and AWS Glue.\n\nOceanofPDF.com",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Real-Time Data Insights Using Amazon Kinesis\n\nLearning Objectives\n\nBy the end of this chapter, you will be able to:\n\nExplain the concept of real-time data streams\n\nCreate a Kinesis stream using Amazon Kinesis Data Streams\n\nUse Amazon Kinesis Data Firehose to create a delivery stream\n\nSet up an analytics application and process data using Amazon Kinesis Data Analytics\n\nThis chapter shows you how to unleash the potential of real-time data insights and analytics using Amazon Kinesis. You'll also combine\n\nAmazon Kinesis capabilities with AWS Lambda to create lightweight, serverless architectures.\n\nIntroduction\n\nWe live in a world surrounded by data. Whether you are using a mobile app, playing a game, browsing a social networking website, or\n\nbuying your favorite accessory from an online store, companies have set up different services to collect, store, and analyze high\n\nthroughput information to stay up to date on customer's choices and behaviors. These types of setups, in general, require complex software\n\nand infrastructures that can be expensive to provision and manage.\n\nMany of us have worked on aggregating data from different sources to accomplish reporting requirements, and most of us can attest that\n\nthis whole data crunching process is often very demanding. However, a more painful trend has been that as soon as the results of this data\n\nare found, the information is out of date again. Technology has drastically changed over the last decade, which has resulted in real-time\n\ndata being a necessity to stay relevant for today's businesses. Moreover, real-time data helps organizations improve on operational\n\nefficiency and many other metrics.\n\nWe also need to be aware of the diminishing value of data. As time goes on, the value of old data continues to decrease, which makes\n\nrecent data very valuable; hence, the need for real-time analysis increases even further.\n\nIn this chapter, we'll look at how Amazon Kinesis makes it possible to unleash the potential of real-time data insights and analytics, by\n\noffering capabilities such as Kinesis Video Streams, Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics.\n\nAmazon Kinesis\n\nAmazon Kinesis is a distributed data streaming platform for collecting and storing data streams from hundreds of thousands of producers.\n\nAmazon Kinesis makes it easy to set up high capacity pipes that can be used to collect and analyze your data in real time. You can process\n\nincoming feeds at any scale, enabling you to respond to different use cases, such as customer spending alerts and click stream analysis.\n\nAmazon Kinesis enables you to provide curated feeds to customers on a real-time basis rather than performing batch processing on large,\n\ntext-based log files later on. You can just send each event to Kinesis and have it analyzed right away to find patterns and exceptions, and\n\nkeep an eye on all of your operational details. This will allow you to take decisive action instantly.\n\nBenefits of Amazon Kinesis\n\nLike other AWS serverless services, Amazon Kinesis has several benefits. Most of the benefits have already been discussed in terms of\n\nother services, so I will restrain myself from going into the details. However, here is the list of the benefits of using Amazon Kinesis'\n\nservices:",
      "content_length": 3287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Easy administration\n\nLow cost\n\nSecurity\n\nPay as you go capabilities\n\nDurability\n\nHigh scalability\n\nChoice of framework\n\nReplayability\n\nContinuous processing\n\nHighly concurrent processing\n\nAmazon Kinesis provides different capabilities, depending on different use cases. We will now look at three of the major (and most\n\nimportant) capabilities in detail.\n\nAmazon Kinesis Data Streams\n\nAmazon Kinesis Data Streams is a managed service that makes it easy for you to collect and process real-time streaming data. Kinesis\n\nData Streams enable you to leverage streaming data to power real-time dashboards so that you can look at critical information about your\n\nbusiness and make quick decisions. The Kinesis Data Stream can scale easily from megabytes to terabytes per hour, and from thousands to\n\nmillions of records per second.\n\nYou can use Kinesis Data Streams in typical scenarios, such as real-time data streaming and analytics, real-time dashboards, and log\n\nanalysis, among many other use cases. You can also use Kinesis Data Streams to bring streaming data as input into other AWS services\n\nsuch as S3, Amazon Redshift, EMR, and AWS Lambda.\n\nHow Kinesis Data Streams Work\n\nKinesis Data Streams are made up of one or more shards. What is a shard? A shard is a uniquely identified sequence of data records in a\n\nstream, providing a fixed unit of capacity. Each shard can ingest data of up to a maximum of 1 MB per second and up to 1,000 records per\n\nsecond, while emitting up to a maximum of 2 MB per second. We can simply increase or decrease the number of shards allocated to your\n\nstream in case of changes in input data. The total capacity of the stream is the sum of the capacities of its shards.\n\nBy default, Kinesis Data Streams keep your data for up to 24 hours, which enables you to replay this data during that window (in case\n\nthat's required). You can also increase this retention period to up to 7 days if there is a need to keep the data for longer periods. However,\n\nyou will incur additional charges for extended windows of data retention.\n\nA producer in Kinesis is any application that puts data into the Kinesis Data Streams, and a consumer consumes data from the data\n\nstream.\n\nThe following diagram illustrates the simple functionality of Kinesis Data Streams. Here, we are capturing real-time streaming events\n\nfrom a data source, such as website logs to Amazon Kinesis Streams, and then providing it as input to another AWS Lambda service for\n\ninterpretation. Then, we are showcasing the results on a PowerBI dashboard, or any other dashboard tool:",
      "content_length": 2572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Figure 5.1: An image showing the simple functionality of Kinesis data streams\n\nExercise 12: Creating a Sample Kinesis Stream\n\nLet's go to the AWS console and create a sample Kinesis stream, which will then be integrated with Lambda to move the real-time data\n\ninto DynamoDB. Whenever an event is published in the Kinesis stream, it will trigger the associated Lambda function, which will then\n\ndeliver that event to the DynamoDB database.\n\nThe following is a high-level diagram showcasing the data flow of our exercise. There are many real-world scenarios that can be\n\naccomplished using this architecture:\n\nFigure 5.2: An image showing the data flow of the architecture\n\nSuppose you run an e-commerce company and want to contact customers that put items in to their shopping carts but don't buy them. You\n\ncan build a Kinesis stream and redirect your application to send information related to failed orders to that Kinesis stream, which can then\n\nbe processed using Lambda and stored in a DynamoDB database. Now, your customer care team can look into the data to get the\n\ninformation related to failed orders in real time, and then contact the customers.\n\nHere are the steps to perform this exercise:\n\n1. Go to AWS services and search for Kinesis. Once it has been selected, you will be redirected to the Kinesis dashboard. Here, you\n\ncan view the services that have been created for all four different flavors of Amazon Kinesis:",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Figure 5.3: A screenshot of the Amazon Kinesis dashboard\n\nNote\n\nOur focus for this exercise is Kinesis Data Streams. We will look at other Kinesis services later in this chapter.\n\n2. Go to Data Streams and click on Create Kinesis stream:\n\nFigure 5.4: A screenshot showing how to create Kinesis streams\n\n3. Provide the name of the Kinesis stream. Let's name it kinesis-to-dynamodb. Also, provide the estimated number of shards\n\nthat you will need to handle the data. As we discussed earlier, read and write capacities are calculated based on the number of\n\nconfigured shards. Since we are creating it for demonstration purposes, let's put its value as 1.\n\nYou will notice that the values against write and read get changed based on the number being provided against the number of\n\nshards. Once you are done, click on Create Kinesis Stream:",
      "content_length": 838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Figure 5.5: A screenshot showing the process of naming the Kinesis stream and\n\nestimating the number of shards\n\n4. Once the stream has been created, you will notice the status of the stream as Active. Now, you are ready to use this stream for your\n\nincoming data:\n\nFigure 5.6: A screenshot showing the status of the stream after creation\n\nSo, we have created a Kinesis data stream and we will integrate it now with DynamoDB using an AWS Lambda function.\n\n5. Let's go ahead and create a new table in DynamoDB that will store the data coming from the Kinesis Data Stream. Go to AWS\n\nservices and search for DynamoDB. Then, click on Create table:",
      "content_length": 643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Figure 5.7: A screenshot showing how to create a new table\n\n6. Name your table sample-table and specify the createdate column partition key. Click on Create. This will create the\n\nrequired destination table for you:\n\nFigure 5.8: A screenshot showing the creation of the destination table\n\n7. In the AWS Lambda service, write a Lambda function to fetch the records from the Kinesis data stream and store them in\n\nDynamoDB.\n\n8. Click on Create function under Lambda service. Click on Blueprints and search for kinesis-process-record. Click on kinesis-\n\nprocess-record template:",
      "content_length": 575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Figure 5.9: A screenshot showing the creation of the function under the Lambda service\n\n9. Give a name to the Lambda function. Create a new role, which will allow Lambda to insert records into the DynamoDB database.\n\nTake a look at the following screenshot to find out which policies you need to attach to the role:\n\nFigure 5.10: A screenshot showing the creation of a new role for the function\n\n10. Provide the required details about the kinesis stream. You can set up the appropriate value of the batch size, depending on the flow\n\nof messages. For now, we will keep the default value. Once you are done, click on create function:",
      "content_length": 632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Figure 5.11: A screenshot of setting the appropriate value for the batch size\n\n11. When you create a Lambda function from a blueprint, you need to create the function first, before changing any code.\n\n12. Go to the section function code and replace the nodeJS code with the one provided in the kinesis-lambda-dynamodb-\n\nintegration.js file.\n\nWe are populating two columns in this code. The first one is the createdate column, which was also defined as PK in our DynamoDB table definition earlier, in step 7. We are using the default value for this column. The second column is the ASCII\n\nconversion for the base64 data, which is coming in as a part of the Kinesis data stream. We are storing both values as data in our\n\nDynamoDB table, sample-table. Then, we are using the putItem method of the AWS.DynamoDBclient class to store the data in a DynamoDB table:\n\nFigure 5.12: A screenshot of the code that's used to populate the two columns\n\n13. Go ahead and save the code. To execute it, we need to create a Kinesis test event that will trigger the Lambda function and store the\n\nevent data in the DynamoDB database. Click on Configure test event, provide a name (for example, KinesisTestEvent), and click on Create.\n\n14. Once the test event is created, go ahead and execute the lambda function. Your lambda function should get executed successfully.\n\nExecute it couple of times and you should start seeing data into your table in DynamoDB database.",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Figure 5.13: A screenshot showing the execution of the Lambda function that we created\n\nearlier\n\nThis concludes our exercise on Amazon Kinesis data events and their integration with the DynamoDB database, via the AWS Lambda\n\nservice.\n\nAmazon Kinesis Firehose\n\nLet's suppose you're working with stock market data and you want to run minute-by-minute analytics on the market stocks (instead of\n\nwaiting until the end of the day). You will have to create dynamic dashboards such as top performing stocks, and update your investment\n\nmodels as soon as new data arrives.\n\nTraditionally, you could achieve this by building the backend infrastructure, setting up the data collection, and then processing the data.\n\nBut it can be really hard to provision and manage a fleet of servers to buffer and batch the data arriving from thousands of sources\n\nsimultaneously. Imagine that one of those servers goes down or something goes wrong in the data stream; you could actually end up losing\n\ndata.\n\nAmazon Kinesis Firehose makes it easy for you to capture and deliver real-time streaming data reliably to Amazon S3, Amazon Redshift,\n\nor Amazon Elasticsearch Service. Using Amazon Firehose, you can respond to data in near real time, enabling you to deliver powerful\n\ninteractive experiences and new item recommendations, and do real-time alert management for critical applications.\n\nAmazon Firehose scales automatically as volume and throughput varies and it takes care of stream management, including batching,\n\ncompressing, encrypting, and loading the data into different target data stores supported by Amazon Firehose. As with other AWS services,\n\nthere is no minimum fee or setup cost required, so you only pay for the data being sent by you by adjusting streaming data quickly and\n\nautomating administration tasks.\n\nAmazon Firehose allows you to focus on your application and deliver great real-time user experiences rather than being stuck with the\n\nprovisioning and management of a backend setup:",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Figure 5.14: A diagram showing the functionalities of Amazon Kinesis Data Firehose\n\nExercise 13: Creating a Sample Kinesis Data Firehose Delivery Stream\n\nIn this exercise, we'll go to the AWS console and create a sample Kinesis Data Firehose delivery stream. As part of this exercise, we will\n\ndeliver data to an S3 bucket:\n\n1. On Amazon Kinesis Dashboard, go to Data Firehose and click on Create delivery stream:\n\nFigure 5.15: A screenshot showing how to create a Firehose delivery stream\n\n2. Provide the delivery stream's name. Let's call it Kinesis-firehose_to_S3. Now, there are two options here to specify the\n\nsource of data. The first one is Direct PUT, which you can use as a source if you want to send data directly from applications, such\n\nas IOT, CloudWatch logs, or any other AWS application. The second one is the Kinesis Stream, which you can use if you have data\n\ncoming via a regular Kinesis stream. Let's take Direct PUT as the source for this exercise. We will discuss using a Kinesis stream\n\nas a data source in a later part of this chapter:",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Figure 5.16: A screenshot showing how to specify the source of data\n\nClick Next to go to Step 2: Process records.\n\n3. On this page, you can transform the records as required. As we discussed earlier in this chapter, Firehose allows you to do ETL\n\nwith streaming data. To do the transformations, write a Lambda function. Let's skip this option for now:",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Figure 5.17: A screenshot showing the options for Record transformation\n\nJust click on Next to move to Step 3: Choose destination.\n\n4. Kinesis Firehose also allows you to convert data formats on the go (for example, Parquet to JSON). You can write a Lambda\n\nfunction to easily achieve this. Let's skip this option for now, and click on Next to move to Step 4: Configure settings.\n\n5. On this page, you need to select the destination of your streaming data. As we discussed earlier, you can send your data to different\n\ndestinations, such as S3, Redshift, or the Elasticsearch service. For this demo, we will choose Amazon S3 as the destination.\n\nSpecify the S3 bucket details, such as where you want to save the data. Here, you can specify an existing bucket or create a new\n\none. Leave the prefix blank. Once you are done, click on Next to move to Step 4: Configure settings:",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Figure 5.18: A screenshot showing how to create a new bucket or provide details about\n\nan existing one\n\n6. Here, you can configure the buffer conditions, encryption, and compression settings. Buffer settings enable Firehose to buffer the\n\nrecords before they get delivered to S3. Let's set the buffer size as 1 MB and the buffer interval as 60 seconds. When either of these\n\ntwo conditions are met, the records will be moved to the destination.\n\nNote that you can specify the buffer interval to be between 60 and 900 seconds:",
      "content_length": 525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Figure 5.19: A screenshot showing the configuration of the buffer conditions,\n\nencryption, and compression settings\n\nLet's keep encryption, compression, and error logging disabled, for now.\n\n7. Also, you need to specify the role that will be used to deliver the data to S3. We will go ahead and create a new role now:",
      "content_length": 317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "Figure 5.20: A screenshot showing how to specify the role that will be used to deliver the\n\ndata to S3\n\n8. At this point, we need to create a new role, so we will open a separate AWS window and search for Roles. Click on Create role.\n\nWe will go back to proceed from step 6 once the role has been created (step 12):\n\nFigure 5.21: A screenshot showing the creation of a new role\n\n9. Select AWS service under trusted entity and choose Kinesis from the list of services that will use this role. Once you select\n\nKinesis, Kinesis Firehose will appear as the possible use case. Click on Permissions:",
      "content_length": 594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Figure 5.22: A screenshot showing the selection of the trusted entity type and service for\n\nthe role\n\n10. Attach the Permission policy now. Search for S3 and attach the AmazonS3FullAccess policy with the role, and click on Review:",
      "content_length": 230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Figure 5.23: A screenshot showing the attachment of the permission policy\n\n11. Click on Review. Provide a name for the role, and click on Create role:",
      "content_length": 150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Figure 5.24: A screenshot showing how to add the role name and description\n\n12. Now, the role has been created, so let's put in the required information on the screen from step 6:\n\nFigure 5.25: A screenshot showing the fulfillment of details like the IAM Role and\n\npolicy name\n\n13. Click on Review to verify the settings for Kinesis Firehose:",
      "content_length": 342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Figure 5.26: A screenshot showing the verification of settings for Kinesis Firehose\n\n14. Click on Create delivery stream, and your Firehose delivery stream should be created successfully:\n\nFigure 5.27: A screenshot showing the successful creation of the delivery stream\n\n15. Let's try to ingest some sample data into our delivery stream and verify whether it reaches the destination.\n\nClick on the delivery stream to go to the details page for that stream. Under Test with demo data, click on Start sending demo\n\ndata. This will start ingesting test data into the Firehose delivery stream:",
      "content_length": 589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Figure 5.28: A screenshot showing the details of a particular stream\n\n16. Once data ingestion has started, you should see the following message:\n\nFigure 5.29: A screenshot showing the confirmation about demo data being sent to\n\nthe delivery stream\n\nYou will have to wait for a few seconds (20 seconds) for the data to be ingested. Once data ingestion is done, you can click on Stop\n\nsending demo data.\n\n17. Now, it is time to verify whether the data has been delivered successfully to S3 or not. Go to the S3 location that we configured\n\nearlier to receive the data, and you should see the data there:\n\nFigure 5.30: A screenshot showing the data has been successfully delivered",
      "content_length": 677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Note that there might be some delay for data to appear in S3, depending on your buffer settings.\n\nThis concludes our demo of Amazon Kinesis Firehose delivery streams.\n\nActivity 6: Performing Data Transformations for Incoming Data\n\nIn the last exercise, we worked on a Kinesis Firehose demo that was integrated with Lambda to move real-time data into S3. You may\n\nhave noticed a Lambda function in the architectural diagram, but we didn't use it in our exercise. There was a data transformation section\n\n(step 3) in the last exercise that we kept disabled.\n\nNow, as part of this activity, we will perform data transformation for incoming data (from Firehose) by using a Lambda function, and then\n\nstore that transformed data in the S3 bucket. With data transformation, we can solve many real-world business problems. We are going to\n\ncreate a Kinesis Firehose data stream, transform the data using a Lambda function, and then finally store it in S3. The following are some\n\nexamples of this:\n\nData format conversion, such as from JSON to CSV, or vice versa\n\nAdding identifiers\n\nData Curation and Filtering\n\nData enhancements, like the addition of date or time\n\nHere are the steps to perform this activity:\n\n1. Start by creating a Kinesis Firehose data stream, and follow the steps that we followed in the last exercise.\n\n2. We disabled data transformation using Lambda in the last exercise. This time, enable the Transform source records with AWS\n\nLambda option.\n\n3. Once it has been enabled, create a Lambda function to do the data transformation on incoming data.\n\n4. There are already some sample functions provided by Amazon. So, for the sake of simplicity, pick one of them, as well. Try out\n\nGeneral Firehose Processing. You can read more about it on the AWS website, if required.\n\n5. Once the Lambda function has been created, ensure that it has the required privileges.\n\n6. Keep the rest of the settings as is.\n\n7. Now, configure an Amazon S3 bucket as the Firehose destination, like we did in the ast exercise.\n\n8. Send the test data from the Test with demo data section by clicking on Start sending demo data:\n\nFigure 5.31: The Test with demo data window",
      "content_length": 2163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "9. Go to the S3 location that was configured earlier to receive the data, and you should see the data file. Upon downloading this data\n\nfile and opening it with Notepad, you should see the data in the CSV format, as shown here:\n\nFigure 5.32: Screenshot showing data in the CSV format\n\nNote\n\nThe solution for this activity can be found on page 161.\n\nAmazon Kinesis Data Analytics\n\nYou are now able to consume real-time streaming data using Amazon Kinesis and Kinesis Firehose, and move it to a particular\n\ndestination. How can you make this incoming data useful for your analysis? How can you make it possible to analyze the data in real time\n\nand perform actionable insights?\n\nAmazon Kinesis Data Analytics is a fully managed service that allows you to interact with real-time streaming data, using SQL. This can\n\nbe used to run standard queries, so that we can analyze the data and send processed information to different business intelligence tools and\n\nvisualize it.\n\nA common use case for the Kinesis Data Analytics application is time series analytics, which refers to extracting meaningful information\n\nfrom data, using time as a key factor. This type of information is useful in many scenarios, such as when you want to continuously check\n\nthe top performing stocks every minute and send that information to your data warehouse to feed your live dashboard, or calculate the\n\nnumber of customers visiting your website every ten minutes and send that data to S3. These time windows of 1 minute and 10 minutes,\n\nrespectively, move forward in time continuously as new data arrives, thus computing new results.\n\nDifferent kinds of time intervals are used, depending on different use cases. Common types of time intervals include sliding and tumbling\n\nwindows. Sharing different windows intervals is out of the scope of this book, but the students are encouraged to look online for more\n\ninformation.\n\nThe following diagram illustrates a sample workflow for Amazon Kinesis Analytics:",
      "content_length": 1984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Figure 5.33: An image showing the workflow of Amazon Kinesis Analytics\n\nYou can configure the Amazon Kinesis Data Analytics application to run your queries continuously. As with other serverless AWS\n\nservices, you only pay for the resources that your queries consume with Amazon Kinesis Data Analytics. There is no upfront investment or\n\nsetup fee.\n\nExercise 14: Setting Up an Amazon Kinesis Analytics Application\n\nIn the AWS console, set up the Amazon Kinesis Analytics application. We will also look at the interactive SQL editor, which allows you to\n\neasily develop and test real-time streaming analytics using SQL, and also provides SQL templates that you can use to easily implement\n\nthis functionality by simply adding SQL from the templates.\n\nUsing a demo stream of stock exchange data that comes with Amazon Kinesis Analytics, we will count the number of trades for each\n\nstock ticker and generate a periodic report every few seconds. You will notice that the report is progressing through time, generating the\n\ntime series analytics where the latest results are emitted every few seconds, based on the chosen time window for this periodic report.\n\nThe steps are as follows:\n\n1. Go to the Data Analytics tab in the Amazon Kinesis dashboard and click on the Create application button to open the Create\n\napplication form. Provide the application's name. Let's call it kinesis-data-analytics, and click on Create application.\n\nYou can leave the Description blank:",
      "content_length": 1469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Figure 5.34: A screenshot showing the creation of the Kinesis Analytics application\n\n2. Once the data analytics application has been created successfully, you should get the following message on the screen:\n\nFigure 5.35: A screenshot showing the success message, stating that the Kinesis\n\nAnalytics application was created successfully\n\n3. Now, you need to connect this application with the source of the streaming data so that our analytics application starts getting data.\n\nClick on Connect Streaming data.",
      "content_length": 508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "4. You can choose either an existing Kinesis stream or a Kinesis Firehose delivery stream. Alternatively, you can configure a new\n\nstream as well. We will configure a new stream here, so let's select Configure a new stream:\n\nFigure 5.36: A screenshot showing how to connect the application with the streaming\n\ndata source\n\n5. Click on Create a demo stream. This will create a new stream and populate it with sample stock ticker data:",
      "content_length": 433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Figure 5.37: A screenshot showing the creation of the demo stream\n\n6. As you can see in the following screenshot, new demo stream creation involves the following steps:\n\nCreating an IAM role, creating and setting up a new Kinesis stream, populating the new stream with data, and finally, auto-\n\ndiscovering the schema and date formats:",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Figure 5.38: A screenshot showing the status of different processes while the demo\n\nstream is being created\n\n7. Once the setup for the demo stream is complete, it gets selected as a source for the Kinesis data stream. The name of the stream in\n\nthis example is SOURCE_SQL_STREAM_001. It takes you back to choosing the streaming data source, with the newly created\n\nstream selected:",
      "content_length": 381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Figure 5.39: A screenshot displaying the name of the created stream, and its details\n\n8. Also, you will notice the sample of the data being generated by the Kinesis data stream. Please note that this schema has been auto-\n\ndiscovered by the Kinesis data analytics application. If you see any issues with the sample data or want to fix it, you can edit it or\n\nretry schema discovery.\n\nWe will keep the other options disabled for now and move on:",
      "content_length": 444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "Figure 5.40: A screenshot displaying a sample of the data generated by the stream\n\n9. Click on Save and continue and you should be redirected to the Kinesis data analytics application page. Now, the Kinesis data\n\nstream setup has been completed, and we can start configuring other settings for our data analytics application:\n\nNote\n\nYou have the option to connect reference data with the real-time streaming data. Reference data can be any of your static data or\n\noutput from other analytics, which can enrich data analytics. It can be either in JSON or CSV data format, and each data analytics\n\napplication can be attached with only one piece of reference data. We will not attach any reference data for now.",
      "content_length": 709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Figure 5.41: A screenshot displaying the READY status of the Kinesis Data Analytics\n\napplication\n\n10. Now, we will go ahead and set up real-time analytics. This will enable us to write SQL queries or use an SQL from many templates\n\nthat are available with it. Click on Go to SQL editor under Real time analytics.\n\nClick on Yes, start application in the pop-up window:",
      "content_length": 367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Figure 5.42: A screenshot showing the dialog box to start an application\n\nNow, we are in the SQL editor. Here, we can see the sample data from earlier that we configured in the Kinesis Data Stream. We\n\nwill also notice a SQL editor, where we can write SQL queries.\n\n11. You can also add SQL from templates. For our demo, we will pick on SQL from the template and fetch the real-time results:",
      "content_length": 391,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Figure 5.43: A screenshot showing the SQL editor used for writing SQL queries\n\n12. Click on Add SQL from templates and choose the second query from the left, which aggregates data in a tumbling time\n\nwindow.\n\nYou will see the SQL query on the right-hand side. Click on Add this query to the editor:",
      "content_length": 298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Figure 5.44: A screenshot showing the list of SQL queries that are generated when the\n\nAggregate function in a tumbling time window is selected\n\n13. If you see any issue with the sample data, you can click on Actions to take the appropriate step:\n\nFigure 5.45: A screenshot showing a list of different actions that can be used in case of\n\nissues with sample data\n\n14. Once your query appears in the SQL editor, click on Save and run SQL:",
      "content_length": 437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Figure 5.46: A screenshot showing the options to save and run SQL\n\n15. Once SQL is executed against the stream data, you will start to see results, as shown in the following screenshot:\n\nFigure 5.47: A screenshot showing real-time analytics once SQL is executed",
      "content_length": 261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "16. Now, the Kinesis data analytics application is running this SQL against live streaming data every 10 seconds because that is the\n\nwindow that's specified in the SQL query. You will notice a change in the results in the following screenshot as compared to our last\n\nscreenshot. This is because the results were refreshed while the screenshots were being taken:\n\nFigure 5.48: A screenshot showing a list of data that changes every 10 seconds\n\nSo, you have accomplished the task of querying the streaming data in real time, using simple standard SQL statements.\n\n17. Next, configure the destination of your real-time analysis. You can send this analysis to a Kinesis stream or a Kinesis Firehose\n\ndelivery stream, and publish it on your BI dashboards. Alternatively, you can store them in Redshift or DynamoDB using the\n\nFirehose delivery stream. Go to the Destination tab and click on Connect to a destination:",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Figure 5.49: A screenshot showing the Destination tab, where an application can be\n\nconnected to any stream\n\nAfter clicking on Destination, you should see the following screenshot:",
      "content_length": 180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Figure 5.50: A screenshot showing the different suggested destinations once the\n\nDestination tab has been selected\n\n18. Choose an existing Kinesis stream, and choose DESTINATION_SQL_STREAM for the in-application stream name; click on\n\nSave and continue.\n\nNow, you have completed the setup for the Kinesis data analytics application.\n\n19. You can review the settings for Source, Real-time analytics, and Destination on the application dashboard, as shown in the\n\nfollowing screenshot. Note that at this point, your data analytics application is running real-time analytics using SQL statements on\n\nreal-time data ingestion, which is happening via a Kinesis stream, and sending the query output to another Kinesis stream:\n\nFigure 5.51: A screenshot showing the settings for the source, real-time analytics, and\n\ndestination for the application\n\n20. Once you have collected the required information, you can click on Actions to stop the data analytics application (and later, to start\n\nit again, as required):",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Figure 5.52: A screenshot showing the status of the application once we have stopped\n\nrunning it\n\nThis concludes our exercise on the Kinesis Data Analytics application.\n\nIn our last exercise, we created a Kinesis Data Analytics stream, where we could analyze data in real time. This is very useful when you\n\nwant to understand the impact of certain data changes in real time, and make decisions for further changes. It has many real-word\n\napplications as well, such as in dynamic pricing on e-commerce websites, where you want to adjust the pricing based on the product\n\ndemand in real time.\n\nSometimes, there can be a requirement to join this real-time analysis with some reference data to create patterns within the data.\n\nAlternatively, you may just want to further enhance your real-time data with some static information to make better sense of your data.\n\nActivity 7: Adding Reference Data to the Application and Creating an Output, and Joining Real-Time Data with the Reference Data\n\nEarlier in this chapter, we saw that the Kinesis Data Analytics application provides capabilities to add reference data into existing real-\n\ntime data. In the next activity, we will enhance our test stock ticker data (that was produced natively by Kinesis Data Streams) by joining\n\nit with static data. Currently, our data contains abbreviations for company names, and we will join it with our static dataset to publish full\n\ncompany names in the query output.\n\nNote\n\nThere is a reference data file named ka-reference-data.json, which is provided in the code section. This is a JSON-formatted\n\nsample file. You can use either CSV or JSON as the format of the reference data.\n\nHere are the steps to complete this activity:\n\n1. Make sure that you have Kinesis data analytics in working condition, and that you are able to do real-time analysis, like we\n\naccomplished in the last exercise.\n\n2. Create an S3 bucket and upload the ka-reference-data.json file into the bucket.\n\n3. Go to the Kinesis data analytics application and add the reference data. Provide the bucket, S3 object, and table details, and\n\npopulate the schema using schema discovery.\n\n4. Make sure that the IAM role is configured properly.\n\n5. Now, you should have the real-time streaming data and reference data available in the Kinesis Data Analytics application.",
      "content_length": 2319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "6. Go to the SQL prompt and write the SQL statement to join the real-time streaming data with the reference data and out the company\n\ndetails whose names are provided in the reference file.\n\n7. You should be able to see the output with both the ticker symbol and the company name as an output in real time, and it should get\n\nrefreshed every few minutes.\n\nNote\n\nThe solution for this activity can be found on page 165.\n\nSummary\n\nIn this chapter, we focused on the concept of real-time data streams. We learned about the key concepts and use cases for Amazon Kinesis\n\nData Streams, Amazon Kinesis Data Firehose, and Amazon Kinesis Data Analytics. We also looked at examples of how these real-time\n\ndata streams integrate with each other and help us build real-world use cases.\n\nIn this book, we embarked on an example-driven journey of building serverless applications on AWS, applications that do not require the\n\ndevelopers to provision, scale, or manage any underlying servers. We started with an overview of traditional application deployments and\n\nchallenges associated with it and how those challenges resulted in the evolution of serverless applications. With serverless introduced, we\n\nlooked at the AWS Cloud computing platform, and focused on Lambda, the main building block of serverless models on AWS.\n\nLater, we looked at other capabilities of the AWS serverless platform, such as S3 storage, API Gateway, SNS notifications, SQS queues,\n\nAWS Glue, AWS Athena, and Kinesis applications. Using an event-driven approach, we studied the main benefits of having a serverless\n\narchitecture, and how it can be leveraged to build enterprise-level solutions. Hopefully, you have enjoyed this book and are ready to create\n\nand run your serverless applications, which will take advantage of the high availability, security, performance, and scalability of AWS. So,\n\nfocus on your product instead of worrying about managing and operating the servers to run it.\n\nOceanofPDF.com",
      "content_length": 1976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Appendix\n\nAbout\n\nThis section is included to assist the students to perform the activities in the book. It includes detailed steps that are to be performed by the\n\nstudents to achieve the objectives of the activities.",
      "content_length": 217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Chapter 1: AWS, Lambda, and Serverless Applications\n\nSolution for Activity 1: Creating a New Lambda Function That Finds the Square Root of the Average of Two Input Numbers\n\n1. Click on Create a function to create your first Lambda function on the AWS Lambda page.\n\n2. On the Create function page, select Author from scratch.\n\n3. In the Author from scratch window, fill in the following details:\n\nName: Enter myFirstLambdaFunction.\n\nRuntime: Choose Node.js 6.10. The Runtime window dropdown shows the list of languages that are supported by AWS Lambda\n\nand you can author your Lambda function code in any of the listed options. For this activity, we will author our code in Node.js.\n\nRole: Choose Create new role from template(s). In this section, you specify an IAM role.\n\nRole name: Enter lambda_basic_execution.\n\nPolicy templates: Select Simple Microservice permissions.\n\n4. Now click on Create function.\n\n5. Go to the Function code section.\n\n6. Use the Edit code inline option, and enter this code:\n\nexports.handler = (event, context, callback) => {\n\n// TODO\n\nlet first_num = 10;\n\nlet second_num = 40;\n\nlet avgNumber = (first_num+second_num)/2\n\nlet sqrtNum = Math.sqrt(avgNumber)\n\ncallback(null, sqrtNum);\n\n};\n\n7. Click on the dropdown next to Select a test event in the top-right corner of the screen and select Configure test event.\n\n8. When the popup appears, click on Create new test event and give it a name. Click on Create and the test event gets created.\n\n9. Click on the Test button next to test events and you should see the following window upon successful execution of the event:\n\nFigure 1.18: Test successful window\n\nSolution for Activity 2: Calculating the Total Lambda Cost",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "1. Note the monthly compute price and compute time provided by the Free Tier.\n\nThe monthly compute price is $0.00001667 per GB-s and the Free Tier provides 400,000 GB-s.\n\n2. Calculate the total compute time in seconds.\n\nTotal compute (seconds) = 20M * (1s) = 20,000,000 seconds\n\n3. Calculate the total compute time in GB-s.\n\nTotal compute (GB-s) = 20,000,000 * 512MB/1024 = 10,000,000 GB-s\n\n4. Calculate the monthly billable compute in GB- s. Here's the formula:\n\nMonthly billable compute (GB- s) = Total compute – Free tier compute\n\n= 10,00,000 GB-s – 400,000 Free Tier GB-s\n\n= 9,600,000 GB-s\n\n5. Calculate the monthly compute charges in dollars. Here's the formula:\n\nMonthly compute charges = Monthly billable compute (GB-s) * Monthly compute price\n\n= 9,600,000 * $0.00001667\n\n= $160.02\n\n6. Calculate the monthly billable requests. Here's the formula:\n\nMonthly billable requests = Total requests – Free tier requests\n\n= 20M requests – 1M Free Tier requests\n\n= 19M Monthly billable requests\n\n7. Calculate the monthly request charges. Here's the formula:\n\nMonthly request charges = Monthly billable requests * Monthly request price\n\n= 19M * $0.2/M\n\n= $3.8\n\n8. Calculate the total cost. Here's the formula:\n\nTotal cost = Monthly compute charge + Monthly request charges\n\n= $160.02 + $3.8\n\n= $163.82",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Chapter 2: Working with the AWS Serverless Platform\n\nSolution for Activity 3: Setting up a Mechanism to Get an Email Alert When An Object is Uploaded into an S3 Bucket\n\n1. Go to the AWS S3 service and click on Create bucket.\n\n2. Provide a Bucket name and select the region where the S3 bucket will be created. Click on Next. Note that the bucket name can't be\n\nduplicated.\n\n3. If you want to change any configuration, you can do it here. Click on Next.\n\n4. Now, you can change the settings related to the security of the S3 bucket. If you want to allow public access to the S3 bucket, you\n\ncan uncheck the options here. Click on Next.\n\n5. Review all of the configuration settings. If you want to change anything, you can go back. Alternatively, click on Finish and your\n\nbucket should be created successfully.\n\n6. Go to the Lambda function that we created in the earlier exercise. Add S3 as a trigger under the Lambda configuration section:\n\nFigure 2.54: Lambda configuration window\n\n7. Click on Configuration required and add the required details related to S3 bucket configuration, mainly the bucket name. Keep\n\nthe rest of the settings as default:",
      "content_length": 1150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Figure 2.55: Configure triggers window\n\n8. Click on Add to add that S3 bucket as a trigger to execute the Lambda function:\n\nFigure 2.56: Window showing S3 bucket being added as a trigger\n\n9. Click on Save to save the changes to the Lambda function:\n\nFigure 2.57: Window showing S3 bucket getting saved\n\n10. Also, the email message will have changed in the Lambda code to reflect our activity. See line # 8 in the following screenshot. You\n\ncan customize it based on your needs:",
      "content_length": 477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Figure 2.58: Window showing code of index.js\n\n11. Now, upload a new sample file to the S3 bucket. You should see an email alert in your mailbox.\n\n12. Go back to the Amazon S3 service, click on the bucket name, and click on the Upload button.\n\n13. Click on Add files and select the file that you want to load into the S3 bucket. Click on Next.\n\n14. Set the file level permissions. Click on Next.\n\n15. Select the storage class. You can continue with the default option. Click on Next.\n\n16. Review the configuration and click on Upload.\n\n17. The file should be uploaded successfully:\n\nFigure 2.59: Overview section of Amazon S3\n\n18. Once the file has been uploaded, go to your mailbox and you should see an email alert:\n\nFigure 2.60: Output showing a new object being uploaded to the S3 bucket\n\nThis concludes our activity.",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Chapter 3: Building and Deploying a Media Application\n\nSolution for Activity 4: Creating an API to Delete the S3 Bucket\n\n1. Go to the AWS API Gateway console and in the API created in this chapter, create a Delete API.\n\n2. Configure the incoming headers and path parameters properly in the Method Request and Integration Request sections.\n\nYour API configuration should look similar to the following screenshot:\n\nFigure 3.37: The DELETE method execution window\n\n3. Remember to change the authorization of the Delete method from NONE to AWS_IAM.\n\n4. Click on the Deploy API.\n\n5. Test the Delete method using the Test Tool (Ready API). Set content-type as application/xml:\n\nFigure 3.38: Output showing the bucket getting deleted\n\nYou should see the bucket getting deleted in the AWS S3 console.",
      "content_length": 792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Chapter 4: Serverless Amazon Athena and the AWS Glue Data Catalog\n\nSolution for Activity 5: Building a AWS Glue catalog for a CSV- Formatted Dataset and Analyzing the Data Using AWS Athena\n\n1. Log in to your AWS account.\n\n2. Upload the data file total-business-inventories-to-sales-ratio.csv (provided with this book) into a S3 bucket.\n\nMake sure that the required permissions are in place:\n\nFigure 4.24: Uploading the data file\n\n3. Go to the AWS Glue service.\n\n4. Select Crawlers and click on Add Crawler.\n\n5. Provide the crawler name and click on Next.\n\n6. Provide the path of the S3 bucket, where the file was uploaded in step 2. Click on Next.\n\n7. Click on Next, as we don't want to add another data store.\n\n8. Choose an existing IAM role that was created in Exercise 11: Using AWS Glue to Build a Metadata Repository. Alternatively, you\n\ncan create a new one. Click on Next.\n\n9. Let's keep it as Run on demand and click on Next.\n\n10. Either you can create a new database here or click on the dropdown to select an existing one. Click on Next.\n\n11. Review the settings and click on Finish. You have successfully created the crawler.\n\n12. Now, go ahead and run the crawler.\n\n13. Once the run of the crawler is completed, you will see a new table being created under the schema that you chose in step 10:",
      "content_length": 1306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "Figure 4.25: The new table after the crawler run was completed\n\n14. Go to tables, and you should see the newly created table, inventory_sales_ratio. Note that the table name is derived from\n\nthe bucket name.\n\n15. Go to the AWS Athena service. You should see a new table name under the database that was selected in step 10.\n\n16. Click on new query and write the following query to get the expected output:\n\nselect month(try(date_parse(observed_date, '%m/%d/%Y'))) a, count(*) from\n\ninventory_sales_ratio\n\nwhere observed_value < 1.25 group by month(try(date_parse(observed_date, '%m/%d/%Y')))\n\norder by a ;\n\n17. When the query gets executed, you should see the expected output:",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Figure 4.26: The output after the query has run\n\n18. Looking at the output, we have a total of 8 months since 1992 where the inventories to sales ratios was < 1.25. We also have the\n\nmonth level count as well.\n\nWe have successfully completed the activity.",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Chapter 5: Real-Time Data Insights Using Amazon Kinesis\n\nSolution for Activity 6: Performing Data Transformations for Incoming Data\n\n1. Start by creating a Kinesis Firehose Data Stream and follow the steps that we completed in the last exercise.\n\n2. We disabled data transformation using Lambda in the last exercise. This time, enable the Transform source records with AWS\n\nLambda option.\n\n3. Once enabled, create a Lambda function to do the data transformation for incoming data:\n\nFigure 5.54: The Transform source records with AWS Lambda window\n\n4. There are already some sample functions that have been provided by Amazon. You can click on Create New and it will open up the\n\nlist of transformation functions provided by AWS. Let's choose General Firehose Processing:\n\nFigure 5.55: The Choose Lambda blueprint window",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "5. This opens up the Lambda function window. Here, you need to provide the name of the function, along with the IAM role\n\ninformation:\n\nFigure 5.56: The Basic information window\n\n6. Edit the code inline and replace the existing code with the code provided in the json2csv_transform.js file, under the code\n\nsection. Keep the rest of the settings as is:\n\nFigure 5.57: Window showing code of index.js\n\n7. Once the Lambda function has been created, go back to the Firehose screen and configure the rest of the settings, such as the\n\nAmazon S3 bucket, which will work the same as the Firehose destination that we configured in the last exercise:",
      "content_length": 641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Figure 5.58: The Convert record format window\n\n8. Also, once the Lambda function has been created, update the IAM role in the Firehose configuration to reflect the required access\n\nfor the Lambda function:\n\nFigure 5.59: The Test with demo data window\n\n9. Everything else remains the same as in the last exercise.\n\n10. Send the test data from the Test with demo data section by clicking on Start sending demo data:\n\nFigure 5.60: Window showing the Start sending demo data button\n\n11. Go to the S3 location that we configured earlier to receive the data and you should see the data file, as shown here:",
      "content_length": 600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Figure 5.61: Window showing the data file added successfully\n\n12. Upon downloading this data file and opening it with Notepad, you should see the data in CSV format, as shown here:\n\nFigure 5.62: Screenshot showing data in the CSV format\n\nSolution for Activity 7: Adding Reference Data to the Application and Creating an Output, Joining Real-Time Data with the Reference Data\n\n1. Ensure that you have Kinesis Data Analytics in working condition and that you are able to do real-time analysis, like we\n\naccomplished in the last exercise:",
      "content_length": 535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Figure 5.63: The kinesis-data-analytics page\n\n2. Create a S3 bucket and upload the ka-reference-data.json file into the bucket:\n\nFigure 5.64: Screenshot showing the ka-reference-data.json file added to the S3 bucket\n\n3. Go to the Kinesis Data Analytics application page and click on Connect reference data. Provide the bucket, S3 object, and table\n\ndetails, and populate the schema using schema discovery:",
      "content_length": 405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Figure 5.65: The Connect reference data source page\n\nYou will notice in the preceding screenshot that the Kinesis application will create the IAM role with required access.\n\nSchema discovery will detect the schema for the reference data file and show you the sample data:\n\nFigure 5.66: The Schema section\n\n4. Click on Save and close button. You will have successfully added the referenced data:",
      "content_length": 394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Figure 5.67: Page showing the referenced data added successfully\n\nNow, you should have the real-time streaming data and reference data available in the Kinesis Data Analytics application. The\n\nfollowing screenshot is showing real-time streaming data: The following image is showing the added reference data:\n\nFigure 5.68: The Real-time analytics section",
      "content_length": 353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Figure 5.69: The Source data section\n\n5. Go to the SQL prompt and write the SQL statement to join real-time streaming data with the reference data, and out the company\n\ndetails whose names are provided in the reference file.\n\n6. Run the following query in the SQL prompt. In this query, we are joining (left join) SOURCE_SQL_STREAM_001 with the\n\nka_reference_data dataset and filtering where company name is not null:\n\nCREATE STREAM \"KINESIS_SQL_STREAM\" (ticker_symbol VARCHAR(14), \"Company_Name\"\n\nvarchar(30), sector VARCHAR(22), change DOUBLE, price DOUBLE);\n\nCREATE PUMP \"STREAM_PUMP\" AS INSERT INTO \"KINESIS_SQL_STREAM\"\n\nSELECT STREAM ticker_symbol, \"kar\".\"Company\", sector, change, price\n\nFROM \"SOURCE_SQL_STREAM_001\" LEFT JOIN \"ka_reference_data\" as \"kar\"\n\nON \"SOURCE_SQL_STREAM_001\".ticker_symbol = \"kar\".\"Ticker\"\n\nwhere \"kar\".\"Company\" is not null ;\n\nNote\n\nYou can use the inner join while removing the where clause to achieve the same results.",
      "content_length": 952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Figure 5.70: The result page for real-time analytics\n\n7. You should be able to see the output with both the ticker symbol and company name as output in real-time. It should get refreshed\n\nevery few minutes:\n\nFigure 5.71: Output showing both the ticker symbol and company name\n\nThis concludes our activity on adding reference data and using it to perform real-time data analytics on Amazon Kinesis Data Analytics.\n\nOceanofPDF.com",
      "content_length": 428,
      "extraction_method": "Unstructured"
    }
  ]
}