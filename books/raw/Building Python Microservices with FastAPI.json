{
  "metadata": {
    "title": "Building Python Microservices with FastAPI",
    "author": "Sherwin John C. Tragura",
    "publisher": "Apress",
    "edition": "1st Edition",
    "isbn": "978-1484271506",
    "total_pages": 420,
    "conversion_date": "2025-11-08T12:43:00.336230",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Building Python Microservices with FastAPI.pdf"
  },
  "chapters": [],
  "pages": [
    {
      "page_number": 1,
      "chapter": null,
      "content": "Building Python\nMicroservices\nwith FastAPI\n\nBuild secure, scalable, and structured Python\nmicroservices from design concepts to infrastructure\n\nS SHERWIN JOHN C. TRAGURA",
      "content_length": 169,
      "extraction_method": "OCR"
    },
    {
      "page_number": 2,
      "chapter": null,
      "content": "Building Python Microservices with \nFastAPI\nBuild secure, scalable, and structured Python microservices \nfrom design concepts to infrastructure\nSherwin John C. Tragura\nBIRMINGHAM—MUMBAI",
      "content_length": 185,
      "extraction_method": "Direct"
    },
    {
      "page_number": 3,
      "chapter": null,
      "content": "Building Python Microservices with FastAPI\t\nCopyright © 2022 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted \nin any form or by any means, without the prior written permission of the publisher, except in the case \nof brief quotations embedded in critical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information \npresented. However, the information contained in this book is sold without warranty, either express \nor implied. Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable \nfor any damages caused or alleged to have been caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the companies and \nproducts mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot \nguarantee the accuracy of this information.\nAssociate Group Product Manager: Pavan Ramchandani\nPublishing Product Manager: Bhavya Rao\nSenior Editor: Mark Dsouza\nContent Development Editor: Divya Vijayan\nTechnical Editor: Shubham Sharma\nCopy Editor: Safis Editing\nProject Coordinator: Sonam Pandey\nProofreader: Safis Editing\nIndexer: Tejal Daruwale Soni\nProduction Designer: Joshua Misquitta\nMarketing Coordinators: Anamika Singh and Marylou De Mello\nFirst published: August 2022\nProduction reference: 1260822\nPublished by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham\nB3 2PB, UK.\nISBN 978-1-80324-596-6\t\nwww.packt.com",
      "content_length": 1585,
      "extraction_method": "Direct"
    },
    {
      "page_number": 4,
      "chapter": null,
      "content": "This book is for everyone who became my support system during those months \nin 2021 when I got sick and for everyone who fought and stayed strong during \nthis pandemic.\n– Sherwin John C. Tragura",
      "content_length": 194,
      "extraction_method": "Direct"
    },
    {
      "page_number": 5,
      "chapter": null,
      "content": "Contributors\nAbout the author\nSherwin John Calleja Tragura is a subject matter expert on Java, ASP .NET MVC, and Python \napplications with some background in frontend frameworks. He has managed a team of developers to \nbuild various applications related to manufacturing and fixed assets, document management, records \nmanagement, POS, and inventory systems. He has a background in building laboratory information \nmanagement systems (LIMS) and hybrid mobile applications as a consultant. He has also provided \ncorporate Bootcamp training services since 2010 for courses on Python, Django, Flask, Jakarta EE, \nC#, ASP .NET MVC, JSF, Java, and some frontend frameworks. He has authored books such as Spring \nMVC Blueprints and Spring 5 Cookbook and a Packt video, Modern Java Web Applications with Spring \nBoot 2.x.\nI want to thank my friends, Icar, Mathieu, and Abby, and my cousins, \nRhonalyn, Mica, and Mila, for their time and effort when I was not well in \n2021. Also, I want to express my gratitude and appreciation to the Packt team \nfor their understanding and consideration given to me. Lastly, this book would \nhave been impossible without the support and time of Owen, who was always \nthere pushing me to finish the book despite the scary situation during \nthe pandemic.",
      "content_length": 1280,
      "extraction_method": "Direct"
    },
    {
      "page_number": 6,
      "chapter": null,
      "content": "About the reviewers\nGlenn Base De Paula is a product of the University of the Philippines Integrated School and is \na computer science graduate of the country’s most prestigious institutions, the University of the \nPhilippines. He has 14 years of industry experience, which was mostly with the government’s ICT \ninstitute and recently in the banking industry.\nHe uses Spring, Grails, and JavaScript for his day-to-day activities. He has developed numerous Java \nweb applications for various projects and was also the technical team lead for several others. He \ncurrently manages a team of Java developers assigned to different projects in one of the country’s \nmost reputable banks.\nHe is often involved in systems analysis and design, source code review, testing, implementation, \ntraining, and mentoring. He learns about better software architecture and design in his free time.\nSathyajith Bhat is an experienced Site Reliability Engineer (SRE) with over 15 years of experience in \nDevOps, Site Reliability Engineering, System Architecture, Performance Tuning, Cloud Computing, \nand Observability. He believes in getting the most out of the tools that he works with. Sathyajith is \nthe author of Practical Docker with Python and a co-author of The CDK Book. He loves working with \nvarious communities and has been recognized as an AWS Community Hero for his contributions to \nthe AWS community.\nI would like to thank my wife, Jyothsna, for being patient and supporting me \nboth in my career and while working on this book.",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 7,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 8,
      "chapter": null,
      "content": "Table of Contents\nPreface\t\nxvii\nPart 1: Application-Related Architectural \nConcepts for FastAPI microservice development\n1\nSetting Up FastAPI for Starters\t\n3\nTechnical requirements\b\n4\nSetting up the development \nenvironment\b\n4\nInitializing and configuring \nFastAPI\b\n5\nDesigning and implementing \nREST APIs\b\n6\nManaging user requests and server \nresponse\b\n10\nParameter type declaration\b\n10\nPath parameters\b\n10\nQuery parameters\b\n12\nDefault parameters\b\n14\nOptional parameters\b\n15\nMixing all types of parameters\b\n16\nRequest body\b\n17\nRequest headers\b\n19\nResponse data\b\n20\nHandling form parameters\b\n22\nManaging cookies\b\n23\nSummary\b\n24\n2\nExploring the Core Features\t\n25\nTechnical requirements\b\n26\nStructuring and organizing \nhuge projects\b\n26\nImplementing the API services\b\n27\nImporting the module components\b\n28\nImplementing the new main.py file\b\n29\nManaging API-related \nexceptions\b\n30",
      "content_length": 879,
      "extraction_method": "Direct"
    },
    {
      "page_number": 9,
      "chapter": null,
      "content": "Table of Contents\nviii\nA single status code response\b\n30\nMultiple status codes\b\n32\nRaising HTTPException\b\n33\nCustom exceptions\b\n34\nA default handler override\b\n37\nConverting objects to JSON-\ncompatible types\b\n38\nManaging API responses\b\n39\nCreating background \nprocesses\b\n41\nUsing asynchronous path \noperations\b\n43\nApplying middleware to filter \npath operations\b\n44\nSummary\b\n46\n3\nInvestigating Dependency Injection\t\n47\nTechnical requirements\b\n48\nApplying IoC/DI\b\n48\nInjecting a dependency function\b\n49\nInjecting a callable class\b\n50\nBuilding nested dependencies\b\n51\nCaching the dependencies\b\n53\nDeclaring Depends() parameter \ntypes\b\n54\nInjecting asynchronous \ndependencies\b\n55\nExploring ways of injecting \ndependencies\b\n56\nDependency injection on services\b\n56\nDependency injection on path \noperators\b\n57\nDependency injection on routers\b\n58\nDependency injection on main.py\b\n61\nOrganizing a project based \non dependencies\b\n62\nThe model layer\b\n63\nThe repository layer\b\n64\nThe repository factory methods\b\n66\nThe service layer\b\n67\nThe REST API and the service layer\b\n67\nThe actual project structure\b\n68\nUsing third-party containers\b\n69\nUsing configurable containers \n– Dependency Injector\b\n69\nUsing a simple configuration \n– Lagom\b\n74\nThe FastAPI and Lagom \nintegration\b\n74\nScoping of dependables\b\n75\nSummary\b\n76\n4\nBuilding the Microservice Application\t\n77\nTechnical requirements\b\n78\nApplying the decomposition \npattern\b\n78\nCreating the sub-applications\b\n80\nMounting the submodules\b\n81\nCreating a common gateway\b\n82",
      "content_length": 1508,
      "extraction_method": "Direct"
    },
    {
      "page_number": 10,
      "chapter": null,
      "content": "Table of Contents \nix\nImplementing the main \nendpoint\b\n82\nEvaluating the microservice \nID\b\n83\nApplying the exception \nhandlers\b\n84\nCentralizing the logging \nmechanism\b\n86\nUtilizing the Loguru module\b\n86\nBuilding the logging \nmiddleware\b\n88\nConsuming the REST API \nservices\b\n90\nUsing the httpx module\b\n91\nUsing the requests module\b\n92\nApplying the domain \nmodeling approach\b\n94\nCreating the layers\b\n94\nIdentifying the domain \nmodels\b\n95\nBuilding the repository \nand service layers\b\n97\nUsing the factory method \npattern\b\n100\nManaging a microservice’s \nconfiguration details\b\n100\nStoring settings as class \nattributes\b\n101\nStoring settings in the properties \nfile\b\n102\nSummary\b\n104\nPart 2: Data-Centric and Communication-Focused \nMicroservices Concerns and Issues\n5\nConnecting to a Relational Database\t\n107\nTechnical requirements\b\n108\nPreparing for database \nconnectivity\b\n108\nCreating CRUD transactions \nusing SQLAlchemy\b\n110\nInstalling the database driver\b\n110\nSetting up the database connection\b\n110\nInitializing the session factory\b\n111\nDefining the Base class\b\n112\nBuilding the model layer\b\n112\nImplementing the repository layer\b\n116\nRunning the transactions\b\n120\nCreating tables \b\n123\nImplementing async \nCRUD transactions using \nSQLAlchemy\b\n124\nInstalling the asyncio-compliant \ndatabase drivers\b\n124\nSetting up the database’s \nconnection\b\n124\nCreating the session factory\b\n125\nCreating the Base class and the \nmodel layer\b\n125\nBuilding the repository layer\b\n126",
      "content_length": 1466,
      "extraction_method": "Direct"
    },
    {
      "page_number": 11,
      "chapter": null,
      "content": "Table of Contents\nx\nRunning the CRUD \ntransactions\b\n129\nUsing GINO for async \ntransactions\b\n130\nInstalling the database driver\b\n131\nEstablishing the database \nconnection\b\n131\nBuilding the model layer\b\n131\nImplementing the CRUD \ntransactions\b\n135\nRunning the CRUD transactions\b\n138\nCreating the tables\b\n139\nUsing Pony ORM for the \nrepository layer\b\n140\nInstalling the database driver\b\n140\nCreating the database’s \nconnectivity\b\n141\nDefining the model classes\b\n141\nImplementing the CRUD \ntransactions\b\n144\nRunning the repository \ntransactions\b\n148\nCreating the tables\b\n148\nBuilding the repository using \nPeewee\b\n148\nInstalling the database driver\b\n148\nCreating the database connection\b\n149\nCreating the tables and the \ndomain layer\b\n150\nImplementing the CRUD \ntransactions\b\n153\nRunning the CRUD transaction\b\n155\nApplying the CQRS design \npattern\b\n156\nDefining the handler interfaces\b\n156\nCreating the command and query \nclasses\b\n156\nCreating the command and query \nhandlers\b\n157\nAccessing the handlers\b\n158\nSummary\b\n160\n6\nUsing a Non-Relational Database\t\n161\nTechnical requirements\b\n162\nSetting up the database \nenvironment\b\n162\nApplying the PyMongo driver \nfor synchronous connections\b\n165\nSetting up the database connectivity\b\n165\nBuilding the model layer\b\n166\nImplementing the repository layer\b\n174\nRunning the transactions\b\n179\nCreating async CRUD \ntransactions using Motor\b\n182\nSetting up the database \nconnectivity\b\n182\nCreating the model layer\b\n183\nRunning the CRUD transactions\b\n185\nImplementing CRUD \ntransactions using \nMongoEngine\b\n186\nEstablishing database connection\b\n187\nBuilding the model layer\b\n187\nImplementing the CRUD \ntransactions\b\n190\nRunning the CRUD transactions\b\n194",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 12,
      "chapter": null,
      "content": "Table of Contents \nxi\nImplementing async \ntransactions using Beanie\b\n195\nCreating the database connection\b\n195\nDefining the model classes\b\n196\nImplementing the CRUD \ntransactions\b\n198\nRunning the repository \ntransactions\b\n200\nBuilding async repository for \nFastAPI using ODMantic\b\n200\nCreating the database connection\b\n201\nCreating the model layer\b\n201\nImplementing the CRUD \ntransactions\b\n202\nRunning the CRUD transaction\b\n205\nCreating CRUD transactions \nusing MongoFrames\b\n206\nCreating the database connection\b\n206\nBuilding the model layer\b\n207\nCreating the repository layer\b\n208\nApplying the repository layer\b\n211\nSummary\b\n212\n7\nSecuring the REST APIs\t\n213\nTechnical requirements\b\n214\nImplementing Basic and Digest \nauthentication\b\n214\nUsing Basic authentication\b\n214\nUsing Digest authentication\b\n218\nImplementing password-based \nauthentication\b\n222\nInstalling the python-multipart \nmodule\b\n222\nUsing OAuth2PasswordBearer and \nOAuth2PasswordRequestForm\b\n222\nExecuting the login transaction\b\n223\nSecuring the endpoints\b\n226\nApplying JWTs\b\n228\nGenerating the secret key\b\n228\nCreating the access_token\b\n228\nCreating the login transaction\b\n229\nAccessing the secured endpoints\b\n230\nCreating scope-based \nauthorization\b\n231\nCustomizing the OAuth2 class\b\n231\nBuilding the permission dictionary\b\n232\nImplementing the login transaction\b\n233\nApplying the scopes to endpoints\b\n234\nBuilding the authorization \ncode flow\b\n236\nApplying OAuth2Authorization\nCodeBearer\b\n236\nImplementing the authorization \nrequest\b\n237\nImplementing the authorization \ncode response\b\n238\nApplying the OpenID \nConnect specification\b\n240\nUsing HTTPBearer\b\n240\nInstalling and configuring the \nKeycloak environment\b\n240\nSetting the Keycloak realm and \nclients\b\n240\nCreating users and user roles\b\n242\nAssigning roles to clients\b\n243\nCreating user permissions through \nscopes\b\n244",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 13,
      "chapter": null,
      "content": "Table of Contents\nxii\nIntegrating Keycloak with FastAPI\b\n245\nImplementing the token \nverification\b\n247\nIntegrating Auth0 with FastAPI\b\n249\nIntegrating Okta with FastAPI\b\n250\nUsing built-in middlewares \nfor authentication\b\n250\nSummary\b\n251\n8\nCreating Coroutines, Events, and Message-Driven Transactions\t\n253\nTechnical requirements\b\n254\nImplementing coroutines\b\n254\nApplying coroutine switching\b\n254\nDesigning asynchronous \ntransactions\b\n258\nUsing the HTTP/2 protocol\b\n261\nCreating asynchronous \nbackground tasks\b\n261\nUsing the coroutines\b\n261\nCreating multiple tasks\b\n263\nUnderstanding Celery \ntasks\b\n263\nCreating and configuring the \nCelery instance\b\n264\nCreating the task\b\n266\nCalling the task\b\n267\nStarting the worker server\b\n267\nMonitoring the tasks\b\n268\nBuilding message-driven \ntransactions using \nRabbitMQ\b\n269\nCreating the Celery instance\b\n269\nMonitoring AMQP messaging\b\n269\nBuilding publish/subscribe \nmessaging using Kafka\b\n270\nRunning the Kafka broker and server\b\n271\nCreating the topic\b\n271\nImplementing the publisher\b\n271\nRunning a consumer on a console\b\n272\nImplementing asynchronous \nServer-Sent Events (SSE)\b\n273\nBuilding an asynchronous \nWebSocket\b\n275\nImplementing the asynchronous \nWebSocket endpoint\b\n275\nImplementing the WebSocket \nclient\b\n276\nApplying reactive \nprogramming in tasks\b\n277\nCreating the Observable data \nusing coroutines\b\n278\nCreating background process\b\n280\nAccessing API resources\b\n281\nCustomizing events\b\n283\nDefining the startup event\b\n283\nDefining shutdown events\b\n284\nSummary\b\n284",
      "content_length": 1521,
      "extraction_method": "Direct"
    },
    {
      "page_number": 14,
      "chapter": null,
      "content": "Table of Contents \nxiii\nPart 3: Infrastructure-Related Issues, Numerical \nand Symbolic Computations, and Testing \nMicroservices\n9\nUtilizing Other Advanced Features\t\n287\nTechnical requirements\b\n288\nApplying session \nmanagement\b\n288\nCreating user sessions\b\n288\nManaging session data\b\n291\nRemoving the sessions\b\n292\nCustomizing BaseHTTPMiddleware\b\n293\nManaging the CORS \nmechanism\b\n295\nCustomizing APIRoute and \nRequest\b\n297\nManaging body, form, or JSON data\b\n297\nEncrypting and decrypting the \nmessage body\b\n300\nChoosing the appropriate \nresponses\b\n302\nSetting up the Jinja2 template \nengine\b\n307\nSetting up the static resources\b\n307\nCreating the template layout\b\n308\nUsing ORJSONResponse and \nUJSONResponse\b\n310\nApplying the OpenAPI 3.x \nspecification\b\n311\nExtending the OpenAPI schema \ndefinition\b\n311\nUsing the internal code base \nproperties\b\n314\nUsing the Query, Form, Body, \nand Path functions\b\n316\nTesting the API endpoints\b\n320\nWriting the unit test cases\b\n321\nMocking the dependencies\b\n321\nRunning test methods\b\n323\nSummary\b\n324\n10\nSolving Numerical, Symbolic, and Graphical Problems\t\n325\nTechnical requirements\b\n326\nSetting up the projects\b\n326\nUsing the Piccolo ORM \b\n326\nThe Beanie ODM\b\n331\nImplementing symbolic \ncomputations\b\n331\nCreating symbolic expressions\b\n331\nSolving linear expressions\b\n332",
      "content_length": 1307,
      "extraction_method": "Direct"
    },
    {
      "page_number": 15,
      "chapter": null,
      "content": "Table of Contents\nxiv\nSolving non-linear expressions\b\n333\nSolving linear and non-linear \ninequalities\b\n333\nCreating arrays and \nDataFrames\b\n334\nApplying NumPy’s linear system \noperations\b\n335\nApplying the pandas module\b\n336\nPerforming statistical \nanalysis\b\n337\nGenerating CSV and XLSX \nreports\b\n338\nPlotting data models\b\n342\nSimulating a BPMN \nworkflow\b\n346\nDesigning the BPMN workflow\b\n346\nImplementing the workflow\b\n348\nUsing GraphQL queries \nand mutations\b\n350\nSetting up the GraphQL platform\b\n350\nCreating the record insertion, \nupdate, and deletion\b\n350\nImplementing the query \ntransactions\b\n353\nRunning the CRUD transactions\b\n353\nUtilizing the Neo4j graph \ndatabase\b\n355\nSetting the Neo4j database\b\n356\nCreating the CRUD transactions\b\n356\nSummary\b\n360\n11\nAdding Other Microservice Features\t\n361\nTechnical requirements\b\n362\nSetting up the virtual \nenvironment\b\n362\nChecking the API \nproperties\b\n364\nImplementing open tracing \nmechanisms\b\n365\nSetting up service registry \nand client-side service \ndiscovery\b\n369\nImplementing client-side service \ndiscovery\b\n370\nSetting up the Netflix Eureka \nservice registry\b\n371\nDeploying and running \napplications using Docker\b\n372\nGenerating the requirements.txt file\b\n372\nCreating the Docker image \b\n373\nUsing the Mongo Docker image\b\n374\nCreating the containers\b\n374\nUsing Docker Compose for \ndeployment\b\n375\nUsing NGINX as an API \nGateway\b\n376\nIntegrating Flask and \nDjango sub-applications\b\n377\nSummary\b\n380",
      "content_length": 1452,
      "extraction_method": "Direct"
    },
    {
      "page_number": 16,
      "chapter": null,
      "content": "Table of Contents \nxv\nIndex\t\n381\nOther Books You May Enjoy\t\n394",
      "content_length": 63,
      "extraction_method": "Direct"
    },
    {
      "page_number": 17,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 18,
      "chapter": null,
      "content": "Preface\nThis book teaches you about the components of the FastAPI framework and how to apply these \ncomponents with some third-party tools to build microservices applications. You will need a background \nin Python programming, knowledge of the principles of API development, and an understanding \nof the principles behind creating an enterprise-grade microservice application. This is more than a \nreference book: it provides some code blueprints that will help you solve real-world applications while \nelaborating on and demonstrating the topics of each chapter.\nWho this book is for\nThis book is for Python web developers, advanced Python users, and backend developers using Flask \nor Django who want to learn how to use the FastAPI framework to implement microservices. Readers \nknowledgeable about REST API and microservices will also benefit from this book. Some parts of the \nbook contain general concepts, processes, and instructions that intermediate-level developers and \nPython enthusiasts can relate to as well.\nWhat this book covers\nChapter 1, Setting Up FastAPI for Starters, introduces how to create FastAPI endpoints using the \ncore module classes and decorators and how the framework manages incoming API requests and \noutgoing responses.\nChapter 2, Exploring the Core Features, introduces FastAPI’s asynchronous endpoints, exception \nhandling mechanism, background processes, APIRouter for project organization, the built-in JSON \nencoder, and FastAPI’s JSON responses.\nChapter 3, Investigating Dependency Injection, explores the Dependency Injection (DI) pattern \nutilized by FastAPI to manage instances and project structure using its Depends() directive and \nthird-party extension modules.\nChapter 4, Building the Microservice Application, is on the principles and design patterns that support \nthe building of microservices, such as decomposition, property configuration, logging, and domain \nmodeling strategy.\nChapter 5, Connecting to a Relational Database, focuses on Python Object Relational Mappers (ORMs), \nwhich can integrate seamlessly with FastAPI to persist and manage data using a PostgreSQL database.",
      "content_length": 2133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 19,
      "chapter": null,
      "content": "Preface\nxviii\nChapter 6, Using a Non-Relational Database, showcases the PyMongo and Motor engines, including \nsome popular Python Object Document Mapper (ODMs), which can connect FastAPI applications \nto a MongoDB server.\nChapter 7, Securing the REST APIs, highlights FastAPI’s built-in security module classes and explores \nsome third-party tools such as JWT, Keycloak, Okta, and Auth0 and how they are applied to implement \ndifferent security schemes to secure an application.\nChapter 8, Creating Coroutines, Events, and Message-Driven Transactions, focuses on the details of \nthe asynchronous aspect of the FastAPI, such as the use of coroutines, the asyncio environment, \nasynchronous background processes using Celery, asynchronous messaging using RabbitMQ and \nApache Kafka, SSE, WebSocket, and asynchronous events.\nChapter 9, Utilizing Other Advanced Features, contains other features that FastAPI can provide, such \nas its support for different response types, the customization of middleware, request and response, \nthe application of other JSON encoders, and the bypassing of the CORS browser policy.\nChapter 10, Solving Numerical, Symbolic, and Graphical Problems, highlights the integration of FastAPI \nwith the numpy, pandas, matplotlib, sympy, and scipy modules to implement API services \nthat can perform numerical and symbolic computations to solve mathematical and statistical problems.\nChapter 11, Adding Other Microservice Features, discusses other architectural concerns, such as \nmonitoring and checking the properties of API endpoints at runtime, OpenTracing, client-side \nservice discovery, managing repository modules, deployment, and creating monorepo architectures \nwith Flask and Django apps.\nTo get the most out of this book\nThis book requires some experience with Python programming using Python 3.8 or 3.9, as well as \nsome API development experience with any Python framework. Knowledge of the standards and best \npractices of coding Python, including some advanced topics such as creating decorators, generators, \ndatabase connectivity, request-response transactions, HTTP status codes, and API endpoints, is required.",
      "content_length": 2150,
      "extraction_method": "Direct"
    },
    {
      "page_number": 20,
      "chapter": null,
      "content": "Download the example code files\nxix\nOpen an account in Okta and Auth0 for the OpenID connect security scheme. Both prefer a company \nemail for signing up.\nIf you are using the digital version of this book, we advise you to type the code yourself or access \nthe code from the book’s GitHub repository (a link is available in the next section). Doing so will \nhelp you avoid any potential errors related to the copying and pasting of code.\nEach chapter has a dedicated project prototype that will describe and explain the topics. If you get \nlost during setup, each project has a backed-up database (.sql or .zip) and a list of modules \n(requirements.txt) to fix some issues. Run the \\i PostgreSQL command to install the script \nfile or use the mongorestore from the installed Mongo Database Tools to load all the database content. \nAlso, each project has a mini-readme that gives a general description of what the prototype wants \nto accomplish.\nDownload the example code files\nYou can download the example code files for this book from GitHub at https://github.com/\nPacktPublishing/Building-Python-Microservices-with-FastAPI. If there’s an \nupdate to the code, it will be updated in the GitHub repository.\nWe also have other code bundles from our rich catalog of books and videos available at https://\ngithub.com/PacktPublishing/. Check them out!\nDownload the color images\nWe also provide a PDF file that has color images of the screenshots and diagrams used in this book. \nYou can download it here: https://packt.link/ohTNw.",
      "content_length": 1525,
      "extraction_method": "Direct"
    },
    {
      "page_number": 21,
      "chapter": null,
      "content": "Preface\nxx\nConventions used\nThere are a number of text conventions used throughout this book.\nCode in text: Indicates code words in text, database table names, folder names, filenames, file \nextensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: \"The \ndelete_user() service is a DELETE API method that uses a username path parameter to \nsearch for a login record for deletion.\"\nA block of code is set as follows:\n@app.delete(\"/ch01/login/remove/{username}\")\ndef delete_user(username: str):\n    del valid_users[username]\n    return {\"message\": \"deleted user\"}\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or items \nare set in bold:\n@app.get(\"/ch01/login/\")\ndef login(username: str, password: str):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    else:\n        user = valid_users.get(username)\nAny command-line input or output is written as follows:\npip install fastapi\npip install uvicorn[standard]\nBold: Indicates a new term, an important word, or words that you see onscreen. For instance, \nwords in menus or dialog boxes appear in bold. Here is an example: \"Select System info from the \nAdministration panel.\"\nTips or Important Notes\t\nAppear like this.",
      "content_length": 1283,
      "extraction_method": "Direct"
    },
    {
      "page_number": 22,
      "chapter": null,
      "content": "Get in touch\nxxi\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: If you have questions about any aspect of this book, email us at customercare@\npacktpub.com and mention the book title in the subject of your message.\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. \nIf you have found a mistake in this book, we would be grateful if you would report this to us. Please \nvisit www.packtpub.com/support/errata and fill in the form.\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would \nbe grateful if you would provide us with the location address or website name. Please contact us at \ncopyright@packt.com with a link to the material.\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you \nare interested in either writing or contributing to a book, please visit authors.packtpub.com.\nShare your thoughts\nOnce you’ve read Building Python Microservices with FastAPI, we’d love to hear your thoughts! Please \nclick here to go straight to the Amazon review page for this book and share your feedback.\nYour review is important to us and the tech community and will help us make sure we’re delivering \nexcellent quality content.",
      "content_length": 1291,
      "extraction_method": "Direct"
    },
    {
      "page_number": 23,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 24,
      "chapter": null,
      "content": "Part 1: \nApplication-Related \nArchitectural Concepts \nfor FastAPI microservice \ndevelopment\nIn this part, we will look at the whole FastAPI framework and explore the systematic and ideal way \nof decomposing a monolithic application into several business units. During the process, you will see \nhow to get started with development and what components there are in FastAPI that can be utilized \nto pursue microservices implementation. \nThis part comprises the following chapters:\n•\t Chapter 1, Setting Up FastAPI for Starters \n•\t Chapter 2, Exploring the Core Components \n•\t Chapter 3, Investigating Dependency Injection \n•\t Chapter 4, Building the Microservice Application",
      "content_length": 672,
      "extraction_method": "Direct"
    },
    {
      "page_number": 25,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 26,
      "chapter": null,
      "content": "1\nSetting Up FastAPI \nfor Starters\nIn any software development work, it is always important to first know the business requirement of \nthe project and the appropriate framework, tools, and deployment platform to use before pursuing the \ntask. Frameworks that are easy to understand and use, seamless during coding, and within standards \nare always picked because of the integrity they provide to solve problems without risking too much \ndevelopment time. And a promising Python framework called FastAPI, created by Sebastian Ramirez, \nprovides experienced developers, experts, and enthusiasts the best option for building REST APIs \nand microservices.\nBut before proceeding to the core details of building microservices using FastAPI, it is best to first \nlearn the building blocks of this framework, such as how it captures clients’ requests, how it builds the \nrules for each HTTP method, and how it manages HTTP responses. Learning the basic components \nis always essential to know the strengths and weaknesses of the framework and to what extent we can \napply FastAPI to solve different enterprise-grade and microservices-related problems.\nThus, in this chapter, we’re going to have a walkthrough of the basic features of FastAPI by covering \nthe following main topics:\n•\t The setup of the development environment\n•\t Initialization and configuration of FastAPI \n•\t Design and implementation of the REST APIs\n•\t Managing user requests and server response\n•\t Handling form parameters\n•\t Handling cookies",
      "content_length": 1505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 27,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n4\nTechnical requirements\nThe software specimen for this chapter is a prototypical administrator-managed online academic \ndiscussion forum, which is an academic discussion hub where alumni, teachers, and students can \nexchange ideas. The prototype is working but it is open for changes, so you can tweak the code while \nreading this chapter. It is not designed to use any database management system, but all the data is \ntemporarily stored in various Python collections. All the applications in this book are compiled and \nrun using Python 3.8. Codes are all uploaded at https://github.com/PacktPublishing/\nBuilding-Python-Microservices-with-FastAPI/tree/main/ch01.\nSetting up the development environment\nThe FastAPI framework is a fast, seamless, and robust Python framework but can only work on Python \nversions 3.6 and above. The Integrated Development Environment (IDE) used in this reference is \nVisual Studio Code (VS Code), which is an open source tool that we can download from this site: \nhttps://code.visualstudio.com/. Just be sure to install the VSC extensions such as Python, \nPython for VS Code, Python Extension Pack, Python Indent, and Material Icon Theme to provide your \neditor syntax checking, syntax highlighting, and other editor support.\nAfter the successful installation of Python and VS Code, we can now install FastAPI using a \nterminal console. To ensure correct installation, first update Python’s package installer (pip) by \nrunning this command:\npython -m pip install --upgrade pip\nAfterward, we install the framework by running this series of commands:\npip install fastapi\npip install uvicorn[standard]\npip install python-multipart\nImportant note\nIf you need to install the complete FastAPI platform, including all optional dependencies, the \nappropriate command is pip install fastapi[all]. Likewise, if you want to install \nand utilize the full-blown uvicorn server, you should run the pip install uvicorn \ncommand. Also, install the bcrypt module for encryption-related tasks.",
      "content_length": 2040,
      "extraction_method": "Direct"
    },
    {
      "page_number": 28,
      "chapter": null,
      "content": "Initializing and configuring FastAPI\n5\nAt this point, you should have installed all the needed FastAPI module dependencies from the pydantic \nand starlette module components in your Python environment. Furthermore, the python-multipart \nmodule is required to create a REST API that handles form parameters. The installed uvicorn, \nhowever, is an ASGI-based server that will run your FastAPI applications. The Asynchronous Server \nGateway Interface (ASGI) server that FastAPI uses makes it the fastest Python framework at the time \nof writing. The uvicorn server has the capability to run both synchronous and asynchronous services.\nAfter the installation and configuration of the essential tools, modules, and IDE, let us now start our \nfirst API implementation using the framework.\nInitializing and configuring FastAPI\nLearning how to create applications using FastAPI is easy and straightforward. A simple application can \nbe created just by creating a main.py file inside your /ch01 project folder. In our online academic \ndiscussion forum, for instance, the application started with this code:\nfrom fastapi import FastAPI\napp = FastAPI()\nThis initializes the FastAPI framework. The application needs to instantiate the core FastAPI class \nfrom the fastapi module and use app as the reference variable to the object. Then, this object is \nused later as a Python @app decorator, which provides our application with some features such as \nroutes, middleware, exception handlers, and path operations.\nImportant note\nYou can replace app with your preferred but valid Python variable name, such as main_app, \nforum, or myapp.\nNow, your application is ready to manage REST APIs that are technically Python functions. But to \ndeclare them as REST service methods, we need to decorate them with the appropriate HTTP request \nmethod provided by the path operation @app decorator. This decorator contains the get(), post(), \ndelete(), put(), head(), patch(), trace(), and options() path operations, which \ncorrespond to the eight HTTP request methods. And these path operations are decorated or annotated \non top of the Python functions that we want to handle the request and response.\nIn our specimen, the first sample that the REST API created was this:\n@app.get(\"/ch01/index\")\ndef index():\n    return {\"message\": \"Welcome FastAPI Nerds\"}",
      "content_length": 2333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 29,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n6\nThe preceding is a GET API service method that returns a JSON object. To locally run our application, \nwe need to execute the following command:\nuvicorn main:app --reload\nThis command will load the forum application to the uvicorn live server through the application’s \nmain.py file with FastAPI object referencing. Live reload is allowed by adding the --reload \noption, which enables the restart of the development server whenever there are changes in the code. \nFigure 1.1 – The uvicorn console log\nFigure 1.1 shows that uvicorn uses localhost to run the application with the default port 8000. \nWe can access our index page through http://localhost:8000/ch01/index. To stop the \nserver, you just need to press the Ctrl + C keyboard keys.\nAfter running our first endpoint, let us now explore how to implement the other types of HTTP \nmethods, namely POST, DELETE, PUT, and PATCH.\nDesigning and implementing REST APIs\nThe Representation State Transfer (REST) API makes up the rules, processes, and tools that allow \ninteraction among microservices. These are method services that are identified and executed through \ntheir endpoint URLs. Nowadays, focusing on API methods before building a whole application is one \nof the most popular and effective microservices design strategies. This approach, called an API-first \nmicroservices development, focuses first on the client’s needs and then later identifies what API service \nmethods we need to implement for these client requirements.\nIn our online academic discussion forum app, software functionality such as user sign-up, login, profile \nmanagement, message posting, and managing post replies are some of the crucial needs we prioritized. \nIn a FastAPI framework, these features are implemented as services using functions that are defined \nusing Python’s def keyword, with the association of the appropriate HTTP request method through \nthe path operations provided by @app.\nThe login service, which requires username and password request parameters from the user, \nis implemented as a GET API method:\n@app.get(\"/ch01/login/\")\ndef login(username: str, password: str):",
      "content_length": 2157,
      "extraction_method": "Direct"
    },
    {
      "page_number": 30,
      "chapter": null,
      "content": "Designing and implementing REST APIs\n7\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    else:\n        user = valid_users.get(username)\n        if checkpw(password.encode(), \n                   user.passphrase.encode()):\n            return user\n        else:\n            return {\"message\": \"invalid user\"}\nThis login service uses bcrypt’s checkpw() function to check whether the password of the user is \nvalid. Conversely, the sign-up service, which also requires user credentials from the client in the form \nof request parameters, is created as a POST API method:\n@app.post(\"/ch01/login/signup\")\ndef signup(uname: str, passwd: str):\n    if (uname == None and passwd == None):\n        return {\"message\": \"invalid user\"}\n    elif not valid_users.get(uname) == None:\n        return {\"message\": \"user exists\"}\n    else:\n        user = User(username=uname, password=passwd)\n        pending_users[uname] = user\n        return user\nAmong the profile management services, the following update_profile() service serves as a \nPUT API service, which requires the user to use an entirely new model object for profile information \nreplacement and the client’s username to serve as the key:\n@app.put(\"/ch01/account/profile/update/{username}\")\ndef update_profile(username: str, id: UUID, \n                     new_profile: UserProfile):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    else:\n        user = valid_users.get(username)\n        if user.id == id:\n            valid_profiles[username] = new_profile",
      "content_length": 1589,
      "extraction_method": "Direct"
    },
    {
      "page_number": 31,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n8\n            return {\"message\": \"successfully updated\"}\n        else:\n            return {\"message\": \"user does not exist\"}\nNot all services that carry out updates are PUT API methods, such as the following update_\nprofile_name() service, which only requires the user to submit a new first name, last name, and \nmiddle initial for partial replacement of a client’s profile. This HTTP request, which is handier and \nmore lightweight than a full-blown PUT method, only requires a PATCH action:\n@app.patch(\"/ch01/account/profile/update/names/{username}\")\ndef update_profile_names(username: str, id: UUID, \n                          new_names: Dict[str, str]):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    elif new_names == None:\n        return {\"message\": \"new names are required\"}\n    else:\n        user = valid_users.get(username)\n        if user.id == id:\n            profile = valid_profiles[username]\n            profile.firstname = new_names['fname']\n            profile.lastname = new_names['lname']\n            profile.middle_initial = new_names['mi']\n            valid_profiles[username] = profile\n            return {\"message\": \"successfully updated\"}\n        else:\n            return {\"message\": \"user does not exist\"}\nThe last essential HTTP services that we included before building the application are the DELETE API \nmethods. We use these services to delete records or information given a unique identification, such as \nusername and a hashed id. An example is the following delete_post_discussion() service \nthat allows a user to delete a posted discussion when given a username and the UUID (Universally \nUnique Identifier) of the posted message:\n@app.delete(\"/ch01/discussion/posts/remove/{username}\")\ndef delete_discussion(username: str, id: UUID):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    elif discussion_posts.get(id) == None:",
      "content_length": 1983,
      "extraction_method": "Direct"
    },
    {
      "page_number": 32,
      "chapter": null,
      "content": "Designing and implementing REST APIs\n9\n        return {\"message\": \"post does not exist\"}\n    else:\n        del discussion_posts[id] \n        return {\"message\": \"main post deleted\"}\nAll path operations require a unique endpoint URL in the str format. A good practice is to start all \nURLs with the same top-level base path, such as /ch01, and then differ when reaching their respective \nsubdirectories. After running the uvicorn server, we can check and validate whether all our URLs \nare valid and running by accessing the documentation URL, http://localhost:8000/docs. \nThis path will show us a OpenAPI dashboard, as shown in Figure 1.2, listing all the API methods \ncreated for the application. Discussions on the OpenAPI will be covered in Chapter 9, Utilizing Other \nAdvanced Features.\nFigure 1.2 – A Swagger OpenAPI dashboard\nAfter creating the endpoint services, let us scrutinize how FastAPI manages its incoming request \nbody and the outgoing response.",
      "content_length": 960,
      "extraction_method": "Direct"
    },
    {
      "page_number": 33,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n10\nManaging user requests and server response\nClients can pass their request data to FastAPI endpoint URLs through path parameters, query parameters, \nor headers to pursue service transactions. There are standards and ways to use these parameters to \nobtain incoming requests. Depending on the goal of the services, we use these parameters to influence \nand build the necessary responses the clients need. But before we discuss these various parameter \ntypes, let us explore first how we use type hinting in FastAPI’s local parameter declaration.\nParameter type declaration\nAll request parameters are required to be type-declared in the method signature of the service method \napplying the PEP 484 standard called type hints. FastAPI supports common types such as None, \nbool, int, and float and container types such as list, tuple, dict, set, frozenset, and \ndeque. Other complex Python types such as datetime.date, datetime.time, datetime.\ndatetime, datetime.delta, UUID, bytes, and Decimal are also supported. \nThe framework also supports the data types included in Python’s typing module, responsible for \ntype hints. These data types are standard notations for Python and variable type annotations that can \nhelp to pursue type checking and model validation during compilation, such as Optional, List, \nDict, Set, Union, Tuple, FrozenSet, Iterable, and Deque.\nPath parameters\nFastAPI allows you to obtain request data from the endpoint URL of an API through a path parameter \nor path variable that makes the URL somewhat dynamic. This parameter holds a value that becomes \npart of a URL indicated by curly braces ({}). After setting off these path parameters within the URL, \nFastAPI requires these parameters to be declared by applying type hints. \nThe following delete_user() service is a DELETE API method that uses a username path \nparameter to search for a login record for deletion:\n@app.delete(\"/ch01/login/remove/{username}\")\ndef delete_user(username: str):\n    if username == None:\n    return {\"message\": \"invalid user\"}\nelse:\n    del valid_users[username]\n    return {\"message\": \"deleted user\"}",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 34,
      "chapter": null,
      "content": "Managing user requests and server response\n11\nMultiple path parameters are acceptable if the leftmost variables are more likely to be filled with values \nthan the rightmost variables. In other words, the importance of the leftmost path variables will make \nthe process more relevant and correct than those on the right. This standard is applied to ensure that \nthe endpoint URL will not look like other URLs, which might cause some conflicts and confusion. \nThe following login_with_token() service follows this standard, since username is a primary \nkey and is as strong as, or even stronger than, its next parameter, password. There is an assurance \nthat the URL will always look unique every time the endpoint is accessed because username will \nalways be required, as well as password:\n@app.get(\"/ch01/login/{username}/{password}\")\ndef login_with_token(username: str, password:str, \n                     id: UUID):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    else:\n        user = valid_users[username]\n        if user.id == id and checkpw(password.encode(), \n                 user.passphrase):\n            return user\n        else:\n            return {\"message\": \"invalid user\"}\nUnlike other web frameworks, FastAPI is not friendly with endpoint URLs that belong to base paths \nor top-level domain paths with different subdirectories. This occurrence happens when we have \ndynamic URL patterns that look the same as the other fixed endpoint URLs when assigned a specific \npath variable. These fixed URLs are implemented sequentially after these dynamic URLs. An example \nof these are the following services:\n@app.get(\"/ch01/login/{username}/{password}\")\ndef login_with_token(username: str, password:str, \n                     id: UUID):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    else:\n        user = valid_users[username]\n        if user.id == id and checkpw(password.encode(), \n                      user.passphrase.encode()):\n            return user\n        else:",
      "content_length": 2074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 35,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n12\n            return {\"message\": \"invalid user\"}\n@app.get(\"/ch01/login/details/info\")\ndef login_info():\n        return {\"message\": \"username and password are \n                            needed\"}\nThis will give us an HTTP Status Code 422 (Unprocessable Entity) when accessing http://\nlocalhost:8080/ch01/login/details/info. There should be no problem accessing the \nURL, since the API service is almost a stub or trivial JSON data. What happened in this scenario is \nthat the fixed path’s details and info path directories were treated as username and password \nparameter values, respectively. Because of confusion, the built-in data validation of FastAPI will show \nus a JSON-formatted error message that says, {\"detail\":[{\"loc\":[\"query\",\"id\"],\"ms\ng\":\"field required\",\"type\":\"value_error.missing\"}]}. To fix this problem, all \nfixed paths should be declared first before the dynamic endpoint URLs with path parameters. Thus, \nthe preceding login_info() service should be declared first before login_with_token().\nQuery parameters\nA query parameter is a key–value pair supplied after the end of an endpoint URL, indicated by a \nquestion mark (?). Just like the path parameter, this also holds the request data. An API service can \nmanage a series of query parameters separated by an ampersand (&). Like in path parameters, all query \nparameters are also declared in the service method. The following login service is a perfect specimen \nthat uses query parameters:\n@app.get(\"/ch01/login/\")\ndef login(username: str, password: str):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    else:\n        user = valid_users.get(username)\n        if checkpw(password.encode(), \n               user.passphrase.encode()):\n            return user\n        else:\n            return {\"message\": \"invalid user\"}",
      "content_length": 1872,
      "extraction_method": "Direct"
    },
    {
      "page_number": 36,
      "chapter": null,
      "content": "Managing user requests and server response\n13\nThe login service method uses username and password as query parameters in the str \ntypes. Both are required parameters, and assigning them with None as parameter values will give a \ncompiler error.\nFastAPI supports query parameters that are complex types, such as list and dict. But these \nPython collection types cannot specify the type of objects to store unless we apply the generic \ntype hints for Python collections. The following delete_users() and update_profile_\nnames() APIs use generic type hints, List and Dict, in declaring query parameters that are \ncontainer types with type checking and data validation:\nfrom typing import Optional, List, Dict\n@app.delete(\"/ch01/login/remove/all\")\ndef delete_users(usernames: List[str]):\n    for user in usernames:\n        del valid_users[user]\n    return {\"message\": \"deleted users\"}\n@app.patch(\"/ch01/account/profile/update/names/{username}\")\ndef update_profile_names(username: str, id: UUID, \n                         new_names: Dict[str, str]):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    elif new_names == None:\n        return {\"message\": \"new names are required\"}\n    else:\n        user = valid_users.get(username)\n        if user.id == id:\n            profile = valid_profiles[username]\n            profile.firstname = new_names['fname']\n            profile.lastname = new_names['lname']\n            profile.middle_initial = new_names['mi']\n            valid_profiles[username] = profile\n            return {\"message\": \"successfully updated\"}\n        else:\n            return {\"message\": \"user does not exist\"}\nFastAPI also allows you to explicitly assign default values to service function parameters.",
      "content_length": 1757,
      "extraction_method": "Direct"
    },
    {
      "page_number": 37,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n14\nDefault parameters\nThere are times that we need to specify default values to the query parameter(s) and path parameter(s) \nof some API services to avoid validation error messages such as field required and value_\nerror.missing. Setting default values to parameters will allow the execution of an API method \nwith or without supplying the parameter values. Depending on the requirement, assigned default values \nare usually 0 for numeric types, False for bool types, empty string for string types, an empty list \n([]) for List types, and an empty dictionary ({}) for Dict types. The following delete pending \nusers() and change_password() services show us how to apply default values to the query \nparameter(s) and path parameter(s):\n@app.delete(\"/ch01/delete/users/pending\")\ndef delete_pending_users(accounts: List[str] = []):\n    for user in accounts:\n        del pending_users[user]\n    return {\"message\": \"deleted pending users\"}\n@app.get(\"/ch01/login/password/change\")\ndef change_password(username: str, old_passw: str = '',\n                         new_passw: str = ''):\n    passwd_len = 8\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    elif old_passw == '' or new_passw == '':\n        characters = ascii_lowercase\n        temporary_passwd = \n             ''.join(random.choice(characters) for i in \n                     range(passwd_len))\n        user = valid_users.get(username)\n        user.password = temporary_passwd\n        user.passphrase = \n                  hashpw(temporary_passwd.encode(),gensalt())\n        return user\n    else:\n        user = valid_users.get(username)\n        if user.password == old_passw:\n            user.password = new_passw\n            user.passphrase = hashpw(new_pass.\nencode(),gensalt())",
      "content_length": 1814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 38,
      "chapter": null,
      "content": "Managing user requests and server response\n15\n            return user\n        else:\n            return {\"message\": \"invalid user\"}\ndelete_pending_users() can be executed even without passing any accounts argument, \nsince accounts will be always an empty List by default. Likewise, change_password() can \nstill continue its process without passing any old_passwd and new_passw, since they are both \nalways defaulted to empty str. hashpw() is a bcrypt utility function that generates a hashed \npassphrase from an autogenerated salt.\nOptional parameters\nIf the path and/or query parameter(s) of a service is/are not necessarily needed to be supplied by \nthe user, meaning the API transactions can proceed with or without their inclusion in the request \ntransaction, then we set them as optional. To declare an optional parameter, we need to import the \nOptional type from the typing module and then use it to set the parameter. It should wrap the \nsupposed data type of the parameter using brackets ([]) and can have any default value if needed. \nAssigning the Optional parameter to a None value indicates that its exclusion from the parameter \npassing is allowed by the service, but it will hold a None value. The following services depict the use \nof optional parameters:\nfrom typing import Optional, List, Dict\n@app.post(\"/ch01/login/username/unlock\")\ndef unlock_username(id: Optional[UUID] = None):\n    if id == None:\n        return {\"message\": \"token needed\"}\n    else:\n        for key, val in valid_users.items():\n            if val.id == id:\n                return {\"username\": val.username}\n        return {\"message\": \"user does not exist\"}\n@app.post(\"/ch01/login/password/unlock\")\ndef unlock_password(username: Optional[str] = None, \n                    id: Optional[UUID] = None):\n    if username == None:\n        return {\"message\": \"username is required\"}\n    elif valid_users.get(username) == None:",
      "content_length": 1907,
      "extraction_method": "Direct"
    },
    {
      "page_number": 39,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n16\n        return {\"message\": \"user does not exist\"}\n    else:\n        if id == None:\n            return {\"message\": \"token needed\"}\n        else:\n            user = valid_users.get(username)\n            if user.id == id:\n                return {\"password\": user.password}\n            else:\n                return {\"message\": \"invalid token\"}\nIn the online academic discussion forum application, we have services such as the preceding unlock_\nusername() and unlock_password() services that declare all their parameters as optional. \nJust do not forget to apply exception handling or defensive validation in your implementation when \ndealing with these kinds of parameters to avoid HTTP Status 500 (Internal Server Error).\nImportant note\nThe FastAPI framework does not allow you to directly assign the None value to a parameter \njust to declare an optional parameter. Although this is allowed with the old Python behavior, \nthis is no longer recommended in the current Python versions for the purpose of built-in type \nchecking and model validation.\nMixing all types of parameters\nIf you are planning to implement an API service method that declares optional, required, and default \nquery and path parameters altogether, you can pursue it because the framework supports it, but \napproach it with some caution due to some standards and rules:\n@app.patch(\"/ch01/account/profile/update/names/{username}\")\ndef update_profile_names(id: UUID, username: str = '' , \n           new_names: Optional[Dict[str, str]] = None):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    elif new_names == None:\n        return {\"message\": \"new names are required\"}\n    else:\n        user = valid_users.get(username)\n        if user.id == id:",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 40,
      "chapter": null,
      "content": "Managing user requests and server response\n17\n            profile = valid_profiles[username]\n            profile.firstname = new_names['fname']\n            profile.lastname = new_names['lname']\n            profile.middle_initial = new_names['mi']\n            valid_profiles[username] = profile\n            return {\"message\": \"successfully updated\"}\n        else:\n            return {\"message\": \"user does not exist\"}\nThe updated version of the preceding update_profile_names() service declares a username \npath parameter, a UUID id query parameter, and an optional Dict[str, str] type. With mixed \nparameter types, all required parameters should be declared first, followed by default parameters, and \nlast in the parameter list should be the optional types. Disregarding this ordering rule will generate \na compiler error.\nRequest body\nA request body is a body of data in bytes transmitted from a client to a server through a POST, PUT, \nDELETE, or PATCH HTTP method operation. In FastAPI, a service must declare a model object to \nrepresent and capture this request body to be processed for further results.\nTo implement a model class for the request body, you should first import the BaseModel class \nfrom the pydantic module. Then, create a subclass of it to utilize all the properties and behavior \nneeded by the path operation in capturing the request body. Here are some of the data models used \nby our application:\nfrom pydantic import BaseModel\nclass User(BaseModel):\n    username: str\n    password: str\nclass UserProfile(BaseModel):\n    firstname: str\n    lastname: str\n    middle_initial: str\n    age: Optional[int] = 0\n    salary: Optional[int] = 0\n    birthday: date\n    user_type: UserType",
      "content_length": 1703,
      "extraction_method": "Direct"
    },
    {
      "page_number": 41,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n18\nThe attributes of the model classes must be explicitly declared by applying type hints and utilizing the \ncommon and complex data types used in the parameter declaration. These attributes can also be set \nas required, default, and optional, just like in the parameters.\nMoreover, the pydantic module allows the creation of nested models, even the deeply nested ones. \nA sample of these is shown here:    \nclass ForumPost(BaseModel):\n    id: UUID\n    topic: Optional[str] = None\n    message: str\n    post_type: PostType\n    date_posted: datetime\n    username: str\nclass ForumDiscussion(BaseModel):\n    id: UUID\n    main_post: ForumPost\n    replies: Optional[List[ForumPost]] = None\n    author: UserProfile\nAs seen in the preceding code, we have a ForumPost model, which has a PostType model attribute, \nand ForumDiscussion, which has a List attribute of ForumPost, a ForumPost model attribute, \nand a UserProfile attribute. This kind of model blueprint is called a nested model approach.\nAfter creating these model classes, you can now inject these objects into the services that are intended to \ncapture the request body from the clients. The following services utilize our User and UserProfile \nmodel classes to manage the request body:\n@app.post(\"/ch01/login/validate\", response_model=ValidUser)\ndef approve_user(user: User):\n    if not valid_users.get(user.username) == None:\n        return ValidUser(id=None, username = None, \n             password = None, passphrase = None)\n    else:\n        valid_user = ValidUser(id=uuid1(), \n             username= user.username, \n             password  = user.password, \n             passphrase = hashpw(user.password.encode(),",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 42,
      "chapter": null,
      "content": "Managing user requests and server response\n19\n                          gensalt()))\n        valid_users[user.username] = valid_user\n        del pending_users[user.username]\n        return valid_user\n@app.put(\"/ch01/account/profile/update/{username}\")\ndef update_profile(username: str, id: UUID, \n                   new_profile: UserProfile):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    else:\n        user = valid_users.get(username)\n        if user.id == id:\n            valid_profiles[username] = new_profile\n            return {\"message\": \"successfully updated\"}\n        else:\n            return {\"message\": \"user does not exist\"}\nModels can be declared required, with a default instance value, or optional in the service method, \ndepending on the specification of the API. Missing or incorrect details such as invalid password \nor None values in the approve_user() service will emit the Status Code 500 (Internal Server Error). \nHow FastAPI handles exceptions will be part of Chapter 2, Exploring the Core Features, discussions.\nImportant note\nThere are two essential points we need to emphasize when dealing with BaseModel class types. \nFirst, the pydantic module has a built-in JSON encoder that converts the JSON-formatted \nrequest body to the BaseModel object. So, there is no need create a custom converter to map \nthe request body to the BaseModel model. Second, to instantiate a BaseModel class, all its \nrequired attributes must be initialized immediately through the constructor’s named parameters.\nRequest headers\nIn a request-response transaction, it is not only the parameters that are accessible by the REST API \nmethods but also the information that describes the context of the client where the request originated. \nSome common request headers such as User-Agent, Host, Accept, Accept-Language, \nAccept-Encoding, Referer, and Connection usually appear with request parameters and \nvalues during request transactions.",
      "content_length": 1986,
      "extraction_method": "Direct"
    },
    {
      "page_number": 43,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n20\nTo access a request header, import first the Header function from the fastapi module. Then, declare \nthe variable that has the same name as the header in the method service as str types and initialize \nthe variable by calling the Header(None) function.  The None argument enables the Header() \nfunction to declare the variable optionally, which is a best practice. For hyphenated request header \nnames, the hyphen (-) should be converted to an underscore (_); otherwise, the Python compiler \nwill flag a syntax error message. It is the task of the Header() function to convert the underscore \n(_) to a hyphen (-) during request header processing.\nOur online academic discussion forum application has a verify_headers() service that retrieves \ncore request headers needed to verify a client’s access to the application:\nfrom fastapi import Header\n@app.get(\"/ch01/headers/verify\")\ndef verify_headers(host: Optional[str] = Header(None), \n                   accept: Optional[str] = Header(None),\n                   accept_language: \n                       Optional[str] = Header(None),\n                   accept_encoding: \n                       Optional[str] = Header(None),\n                   user_agent: \n                       Optional[str] = Header(None)):\n    request_headers[\"Host\"] = host\n    request_headers[\"Accept\"] = accept\n    request_headers[\"Accept-Language\"] = accept_language\n    request_headers[\"Accept-Encoding\"] = accept_encoding\n    request_headers[\"User-Agent\"] = user_agent\n    return request_headers\nImportant note\nNon-inclusion of the Header() function call in the declaration will let FastAPI treat the \nvariables as query parameters. Be cautious also with the spelling of the local parameter names, \nsince they are the request header names per se except for the underscore.\nResponse data\nAll API services in FastAPI should return JSON data, or it will be invalid and may return None by \ndefault. These responses can be formed using dict, BaseModel, or JSONResponse objects. \nDiscussions on JSONResponse will be discussed in the succeeding chapters.",
      "content_length": 2106,
      "extraction_method": "Direct"
    },
    {
      "page_number": 44,
      "chapter": null,
      "content": "Managing user requests and server response\n21\nThe pydantic module’s built-in JSON converter will manage the conversion of these custom \nresponses to a JSON object, so there is no need to create a custom JSON encoder:\n@app.post(\"/ch01/discussion/posts/add/{username}\")\ndef post_discussion(username: str, post: Post, \n                    post_type: PostType):\n    if valid_users.get(username) == None:\n        return {\"message\": \"user does not exist\"}\n    elif not (discussion_posts.get(id) == None):\n        return {\"message\": \"post already exists\"}\n    else:\n        forum_post = ForumPost(id=uuid1(), \n          topic=post.topic, message=post.message, \n          post_type=post_type, \n          date_posted=post.date_posted, username=username)\n        user = valid_profiles[username]\n        forum = ForumDiscussion(id=uuid1(), \n         main_post=forum_post, author=user, replies=list())\n        discussion_posts[forum.id] = forum\n        return forum\nThe preceding post_discussion() service returns two different hardcoded dict objects, with \nmessage as the key and an instantiated ForumDiscussion model.\nOn the other hand, this framework allows us to specify the return type of a service method. The setting \nof the return type happens in the response_model attribute of any of the @app path operations. \nUnfortunately, the parameter only recognizes BaseModel class types:\n@app.post(\"/ch01/login/validate\", response_model=ValidUser)\ndef approve_user(user: User):\n    \n    if not valid_users.get(user.username) == None:\n        return ValidUser(id=None, username = None, \n                   password = None, passphrase = None)\n    else:\n        valid_user = ValidUser(id=uuid1(), \n         username= user.username, password = user.password,\n          passphrase = hashpw(user.password.encode(),\n                 gensalt()))",
      "content_length": 1826,
      "extraction_method": "Direct"
    },
    {
      "page_number": 45,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n22\n        valid_users[user.username] = valid_user\n        del pending_users[user.username]\n        return valid_user\nThe preceding approve_user() service specifies the required return of the API method, which \nis ValidUser.\nNow, let us explore how FastAPI handles form parameters.\nHandling form parameters\nWhen API methods are designed to handle web forms, the services involved are required to retrieve \nform parameters instead of the request body because this form data is normally encoded as an \napplication/x-www-form-urlencoded media type. These form parameters are conventionally \nstring types, but the pydantic module’s JSON encoder can convert each parameter value to its \nrespective valid type.  \nAll the form parameter variables can be declared required, with default values, or optional using the \nsame set of Python types we used previously. Then, the fastapi module has a Form function that \nneeds to be imported to initialize these form parameter variables during their declaration. To set \nthese form parameters as required, the Form() function must have the ellipses (…) argument, thus \ncalling it as Form(…): \nfrom fastapi import FastAPI, Form\n@app.post(\"/ch01/account/profile/add\", \n                        response_model=UserProfile)\ndef add_profile(uname: str, \n                fname: str = Form(...), \n                lname: str = Form(...),\n                mid_init: str = Form(...),\n                user_age: int = Form(...),\n                sal: float = Form(...),\n                bday: str = Form(...),\n                utype: UserType = Form(...)):\n    if valid_users.get(uname) == None:\n        return UserProfile(firstname=None, lastname=None, \n              middle_initial=None, age=None, \n              birthday=None, salary=None, user_type=None)\n    else:",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 46,
      "chapter": null,
      "content": "Managing cookies\n23\n        profile = UserProfile(firstname=fname, \n             lastname=lname, middle_initial=mid_init, \n             age=user_age, birthday=datetime.strptime(bday,\n                '%m/%d/%Y'), salary=sal, user_type=utype)\n        valid_profiles[uname] = profile\n        return profile\nThe preceding add_profile() service shows us how to call the Form(…) function to return a \nForm object during the parameter declaration.\nImportant note\nForm-handling services will not work if the python-multipart module is not installed.\nSometimes, we need browser cookies to establish an identity for our application, leave trails in the \nbrowser for every user transaction, or store product information for a purpose. If FastAPI can manage \nform data, it can also do the same with cookies.\nManaging cookies\nA cookie is a piece of information stored in the browser to pursue some purpose, such as login user \nauthorization, web agent response generation, and session handling-related tasks. One cookie is always \na key-value pair that are both string types. \nFastAPI allows services to create cookies individually through the Response library class from its \nfastapi module. To use it, it needs to appear as the first local parameter of the service, but we do \nnot let the application or client pass an argument to it. Using the dependency injection principle, \nthe framework will provide the Response instance to the service and not the application. When \nthe service has other parameters to declare, the additional declaration should happen right after the \ndeclaration of the Response parameter.\nThe Response object has a set_cookie() method that contains two required named parameters: \nthe key, which sets the cookie name, and the value, which stores the cookie value. This method only \ngenerates one cookie and stores it in the browser afterward:\n@app.post(\"/ch01/login/rememberme/create/\")\ndef create_cookies(resp: Response, id: UUID, \n                   username: str = ''):\n    resp.set_cookie(key=\"userkey\", value=username)\n    resp.set_cookie(key=\"identity\", value=str(id))\n    return {\"message\": \"remember-me tokens created\"}",
      "content_length": 2142,
      "extraction_method": "Direct"
    },
    {
      "page_number": 47,
      "chapter": null,
      "content": "Setting Up FastAPI for Starters\n24\nThe preceding create_cookies() method shows us the creation of remember-me tokens such \nas userkey and identity for the remember-me authorization of our online academic discussion \nforum project. \nTo retrieve these cookies, local parameters that have the same name as the cookies are declared in \nthe service method as str types, since cookie values are always strings. As with Header and Form, \nthe fastapi module also provides a Cookie function that is needed to initialize each declared \ncookie parameter variable. The Cookie() function should always have the None argument to set the \nparameters optionally, ensuring that the API method executes without problems whenever the headers \nare not present in the request transaction. The following access_cookie() service retrieves all \nthe remember-me authorization cookies created by the previous service:\n@app.get(\"/ch01/login/cookies\")\ndef access_cookie(userkey: Optional[str] = Cookie(None), \n           identity: Optional[str] = Cookie(None)):\n    cookies[\"userkey\"] = userkey\n    cookies[\"identity\"] = identity\n    return cookies\nSummary\nThis chapter is essential to familiarize ourselves with FastAPI and understand its basic components. \nThe concept that we can get from this chapter can measure how much adjustment and effort we need \nto invest into translating or rewriting some existing applications to FastAPI. Knowing its basics will \nhelp us learn how to install its modules, structure the project directories, and learn the core library \nclasses and functions needed to build a simple enterprise-grade application. \nWith the help of our recipe online academic discussion forum application, this chapter showed us how \nto build different REST APIs associated with HTTP methods using the FastAPI module class and \nPython def functions. From there, we learned how to capture incoming request data and headers \nusing the local parameters of the API methods and how these API methods should return a response \nto the client. And through this chapter, we saw how easy it is for FastAPI to capture form data from \n<form></form> of any UI templates and that is using the Form function. Aside from the Form \nfunction, the FastAPI module also has the Cookie function to help us create and retrieve cookies \nfrom the browser, and Header to retrieve the request header part of an incoming request transaction. \nOverall, this chapter has prepared us for advanced discussions that will center on other features of \nFastAPI that can help us upgrade our simple applications to full-blown ones. The next chapter will cover \nthese essential core features, which will provide our application with the needed response encoder and \ngenerator, exception handlers, middleware, and other components related to asynchronous transactions.",
      "content_length": 2812,
      "extraction_method": "Direct"
    },
    {
      "page_number": 48,
      "chapter": null,
      "content": "2\nExploring the Core Features\nIn the previous chapter, we found out how easy it is to install and start developing REST APIs using \nthe FastAPI framework. Handling requests, cookies, and form data was fast, easy, and straightforward \nwith FastAPI, as was building the different HTTP path operations.\nTo learn about the framework’s features further, this chapter will guide us on how to upgrade our REST \nAPIs by adding some essential FastAPI features to the implementation. These include some handlers \nthat can help minimize unchecked exceptions, JSON encoders that can directly manage endpoint \nresponses, background jobs that can create audit trails and logs, and multiple threads to run some API \nmethods asynchronously with the uvicorn’s main thread. Moreover, issues such as managing source \nfiles, modules, and packages for huge enterprise projects will also be addressed in this chapter. This \nchapter will use and dissect an intelligent tourist system prototype to assist with elaborating upon and \nexemplifying FastAPI’s core modules.\nBased on these aforementioned features, this chapter will discuss the following major concepts that \ncan help us extend our learning about this framework:\n•\t Structuring and organizing huge projects\n•\t Managing API-related exceptions\n•\t Converting objects to JSON-compatible types\n•\t Managing API responses\n•\t Creating background processes\n•\t Using asynchronous path operations\n•\t Applying middleware to filter path operations",
      "content_length": 1471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 49,
      "chapter": null,
      "content": "Exploring the Core Features\n26\nTechnical requirements\nThis chapter will implement a prototype of an intelligent tourist system designed to provide booking \ninformation and reservation about tourist spots. It can provide user details, tourist spot details, and \nlocation grids. It also allows users or tourists to comment on tours and rate them. The prototype has \nan administrator account for adding and removing all the tour details, managing users, and providing \nsome listings. The application will not use any database management system yet, so all the data is \ntemporarily stored in Python collections. The code is all uploaded at https://github.com/\nPacktPublishing/Building-Python-Microservices-with-FastAPI/tree/\nmain/ch02.\nStructuring and organizing huge projects\nIn FastAPI, big projects are organized and structured by adding packages and modules without \ndestroying the setup, configuration, and purpose. The project should always be flexible and scalable \nin case of additional features and requirements. One component must correspond to one package, \nwith several modules equivalent to a blueprint in a Flask framework.\nIn this prototypical intelligent tourist system, the application has several modules such as the login, \nadministration, visit, destination, and feedback-related functionalities. The two most crucial are the \nvisit module, which manages all the travel bookings of the users, and the feedback module, which \nenables clients to post their feedback regarding their experiences at every destination. These modules \nshould be separated from the rest since they provide the core transactions. Figure 2.1 shows how to \ngroup implementations and separate a module from the rest using packages:\nFigure 2.1 – The FastAPI project structure",
      "content_length": 1762,
      "extraction_method": "Direct"
    },
    {
      "page_number": 50,
      "chapter": null,
      "content": "Structuring and organizing huge projects\n27\nEach package in Figure 2.1 contains all the modules where the API services and some dependencies \nare implemented. All the aforementioned modules now have their own respective packages that make \nit easy to test, debug, and expand the application. Testing FastAPI components will be discussed in \nthe upcoming chapters.\nImportant note\nFastAPI does not require adding the __init__.py file into each Python package when using \nVS Code Editor and Python 3.8 during development, unlike in Flask. The __pycache__ folder \ngenerated inside a package during compilation contains binaries of the module scripts accessed \nand utilized by other modules. The main folder will also become a package since it will have its \nown __pycache__ folder together with the others. But we must exclude __pycache__ \nwhen deploying the application to the repository, since it may take up a lot of space. \nOn the other hand, what remains in the main folder are the core components such as the background \ntasks, custom exception handlers, middleware, and the main.py file. Now, let us learn about how \nFastAPI can bundle all these packages as one huge application when deployed. \nImplementing the API services\nFor these module packages to function, the main.py file must call and register all their API \nimplementations through the FastAPI instance. The scripts inside each package are already REST \nAPI implementations of the microservices, except that they are built by APIRouter instead of the \nFastAPI object. APIRouter also has the same path operations, query and request parameter \nsetup, handling of form data, generation of responses, and parameter injection of model objects. \nWhat is lacking in APIRouter is the support for an exception handler, middleware declaration, \nand customization:\nfrom fastapi import APIRouter\nfrom login.user import Signup, User, Tourist, \n      pending_users, approved_users\nrouter = APIRouter()\n@router.get(\"/ch02/admin/tourists/list\")\ndef list_all_tourists():\n    return approved_users\nThe list_all_tourists() API method operation here is part of the manager.py module in \nthe admin package, implemented using APIRouter due to project structuring. The method returns \na list of tourist records that are allowed to access the application, which can only be provided by the \nuser.py module in the login package.",
      "content_length": 2367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 51,
      "chapter": null,
      "content": "Exploring the Core Features\n28\nImporting the module components\nModule scripts can share their containers, BaseModel classes, and other resource objects to other \nmodules using Python’s from… import statement. Python’s from… import statement is better \nsince it allows us to import specific components from a module, instead of including unnecessary ones:\nfrom fastapi import APIRouter, status\nfrom places.destination import Tour, TourBasicInfo, \n    TourInput, TourLocation, tours, tours_basic_info, \n    tours_locations\nrouter = APIRouter()\n@router.put(\"/ch02/admin/destination/update\", \n            status_code=status.HTTP_202_ACCEPTED)\ndef update_tour_destination(tour: Tour):\n    try:\n        tid = tour.id\n        tours[tid] = tour\n        tour_basic_info = TourBasicInfo(id=tid, \n           name=tour.name, type=tour.type, \n           amenities=tour.amenities, ratings=tour.ratings)\n        tour_location = TourLocation(id=tid, \n           name=tour.name, city=tour.city, \n           country=tour.country, location=tour.location )\n        tours_basic_info[tid] = tour_basic_info\n        tours_locations[tid] = tour_location\n        return { \"message\" : \"tour updated\" }\n    except:\n        return { \"message\" : \"tour does not exist\" } \nThe update_tour_destination() operation here will not work without importing the Tour, \nTourBasicInfo, and TourLocation model classes from destination.py in the places \npackage. It shows the dependency between modules that happens when structuring is imposed on \nbig enterprise web projects.\nModule scripts can also import components from the main project folder when needed by the \nimplementation. One such example is accessing the middleware, exception handlers, and tasks from \nthe main.py file.",
      "content_length": 1740,
      "extraction_method": "Direct"
    },
    {
      "page_number": 52,
      "chapter": null,
      "content": "Structuring and organizing huge projects\n29\nImportant note\nAvoid cycles when dealing with the from… import statement. A cycle happens when a \nmodule script, a.py, accesses components from b.py that import resource objects from \na.py. FastAPI does not accept this scenario and will issue an error message.\nImplementing the new main.py file\nTechnically, the project’s packages and its module scripts will not be recognized by the framework \nunless their respective router object is added or injected into the application’s core through \nthe main.py file. main.py, just as the other project-level scripts do, uses FastAPI and not \nAPIRouter to create and register components, as well as the package’s modules. The FastAPI class \nhas an include_router() method that adds all these routers and injects them into the framework \nto make them part of the project structure. Beyond registering the routers, this method can also add \nother attributes and components to the router such as URL prefixes, tags, dependencies such as exception \nhandlers, and status codes:\nfrom fastapi import FastAPI, Request\nfrom admin import manager\nfrom login import user\nfrom feedback import post\nfrom places import destination\nfrom tourist import visit\napp = FastAPI()\napp.include_router(manager.router)\napp.include_router(user.router)\napp.include_router(destination.router)\napp.include_router(visit.router)\napp.include_router(\n    post.router,\n    prefix=\"/ch02/post\"\n)",
      "content_length": 1444,
      "extraction_method": "Direct"
    },
    {
      "page_number": 53,
      "chapter": null,
      "content": "Exploring the Core Features\n30\nThis code is the main.py implementation of the intelligent tourist system prototype tasked to import \nall the registers of the module’s scripts from the different packages, before adding them as components \nto the framework. Run the application using the following command:\nuvicorn main:app –-reload\nThis will allow you to access all the APIs of these modules at http://localhost:8000/docs.\nWhat happens to the application when API services encounter runtime problems during execution? Is \nthere a way to manage these problems besides applying Python’s try-except block? Let us explore \nimplementing API services with exception-handling mechanisms further.\nManaging API-related exceptions\nThe FastAPI framework has a built-in exception handler derived from its Starlette toolkit that always \nreturns default JSON responses whenever HTTPException is encountered during the execution of \nthe REST API operation. For instance, accessing the API at http://localhost:8000/ch02/\nuser/login without providing the username and password will give us the default JSON \noutput depicted in Figure 2.2:\nFigure 2.2 – The default exception result\nIn some rare cases, the framework sometimes chooses to return the HTTP response status instead \nof the default JSON content. But developers can still opt to override these default handlers to choose \nwhich responses to return whenever a specific exception cause happens. \nLet us now explore how to formulate a standardized and appropriate way of managing runtime errors \nin our API implementation.\nA single status code response\nOne way of managing the exception-handling mechanism of your application is to apply a try-except \nblock to manage the return responses of your API when it encounters an exception or none. After \napplying try-block, the operation should trigger a single status code, most often Status Code \n200 (SC 200). The path operation of FastAPI and APIRouter has a status_code parameter \nthat we can use to indicate the type of status code we want to raise.\nIn FastAPI, status codes are integer constants that are found in the status module. It also allows \ninteger literals to indicate the needed status code if they are a valid status code number.",
      "content_length": 2230,
      "extraction_method": "Direct"
    },
    {
      "page_number": 54,
      "chapter": null,
      "content": "Managing API-related exceptions\n31\nImportant Note\nA status code is a 3-digit number that indicates a reason for, information on, or status of the \nHTTP response of a REST API operation. The status code range 200 to 299 denotes a successful \nresponse, 300 to 399 pertains to redirection, 400-499 pertains to client-related problems, and \n500 to 599 is related to server errors.\nThis technique is rarely used because there are times that an operation needs to be clear in recognizing \nevery exception that it encounters, which can only be done by returning HTTPException instead \nof a custom error message wrapped in a JSON object:\nfrom fastapi import APIRouter, status\n@router.put(\"/ch02/admin/destination/update\", \n              status_code=status.HTTP_202_ACCEPTED)\ndef update_tour_destination(tour: Tour):\n    try:\n        tid = tour.id\n        tours[tid] = tour\n        tour_basic_info = TourBasicInfo(id=tid, \n           name=tour.name, type=tour.type, \n           amenities=tour.amenities, ratings=tour.ratings)\n        tour_location = TourLocation(id=tid, \n           name=tour.name, city=tour.city, \n           country=tour.country, location=tour.location )\n        tours_basic_info[tid] = tour_basic_info\n        tours_locations[tid] = tour_location\n        return { \"message\" : \"tour updated\" }\n    except:\n        return { \"message\" : \"tour does not exist\" }\n@router.get(\"/ch02/admin/destination/list\", \n            status_code=200)\ndef list_all_tours():\n    return tours",
      "content_length": 1481,
      "extraction_method": "Direct"
    },
    {
      "page_number": 55,
      "chapter": null,
      "content": "Exploring the Core Features\n32\nThe list_all_tours() method shown here is the kind of REST API service that should emit Status \nCode 200 – it gives an error-free result just by rendering the Python collection with data. Observe that \nthe literal integer value, 200, or SC 200, assigned to the status_code parameter of the GET path \noperation always raises an OK status. On the other hand, the update_tour_destination() \nmethod shows another approach in emitting status codes by using a try-except block, wherein \nboth blocks return a custom JSON response. Whichever scenario happens, it will always trigger SC \n202, which may not apply to some REST implementations. After the status module is imported, \nits HTTP_202_ACCEPTED constant is used to set the value of the status_code parameter.\nMultiple status codes\nIf we need each block in try-except to return their respective status code, we need to avoid \nusing the status_code parameter of the path operations and use JSONResponse instead. \nJSONResponse is one of the FastAPI classes used to render a JSON response to the client. It is \ninstantiated, constructor-injected with values for its content and status_code parameters, \nand returned by the path operations. By default, the framework uses this API to help path operations \nrender responses as JSON types. Its content parameter should be a JSON-type object, while the \nstatus_code parameter can be an integer constant and a valid status code number, or it can be a \nconstant from the module status:\nfrom fastapi.responses import JSONResponse\n@router.post(\"/ch02/admin/destination/add\")\nadd_tour_destination(input: TourInput):\n    try:\n        tid = uuid1()\n        tour = Tour(id=tid, name=input.name,\n           city=input.city, country=input.country, \n           type=input.type, location=input.location,\n           amenities=input.amenities, feedbacks=list(), \n           ratings=0.0, visits=0, isBooked=False)\n        tour_basic_info = TourBasicInfo(id=tid, \n           name=input.name, type=input.type, \n           amenities=input.amenities, ratings=0.0)\n        tour_location = TourLocation(id=tid, \n           name=input.name, city=input.city, \n           country=input.country, location=input.location )\n        tours[tid] = tour\n        tours_basic_info[tid] = tour_basic_info",
      "content_length": 2292,
      "extraction_method": "Direct"
    },
    {
      "page_number": 56,
      "chapter": null,
      "content": "Managing API-related exceptions\n33\n        tours_locations[tid] = tour_location\n        tour_json = jsonable_encoder(tour)\n        return JSONResponse(content=tour_json, \n            status_code=status.HTTP_201_CREATED)\n    except:\n        return JSONResponse(\n         content={\"message\" : \"invalid tour\"}, \n         status_code=status.HTTP_500_INTERNAL_SERVER_ERROR)\nThe add_tour_destination() operation here has a try-except block where its try block \nreturns the tour details and SC 201, while its catch block returns an error message inside a JSON-\ntype object with a server error of SC 500.\nRaising HTTPException\nAnother way of managing possible errors is by letting the REST API throw the HTTPException \nobject. HTTPException is a FastAPI class that has required constructor parameters: detail, \nwhich needs an error message in the str type, and status_code, which asks for a valid integer \nvalue. The detail part is converted to JSON-type and returned to the user as a response after the \nHTTPException instance is thrown by the operation. \nTo throw HTTPException, a validation process using any variations of if statements is more \nappropriate than using the try-except block because the cause of the error needs to be identified \nbefore throwing the HTTPException object using the raise statement. Once raise is executed, \nthe whole operation will halt and send the HTTP error message in JSON-type to the client with the \nspecified status code:\nfrom fastapi import APIRouter, HTTPException, status\n@router.post(\"/ch02/tourist/tour/booking/add\")\ndef create_booking(tour: TourBasicInfo, touristId: UUID):\n    if approved_users.get(touristId) == None:\n         raise HTTPException(status_code=500,\n            detail=\"details are missing\")\n    booking = Booking(id=uuid1(), destination=tour,\n      booking_date=datetime.now(), tourist_id=touristId)\n    approved_users[touristId].tours.append(tour)\n    approved_users[touristId].booked += 1\n    tours[tour.id].isBooked = True",
      "content_length": 1981,
      "extraction_method": "Direct"
    },
    {
      "page_number": 57,
      "chapter": null,
      "content": "Exploring the Core Features\n34\n    tours[tour.id].visits += 1\n    return booking\nThe create_booking() operation here simulates a booking process for a tourist account, but \nbefore the procedure starts, it first checks whether the tourist is still a valid user; otherwise, it will raise \nHTTPException, halting all the operations in order to return an error message.\nCustom exceptions\nIt is also possible to create a user-defined HTTPException object to handle business-specific \nproblems. This custom exception requires a custom handler needed to manage its response to the client \nwhenever an operation raises it. These custom components should be available to all API methods \nacross the project structure; thus, they must be implemented at the project-folder level.\nIn our application, there are two custom exceptions created in handler_exceptions.py, the \nPostFeedbackException and PostRatingFeedback exceptions, which handle problems \nrelated to posting feedback and ratings on a particular tour:\nfrom fastapi import FastAPI, Request, status, HTTPException\nclass PostFeedbackException(HTTPException):\n    def __init__(self, detail: str, status_code: int):\n        self.status_code = status_code\n        self.detail = detail\n        \nclass PostRatingException(HTTPException):\n    def __init__(self, detail: str, status_code: int):\n        self.status_code = status_code\n        self.detail = detail\nA valid FastAPI exception is a subclass of an HTTPException object inheriting the essential \nattributes, namely the status_code and detail attributes. We need to supply values to these \nattributes before the path operation raises the exception. After creating these custom exceptions, a \nspecific handler is implemented and mapped to an exception.",
      "content_length": 1750,
      "extraction_method": "Direct"
    },
    {
      "page_number": 58,
      "chapter": null,
      "content": "Managing API-related exceptions\n35\nThe FastAPI @app decorator in main.py has an exception_handler() method, used to define a \ncustom handler and map it to the appropriate custom exception. A handler is simply a Python function \nwith two local parameters, Request and the custom exception that it manages. The purpose of the \nRequest object is to retrieve cookies, payloads, headers, query parameters, and path parameters from \nthe path operation if the handler expects any of this request data. Now, once the custom exception is \nraised, the handler is set to generate a JSON-type response to the client containing the detail and \nthe status_code attributes provided by the path operation that raised the exception:\nfrom fastapi.responses import JSONResponse\nfrom fastapi import FastAPI, Request, status, HTTPException\n@app.exception_handler(PostFeedbackException)\ndef feedback_exception_handler(req: Request, \n          ex: PostFeedbackException):\n    return JSONResponse(\n        status_code=ex.status_code,\n        content={\"message\": f\"error: {ex.detail}\"}\n        )\n    \n@app.exception_handler(PostRatingException)\ndef rating_exception_handler(req: Request, \n             ex: PostRatingException):\n     return JSONResponse(\n        status_code=ex.status_code,\n        content={\"message\": f\"error: {ex.detail}\"}\n        )\nWhen an operation in post.py raises PostFeedbackException, the feedback_exception_\nhandler() given here will trigger its execution to generate a response that can provide details about \nwhat has caused the feedback problem. The same thing will happen to PostRatingException \nand its rating_exception_handler():\nfrom handlers import PostRatingException,\n                         PostFeedbackException\n    \n@router.post(\"/feedback/add\")\ndef post_tourist_feedback(touristId: UUID, tid: UUID,",
      "content_length": 1814,
      "extraction_method": "Direct"
    },
    {
      "page_number": 59,
      "chapter": null,
      "content": "Exploring the Core Features\n36\n      post: Post, bg_task: BackgroundTasks):\n    if approved_users.get(touristId) == None and \n          tours.get(tid) == None:\n        raise PostFeedbackException(detail='tourist and \n                tour details invalid', status_code=403)\n    assessId = uuid1()\n    assessment = Assessment(id=assessId, post=post, \n          tour_id= tid, tourist_id=touristId) \n    feedback_tour[assessId] = assessment\n    tours[tid].ratings = (tours[tid].ratings + \n                            post.rating)/2\n    bg_task.add_task(log_post_transaction, \n           str(touristId), message=\"post_tourist_feedback\")\n    assess_json = jsonable_encoder(assessment)\n    return JSONResponse(content=assess_json, \n                         status_code=200)\n@router.post(\"/feedback/update/rating\")\ndef update_tour_rating(assessId: UUID, \n               new_rating: StarRating):\n    if feedback_tour.get(assessId) == None:\n        raise PostRatingException(\n         detail='tour assessment invalid', status_code=403)\n    tid = feedback_tour[assessId].tour_id\n    tours[tid].ratings = (tours[tid].ratings + \n                            new_rating)/2\n    tour_json = jsonable_encoder(tours[tid])\n    return JSONResponse(content=tour_json, status_code=200)\npost_tourist_feedback() and update_tour_rating() here are the API operations that \nwill raise the PostFeedbackException and PostRatingException custom exceptions, \nrespectively, triggering the execution of their handlers. The detail and status_code values \ninjected into the constructor are passed to the handlers to create the response.",
      "content_length": 1600,
      "extraction_method": "Direct"
    },
    {
      "page_number": 60,
      "chapter": null,
      "content": "Managing API-related exceptions\n37\nA default handler override\nThe optimum way to override the exception-handling mechanism of your application is to replace the \nglobal exception handler of the FastAPI framework that manages its core Starlette’s HTTPException \nand the RequestValidationError triggered by Pydantic’s request validation process. For \ninstance, if we want to change the response format of the global exception sent to the client using \nraise from JSON-type to plain text, we can create custom handlers for each of the aforementioned \ncore exceptions that will pursue the format conversion. The following snippets of main.py show \nthese types of custom handlers:\nfrom fastapi.responses import PlainTextResponse \nfrom starlette.exceptions import HTTPException as \n         GlobalStarletteHTTPException\nfrom fastapi.exceptions import RequestValidationError\nfrom handler_exceptions import PostFeedbackException, \n        PostRatingException\n@app.exception_handler(GlobalStarletteHTTPException)\ndef global_exception_handler(req: Request, \n                 ex: str\n    return PlainTextResponse(f\"Error message: \n       {ex}\", status_code=ex.status_code)\n@app.exception_handler(RequestValidationError)\ndef validationerror_exception_handler(req: Request, \n                 ex: str\n    return PlainTextResponse(f\"Error message: \n       {str(ex)}\", status_code=400)\nBoth the global_exception_handler() and validationerror_exception_\nhandler() handlers are implemented to change the framework’s JSON-type exception response \nto PlainTextResponse. An alias, GlobalStarletteHTTPException, is assigned to \nStarlette’s HTTPException class to distinguish it from FastAPI’s HTTPException, which we \npreviously used to build custom exceptions. On the other hand, PostFeedbackException and \nPostRatingException are both implemented in the handler_exceptions.py module.\nJSON objects are all over the FastAPI framework’s REST API implementation, from the incoming \nrequest to the outgoing responses. However, what if the JSON data involved in the process is not a \nFastAPI JSON-compatible type? The following discussion will expound more upon this kind of object.",
      "content_length": 2156,
      "extraction_method": "Direct"
    },
    {
      "page_number": 61,
      "chapter": null,
      "content": "Exploring the Core Features\n38\nConverting objects to JSON-compatible types\nIt is easier for FastAPI to process JSON-compatible types such as dict, list, and BaseModel \nobjects because they can be easily converted to JSON by the framework using its default JSON editor. \nHowever, there are circumstances in which runtime exceptions are raised when processing BaseModel, \ndata model, or JSON objects containing data. One of the many reasons for this is that these data \nobjects have attributes that are not supported by JSON rules, such as UUID and non-built-in date \ntypes. Regardless, using a framework’s module classes, these objects can still be utilized by converting \nthem into JSON-compatible ones.\nWhen it comes to the direct handling of the API operation’s responses, FastAPI has a built-in method \nthat can encode typical model objects to convert them to JSON-compatible types before persisting \nthem to any datastore or passing them to the detail parameter of JSONResponse. This method, \njsonable_encoder(), returns a dict type with all the keys and values compatible with JSON:\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import JSONResponse\nclass Tourist(BaseModel):\n    id: UUID\n    login: User\n    date_signed: datetime\n    booked: int\n    tours: List[TourBasicInfo]\n    \n@router.post(\"/ch02/user/signup/\")\nasync def signup(signup: Signup):\n    try:\n        userid = uuid1()\n        login = User(id=userid, username=signup.username, \n               password=signup.password)\n        tourist = Tourist(id=userid, login=login, \n          date_signed=datetime.now(), booked=0, \n          tours=list() )\n        tourist_json = jsonable_encoder(tourist)\n        pending_users[userid] = tourist_json\n        return JSONResponse(content=tourist_json, \n            status_code=status.HTTP_201_CREATED)\n    except:",
      "content_length": 1843,
      "extraction_method": "Direct"
    },
    {
      "page_number": 62,
      "chapter": null,
      "content": "Managing API responses\n39\n        return JSONResponse(content={\"message\": \n         \"invalid operation\"}, \n         status_code=status.HTTP_500_INTERNAL_SERVER_ERROR)\nOur application has a POST operation, signup(), shown here that captures the profile of a newly \ncreated user to be approved by the administrator. If you observe the Tourist model class, it has a \ndate_signed attribute that is declared as datettime, and temporal types are not always JSON-\nfriendly. Having model objects with non-JSON-friendly components in FastAPI-related operations \ncan cause serious exceptions. To avoid these Pydantic validation issues, it is always advisable to use \njsonable_encoder() to manage the conversion of all the attributes of our model object into \nJSON-types.\nImportant note\nThe json module with its dumps() and loads() utility methods can be used instead of \njsonable_encoder() but a custom JSON encoder should be created to successfully map \nthe UUID type, the formatted date type, and other complex attribute types to str.\nChapter 9, Utilizing Other Advanced Features, will discuss other JSON encoders that can encode and \ndecode JSON responses faster than the json module.\nManaging API responses\nThe use of jsonable_encoder() can help an API method not only with data persistency \nproblems but also with the integrity and correctness of its response. In the signup() service \nmethod, JSONResponse returns the encoded Tourist model instead of the original object to \nensure that the client always received a JSON response. Aside from raising status codes and providing \nerror messages, JSONResponse can also do some tricks in handling the API responses to the client. \nAlthough optional in many circumstances, applying the encoder method when generating responses \nis recommended to avoid runtime errors:\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import JSONResponse\n@router.get(\"/ch02/destinations/details/{id}\")\ndef check_tour_profile(id: UUID):\n    tour_info_json = jsonable_encoder(tours[id])\n    return JSONResponse(content=tour_info_json)",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 63,
      "chapter": null,
      "content": "Exploring the Core Features\n40\ncheck_tour_profile() here uses JSONResponse to ensure that its response is JSON-\ncompatible and is fetched from the purpose of managing its exceptions. Moreover, it can also be used \nto return headers together with the JSON-type response:\n@router.get(\"/ch02/destinations/list/all\")\ndef list_tour_destinations():\n    tours_json = jsonable_encoder(tours)\n    resp_headers = {'X-Access-Tours': 'Try Us', \n       'X-Contact-Details':'1-900-888-TOLL', \n       'Set-Cookie':'AppName=ITS; Max-Age=3600; Version=1'}\n    return JSONResponse(content=tours_json, \n          headers=resp_headers)\nThe application’s list_tour_destinations() here returns three cookies: AppName, Max-Age, \nand Version, and two user-defined response headers. Headers that have names beginning with X- \nare custom headers. Besides JSONResponse, the fastapi module also has a Response class \nthat can create response headers:\nfrom fastapi import APIRouter, Response\n@router.get(\"/ch02/destinations/mostbooked\")\ndef check_recommended_tour(resp: Response):\n    resp.headers['X-Access-Tours'] = 'TryUs'\n    resp.headers['X-Contact-Details'] = '1900888TOLL'\n    resp.headers['Content-Language'] = 'en-US'\n    ranked_desc_rates = sort_orders = sorted(tours.items(),\n         key=lambda x: x[1].ratings, reverse=True)\n    return ranked_desc_rates;\nOur prototype’s check_recommend_tour() uses Response to create two custom response \nheaders and a known Content-Language. Always remember that headers are all str types and are \nstored in the browser for many reasons, such as creating an identity for the application, leaving user \ntrails, dropping advertisement-related data, or leaving an error message to the browser when an API \nencounters one:\n@router.get(\"/ch02/tourist/tour/booked\")\ndef show_booked_tours(touristId: UUID):\n    if approved_users.get(touristId) == None:\n         raise HTTPException(\n         status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,",
      "content_length": 1954,
      "extraction_method": "Direct"
    },
    {
      "page_number": 64,
      "chapter": null,
      "content": "Creating background processes\n41\n         detail=\"details are missing\", \n         headers={\"X-InputError\":\"missing tourist ID\"})\n    return approved_users[touristId].tours\nHTTPException, as shown in the show_booked_tours() service method here, not only \ncontains the status code and error message but also some headers in case the operation needs to leave \nsome error information to the browser once it is raised. \nLet us now explore the capability of FastAPI to create and manage transactions that are designed to \nrun in the background using some server threads.\nCreating background processes\nThe FastAPI framework is also capable of running background jobs as part of an API service execution. \nIt can even run more than one job almost simultaneously without intervening in the main service \nexecution. The class responsible for this is BackgroundTasks, which is part of the fastapi \nmodule. Conventionally, we declare this at the end of the parameter list of the API service method \nfor the framework to inject the BackgroundTask instance.\nIn our application, the task is to create audit logs of all API service executions and store them in an \naudit_log.txt file. This operation is part of the background.py script that is part of the main \nproject folder, and the code is shown here:\nfrom datetime import datetime\ndef audit_log_transaction(touristId: str, message=\"\"):\n    with open(\"audit_log.txt\", mode=\"a\") as logfile:\n        content = f\"tourist {touristId} executed {message} \n            at {datetime.now()}\"\n        logfile.write(content)\nHere, audit_log_transaction() must be injected into the application using BackgroundTasks’s \nadd_task() method to become a background process that will be executed by the framework later:\nfrom fastapi import APIRouter, status, BackgroundTasks\n@router.post(\"/ch02/user/login/\")\nasync def login(login: User, bg_task:BackgroundTasks):\n    try:\n        signup_json = \n           jsonable_encoder(approved_users[login.id])",
      "content_length": 1969,
      "extraction_method": "Direct"
    },
    {
      "page_number": 65,
      "chapter": null,
      "content": "Exploring the Core Features\n42\n        bg_task.add_task(audit_log_transaction,\n            touristId=str(login.id), message=\"login\")\n        return JSONResponse(content=signup_json, \n            status_code=status.HTTP_200_OK)\n    except:\n        return JSONResponse(\n         content={\"message\": \"invalid operation\"}, \n         status_code=status.HTTP_500_INTERNAL_SERVER_ERROR)\n    \n@router.get(\"/ch02/user/login/{username}/{password}\")\nasync def login(username:str, password: str, \n                    bg_task:BackgroundTasks):\n     tourist_list = [ tourist for tourist in \n        approved_users.values() \n          if tourist['login']['username'] == username and \n              tourist['login']['password'] == password] \n     if len(tourist_list) == 0 or tourist_list == None:\n        return JSONResponse(\n           content={\"message\": \"invalid operation\"}, \n           status_code=status.HTTP_403_FORBIDDEN)\n     else:\n        tourist = tourist_list[0]\n        tour_json = jsonable_encoder(tourist)\n        bg_task.add_task(audit_log_transaction, \n          touristId=str(tourist['login']['id']), \nmessage=\"login\")\n        return JSONResponse(content=tour_json, \n            status_code=status.HTTP_200_OK)\nThe login() service method is just one of the services of our application that logs its details. It uses \nthe bg_task object to add audit_log_transaction() into the framework to be processed \nlater. Transactions such as logging, SMTP-/FTP-related requirements, events, and some database-\nrelated triggers are the best candidates for background jobs.     \nImportant Note\nClients will always get their response from the REST API method despite the execution time \nof the background task. Background tasks are for processes that will take enough time that \nincluding them in the API operation could cause performance degradation.",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 66,
      "chapter": null,
      "content": "Using asynchronous path operations\n43\nUsing asynchronous path operations\nWhen it comes to improving performance, FastAPI is an asynchronous framework, and it uses Python’s \nAsyncIO principles and concepts to create a REST API implementation that can run separately and \nindependently from the application’s main thread. The idea also applies to how a background task is \nexecuted. Now, to create an asynchronous REST endpoint, attach async to the func signature of \nthe service:\n@router.get(\"/feedback/list\")\nasync def show_tourist_post(touristId: UUID):\n    tourist_posts = [assess for assess in feedback_tour.\nvalues() \n            if assess.tourist_id == touristId]\n    tourist_posts_json = jsonable_encoder(tourist_posts) \n    return JSONResponse(content=tourist_posts_json,\n                   status_code=200)\nOur application has a show_tourist_post() service that can retrieve all the feedback posted \nby a certain touristId about a vacation tour that they have experienced. The application will not \nbe affected no matter how long the service will take because its execution will be simultaneous to the \nmain thread.  \nImportant Note\nThe feedback APIRouter uses a /ch02/post prefix indicated in its main.py’s include_\nrouter() registration. So, to run show_tourist_post(), the URL should be http://\nlocalhost:8000/ch02/post.\nAn asynchronous API endpoint can invoke both synchronous and asynchronous Python functions \nthat can be DAO (Data Access Object), native services, or utility. Since FastAPI also follows the \nAsync/Await design pattern, the asynchronous endpoint can call an asynchronous non-API \noperation using the await keyword, which halts the API operation until the non-API transaction \nis done processing a promise:\nfrom utility import check_post_owner\n@router.delete(\"/feedback/delete\")\nasync def delete_tourist_feedback(assessId: UUID, \n              touristId: UUID ):\n    if approved_users.get(touristId) == None and \n            feedback_tour.get(assessId):\n        raise PostFeedbackException(detail='tourist and",
      "content_length": 2039,
      "extraction_method": "Direct"
    },
    {
      "page_number": 67,
      "chapter": null,
      "content": "Exploring the Core Features\n44\n              tour details invalid', status_code=403)    post_\ndelete = [access for access in feedback_tour.values()\n               if access.id == assessId]\n    for key in post_delete:\n        is_owner = await check_post_owner(feedback_tour, \n                       access.id, touristId)\n        if is_owner:\n            del feedback_tour[access.id]\n    return JSONResponse(content={\"message\" : f\"deleted\n          posts of {touristId}\"}, status_code=200)\ndelete_tourist_feedback() here is an asynchronous REST API endpoint that calls an \nasynchronous Python function, check_post_owner(), from the utility.py script. For the \ntwo components to have a handshake, the API service invokes check_post_owner(), using an \nawait keyword for the former to wait for the latter to finish its validation, and retrieves the promise \nthat it can get from await. \nImportant Note\nThe await keyword can only be used with the async REST API and native transactions, \nnot with synchronous ones. \nTo improve performance, you can add more threads within the uvicorn thread pool by including \nthe --workers option when running the server. Indicate your preferred number of threads after \ncalling the option:\nuvicorn main:app --workers 5 --reload\nChapter 8, Creating Coroutines, Events, and Message-Driven Transactions, will discuss the AsyncIO \nplatform and the use of coroutines in more detail. \nAnd now, the last, most important core feature that FastAPI can provide is the middleware or the \n\"request-response filter.\"    \nApplying middleware to filter path operations\nThere are FastAPI components that are inherently asynchronous and one of them is the middleware. \nIt is an asynchronous function that acts as a filter for the REST API services. It filters out the incoming \nrequest to pursue validation, authentication, logging, background processing, or content generation \nfrom the cookies, headers, request parameters, query parameters, form data, or authentication \ndetails of the request body before it reaches the API service method. Equally, it takes the outgoing \nresponse body to pursue rendition change, response header updates and additions, and other kinds of",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 68,
      "chapter": null,
      "content": "Applying middleware to filter path operations\n45\ntransformation that could possibly be applied to the response before it reaches the client. Middleware \nshould be implemented at the project level and can even be part of main.py:\n@app.middleware(\"http\")\nasync def log_transaction_filter(request: Request, \n             call_next):\n    start_time = datetime.now()\n    method_name= request.method\n    qp_map = request.query_parasms\n    pp_map = request.path_params\n    with open(\"request_log.txt\", mode=\"a\") as reqfile:\n        content = f\"method: {method_name}, query param: \n            {qp_map}, path params: {pp_map} received at \n            {datetime.now()}\"\n        reqfile.write(content)\n    response = await call_next(request)\n    process_time = datetime.now() - start_time\n    response.headers[\"X-Time-Elapsed\"] = str(process_time)\n    return response\nTo implement middleware, first, create an async function that has two local parameters: the \nfirst one is Request and the second one is a function called call_next(), which takes the \nRequest parameter as its argument to return the response. Then, decorate the method with @app.\nmiddleware(\"http\") to inject the component into the framework.\nThe tourist application has one middleware implemented by the asynchronous add_transaction_\nfilter() here that logs the necessary request data of a particular API method before its execution \nand modifies its response object by adding a response header, X-Time-Elapsed, which carries \nthe running time of the execution. \nThe execution of await call_next(request) is the most crucial part of the middleware \nbecause it explicitly controls the execution of the REST API service. It is the area of the component \nwhere Request passes through to the API execution for processing. Equally, it is where Response \ntunnels out, going to the client.\nBesides logging, middleware can also be used for implementing one-way or two-way authentication, \nchecking user roles and permissions, global exception handling, and other filtering-related operations \nright before the execution of call_next(). When it comes to controlling the outgoing Response, \nit can be used to modify the content type of the response, remove some existing browser cookies, \nmodify the response detail and status code, redirections, and other response transformation-related \ntransactions. Chapter 9, Utilizing Other Advanced Features, will discuss the types of middleware, \nmiddleware chaining, and other means to customize middleware to help build a better microservice.",
      "content_length": 2534,
      "extraction_method": "Direct"
    },
    {
      "page_number": 69,
      "chapter": null,
      "content": "Exploring the Core Features\n46\nImportant note\nThe FastAPI framework has some built-in middleware that is ready to be injected into the application \nsuch as GzipMiddleware, ServerErrorMiddleware, TrustedHostMiddleware, \nExceptionMiddleware, CORSMiddleware, SessionMiddleware, and \nHTTPSRedirectionMiddleware. \nSummary\nExploring the core details of a framework always helps us create a comprehensive plan and design to \nbuild quality applications to the required standards. We have learned that FastAPI injects all its incoming \nform data, request parameters, query parameters, cookies, request headers, and authentication details \ninto the Request object, and the outgoing cookies, response headers, and response data are carried \nout to the client by the Response object. When managing the response data, the framework has a \nbuilt-in jsonable_encoder() function that can convert the model into JSON types to be rendered \nby the JSONResponse object. Its middleware is one powerful feature of FastAPI because we can \ncustomize it to handle the Request object before it reaches the API execution and the Response \nobject before the client receives it.\nManaging the exceptions is always the first step to consider before creating a practical and sustainable \nsolution for the resiliency and health of a microservice architecture. Alongside its robust default \nStarlette global exception handler and Pydantic model validator, FastAPI allows exception-handling \ncustomization that provides the flexibility needed when business processes become intricate.\nFastAPI follows Python’s AsyncIO principles and standards for creating async REST endpoints, which \nmakes implementation easy, handy, and reliable. This kind of platform is helpful for building complex \narchitectures that require more threads and asynchronous transactions. \nThis chapter is a great leap toward fully learning about the principles and standards of how FastAPI \nmanages its web containers. The features highlighted in this chapter hitherto open up a new level of \nknowledge that we need to explore further if we want to utilize FastAPI to build great microservices. \nIn the next chapter, we will be discussing FastAPI dependency injection and how this design pattern \naffects our FastAPI projects.",
      "content_length": 2262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 70,
      "chapter": null,
      "content": "3\nInvestigating Dependency \nInjection\nSince the first chapter, Dependency Injection (DI) has been accountable for building clean and robust \nFastAPI REST services and was apparent in some of our sample APIs that required BaseModel, \nRequest, Response, and BackgroundTasks in their processes. Applying DI proves that \ninstantiating some FastAPI classes is not always the ideal approach, since the framework has a built-in \ncontainer that can provide the objects of these classes for the API services. This method of object \nmanagement makes FastAPI easy and efficient to use.\nFastAPI has a container where the DI policy is applied to instantiate module classes and even functions. \nWe only need to specify and declare these module APIs to the services, middleware, authenticator, \ndata sources, and test cases because the rest of the object assembly, management, and instantiation is \nnow the responsibility of the built-in container.\nThis chapter will help you to understand how to manage objects needed by the application, such as \nminimizing some instances and creating loose bindings among them. Knowing the effectiveness of \nDI on FastAPI is the first step in designing our microservice applications. Our discussions will focus \non the following:\n•\t Applying Inversion of Control (IoC) and DI\n•\t Exploring ways of injecting dependencies\n•\t Organizing a project based on dependencies\n•\t Using third-party containers\n•\t Scoping of instances",
      "content_length": 1442,
      "extraction_method": "Direct"
    },
    {
      "page_number": 71,
      "chapter": null,
      "content": "Investigating Dependency Injection\n48\nTechnical requirements\nThis chapter uses a software prototype called online recipe system, which manages, evaluates, rates, \nand reports recipes of different types and origins. Applying a DI pattern is the priority of this project, \nso expect some changes in the development strategies and approaches, such as adding model, \nrepository, and service folders. This software is for food enthusiasts or chefs who want to \nshare their specialties, newbies looking for recipes to experiment with, and guests who just like \nto browse through different food menus. This open-ended application does not use any database \nmanagement system yet, so all the data is temporarily stored in Python containers. Code is all uploaded \nat https://github.com/PacktPublishing/Building-Python-Microservices-\nwith-FastAPI/tree/main/ch03.\nApplying IoC/DI\nFastAPI is a framework that supports the IoC principle, which means that it has a container that \ncan instantiate objects for an application. In a typical programming scenario, we instantiate classes \nto use them in many ways to build a running application. But with IoC, the framework instantiates \nthe components for the application. Figure 3.1 shows the whole picture of the IoC principle and the \nparticipation of one of its forms, called the DI.\nFigure 3.1 – The IoC principle\nFor FastAPI, DI is not only a principle but a mechanism to integrate an object into a component that \nleads to creating a loosely coupled but highly cohesive software structure. Almost all components can \nbe candidates for DI, including functions. But for now, let us focus on callable components that provide \nsome JSON objects once they are injected into an API service – injectable and callable components \nthat we call dependency functions.",
      "content_length": 1795,
      "extraction_method": "Direct"
    },
    {
      "page_number": 72,
      "chapter": null,
      "content": "Applying IoC/DI\n49\nInjecting a dependency function\nA dependency function is a typical Python function that has parameters such as a path operation or an \nAPI service and a return value of a JSON type too. A sample implementation is create_login(), \nas shown in the following code, which is in the project’s /api/users.py module:\ndef create_login(id:UUID, username: str, password:str, \n                 type: UserType):\n    account = {\"id\": id, \"username\": username, \"password\":\n                 password, \"type\": type}\n    return account\nThe function requires the id, username, and password parameters and type to continue its \nprocess and return a valid JSON account object, derived from these parameters. A dependency \nfunction sometimes uses some underlying formula, resource, or complex algorithms to derive its \nfunction value, but for now, we utilize it as a placeholder of data or dict.\nCommon to dependency functions are method parameters that serve as placeholders to a REST \nAPI’s incoming request. These are wired into the API’s method parameter list as a domain model to \nthe query parameters or request body through DI. The Depends() function from the fastapi \nmodule pursues the injection before it wires the injectable to a local parameter. The module function \ncan only take one parameter for injection:\nfrom fastapi import APIRouter, Depends\n@router.get(\"/users/function/add\")\ndef populate_user_accounts(\n              user_account=Depends(create_login)):\n    account_dict = jsonable_encoder(user_account)\n    login = Login(**account_dict)\n    login_details[login.id] = login\n    return login\nThe preceding is a code fragment from our online recipe system that shows how Depends() \ninjects create_login()into the framework’s container and fetches its instance for wiring to the \npopulate_user_accounts() service. Syntactically, the injection process only needs the name \nof the function dependency without the parenthesis. Again, the purpose of create_login() is \nto capture the query parameters of the API service. The jsonable_encoder() is very useful to \nmany APIs in converting these injectables to JSON-compatible types such as dict, which are essential \nfor instantiating the needed data models and generating responses.",
      "content_length": 2243,
      "extraction_method": "Direct"
    },
    {
      "page_number": 73,
      "chapter": null,
      "content": "Investigating Dependency Injection\n50\nImportant Note\nThe term dependencies can be used interchangeably with injectables, dependables, resources, \nproviders, or components.\nInjecting a callable class\nFastAPI also allows classes to be injected into any components, since they can also be considered \ncallable components. A class becomes callable during instantiation when the call to its constructor, \n__init__(self), is done. Some of these classes have no-arg constructors, while others, such as \nthe following Login class, require constructor arguments:\nclass Login:\n    def __init__(self, id: UUID, username: str, \n                 password: str, type: UserType): \n        self.id = id\n        self.username = username\n        self.password = password\n        self.type= type\nThe Login class, located in /model/users.py, needs id, username, password, and type passed \nto its constructor before the instantiation. A possible instantiation would be Login(id=' 249a0837-\nc52e-48cd-bc19-c78e6099f931', username='admin', password='admin2255', \ntype=UserType.admin). Overall, we can observe the similarity between a class and a dependency \nfunction based on their callable behavior and the ability to capture request data, such as a model attribute.\nConversely, the populate_login_without_service() shown in the following code block \nshows how Depends() injects Login to the service. The Depends() function tells the built-in \ncontainer to instantiate Login and fetches that instance, ready to be assigned to the user_account \nlocal parameter:\n@router.post(\"/users/datamodel/add\")\ndef populate_login_without_service(\n              user_account=Depends(Login)):\n    account_dict = jsonable_encoder(user_account)\n    login = Login(**account_dict)\n    login_details[login.id] = login\n    return login",
      "content_length": 1792,
      "extraction_method": "Direct"
    },
    {
      "page_number": 74,
      "chapter": null,
      "content": "Applying IoC/DI\n51\nImportant Note\nAll dependencies should be declared at the right-most part of the service’s parameter list. If \nthere are query, path, or form parameters, injectables should come last. Moreover, the use of \nthe jsonable_encoder() function can be an option if the injectables do not contain data \nthat are hard to encode by default. \nBuilding nested dependencies\nThere are some scenarios when injectables are also dependent on other dependencies. When we \ninject a function dependency to another function, a class dependency to another class injectable, \nor a function resource to a class, the goal is to build nested dependencies. Nested dependencies are \nbeneficial to REST APIs, with lengthy and complex request data that needs structuring and grouping \nthrough sub-domain models. These sub-domains or domain models within a model are later encoded \nas sub-dependencies into JSON-compatible types by FastAPI:\nasync def create_user_details(id: UUID, firstname: str,\n         lastname: str, middle: str, bday: date, pos: str, \n         login=Depends(create_login)):\n    user = {\"id\": id, \"firstname\": firstname, \n            \"lastname\": lastname, \"middle\": middle, \n            \"bday\": bday, \"pos\": pos, \"login\": login}\n    return user\nThe preceding asynchronous create_user_details() function shows that even a dependency \nfunction needs another dependency to satisfy its purpose. This function is dependent on create_\nlogin(), which is another dependable component. With this nested dependency setup, wiring the \ncreate_user_details() into an API service also includes the injection of create_login() \ninto the container. In short, there is a chain of DIs that will be created when nested dependencies are \napplied:\n@router.post(\"/users/add/profile\")\nasync def add_profile_login(\n          profile=Depends(create_user_details)): \n    user_profile = jsonable_encoder(profile)\n    user = User(**user_profile)\n    login = user.login\n    login = Login(**login)\n    user_profiles[user.id] = user",
      "content_length": 2010,
      "extraction_method": "Direct"
    },
    {
      "page_number": 75,
      "chapter": null,
      "content": "Investigating Dependency Injection\n52\n    login_details[login.id] = login\n    return user_profile\nThe preceding add_profile_login() service provides a clear picture of its dependency on \ncreate_user_details(), including its underlying login details. The FastAPI container \nsuccessfully created the two functions through chained DI to capture the request data during the \nAPI’s request transactions. \nConversely, a class can also be dependable on another class. An example is the Profile class, shown \nhere, which is dependent on UserDetails and Login classes:\nclass Login:\n    def __init__(self, id: UUID, username: str, \n                 password: str, type: UserType): \n        self.id = id\n        self.username = username\n        self.password = password\n        self.type= type\nclass UserDetails: \n    def __init__(self, id: UUID, firstname: str, \n            lastname: str, middle: str, bday: date, \n               pos: str ):\n        self.id = id \n        self.firstname = firstname \n        self.lastname = lastname \n        self.middle = middle \n        self.bday = bday \n        self.pos = pos\n        \nclass Profile:\n    def __init__(self, id: UUID, date_created: date, \n        login=Depends(Login), user=Depends(UserDetails)): \n        self.id = id \n        self.date_created = date_created\n        self.login = login \n        self.user = user",
      "content_length": 1356,
      "extraction_method": "Direct"
    },
    {
      "page_number": 76,
      "chapter": null,
      "content": "Applying IoC/DI\n53\nThere is a nested dependency here because two classes will be injected altogether once Profile is \nwired to a REST API service. \nA clear advantage of these chained dependencies is depicted in the following add_profile_\nlogin_models() service:\n@router.post(\"/users/add/model/profile\")\nasync def add_profile_login_models(\n                   profile=Depends(Profile)): \n     user_details = jsonable_encoder(profile.user)\n     login_details = jsonable_encoder(profile.login)\n     user = UserDetails(**user_details)\n     login = Login(**login_details)\n     user_profiles[user.id] = user\n     login_details[login.id] = login\n     return {\"profile_created\": profile.date_created}\nThe extraction of profile.user and profile.login makes it easier for the service to identify \nwhat query data to deserialize. It also helps the service determine which group of data needs Login \ninstantiation and which is for UserDetails. As a result, it will be easier to manage the persistency \nof these objects in their respective dict repositories. \nCreating explicit dependencies between function and class will be discussed later, but for now, let \nus examine how to fine-tune whenever we use lots of these nested dependencies in our application.\nCaching the dependencies\nAll dependencies are cacheable, and FastAPI caches all these dependencies during a request transaction. \nIf a dependable is common to all services, FastAPI will not allow you to fetch these objects from its \ncontainer by default. Rather, it will look for this injectable from its cache to be used multiple times \nacross the API layer. Saving dependencies, especially nested ones, is a good feature of FastAPI because \nit optimizes the performance of the REST service.\nConversely, Depends() has a use_cache parameter that we can set to False if we want to \nbypass this caching mechanism. Configuring this hook will not save the dependencies from the cache \nduring request transactions, allowing Depends() to fetch the instances from the container more \nfrequently. Another version of the add_profile_login_models() service, shown here, shows \nhow to disable dependency caching:\n@router.post(\"/users/add/model/profile\")\nasync def add_profile_login_models(",
      "content_length": 2223,
      "extraction_method": "Direct"
    },
    {
      "page_number": 77,
      "chapter": null,
      "content": "Investigating Dependency Injection\n54\n   profile:Profile=Depends(Profile, use_cache=False)): \n     user_details = jsonable_encoder(profile.user)\n     login_details = jsonable_encoder(profile.login)\n     … … … … … …\n     return {\"profile_created\": profile.date_created}\nAnother obvious change in the preceding service implementation is the presence of the Profile \ndata type in the local parameter declaration. Is this really allowed by FastAPI?\nDeclaring Depends() parameter types\nGenerally, we do not declare types of the local parameters that will reference the injected dependencies. \nDue to type hints, we can optionally associate the references with their appropriate object types. For \ninstance, we can re-implement populate_user_accounts() to include the type of user_\naccount, such as the following one:\n@router.get(\"/users/function/add\")\ndef populate_user_accounts(\n           user_account:Login=Depends(create_login)):\n    account_dict = jsonable_encoder(user_account)\n    login = Login(**account_dict)\n    login_details[login.id] = login\n    return login\nThis scenario happens very rarely, since create_login() is a dependency function, and we usually \ndo not create classes only to provide the blueprint type of its returned values. But when we use class \ndependables, declaring the appropriate class type to the wired object is feasible, as in the following \nadd_profile_login_models() service, which declares the profile parameter as Profile:\n@router.post(\"/users/add/model/profile\")\nasync def add_profile_login_models(\n              profile:Profile=Depends(Profile)): \n     user_details = jsonable_encoder(profile.user)\n     login_details = jsonable_encoder(profile.login)\n     … … … … … …\n     return {\"profile_created\": profile.date_created}",
      "content_length": 1758,
      "extraction_method": "Direct"
    },
    {
      "page_number": 78,
      "chapter": null,
      "content": "Applying IoC/DI\n55\nAlthough the declaration is syntactically valid, the expression looks repetitive and redundant because \nthe Profile type appears twice in the declaration portion. To avoid this redundancy, we can replace \nthe statement with a shorthand version by omitting the class name inside the Depends() function. \nThus, a better way of declaring the preceding profile should be the following:\n@router.post(\"/users/add/model/profile\")\nasync def add_profile_login_models(\n              profile:Profile=Depends()): \n     user_details = jsonable_encoder(profile.user)\n     ... ... ... ... ... ...\n     return {\"profile_created\": profile.date_created}\nThe changes reflected on the parameter list will not affect the performance of the request transaction \nof the add_profile_login_models() service.\nInjecting asynchronous dependencies\nA FastAPI built-in container does not only manage synchronous function dependables but also \nasynchronous ones. The following create_user_details() is an asynchronous dependency, \nread to be wired to a service:\nasync def create_user_details(id: UUID, firstname: str, \n       lastname: str, middle: str, bday: date, pos: str, \n       login=Depends(create_login)):\n    user = {\"id\": id, \"firstname\": firstname, \n            \"lastname\": lastname, \"middle\": middle, \n            \"bday\": bday, \"pos\": pos, \"login\": login}\n    return user\nThe container can manage both synchronous and asynchronous function dependency. It can allow \nthe wiring of asynchronous dependables on an asynchronous API service or some asynchronous ones \non a synchronous API. In cases where the dependency and the services are both asynchronous, \napplying the async/await protocol is recommended to avoid discrepancies in the results. create_\nuser_details(), which is dependent on a synchronous create_login(), is wired on \nadd_profile_login(), which is an asynchronous API.",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 79,
      "chapter": null,
      "content": "Investigating Dependency Injection\n56\nAfter learning how DI design pattern works in FastAPI, the next step is to know the different levels \nof strategy for applying Depends() in our application.\nExploring ways of injecting dependencies\nFrom the previous discussions, we know that FastAPI has a built-in container through which some \nobjects are injected and instantiated. Likewise, we have learned that the only FastAPI components that \nare injectables are those so-called dependables, injectables, or dependencies. Now, let us enumerate \ndifferent ways to pursue the DI pattern in our application.\nDependency injection on services\nThe most common area where DI occurs is in the parameter list of a service method. Any discussions \nregarding this strategy have been tackled already in the previous examples, so we only need to present \nadditional points concerning this strategy: \n•\t First, the number of custom injectables a service method should take is also part of the concern. \nWhen it comes to complex query parameters or request bodies, API services can take more than \none injectable as long as there are no similar instance variable names among these dependables. \nThis variable name collision among the dependables will lead to having one parameter entry \nfor the conflicted variable during the request transactions, thus sharing the same value for all \nthese conflicting variables.\n•\t Second, the appropriate HTTP method operation to work with the injectables is also one aspect \nto consider. Both function and class dependencies can work with the GET, POST, PUT, and \nPATCH operations, except for those dependables with attribute types such as numeric Enum \nand UUID that can cause an HTTP Status 422 (Unprocessable Entity) due to conversion \nproblems. We must plan which HTTP method is applicable for some dependable(s) first, before \nimplementing the service method.\n•\t Third, not all dependables are placeholders of request data. Unlike the class dependables, \ndependency functions are not specifically used for returning objects or dict. Some of them are \nused in filtering request data, scrutinizing authentication details, managing form data, verifying \nheader values, handling cookies, and throwing some errors when there are violations of some rules. \nThe following get_all_recipes() service is dependent on a get_recipe_service() \ninjectable that will query all the recipes from the dict repository of the application:\n@router.get(\"/recipes/list/all\")\ndef get_all_recipes(handler=Depends(get_recipe_service)):\n      return handler.get_recipes()",
      "content_length": 2565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 80,
      "chapter": null,
      "content": "Exploring ways of injecting dependencies\n57\nThe dependency function provides the needed transactions such as saving and retrieving records of \nrecipes. Instead of the usual instantiation or method call, a better strategy is to inject these dependable \nservices into the API implementation. The handler method parameter, which refers to the instance \nof get_recipe_service(), invokes the get_recipes() transactions of a particular service \nto retrieve all the menus and ingredients stored in the repository.\nDependency injection on path operators\nThere is always an option to implement triggers, validators, and exception handlers as injectable \nfunctions. Since these dependables work like filters to the incoming request, their injection happens \nin the path operator and not in the service parameter list. The following code is an implementation of \nthe check_feedback_length() validator, found in /dependencies/posts.py, which \nchecks whether the feedback posted by a user regarding a recipe should be at least 20 characters, \nincluding spaces: \ndef check_feedback_length(request: Request): \n    feedback = request.query_params[\"feedback\"]\n    if feedback == None:\n        raise HTTPException(status_code=500, \n           detail=\"feedback does not exist\")\n    if len(feedback) < 20:\n        raise HTTPException(status_code=403, \n           detail=\"length of feedback … not lower … 20\")\nThe validator pauses the API execution to retrieve the feedback from a post to be validated if its \nlength is lower than 20. If the dependency function finds it True, it will throw an HTTP Status 403. \nAlternatively, it will emit a Status Code 500 if the feedback is missing from the request data; otherwise, \nit will let the API transaction finish its task.\nCompared to the create_post() and post_service() dependables, the following script \nshows that the check_feedback_length() validator is not invoked anywhere inside the \ninsert_post_feedback() service:\nasync def create_post(id:UUID, feedback: str, \n    rating: RecipeRating, userId: UUID, date_posted: date): \n    post = {\"id\": id, \"feedback\": feedback, \n            \"rating\": rating, \"userId\" : userId, \n            \"date_posted\": date_posted}\n    return post",
      "content_length": 2207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 81,
      "chapter": null,
      "content": "Investigating Dependency Injection\n58\n@router.post(\"/posts/insert\",\n      dependencies=[Depends(check_feedback_length)])\nasync def insert_post_feedback(post=Depends(create_post), \n          handler=Depends(post_service)): \n    post_dict = jsonable_encoder(post)\n    post_obj = Post(**post_dict)\n    handler.add_post(post_obj)\n    return post\nThe validator will always work closely with the incoming request transaction, whereas the other two \ninjectables, post and handler, are part of the API’s transactions.\nImportant Note\nThe path router of APIRouter can accommodate more than one injectable, which is why its \ndependencies parameter always needs a List value ([]).\nDependency injection on routers\nHowever, some transactions are not localized to work on one specific API. There are dependency \nfunctions created to work with a certain group of REST API services within an application, such as the \nfollowing count_user_by_type() and handler check_credential_error() events \nthat are designed to manage incoming requests of REST APIs under the user.router group. This \nstrategy requires DI at the APIRouter level:\nfrom fastapi import Request, HTTPException\nfrom repository.aggregates import stats_user_type\nimport json\ndef count_user_by_type(request: Request):\n    try:\n      count = \n          stats_user_type[request.query_params.get(\"type\")]\n      count += 1\n      stats_user_type[request.query_params.get(\"type\")] =\n          count\n      print(json.dumps(stats_user_type))\n    except:",
      "content_length": 1490,
      "extraction_method": "Direct"
    },
    {
      "page_number": 82,
      "chapter": null,
      "content": "Exploring ways of injecting dependencies\n59\n      stats_user_type[request.query_params.get(\"type\")] = 1\ndef check_credential_error(request: Request): \n    try:\n      username = request.query_params.get(\"username\")\n      password = request.query_params.get(\"password\")\n      if username == password:\n        raise HTTPException(status_code=403, \n         detail=\"username should not be equal to password\")\n    except:\n      raise HTTPException(status_code=500, \n           detail=\"encountered internal problems\")         \nBased on the preceding implementations, the goal of count_user_by_type() is to build an \nupdated frequency of users in stats_user_type according to UserType. Its execution starts \nright after the REST API receives a new user and login details from the client. While it checks the \nUserType of the new record, the API service pauses briefly and resumes after the function dependency \nhas completed its tasks.  \nConversely, the task of check_credential_error() is to ensure that the username and \npassword of the new user should not be the same. It throws an HTTP Status 403  when the credentials \nare the same, which will halt the whole REST service transaction.  \nInjecting these two dependables through APIRouter means that all the REST API services registered \nin that APIRouter will always trigger the executions of these dependencies. The dependencies can \nonly work with API services designed to persist the like of the user and login details, as shown here:\nfrom fastapi import APIRouter, Depends\nrouter = APIRouter(dependencies=[\n                      Depends(count_user_by_type), \n                      Depends(check_credential_error)])\n@router.get(\"/users/function/add\")\ndef populate_user_accounts(\n          user_account:Login=Depends(create_login)):\n    account_dict = jsonable_encoder(user_account)\n    login = Login(**account_dict)\n    login_details[login.id] = login\n    return login",
      "content_length": 1918,
      "extraction_method": "Direct"
    },
    {
      "page_number": 83,
      "chapter": null,
      "content": "Investigating Dependency Injection\n60\ncheck_credential_error(), which is injected into the APIRouter component, filters the \nusername and password derived from the create_login() injectable function. Likewise, it \nfilters the create_user_details() injectable of the add_profile_login() service, as \nshown in the following snippet:\n@router.post(\"/users/add/profile\")\nasync def add_profile_login(\n          profile=Depends(create_user_details)): \n    user_profile = jsonable_encoder(profile)\n    user = User(**user_profile)\n    login = user.login\n    login = Login(**login)\n    user_profiles[user.id] = user\n    login_details[login.id] = login\n    return user_profile\n@router.post(\"/users/datamodel/add\")\ndef populate_login_without_service(\n          user_account=Depends(Login)):\n    account_dict = jsonable_encoder(user_account)\n    login = Login(**account_dict)\n    login_details[login.id] = login\n    return login\nThe Login injectable class also undergoes filtering through check_credential_error(). \nIt also contains the username and password parameters that the injectable function can filter. \nConversely, the injectable Profile of the following add_profile_login_models() service is \nnot excluded from the error-checking mechanism because it has a Login dependency in its constructor. \nHaving the Login dependable means check_cedential_error() will also filter Profile.\nWith check_credential_error() is the count_user_by_type() injectable that counts \nthe number of users that access the API service:\n@router.post(\"/users/add/model/profile\")\nasync def add_profile_login_models(\n          profile:Profile=Depends(Profile)): \n     user_details = jsonable_encoder(profile.user)\n     ... ... ... ... ... ...",
      "content_length": 1709,
      "extraction_method": "Direct"
    },
    {
      "page_number": 84,
      "chapter": null,
      "content": "Exploring ways of injecting dependencies\n61\n     login = Login(**login_details)\n     user_profiles[user.id] = user\n     login_details[login.id] = login\n     return {\"profile_created\": profile.date_created}\nA dependency function wired into APIRouter should apply defensive programming and a proper \ntry-except to avoid parameter conflicts with the API services. If we were to run check_\ncredential_error() with a list_all_user() service, for instance, expect some runtime \nproblems because there is no login persistence involved during data retrieval.\nImportant Note\nLike its path operators, the constructor of APIRouter can also accept more than one injectable \nbecause its dependencies parameter will allow a List ([]) of valid ones.\nDependency injection on main.py\nThere are portions of the software that are very hard to automate because of their vast and complex \nscopes, so considering them will always be a waste of time and effort. These cross-cutting concerns \nspan from the UI level down to the data tier, which explains why these functionalities are impractical \nand even inconceivable to manage and implement using typical programming paradigms. These \ncross-cutting concerns are transactions such as exception logging, caching, instrumentation, and user \nauthorization, common to any application.\nFastAPI has an easy remedy to address these features: to create them as injectables to the FastAPI \ninstance of main.py: \nfrom fastapi import Request\nfrom uuid import uuid1\nservice_paths_log = dict()\ndef log_transaction(request: Request): \n    service_paths_log[uuid1()] = request.url.path\nThe preceding log_transaction() is a simple logger of the URL paths invoked or accessed by the \nclient. While the application is running, this cross-cut should propagate the repository with different \nURLs coming from any APIRouter. This task can only happen when we inject this function through \nthe FastAPI instance of main.py:\nfrom fastapi import FastAPI, Depends\nfrom api import recipes, users, posts, login, admin,",
      "content_length": 2018,
      "extraction_method": "Direct"
    },
    {
      "page_number": 85,
      "chapter": null,
      "content": "Investigating Dependency Injection\n62\n        keywords, admin_mcontainer, complaints\nfrom dependencies.global_transactions import    \n        log_transaction\napp = FastAPI(dependencies=[Depends(log_transaction)])\napp.include_router(recipes.router, prefix=\"/ch03\")\napp.include_router(users.router, prefix=\"/ch03\")\n   … … … … … …\napp.include_router(admin.router, prefix=\"/ch03\")\napp.include_router(keywords.router, prefix=\"/ch03\")\napp.include_router(admin_mcontainer.router, prefix=\"/ch03\")\napp.include_router(complaints.router, prefix=\"/ch03\")\nDependencies auto-wired to the FastAPI constructor are known as global dependencies because they \nare accessible by any REST APIs from the routers. For instance, log_transaction(), depicted \nin the preceding script, will execute every time the APIs from the recipes, users, posts, or \ncomplaints routers process their respective request transactions.\nImportant Note\nLike APIRouter, the constructor of FastAPI allows more than function dependency.\nAside from these strategies, DI can also help us organize our application by having repository, \nservice, and model layers. \nOrganizing a project based on dependencies\nIt is feasible to use a repository-service pattern in some complex FastAPI applications through DI. The \nrepository-service pattern is responsible for the creation of the repository layer of the application, \nwhich manages the Creation, Reading, Updates, and Deletion (CRUD) of data source. A repository \nlayer requires data models that depict the table schemas of a collection or database. The repository \nlayer needs the service layer to establish a connection with other parts of the application. The service \nlayer operates like a business layer, where the data sources and business processes meet to derive all the \nnecessary objects needed by the REST API. The communication between the repository and service \nlayers can only be possible by creating injectables. Now, let us explore how the layers shown in Figure \n3.2 are built by DI using injectable components.",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 86,
      "chapter": null,
      "content": "Organizing a project based on dependencies\n63\nFigure 3.2 – The repository-service layers\nThe model layer\nThis layer is purely composed of resources, collections, and Python classes that can be used by the \nrepository layer to create CRUD transactions. Some model classes are dependable on other models, \nbut some are just independent blueprints designed for data placeholder. Some of the application’s \nmodel classes that store recipe-related details are shown here:\nfrom uuid import UUID\nfrom model.classifications import Category, Origin\nfrom typing import Optional, List\nclass Ingredient:",
      "content_length": 591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 87,
      "chapter": null,
      "content": "Investigating Dependency Injection\n64\n    def __init__(self, id: UUID, name:str, qty : float,\n               measure : str):\n        self.id = id\n        self.name = name\n        self.qty = qty\n        self.measure = measure\n        \nclass Recipe:\n    def __init__(self, id: UUID, name: str, \n           ingredients: List[Ingredient], cat: Category, \n             orig: Origin):\n        self.id = id\n        self.name = name\n        self.ingredients = ingredients\n        self.cat = cat\n        self.orig = orig\nThe repository layer\nThis layer is composed of class dependencies, which have access to the data store or improvised dict \nrepositories, just like in our online recipe system. Together with the model layer, these repository \nclasses build the CRUD transactions needed by the REST API. The following is an implementation \nof RecipeRepository that has two transactions, namely insert_recipe() and query_\nrecipes():\nfrom model.recipes import Recipe\nfrom model.recipes import Ingredient\nfrom model.classifications import Category, Origin\nfrom uuid import uuid1 \nrecipes = dict()\nclass RecipeRepository: \n    def __init__(self): \n        ingrA1 = Ingredient(measure='cup', qty=1, \n             name='grape tomatoes', id=uuid1())\n        ingrA2 = Ingredient(measure='teaspoon', qty=0.5, \n             name='salt', id=uuid1())",
      "content_length": 1331,
      "extraction_method": "Direct"
    },
    {
      "page_number": 88,
      "chapter": null,
      "content": "Organizing a project based on dependencies\n65\n        ingrA3 = Ingredient(measure='pepper', qty=0.25, \n             name='pepper', id=uuid1())\n        … … … … … …\n        recipeA = Recipe(orig=Origin.european ,\n         ingredients= [ingrA1, ingrA2, ingrA3, ingrA4, \n              ingrA5, ingrA6, ingrA7, ingrA8, ingrA9], \n         cat= Category.breakfast, \n         name='Crustless quiche bites with asparagus and \n               oven-dried tomatoes', \n         id=uuid1())\n        ingrB1 = Ingredient(measure='tablespoon', qty=1, \n           name='oil', id=uuid1())\n        ingrB2 = Ingredient(measure='cup', qty=0.5, \n           name='chopped tomatoes', id=uuid1())\n        … … … … … …\n        recipeB = Recipe(orig=Origin.carribean ,\n           ingredients= [ingrB1, ingrB2, ingrB3, ingrB4, \n             ingrB5], \n           cat= Category.breakfast, \n           name='Fried eggs, Caribbean style', id=uuid1())\n        ingrC1 = Ingredient(measure='pounds', qty=2.25, \n           name='sweet yellow onions', id=uuid1())\n        ingrC2 = Ingredient(measure='cloves', qty=10, \n           name='garlic', id=uuid1())\n        … … … … … …\n        recipeC = Recipe(orig=Origin.mediterranean ,\n          ingredients= [ingrC1, ingrC2, ingrC3, ingrC4, \n             ingrC5, ingrC6, ingrC7, ingrC8], \n          cat= Category.soup, \n          name='Creamy roasted onion soup', id=uuid1())\n        \n        recipes[recipeA.id] = recipeA\n        recipes[recipeB.id] = recipeB",
      "content_length": 1464,
      "extraction_method": "Direct"
    },
    {
      "page_number": 89,
      "chapter": null,
      "content": "Investigating Dependency Injection\n66\n        recipes[recipeC.id] = recipeC\n        \n    def insert_recipe(self, recipe: Recipe):\n        recipes[recipe.id] = recipe\n  \n    def query_recipes(self):\n        return recipes\nIts constructor is used to populate the recipes with some initial data. The constructor of an injectable \nrepository class plays a role in datastore setup and configuration, and this is where we auto-wire \ndependency if there is any. Conversely, the implementation includes two Enum classes – Category and \nOrigin – which provide lookup values to the recipe’s menu category and place of origin respectively.\nThe repository factory methods\nThis layer uses the factory design pattern to add a more loose coupling design between the repository \nand service layer. Although this approach is optional, this is still an option to manage the threshold of \ninterdependency between the two layers, especially when there are frequent changes in the performance, \nprocesses, and results of the CRUD transactions. The following are the repository factory methods \nused by our application:\ndef get_recipe_repo(repo=Depends(RecipeRepository)):\n    return repo\ndef get_post_repo(repo=Depends(PostRepository)): \n    return repo\ndef get_users_repo(repo=Depends(AdminRepository)): \n    return repo\ndef get_keywords(keywords=Depends(KeywordRepository)): \n    return keywords\ndef get_bad_recipes(repo=Depends(BadRecipeRepository)): \n    return repo",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 90,
      "chapter": null,
      "content": "Organizing a project based on dependencies\n67\nWe can see from the preceding script that RecipeRepository is a dependable object of the \nfactory methods, which are also injectable components but of the service layer. For instance, get_\nrecipe_repo() will be wired to a service class to pursue the implementation of native services \nthat require some transactions from RecipeRepository. In a way, we are indirectly wiring the \nrepository class to the service layer.\nThe service layer\nThis layer has all the application’s services with the domain logic, such as our RecipeService, which \nprovides business processes and algorithms to RecipeRepository. The get_recipe_repo() \nfactory is injected through its constructor to provide CRUD transactions from RecipeRepository. \nThe injection strategy used here is the function of class dependency, which is depicted in the \nfollowing code:\nfrom model.recipes import Recipe\nfrom repository.factory import get_recipe_repo\nclass RecipeService: \n    def __init__(self, repo=Depends(get_recipe_repo)):\n        self.repo = repo\n        \n    def get_recipes(self):\n        return self.repo.query_recipes()\n    \n    def add_recipe(self, recipe: Recipe):\n        self.repo.insert_recipe(recipe)\nThe constructor of a typical Python class is always the appropriate place to inject components, which can \neither be a function or class dependable. With the preceding RecipeService, its get_recipes() \nand add_recipe() are realized because of the transactions derived from get_recipe_repo().\nThe REST API and the service layer\nThe REST API methods can directly inject the service class or factory method if it needs to access the \nservice layer. In our application, there is a factory method associated with each service class to apply \nthe same strategy used in the RecipeRepository injection. That is why, in the following script, \nthe get_recipe_service() method is wired to the REST API instead of RecipeService:\nclass IngredientReq(BaseModel):\n    id: UUID",
      "content_length": 1988,
      "extraction_method": "Direct"
    },
    {
      "page_number": 91,
      "chapter": null,
      "content": "Investigating Dependency Injection\n68\n    name:str\n    qty: int\n    measure: str\n      \nclass RecipeReq(BaseModel):\n    id: UUID \n    name: str\n    ingredients: List[IngredientReq]\n    cat: Category\n    orig: Origin\n     \nrouter = APIRouter()\n@router.post(\"/recipes/insert\")\ndef insert_recipe(recipe: RecipeReq, \n            handler=Depends(get_recipe_service)): \n    json_dict = jsonable_encoder(recipe)\n    rec = Recipe(**json_dict)\n    handler.add_recipe(rec)\n    return JSONResponse(content=json_dict, status_code=200)\n@router.get(\"/recipes/list/all\")\ndef get_all_recipes(handler=Depends(get_recipe_service)):\n    return handler.get_recipes()\nThe insert_recipe() is a REST API that accepts a recipe and its ingredients from a client for \npersistency, while get_all_recipes() returns List[Recipe] as a response.\nThe actual project structure\nWith the power of DI, we have created an online recipe system with an organized set of models, \nrepository, and service layers. The project structure shown in Figure 3.3 is quite different from the \nprevious prototypes because of the additional layers, but it still has main.py and all the packages \nand modules with their respective APIRouter.",
      "content_length": 1188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 92,
      "chapter": null,
      "content": "Using third-party containers\n69\nFigure 3.3 – The Online Recipe System’s project structure\nAt this point, DI has offered many advantages to FastAPI applications, from the engineering of object \ninstantiation to breaking down monolithic components to set up loosely coupled structures. But \nthere is only one slight problem: FastAPI’s default container. The framework’s container has no easy \nconfiguration to set all its managed objects to a singleton scope. Most applications prefer fetching \nsingleton objects to avoid wasting memory in the Python Virtual Machine (PVM). Moreover, the \nbuilt-in container is not open to a more detailed container configuration, such as having a multiple \ncontainer setup. The next series of discussions will focus on the limitation of FastAPI’s default container \nand solutions to overcome it.  \nUsing third-party containers\nDI has a lot to offer to improve our application, but it still depends on the framework we use to get \nthe full potential of this design pattern. FastAPI’s container is very acceptable to some when the \nconcerns are simply on object management and project organization. However, when it comes to \nconfiguring the container to add more advanced features, it is not feasible for short-term projects, \nand it will be impossible for huge applications due to constraints. So, the practical way is to rely on \nthird-party modules for the set of utilities needed to support all these advancements. So, let us explore \nthese popular external modules that integrate seamlessly with FastAPI, the Dependency Injector and \nLagom, which we can use to set up a complete and manageable container.\nUsing configurable containers – Dependency Injector\nWhen it comes to configurable containers, the Dependency Injector has several module APIs that can \nbe used to build variations of custom containers that can manage, assemble, and inject objects. But \nbefore we can use this module, we need to install it first using pip:\npip install dependency-injector",
      "content_length": 1995,
      "extraction_method": "Direct"
    },
    {
      "page_number": 93,
      "chapter": null,
      "content": "Investigating Dependency Injection\n70\nThe containers and providers module\nAmong all the API types, Dependency Injector is popular with its containers and providers. One of its \ncontainer types is DeclarativeContainer, which can be subclassed to contain all its providers. \nIts providers can be Factory, Dict, List, Callable, Singleton, or other containers. Both the \nDict and List providers are easy to set up because they only need list and dict respectively to \nbe instantiated. A Factory provider, conversely, instantiates any class, such as a repository, service, \nor a generic Python class, while Singleton only creates one instance per class, which is valid \nthroughout the application’s runtime. The Callable provider manages function dependencies, \nwhile Container instantiates other containers. Another container type is DynamicContainer, \nwhich is built from a configuration file, databases, or other resources.\nThe container types\nAside from these container APIs, the Dependency Injector allows us to customize a container based \non the volume of the dependable objects, project structure, or other criteria from the project. The \nmost common style or setup is the single declarative container that fits in small-, medium-, or large-\nscale applications. Our online recipe system prototype owns a single declarative container, which is \nimplemented in the following script:\nfrom dependency_injector import containers, providers\nfrom repository.users import login_details\nfrom repository.login import LoginRepository\nfrom repository.admin import AdminRepository\nfrom repository.keywords import KeywordRepository\nfrom service.recipe_utilities import get_recipe_names \nclass Container(containers.DeclarativeContainer):\n    loginservice = providers.Factory(LoginRepository)\n    adminservice = providers.Singleton(AdminRepository)\n    keywordservice = providers.Factory(KeywordRepository)\n    recipe_util = providers.Callable(get_recipe_names) \n    login_repo = providers.Dict(login_details)\nBy simply subclassing DeclarativeContainer, we can easily create a single container, \nwith its instances injected by the various providers previously mentioned. LoginRepository \nand KeywordRepository are both injected as new instances through the Factory provider. \nAdminRepository is an injected singleton object, get_recipe_names() is an injected \nfunction dependable, and login_details is an injected dictionary containing login credentials.",
      "content_length": 2441,
      "extraction_method": "Direct"
    },
    {
      "page_number": 94,
      "chapter": null,
      "content": "Using third-party containers\n71\nFastAPI and Dependency Injector integration\nTo wire the dependencies to a component through the Dependency Injector, the @inject decorator \nis applied. @inject is imported from the dependency_injector.wiring module and is \ndecorated over the dependent component. \nAfterward, the instance will be fetched from the container using the Provide wiring marker. Wiring \nmarkers search for the Provider object that references the injectable in the container, and if it \nexists, it will prepare for auto-wiring. Both @inject and Provide belong to the same API module:\nfrom repository.keywords import KeywordRepository\nfrom containers.single_container import Container\nfrom dependency_injector.wiring import inject, Provide\nfrom uuid import UUID\nrouter = APIRouter()\n@router.post(\"/keyword/insert\")\n@inject\ndef insert_recipe_keywords(*keywords: str, \n         keywordservice: KeywordRepository = \n           Depends(Provide[Container.keywordservice])): \n    if keywords != None:\n        keywords_list = list(keywords)\n        keywordservice.insert_keywords(keywords_list)\n        return JSONResponse(content={\"message\": \n          \"inserted recipe keywords\"}, status_code=201)\n    else:\n        return JSONResponse(content={\"message\": \n          \"invalid operation\"}, status_code=403)\nThe integration happens when the Depends() function directive is invoked to register the wiring \nmarker and the Provider instance to FastAPI. Aside from the acknowledgment, the registration \nadds type hints and Pydantic validation rules to the third-party Provider to appropriately wire \nthe injectables into FastAPI. The preceding script imports Container from its module to wire \nKeywordRepository through @inject, the wire marker, and the keywordservice \nProvider of Dependency Injector.",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 95,
      "chapter": null,
      "content": "Investigating Dependency Injection\n72\nNow, the last piece of the puzzle is to assemble, create, and deploy the single declarative container \nthrough the FastAPI platform. This last integration measure requires instantiating the container inside \nthe module where the injections happened and then invoking its wire() method, which builds the \nassemblage. Since the preceding insert_recipe_keywords() is part of /api/keywords.\npy, we should add the following lines in the keywords module script, particularly at its end portion:\nimport sys\n… … … … …\ncontainer = Container()\ncontainer.wire(modules=[sys.modules[__name__]])\nThe multiple container setup\nFor large applications, the number of repository transactions and services increases based on the \nfunctionality and special features of the application. If the single declarative type becomes unfeasible \nfor a growing application, then we can always replace it with a multiple-container setup.\nDependency Injector allows us to create a separate container for each group of services. Our application \nhas created a sample setup found in /containers/multiple_containers.py, just in case \nthis prototype becomes full-blown. That sample of multiple declarative containers is shown as follows:\nfrom dependency_injector import containers, providers\nfrom repository.login import LoginRepository\nfrom repository.admin import AdminRepository\nfrom repository.keywords import KeywordRepository\nclass KeywordsContainer(containers.DeclarativeContainer): \n    keywordservice = providers.Factory(KeywordRepository)\n    … … … … …\nclass AdminContainer(containers.DeclarativeContainer): \n    adminservice = providers.Singleton(AdminRepository)\n    … … … … …\nclass LoginContainer(containers.DeclarativeContainer): \n    loginservice = providers.Factory(LoginRepository)\n    … … … … …",
      "content_length": 1813,
      "extraction_method": "Direct"
    },
    {
      "page_number": 96,
      "chapter": null,
      "content": "Using third-party containers\n73\nclass RecipeAppContainer(containers.DeclarativeContainer): \n    keywordcontainer = \n          providers.Container(KeywordsContainer)\n    admincontainer = providers.Container(AdminContainer)\n    logincontainer = providers.Container(LoginContainer)\n    … … … … …\nBased on the preceding configuration, the three different instances of DeclarativeContainer \ncreated are KeywordsContainer, AdminContainer, and LoginContainer. The \nKeywordsContainer instance will assemble all dependencies related to keywords, AdminContainer \nwill hold all instances related to administrative tasks, and LoginContainer for login- and user-\nrelated services. Then, there is RecipeAppContainer, which will consolidate all these containers \nthrough DI also.\nThe injection of the dependencies to the API is like the single declarative style, except that the container \nneeds to be indicated in the wiring marker. The following is an admin-related API that shows how we \nwire dependencies to REST services:\nfrom dependency_injector.wiring import inject, Provide\nfrom repository.admin import AdminRepository\nfrom containers.multiple_containers import \n         RecipeAppContainer\n    \nrouter = APIRouter()\n@router.get(\"/admin/logs/visitors/list\")\n@inject\ndef list_logs_visitors(adminservice: AdminRepository =    \n   Depends(\n     Provide[\n      RecipeAppContainer.admincontainer.adminservice])): \n    logs_visitors_json = jsonable_encoder(\n           adminservice.query_logs_visitor())\n    return logs_visitors_json\nThe presence of admincontainer inside Provide checks first for the container of the same name \nbefore it fetches the adminservice provider that references the service dependable. The rest of the \ndetails are just the same with a single declarative, including the FastAPI integration and object assembly.",
      "content_length": 1824,
      "extraction_method": "Direct"
    },
    {
      "page_number": 97,
      "chapter": null,
      "content": "Investigating Dependency Injection\n74\nWhat is highlighted here about Dependency Injector is just basic configurations for simple applications. \nThere are still other features and integrations that this module can provide to optimize our application \nusing DI. Now, if we need thread-safe and non-blocking but with simple, streamlined, and straightforward \nAPIs, setup and configuration, there is the Lagom module.\nUsing a simple configuration – Lagom\nThe third-party Lagom module is widely used because of its simplicity when it comes to wiring \ndependables. It is also ideal for building asynchronous microservice-driven applications because \nit is thread-safe at runtime. Moreover, it can easily integrate into many web frameworks, including \nFastAPI. To apply its APIs, we need to install it first using pip:\npip install lagom\nThe container\nContainers in Lagom are created instantly using the Container class from its module. Unlike in \nDependency Injector, Lagom’s containers are created before the injection happens inside the module \nof the REST APIs:\nfrom lagom import Container\nfrom repository.complaints import BadRecipeRepository\ncontainer = Container()\ncontainer[BadRecipeRepository] = BadRecipeRepository()\nrouter = APIRouter()\nAll dependables are injected into the container through typical instantiation. The container behaves like \na dict when adding new dependables because it also uses a key-value pair as an entry. When we inject \nan object, the container needs its class name as its key and the instance as its value. Moreover, the DI \nframework also allows instantiation with arguments if the constructors require some parameter values.\nThe FastAPI and Lagom integration\nBefore the wiring happens, integration to the FastAPI platform must come first by instantiating a \nnew API class called FastApiIntegration, which is found in the lagom.integrations.\nfast_api module. It takes container as a required parameter:\nfrom lagom.integrations.fast_api import FastApiIntegration\ndeps = FastApiIntegration(container)",
      "content_length": 2029,
      "extraction_method": "Direct"
    },
    {
      "page_number": 98,
      "chapter": null,
      "content": "Scoping of dependables\n75\nThe dependables\nThe instance of FastAPIIntegration has a depends() method, which we will use to perform \nthe injection. One of the best features of Lagom is its easy and seamless integration mechanism into any \nframework. Thus, wiring the dependencies will not need FastAPI’s Depends() function anymore:\n@router.post(\"/complaint/recipe\")\ndef report_recipe(rid: UUID, \n     complaintservice=deps.depends(BadRecipeRepository)): \n        complaintservice.add_bad_recipe(rid)\n        return JSONResponse(content={\"message\": \n           \"reported bad recipe\"}, status_code=201)\nThe preceding report_recipe() utilizes BadRecipeRepository as an injectable service. Since \nit is part of the container, Lagom’s depends() function will search for the object in the container, \nand then it will be wired to the API service, if that exists, to save the complaints to the dict datastore.\nSo far, these two third-party modules are the most popular and elaborative when employing DI in our \napplications. These modules may change through future updates, but one thing is for sure: IoC and \nDI design patterns will always be the powerful solution in managing memory usage in an application. \nLet us now discusses issues surrounding memory space, container, and object assembly.\nScoping of dependables\nIn FastAPI, the scope of dependables can be either a new instance or a singleton. FastAPI’s DI does \nnot support the creation of singleton objects by default. In every execution of an API service with \ndependencies, FastAPI always fetches a new instance of each wired dependable, which can be proven \nby getting the object ID using id().\nA singleton object is created only once by a container, no matter how many times the framework \ninjects it. Its object ID remains the same the entire runtime of the application. Services and repository \nclasses are preferred to be singleton to control the increase of memory utilization of the application. And \nsince it is not easy to create a singleton with FastAPI, we can use either Dependency Injector or Lagom.",
      "content_length": 2065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 99,
      "chapter": null,
      "content": "Investigating Dependency Injection\n76\nThere is a Singleton provider in Dependency Injector that is responsible for the creation \nof singleton dependencies. This provider was mentioned already during the discussions on its \nDeclarativeContainer setup. With Lagom, there are two ways to create singleton injectables: \n(a) using its Singleton class, and (b) through the constructor of FastAPIIntegration. \nThe Singleton class wraps the instance of the dependency before injecting it into the container. \nThe following sample snippet shows one example:\ncontainer = Container()\ncontainer[BadRecipeRepository] = \n         Singleton(BadRecipeRepository())\nThe other way is to declare the dependency in the request_singletons parameter of the \nconstructor of FastAPIIntegration. The following snippet shows how it is done:\ncontainer = Container()\ncontainer[BadRecipeRepository] = BadRecipeRepository()\ndeps = FastApiIntegration(container,\n      request_singletons=[BadRecipeRepository])\nBy the way, the request_singletons parameter is a List type, so it will allow us to declare at \nleast one dependable when we want to make singletons.\nSummary\nOne aspect that makes a framework easy and practical to use is its support for the IoC principle. \nFastAPI has a built-in container that we can utilize to establish dependency among components. \nThe use of a DI pattern to integrate all these components through wiring is a strong prerequisite in \nbuilding microservice-driven applications. From simple injection using Depends(), we can extend \nDI to build pluggable components for database integration, authentication, security, and unit testing.\nThis chapter also introduced some third-party modules such as Dependency Injector and Lagom that \ncan design and customize containers. Because of the limitations of FastAPI on DI, there are external \nlibraries that can help extend its responsibility to assemble, control, and manage object creation in a \ncontainer. These third-party APIs can also create singleton objects, which can help decrease the heap \nsize in the PVM.  \nAside from performance tuning and memory management, DI can contribute to the organization \nof a project, especially huge applications. The addition of model, repository, and service layers is a \nremarkable effect of creating dependencies. Injection opens the development to other design patterns, \nsuch as factory method, service, and data access object patterns. In the next chapter, we will start to \nbuild some microservice-related components based on the core design patterns of microservices.",
      "content_length": 2559,
      "extraction_method": "Direct"
    },
    {
      "page_number": 100,
      "chapter": null,
      "content": "4\nBuilding the \nMicroservice Application\nPreviously, we spent a lot of time building API services for various applications using the core features \nof FastAPI. We also started applying important design patterns such as Inversion of Control (IoC) and \nDependency Injection (DI), which are essential for managing FastAPI container objects. External Python \npackages were installed and used to provide options on what containers to use in managing objects. \nThese design patterns can help not only with managed objects in container but also when building \nscalable, enterprise-grade, and unconventionally complex applications. Most of these design patterns help \nbreak down monolithic architecture into loosely coupled components that are known as microservices.\nIn this chapter, we will explore some architectural design patterns and principles that can provide \nstrategies and ways to initiate the building of our microservices from a monolithic application. Our \nfocus will be on breaking the huge application into business units, creating a sole gateway to bundle \nthese business units, applying domain modeling to each of the microservices, and managing other \nconcerns such as logging and application configuration.\nAside from expounding the benefits and disadvantages of each design pattern, another objective is to \napply these architectural patterns to our software specimen to show its effectiveness and feasibility. \nAnd to support these goals, the following topics will be covered in this chapter:\n•\t Applying the decomposition pattern\n•\t Creating a common gateway\n•\t Centralizing the logging mechanism\n•\t Consuming the REST APIs\n•\t Applying the domain modeling approach\n•\t Managing a microservice’s configuration details",
      "content_length": 1730,
      "extraction_method": "Direct"
    },
    {
      "page_number": 101,
      "chapter": null,
      "content": "Building the Microservice Application\n78\nTechnical requirements\nThis chapter uses a university ERP system prototype that focuses on the students, faculty, and library \nsubmodules, but more on student-library and faculty-library operations (for example, book borrowing \nand issuing). Each submodule has its administration, management, and transaction services, and \nthey are independent of each other even though they are part of an ERP specification. Currently, this \nsample prototype does not use any database management system, so all the data is temporarily stored \nin Python containers. The code is all uploaded at https://github.com/PacktPublishing/\nBuilding-Python-Microservices-with-FastAPI under the ch04, ch04-student, \nch04-faculty, and ch04-library projects.\nApplying the decomposition pattern\nIf we apply the monolithic strategy used in building the prototypes presented in the previous chapters, \nbuilding this ERP will not be cost-effective in terms of resources and effort. There will be features that \nmight become too dependent on other functions, which will put the teams of developers in a difficult \nsituation whenever transaction problems occur due to these tightly coupled features. The best way \nto implement our University ERP prototype is to decompose the whole specification into smaller \nmodules before the implementation starts. \nThere are two appropriate ways in which to decompose our application prototype, namely decomposition \nby business units and decomposition by subdomains: \n•\t Decomposition by business units is used when the breakdown of the monolithic application is \nbased on organizational structures, architectural components, and structural units. Usually, \nits resulting modules have fixed and structured processes and functionality that are seldom \nenhanced or upgraded. \n•\t Decomposition by subdomain uses domain models and their corresponding business processes as \nthe basis of the breakdown. Unlike the former, this decomposition strategy deals with modules \nthat continuously evolve and change to capture the exact structure of the modules. \nOf the two options, decomposition by business units is the more practical decomposition strategy \nto use for our monolithic University ERP prototype. Since the information and business flow used \nby universities has been part of its foundation for years, we need to organize and breakdown its \nvoluminous and compounded operations by colleges or departments. Figure 4.1 shows the derivation \nof these submodules:",
      "content_length": 2505,
      "extraction_method": "Direct"
    },
    {
      "page_number": 102,
      "chapter": null,
      "content": "Applying the decomposition pattern\n79\nFigure 4.1 – Decomposition by business units\nAfter determining the submodules, we can implement them as independent microservices using \nthe FastAPI framework. We can call an implementation of a business unit or module a microservice \nif its services can collectively stand as one component. Also, it must be able to collaborate with other \nmicroservices through interconnection based on the URL address and port number. Figure 4.2 shows \nthe project directories of the faculty, library, and student management modules implemented as \nFastAPI microservice applications. Chapter 1, Setting Up FastAPI for Starters, to Chapter 3, Investigating \nDependency Injection, gave us the foundation to build a FastAPI microservice:\nFigure 4.2 – The faculty, library, and student microservice applications\nEach of these microservices is independent of the others in terms of its server instance and management. \nStarting and shutting down one of them will not affect the other two, as each can have a different context \nroot and port. Each application can have a separate logging mechanism, dependency environment, \ncontainer, configuration file, and any other aspect of a microservice, which will be discussed in the \nsubsequent chapters.\nBut FastAPI has another way of designing microservices using a mount sub-application.",
      "content_length": 1351,
      "extraction_method": "Direct"
    },
    {
      "page_number": 103,
      "chapter": null,
      "content": "Building the Microservice Application\n80\nCreating the sub-applications\nFastAPI allows you to build independent sub-applications inside the main application. Here, main.py \nserves as a gateway that provides a pathname to these mounted applications. It also creates the mounts \nspecifying the context path mapped to the FastAPI instance of each sub-application. Figure 4.3 shows \na new university ERP implementation that has been built using mounts:\nFigure 4.3 – The main project with the mounts\nHere, faculty_mgt, library_mgt, and student_mgt are typical independent microservice \napplications mounted into the main.py component, the top-level application. Each sub-application \nhas a main.py component, such as library_mgt, which has its FastAPI instance created in its \nlibrary_main.py setup, as shown in the following code snippet:\nfrom fastapi import FastAPI\nlibrary_app = FastAPI()\nlibrary_app.include_router(admin.router)\nlibrary_app.include_router(management.router)\nThe student sub-application has a student_main.py setup that creates its FastAPI instance, as \nshown in the following code:\nfrom fastapi import FastAPI\nstudent_app = FastAPI()\nstudent_app.include_router(reservations.router)\nstudent_app.include_router(admin.router)",
      "content_length": 1237,
      "extraction_method": "Direct"
    },
    {
      "page_number": 104,
      "chapter": null,
      "content": "Mounting the submodules\n81\nstudent_app.include_router(assignments.router)\nstudent_app.include_router(books.router)\nLikewise, the faculty sub-application also has its faculty_main.py setup, as highlighted in the \nfollowing code, for the same purpose, to build the microservice architecture:\nfrom fastapi import FastAPI\nfaculty_app = FastAPI()\nfaculty_app.include_router(admin.router)\nfaculty_app.include_router(assignments.router)\nfaculty_app.include_router(books.router)\nThese sub-applications are typical FastAPI microservice applications containing all of the essential \ncomponents such as routers, middleware exception handlers, and all the necessary packages to build \nREST API services. The only difference from the usual applications is that their context paths or URLs \nare defined and decided by the top-level application that handles them. \nImportant note\nOptionally, we can run the library_mgt sub-application independently from main.py \nthrough the uvicorn main:library_app --port 8001 command, faculty_mgt \nthrough uvicorn main:faculty_app --port 8082, and student_mgt through \nuvicorn main:student_app --port 8003. The option of running them independently \ndespite the mount explains why these mounted sub-applications are all microservices.\nMounting the submodules\nAll the FastAPI decorators of each sub-application must be mounted in the main.py component \nof the top-level application for them to be accessed at runtime. The mount() function is invoked \nby the FastAPI decorator object of the top-level application, which adds all FastAPI instances of the \nsub-applications into the gateway application (main.py) and maps each with its corresponding URL \ncontext. The following script shows how the mounting of the library, student, and faculty subsystems \nis implemented in the main.py component of the University ERP top-level system:\nfrom fastapi import FastAPI\nfrom student_mgt import student_main\nfrom faculty_mgt import faculty_main\nfrom library_mgt import library_main\napp = FastAPI()",
      "content_length": 2007,
      "extraction_method": "Direct"
    },
    {
      "page_number": 105,
      "chapter": null,
      "content": "Building the Microservice Application\n82\napp.mount(\"/ch04/student\", student_main.student_app)\napp.mount(\"/ch04/faculty\", faculty_main.faculty_app)\napp.mount(\"/ch04/library\", library_main.library_app)\nWith this setup, the mounted /ch04/student URL will be used to access all the API services of \nthe student module app, /ch04/faculty will be used for all the services of the faculty module, and \n/ch04/library will be used for the library-related REST services. These mounted paths become \nvalid once they are declared in mount() because FastAPI automatically handles all of these paths \nthrough the root_path specification.\nSince all three sub-applications of our university ERP system are independent microservices, now let us \napply another design strategy that can help manage the requests to these applications just by using the \nmain URL of the ERP system. Let us utilize the main application as a gateway to our sub-applications.\nCreating a common gateway\nIt will be easier if we use the URL of the main application to manage the requests and redirect users \nto any of the three sub-applications. The main application can stand as a pseudo-reverse proxy \nor an entry point for user requests, which will always redirect user requests to any of the desired \nsub-applications. This kind of approach is based on a design pattern called API Gateway. Now, let us \nexplore how we can apply this design to manage independent microservices mounted onto the main \napplication using a workaround.\nImplementing the main endpoint\nThere are so many solutions when it comes to implementing this gateway endpoint, and among them \nis having a simple REST API service in the top-level application with an integer path parameter that \nwill identify the ID parameter of the microservice. If the ID parameter is invalid, the endpoint will \nonly return the {'message': 'University ERP Systems'} JSON string instead of an \nerror. The following script is a straightforward implementation of this endpoint:\nfrom fastapi import APIRouter\nrouter = APIRouter()\n@router.get(\"/university/{portal_id}\")\ndef access_portal(portal_id:int): \n    return {'message': 'University ERP Systems'}",
      "content_length": 2161,
      "extraction_method": "Direct"
    },
    {
      "page_number": 106,
      "chapter": null,
      "content": "Evaluating the microservice ID\n83\nThe access_portal API endpoint is created as a GET path operation with portal_id as its \npath parameter. The portal_id parameter is essential to this process because it will determine \nwhich among the Student, Faculty, and Library microservices the user wants to access. Therefore, \naccessing the /ch04/university/1 URL should lead the user to the student application, /ch04/\nuniversity/2 to the faculty microservice, and /ch04/university/3 to the library application.\nEvaluating the microservice ID\nThe portal_id parameter will automatically be fetched and evaluated using a dependable function \nthat is injected into the APIRouter instance where the API endpoint is implemented. As discussed \nin Chapter 3, Investigating Dependency Injection, a dependable function or object can serve as a filter \nor validator of all incoming requests of any services once injected into an APIRouter or FastAPI \ninstance. The dependable function used in this ERP prototype, as shown in the following script, \nevaluates whether the portal_id parameter is 1, 2, or 3 only:\ndef call_api_gateway(request: Request): \n    portal_id = request.path_params['portal_id']\n    print(request.path_params)\n    if portal_id == str(1): \n        raise RedirectStudentPortalException() \n    elif portal_id == str(2): \n        raise RedirectFacultyPortalException() \n    elif portal_id == str(3): \n        raise RedirectLibraryPortalException()\nclass RedirectStudentPortalException(Exception):\n    pass\nclass RedirectFacultyPortalException(Exception):\n    pass\nclass RedirectLibraryPortalException(Exception):\n    pass",
      "content_length": 1619,
      "extraction_method": "Direct"
    },
    {
      "page_number": 107,
      "chapter": null,
      "content": "Building the Microservice Application\n84\nThe given solution is a feasible workaround to trigger a custom event since FastAPI has no built-in \nevent handling except for the startup and shutdown event handlers, which are topics in Chapter 8, \nCreating Coroutines, Events, and Message-Driven Transactions. So, once call_api_gateway() \nfinds portal_id to be a valid microservice ID, it will raise some custom exceptions. It will throw \nRedirectStudentPortalException if the user wants to access the Student microservice. On \nthe other hand, the RedirectFacultyPortalException error will be raised if the user desires \nthe Faculty microservice. Otherwise, the RedirectLibraryPortalException error will be \ntriggered when the Library microservice is the one wanted by the user. But first, we need to inject \ncall_api_gateway() into the APIRouter instance handling the gateway endpoint through \nthe main.py component of the top-level ERP application. The following script shows you how it is \ninjected into university.router using the concepts discussed earlier:\nfrom fastapi import FastAPI, Depends, Request, Response\nfrom gateway.api_router import call_api_gateway\nfrom controller import university\napp = FastAPI()\napp.include_router (university.router, \n           dependencies=[Depends(call_api_gateway)], \n           prefix='/ch04')\nAll of these raised exceptions require an exception handler that will listen to the throws and execute \nsome of the tasks required to pursue the microservices.\nApplying the exception handlers\nThe exception handler does a redirection to the appropriate microservice. As you learned in Chapter \n2, Exploring the Core Features, each thrown exception must have its corresponding exception handler \nto pursue the required response after the exception handling. Here are the exception handlers that \nwill handle the custom exception thrown by call_api_gateway():\nfrom fastapi.responses import RedirectResponse\nfrom gateway.api_router import call_api_gateway, \n     RedirectStudentPortalException, \n     RedirectFacultyPortalException, \n     RedirectLibraryPortalException",
      "content_length": 2096,
      "extraction_method": "Direct"
    },
    {
      "page_number": 108,
      "chapter": null,
      "content": "Applying the exception handlers\n85\n@app.exception_handler(RedirectStudentPortalException)\ndef exception_handler_student(request: Request, \n   exc: RedirectStudentPortalException) -> Response:\n    return RedirectResponse(\n        url='http://localhost:8000/ch04/student/index')\n@app.exception_handler(RedirectFacultyPortalException)\ndef exception_handler_faculty(request: Request, \n   exc: RedirectFacultyPortalException) -> Response:\n    return RedirectResponse(\n       url='http://localhost:8000/ch04/faculty/index')\n@app.exception_handler(RedirectLibraryPortalException)\ndef exception_handler_library(request: Request, \n   exc: RedirectLibraryPortalException) -> Response:\n    return RedirectResponse(\n       url='http://localhost:8000/ch04/library/index')\nHere, exception_handler_student() will redirect the user to the mount path of the Student \nmicroservice, while exception_handler_faculty() will redirect the user to the Faculty \nsub-application. Additionally, exception_handler_library() will let the user access the \nLibrary microservice. Exception handlers are the last component needed to complete the API Gateway \narchitecture. The exceptions trigger the redirection to the independent microservices mounted on \nthe FastAPI framework.\nAlthough there are other, better solutions to achieve the gateway architecture, our approach is still \nprocedural and pragmatic without having to resort to external modules and tools, just the core \ncomponents of FastAPI. Chapter 11, Adding Other Microservices Features, will discuss establishing \nan effective API Gateway architecture using Docker and NGINX. \nNow, let us explore how to set up a centralized logging mechanism for this kind of microservices setup.",
      "content_length": 1711,
      "extraction_method": "Direct"
    },
    {
      "page_number": 109,
      "chapter": null,
      "content": "Building the Microservice Application\n86\nCentralizing the logging mechanism\nWe have created an audit trail mechanism with middleware and Python file transactions in Chapter 2, \nExploring the Core Features. We have found out that middleware, which can only be set up through the \nFastAPI decorator of the top-level application, can manage incoming Request and outgoing Response of \nany API services. This time, we will be using custom middleware to set up a centralized logging feature \nthat will log all service transactions of the top-level application alongside its independent mounted \nmicroservices. Of the many approaches for integrating these logging concerns into the application \nwithout changing the API services, we will concentrate on the following pragmatic custom approach \nwith the custom middleware and Loguru module.\nUtilizing the Loguru module\nAn application log is essential to any enterprise-grade application. For monolithic applications deployed \nin a single server, logging means letting service transactions write their log messages to a single file. \nOn the other hand, logging can be too complex and complicated to implement in an independent \nmicroservices setup, especially when these services are for deployment to different servers or Docker \ncontainers. Its logging mechanism could even cause runtime problems if the module used is not \nadaptable to asynchronous services.\nFor FastAPI instances that support both asynchronous and synchronous API services that run on an \nASGI server, using Python’s logging module always generates the following error log:\n2021-11-08 01:17:22,336 - uvicorn.error - ERROR - Exception in \nASGI application\nTraceback (most recent call last):\n  File \"c:\\alibata\\development\\language\\python\\\n  python38\\lib\\site-packages\\uvicorn\\protocols\\http\\\n  httptools_impl.py\", line 371, in run_asgi\n    result = await app(self.scope, self.receive, self.send)\n  File \"c:\\alibata\\development\\language\\python\\\n  python38\\lib\\site-packages\\uvicorn\\middleware\\\n  proxy_headers.py\", line 59, in __call__\n    return await self.app(scope, receive, send)\nOpting for another logging extension is the only solution to avoid the error generated by the logging \nmodule. The best option is one that can fully support the FastAPI framework, which is the loguru \nextension. But first, we need to install it using the pip command:\npip install loguru",
      "content_length": 2380,
      "extraction_method": "Direct"
    },
    {
      "page_number": 110,
      "chapter": null,
      "content": "Centralizing the logging mechanism\n87\nLoguru is a straightforward and easy-to-use logging extension. We can immediately log using its default \nhandler, the sys.stderr handler, even without adding much configurations. Since our application \nneeds to place all messages in a log file, we need to add the following lines to the main.py component \nof the top-level application right after the instantiation of FastAPI:\nfrom loguru import logger\nfrom uuid import uuid4\napp = FastAPI()\napp.include_router (university.router, \n         dependencies=[Depends(call_api_gateway)], \n         prefix='/ch04')\nlogger.add(\"info.log\",format=\"Log: [{extra[log_id]}: \n   {time} - {level} - {message} \", level=\"INFO\", \n   enqueue = True)\nNote that its logger instance has an add() method where we can register sinks. The first part \nof the sinks is the handler that decides whether to emit the logs in sys.stdout or the file. In our \nuniversity ERP prototype, we need to have a global info.log file that contains all the log messages \nof the sub-applications. \nA crucial part of the log sink is the level type, which indicates the granularity of log messages that \nneed to be managed and logged. If we set the level parameter of add() to INFO, it tells the logger \nto consider only those messages under the INFO, SUCCESS, WARNING, ERROR, and CRITICAL \nweights. The logger will bypass log messages outside these levels.\nAnother part of the sinks is the format log, where we can create a custom log message layout to \nreplace its default format. This format is just like a Python interpolated string without the \"f\" that \ncontains placeholders such as {time}, {level}, {message}, and any custom placeholders that \nneed to be replaced by logger at runtime. \nIn log.file, we want our logs to start with the Log keyword followed immediately by the custom-\ngenerated log_id parameter and then the time the logging happened, the level, and the message. \nAnd to add support for asynchronous logging, the add() function has an enqueue parameter that \nwe can enable anytime. In our case, this parameter is default to True just to prepare for any async/\nawait execution.",
      "content_length": 2141,
      "extraction_method": "Direct"
    },
    {
      "page_number": 111,
      "chapter": null,
      "content": "Building the Microservice Application\n88\nThere is a lot to explore with Loguru’s features and functionality. For instance, we can create additional \nhandlers for loggers to emit where each of these handlers has different retention, rotation, and rendition \ntypes. Additionally, Loguru can allow us to add colors to our logs through some color markups such \nas <red>, <blue>, or <cyan>. It also has an @catch() decorator that can be applied to manage \nexceptions at runtime. All the logging features we need to set up our unified application log are in \nLoguru. Now that we have configured our Loguru in the top-level application, we need to let its logging \nmechanism work across the three sub-applications or microservices without modifying their code.\nBuilding the logging middleware\nThe core component of this centralized application log is the custom middleware that we must implement \nin the main.py component where we set up Loguru. FastAPI’s mount allows us to centralize some \ncross-cutting concerns such as logging without adding anything to the sub-applications. One middleware \nimplementation in the main.py component of the top-level application is good enough to pursue \nlogging across the independent microservices. The following is the middleware implementation for \nour specimen application:\n@app.middleware(\"http\")\nasync def log_middleware(request:Request, call_next):\n    log_id = str(uuid4())\n    with logger.contextualize(log_id=log_id):\n        logger.info('Request to access ' + \n             request.url.path)\n        try:\n            response = await call_next(request)\n        except Exception as ex: \n            logger.error(f\"Request to \" + \n              request.url.path + \" failed: {ex}\")\n            response = JSONResponse(content=\n               {\"success\": False}, status_code=500)\n        finally: \n            logger.info('Successfully accessed ' + \n               request.url.path)\n            return response",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 112,
      "chapter": null,
      "content": "Building the logging middleware\n89\nFirst, log_middleware() will generate a log_id parameter every time it intercepts any API \nservices from the main app or the sub-applications. Then, the log_id parameter is injected into \nthe dict of context information through Loguru’s contextualize() method since log_id is \npart of the log information, as indicated in our log format setup. Afterward, logging starts before the \nAPI service is executed and after its successful execution. When exceptions are encountered during \nthe process, the logger will still generate a log message with the Exception message. So, whenever \nwe access any API services anywhere from the ERP prototype, the following log messages will be \nwritten in info.log:\nLog: [1e320914-d166-4f5e-a39b-09723e04400d: 2021-11-\n28T12:02:25.582056+0800 - INFO - Request to access /ch04/\nuniversity/1 \nLog: [1e320914-d166-4f5e-a39b-09723e04400d: 2021-11-\n28T12:02:25.597036+0800 - INFO - Successfully accessed /ch04/\nuniversity/1 \nLog: [fd3badeb-8d38-4aec-b2cb-017da853e3db: 2021-11-\n28T12:02:25.609162+0800 - INFO - Request to access /ch04/\nstudent/index \nLog: [fd3badeb-8d38-4aec-b2cb-017da853e3db: 2021-11-\n28T12:02:25.617177+0800 - INFO - Successfully accessed /ch04/\nstudent/index \nLog: [4cdb1a46-59c8-4762-8b4b-291041a95788: 2021-11-\n28T12:03:25.187495+0800 - INFO - Request to access /ch04/\nstudent/profile/add \nLog: [4cdb1a46-59c8-4762-8b4b-291041a95788: 2021-11-\n28T12:03:25.203421+0800 - \nINFO - Request to access /ch04/faculty/index \nLog: [5cde7503-cb5e-4bda-aebe-4103b2894ffe: 2021-11-\n28T12:03:33.432919+0800 - INFO - Successfully accessed /ch04/\nfaculty/index \nLog: [7d237742-fdac-4f4f-9604-ce49d3c4c3a7: 2021-11-\n28T12:04:46.126516+0800 - INFO - Request to access /ch04/\nfaculty/books/request/list \nLog: [3a496d87-566c-477b-898c-8191ed6adc05: 2021-11-\n28T12:04:48.212197+0800 - INFO - Request to access /ch04/\nlibrary/book/request/list \nLog: [3a496d87-566c-477b-898c-8191ed6adc05: 2021-11-\n28T12:04:48.221832+0800 - INFO - Successfully accessed /ch04/\nlibrary/book/request/list \nLog: [7d237742-fdac-4f4f-9604-ce49d3c4c3a7: 2021-11-\n28T12:04:48.239817+0800 -",
      "content_length": 2129,
      "extraction_method": "Direct"
    },
    {
      "page_number": 113,
      "chapter": null,
      "content": "Building the Microservice Application\n90\nLog: [c72f4287-f269-4b21-a96e-f8891e0a4a51: 2021-11-\n28T12:05:28.987578+0800 - INFO - Request to access /ch04/\nlibrary/book/add \nLog: [c72f4287-f269-4b21-a96e-f8891e0a4a51: 2021-11-\n28T12:05:28.996538+0800 - INFO - Successfully accessed /ch04/\nlibrary/book/add\nThe given snapshot of log messages proves that we have a centralized setup because the middleware \nfilters all API service execution and performs the logging transaction. It shows that the logging started \nfrom accessing the gateway down to executing the API services from the faculty, student, and library \nsub-applications. Centralizing and managing cross-cutting concerns is one advantage that can be \nprovided by using FastAPI’s mounting when building independent microservices.\nBut when it comes to the interactions among these independent sub-applications, can mounting \nalso be an advantage? Now, let us explore how independent microservices in our architecture can \ncommunicate by utilizing each other’s API resources.\nConsuming the REST API services\nJust like in an unmounted microservices setup, mounted ones can also communicate by accessing \neach other’s API services. For instance, if a faculty member or student wants to borrow a book from \nthe library, how can that setup be implemented seamlessly? \nIn Figure 4.4, we can see that interactions can be possible by establishing a client-server communication \nwherein one API service can serve as a resource provider, and the others are the clients:\nFigure 4.4 – Interaction with the faculty, student, and library microservices",
      "content_length": 1591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 114,
      "chapter": null,
      "content": "Using the httpx module\n91\nConsuming API resources in FastAPI can be straightforward using the httpx and requests \nexternal modules. The following discussions will focus on how these two modules can help our \nmounted services interact with each other.\nUsing the httpx module\nThe httpx external module is a Python extension that can consume both asynchronous and \nsynchronous REST APIs and has HTTP/1.1 and HTTP/2 support. It is a fast and multi-purpose toolkit \nthat is used to access API services running on WSGI-based platforms, as well as, on ASGI, like the \nFastAPI services. But first, we need to install it using pip:\npip install httpx\nThen, we can use it directly without further configuration to make two microservices interact, for \ninstance, our student module submitting assignments to the faculty module:\nimport httpx\n@router.get('/assignments/list')\nasync def list_assignments(): \n   async with httpx.AsyncClient() as client:\n    response = await client.get(\n     \"http://localhost:8000/ch04/faculty/assignments/list\")\n    return response.json()\n@router.post('/assignment/submit')\ndef submit_assignment(assignment:AssignmentRequest ):\n   with httpx.Client() as client:\n      response = client.post(\"http://localhost:8000/\n          ch04/faculty/assignments/student/submit\",  \n           data=json.dumps(jsonable_encoder(assignment)))\n      return response.content\nThe httpx module can process the GET, POST, PATCH, PUT, and DELETE path operations. It can \nallow the passing of different request parameters to the requested API without so much complexity. \nThe post() client operation, for instance, can accept headers, cookies, params, json, files, and model \ndata as parameter values. We use the with context manager to directly manage the streams created by \nits Client() or AsyncClient() instances, which are closeable components.",
      "content_length": 1845,
      "extraction_method": "Direct"
    },
    {
      "page_number": 115,
      "chapter": null,
      "content": "Building the Microservice Application\n92\nThe preceding list_assignments service is a client that uses the AsyncClient() instance \nto pursue its GET request from an asynchronous /ch04/faculty/assignments/list API \nendpoint from the faculty module. AsyncClient accesses the WSGI-based platform to execute \nany asynchronous services, not the synchronous ones, or else it will throw Status Code 500. It might \nrequire additional configuration details in its constructor for some complex cases, where it needs to \nfurther manage resource access through ASGI.\nOn the other hand, the submit_assignment service is a synchronous client that accesses \nanother synchronous endpoint, ch04/faculty/assignments/student/submit, which \nis a POST HTTP operation. In this case, the Client() instance is used to access the resource to \nsubmit an assignment to the Faculty module through a POST request. AssignmentRequest is \na BaseModel object that needs to be filled up by the client for submission to the request endpoint. \nUnlike params and json, which are passed straightforwardly as dict, data is a model object \nthat must be first converted into dict by jsonable_encoder() and json.dumps() to make \nthe transport feasible across the HTTP. The new converted model becomes the argument value of the \ndata parameter of the POST client operation.\nWhen it comes to the response of the client services, we can allow the response to be treated as a \npiece of text using the module’s content or as a JSON result using json(). It now depends on the \nrequirement of the client service as to what response type to use for the application. \nUsing the requests module\nAnother option to establish client-server communication among microservices is the requests \nmodule. Although httpx and requests are almost compatible, the latter offers other features \nsuch as auto-redirection and explicit session handling. The only problem with requests is its \nnon-direct support to asynchronous APIs and its slow performance when accessing resources. Despite \nits drawbacks, the requests module is still the standard way of consuming REST APIs in Python \nmicroservice development. First, we need to install it before we can use it:\npip install requests\nIn our ERP prototype, the requests extension was used by the faculty microservice to borrow books \nfrom the library module. Let’s look at the Faculty client services that show us how the requests \nmodule is used to access the synchronous API of library:\n@router.get('/books/request/list')\ndef list_all_request(): \n    with requests.Session() as sess:\n        response = sess.get('http://localhost:8000/\n           ch04/library/book/request/list')\n        return response.json()",
      "content_length": 2693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 116,
      "chapter": null,
      "content": "Using the requests module\n93\n@router.post('/books/request/borrow')\ndef request_borrow_book(request:BookRequestReq): \n    with requests.Session() as sess:\n        response = sess.post('http://localhost:8000/\n           ch04/library/book/request', \n             data=dumps(jsonable_encoder(request)))\n        return response.content\n@router.get('/books/issuance/list')\ndef list_all_issuance(): \n    with requests.Session() as sess:\n        response = sess.get('http://localhost:8000/\n            ch04/library/book/issuance/list')\n        return response.json()\n@router.post('/books/returning')\ndef return_book(returning: BookReturnReq): \n    with requests.Session() as sess:\n        response = sess.post('http://localhost:8000/\n            ch04/library/book/issuance/return', \n              data=dumps(jsonable_encoder(returning)))\n        return response.json()\nThe requests module has a Session() instance, which is equivalent to Client() in the httpx \nmodule. It provides all the necessary client operations that will consume the API endpoints from \nthe FastAPI platform. Since Session is a closeable object, the context manager is, again, used here \nto handle the streams that will be utilized during the access of the resources and transport of some \nparameter values. Like in httpx, parameter details such as params, json, header, cookies, \nfiles, and data are also part of the requests module and are ready for transport through the \nclient operation if needed by the API endpoints.\nFrom the preceding code, we can see that sessions are created to implement the list_all_request \nand list_all_issuance GET client services. Here, request_borrow_book is a POST client \nservice that requests a book in the form of BookRequestReq from the /ch04/library/book/\nrequest API endpoint. Similar to httpx, jsonable_encoder() and json.dumps() must \nbe used to convert the BaseModel object into dict in order to be transported as a data parameter \nvalue. The same approach is also applied to the return_book POST client service, which returns \nthe book borrowed by the faculty. The responses of these client services can also be content or \njson() just like what we have in the httpx extension.",
      "content_length": 2186,
      "extraction_method": "Direct"
    },
    {
      "page_number": 117,
      "chapter": null,
      "content": "Building the Microservice Application\n94\nUsing the requests and httpx modules allows these mounted microservices to interact with \neach other based on some specification. Consuming exposed endpoints from other microservices \nminimizes tight coupling and strengthens the importance of the decomposition design pattern of \nbuilding independent microservices. \nThe next technique gives you the option of managing components within a microservice using \ndomain modeling.\nApplying the domain modeling approach\nApplications that are database-focused or built from core functionalities without collaborating with \nmodels are either not easy to manage when they scale up or not friendly when enhanced or bug-fixed. \nThe reason behind this is the absence of the structure and flow of business logic to follow, study, \nand analyze. Understanding the behavior of an application and deriving the domain models with \nthe business logic behind them encompasses the best approach when it comes to establishing and \norganizing a structure in an application. This principle is called the domain modeling approach, which \nwe will now apply to our ERP specimen.\nCreating the layers\nLayering is one implementation that is inevitable when applying domain-driven development. There \nis a dependency between layers that, sometimes, might pose problems when fixing bugs during \ndevelopment. But what are important in layered architectures are the concepts, structures, categories, \nfunctionalities, and roles that layering can create, which helps in understanding the specification of the \napplication. Figure 4.5 shows the models, repositories, services, and controllers of the sub-applications: \n \nFigure 4.5 – Layered architecture\nThe most crucial layer is the models layer, which consists of the domain model classes that describe \nthe domain and business processes involved in the application.",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 118,
      "chapter": null,
      "content": "Identifying the domain models\n95\nIdentifying the domain models\nThe domain model layer is the initial artifact of the application because it provides the contextual \nframework of the application. Business processes and transactions can be easily classified and managed if \ndomains are first determined during the initial phase of the development. The code organization created \nby domain layering can provide code traceability, which can ease source code updates and debugging.\nIn our ERP specimen, these models are categorized into two: the data and request models. The data \nmodels are those used to capture and store data in its temporary data stores, while the request models \nare the BaseModel objects used in the API services.\nFor instance, the faculty module has the following data models:\nclass Assignment: \n    def __init__(self, assgn_id:int, title:str, \n        date_due:datetime, course:str):\n        self.assgn_id:int = assgn_id \n        self.title:str = title \n        self.date_completed:datetime = None\n        self.date_due:datetime = date_due\n        self.rating:float = 0.0 \n        self.course:str = course\n        \n    def __repr__(self): \n      return ' '.join([str(self.assgn_id), self.title,\n        self.date_completed.strftime(\"%m/%d/%Y, %H:%M:%S\"),\n        self.date_due.strftime(\"%m/%d/%Y, %H:%M:%S\"), \n        str(self.rating) ])\n    def __expr__(self): \n      return ' '.join([str(self.assgn_id), self.title, \n       self.date_completed.strftime(\"%m/%d/%Y, %H:%M:%S\"), \n       self.date_due.strftime(\"%m/%d/%Y, %H:%M:%S\"), \n        str(self.rating) ])\nclass StudentBin: \n    def __init__(self, bin_id:int, stud_id:int, \n      faculty_id:int):",
      "content_length": 1671,
      "extraction_method": "Direct"
    },
    {
      "page_number": 119,
      "chapter": null,
      "content": "Building the Microservice Application\n96\n        self.bin_id:int = bin_id \n        self.stud_id:int = stud_id \n        self.faculty_id:int = faculty_id \n        self.assignment:List[Assignment] = list()\n        \n    \n    def __repr__(self): \n        return ' '.join([str(self.bin_id), \n         str(self.stud_id), str(self.faculty_id)])\n    def __expr__(self): \n        return ' '.join([str(self.bin_id), \n         str(self.stud_id), str(self.faculty_id)])\nThese data model classes always have their constructors implemented if constructor injection is needed \nduring instantiation. Moreover, the __repr__() and __str__() dunder methods are optionally \nthere to provide efficiency for developers when accessing, reading, and logging these objects.\nOn the other hand, the request models are familiar because they were already discussed in the previous \nchapter. Additionally, the faculty module has the following request models:\nclass SignupReq(BaseModel):     \n    faculty_id:int\n    username:str\n    password:str\nclass FacultyReq(BaseModel): \n    faculty_id:int\n    fname:str\n    lname:str\n    mname:str\n    age:int\n    major:Major\n    department:str\nclass FacultyDetails(BaseModel): \n    fname:Optional[str] = None\n    lname:Optional[str] = None",
      "content_length": 1247,
      "extraction_method": "Direct"
    },
    {
      "page_number": 120,
      "chapter": null,
      "content": "Building the repository and service layers\n97\n    mname:Optional[str] = None\n    age:Optional[int] = None\n    major:Optional[Major] = None\n    department:Optional[str] = None\nThe request models listed in the preceding snippet are just simple BaseModel types. For further \ndetails on how to create BaseModel classes, Chapter 1, Setting Up FastAPI for Starters, provides \nguidelines for creating different kinds of BaseModel classes to capture different requests from clients.\nBuilding the repository and service layers\nThe two most popular domain modeling patterns that are crucial in building the layers of this approach \nare the repository and service layer patterns. The repository aims to create strategies for managing data \naccess. Some repository layers only provide data connectivity to the data store like in our specimen here, \nbut oftentimes, repository's goal is to interact with the Object Relational Model (ORM) framework \nto optimize and manage data transactions. But aside from the access, this layer provides a high-level \nabstraction for the application so that the specific database technology or dialect used will not matter \nto the applications. It serves as an adapter to any database platform to pursue data transactions for \nthe application, nothing else. The following is a repository class of the faculty module, which manages \nthe domain for creating assignments for their students:\nfrom fastapi.encoders import jsonable_encoder\nfrom typing import List, Dict, Any\nfrom faculty_mgt.models.data.facultydb import \n     faculty_assignments_tbl\nfrom faculty_mgt.models.data.faculty import Assignment\nfrom collections import namedtuple\nclass AssignmentRepository: \n      \n    def insert_assignment(self, \n           assignment:Assignment) -> bool: \n        try:\n            faculty_assignments_tbl[assignment.assgn_id] = \n                assignment\n        except: \n            return False \n        return True",
      "content_length": 1931,
      "extraction_method": "Direct"
    },
    {
      "page_number": 121,
      "chapter": null,
      "content": "Building the Microservice Application\n98\n    \n    def update_assignment(self, assgn_id:int, \n           details:Dict[str, Any]) -> bool: \n       try:\n           assignment = faculty_assignments_tbl[assgn_id]\n           assignment_enc = jsonable_encoder(assignment)\n           assignment_dict = dict(assignment_enc)\n           assignment_dict.update(details)         \n           faculty_assignments_tbl[assgn_id] =   \t\n \t\n \t\n           Assignment(**assignment_dict)\n       except: \n           return False \n       return True\n   \n    def delete_assignment(self, assgn_id:int) -> bool: \n        try:\n            del faculty_assignments_tbl[assgn_id] \n        except: \n            return False \n        return True\n    \n    def get_all_assignment(self):\n        return faculty_assignments_tbl \nHere, AssignmentRepository manages the Assignment domain object using its four \nrepository transactions. Additionally, insert_assignment() creates a new Assignment \nentry in the faculty_assignment_tbl dictionary, and update_assignment() accepts \nnew details or the corrected information of an existing assignment and updates it. On the other hand, \ndelete_assignment() deletes an existing Assignment entry from the data store using its \nassign_id parameter. To retrieve all the created assignments, the repository class has get_all_\nassignment(), which returns all the entries of faculty_assignments_tbl.",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 122,
      "chapter": null,
      "content": "Building the repository and service layers\n99\nThe service layer pattern defines the algorithms, operations, and process flows of the applications. \nOftentimes, it interacts with the repository to build the necessary business logic, management, and \ncontrols for the other components of the application, such as the API services or controllers. Usually, \none service caters to one repository class or more depending on the specification of the project. The \nfollowing code snippet is a service that interfaces a repository to provide additional tasks such as \nUUID generation for a student workbin:\nfrom typing import List, Dict , Any\nfrom faculty_mgt.repository.assignments import  \n            AssignmentSubmissionRepository\nfrom faculty_mgt.models.data.faculty import Assignment\nfrom uuid import uuid4\nclass AssignmentSubmissionService: \n    \n    def __init__(self): \n        self.repo:AssignmentSubmissionRepository = \n            AssignmentSubmissionRepository()\n        \n    def create_workbin(self, stud_id:int, faculty_id:int): \n        bin_id = uuid4().int\n        result = self.repo.create_bin(stud_id, bin_id, \n                     faculty_id )\n        return (result, bin_id)\n    \n    def add_assigment(self, bin_id:int, \n                   assignment: Assignment): \n        result = self.repo.insert_submission(bin_id, \n                     assignment ) \n        return result\n    \n    def remove_assignment(self, bin_id:int, \n                   assignment: Assignment): \n        result = self.repo.insert_submission(bin_id, \n                      assignment )\n        return result",
      "content_length": 1594,
      "extraction_method": "Direct"
    },
    {
      "page_number": 123,
      "chapter": null,
      "content": "Building the Microservice Application\n100\n    def list_assignments(self, bin_id:int): \n        return self.repo.get_submissions(bin_id)\nThe AssignmentSubmissionService cited in the preceding code has methods that utilize the \nAssignmentSubmissionRepository transactions. It provides them with parameters and returns \nthe bool results for evaluation by other components. Other services might look more complicated than \nthis sample because algorithms and tasks are usually added to pursue the requirements of the layers. \nThe successful wiring of a repository class to the service happens in the latter’s constructor. Usually, \nthe repository class is instantiated just like in the preceding sample. Another fantastic option is to \nuse DI, as discussed in Chapter 3.\nUsing the factory method pattern\nThe factory method design pattern is always a good approach for managing injectable classes and functions \nusing the Depends() component. Chapter 3 showcased factory methods as mediums to inject the \nrepository components into the service instead of instantiating them directly within the service. The \ndesign pattern provides loose coupling between components or layers. This approach is highly applicable \nto large applications wherein some modules and sub-components are reused and inherited. \nNow, let us look at how the top-level application can manage the different configuration details of \nthese mounted and independent microservice applications.\nManaging a microservice’s configuration details\nSo far, this chapter has provided us with some popular design patterns and strategies that can give us \na kickstart on how to provide our FastAPI microservices with the best structures and architecture. \nThis time, let us explore how the FastAPI framework supports storing, assigning, and reading \nconfiguration details to mounted microservice applications such as database credentials, networking \nconfiguration data, application server information, and deployment details. First, we need to install \npython-dotenv using pip:\npip install python-dotenv\nAll of these settings are values that are external to the implementation of the microservice applications. \nInstead of hardcoding them into the code as variable data, usually, we store them in the env, property, \nor INI files. However, challenges arise when assigning these settings to different microservices.",
      "content_length": 2365,
      "extraction_method": "Direct"
    },
    {
      "page_number": 124,
      "chapter": null,
      "content": "Managing a microservice’s configuration details\n101\nFrameworks that support the externalized configuration design pattern have an internal processing \nfeature that fetches environment variables or settings without requiring additional parsing or decoding \ntechniques. For instance, the FastAPI framework has built-in support for externalized settings through \npydantic’s BaseSettings class.\nStoring settings as class attributes\nIn our architecture setup, it should be the top-level application that will manage the externalized \nvalues. One way is to store them in a BaseSettings class as attributes. The following are classes \nof the BaseSettings type with their respective application details:\nfrom pydantic import BaseSettings\nfrom datetime import date\nclass FacultySettings(BaseSettings): \n    application:str = 'Faculty Management System' \n    webmaster:str = 'sjctrags@university.com'\n    created:date = '2021-11-10'\nclass LibrarySettings(BaseSettings): \n    application:str = 'Library Management System' \n    webmaster:str = 'sjctrags@university.com'\n    created:date = '2021-11-10' \nclass StudentSettings(BaseSettings): \n    application:str = 'Student Management System' \n    webmaster:str = 'sjctrags@university.com'\n    created:date = '2021-11-10'\nHere, FacultySettings will be assigned to the faculty module since it carries some \ninformation regarding the module. LibrarySettings is for the library module to utilize, while \nStudentSettings is for the student module. \nTo fetch the values, first, a component in a module must import the BaseSettings class from the \nmain project’s /configuration/config.py module. Then, it needs an injectable function to \ninstantiate it before injecting it into a component that needs to utilize the values. The following script \nis part of  /student_mgt/student_main.py, where the settings need to be retrieved: \nfrom configuration.config import StudentSettings",
      "content_length": 1908,
      "extraction_method": "Direct"
    },
    {
      "page_number": 125,
      "chapter": null,
      "content": "Building the Microservice Application\n102\nstudent_app = FastAPI()\nstudent_app.include_router(reservations.router)\nstudent_app.include_router(admin.router)\nstudent_app.include_router(assignments.router)\nstudent_app.include_router(books.router)\ndef build_config(): \n    return StudentSettings()\n@student_app.get('/index')\ndef index_student(\n   config:StudentSettings = Depends(build_config)): \n    return {\n        'project_name': config.application,\n        'webmaster': config.webmaster,\n        'created': config.created\n      }\nHere, build_config() is an injectable function that injects the StudentSettings instance \ninto the /index endpoint of the student microservice. After the DI, the application, webmaster, and \ncreated values will become accessible from the config wired object. These settings will appear on \nthe browser right after calling the /ch04/university/1 gateway URL.\nStoring settings in the properties file\nAnother option is to store all these settings inside a physical file with an extension of .env, \n.properties, or .ini. For instance, this project has the erp_settings.properties file \nfound in the /configuration folder, and it contains the following application server details in \nkey-value pair format:\nproduction_server = prodserver100\nprod_port = 9000\ndevelopment_server = devserver200\ndev_port = 10000",
      "content_length": 1333,
      "extraction_method": "Direct"
    },
    {
      "page_number": 126,
      "chapter": null,
      "content": "Managing a microservice’s configuration details\n103\nTo fetch these details, the application needs another BaseSettings class implementation that declares \nthe key of the key-value pair as attributes. The following class shows how production_server, \nprod_port, development_server, and dev_port are declared without any assigned values:\nimport os\nclass ServerSettings(BaseSettings): \n    production_server:str\n    prod_port:int\n    development_server:str \n    dev_port:int\n    \n    class Config: \n        env_file = os.getcwd() + \n           '/configuration/erp_settings.properties'\nAside from the class variable declaration, BaseSetting requires an implementation of an inner class, \ncalled Config, with a predefined env_file assigned to the current location of the properties file. \nThe same processes are involved when it comes to accessing the property details from the file. After \nimporting ServerSettings, it needs an injectable function to inject its instance to the components \nthat need the details. The following script is an updated version of /student_mgt/student_main.\npy, which includes access to the development_server and development_port settings:\nfrom fastapi import FastAPI, Depends\nfrom configuration.config import StudentSettings, \n      ServerSettings\nstudent_app = FastAPI()\nstudent_app.include_router(reservations.router)\nstudent_app.include_router(admin.router)\nstudent_app.include_router(assignments.router)\nstudent_app.include_router(books.router)\ndef build_config(): \n    return StudentSettings()\ndef fetch_config():\n    return ServerSettings()",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 127,
      "chapter": null,
      "content": "Building the Microservice Application\n104\n@student_app.get('/index')\ndef index_student(\n     config:StudentSettings = Depends(build_config), \n     fconfig:ServerSettings = Depends(fetch_config)): \n    return {\n        'project_name': config.application,\n        'webmaster': config.webmaster,\n        'created': config.created,\n        'development_server' : fconfig.development_server,\n        'dev_port': fconfig.dev_port\n      }\nBased on this enhanced script, running the /ch04/university/1 URL will redirect the browser \nto a screen showing additional server details from the properties file. Managing configuration \ndetails in FastAPI is easy, as we either save them inside a class or inside a file. No external module is \nnecessary, and no special coding effort is required to fetch all these settings, just the creation of the \nBaseSettings classes. This easy setup contributes to building flexible and adaptable microservice \napplications that can run on varying configuration details.\nSummary\nThe chapter started with the decomposition pattern, which is useful for breaking down a monolithic \napplication into granularized, independent, and scalable modules. The FastAPI application that \nimplemented these modules exhibited some principles included in the 12-Factor Application principles \nof a microservice, such as having independence, configuration files, logging systems, code bases, port \nbinding, concurrency, and easy deployment.\nAlongside decomposition, this chapter also showcased the mounting of different independent \nsub-applications onto the FastAPI platform. Only FastAPI can group independent microservices using \nmounts and bind them into one port with their corresponding context roots. From this feature, we \ncreated a pseudo-API Gateway pattern that serves as a façade to the independent sub-applications.\nDespite the possible drawbacks, the chapter also highlighted domain modeling as an option for \norganizing components in a FastAPI microservice. The domain, repository, and service layers help \nmanage the information flow and task distribution based on the project specification. Tracing, testing, \nand debugging are easy when domain layers are in place.\nIn the next chapter, we will focus on integrating our microservice applications with a relational database \nplatform. The focus is to establish database connectivity and utilize our data models to implement \nCRUD transactions within the repository layer.",
      "content_length": 2443,
      "extraction_method": "Direct"
    },
    {
      "page_number": 128,
      "chapter": null,
      "content": "Part 2: \nData-Centric and \nCommunication-Focused \nMicroservices Concerns \nand Issues\nIn this part of the book, we will be exploring other FastAPI components and features to solve other \ndesign patterns that the API framework can build, looking at data, communication, messaging, \nreliability, and security. External modules will also be highlighted in order to pursue other behavior \nand frameworks, such as ORM and reactive programming. \nThis part comprises the following chapters:\n•\t Chapter 5, Connecting to a Relational Database \n•\t Chapter 6, Using a Non-Relational Database \n•\t Chapter 7, Securing the REST APIs \n•\t Chapter 8, Creating Coroutines, Events, and Message-Driven Transactions",
      "content_length": 693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 129,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 130,
      "chapter": null,
      "content": "5\nConnecting to a \nRelational Database\nOur previous applications have only used Python collections to hold data records instead of persistent \ndata stores. This setup causes data wiping whenever the Uvicorn server restarts because these collections \nonly store the data in volatile memory, such as RAM. From this chapter onward, we will be applying \ndata persistency to avoid data loss and provide a platform to manage our records, even when the \nserver is in shutdown mode.\nThis chapter will focus on different Object Relational Mappers (ORMs) that can efficiently manage \nclients’ data using objects and a relational database. Object-relational mapping is a technique where \nSQL statements for Creating, Reading, Updating and Deleting (CRUD) are implemented and \nexecuted in an object-oriented programming approach. ORM requires all relationships or tables to \nbe mapped to their corresponding entity or model classes to avoid tightly coupled connections to \nthe database platform. And these model classes are the ones that are used to connect to the database.\nAside from introducing ORM, this chapter will also discuss a design pattern called Command and \nQuery Responsibility Segregation (CQRS), which can help resolve conflicts between read and write \nORM transactions at the domain level. CQRS can help minimize the running time spent by read and \nwrite SQL transactions to improve the overall performance of the application over time compared to \nthe data modeling approach.\nOverall, the main objective of this chapter is to prove that the FastAPI framework supports all \npopular ORMs to provide applications with backend database access, which it does by using popular \nrelational database management systems, and apply optimization to CRUD transactions using the \nCQRS design pattern.",
      "content_length": 1793,
      "extraction_method": "Direct"
    },
    {
      "page_number": 131,
      "chapter": null,
      "content": "Connecting to a Relational Database\n108\nIn this chapter, we will cover the following topics:\n•\t Preparing for database connectivity\n•\t Creating synchronous CRUD transactions using SQLAlchemy\n•\t Implementing asynchronous CRUD transactions using SQLAlchemy\n•\t Using GINO for asynchronous CRUD transactions\n•\t Using Pony ORM for the repository layer\n•\t Building the repository using Peewee\n•\t Applying the CQRS design pattern\nTechnical requirements\nThe application prototype that’s been created for this chapter is called fitness club management system; \nit caters to membership and gym fitness operations. This prototype has administration, membership, \nclass management, and attendance modules that utilize a PostgreSQL database as their data storage. \nMoreover, this uncommon application has four pieces of database connectivity that have been configured \nusing different ORM variations to provide you with options for your applications. Also, the prototype \nis just a simple FastAPI application that’s been created to help you focus on the data modeling features, \ndata persistency, and query building required for the discussions in this chapter. This code for this \nchapter can be found at https://github.com/PacktPublishing/Building-Python-\nMicroservices-with-FastAPI in the ch05a and ch05b projects.\nPreparing for database connectivity\nLet us consider some application-related concerns before we start discussing database connectivity \nin FastAPI:\n•\t First, all the application prototypes from this chapter onward will be using PostgreSQL as the \nsole relational DBMS. We can download its installer from https://www.enterprisedb.\ncom/downloads/postgres-postgresql-downloads. \n•\t Second, the fitness club management system prototype has an existing database called fcms \nwith six tables, namely signup, login, profile_members, profile_trainers, \nattendance_member, and gym_class. All these tables, along with their metadata and \nrelationships, can be seen in the following diagram:",
      "content_length": 1985,
      "extraction_method": "Direct"
    },
    {
      "page_number": 132,
      "chapter": null,
      "content": "Preparing for database connectivity\n109\nFigure 5.1 – The fcms tables\nThe project folder contains a script called fcms_postgres.sql that installs all these schemas.\nNow that we’ve installed the latest version of PostgreSQL and run the fcms script file, let us learn \nabout SQLAlchemy, the most widely used ORM library in the Python arena.\nImportant note\nThis chapter will compare and contrast the features of different Python ORMs. With this \nexperimental setup, each project will have a multitude of database connectivity, which is against \nthe convention of having a single piece of database connectivity per project.",
      "content_length": 618,
      "extraction_method": "Direct"
    },
    {
      "page_number": 133,
      "chapter": null,
      "content": "Connecting to a Relational Database\n110\nCreating CRUD transactions using SQLAlchemy\nSQLAlchemy is the most popular ORM library and can establish communication between any Python-\nbased application and database platform. It is reliable because it is continuously updated and tested \nto be efficient, high-performing, and accurate with its SQL reads and writes. \nThis ORM is a boilerplated interface that aims to create a database-agnostic data layer that can connect \nto any database engine. But compared to other ORMs, SQLAlchemy is DBA-friendly because it can \ngenerate optimized native SQL statements. When it comes to formulating its queries, it only requires \nPython functions and expressions to pursue the CRUD operations.\nBefore we start using SQLAlchemy, check whether you have the module installed in your system by \nusing the following command:\npip list \nIf SQLAlchemy is not in the list, install it using the pip command:\npip install SQLAlchemy\nCurrently, the version being used while developing the fitness club management system app is 1.4.\nInstalling the database driver\nSQLAlchemy will not work without the required database driver. It is mandatory to install the \npsycopg2 dialect since the database of choice is PostgreSQL:\npip install psycopg2\nPsycopg2 is a DB API 2.0-compliant PostgreSQL driver that does connection pooling and can work with \nmulti-threaded FastAPI applications. This wrapper or dialect is also essential in building synchronous \nCRUD transactions for our application. Once it’s been installed, we can start looking at SQLAlchemy’s \ndatabase configuration details. All the code related to SQLAlchemy can be found in the ch05a project.\nSetting up the database connection\nTo connect to any database, SQLAlchemy requires an engine that manages the connection pooling \nand the installed dialect. The create_engine() function from the sqlalchemy module is the \nsource of the engine object. But to successfully derive it, create_engine() requires a database \nURL string to be configured. This URL string contains the database name, the database API driver, \nthe account credentials, the IP address of the database server, and its port. The following script shows \nhow to create the engine that will be used in our fitness club management system prototype:\nfrom sqlalchemy import create_engine",
      "content_length": 2322,
      "extraction_method": "Direct"
    },
    {
      "page_number": 134,
      "chapter": null,
      "content": "Creating CRUD transactions using SQLAlchemy\n111\nDB_URL =   \n   \"postgresql://postgres:admin2255@localhost:5433/fcms\"\nengine = create_engine(DB_URL)\nengine is a global object and must be created only once in the entire application. Its first database \nconnection happens right after the first SQL transaction of the application because it follows the lazy \ninitialization design pattern.\nMoreover, engine in the previous script is essential for creating the ORM session that will be used \nby SQLAlchemy to execute CRUD transactions.\nInitializing the session factory\nAll CRUD transactions in SQLAlchemy are driven by sessions. Each session manages a group of \ndatabase \"writes\" and \"reads,\" and it checks whether to execute them or not. For instance, it maintains \na group of inserted, updated, and deleted objects, checks whether the changes are valid, and then \ncoordinates with the SQLAlchemy core to pursue the changes to the database if all transactions have \nbeen validated. It follows the behavior of the Unit of Work design pattern. SQLAlchemy relies on \nsessions for data consistency and integrity.\nBut before we create a session, the data layer needs a session factory that is bound to the derived engine. \nThe ORM has a sessionmaker()directive from the sqlalchemy.orm module, which requires \nthe engine object. The following script shows how to invoke sessionmaker():\nfrom sqlalchemy.orm import sessionmaker\nengine = create_engine(DB_URL)\nSessionFactory = sessionmaker(autocommit=False, \n                     autoflush=False, bind=engine)\nApart from engine binding, we also need to set the session’s autocommit property to False to \nimpose commit() and rollback() transactions. The application should be the one to flush all \nchanges to the database, so we need to set its autoflush feature to False as well. \nApplications can create more than one session through the SessionFactory() call, but having \none session per APIRouter is recommended.",
      "content_length": 1953,
      "extraction_method": "Direct"
    },
    {
      "page_number": 135,
      "chapter": null,
      "content": "Connecting to a Relational Database\n112\nDefining the Base class\nNext, we need to set up the Base class, which is crucial in mapping model classes to database tables. \nAlthough SQLAlchemy can create tables at runtime, we opted to utilize an existing schema definition \nfor our prototype. Now, this Base class must be subclassed by the model classes so that the mapping \nto the tables will happen once the server starts. The following script shows how straightforward it is \nto set up this component:\nfrom sqlalchemy.ext.declarative import declarative_base\nBase = declarative_base()\nInvoking the declarative_base() function is the easiest way of creating the Base class rather than \ncreating registry() to call generate_base(), which can also provide us with the Base class.\nNote that all these configurations are part of the /db_config/sqlalchemy_connect.py \nmodule of the prototype. They are bundled into one module since they are crucial in building the \nSQLAlchemy repository. But before we implement the CRUD transactions, we need to create the \nmodel layer using the Base class.\nBuilding the model layer\nThe model classes of SQLAlchemy have all been placed in the /models/data/sqlalchemy_\nmodels.py file of the fitness club project folder. If BaseModel is important to API request models, \nthe Base class is essential in building the data layer. It is imported from the configuration file to define \nSQLAlchemy entities or models. The following code is from the module script, which shows how we \ncan create model class definitions in SQLAlchemy ORM:\nfrom sqlalchemy import Time, Boolean, Column, Integer, \n    String, Float, Date, ForeignKey\nfrom sqlalchemy.orm import relationship\nfrom db_config.sqlalchemy_connect import Base\nclass Signup(Base):\n    __tablename__ = \"signup\"\n    id = Column(Integer, primary_key=True, index=True)\n    username = Column('username', String, unique=False, \n                       index=False)\n    password = Column('password' ,String, unique=False, \n                       index=False)",
      "content_length": 2022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 136,
      "chapter": null,
      "content": "Creating CRUD transactions using SQLAlchemy\n113\nThe Signup class is a sample of a SQLAlchemy model because it inherits the Base class’s properties. \nIt is a mapped class because all its attributes are reflections of the column metadata of its physical table \nschema counterpart. The model has a primary_key property set to True because SQLAlchemy \nrecommends each table schema have at least one primary key. The rest of the Column objects are \nmapped to column metadata that’s non-primary but can be unique or indexed. Each model class \ninherits the __tablename__ property, which sets the name of the mapped table.\nMost importantly, we need to ensure that the data type of the class attribute matches the column type \nof its column counterpart in the table schema. The column attribute must have the same name as the \ncolumn counterpart. Otherwise, we need to specify the actual column name in the first argument of \nthe Column class, as shown in the username and password columns of Signup. But most of \nthe time, we must always make sure they are the same to avoid confusion.     \nMapping table relationships\nSQLAlchemy strongly supports different types of parent-child or associative table relationships. Model \nclasses involved in the relationship require the relationship() directive from the sqlalchemy.\norm module to be utilized to establish one-to-many or one-to-one relationships among model classes. \nThis directive creates a reference from the parent to the child class using some foreign key indicated \nin the table schema definition. \nA child model class uses the ForeignKey construct in its foreign key column object to link the \nmodel class to its parent’s reference key column object. This directive indicates that the values in this \ncolumn should be within the values stored in the parent table’s reference column. The ForeignKey \ndirective applies to both the primary and non-primary Column objects. The following model class \ndefines a sample column relationship in our database schema:\nclass Login(Base): \n    __tablename__ = \"login\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    username = Column(String, unique=False, index=False)\n    password = Column(String, unique=False, index=False)\n    date_approved = Column(Date, unique=False, index=False)\n    user_type = Column(Integer, unique=False, index=False)\n    \n    trainers = relationship('Profile_Trainers', \n         back_populates=\"login\", uselist=False)\n    members = relationship('Profile_Members', \n         back_populates=\"login\", uselist=False)",
      "content_length": 2544,
      "extraction_method": "Direct"
    },
    {
      "page_number": 137,
      "chapter": null,
      "content": "Connecting to a Relational Database\n114\nThis Login model is linked to two children, Profile_Trainers and Profile_Members, \nbased on its configuration. Both child models have the ForeignKey directive in their id column \nobjects, as shown in the following model definitions:\nclass Profile_Trainers(Base):\n    __tablename__ = \"profile_trainers\"\n    id = Column(Integer, ForeignKey('login.id'), \n         primary_key=True, index=True, )\n    firstname = Column(String, unique=False, index=False)\n    … … … … …\n    … … … … …\n    login = relationship('Login', \n         back_populates=\"trainers\")\n    gclass = relationship('Gym_Class', \n         back_populates=\"trainers\")\n    \nclass Profile_Members(Base): \n    __tablename__ = \"profile_members\"\n    id = Column(Integer, ForeignKey('login.id'), \n         primary_key=True, index=True)\n    firstname = Column(String, unique=False, index=False)\n    lastname = Column(String, unique=False, index=False)\n    age = Column(Integer, unique=False, index=False)\n    … … … … … …\n    … … … … … …\n    trainer_id = Column(Integer, \n        ForeignKey('profile_trainers.id'), unique=False, \n        index=False)\n    login = relationship('Login', back_populates=\"members\")\n    attendance = relationship('Attendance_Member', \n          back_populates=\"members\")\n    gclass = relationship('Gym_Class', \n          back_populates=\"members\")",
      "content_length": 1364,
      "extraction_method": "Direct"
    },
    {
      "page_number": 138,
      "chapter": null,
      "content": "Creating CRUD transactions using SQLAlchemy\n115\nThe relationship() directive is the sole directive for creating table relationships. We need \nto specify some of its parameters, such as the name of the child model class and the backreference \nspecification. The back_populates parameter refers to the complementary attribute names of \nthe related model classes. This indicates the rows that need to be fetched using some relationship \nloading technique during join query transactions. The backref parameter can also be used instead \nof back_populates.\nOn the other hand, relationship() can return either a List or scalar object, depending on the \nrelationship type. If it is a one-to-one type, the parent class should set the useList parameter to False \nto indicate that it will return a scalar value. Otherwise, it will select a list of records from the child table. \nThe previous Login class definition shows that Profile_Trainers and Profile_Members \nhold a one-to-one relationship with Login because Login sets its uselist to False. On the \nother hand, the model relationship between Profile_Members and Attendance_Member is a \none-to-many type because uselist is set to True by default, as shown by the following definitions:\nclass Attendance_Member(Base):\n    __tablename__ = \"attendance_member\"\n    id = Column(Integer, primary_key=True, index=True)\n    member_id = Column(Integer, \n        ForeignKey('profile_members.id'), unique=False, \n        index=False)\n    timeout = Column(Time, unique=False, index=False)\n    timein = Column(Time, unique=False, index=False)\n    date_log = Column(Date, unique=False, index=False)\n    \n    members = relationship('Profile_Members', \n             back_populates=\"attendance\")\nWhile setting the model relationships, we must also consider the relationship loading type that these \nrelated model classes will be using during the join query transactions. We specify this detail in the \nlazy parameter of relationship(), which is assigned to select by default. This is because \nSQLAlchemy uses a lazy loading technique by default in retrieving join queries. However, you can \nmodify it to use joined (lazy=\"joined\"), subquery (lazy=\"subquery\"), select in \n(lazy=\"selectin\"), raise (lazy=\"raise\"), or no (lazy=\"no\") loading. Among the \noptions, the joined approach is better for INNER JOIN transactions.",
      "content_length": 2344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 139,
      "chapter": null,
      "content": "Connecting to a Relational Database\n116\nImplementing the repository layer\nIn the SQLAlchemy ORM, creating the repository layer requires the model classes and a Session \nobject. The Session object, derived from the SessionFactory()directive, establishes all \nthe communication to the database and manages all the model objects before the commit() or \nrollback() transaction. When it comes to the queries, the Session entity stores the result set \nof records in a data structure called an identity map, which maintains the unique identity of each data \nrecord using the primary keys. \nAll repository transactions are stateless, which means the session is automatically closed after loading \nthe model objects for insert, update, and delete transactions when the database issues a commit() \nor rollback() operation. We import the Session class from the sqlalchemy.orm module.\nBuilding the CRUD transactions\nNow, we can start building the repository layer of the fitness club application since we have already \nsatisfied the requirements to build the CRUD transactions. The following SignupRepository \nclass is the blueprint that will show us how to insert, update, delete, and retrieve record(s) to/from \nthe signup table:\nfrom typing import Dict, List, Any\nfrom sqlalchemy.orm import Session\nfrom models.data.sqlalchemy_models import Signup\nfrom sqlalchemy import desc\nclass SignupRepository: \n    \n    def __init__(self, sess:Session):\n        self.sess:Session = sess\n    \n    def insert_signup(self, signup: Signup) -> bool: \n        try:\n            self.sess.add(signup)\n            self.sess.commit()\n        except: \n            return False \n        return True",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 140,
      "chapter": null,
      "content": "Creating CRUD transactions using SQLAlchemy\n117\nSo far, insert_signup() is the most accurate way of persisting records to the signup table \nusing SQLAlchemy. Session has an add() method, which we can invoke to add all record objects \nto the table, and a commit() transaction to finally flush all the new records into the database. The \nflush() method of Session is sometimes used instead of commit() to pursue the insertion and \nclose Session, but most developers often use the latter. Note that the signup table contains all the \ngym members and trainers who want to gain access to the system. Now, the next script implements \nupdate record transaction:\n    def update_signup(self, id:int, \n           details:Dict[str, Any]) -> bool: \n       try:\n             self.sess.query(Signup).\n                 filter(Signup.id == id).update(details)     \n             self.sess.commit() \n       except: \n           return False \n       return True\nThe update_signup() provides a short, straightforward, and robust solution to updating a \nrecord in SQLAlchemy. Another possible solution is to query the record through self.sess.\nquery(Signup).filter(Signup.id == id).first(), replace the attribute values of the \nretrieved object with the new values from the details dictionary, and then invoke commit(). This \nway is acceptable, but it takes three steps rather than calling the update() method after filter(), \nwhich only takes one. Next script is an implementation of a delete record transaction:\n    def delete_signup(self, id:int) -> bool: \n        try:\n           signup = self.sess.query(Signup).\n                  filter(Signup.id == id).delete()\n           self.sess.commit()\n        except: \n            return False \n        return True",
      "content_length": 1739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 141,
      "chapter": null,
      "content": "Connecting to a Relational Database\n118\nOn the other hand, delete_signup() follows the strategy of update_signup(), which uses \nfilter() first before delete() is called. Another way of implementing this is to retrieve the \nobject using sess.query() again and pass the retrieved object as an argument to the Session \nobject’s delete(obj), which is a different function. Always remember to invoke commit() to \nflush the changes. Now, the following script shows how to implement the query transactions:\n    def get_all_signup(self):\n        return self.sess.query(Signup).all() \n    def get_all_signup_where(self, username:str):\n        return self.sess.\n             query(Signup.username, Signup.password).\n             filter(Signup.username == username).all() \n    \n    def get_all_signup_sorted_desc(self):\n        return self.sess.\n            query(Signup.username,Signup.password).\n            order_by(desc(Signup.username)).all()\n    \n    def get_signup(self, id:int): \n        return self.sess.query(Signup).\n             filter(Signup.id == id).one_or_none()\nMoreover, SignupRepository also highlights multiple and single records being retrieved in \nmany forms. The Session object has a query() method, which requires model class(es) or model \ncolumn names as argument(s). The function argument performs the record retrieval with column \nprojection. For instance, the given get_all_signup() selects all signup records with all the \ncolumns projected in the result. If we want to include only username and password, we can write \nour query as sess.query(Signup.username, Signup.password), just like in the given \nget_all_signup_where(). This query() method also shows how to manage constraints \nusing the filter() method with the appropriate conditional expressions. Filtering always comes \nafter column projection. \nOn the other hand, the Session object has an order_by() method that takes column names as \nparameters. It is performed last in the series of query transactions, before the result is extracted. The \ngiven sample, get_all_signup_sorted_desc(), sorts all Signup objects in descending \norder by username.",
      "content_length": 2125,
      "extraction_method": "Direct"
    },
    {
      "page_number": 142,
      "chapter": null,
      "content": "Creating CRUD transactions using SQLAlchemy\n119\nThe last portion of the query() builder returns the result of the transactions, whether it is a list \nof records or a single record. The all() function ends the query statement that returns multiple \nrecords, while first(), scalar(), one(), or one_or_none() can be applied if the result is a \nsingle row. In get_signup(), one_or_none() is utilized to raise an exception when no record \nis returned. For SQLAlchemy’s query transactions, all these functions can close the Session object. \nThe repository classes for SQLAlchemy are in the ch05a folder’s /repository/sqlalchemy/\nsignup.py module script file.\nCreating the JOIN queries\nFor all the ORMs supported by FastAPI, only SQLAlchemy implements join queries pragmatically \nand functionally, just like how we implemented the previous CRUD transactions. We used almost all \nof the methods we need to create joins previously except for join().  \nLet us look at LoginMemberRepository, which shows how we can create a join query statement \nin SQLAlchemy with model classes in one-to-one relationships:\nclass LoginMemberRepository(): \n    def __init__(self, sess:Session):\n        self.sess:Session = sess\n    \n    def join_login_members(self):\n        return self.sess.\n           query(Login, Profile_Members).\n             filter(Login.id == Profile_Members.id).all()\njoin_login_members() shows the conventional way of creating JOIN queries. This solution \nrequires passing the parent and child classes as query parameters and overriding the ON condition \nthrough the filter() method. The parent model class must come first in the column projection \nbefore the child class in the query() builder to extract the preferred result.\nAnother way is to use the select_from() function instead of query() to distinguish the parent \nclass from the child. This approach is more appropriate for a one-to-one relationship.\nOn the other hand, MemberAttendanceRepository showcases the one-to-many relationship \nbetween the Profile_Members and Attendance_Member model classes:\nclass MemberAttendanceRepository(): \n    def __init__(self, sess:Session):\n        self.sess:Session = sess\n    \n    def join_member_attendance(self):",
      "content_length": 2209,
      "extraction_method": "Direct"
    },
    {
      "page_number": 143,
      "chapter": null,
      "content": "Connecting to a Relational Database\n120\n        return self.sess.\n           query(Profile_Members, Attendance_Member).\n           join(Attendance_Member).all()\n    def outer_join_member(self):\n         return self.sess.\n            query(Profile_Members, Attendance_Member).\n            outerjoin(Attendance_Member).all()\njoin_member_attendance() shows the use of the join() method in building the INNER \nJOIN queries between Profile_Members and Attendance_Member. filter() is not \nneeded anymore to build the ON condition because join() automatically detects and recognizes \nthe relationship() parameters and the ForeignKey constructs defined at the beginning. \nBut if there are other additional constraints, filter() can always be invoked, but only after the \njoin() method.\nThe outer_join_member() repository method implements an OUTER JOIN query from the \none-to-many relationship. The outerjoin() method will extract all Profile_Members records \nmapped to their corresponding Attendance_Member or return null if there are none. \nRunning the transactions\nNow, let us apply these repository transactions to the administration-related API services of our \napplication. Instead of using collections to store all the records, we will be utilizing the ORM’s transactions \nto manage the data using PostgreSQL. First, we need to import the essential components required by \nthe repository, such as SessionFactory, the repository class, and the Signup model class. APIs \nsuch as Session and other typing APIs can only be part of the implementation for type hints. \nThe following script shows a portion of the administrator’s API services highlighting the insertion \nand retrieval services for new access registration:\nfrom fastapi import APIRouter, Depends\nfrom fastapi.responses import JSONResponse\nfrom sqlalchemy.orm import Session\nfrom db_config.sqlalchemy_connect import SessionFactory\nfrom repository.sqlalchemy.signup import SignupRepository,\n   LoginMemberRepository, MemberAttendanceRepository\nfrom typing import List\nrouter = APIRouter()",
      "content_length": 2044,
      "extraction_method": "Direct"
    },
    {
      "page_number": 144,
      "chapter": null,
      "content": "Creating CRUD transactions using SQLAlchemy\n121\ndef sess_db():\n    db = SessionFactory()\n    try:\n        yield db\n    finally:\n        db.close()\nFirst, we need to create the Session instance through SessionFactory(), which we derived \nfrom sessionmaker(), since the repository layer is dependent on the session. In our application, a \nsess_db() custom generator is used to open and destroy the Session instance. It is injected into the \nAPI service methods to tell the Session instance to proceed with instantiating SignupRepository: \n@router.post(\"/signup/add\")\ndef add_signup(req: SignupReq, \n          sess:Session = Depends(sess_db)):\n    repo:SignupRepository = SignupRepository(sess)\n    signup = Signup(password= req.password, \n                 username=req.username,id=req.id)\n    result = repo.insert_signup(signup)\n    if result == True:\n        return signup\n    else: \n        return JSONResponse(content={'message':'create \n                  signup problem encountered'}, \n                status_code=500)\nOnce instantiated, the repository can provide record insertion through insert_signup(), which \ninserts the Signup record. Another of its methods is get_all_signup(), which retrieves all \nlogin accounts for approval:\n@router.get(\"/signup/list\", response_model=List[SignupReq])\ndef list_signup(sess:Session = Depends(sess_db)):\n    repo:SignupRepository = SignupRepository(sess)\n    result = repo.get_all_signup()\n    return result\n@router.get(\"/signup/list/{id}\", response_model=SignupReq)\ndef get_signup(id:int, sess:Session = Depends(create_db)):",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 145,
      "chapter": null,
      "content": "Connecting to a Relational Database\n122\n    repo:SignupRepository = SignupRepository(sess)\n    result = repo.get_signup(id)\n    return result\nBoth the get_signup() and list_signup() services have a request_model of the \nSignupReq type, which determines the expected output of the APIs. But as you may have noticed, \nget_signup() returns the Signup object, while list_signup() returns a list of Signup \nrecords. How is that possible? If request_model is used to capture the query result of the SQLAlchemy \nquery transactions, the BaseModel class or request model must include a nested Config class \nwith its orm_mode set to True. This built-in configuration enables type mapping and validation of \nBaseModel for the SQLAlchemy model types used by the repository, before all the record objects are \nfiltered and stored in the request models. More information about the response_model parameter \ncan be found in Chapter 1, Setting Up FastAPI for Starters.\nSignupReq, which is used by the query services of our application, is defined as follows:\nfrom pydantic import BaseModel\nclass SignupReq(BaseModel): \n    id : int \n    username: str \n    password: str \n        \n    class Config:\n        orm_mode = True\nThe script shows how orm_mode is enabled using the equals sign (=) rather than the typical colon \nsymbol (:), which means orm_mode is a configuration detail and not part of the class attribute.\nOverall, using SQLAlchemy for the repository layer is systematic and procedural. It is easy to map \nand synchronize the model classes with the schema definitions. Establishing relationships through \nthe model classes is handy and predictable. Although there are lots of APIs and directives involved, \nit is still the most widely supported library for domain modeling and repository construction. Its \ndocumentation (https://docs.sqlalchemy.org/en/14/) is complete and informative \nenough to guide developers regarding the different API classes and methods.\nAnother feature of SQLAlchemy that’s loved by many is its capability to generate table schemas at \nthe application level.",
      "content_length": 2078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 146,
      "chapter": null,
      "content": "Creating CRUD transactions using SQLAlchemy\n123\nCreating tables \nUsually, SQLAlchemy works with the table schemas that have already been generated by the database \nadministrator. In this project, the ORM setup started with designing the domain model classes before \nmapping them to the actual tables. But SQLAlchemy can auto-create table schemas at runtime for \nthe FastAPI platform, which may be helpful during the testing or prototyping stage of the project. \nThe sqlalchemy module has a Table() directive that can create a table object with the essential \ncolumn metadata using the Column() method, which we used in the mapping. The following is a \nsample script that shows how the ORM creates the signup table at the application level:\nfrom sqlalchemy import Table, Column, Integer, String, \n               MetaData\nfrom db_config.sqlalchemy_connect import engine\nmeta = MetaData()\nsignup = Table(\n   'signup', meta, \n   Column('id', Integer, primary_key = True, \n          nullable=False), \n   Column('username', String, unique = False, \n          nullable = False), \n   Column('password', String, unique = False, \n          nullable = False), \n)\nmeta.create_all(bind=engine)\nPart of the schema definition is MetaData(), a registry that contains the necessary methods for \ngenerating the tables. When all the schema definitions are signed off, the create_all() method \nof the MetaData() instance is executed with the engine to create the tables. This process may \nsound straightforward, but we seldom pursue this DDL feature of SQLAlchemy in projects at the \nproduction stage.\nNow, let us explore how SQLAlchemy can be used to create asynchronous CRUD transactions for \nasynchronous API services.",
      "content_length": 1701,
      "extraction_method": "Direct"
    },
    {
      "page_number": 147,
      "chapter": null,
      "content": "Connecting to a Relational Database\n124\nImplementing async CRUD transactions using \nSQLAlchemy\nFrom version 1.4, SQLAlchemy supports asynchronous I/O (AsyncIO) features, which enables support \nfor asynchronous connections, sessions, transactions, and database drivers. Most of the procedures \nfor creating the repository are the same as those for the synchronous setup. The only difference is \nthe non-direct access that the CRUD commands have with the asynchronous Session object. Our \nch05b project showcases the asynchronous side of SQLAlchemy. \nInstalling the asyncio-compliant database drivers\nBefore we begin setting up the database configuration, we need to install the following asyncio-\ncompliant drivers: aiopg and asyncpg. First, we need to install aiopg, a library that will assist \nwith any asynchronous access to PostgreSQL:\npip install aiopg\nNext, we must install asyncpg, which helps build PostgreSQL asynchronous transactions through \nPython’s AsyncIO framework:\npip install asyncpg\nThis driver is a non-database API-compliant driver because it runs on top of the AsyncIO environment \ninstead of the database API specification for synchronous database transactions.\nSetting up the database’s connection\nAfter installing the necessary drivers, we can derive the database engine through the application’s \ncreate_async_engine() method, which creates an asynchronous version of SQLAlchemy’s \nEngine, known as AsyncEngine. This method has parameters to set such as future, which can \nenable a variety of asynchronous features during CRUD transactions when set to True. Also, it has \nan echo parameter that can provide us with the generated SQL queries in the server log at runtime. \nBut the most essential is the database URL, which now reflects the asynchronous database access \nthrough calling the asyncpg protocol. The following is the complete script for the asynchronous \nconnection to the PostgreSQL database:\nfrom sqlalchemy.ext.asyncio import create_async_engine\nDB_URL = \n  \"postgresql+asyncpg://postgres:admin2255@\n       localhost:5433/fcms\"",
      "content_length": 2065,
      "extraction_method": "Direct"
    },
    {
      "page_number": 148,
      "chapter": null,
      "content": "Implementing async CRUD transactions using SQLAlchemy\n125\nengine = create_async_engine(DB_URL, future=True, \n               echo=True)\nThe additional \"+asyncpg\" detail in DB_URL indicates that psycopg2 will no longer be the core \ndatabase driver for PostgreSQL; instead, asyncpg will be used. This detail enables AsyncEngine \nto utilize asyncpg to establish the connection to the database. Omitting this detail will instruct \nthe engine to recognize the psycopg2 database API driver, which will cause problems during the \nCRUD transactions. \nCreating the session factory\nLike in the synchronous version, the sessionmaker() directive is utilized to create the session \nfactory with some new parameters set to enable AsyncSession. First, its expire_on_commit \nparameter is set to False to make that model instances and its attribute values accessible for the \nduration of the transaction, even after calling commit(). Unlike in the synchronous environment, \nall entity classes and their column objects are still accessible by other processes, even after transaction \ncommit. Then, its class_ parameter bears the class name AsyncSession, the entity that will \ntake control of the CRUD transactions. Of course, sessionmaker() still needs the engine for \nAsyncConnection and its underlying asynchronous context managers. \nThe following script shows how the session factory is derived using the sessionmaker() directive:\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import sessionmaker\nengine = create_async_engine(DB_URL, future=True, \n               echo=True)\nAsynSessionFactory = sessionmaker(engine, \n       expire_on_commit=False, class_=AsyncSession)\nThe full configuration for the asynchronous SQLAlchemy database connection can be found in the \n/db_config/sqlalchemy_async_connect.py module script file. Let us now create the \nmodel layer.\nCreating the Base class and the model layer\nCreating the Base class using declarative_base() and creating the model classes using Base \nis the same as what we did in the synchronous version. No additional parameters are needed to build \nthe data layer for the asynchronous repository transactions.",
      "content_length": 2163,
      "extraction_method": "Direct"
    },
    {
      "page_number": 149,
      "chapter": null,
      "content": "Connecting to a Relational Database\n126\nBuilding the repository layer\nImplementing asynchronous CRUD transactions is entirely different from implementing synchronous \nones. The ORM supports the use of the execute() method of the AsyncConnection API to run \nsome of the built-in ORM core methods, namely update(), delete(), and insert(). When it \ncomes to query transactions, the new select() directive from the sqlalchemy.future module \nis used instead of the core select() method. And since execute() is an async method, this \nrequires that all repository transactions are async too to apply the Async/Await design pattern. The \nfollowing AttendanceRepository uses the asynchronous type of SQLAlchemy:\nfrom typing import List, Dict, Any\nfrom sqlalchemy import update, delete, insert\nfrom sqlalchemy.future import select\nfrom sqlalchemy.orm import Session\nfrom models.data.sqlalchemy_async_models import \n            Attendance_Member\nclass AttendanceRepository: \n    \n    def __init__(self, sess:Session):\n        self.sess:Session = sess\n    \n    async def insert_attendance(self, attendance: \n           Attendance_Member) -> bool: \n        try:\n            sql = insert(Attendance_Member).\n                   values(id=attendance.id, \n                     member_id=attendance.member_id, \n                     timein=attendance.timein, \n                     timeout=attendance.timeout, \n                     date_log=attendance.date_log)\n            sql.execution_options(\n                   synchronize_session=\"fetch\")\n            await self.sess.execute(sql)        \n        except: \n            return False \n        return True",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 150,
      "chapter": null,
      "content": "Implementing async CRUD transactions using SQLAlchemy\n127\nThe given asynchronous insert_attendance() method in the preceding script shows the use \nof the insert() directive in creating an attendance log for a gym member. First, we need to pass \nthe model class name to insert() to let the session know what table to access for the transaction. \nAfterward, it emits the values() method to project all the column values for insertion. Lastly, \nwe need to call the execute() method to run the final insert() statement and automatically \ncommit the changes since we didn’t turn off the autocommit parameter of sessionmaker() \nduring the configuration. Do not forget to invoke await before running the asynchronous method \nbecause everything runs on top of the AsyncIO platform this time.    \nAlso, you have the option to add some additional execution details before running execute(). \nOne of these options is synchronize_session, which tells the session to always synchronize \nthe model attribute values and the updated values from the database using the fetch method.\nAlmost the same procedure is applied to the update_attendance() and delete_attendance() \nmethods. We can run them through execute() and nothing else:\n    async def update_attendance(self, id:int, \n           details:Dict[str, Any]) -> bool: \n       try:\n           sql = update(Attendance_Member).where(\n              Attendance_Member.id == id).values(**details)\n           sql.execution_options(\n              synchronize_session=\"fetch\")\n           await self.sess.execute(sql)\n           \n       except: \n           return False \n       return True\n   \n    async def delete_attendance(self, id:int) -> bool: \n        try:\n           sql = delete(Attendance_Member).where(\n                Attendance_Member.id == id)\n           sql.execution_options(\n                synchronize_session=\"fetch\")\n           await self.sess.execute(sql)\n        except: \n            return False \n        return True",
      "content_length": 1967,
      "extraction_method": "Direct"
    },
    {
      "page_number": 151,
      "chapter": null,
      "content": "Connecting to a Relational Database\n128\nWhen it comes to queries, the repository class contains get_all_attendance(), which retrieves \nall the attendance records, and get_attendance(), which retrieves the attendance log of a \nparticular member through its id. Constructing the select() method is a straightforward and \npragmatic task since it is similar to writing a native SELECT statement in SQL development. First, \nthe method needs to know what columns to project, and then it caters to some constraints if there are \nany. Then, it needs the execute() method to run the query asynchronously and extract the Query \nobject. The resulting Query object has a scalars() method, which we can call to retrieve the list \nof records. Do not forget to close the session by calling the all() method. \ncheck_attendance(), on the other hand, uses the scalar() method of the Query object to \nretrieve one record: a specific attendance. Aside from record retrieval, scalar() also closes the session:\n    async def get_all_attendance(self):\n        q = await self.sess.execute(\n               select(Attendance_Member))\n        return q.scalars().all()\n    \n    async def get_attendance(self, id:int): \n        q = await self.sess.execute(\n           select(Attendance_Member).\n             where(Attendance_Member.member_id == id))\n        return q.scalars().all()\n    async def check_attendance(self, id:int): \n        q = await self.sess.execute(\n          select(Attendance_Member).\n              where(Attendance_Member.id == id))\n        return q.scalar()        \nThe repository classes for the asynchronous SQLAlchemy can be found in the /repository/\nsqlalchemy/attendance.py module script file. Now, let us apply these asynchronous transactions \nto pursue some attendance monitoring services for our fitness gym application.\nImportant note\nThe ** operator in update_attendance() is a Python operator overload that converts \na dictionary into kwargs. Thus, the result of **details is a kwargs argument for the \nvalues() method of the select() directive.",
      "content_length": 2048,
      "extraction_method": "Direct"
    },
    {
      "page_number": 152,
      "chapter": null,
      "content": "Implementing async CRUD transactions using SQLAlchemy\n129\nRunning the CRUD transactions\nThere two big differences between AsyncIO-driven SQLAlchemy and the database API-compliant \noption when creating the Session instance: \n•\t First, AsyncSession, which was created by the AsyncSessionFactory() directive, \nneeds an asynchronous with context manager because of the connection’s AsyncEngine, \nwhich needs to be closed after every commit() transaction. Closing the session factory is \nnot part of the procedure in the synchronous ORM version. \n•\t Second, after its creation, AsyncSession will only start executing all the CRUD transactions \nwhen the service calls its begin() method. The main reason is that AsyncSession can \nbe closed and needs to be closed once the transaction has been executed. That is why another \nasynchronous context manager is used to manage AsyncSession. \nThe following code shows the APIRouter script, which implements the services for monitoring \ngym member attendance using the asynchronous AttendanceRepository:\nfrom fastapi import APIRouter\nfrom db_config.sqlalchemy_async_connect import \n          AsynSessionFactory\nfrom repository.sqlalchemy.attendance import \n         AttendanceRepository\nfrom models.requests.attendance import AttendanceMemberReq\nfrom models.data.sqlalchemy_async_models import \n         Attendance_Member\nrouter = APIRouter()\n@router.post(\"/attendance/add\")\nasync def add_attendance(req:AttendanceMemberReq ):\n    async with AsynSessionFactory() as sess:\n        async with sess.begin():\n            repo = AttendanceRepository(sess)\n            attendance = Attendance_Member(id=req.id,  \n                member_id=req.member_id, \n                timein=req.timein, timeout=req.timeout, \n                date_log=req.date_log)\n            return await repo.insert_attendance(attendance)",
      "content_length": 1840,
      "extraction_method": "Direct"
    },
    {
      "page_number": 153,
      "chapter": null,
      "content": "Connecting to a Relational Database\n130\n@router.patch(\"/attendance/update\")\nasync def update_attendance(id:int, \n                     req:AttendanceMemberReq ):\n    async with AsynSessionFactory() as sess:\n        async with sess.begin():\n            repo = AttendanceRepository(sess)\n            attendance_dict = req.dict(exclude_unset=True)\n            return await repo.update_attendance(id, \n                    attendance_dict)\n@router.delete(\"/attendance/delete/{id}\")\nasync def delete_attendance(id:int): \n     async with AsynSessionFactory() as sess:\n        async with sess.begin():\n            repo = AttendanceRepository(sess)\n            return await repo.delete_attencance(id)\n@router.get(\"/attendance/list\")\nasync def list_attendance():\n     async with AsynSessionFactory() as sess:\n        async with sess.begin():\n            repo = AttendanceRepository(sess)\n            return await repo.get_all_attendance()\nThe preceding script shows no direct parameter passing between the repository class and the \nAsyncSession instance. The session must comply with the two context managers before it becomes \na working one. This syntax is valid under SQLAlchemy 1.4, which may undergo some changes in the \nfuture with SQLAlchemy’s next releases. \nOther ORM platforms that have been created for asynchronous transactions are easier to use. One \nof these is GINO.\nUsing GINO for async transactions\nGINO, which stands for GINO Is Not ORM, is a lightweight asynchronous ORM that runs on top of \nan SQLAlchemy Core and AsyncIO environment. All its APIs are asynchronous-ready so that you can \nbuild contextual database connections and transactions. It has built-in JSONB support so that it can \nconvert its results into JSON objects. But there is one catch: GINO only supports PostgreSQL databases.",
      "content_length": 1801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 154,
      "chapter": null,
      "content": "Using GINO for async transactions\n131\nWhile creating the gym fitness project, the only available stable GINO version is 1.0.1, which requires \nSQLAlchemy 1.3. Therefore, installing GINO will automatically uninstall SQLAlchemy 1.4, thus adding \nthe GINO repository to the ch05a project to avoid any conflicts with the async version of SQLAlchemy. \nYou can use the following command to install the latest version of GINO:\npip install gino\nInstalling the database driver\nSince the only RDBMS it supports is PostgreSQL, you only need to install asyncpg using the \npip command.\nEstablishing the database connection\nNo other APIs are needed to open a connection to the database except for the Gino directive. We \nneed to instantiate the class to start building the domain layer. The Gino class can be imported from \nthe ORM’s gino module, as shown by the following script:\nfrom gino import Gino\ndb = Gino()\nIts instance is like a façade that controls all database transactions. It starts by establishing a database \nconnection once it’s been provided with the correct PostgreSQL administrator credentials. The full \nGINO database connectivity script can be found in the /db_config/gino_connect.py script \nfile. Let us now build the model layer.\nBuilding the model layer\nThe model class definition in GINO has similarities with SQLAlchemy when it comes to structuring, \ncolumn metadata, and even the existence of the __tablename__ property. The only difference is \nthe superclass type because GINO uses the Model class from the database reference instance’s db. \nThe following script shows how the Signup domain model is mapped to the signup table:\nfrom db_config.gino_connect import db\nclass Signup(db.Model):\n    __tablename__ = \"signup\"\n    id = db.Column(db.Integer, primary_key=True, \n               index=True)\n    username = db.Column('username',db.String,",
      "content_length": 1856,
      "extraction_method": "Direct"
    },
    {
      "page_number": 155,
      "chapter": null,
      "content": "Connecting to a Relational Database\n132\n               unique=False, index=False)\n    password = db.Column('password',db.String, \n               unique=False, index=False)\nLike in SQLAlchemy, the __tablename__ property is mandatory for all model classes to indicate \ntheir mapped table schema. When defining the column metadata, the db object has a Column \ndirective that can set properties such as the column type, primary key, unique, default, nullable, and \nindex. The column types also come from the db reference object, and these types are also the same \nfor SQLAlchemy – that is, String, Integer, Date, Time, Unicode, and Float. \nAnd just in case the name of the model attribute does not match the column name, the Column \ndirective has its first parameter register the name of the actual column and maps it to the model \nattributes. The username and password columns are example cases of mapping the class attributes \nto the table’s column names.\nMapping table relationships   \nAt the time of writing, GINO only supports the many-to-one relationship by default. The db reference \nobject has a ForeignKey directive, which establishes a foreign key relationship with the parent \nmodel. It just needs the actual reference key column and table name of the parent table to pursue \nthe mapping. Setting the ForeignKey property in the Column object of the child model class is \nenough configuration to perform a LEFT OUTER JOIN to retrieve all the child records of the parent \nmode class. GINO has no relationship() function to address more details regarding how to \nfetch the child records of the parent model class. However, it has built-in loaders to automatically \ndetermine the foreign key and perform a many-to-one join query afterward. A perfect setup for this \njoin query is the relationship configuration between the Profile_Trainers and Gym_Class \nmodel classes, as shown in the following script:\nclass Profile_Trainers(db.Model):\n    __tablename__ = \"profile_trainers\"\n    id = db.Column(db.Integer, db.ForeignKey('login.id'), \n              primary_key=True, index=True)\n    firstname = db.Column(db.String, unique=False, \n              index=False)\n    … … … … … …\n    shift = db.Column(db.Integer, unique=False, \n              index=False)\nclass Gym_Class(db.Model): \n    __tablename__ = \"gym_class\"\n    id = db.Column(db.Integer, primary_key=True,",
      "content_length": 2362,
      "extraction_method": "Direct"
    },
    {
      "page_number": 156,
      "chapter": null,
      "content": "Using GINO for async transactions\n133\n          index=True)\n    member_id = db.Column(db.Integer, \n       db.ForeignKey('profile_members.id'), unique=False, \n         index=False)\n    trainer_id = db.Column(db.Integer, \n      db.ForeignKey('profile_trainers.id'), unique=False,\n         index=False)\n    approved = db.Column(db.Integer, unique=False, \n       index=False)\nWe will have to make some changes if we need to build a query that will deal with a one-to-many or a \none-to-one relationship. For the LEFT OUTER JOIN query to work, the parent model class must have \na set collection defined to contain all the child records during join queries involving one-to-many \nrelationships. For a one-to-one relationship, the parent only needs to instantiate the child model:\nclass Login(db.Model): \n    __tablename__ = \"login\"\n    id = db.Column(db.Integer, primary_key=True, \n               index=True)\n    username = db.Column(db.String, unique=False, \n               index=False)\n    … … … … … …\n    def __init__(self, **kw):\n        super().__init__(**kw)\n        self._child = None\n    @property\n    def child(self):\n        return self._child\n    @child.setter\n    def child(self, child):\n        self._child = child\nclass Profile_Members(db.Model): \n    __tablename__ = \"profile_members\"\n    id = db.Column(db.Integer, db.ForeignKey('login.id'),",
      "content_length": 1350,
      "extraction_method": "Direct"
    },
    {
      "page_number": 157,
      "chapter": null,
      "content": "Connecting to a Relational Database\n134\n          primary_key=True, index=True)\n    … … … … … … \n    weight = db.Column(db.Float, unique=False, index=False)\n    trainer_id = db.Column(db.Integer, \n        db.ForeignKey('profile_trainers.id'), unique=False, \n            index=False)\n    \n    def __init__(self, **kw):\n        super().__init__(**kw)\n        self._children = set()\n    @property\n    def children(self):\n        return self._children\n    @children.setter\n    def children(self, child):\n        self._children.add(child)\nThis set collection or child object must be instantiated in the parent’s __init__() to be accessed by \nthe ORM’s loader through the children or child @property, respectively. Using @property is the \nonly way to manage joined records. \nNote that the existence of the loader APIs is proof that GINO does not support the automated \nrelationship that SQLAlchemy has. If we want to deviate from its core setup, Python programming is \nneeded to add some features not supported by the platform, such as the one-to-many setup between \nProfile_Members and Gym_Class, and between Login and Profile_Members/Profile_\nTrainers. In the previous script, notice the inclusion of a constructor and the custom children \nPython property in Profile_Members, as well as the custom child property in Login. This is \nbecause GINO only has a built-in parent property.\nYou can find the domain models of GINO in the /models/data/gino_models.py script.\nImportant note\n@property is a Python decorator that’s used to implement a getter/setter in a class. This \nhides an instance variable from the accessor and exposes its getter and setter property fields. \nUsing @property is one way to implement the encapsulation principle in Python.",
      "content_length": 1741,
      "extraction_method": "Direct"
    },
    {
      "page_number": 158,
      "chapter": null,
      "content": "Using GINO for async transactions\n135\nImplementing the CRUD transactions\nLet us consider the following TrainerRepository, which manages trainer profiles. Its insert_\ntrainer() method shows the conventional way of implementing insert transactions. GINO requires \nits model class to call create(), an inherited method from the db reference object. All the column \nvalues are passed to the create() method through named parameters or as a bundle using kwargs \nbefore the record object is persisted. But GINO allows another insert option that uses the instance of \nthe model derived by injecting column values into its constructor. The created instance has a method \ncalled create() that inserts the record object without requiring any parameters:\nfrom models.data.gino_models import Profile_Members, \n           Profile_Trainers, Gym_Class\nfrom datetime import date, time\nfrom typing import List, Dict, Any\nclass TrainerRepository: \n    \n    async def insert_trainer(self, \n             details:Dict[str, Any]) -> bool: \n        try:\n            await Profile_Trainers.create(**details)\n        except Exception as e: \n            print(e)\n            return False \n        return True\nupdate_trainer() highlights how GINO updates table records. Based on the script, updating \nthe table in the GINO way involves doing the following: \n•\t First, it requires the get() class method of the model class to retrieve the record object with \nthe id primary key.\n•\t Second, the extracted record has an instance method called update() that will automatically \nmodify the mapped row with the new data specified in its kwargs argument. The apply() \nmethod will commit the changes and close the transaction:\n    async def update_trainer(self, id:int, \n                 details:Dict[str, Any]) -> bool: \n       try:",
      "content_length": 1798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 159,
      "chapter": null,
      "content": "Connecting to a Relational Database\n136\n            trainer = await Profile_Trainers.get(id)\n            await trainer.update(**details).apply()       \n       except: \n           return False \n       return True\nAnother option is to use the SQLAlchemy ModelClass.update.values(ModelClass).\nwhere(expression) clause, which, when applied to update_trainer(), will give us this \nfinal statement: \nProfile_Trainers.update.values(**details).\n     where(Profile_Trainers.id == id).gino.status()\nIts delete_trainer() also follows the same approach as the GINO update transaction. This \ntransaction is a two-step process, and the last step requires calling the delete() instance method \nof the extracted record object:\n    async def delete_trainer(self, id:int) -> bool: \n        try:\n           trainer = await Profile_Trainers.get(id)\n           await trainer.delete()        \n        except: \n            return False \n        return True\nOn the other hand, TrainerRepository has two methods, get_member() and get_all_\nmember(), which show how GINO constructs query statements: \n•\t The former retrieves a specific record object using its primary key through the get() class \nmethod of the model class \n•\t The latter uses the gino extension of query to utilize the all() method, which retrieves \nthe records:\n    async def get_all_member(self):\n        return await Profile_Trainers.query.gino.all()\n    async def get_member(self, id:int): \n            return await Profile_Trainers.get(id)",
      "content_length": 1484,
      "extraction_method": "Direct"
    },
    {
      "page_number": 160,
      "chapter": null,
      "content": "Using GINO for async transactions\n137\nBut what translates database rows into model objects in a query’s execution is the built-in loader of \nGINO. If we expand further on the solution presented in get_all_member(), this will look like this:\nquery = db.select([Profile_Trainers])\nq = query.execution_options(\n         loader=ModelLoader(Profile_Trainers))\nusers = await q.gino.all()\nIn the GINO ORM, all queries utilize ModelLoader to load each database record into a model object:\nclass GymClassRepository:\n        \n    async def join_classes_trainer(self):\n        query = Gym_Class.join(Profile_Trainers).select()\n        result = await query.gino.load(Gym_Class.\n            distinct(Gym_Class.id).\n                load(parent=Profile_Trainers)).all()\n        return result \n    \n    async def join_member_classes(self):\n        query = Gym_Class.join(Profile_Members).select()\n        result = await query.gino.load(Profile_Members.\n           distinct(Profile_Members.id).\n              load(add_child=Gym_Class)).all()\n        return result\nIf the normal query requires ModelLoader, what is needed for the JOIN query transactions? \nGINO has no automated support for table relationships, and creating JOIN queries is impossible \nwithout ModelLoader. The join_classes_trainer() method implements a one-to-\nmany query for Profile_Trainers and Gym_Class. The distinct(Gym_Class.id).\nload(parent=Profile_Trainers) clause in the query creates a ModelLoader for GymClass, \nwhich will merge and load the Profile_Trainers parent record into its child Gym_Class. \njoin_member_classes() creates one-to-many joins, while distinct(Profile_Members.\nid).load(add_child=Gym_Class) creates a ModelLoader to build the set of Gym_Class \nrecords, as per the Profile_Members parent.",
      "content_length": 1767,
      "extraction_method": "Direct"
    },
    {
      "page_number": 161,
      "chapter": null,
      "content": "Connecting to a Relational Database\n138\nOn the other hand, the many-to-one relationship of Gym_Class and Profile_Members uses the \nload() function of Profile_Member, which is a different approach to matching the Gym_Class \nchild records to Profile_Members. The following joined query is the opposite of the one-to-many \nsetup because the Gym_Class records here are on the left-hand side while the profiles are on the right:\n    async def join_classes_member(self):\n        result = await \n          Profile_Members.load(add_child=Gym_Class)\n           .query.gino.all()\nSo, the loader plays an important role in building queries in GINO, especially joins. Although it makes \nquery building difficult, it still gives flexibility to many complex queries. \nAll the repository classes for GINO can be found in the /repository/gino/trainers.py script.\nRunning the CRUD transactions\nFor our repositories to run in the APIRouter module, we need to open the database connection by \nbinding the db reference object to the actual database through DB_URL. It is ideal to use a dependable \nfunction for the binding procedure because the easier form of rolling out is done through APIRouter \ninjection. The following script shows how to set up this database binding:\nfrom fastapi import APIRouter, Depends\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import JSONResponse\nfrom db_config.gino_connect import db\nfrom models.requests.trainers import ProfileTrainersReq\nfrom repository.gino.trainers import TrainerRepository\nasync def sess_db():\n    await db.set_bind(\n     \"postgresql+asyncpg://\n       postgres:admin2255@localhost:5433/fcms\")\n    \nrouter = APIRouter(dependencies=[Depends(sess_db)])\n@router.patch(\"/trainer/update\" )\nasync def update_trainer(id:int, req: ProfileTrainersReq): \n    mem_profile_dict = req.dict(exclude_unset=True)\n    repo = TrainerRepository()",
      "content_length": 1884,
      "extraction_method": "Direct"
    },
    {
      "page_number": 162,
      "chapter": null,
      "content": "Using GINO for async transactions\n139\n    result = await repo.update_trainer(id, \n           mem_profile_dict)\n    if result == True: \n        return req \n    else: \n        return JSONResponse(\n    content={'message':'update trainer profile problem \n         encountered'}, status_code=500)\n \n@router.get(\"/trainer/list\")\nasync def list_trainers(): \n    repo = TrainerRepository()\n    return await repo.get_all_member()\nThe list_trainers() and update_trainer() REST services shown in the preceding code \nare some services of our fitness club application that will successfully run TrainerRepository \nafter injecting sess_db() into APIRouter. GINO does not ask for many details when establishing \nthe connection to PostgreSQL except for DB_URL. Always specify the asyncpg dialect in the URL \nbecause it is the only driver that’s supported by GINO as a synchronous ORM. \nCreating the tables\nGINO and SQLAlchemy have the same approach to creating a table schema at the framework level. \nBoth require the MetaData and Column directives for building the Table definitions. Then, an \nasynchronous function is preferred to derive the engine using the create_engine() method with \nour DB_URL. Like in SQLAlchemy, this engine plays a crucial role in building the tables through \ncreate_all(), but this time, it uses GINO’s GinoSchemaVisitor instance. The following script \nshows the complete implementation of how tables are generated in GINO using the AsyncIO platform:\nfrom sqlalchemy import Table, Column, Integer, String, \n           MetaData, ForeignKey'\nimport gino\nfrom gino.schema import GinoSchemaVisitor\nmetadata = MetaData()\nsignup = Table(\n    'signup', metadata,",
      "content_length": 1667,
      "extraction_method": "Direct"
    },
    {
      "page_number": 163,
      "chapter": null,
      "content": "Connecting to a Relational Database\n140\n    Column('id', Integer, primary_key=True),\n    Column('username', String),\n    Column('password', String),\n)\n   … … … … …\nasync def db_create_tbl():\n    engine = await gino.create_engine(DB_URL)\n    await GinoSchemaVisitor(metadata).create_all(engine)\nAs stated in SQLAlchemy, performing DDL transactions such as schema auto-generation at the start \nis optional because it may cause FastAPI’s performance to degrade, and even some conflicts in the \nexisting database schema.\nNow, let us explore another ORM that requires custom Python coding: Pony ORM. \nUsing Pony ORM for the repository layer\nPony ORM relies on Python syntax for building the model classes and repository transactions. This \nORM only uses Python data types such as int, str, and float, as well as class types to implement \nmodel definitions. It applies Python lambda expressions to establish CRUD transactions, especially \nwhen mapping table relationships. Also, Pony heavily supports JSON conversion of record objects \nwhen reading records. On the other hand, Pony can cache the query objects, which provides faster \nperformance than the others. The code for Pony ORM can be found in the ch05a project.\nTo use Pony, we need to install it using pip. This is because it is a third-party platform:\npip install pony\nInstalling the database driver\nSince Pony is an ORM designed to build synchronous transactions, we will need the psycopg2 \nPostgreSQL driver. We can install it using the pip command:\npip install psycopg2",
      "content_length": 1526,
      "extraction_method": "Direct"
    },
    {
      "page_number": 164,
      "chapter": null,
      "content": "Using Pony ORM for the repository layer\n141\nCreating the database’s connectivity\nThe approach to establishing database connectivity in Pony is simple and declarative. It only needs \nthe Database directive from the pony.orm module to be instantiated to connect to the database \nusing the correct database credentials. The following script is used in the fitness club prototype:\nfrom pony.orm import  Database\ndb = Database(\"postgres\", host=\"localhost\", port=\"5433\", \n  user=\"postgres\", password=\"admin2255\", database=\"fcms\")\nAs you can see, the first parameter of the constructor is the database dialect, followed by kwargs, \nwhich contains all the details about the connection. The full configuration can be found in the \n/db_config/pony_connect.py script file. Now, let us create Pony's model classes.\nDefining the model classes\nThe created database object, db, is the only component needed to define a Pony entity, a term that refers \nto a model class. It has an Entity attribute, which is used to subclass each model class to provide \nthe _table_ attribute, which is responsible for the table-entity mapping. All entity instances are \nbound to db and mapped to the tables. The following script shows how the Signup class becomes \nan entity of the model layer:\nfrom pony.orm import  Database, PrimaryKey, Required, \n         Optional, Set\nfrom db_config.pony_connect import db\nfrom datetime import date, time\n    \nclass Signup(db.Entity):\n    _table_ = \"signup\"\n    id = PrimaryKey(int)\n    username = Required(str, unique=True, max_len=100, \n         nullable=False, column='username')\n    password = Required(str, unique=Fals, max_len=100, \n         nullable=False, column='password')",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 165,
      "chapter": null,
      "content": "Connecting to a Relational Database\n142\nThe pony.orm module contains Required, Optional, PrimaryKey, or Set directives, which \nare used to create column attributes. Since each entity must have a primary key, PrimaryKey is used \nto define the column attribute of the entity. If the class has no primary key, Pony ORM will implicitly \ngenerate an id primary for the entity with the following definition:\nid = PrimaryKey(int, auto=True)\nOn the other hand, the Set directive indicates relationships between entities. All these directives \nhave a mandatory attribute column type, which declares the column value type in Python syntax (for \nexample, int, str, float, date, or time) or any class type. Other column attributes include \nauto, max_len, index, unique, nullable, default, and column. Now, let us establish a \nrelationship between model classes:\nclass Login(db.Entity): \n    _table_ = \"login\"\n    id = PrimaryKey(int)\n    … … … … … …\n    date_approved = Required(date)\n    user_type = Required(int)\n    \n    trainers = Optional(\"Profile_Trainers\", reverse=\"id\")\n    members = Optional(\"Profile_Members\", reverse=\"id\")\nThe given Login class has two additional attributes, trainers and members, which serve as \nreference keys to the Profile_Trainers and Profile_Members models, respectively. In \nturn, these child entities have their respective class attributes pointing back at the Login model, \nestablishing a relationship. These column attributes and their reference-foreign keys relationship must \nmatch the physical database schema. The following code shows examples of Pony’s child model classes:\nclass Profile_Trainers(db.Entity):\n    _table_ = \"profile_trainers\"\n    id = PrimaryKey(\"Login\", reverse=\"trainers\")\n    firstname = Required(str)\n    … … … … … …\n    tenure = Required(float)\n    shift = Required(int)\n    \n    members = Set(\"Profile_Members\", \n           reverse=\"trainer_id\")\n    gclass = Set(\"Gym_Class\", reverse=\"trainer_id\")",
      "content_length": 1950,
      "extraction_method": "Direct"
    },
    {
      "page_number": 166,
      "chapter": null,
      "content": "Using Pony ORM for the repository layer\n143\nclass Profile_Members(db.Entity): \n    _table_ = \"profile_members\"\n    id = PrimaryKey(\"Login\", reverse=\"members\")\n    firstname = Required(str)\n    … … … … … …\n    trainer_id = Required(\"Profile_Trainers\", \n            reverse=\"members\")\n    … … … … … …\nDefining the relationship attributes depends on the relationship type between the two entities. \nAttributes should be defined as Optional(parent)-Required(child) or Optional(parent)-Optional(child) \nif the relationship type is one-to-one. For one-to-many, attributes should be defined as Set(parent)-\nRequired(child). Finally, for many-to-one, the attributes must be defined as Set(parent)-Set(child). \nLogin has a one-to-one relationship with Profile_Members, which explains the use of the \nOptional attribute to point to the id key of Profile_Members. The primary keys are always \nthe reference keys in this relationship for Pony. \nOn the other hand, the Profile_Trainers model has a one-to-many setup with Profile_\nMembers, which explains why the trainer_id attribute of the former uses the Required \ndirective to point to the Set attribute members of the latter. Sometimes, the framework requires \nbackreference through the directive’s reverse parameter. The preceding code also depicts the same \nscenario between the Profile_Members and Gym_Class models, where the gclass attribute \nof Profile_Members is declared as a Set collection that contains all the enrolled gym classes of \nthe member. The reference key can be a primary key or just a typical class attribute in this relationship. \nThe following snippet shows the blueprint of the Gym_Class model:\nclass Gym_Class(db.Entity): \n    _table_ = \"gym_class\"\n    id = PrimaryKey(int)\n    member_id = Required(\"Profile_Members\", \n         reverse=\"gclass\")\n    trainer_id = Required(\"Profile_Trainers\", \n         reverse=\"gclass\")\n    approved = Required(int)\ndb.generate_mapping()",
      "content_length": 1935,
      "extraction_method": "Direct"
    },
    {
      "page_number": 167,
      "chapter": null,
      "content": "Connecting to a Relational Database\n144\nUnlike in other ORMs, Pony needs generate_mapping() to be executed to realize all the entity \nmappings to the actual tables. The method is an instance method of the db instance that must appear \nin the last part of the module script, as shown in the previous snippet, where Gym_Class was the last \nPony model class to be defined. All the Pony model classes can be found in the /models/data/\npony_models.py script file.\nNote that we can create Pony entities manually or digitally using Pony ORM ER Diagram Editor, which \nwe can access at https://editor.ponyorm.com/. The editor can provide us with both free \nand commercial accounts. Let us now implement the CRUD transactions.\nImplementing the CRUD transactions\nCRUD transactions in Pony are session-driven. But unlike SQLAlchemy, its repository classes do not \nrequire injecting db_session into the repository constructor. Each transaction in Pony will not \nwork without db_session. The following code shows a repository class that implements all the \ntransactions needed to manage a list of gym members:\nfrom pony.orm import db_session, left_join\nfrom models.data.pony_models import Profile_Members, \n            Gym_Class, Profile_Trainers\nfrom datetime import date, time\nfrom typing import List, Dict, Any\nfrom models.requests.members import ProfileMembersReq \nclass MemberRepository: \n    \n    def insert_member(self, \n            details:Dict[str, Any]) -> bool: \n        try:\n            with db_session:\n                Profile_Members(**details)\n        except: \n            return False \n        return True",
      "content_length": 1607,
      "extraction_method": "Direct"
    },
    {
      "page_number": 168,
      "chapter": null,
      "content": "Using Pony ORM for the repository layer\n145\nIn Pony, inserting a record means instantiating the model class with the injected record values. An \nexample is insert_member(), which inserts a profile by instantiating the Profile_Members \nmodel with the injected membership details. However, the case is different when updating records, \nas shown in the following script:      \n    def update_member(self, id:int, \n               details:Dict[str, Any]) -> bool: \n       try:\n          with db_session:\n            profile = Profile_Members[id]\n            profile.id = details[\"id\"]\n            … … … … … …\n            profile.trainer_id = details[\"trainer_id\"]\n       except: \n           return False \n       return True\nUpdating a record in Pony, which is implemented in the update_member() script, means retrieving \nthe record object through indexing using its id. The retrieved object is automatically converted into a \nJSON-able object since Pony has built-in support for JSON. Then, the new values of those attributes \nare overwritten as they must be changed. This UPDATE transaction is, again, within the bounds of \ndb_session, thus automatically refreshing the record(s) after the overwrites.\nOn the other hand, delete_member() of the repository class shows the same approach with \nUPDATE, except that a delete() class method is invoked right after retrieving the object record. \nThe following is the script for this operation:\n    def delete_member(self, id:int) -> bool: \n        try:\n           with db_session: \n               Profile_Members[id].delete()\n        except: \n            return False \n        return True\nThe delete transaction is also db_session bound, so invoking delete() automatically refreshes \nthe table. The following code shows Pony’s implementation for query transactions:\n    def get_all_member(self):\n        with db_session:",
      "content_length": 1859,
      "extraction_method": "Direct"
    },
    {
      "page_number": 169,
      "chapter": null,
      "content": "Connecting to a Relational Database\n146\n            members = Profile_Members.select()\n            result = [ProfileMembersReq.from_orm(m) \n                 for m in members]\n            return result\n    \n    def get_member(self, id:int): \n        with db_session:\n            login = Login.get(lambda l: l.id == id)\n            member = Profile_Members.get(\n                lambda m: m.id == login)\n            result = ProfileMembersReq.from_orm(member)\n        return result\nget_member() retrieves a single record using the get() class method, which requires a lambda \nexpression in its parameter. Since Login has a one-to-one relationship with Profile_Members, \nfirst, we must extract the Login record of the member and use the login object to fetch the record \nthrough the get() helper function of the Profile_Members entity. This approach is also applicable \nto other entities with other entity relationships. Now, get_all_member() retrieves a result set \nusing the select() method. The select() method can also utilize a lambda expression if there \nare constraints in the retrieval operation. \nPony model classes have the get() and select() methods, which both return Query objects that \nFastAPI cannot process directly. So, we need an ORM-friendly Pydantic model to extract the final \nentities from these Query objects. Like in SQLAlchemy, a ModelBase class with a nested Config \nclass is required to retrieve the records from the Query object. The nested class must configure \norm_mode to True. If relationship mappings are involved, the request model must also declare the \nattributes involved in the relationship and their corresponding child object converters. The method \nconverters, decorated by Pydantic’s @validator, are automatically called by Pony to interpret \nand validate the Query objects into JSON-able components such as List or entity objects. The \nfollowing code shows the request model that’s used to extract the records from select() through \nlist comprehension and the Profile_Member dict object from get():\nfrom typing import List, Any\nfrom pydantic import BaseModel, validator\nclass ProfileMembersReq(BaseModel): \n    id: Any\n    firstname: str\n    lastname: str\n    age: int",
      "content_length": 2207,
      "extraction_method": "Direct"
    },
    {
      "page_number": 170,
      "chapter": null,
      "content": "Using Pony ORM for the repository layer\n147\n    height: float\n    weight: float\n    membership_type: str\n    trainer_id: Any\n    \n    gclass: List\n    \n    @validator('gclass', pre=True, \n         allow_reuse=True, check_fields=False)\n    def gclass_set_to_list(cls, values):\n        return [v.to_dict() for v in values]\n    @validator('trainer_id', pre=True, \n         allow_reuse=True, check_fields=False)\n    def trainer_object_to_map(cls, values):\n        return values.to_dict()\n    \n    class Config:\n        orm_mode = True\nThe presence of the gclass_set_to_list () and trainer_object_to_map() converts \nin ProfileMembersReq enables data to be populated to the child objects in the gclass and \ntrainer_id attributes, respectively. These additional features indicate why executing select() \ncan already retrieve the INNER JOIN queries. \nTo build LEFT JOIN query transactions, the ORM has a built-in directive called left_join(), which \nis used to extract the Query object bearing the LEFT JOIN raw objects through a Python generator. \nThe following code shows another repository class that showcases the use of left_join():\nclass MemberGymClassRepository:\n    \n    def join_member_class(self): \n      with db_session: \n        generator_args = (m for m in Profile_Members \n              for g in m.gclass)\n        joins = left_join(tuple_args)        \n        result = [ProfileMembersReq.from_orm(m)",
      "content_length": 1405,
      "extraction_method": "Direct"
    },
    {
      "page_number": 171,
      "chapter": null,
      "content": "Connecting to a Relational Database\n148\n              for m in joins ]\n        return result\nAll the repository classes can be found in the /repository/pony/members.py script file.\nNow, what makes Pony faster is that it uses an identity map, which contains all the record objects that \nhave been retrieved from every single query transaction. The ORM applies the Identity Map design \npattern to apply its caching mechanism to make read and write executions fast. It only requires memory \nmanagement and monitoring to avoid memory leak problems in complex and huge applications. \nRunning the repository transactions\nSince db_session is already managed internally, no additional requirements will be needed from \nPony for the APIRouter script to run the repository transactions. The repository classes are directly \naccessed and instantiated in each of the APIs to access the CRUD transactions.\nCreating the tables\nIf the tables are non-existent yet, Pony can generate those tables through its entity classes. This DDL \ntransaction is enabled when the create_tables parameter of the generate_mapping() \nmethod of db is set to True. \nFor the most compact and simplest ORM in terms of syntax, we have Peewee.\nBuilding the repository using Peewee\nAmong the different ORMs, Peewee is the simplest and smallest in terms of ORM features and APIs. \nThe framework is easy to understand and use; it is not comprehensive, but it has intuitive ORM syntax. \nIts strength is in building and executing query transactions. \nPeewee is not designed for asynchronous platforms, but it can work with them by using some async-\nrelated libraries that it supports. We need to install at least Python 3.7 for Peewee to work with FastAPI, \nan asynchronous framework. To install Peewee, we need to execute the following command:\npip install peewee\nInstalling the database driver\nThe ORM needs psycopg2 as the PostgreSQL database driver. We can install it using pip:\npip install psycopg2",
      "content_length": 1959,
      "extraction_method": "Direct"
    },
    {
      "page_number": 172,
      "chapter": null,
      "content": "Building the repository using Peewee\n149\nCreating the database connection\nFor Peewee to work with FastAPI, we must build a multi-threading mechanism where Peewee \ncan cater to more than one request transaction on the same thread, and per request can do more \nexecutions simultaneously using different local threads. This customized multi-threading component, \nwhich can be created using the ContextVar class, bridges Peewee to the FastAPI platform. But \nfor Peewee to utilize these threads, we also need to customize its _ConnectionState with the \nnewly created threading state, db_state. The following code shows how db_state and a custom \n_ConnectionState can be derived:\nfrom peewee import _ConnectionState\nfrom contextvars import ContextVar\ndb_state_default = {\"closed\": None, \"conn\": None, \n         \"ctx\": None, \"transactions\": None}\ndb_state = ContextVar(\"db_state\", \n          default=db_state_default.copy())\nclass PeeweeConnectionState(_ConnectionState):\n    def __init__(self, **kwargs):\n        super().__setattr__(\"_state\", db_state)\n        super().__init__(**kwargs)\n    def __setattr__(self, name, value):\n        self._state.get()[name] = value\n    def __getattr__(self, name):\n        return self._state.get()[name]\nTo apply the new db_state and _ConnectionState classes, cited in the preceding code as \nPeeweeConnectionState, we need to open the database connection through the Database \nclass. Peewee has several variations of the Database class, depending on the type of database the \napplication will choose to connect to. Since we will be using PostgreSQL, PostgresqlDatabase is \nthe correct class to initialize with all the necessary database details. After establishing the connection, \nthe db instance will have a _state attribute that will point to the PeeweeConnectionState \ninstance. The following snippet shows how to connect to our fitness gym database’s fcms using the \ndatabase credentials:\nfrom peewee import PostgresqlDatabase",
      "content_length": 1961,
      "extraction_method": "Direct"
    },
    {
      "page_number": 173,
      "chapter": null,
      "content": "Connecting to a Relational Database\n150\ndb = PostgresqlDatabase(\n    'fcms',\n    user='postgres',\n    password='admin2255',\n    host='localhost',\n    port=5433, \n)\ndb._state = PeeweeConnectionState()\nThe preceding code also emphasizes that the default state of the database connection must be replaced \nwith a non-blocking one that can work with the FastAPI platform. This configuration can be found \nin the /db_config/peewee_connect.py script file. Let us now build Peewee's model layer.\nCreating the tables and the domain layer\nPeewee prefers auto-generating tables based on its model classes, unlike other ORMs. Peewee recommends \nreverse engineering, where tables are created rather than only being mapped to existing tables. Letting \nthe application generate the tables lessens the hassle of establishing relationships and primary keys. \nThis ORM is unique because it has an \"implied\" approach to creating primary keys and foreign keys. \nThe following script shows how Peewee model classes are defined:\nfrom peewee import Model, ForeignKeyField, CharField, \n   IntegerField, FloatField, DateField, TimeField\nfrom db_config.peewee_connect import db\nclass Signup(Model):\n    username = CharField(unique=False, index=False)\n    password = CharField(unique=False, index=False)\n    \n    class Meta:\n      database = db\n      db_table = 'signup'",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 174,
      "chapter": null,
      "content": "Building the repository using Peewee\n151\nWe can’t see any primary keys in the model classes presented because the Peewee engine will create them \nduring its schema auto-generation. The physical foreign key column and the model attribute will have \nthe same name derived from its model name, with the modelname_id pattern in lowercase form. If \nwe insist on adding the primary key for the model, a conflict will occur, making Peewee dysfunctional. \nWe must let Peewee create the physical tables from the model classes to avoid this mishap.\nAll model classes inherit properties from the Model directive of the ORM. It also has column directives \nsuch as IntegerField, FloatField, DateField, and TimeField for defining the column \nattributes of the model classes. Moreover, each domain class has a nested Meta class, which registers \nthe references to database and db_table, which is mapped to the model class. Other properties \nthat we can set here are primary_key, indexes, and constraints. \nThe only problem in having this auto-generation is when creating table relationships. Linking the \nforeign key attributes of the child classes to the non-existent primary keys of the parent classes is \ndifficult before auto-generation. For instance, the following Profile_Trainers model implies a \nmany-to-one relationship with the Login class, which is only defined by the ForeignKeyField \ndirective with the trainer backreference attribute and not by the login_id foreign key:\nclass Profile_Trainers(Model):\n    login = ForeignKeyField(Login, \n         backref=\"trainers\", unique=True)\n    … … … … … …\n    shift = IntegerField(unique=False, index=False)\n       \n    class Meta:\n      database = db\n      db_table = 'profile_trainers'\nThe login_id column that’s generated after auto-generation can be seen in the following screenshot:\nFigure 5.2 – The generated profile_trainers schema",
      "content_length": 1877,
      "extraction_method": "Direct"
    },
    {
      "page_number": 175,
      "chapter": null,
      "content": "Connecting to a Relational Database\n152\nForeign key attributes are declared using the ForeignKeyField directive, which accepts at least \nthree crucial parameters:\n•\t The parent model’s name \n•\t The backref parameter, which references the child record (if in a one-to-one relationship) \nor a set of child objects (if in a one-to-many or many-to-one relationship)\n•\t The unique parameter, which indicates a one-to-one relationship when set to True or \nFalse otherwise\nAfter defining all the models, including their relationships, we need to call the following methods \nfrom Peewee’s db instance for the table mapping to occur:\n•\t connect() to establish the connection \n•\t create_tables() to pursue the schema generation based on its list of model classes\nThe following script shows a snapshot of the class definitions, including the call to the two db methods:\nclass Login(Model): \n    username = CharField(unique=False, index=False)\n    … … … … … …\n    user_type = IntegerField(unique=False, index=False)\n    \n    class Meta:\n      database = db\n      db_table = 'login'\n    \nclass Gym_Class(Model): \n    member = ForeignKeyField(Profile_Members, \n          backref=\"members\")\n    trainer = ForeignKeyField(Profile_Trainers, \n          backref=\"trainers\")\n    approved = IntegerField(unique=False, index=False)\n       \n    class Meta:\n      database = db\n      db_table = 'gym_class'\ndb.connect()",
      "content_length": 1395,
      "extraction_method": "Direct"
    },
    {
      "page_number": 176,
      "chapter": null,
      "content": "Building the repository using Peewee\n153\ndb.create_tables([Signup, Login, Profile_Members, \n     Profile_Trainers, Attendance_Member, Gym_Class],\n           safe=True)\nAs we can see, we need to set the safe parameter of create_tables() to True so that Peewee \nwill only perform schema auto-generation once during the initial server startup of the application. All \nthe model classes for the Peewee ORM can be found in the /models/data/peewee_models.\npy script file. Now, let us implement the repository layer.\nImplementing the CRUD transactions\nCreating the asynchronous connection and building the model layer for the application in the Peewee \nORM is tricky, but implementing its repository layer is straightforward. All the method operations \nare entirely derived from its model classes. For instance, insert_login(), which is shown in \nthe following snippet, shows how the create() static method of Login takes the login details for \nrecord insertion: \nfrom typing import Dict, List, Any\nfrom models.data.peewee_models import Login, \n   Profile_Trainers, Gym_Class, Profile_Members\nfrom datetime import date\nclass LoginRepository:\n    \n    def insert_login(self, id:int, user:str, passwd:str, \n          approved:date, type:int) -> bool: \n        try:\n            Login.create(id=id, username=user, \n                password=passwd, date_approved=approved, \n                user_type=type)\n        except Exception as e: \n           return False \n        return True\nThis method can be re-implemented to perform bulk inserts, but Peewee has an alternative way to \npursue multiple insertions through its insert_many() class method. Using insert_many() \nrequires more accurate column details for mapping multiple schema values. It also needs an invocation \nof the execute() method to perform all the bulk insertions and close the operation afterward.",
      "content_length": 1852,
      "extraction_method": "Direct"
    },
    {
      "page_number": 177,
      "chapter": null,
      "content": "Connecting to a Relational Database\n154\nSimilarly, the update() class method requires the execute() method after filtering the record \nthat needs updating using the id primary key. This is shown in the following code snippet:\n    def update_login(self, id:int, \n              details:Dict[str, Any]) -> bool: \n       try:\n           query = Login.update(**details).\n                  where(Login.id == id)\n           query.execute()\n       except: \n           return False \n       return True\nWhen it comes to record deletion, delete_login() shows the easy approach – that is, by using \ndelete_by_id(). But the ORM has another way, which is to retrieve the record object using the \nget() class method – for example, Login.get(Login.id == id) – and eventually delete \nthe record through the delete_instance() instance method of the record object. The following \ndelete_login() transaction shows how to utilize the delete_by_id() class method:\n    def delete_login(self, id:int) -> bool: \n        try:\n           query = Login.delete_by_id(id)\n        except: \n            return False \n        return True\nThe following scripts, which are for get_all_login() and get_login(), highlight how Peewee \nretrieves records from the database. Peewee uses its get() class method to retrieve a single record \nusing the primary key; the same method was applied to its UPDATE transaction in the previous code \nsnippet. Similarly, Peewee uses a class method to extract multiple records, but this time, it uses the \nselect() method. The resulting object can’t be encoded by FastAPI unless it’s contained in the \nList collection, which serializes the rows of data into a list of JSON-able objects:  \n    def get_all_login(self):\n        return list(Login.select())\n    \n    def get_login(self, id:int): \n        return Login.get(Login.id == id)",
      "content_length": 1828,
      "extraction_method": "Direct"
    },
    {
      "page_number": 178,
      "chapter": null,
      "content": "Building the repository using Peewee\n155\nOn the other hand, the following repository classes show how to create JOIN queries using its \njoin() method: \nfrom peewee import JOIN\nclass LoginTrainersRepository:\n    \n    def join_login_trainers(self): \n        return list(Profile_Trainers.\n          select(Profile_Trainers, Login).join(Login))\nclass MemberGymClassesRepository:\n    def outer_join_member_gym(self): \n        return list(Profile_Members.\n          select(Profile_Members,Gym_Class).join(Gym_Class, \n                    join_type=JOIN.LEFT_OUTER))\njoin_login_trainers() of LoginTrainersRepository builds the INNER JOIN query \nof the Profile_Trainers and Login objects. The leftmost model indicated in the parameter \nof the Profile_Trainers object’s select() directive is the parent model type, followed by its \nchild model class in a one-to-one relationship. The select() directive emits the join() method \nwith the model class type, which indicates the type of records that belong to the right-hand side of the \nquery. The ON condition(s) and the foreign key constraints are optional but can be declared explicitly \nby adding the on and join_type attributes of the join() construct. An example of this query \nis outer_join_member_gym() of MemberGymClassesRepository, which implements \na LEFT OUTER JOIN of Profile_Members and Gym_Class using the LEFT_OUTER option \nof the join_type attribute of join().\nJoins in Peewee also need the list() collection to serialize the retrieved records. All the repository \nclasses for Peewee can be found in the /repository/peewee/login.py script.\nRunning the CRUD transaction\nSince Peewee’s database connection is set at the model layer, no additional requirements are required \nfor APIRouter or FastAPI to run the CRUD transactions. API services can easily access all the \nrepository classes without calling methods or directives from the db instance.\nSo far, we have experimented with popular ORMs to integrate a relational database into the FastAPI \nframework. If applying an ORM is not enough for a microservice architecture, we can utilize some \ndesign patterns that can further refine the CRUD performance, such as CQRS.",
      "content_length": 2173,
      "extraction_method": "Direct"
    },
    {
      "page_number": 179,
      "chapter": null,
      "content": "Connecting to a Relational Database\n156\nApplying the CQRS design pattern\nCQRS is a microservice design pattern responsible for segregating query transactions (reads) from the \ninsert, update, and delete operations (writes). The separation of these two groups lessens the cohesion \naccess to these transactions, which provides less traffic and faster performance, especially when the \napplication becomes complex. Moreover, this design pattern creates a loose-coupling feature between \nthe API services and the repository layer, which gives us an advantage if there are several turnovers \nand changes in the repository layers. \nDefining the handler interfaces\nTo pursue CQRS, we need to create the two interfaces that define the query and the command \ntransactions. The following code shows the interfaces that will identify the read and write transactions \nfor Profile_Trainers:\nclass IQueryHandler: \n    pass \nclass ICommandHandler: \n    pass\nHere, IQueryHandler and ICommandHandler are informal interfaces because Python does \nnot have an actual definition of an interface.\nCreating the command and query classes\nNext, we need to implement the command and query classes. The command serves as an instruction \nto pursue the write transactions. It also carries the state of the result after they have been executed. On \nthe other hand, the query instructs the read transaction to retrieve record(s) from the database and \ncontain the result afterward. Both components are serializable classes with getter/setter attributes. The \nfollowing code shows the script for ProfileTrainerCommand, which uses Python’s @property \nattribute to store the state of the INSERT execution:\nfrom typing import Dict, Any\nclass ProfileTrainerCommand: \n    \n    def __init__(self): \n        self._details:Dict[str,Any] = dict()\n        \n    @property",
      "content_length": 1829,
      "extraction_method": "Direct"
    },
    {
      "page_number": 180,
      "chapter": null,
      "content": "Applying the CQRS design pattern\n157\n    def details(self):\n        return self._details\n    @details.setter\n    def details(self, details):\n        self._details = details\nThe details property will store all the column values of the trainer’s profile record that need to \nbe persisted.\nThe following script implements a sample query class:\nclass ProfileTrainerListQuery: \n    \n    def __init__(self): \n        self._records:List[Profile_Trainers] = list()\n        \n    @property\n    def records(self):\n        return self._records\n    @records.setter\n    def records(self, records):\n        self._records = records\nThe constructor of ProfileTrainerListQuery prepares a dictionary object that will contain \nall the retrieved records after the query transaction has been executed.\nCreating the command and query handlers\nWe will be using our previous interfaces to define the command and query handlers. Note that the \ncommand handler accesses and executes the repository to execute the write transactions, while the query \nhandler processes the read transactions. These handlers serve as the façade between the API services \nand the repository layer. The following code shows the script for AddTrainerCommandHandler, \nwhich manages the INSERT transaction for the trainer’s profile:\nfrom cqrs.handlers import ICommandHandler\nfrom repository.gino.trainers import TrainerRepository\nfrom cqrs.commands import ProfileTrainerCommand",
      "content_length": 1426,
      "extraction_method": "Direct"
    },
    {
      "page_number": 181,
      "chapter": null,
      "content": "Connecting to a Relational Database\n158\nclass AddTrainerCommandHandler(ICommandHandler): \n    \n    def __init__(self): \n        self.repo:TrainerRepository = TrainerRepository()\n        \n    async def handle(self, \n             command:ProfileTrainerCommand) -> bool:\n        result = await self.repo.\n               insert_trainer(command.details)\n        return result\nThe handler depends on ProfileTrainerCommand for the record values that are crucial to the \nasynchronous execution of its handle() method. \nThe following script shows a sample implementation for a query handler:\nclass ListTrainerQueryHandler(IQueryHandler): \n    def __init__(self): \n        self.repo:TrainerRepository = TrainerRepository()\n        self.query:ProfileTrainerListQuery = \n             ProfileTrainerListQuery()\n        \n    async def handle(self) -> ProfileTrainerListQuery:\n        data = await self.repo.get_all_member();\n        self.query.records = data\n        return self.query\nQuery handlers return their query to the services and not the actual values. The handle() method \nof ListTrainerQueryHandler returns ProfileTrainerListQuery, which contains the \nlist of records from the read transaction. This mechanism is one of the main objectives of applying \nCQRS to microservices. \nAccessing the handlers\nCQRS, aside from managing the friction between the read and write executions, does not allow the \nAPI services to interact directly with the execution of CRUD transactions. Moreover, it streamlines \nand simplifies the access of the CRUD transactions by assigning only the handler that’s needed by a \nparticular service.",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 182,
      "chapter": null,
      "content": "Applying the CQRS design pattern\n159\nThe following script shows how AddTrainerCommand is only directly associated with the add_\ntrainer() service and how LisTrainerQueryHandler is only directly associated with the \nlist_trainers() service:\nfrom cqrs.commands import ProfileTrainerCommand\nfrom cqrs.queries import ProfileTrainerListQuery\nfrom cqrs.trainers.command.create_handlers import \n      AddTrainerCommandHandler\nfrom cqrs.trainers.query.query_handlers import \n      ListTrainerQueryHandler\nrouter = APIRouter(dependencies=[Depends(get_db)])\n@router.post(\"/trainer/add\" )\nasync def add_trainer(req: ProfileTrainersReq): \n    handler = AddTrainerCommandHandler()\n    mem_profile = dict()\n    mem_profile[\"id\"] = req.id\n    … … … … … …\n    mem_profile[\"shift\"] = req.shift\n    command = ProfileTrainerCommand()\n    command.details = mem_profile\n    result = await handler.handle(command)\n    if result == True: \n        return req \n    else: \n        return JSONResponse(content={'message':'create \n          trainer profile problem encountered'}, \n            status_code=500)\n@router.get(\"/trainer/list\")\nasync def list_trainers(): \n    handler = ListTrainerQueryHandler()\n    query:ProfileTrainerListQuery = await handler.handle() \n    return query.records",
      "content_length": 1263,
      "extraction_method": "Direct"
    },
    {
      "page_number": 183,
      "chapter": null,
      "content": "Connecting to a Relational Database\n160\nWe can identify transactions that are accessed frequently in APIRouter through CQRS. It helps us \nfind which transactions need performance tuning and focus, which can help us avoid performance \nissues when the amount of access increases. When it comes to enhancement and upgrades, the design \npattern can help developers find what domain to prioritize because of the separation of aspects in the \nrepository layer. Generally, it offers flexibility to the application when its business processes need to \nbe revamped. All the CQRS-related scripts can be found in the /cqrs/ project folder.\nSummary\nApplying ORM always has advantages and disadvantages for any application. It can bloat the application \nwith so many configurations and layers of components, and it can even slow down the application \nif not managed well. But ORM, in general, can help optimize query development by simplifying the \nconstructs by using its APIs and eliminating unimportant repetitive SQL scripts. Overall, it can reduce \nthe time and cost of software development compared to using cursor from psycopg2.\nIn this chapter, four Python ORMs were used, studied, and experimented with to help FastAPI create \nits repository layer. First, there is SQLAlchemy, which provides a boilerplated approach to creating \nstandard and asynchronous data persistency and query operations. Then, there is GINO, which uses \nthe AsyncIO environment to implement asynchronous CRUD transactions with its handy syntax. \nAlso, there is Pony, the most Pythonic among the ORMs presented because it uses hardcore Python \ncode to build its repository transactions. Lastly, there is Peewee, known for its concise syntax but \ntricky composition for the asynchronous database connection and CRUD transactions. Each ORM \nhas its strengths and weaknesses, but all provide a logical solution rather than applying brute-force \nand native SQL.\nIf the ORM needs fine-tuning, we can add some degree of optimization by using data-related design \npatterns such as CQRS, which minimizes friction between the \"read\" and \"write\" CRUD transactions. \nThis chapter has highlighted the flexibility of FastAPI when utilizing ORMs to establish a connection \nto a relational database such as PostgreSQL. But what if we use a NoSQL database such as MongoDB \nto store information? Will FastAPI perform with the same level of performance when performing \nCRUD to and from MongoDB? The next chapter will discuss various solutions for integrating FastAPI \ninto MongoDB.",
      "content_length": 2532,
      "extraction_method": "Direct"
    },
    {
      "page_number": 184,
      "chapter": null,
      "content": "6\nUsing a Non-Relational \nDatabase\nSo far, we have learned that relational databases store data using table columns and rows. All these \ntable records are structurally optimized and designed using different keys, such as primary, unique, and \ncomposite keys. The tables are connected using foreign/reference keys. Foreign key integrity plays a \nsignificant role in the table relationship of a database schema because it gives consistency and integrity \nto the data that’s persisted in the tables. Chapter 5, Connecting to a Relational Database, provided \nconsiderable proof that FastAPI can connect to relational databases using any of the present ORMs \nsmoothly without lots of complexities. This time, we will focus on using non-relational databases as \ndata storage for our FastAPI microservice application. \nIf FastAPI uses ORM for relational databases, it uses Object Document Mapping (ODM) to manage \ndata using non-relational data stores or NoSQL databases. There are no tables, keys, and foreign \nkey constraints involved in ODM, but a JSON document is needed to hold the various pieces of \ninformation. Different NoSQL databases vary in the storage model type that’s used to store data. The \nsimplest among these databases manages data as key-value pairs, such as Redis, while complicated \ndatabases utilize schema-free document structures easily mapped to objects. This is usually done \nin MongoDB. Some use columnar data stores such as Cassandra, while some have graph-oriented \ndata storage such as Neo4j. However, this chapter will focus on the FastAPI-MongoDB connectivity \nand the different ODM we can apply to pursue data management with a document-based database.\nThe main objective of this chapter is to study, formalize, and scrutinize different ways to use MongoDB \nas a database for our FastAPI application. Building the repository layer and showcasing the CRUD \nimplementation will be the main highlight. \nIn this chapter, we will cover the following topics:\n•\t Setting up the database environment\n•\t Applying the PyMongo driver for synchronous connections",
      "content_length": 2078,
      "extraction_method": "Direct"
    },
    {
      "page_number": 185,
      "chapter": null,
      "content": "Using a Non-Relational Database\n162\n•\t Creating asynchronous CRUD transactions using Motor\n•\t Implementing CRUD transactions using MongoEngine\n•\t Implementing asynchronous CRUD transactions using Beanie \n•\t Building an asynchronous repository for FastAPI using ODMantic\n•\t Creating CRUD transactions using MongoFrames\nTechnical requirements\nThis chapter focuses on an eBookstore web portal, online book reselling system, where users can sell and \nbuy books from home through the internet. The virtual store allows users to view the sellers' profiles, \nbook catalogs, list of orders, and archive of purchases. When it comes to the e-commerce side, the user \ncan select their preferred books and add them to a cart. Then, they can check out the items as orders \nand pursue the payment transaction afterward. All the data is stored in a MongoDB database. The code \nfor this chapter can be found at https://github.com/PacktPublishing/Building-\nPython-Microservices-with-FastAPI in the ch06 project.\nSetting up the database environment\nBefore we start discussing the application’s database connectivity, we need to download the appropriate \nMongoDB database server from https://www.mongodb.com/try/download/community. \nonline book reselling system uses MongoDB 5.0.5 for a Windows platform. The installation will provide \ndefault service configuration details for the service name, data directory, and log directory. However, \nit is advised that you use different directory paths instead of the default ones. \nAfter the installation, we can start the MongoDB server by running /bin/mongod.exe. This will \nautomatically create a database directory called /data/db in the C:/ drive (Windows). We can \nplace the /data/db directory in some other location, but be sure to run the mongod command \nwith the --dbpath option while specifying <new path>/data/db.\nThe MongoDB platform has utilities that can aid in managing database collections, and one of them is \nMongoDB Compass. It can provide a GUI experience that allows you to browse, explore, and easily \nmanipulate the database and its collections. Also, it has built-in performance metrics, query views, and \nschema visualization features that can help with scrutinizing the correctness of the database structure. \nThe following screenshot shows the dashboard for MongoDB Compass version 1.29.6:",
      "content_length": 2339,
      "extraction_method": "Direct"
    },
    {
      "page_number": 186,
      "chapter": null,
      "content": "Setting up the database environment\n163\nFigure 6.1 – The MongoDB Compass dashboard\nThe preceding dashboard shows the document structure of the login collection, which is part of the \nobrs database. It gives us the spread of the data, which is an easy way to view its embedded documents, \nsuch as profile and list of books for sale.\nOnce the server and utility have been installed, we need to design the data collections for our database \nusing the class diagram. A class diagram is a UML approach to describing the components of a class and \nvisualizing the associations and structures of the model classes involved in a system. The class diagram \nis one of the solutions that’s used to design the document structure of the MongoDB database since \nthere are no records, tables, or keys involved that are essential for ERD, like in a relational database. \nDesigning a NoSQL database always requires an equal balance between the data retrieval methods \nand the data composition of the database. Data that will be stored in MongoDB always needs an ideal, \nfeasible, and appropriate document structure, associations, aggregations, and layout. The following \ndiagram shows the class diagram for our application’s MongoDB database, obrs:",
      "content_length": 1231,
      "extraction_method": "Direct"
    },
    {
      "page_number": 187,
      "chapter": null,
      "content": "Using a Non-Relational Database\n164\nFigure 6.2 – The class diagram for the obrs database\nOur application uses all the collections depicted in the preceding diagram to store all the information \nit captures from the client. Each context box represents one collection, with all the attributes and \nexpected underlying transactions indicated inside the box. It also shows the associations that bind \nthese collections, such as the one-to-one association between login and profile and the many-\nto-one association between BookForSale and UserProfile.\nNow that the database server has been installed and designed, let us look at the different ways of \nestablishing a connection from our FastAPI microservice application to its MongoDB database.",
      "content_length": 739,
      "extraction_method": "Direct"
    },
    {
      "page_number": 188,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n165\nApplying the PyMongo driver for synchronous \nconnections\nWe will start by learning how the FastAPI application connects to MongoDB using the PyMongo \ndatabase driver. This driver is equivalent to psycopg2, which allows us to access PostgreSQL without \nusing any ORM. Some popular ODMs, such as MongoEngine and Motor, use PyMongo as their core \ndriver, which gives us the reason to explore PyMongo first before we touch on issues regarding popular \nODMs. Studying the driver’s behavior can provide baseline transactions that will show how an ODM \nbuilds the database connectivity, models, and CRUD transactions. But before we proceed with the \ndetails, we need to install the pymongo extension using pip:\npip install pymongo \nSetting up the database connectivity\nPyMongo uses its MongoClient module class to connect to any MongoDB database. We instantiate it \nwith the specified host and port to extract the client object, such as MongoClient(\"localhost\", \n\"27017\"), or a database URI, such as MongoClient('mongodb://localhost:27017/'). \nOur application uses the latter to connect to its database. But if we instantiate without providing the \nparameters, it will use the default localhost and 27017 details.\nAfter extracting the client object, we can use it to access the database through a dot (.) operator or \nattribute-style access if the database name follows the Python naming convention; for \nexample, client.obrs. Otherwise, we can use the bracket symbols ([]) or dictionary-style access; \nfor example, client[\"obrs_db\"]. Once the database object has been retrieved, we can access \nthe collections using the access rules. Note that a collection is equivalent to a table in a relational \ndatabase, where the collated records, known as documents, are stored. The following code shows a \ngenerator function that’s used by the application to open database connectivity and access the necessary \ncollections in preparation for the CRUD implementation:\nfrom pymongo import MongoClient\ndef create_db_collections():\n    client = MongoClient('mongodb://localhost:27017/')\n    try:\n        db = client.obrs\n        buyers = db.buyer\n        users = db.login\n        print(\"connect\")\n        yield {\"users\": users, \"buyers\": buyers}",
      "content_length": 2287,
      "extraction_method": "Direct"
    },
    {
      "page_number": 189,
      "chapter": null,
      "content": "Using a Non-Relational Database\n166\n    finally:\n        client.close()\nA generator function such as create_db_collections() is preferred because the yield \nstatement works perfectly when it comes to managing the database connection over the return \nstatement. The yield statement suspends the function’s execution when it sends a value back to the \ncaller but retains the state where the function can resume at the point where it left off. This feature \nis applied by the generator to close the database connection when it resumes the execution at the \nfinally clause. The return statement, on the other hand, will not be applicable for this purpose \nbecause return will finish the whole transaction before it sends a value to the caller. \nHowever, before we invoke the generator, let us scrutinize how PyMongo builds its model layer to \npursue the necessary CRUD transactions.\nBuilding the model layer\nDocuments in MongoDB are represented and collated as JSON-style structures, specifically BSON \ndocuments. A BSON document offers more data types than the JSON structure. We can use dictionaries \nto represent and persist these BSON documents in PyMongo. Once a dictionary has been persisted, \nthe BSON-type document will look like this:\n{\n   _id:ObjectId(\"61e7a49c687c6fd4abfc81fa\"),\n   id:1,\n   user_id:10,\n   date_purchased:\"2022-01-19T00:00:00.000000\",\n   purchase_history: \n   [\n       {\n        purchase_id:100,\n        shipping_address:\"Makati City\",\n        email:\"mailer@yahoo.com\",\n        date_purchased:\"2022-01-19T00:00:00.000000\",\n        date_shipped:\"2022-01-19T00:00:00.000000\",\n        date_payment:\"2022-01-19T00:00:00.000000\"\n      },\n      {\n        purchase_id:110,\n        shipping_address:\"Pasig City\",\n        email:\"edna@yahoo.com\",",
      "content_length": 1760,
      "extraction_method": "Direct"
    },
    {
      "page_number": 190,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n167\n        date_purchased:\"2022-01-19T00:00:00.000000\",\n        date_shipped:\"2022-01-19T00:00:00.000000\",\n        date_payment:\"2022-01-19T00:00:00.000000\"\n      }\n    ],\n   customer_status: \n   {\n        status_id:90,\n        name:\"Sherwin John C. Tragura\",\n        discount:50,\n        date_membership:\"2022-01-19T00:00:00.000000\"\n   }\n}\nCommon Python data types such as str, int, and float are supported by the BSON specification, \nbut there are types such as ObjectId, Decimal128, RegEx, and Binary that are intrinsic only \nto the bson module. The specification only supports the timestamp and datetime temporal \ntypes. To install bson, use the following pip command:\npip install bson\nImportant note\nBSON is short for Binary JSON, a serialized and binary encoding for JSON-like documents. The \nspecification behind this is lightweight and flexible. The efficient encoding format is explained \nin more detail at https://bsonspec.org/spec.html.\nObjectId is an essential data type in a MongoDB document because it serves as a unique identifier for \nthe main document structure. It is a 12-byte field that consists of a 4-byte UNIX embedded timestamp, \nthe 3-byte machine ID of the MongoDB server, a 2-byte process ID, and a 3-byte arbitrary value for \nthe ID’s increments. Conventionally, the declared field of the document, _id, always refers to the \nObjectId value of the document structure. We can allow the MongoDB server to generate the _id \nobject for the document or create an instance of the object type during persistence. When retrieved, \nObjectId can be in 24 hexadecimal digit or string format. Note that the _id field is the key indicator \nthat a dictionary is ready to be persisted as a valid BSON document. Now, BSON documents can also \nbe linked with one another using some associations.",
      "content_length": 1862,
      "extraction_method": "Direct"
    },
    {
      "page_number": 191,
      "chapter": null,
      "content": "Using a Non-Relational Database\n168\nEstablishing document associations\nMongoDB has no concept of referential integrity constraints, but a relationship among documents \nis possible based on structures. There are two types of documents: main and embedded documents. \nA document has a one-to-one association with another if it is an embedded document of the other. \nLikewise, a document has a many-to-one association if a list in that document is linked to the main \ndocument structure. \nThe previous purchase BSON document shows a sample of the principal buyer document with a \none-to-one association with the customer_status embedded document and a many-to-one \nassociation with the purchase_history documents. As depicted from this sample document, \nembedded documents have no separate collection because they have no respective _id field to make \nthem stand as primary documents. \nUsing the BaseModel classes for transactions\nSince PyMongo has no predefined model classes, the Pydantic models of FastAPI can be used to \nrepresent MongoDB documents with all the necessary validation rules and encoders. We can use the \nBaseModel classes to contain document details and pursue insert, update, and delete transactions \nsince the Pydantic models are compatible with MongoDB documents. The following models are being \nused by our online book reselling application to store and retrieve the buyer, purchase_history, \nand customer_status document details:\n from pydantic import BaseModel, validator\nfrom typing import List, Optional, Dict\nfrom bson import ObjectId\nfrom datetime import date\nclass PurchaseHistoryReq(BaseModel):\n    purchase_id: int\n    shipping_address: str \n    email: str   \n    date_purchased: date\n    date_shipped: date\n    date_payment: date\n    @validator('date_purchased')\n    def date_purchased_datetime(cls, value):\n        return datetime.strptime(value, \n           \"%Y-%m-%dT%H:%M:%S\").date()",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 192,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n169\n    \n    @validator('date_shipped')\n    def date_shipped_datetime(cls, value):\n        return datetime.strptime(value, \n           \"%Y-%m-%dT%H:%M:%S\").date()\n    \n    @validator('date_payment')\n    def date_payment_datetime(cls, value):\n        return datetime.strptime(value, \n           \"%Y-%m-%dT%H:%M:%S\").date()\n    \n    class Config:\n        arbitrary_types_allowed = True\n        json_encoders = {\n            ObjectId: str\n        }\n        \nclass PurchaseStatusReq(BaseModel):\n    status_id: int \n    name: str\n    discount: float \n    date_membership: date\n    @validator('date_membership')\n    def date_membership_datetime(cls, value):\n        return datetime.strptime(value, \n            \"%Y-%m-%dT%H:%M:%S\").date()\n    \n    class Config:\n        arbitrary_types_allowed = True\n        json_encoders = {\n            ObjectId: str\n        }\n        \nclass BuyerReq(BaseModel):",
      "content_length": 948,
      "extraction_method": "Direct"
    },
    {
      "page_number": 193,
      "chapter": null,
      "content": "Using a Non-Relational Database\n170\n    _id: ObjectId\n    Buyer_id: int\n    user_id: int\n    date_purchased: date\n    purchase_history: List[Dict] = list()\n    customer_status: Optional[Dict]\n    @validator('date_purchased')\n    def date_purchased_datetime(cls, value):\n        return datetime.strptime(value, \n            \"%Y-%m-%dT%H:%M:%S\").date()\n  \n    class Config:\n        arbitrary_types_allowed = True\n        json_encoders = {\n            ObjectId: str\n        }\nFor these request models to recognize the BSON data types, we should make some modifications to \nthe default behavior of these models. Just like earlier in this chapter, where we added the orm_mode \noption, there is also a need to add a nested Config class to the BaseModel blueprint with the \narbitrary_types_allowed option set to True. This additional configuration will recognize the \nBSON data types used in the attribute declaration, including compliance with the necessary underlying \nvalidation rules for the corresponding BSON data types used. Moreover, the json_encoders \noption should also be part of the configuration to convert the ObjectId property of the document \ninto a string during a query transaction.\nUsing Pydantic validation\nHowever, some other types are too complex for json_encoders to process, such as the BSON \ndatettime field being converted into a Python datetime.date. Since the ODM cannot \nautomatically convert a MongoDB datetime into a Python date type, we need to create a custom \nvalidation and parse this BSON datetime through Pydantic’s @validation decorator. We must \nalso use custom validators and parsers in the FastAPI services to convert all incoming Python date \nparameters into BSON datetime. This will be covered later.",
      "content_length": 1736,
      "extraction_method": "Direct"
    },
    {
      "page_number": 194,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n171\n@validator creates a class method that accepts class name as the first parameter, not the \ninstance, of the field(s) to be validated and parsed. Its second parameter is an option that specifies \nthe field name or class attribute that needs to be converted into another data type, such as date_\npurchased, date_shipped, or date_payment of the PurchaseRequestReq model. The \npre attribute of @validator tells FastAPI to process the class methods before any built-in validation \ncan be done in the API service implementation. These methods are executed right after APIRouter \nruns its custom and built-in FastAPI validation rules for the request models, if there are any.\nNote that these request models have been placed in the /models/request/buyer.py module \nof the application.\nUsing the Pydantic @dataclass to query documents\nWrapping the queried BSON documents using the BaseModel model classes is still the best approach \nto implementing the query transaction. But since BSON has issues with the Python datetime.\ndate fields, we cannot always utilize the request model classes that are used for the CRUD transaction \nby wrapping retrieved BSON documents. Sometimes, using the model yields an \"invalid date \nformat (type=value_error.date)\" error because all the models have the Python datetime.\ndate fields, whereas the incoming data has a BSON datetime or timestamp. Instead of adding \nmore complexities to the request models, we should resort to another approach to extracting the \ndocuments – that is, utilizing the Pydantic @dataclass. The following data classes are defined for \nwrapping the extracted buyer documents:\nfrom pydantic.dataclasses import dataclass\nfrom dataclasses import field\nfrom pydantic import validator\nfrom datetime import date, datetime\nfrom bson import ObjectId\nfrom typing import List, Optional\nclass Config:\n        arbitrary_types_allowed = True\n@dataclass(config=Config)\nclass PurchaseHistory:\n    purchase_id: Optional[int] = None",
      "content_length": 2024,
      "extraction_method": "Direct"
    },
    {
      "page_number": 195,
      "chapter": null,
      "content": "Using a Non-Relational Database\n172\n    shipping_address: Optional[str] = None\n    email: Optional[str] = None   \n    date_purchased: Optional[date] = \"1900-01-01T00:00:00\"\n    date_shipped: Optional[date] = \"1900-01-01T00:00:00\"\n    date_payment: Optional[date] = \"1900-01-01T00:00:00\"\n    \n    @validator('date_purchased', pre=True)\n    def date_purchased_datetime(cls, value):\n        return datetime.strptime(value, \n           \"%Y-%m-%dT%H:%M:%S\").date()\n    \n    @validator('date_shipped', pre=True)\n    def date_shipped_datetime(cls, value):\n        return datetime.strptime(value, \n           \"%Y-%m-%dT%H:%M:%S\").date()\n    \n    @validator('date_payment', pre=True)\n    def date_payment_datetime(cls, value):\n        return datetime.strptime(value, \n           \"%Y-%m-%dT%H:%M:%S\").date()\n@dataclass(config=Config)\nclass PurchaseStatus:\n    status_id: Optional[int] = None\n    name: Optional[str] = None\n    discount: Optional[float] = None\n    date_membership: Optional[date] = \"1900-01-01T00:00:00\"\n    \n    @validator('date_membership', pre=True)\n    def date_membership_datetime(cls, value):\n        return datetime.strptime(value, \n           \"%Y-%m-%dT%H:%M:%S\").date()\n      \n@dataclass(config=Config)\nclass Buyer:\n    buyer_id: int",
      "content_length": 1248,
      "extraction_method": "Direct"
    },
    {
      "page_number": 196,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n173\n    user_id: int \n    date_purchased: date \n    purchase_history: List[PurchaseHistory] = \n          field(default_factory=list )\n    customer_status: Optional[PurchaseStatus] = \n          field(default_factory=dict)\n    _id: ObjectId = field(default=ObjectId())\n    \n    @validator('date_purchased', pre=True)\n    def date_purchased_datetime(cls, value):\n        print(type(value))\n        return datetime.strptime(value, \n             \"%Y-%m-%dT%H:%M:%S\").date()\n@dataclass is a decorator function that adds an __init__() to a Python class to initialize \nits attributes and other special functions, such as __repr__(). The PurchasedHistory, \nPurchaseStatus, and Buyer custom classes shown in the preceding code are typical classes that \ncan be converted into request model classes. FastAPI supports both BaseModel and data classes when \ncreating model classes. Apart from being under the Pydantic module, using @dataclass is not a \nreplacement for using BaseModel when creating model classes. This is because the two components \nare different in terms of their flexibility, features, and hooks. BaseModel is configuration-friendly \nand can be adapted to many validation rules and type hints, while @dataclass has problems \nrecognizing some Config attributes such as extra, allow_population_by_field_name, \nand json_encoders. If a data class requires some additional details, a custom class is needed to \ndefine these configurations and set the config parameter of the decorator. For instance, the Config \nclass in the preceding code, which sets arbitrary_types_allowed to True, has been added \nto the three model classes. \nBesides config, the decorator has other parameters such as init, eq, and repr that accept bool \nvalues to generate their respective hook methods. The frozen parameter enables exception handling \nconcerning field type mismatches when set to True.\nWhen it comes to data parsing, transition, and conversion, @dataclass is always dependent on \naugmented validations, unlike BaseModel, which can process data type conversion simply by adding \njson_encoders. In the data classes shown previously, all the validators focus on BSON datetime \nto Python datetime.date conversion during the document retrieval process. These validations \nwill occur before any custom or built-in validation in APIRouter because the pre parameter of \nthe @validator decorator is set to True.",
      "content_length": 2447,
      "extraction_method": "Direct"
    },
    {
      "page_number": 197,
      "chapter": null,
      "content": "Using a Non-Relational Database\n174\nWhen dealing with default values, BaseModel classes can use typical type hints such as Optional \nor object instantiation such as dict() or list() to define the preconditional state of its complex \nattributes. With @dataclass, a ValueError exception is always thrown at compile time when \ntype hints are applied to set default values of complex field types such as list, dict, and ObjectId. \nIt requires the field() specifier from Python’s dataclasses module to set the default values of \nthese fields, either by assigning an actual value through the specifier’s default parameter or invoking \na function or lambda that returns a valid value through the default_factory parameter. The \nuse of field() indicates that Pydantic’s @dataclass is an exact replacement of Python’s core \ndata classes but with some additional features, such as the config parameter and the inclusion of \nthe @validator components. \nNote that it is advised that all @dataclass models have default values when using type hints or \nfield(), especially for embedded documents and for models with the date or datetime types, \nto avoid some missing constructor parameter(s) errors. On the other hand, an @dataclass can \nalso create embedded structures in the BaseModel classes, for example, by defining attributes with \nthe class types. This is highlighted in the Buyer model.\nAll these model classes have been placed in the /models/data/pymongo.py script. Let us now \napply these data models to create the repository layer.\nImplementing the repository layer\nPyMongo needs collection to build the repository layer of the application. Besides the collection \nobject, the insert, delete, and update transactions will also need the BaseModel classes to contain all \nthe details from the client and convert them into BSON documents after the transaction. Meanwhile, \nour query transactions will require the data classes to convert all BSON documents into JSON-\nable resources during the document retrieval process. Now, let us look at how a repository can be \nimplemented using a PyMongo driver.\nBuilding the CRUD transactions\nThe repository class in the following code block implements the CRUD transactions that aim to \nmanage the buyer, purchase_history, and customer_status information based on basic \nspecifications of the online book reselling system:\nfrom typing import Dict, Any\nclass BuyerRepository: \n    \n    def __init__(self, buyers): \n        self.buyers = buyers\n    \n    def insert_buyer(self, users,",
      "content_length": 2516,
      "extraction_method": "Direct"
    },
    {
      "page_number": 198,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n175\n          details:Dict[str, Any]) -> bool: \n        try:\n           user = users.find_one(\n                {\"_id\": details[\"user_id\"]})\n           print(user)\n           if user == None:\n               return False\n           else: \n               self.buyers.insert_one(details)\n                  \n        except Exception as e:\n            return False \n        return True\nLet us examine insert_buyer(), which inserts details about a registered book buyer who had some \nprevious transactions in the system as a login user. The PyMongo collection offers helper methods \nfor processing CRUD transactions, such as insert_one(), which adds a single main document \nfrom its Dict parameter. It also has insert_many(), which accepts a valid list of dictionaries that \ncan be persisted as multiple documents. These two methods can generate an ObjectId for the _id \nfield of the BSON document during the insertion process. The buyer’s details are extracted from the \nBuyerReq Pydantic model.\nNext, update_buyer() shows how to update a specific document in the buyer collection:\n    def update_buyer(self, id:int, \n              details:Dict[str, Any]) -> bool: \n       try:\n          self.buyers.update_one({\"buyer_id\": id},\n                  {\"$set\":details})\n       except: \n           return False \n       return True\n   \n    def delete_buyer(self, id:int) -> bool: \n        try:\n            self.buyers.delete_one({\"buyer_id\": id})\n        except: \n            return False \n        return True",
      "content_length": 1552,
      "extraction_method": "Direct"
    },
    {
      "page_number": 199,
      "chapter": null,
      "content": "Using a Non-Relational Database\n176\nThe collection has an update_one() method that requires two parameters: a unique and valid \nfield/value dictionary pair that will serve as the search key of the record search, and another dictionary \npair with the predefined $set key with the dictionary of updated details for replacement. It also has \nupdate_many(), which can update multiple documents, given that the primary dictionary field/\nvalue parameter is not unique.\ndelete_buyer() is the transaction that deletes a buyer document using a unique and valid field/\nvalue pair such as {\"buyer_id\": id}. If this parameter or search key is a common/non-unique \ndata, the collection offers delete_many(), which can delete multiple documents. Now, the following \nscript shows how to implement query transactions in PyMongo\nfrom dataclasses import asdict\nfrom models.data.pymongo import Buyer\nfrom datetime import datetime\nfrom bson.json_util import dumps\nimport json\n    … … …\n    … … …  \n    … … …\n    def get_all_buyer(self):\n        buyers = [asdict(Buyer(**json.loads(dumps(b)))) \n              for b in self.buyers.find()]\n        return buyers\n    \n    def get_buyer(self, id:int): \n        buyer = self.buyers.find_one({\"buyer_id\": id})\n        return asdict(Buyer(**json.loads(dumps(buyer))))\nWhen querying documents, PyMongo has a find() method, which retrieves all the documents in \nthe collection, and find_one(), which can get a unique and single document. Both methods need \ntwo parameters: the conditional or logical query parameter in the form of a dictionary field/value pair \nand the set of fields that needs to appear in the record. get_buyer() in the previous code block \nshows how to retrieve a buyer document through the unique buyer_id field. The absence of its \nsecond parameter means the presence of all the fields in the result. Meanwhile, get_all_buyer() \nretrieves all the buyer documents without constraints. Constraints or filter expressions are formulated \nusing BSON comparison operators, as shown in the following table:",
      "content_length": 2041,
      "extraction_method": "Direct"
    },
    {
      "page_number": 200,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n177\nFor instance, retrieving buyer documents with user_id greater than 5 requires the buyers.\nfind({\"user_id\": {\"$gte\": 5}}) query operation. If we need to build compound filters, \nwe must apply the following logical operators:\nRetrieving buyer documents with buyer_id less than 50 and buyer_id greater than 10 will require the \nfind({'and': [{'buyer_id': {'$lt': 50}}, {'user_id':{'$gt':10}}]}) query.\nBoth methods return BSON documents that are not JSON-able components of the FastAPI framework. \nTo convert the documents into JSON, the bson.json_util extension has a dumps() method that \ncan convert a single document or list of documents into a JSON string. Both get_all_buyer() \nand get_buyer()convert every single document retrieved into JSON so that each can be mapped \nto the Buyer data class. The main objective of the mapping is to convert the datetime fields into \nPython datetime.date while utilizing the validators of the Buyer data class. The mapping \nwill only be successful if the loads() method of the json extension is used to convert str into \na dict data structure. After generating the list of Buyer data classes, the asdict() method of \nPython’s dataclasses module is needed to transform the list of Buyer data classes into a list of \ndictionaries to be consumed by APIRouter.\nManaging document association\nTechnically, there are two ways to construct a document association in PyMongo. The first one is to \nuse the DBRef class of the bison.dbref module to link the parent and child documents. The \nonly prerequisite is for both documents to have an _id value of the ObjectId type and have their \nrespective collection exist. For instance, if PurchaseHistoryReq is a core document, we can \ninsert one purchase record into the list through the following query: \nbuyer[\"purchase_history\"].append(new  DBRef(\"purchase_history\", \n\"49a3e4e5f462204490f70911\"))",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 201,
      "chapter": null,
      "content": "Using a Non-Relational Database\n178\nHere, the first parameter of the DBRef constructor pertains to the name of the collection where the \nchild document is placed, while the second one is the ObjectId property of the child document in \nstring format. However, some people use an ObjectId instance instead of the string version. On \nthe other hand, to find a specific purchase_history document from the buyer collection using \nDBRef, we can write our query like this:\nbuyer.find({ \"purchase_history \": DBRef(\"purchase_\nhistory\",ObjectId(\"49a3e4e5f462204490f70911\")) })\nThe second way is to add the whole BSON document structure to the list field of buyer through \nthe BuyerReq model. This solution applies to embedded documents that do not have _id and \ncollection but are essential to the core document. add_purchase_history() in the \nfollowing code shows how this approach is applied to create a many-to-one association between the \npurchase_history and buyer documents:\ndef add_purchase_history(self, id:int, \n                details:Dict[str, Any]): \n        try:\n            buyer = self.buyers.find_one({\"buyer_id\": id})\n            buyer[\"purchase_history\"].append(details)\n            self.buyers.update_one({\"buyer_id\": id},\n           {\"$set\": {\"purchase_history\": \n                     buyer[\"purchase_history\"]}})\n        except Exception as e: \n           return False \n        return True\n    \n    def add_customer_status(self, id:int, \n                  details:Dict[str, Any]): \n        try:\n            buyer = self.buyers.find_one({\"buyer_id\": id})\n            self.buyers.update_one({\"buyer_id\": id},\n                {\"$set\":{\"customer_status\": details}})\n        except Exception as e: \n           return False \n        return True",
      "content_length": 1749,
      "extraction_method": "Direct"
    },
    {
      "page_number": 202,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n179\nThe add_customer_status() method shows how to implement the second approach in \nbuilding a one-to-one association between the buyer and purchase_status documents. The \nfirst approach, which involves using DBRef, can also be applied if PurchaseStatusReq is an \nindependent core document.\nThe complete repository class can be found in the /repository/pymongo/buyer.py script \nfile. Now, let us apply these CRUD transactions to our API services.\nRunning the transactions\nBefore executing the BuyerRepository transactions, the create_db_collections() \ngenerator should be injected into the API services using Depends. Since PyMongo has difficulty \nprocessing Python types that are not BSON-supported, such as datettime.date, custom validations \nand serializers are sometimes required to pursue some transactions. \nImportant note\nThe implementation of @validator inside @dataclass and BaseModel converts \noutgoing BSON datetime parameters into Python date during query retrieval. Meanwhile, \nthe JSON encoder validation in this API layer converts incoming Python date values into \nBSON datetime values during the transition from the application to MongoDB. \nFor instance, the add_buyer(), update_buyer(), and add_purchase_history() transaction \nmethods in the following code require a custom serializer such as json_serialize_date() to \ntransform the Python datetime.date value into the datettime.datetime type so that it \ncomplies with PyMongo’s BSON specification: \nfrom fastapi import APIRouter, Depends\nfrom fastapi.responses import JSONResponse\nfrom models.request.buyer import BuyerReq, \n      PurchaseHistoryReq, PurchaseStatusReq\nfrom repository.pymongo.buyer import BuyerRepository\nfrom db_config.pymongo_config import create_db_collections\nfrom datetime import date, datetime\nfrom json import dumps, loads\nfrom bson import ObjectId\nrouter = APIRouter()",
      "content_length": 1917,
      "extraction_method": "Direct"
    },
    {
      "page_number": 203,
      "chapter": null,
      "content": "Using a Non-Relational Database\n180\ndef json_serialize_date(obj):\n    if isinstance(obj, (date, datetime)):\n        return obj.strftime('%Y-%m-%dT%H:%M:%S')\n    raise TypeError (\"The type %s not serializable.\" % \n            type(obj))\ndef json_serialize_oid(obj):\n    if isinstance(obj, ObjectId):\n        return str(obj)\n    elif isinstance(obj, date):\n        return obj.isoformat()\n    raise TypeError (\"The type %s not serializable.\" % \n            type(obj))\n@router.post(\"/buyer/add\")\ndef add_buyer(req: BuyerReq, \n            db=Depends(create_db_collections)): \n    buyer_dict = req.dict(exclude_unset=True)\n    buyer_json = dumps(buyer_dict, \n              default=json_serialize_date)\n    repo:BuyerRepository = BuyerRepository(db[\"buyers\"])\n    result = repo.insert_buyer(db[\"users\"], \n            loads(buyer_json))  \n   \n    if result == True: \n        return JSONResponse(content={\"message\": \n          \"add buyer successful\"}, status_code=201) \n    else: \n        return JSONResponse(content={\"message\": \n          \"add buyer unsuccessful\"}, status_code=500) \n@router.patch(\"/buyer/update\")\ndef update_buyer(id:int, req:BuyerReq, \n           db=Depends(create_db_collections)): \n    buyer_dict = req.dict(exclude_unset=True)\n    buyer_json = dumps(buyer_dict,",
      "content_length": 1275,
      "extraction_method": "Direct"
    },
    {
      "page_number": 204,
      "chapter": null,
      "content": "Applying the PyMongo driver for synchronous connections\n181\n             default=json_serialize_date)\n    repo:BuyerRepository = BuyerRepository(db[\"buyers\"])\n    result = repo.update_buyer(id, loads(buyer_json))  \n   \n    if result == True: \n        return JSONResponse(content={\"message\": \n         \"update buyer successful\"}, status_code=201) \n    else: \n        return JSONResponse(content={\"message\": \n         \"update buyer unsuccessful\"}, status_code=500)\n@router.post(\"/buyer/history/add\")\ndef add_purchase_history(id:int, req:PurchaseHistoryReq, \n           db=Depends(create_db_collections)): \n    history_dict = req.dict(exclude_unset=True)\n    history_json = dumps(history_dict, \n           default=json_serialize_date)\n    repo:BuyerRepository = BuyerRepository(db[\"buyers\"])\n    result = repo.add_purchase_history(id, \n           loads(history_json))  \nThe json_serialize_date() function becomes part of the JSON serialization process of the \ndumps() method but only handles the temporal type conversion while transforming the buyer \ndetails into JSON objects. It is applied in the INSERT and UPDATE transactions of the repository \nclass to extract the serialized JSON string equivalent of the BuyerReq, PurchaseHistoryReq, \nand PurchaseStatusReq models. \nNow, another custom converter is applied in the data retrievals of the list_all_buyer() and \nget_buyer() methods:\n@router.get(\"/buyer/list/all\")\ndef list_all_buyer(db=Depends(create_db_collections)): \n  repo:BuyerRepository = BuyerRepository(db[\"buyers\"])\n  buyers = repo.get_all_buyer() \n  return loads(dumps(buyers, default=json_serialize_oid))\n@router.get(\"/buyer/get/{id}\")\ndef get_buyer(id:int, db=Depends(create_db_collections)):",
      "content_length": 1705,
      "extraction_method": "Direct"
    },
    {
      "page_number": 205,
      "chapter": null,
      "content": "Using a Non-Relational Database\n182\n  repo:BuyerRepository = BuyerRepository(db[\"buyers\"])\n  buyer = repo.get_buyer(id)\n  return loads(dumps(buyer, default=json_serialize_oid))\nThe data models involved in our query transactions are data classes, so the results of the two preceding \nquery methods have already been mapped and transformed into JSON format. However, unfortunately, \nthey’re not JSON-able enough for the FastAPI framework. Aside from BSON datetime types, the \nPyMongo ODM cannot automatically convert ObjectId into a default type in Python, thus throwing \nValueError during data retrieval from MongoDB. To fix this problem, dumps()needs a custom \nserializer, such as json_serialize_oid(), to convert all ObjectId parameters in MongoDB \ninto FastAPI transitions. It also converts BSON datetime values into Python date values following \nthe ISO-8601 format. The valid JSON string from dumps() will enable the loads() method to \nproduce a JSON-able result for the FastAPI services. The complete API services can be found in the \n/api/buyer.py script file.\nAfter complying with all the requirements, PyMongo can help store and manage all the information \nusing the MongoDB server. However, the driver only works for synchronous CRUD transactions. If \nwe opt for an asynchronous way of implementing CRUD, we must always resort to the Motor driver.\nCreating async CRUD transactions using Motor\nMotor is an asynchronous driver that relies on the AsyncIO environment of the FastAPI. It wraps \nPyMongo to produce non-blocking and coroutine-based classes and methods needed to create \nasynchronous repository layers. It is almost like PyMongo when it comes to most of the requirements \nexcept for the database connectivity and repository implementation.\nBut before we proceed, we need to install the motor extension using the following pip command:\npip install motor\nSetting up the database connectivity\nUsing the AsyncIO platform of the FastAPI, the Motor driver opens a connection to the MongoDB \ndatabase through its AsyncIOMotorClient class. When instantiated, the default connection credential \nis always localhost at port 27017. Alternatively, we can specify the new details in str format \nthrough its constructor. The following script shows how to create a global AsyncIOMotorClient \nreference with the specified database credentials:\nfrom motor.motor_asyncio import AsyncIOMotorClient\ndef create_async_db():\n    global client\n    client = AsyncIOMotorClient(str(\"localhost:27017\"))",
      "content_length": 2493,
      "extraction_method": "Direct"
    },
    {
      "page_number": 206,
      "chapter": null,
      "content": "Creating async CRUD transactions using Motor\n183\ndef create_db_collections():\n    db = client.obrs\n    buyers = db[\"buyer\"]\n    users = db[\"login\"]\n    return {\"users\": users, \"buyers\": buyers}\ndef close_async_db(): \n    client.close()\nThe format of the database URI is a string with a colon (:) in between the details. Now, the application \nneeds the following Motor methods to start the database transactions: \n•\t create_async_db(): A method for establishing the database connection and loading \nschema definitions\n•\t close_async_db(): A method for closing the connection\nAPIRouter will require event handlers to manage these two core methods as application-level events. \nLater, we will register create_async_db() as a startup event and close_async_db() as a \nshutdown event. On the other hand, the create_db_collections() method creates some \nreferences to the login and buyer collections, which will be needed by the repository transactions later. \nIn general, creating the database connection and getting the reference to the document collections do \nnot require the async/await expression since no I/O is involved in the process. These methods \ncan be found in the /db_config/motor_config.py script file. It is time now to create Motor's \nrepository layer.\nCreating the model layer\nPyMongo and Motor share the same approaches in creating both the request and data models. All base \nmodels, data classes, validators, and serializers used by PyMongo also apply to Motor connectivity.\nBuilding the asynchronous repository layer\nWhen it comes to the CRUD implementation, both PyMongo and Motor have some slight differences \nin the syntax but a considerable difference in the performance of each transaction. Their helper \nmethods for inserting, updating, and deleting documents, including the necessary method parameters, \nare all the same, except that Motor has the non-blocking versions. Invoking the non-blocking Motor \nmethods inside the repository requires an async/await expression. Here is an asynchronous version \nof PyMongo’s BuyerRepository:\nclass BuyerRepository:",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 207,
      "chapter": null,
      "content": "Using a Non-Relational Database\n184\n    def __init__(self, buyers): \n        self.buyers = buyers\n    \n    async def insert_buyer(self, users, \n           details:Dict[str, Any]) -> bool: \n        try:\n           user = await users.find_one({\"_id\": \n                details[\"user_id\"]})\n           … … … … …\n           else: \n               await self.buyers.insert_one(details)\n           … … … … …\n        return True\n    \n    async def add_purchase_history(self, id:int, \n            details:Dict[str, Any]): \n        try:\n            … … … … …\n            await self.buyers.update_one({\"buyer_id\": id},\n                   {\"$set\":{\"purchase_history\": \n                     buyer[\"purchase_history\"]}})\n            … … … … …\n        return True\ninsert_buyer() in the preceding code block is defined as async because insert_one() is \na non-blocking operation that requires an await invocation. The same goes for add_purchase_\nhistory(), which updates the purchase_history embedded documents using the non-blocking \nupdate_one():\n    async def get_all_buyer(self):\n        cursor = self.buyers.find()\n        buyers = [asdict(Buyer(**json.loads(dumps(b)))) \n           for b in await cursor.to_list(length=None)]\n        return buyers\n    \n    async def get_buyer(self, id:int):",
      "content_length": 1279,
      "extraction_method": "Direct"
    },
    {
      "page_number": 208,
      "chapter": null,
      "content": "Creating async CRUD transactions using Motor\n185\n        buyer = await self.buyers.find_one(\n                    {\"buyer_id\": id})\n        return asdict(Buyer(**json.loads(dumps(buyer))))\nThe delete_many() and find_one() operations are also invoked through an await expression. \nHowever, find() in Motor is not asynchronous and behaves differently than it does with PyMongo. The \nreason is that find() is not an I/O operation in Motor, and it returns an AsyncIOMotorCursor \nor asynchronous cursor, an iterable type that contains all the BSON documents. We apply async \nto the cursor when retrieving all its stored documents. The get_all_buyer() transaction in the \npreceding code shows how we call the find() operation and invoke the cursor to extract the necessary \ndocuments for JSON transformation. This repository class can be found in the /repository/\nmotor/buyer.py script file. Let us now apply these CRUD transactions to our API services.\nRunning the CRUD transactions\nFor the repository to work with APIRouter, we need to create two event handlers to manage \nthe database connection and document collection retrieval. The first event, which is the startup \nevent that the Uvicorn server executes before the application runs, should trigger the create_\nasync_db() method’s execution to instantiate AsyncIOMotorClient and make references \nto the collections. The second event, which is the shutdown event, runs when the Uvicorn server is \nshutting down and should trigger the close_async_db() execution to close the connection. \nAPIRouter has an add_event_handler() method to create these two event handlers. The \nfollowing is a portion of the APIRouter script that shows how to prepare the database connection \nfor the BuyerRepository transactions:\n… … … … … …\nfrom db_config.motor_config import create_async_db,\n  create_db_collections, close_async_db\n… … … … … …\nrouter = APIRouter()\nrouter.add_event_handler(\"startup\", \n            create_async_db)\nrouter.add_event_handler(\"shutdown\", \n            close_async_db)\nThe \"startup\" and \"shutdown\" values are pre-built configuration values and not just any arbitrary \nstring values used to indicate the type of event handlers. We will discuss these event handlers in more \ndetail in Chapter 8, Creating Coroutines, Events, and Message-Driven Transactions.",
      "content_length": 2312,
      "extraction_method": "Direct"
    },
    {
      "page_number": 209,
      "chapter": null,
      "content": "Using a Non-Relational Database\n186\nAfter setting these events handlers, the API services can now invoke the repository transactions \nasynchronously using an await/async expression. The validations and serialization utilities that are \napplied in PyMongo can also be utilized here in this version of BuyerRepository. The collections \nwill be available to the API services upon injecting create_db_collections() into the API \nservices. The add_buyer() API service showcases the implementation of an asynchronous REST \ntransaction using the Motor driver:\n@router.post(\"/buyer/async/add\")\nasync def add_buyer(req: BuyerReq, \n          db=Depends(create_db_collections)): \n    buyer_dict = req.dict(exclude_unset=True)\n    buyer_json = dumps(buyer_dict, \n              default=json_serialize_date)\n    repo:BuyerRepository = BuyerRepository(db[\"buyers\"])\n   \n    result = await repo.insert_buyer(db[\"users\"], \n                  loads(buyer_json))  \n    if result == True: \n        return JSONResponse(content={\"message\":\n            \"add buyer successful\"}, status_code=201) \n    else: \n        return JSONResponse(content={\"message\": \n            \"add buyer unsuccessful\"}, status_code=500)\nUsing PyMongo and Mongo drivers provides a minimal and exhaustive implementation of the MongoDB \ntransactions. The core implementation of every CRUD transaction varies from one developer to \nanother, and the approaches that are used to scrutinize and analyze the processes involved are managed \nin different ways. Also, there are no established standards for defining the document fields, such as \ndata uniqueness, the length of the field value, the value range, and even the idea of adding a unique \nID. To address these issues surrounding PyMongo and Motor, let us explore other ways of opening a \nconnection to MongoDB to create CRUD transactions, such as using an ODM.\nImplementing CRUD transactions using MongoEngine\nMongoEngine is an ODM that uses PyMongo to create an easy-to-use framework that can assist in \nmanaging MongoDB documents. It offers API classes that can help generate model classes using \nits field types and attribute metadata. It provides a declarative way of creating and structuring the \nembedded documents.",
      "content_length": 2221,
      "extraction_method": "Direct"
    },
    {
      "page_number": 210,
      "chapter": null,
      "content": "Implementing CRUD transactions using MongoEngine\n187\nBefore we explore this ODM, we need to install it using the following pip command:\npip install mongoengine\nEstablishing database connection\nMongoEngine has one of the most straightforward ways to establish a connection. Its mongoengine \nmodule has a connect() helper method that connects to the MongoDB database when it’s given \nthe appropriate database connections. Our application must have a generator method to create a \nreference to the database connection and close this created connection after the transactions expire. \nThe following script showcases the MongoEngine database connectivity:\nfrom mongoengine import connect\ndef create_db():\n    try:\n        db = connect(db=\"obrs\", host=\"localhost\", \n                 port=27017)\n        yield db\n    finally: \n        db.close()\nThe connect() method has a mandatory first parameter, named db, which indicates the name \nof the database. The remaining parameters refer to the other remaining details of the database \nconnection, such as host, port, username, and password. This configuration can be found \nin the /db_config/mongoengine_config.py script file. Let us now create data models for \nour MongoEngine repository.\nBuilding the model layer\nMongoEngine provides a convenient and declarative way of mapping BSON documents to the model \nclasses through its Document API class. A model class must subclass Document to inherit the \nstructure and properties of a qualified and valid MongoDB document. The following is a Login \ndefinition that’s been created using the Document API class:\nfrom mongoengine import Document, StringField, \n         SequenceField, EmbeddedDocumentField\nimport json\nclass Login(Document): \n    id = SequenceField(required=True, primary_key=True)",
      "content_length": 1782,
      "extraction_method": "Direct"
    },
    {
      "page_number": 211,
      "chapter": null,
      "content": "Using a Non-Relational Database\n188\n    username = StringField(db_field=\"username\", \n         max_length=50, required=True, unique=True)\n    password = StringField(db_field=\"password\", \n         max_length=50, required=True)\n    profile = EmbeddedDocumentField(UserProfile, \n         required=False)\n    \n    def to_json(self):\n            return {\n            \"id\": self.id,\n            \"username\": self.username,\n            \"password\": self.password,\n            \"profile\": self.profile\n        }\n        \n    @classmethod\n    def from_json(cls, json_str):\n        json_dict = json.loads(json_str)\n        return cls(**json_dict)\nUnlike PyMongo and the Motor drivers, MongoEngine can define class attributes using its Field \nclasses and their properties. Some of its Field classes include StringField, IntField, \nFloatField, BooleanField, and DateField. These can declare the str, int, float, \nbool, and datetime.date class attributes, respectively. \nAnother convenient feature that this ODM has is that it can create SequenceField, which behaves \nthe same as the auto_increment column field in a relational database or Sequence in an \nobject-relational database. The id field of a model class should be declared as SequenceField so \nthat it serves as the primary key of the document. Like in a typical sequence, this field has utilities to \nincrement its value or reset it to zero, depending on what document record must be accessed. \nOther than the field types, field classes can also provide field arguments to attributes such as choices, \nrequired, unique, min_value, max_value, max_length, and min_length to give \nconstraints to the field values. The choices parameter, for instance, accepts an iterable of string values \nthat will serve as an enumeration. The required parameter indicates whether the field always needs \na field value, while the unique parameter means the field value has no duplicates in the collection. \nViolating the unique parameter will lead to the following error message:\nTried to save duplicate unique keys (E11000 duplicate key error \ncollection: obrs.login index: username_...)",
      "content_length": 2113,
      "extraction_method": "Direct"
    },
    {
      "page_number": 212,
      "chapter": null,
      "content": "Implementing CRUD transactions using MongoEngine\n189\nmin_value and max_value, on the other hand, indicate the minimum and maximum values for \nthe numeric fields, respectively. min_length specifies the minimum length of a string value, while \nmax_length sets the maximum string length. The db_field parameter, on the other hand, can also \nbe applied when specifying another document field name instead of the class attribute name. The given \nLogin class also has username and password fields defined to hold string values, an id primary \nkey defined as SequenceField, and an embedded document field to establish document association.\nCreating document association\nThe profile field of Login creates a one-to-one association between the Login document and \nUserProfile. But before the association can work, we need to define the profile field as being of \nthe EmbeddedDocumentField type and UserProfile as being of the EmbeddedDocument \ntype. The following is the complete blueprint of UserProfile: \nclass UserProfile(EmbeddedDocument):\n   firstname = StringField(db_field=\"firstname\", \n          max_length=50, required=True)\n   lastname = StringField(db_field=\"lastname\", \n          max_length=50, required=True)\n   middlename = StringField(db_field=\"middlename\", \n          max_length=50, required=True)\n   position = StringField(db_field=\"position\", \n          max_length=50, required=True)\n   date_approved = DateField(db_field=\"date_approved\", \n          required=True)\n   status = BooleanField(db_field=\"status\", required=True)\n   level = IntField(db_field=\"level\", required=True)\n   login_id = IntField(db_field=\"login_id\", required=True)\n   booksale = EmbeddedDocumentListField(BookForSale, \n           required=False)\n   \n   def to_json(self):\n            return {\n            \"firstname\": self.firstname,\n            \"lastname\": self.lastname,\n            \"middlename\": self.middlename,\n            \"position\": self.position,\n            \"date_approved\":",
      "content_length": 1963,
      "extraction_method": "Direct"
    },
    {
      "page_number": 213,
      "chapter": null,
      "content": "Using a Non-Relational Database\n190\n               self.date_approved.strftime(\"%m/%d/%Y\"),\n            \"status\": self.status,\n            \"level\": self.level,\n            \"login_id\": self.login_id,\n            \"books\": self.books\n        }\n        \n   @classmethod\n   def from_json(cls, json_str):\n        json_dict = json.loads(json_str)\n        return cls(**json_dict)\nThe EmbeddedDocument API is a Document without an id and has no collection of its own. \nSubclasses of this API are model classes that have been created to be part of a core document structure, \nsuch as UserProfile being part of the Login details. Now, the field that refers to this document \nhas a required property set to False since an embedded document can’t be present at all times.\nOn the other hand, a field declared as EmbeddedDocumentList is used to create a many-to-one \nassociation between documents. The preceding UserProfile class is strongly connected to a list of \nBookForSale embedded documents because of its declared booksale field. Again, the field type \nshould always set its required property to False to avoid problems when dealing with empty values.\nApplying custom serialization and deserialization\nThere are no built-in hooks for validation and serialization in this ODM. Every model class in the online \nbook reselling application has implemented a from_json() class method that converts JSON details \ninto a valid Document instance. When converting the BSON document into a JSON object, model \nclasses must have the custom to_json() instance method, which builds the JSON structure and \nautomatically transforms the BSON datetime into JSON-able date objects through formatting. \nLet us now create the repository layer using the model classes.\nImplementing the CRUD transactions\nMongoEngine provides the most convenient and straightforward approach to building the repository \nlayer for the application. All its operations come from the Document model class and they are easy \nto use. LoginRepository uses the ODM to implement its CRUD transactions:\nfrom typing import Dict, Any\nfrom models.data.mongoengine import Login\nclass LoginRepository:",
      "content_length": 2140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 214,
      "chapter": null,
      "content": "Implementing CRUD transactions using MongoEngine\n191\n    def insert_login(self, details:Dict[str, Any]) -> bool: \n        try:\n            login = Login(**details)\n            login.save()\n        except Exception as e:\n            print(e)\n            return False \n        return True\n    \n    def update_password(self, id:int, newpass:str) -> bool: \n       try:\n          login = Login.objects(id=id).get()\n          login.update(password=newpass)\n       except: \n           return False \n       return True\n   \n    def delete_login(self, id:int) -> bool: \n        try:\n            login = Login.objects(id=id).get()\n            login.delete()\n        except: \n            return False \n        return True\nIt only takes two lines for the insert_login() method to save the Login document. After creating \nthe Login instance with the necessary document details, we simply call the save() method of the \nDocument instance to pursue the insert transaction. When it comes to modifying some document \nvalues, the Document API class has an update() method that manages changes in state for every \nclass attribute. But first, we need to find the document using the objects() utility method, which \nretrieves document structures from the collection. This objects() method can fetch a document \nby providing its parameter with an id field value or extracting a list of document records by supplying \nthe method with a generic search expression. The instance of the retrieved document must invoke \nits update() method to pursue the modification of some, if not all, of its field values. The given \nupdate_password() method updates the password field of Login, which gives us a good template \nregarding how to pursue update operations on other field attributes.",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 215,
      "chapter": null,
      "content": "Using a Non-Relational Database\n192\nOn the other hand, delete_login() shows how to delete a Login document from its collection \nafter it searches for the object using a simple call to the instance’s delete() method. The following \nscript shows how to perform query transactions in MongoEngine:\n    def get_all_login(self):\n        login = Login.objects()\n        login_list = [l.to_json() for l in login]\n        return login_list\n    \n    def get_login(self, id:int): \n        login = Login.objects(id=id).get()\n        return login.to_json()\nThe only way to perform single- or multiple-document retrieval is to utilize the objects() method. \nThere is no need to implement JSON converters for the query results because every Document model \nclass has a to_json() method to provide the JSON-able equivalent of the instance. The given \nget_all_login() transaction uses list comprehension to create a list of JSON documents from \nthe result of objects(), while the get_login() method invokes to_json() after extracting \na single document.\nManaging the embedded documents\nIt is easier to implement document associations with an ODM than the core PyMongo and Motor \ndatabase drivers. Since the operations of MongoEngine are comfortable to use, it takes only a few \nlines to manage the embedded documents. In the following UserProfileRepository script, \ninsert_profile() shows how adding a UserProfile detail to the Login document can be \ndone by performing a simple object search and an update() call:\nfrom typing import Dict, Any\nfrom models.data.mongoengine import Login, UserProfile, \n      BookForSale\nclass UserProfileRepository(): \n    \n    def insert_profile(self, login_id:int, \n             details:Dict[str, Any]) -> bool: \n        try:\n            profile = UserProfile(**details)\n            login = Login.objects(id=login_id).get()\n            login.update(profile=profile)",
      "content_length": 1882,
      "extraction_method": "Direct"
    },
    {
      "page_number": 216,
      "chapter": null,
      "content": "Implementing CRUD transactions using MongoEngine\n193\n        except Exception as e:\n            print(e)\n            return False \n        return True\n    \n    def add_book_sale(self, login_id:int, \n             details:Dict[str, Any]): \n        try:\n            sale = BookForSale(**details)\n            login = Login.objects(id=login_id).get()\n            login.profile.booksale.append(sale)         \n            login.update(profile=login.profile)\n        except Exception as e:\n            print(e)\n            return False \n        return True\nLikewise, the given add_book_sale() transaction creates a many-to-one association between \nBookForSale and UserProfile using the same approach applied in insert_profile() \nwith an additional List's append() operation.\nQuerying the embedded documents is also feasible in MongoEngine. The ODM has a filter() \nmethod that uses field lookup syntax to refer to a specific document structure or list of embedded \ndocuments. This field lookup syntax consists of the field name of the embedded document, followed by \na double underscore in place of the dot in the usual object attribute access syntax. Then, it has another \ndouble underscore to cater to some operators, such as lt, gt, eq, and exists. In the following \ncode, get_all_profile()uses the profile__login_id__exists=True field lookup \nto filter all user_profile embedded documents that have valid login structures. However, the \nget_profile() transaction does not need to use filter() and field lookups because it can \nsimply access the specific login document to fetch its profile details:\n     def get_all_profile(self):\n        profiles = Login.objects.filter(\n               profile__login_id__exists=True)\n        profiles_dict = list(\n              map(lambda h: h.profile.to_json(), \n                Login.objects().filter(\n                    profile__login_id__exists=True)))\n        return profiles_dict",
      "content_length": 1916,
      "extraction_method": "Direct"
    },
    {
      "page_number": 217,
      "chapter": null,
      "content": "Using a Non-Relational Database\n194\n    def get_profile(self, login_id:int): \n        login = Login.objects(id=login_id).get()\n        profile = login.profile.to_json()\n        return profile\nThe preceding query transactions are just simple implementations compared to some other complex \nMongoEngine queries, which involve complicated embedded document structures that require complex \nfield lookup syntax. Let us now apply the CRUD transactions to our API services.\nRunning the CRUD transactions\nCRUD will not work without passing our create_db() method to the startup event and disconnect_\ndb() to the shutdown event. The former will open the MongoDB connection during the Uvicorn \nstartup, while the latter will close it during server shutdown.\nThe following script shows the application’s profile router with a create_profile() REST \nservice that asks clients for a profile detail, given a specific login record, and pursues the insert \ntransaction using UserProfileRepository: \nfrom fastapi import APIRouter, Depends\nfrom fastapi.responses import JSONResponse\nfrom models.request.profile import UserProfileReq, \n         BookForSaleReq\nfrom repository.mongoengine.profile import \n         UserProfileRepository\nfrom db_config.mongoengine_config import create_db\nrouter = APIRouter()\n@router.post(\"/profile/login/add\", \n      dependencies=[Depends(create_db)])\ndef create_profile(login_id:int, req:UserProfileReq): \n    profile_dict = req.dict(exclude_unset=True)\n    repo:UserProfileRepository = UserProfileRepository()\n    result = repo.insert_profile(login_id, profile_dict)\n    if result == True: \n        return req \n    else:",
      "content_length": 1636,
      "extraction_method": "Direct"
    },
    {
      "page_number": 218,
      "chapter": null,
      "content": "Implementing async transactions using Beanie\n195\n        return JSONResponse(content={\"message\": \n          \"insert profile unsuccessful\"}, status_code=500) \ncreate_profile() is a standard API service that deals with MongoEngine’s synchronous \ninsert_profile() transaction. When it comes to asynchronous REST services, it is not advisable \nto use MongoEngine because its platform only works with synchronous ones. In the next section, we \nwill discuss an ODM that’s popular in building an asynchronous repository layer.\nImplementing async transactions using Beanie\nBeanie is a non-boilerplate mapper that utilizes the core features of Motor and Pydantic. This ODM \noffers a more straightforward approach to implementing asynchronous CRUD transactions than its \nprecursor, the Motor driver.\nTo use Beanie, we need to install it using the following pip command:\npip install beanie\nImportant note\nInstalling Beanie may uninstall the current version of your Motor module because it sometimes \nrequires lower version of Motor module. Pursuing this will produce errors in your existing \nMotor transactions.\nCreating the database connection\nBeanie uses the Motor driver to open a database connection to MongoDB. Instantiating the Motor’s \nAsyncIOMotorClient class with the database URL is the first step of configuring it. But what \nmakes Beanie unique compared to other ODMs is how it pre-initializes and pre-recognizes the model \nclasses that will be involved in a CRUD transaction. The ODM has an asynchronous init_beanie() \nhelper method that initiates the model class initialization using the database name. Calling this \nmethod will also set up the collection-domain mapping, where all the model classes are registered in \nthe document_models parameter of init_beanie(). The following script shows the database \nconfiguration that’s required to access our MongoDB database, obrs:\nfrom motor.motor_asyncio import AsyncIOMotorClient\nfrom beanie import init_beanie\nfrom models.data.beanie import Cart, Order, Receipt\nasync def db_connect():\n    global client\n    client =",
      "content_length": 2067,
      "extraction_method": "Direct"
    },
    {
      "page_number": 219,
      "chapter": null,
      "content": "Using a Non-Relational Database\n196\n     AsyncIOMotorClient(f\"mongodb://localhost:27017/obrs\")\n    await init_beanie(client.obrs, \n         document_models=[Cart, Order, Receipt])\n    \nasync def db_disconnect():\n     client.close()\nHere, db_connect() uses an async/await expression because its method invocation to init_\nbeanie() is asynchronous. db_disconnect() will close the database connection by calling the \nclose() method of the AsyncIOMotorClient instance. Both of these methods are executed \nas events, just like in MongoEngine. Their implementation can be found in the /db_config/\nbeanie_config.py script file. Let us now create the model classes.\nDefining the model classes\nThe Beanie ODM has a Document API class that’s responsible for defining its model classes, mapping \nthem to MongoDB collections, and handling repository transactions, just like in MongoEngine. \nAlthough there is no Field directive for defining class attributes, the ODM supports Pydantic’s \nvalidation and parsing rules and typing extension for declaring models and their attributes. But \nit also has built-in validation and encoding features, which can be used together with Pydantic. The \nfollowing script shows how to define Beanie model classes while it’s being configured:\nfrom typing import Optional, List\nfrom beanie import Document\nfrom bson import datetime \nclass Cart(Document):\n    id: int \n    book_id: int \n    user_id: int\n    qty: int\n    date_carted: datetime.datetime\n    discount: float\n \n    class Collection:\n        name = \"cart\"\n    … … … … … …\n                \nclass Order(Document):",
      "content_length": 1591,
      "extraction_method": "Direct"
    },
    {
      "page_number": 220,
      "chapter": null,
      "content": "Implementing async transactions using Beanie\n197\n    id: int \n    user_id: int\n    date_ordered: datetime.datetime\n    orders: List[Cart] = list()\n        \n    class Collection:\n        name = \"order\"\n    … … … … … …\n        \nclass Receipt(Document): \n    id: int \n    date_receipt: datetime.datetime \n    total: float \n    payment_mode: int\n    order: Optional[Order] = None\n    \n    class Collection:\n        name = \"receipt\"\n    class Settings:\n        use_cache = True\n        cache_expiration_time =    \n             datetime.timedelta(seconds=10)\n        cache_capacity = 10\nThe id attribute of the given Document classes automatically translates into an _id value. This \nserves as the primary key of the document. Beanie allows you to replace the default ObjectId type \nof _id with another type, such as int, which is not possible in other ODMs. And with Motor, this \nODM needs custom JSON serializers because it has difficulty converting BSON datetime types \ninto Python datetime.date types during CRUD transactions.\nA document in Beanie can be configured by adding the Collection and Settings nested classes. \nThe Collection class can replace the default name of the collection where the model is supposed \nto be mapped. It can also provide indexes to document fields if needed. The Settings inner \nclass, on the other hand, can override existing BSON encoders, apply caching, manage concurrent \nupdates, and add validation when the document is being saved. These three model classes include \nthe collection configuration in their definitions to replace the names of their respective collections \nwith their class names.",
      "content_length": 1629,
      "extraction_method": "Direct"
    },
    {
      "page_number": 221,
      "chapter": null,
      "content": "Using a Non-Relational Database\n198\nCreating the document associations\nPython syntax, Pydantic rules, and API classes are used to establish links between documents in this \nmapper. To create a one-to-one association between Order and Receipt, for instance, we only \nneed to set an Order field attribute that will link to a single Receipt instance. For many-to-one \nassociations, such as the relationship between Order and Cart, the Cart document should only \nneed a list field that will contain all the Order embedded documents. \nHowever, the ODM has a Link type, which can be used to define class fields to generate these \nassociations. Its CRUD operations, such as save(), insert(), and update(), strongly support \nthese Link types, so long as the link_rule parameter is provided in their parameters. For query \ntransactions, the find() method can include the Link documents during document fetching, \ngiven that its fetch_links parameter is set to True. Now, let us implement the repository layer \nusing the model classes.\nImplementing the CRUD transactions\nImplementing repositories with Beanie is similar to how it’s done with MongoEngine – that is, it uses \nshort and direct CRUD syntax due to the convenient helper methods like create(), update(), and \ndelete(), provided by the Document API class. However, the Beanie mapper creates an asynchronous \nrepository layer because all the API methods that are inherited by the model classes are non-blocking. \nThe following code for the CartRepository class shows a sample implementation of an asynchronous \nrepository class using this Beanie ODM:\nfrom typing import Dict, Any\nfrom models.data.beanie import Cart\nclass CartRepository: \n    \n    async def add_item(self, \n             details:Dict[str, Any]) -> bool: \n        try:\n            receipt = Cart(**details)\n            await receipt.insert()\n        except Exception as e:\n            print(e)\n            return False \n        return True\n    \n    async def update_qty(self, id:int, qty:int) -> bool: \n       try:",
      "content_length": 2028,
      "extraction_method": "Direct"
    },
    {
      "page_number": 222,
      "chapter": null,
      "content": "Implementing async transactions using Beanie\n199\n          cart = await Cart.get(id)\n          await cart.set({Cart.qty:qty})\n       except: \n           return False \n       return True\n    \n    async def delete_item(self, id:int) -> bool: \n        try:\n            cart = await Cart.get(id)\n            await cart.delete()\n        except: \n            return False \n        return True\nThe add_item() method showcases the use of the asynchronous insert()method to persist a \nnewly created Cart instance. The Document API also has a create() method that works like \ninsert(). Another option is to use the insert_one() class method instead of the instance \nmethods. Moreover, adding multiple documents is allowed in this ODM because an insert_many() \noperation exists to pursue that kind of insert.\nUpdating a document can be initiated using two methods, namely set() and replace().\nupdate_qty() in the preceding script chooses the set() operation to update the current qty \nvalue of the items placed in a cart. \nWhen it comes to document removal, the ODM only has the delete() method to pursue the \ntransactions. This is present in the delete_item() transaction in the preceding code.\nRetrieving a single document or a list of documents using this ODM is easy. No further serialization \nand cursor wrapping is needed during its query operations. When fetching a single document structure, \nthe mapper provides the get() method if the fetching process only requires the _id field; it provides \nfind_one() when the fetching process requires a conditional expression. Moreover, Beanie has a \nfind_all() method that fetches all the documents without constraints and the find() method \nfor retrieving data with conditions. The following code shows the query transaction for retrieving \ncart items from the database:\nasync def get_cart_items(self):\n        return await Cart.find_all().to_list()\n    \n    async def get_items_user(self, user_id:int): \n        return await Cart.find(\n              Cart.user_id == user_id).to_list()",
      "content_length": 2025,
      "extraction_method": "Direct"
    },
    {
      "page_number": 223,
      "chapter": null,
      "content": "Using a Non-Relational Database\n200\n    \n    async def get_item(self, id:int): \n        return await Cart.get(id)\nBoth the find() and find_all() operations are used in the methods to return a FindMany \nobject that has a to_list() utility that returns a list of JSON-able documents. Let us now apply \nour CRUD transactions to the API services.\nRunning the repository transactions\nThe CartRepository methods will only run successfully if db_connect() from the configuration \nfile is injected into the router. Although injecting it into each API service is acceptable, our solution \nprefers injecting the component into APIRouter using Depends:\nfrom repository.beanie.cart import CartRepository\nfrom db_config.beanie_config import db_connect\nrouter = APIRouter(dependencies=[Depends(db_connect)])\n@router.post(\"/cart/add/item\")\nasync def add_cart_item(req:CartReq): \n    repo:CartRepository = CartRepository()\n    result = await repo.add_item(loads(cart_json))\n          \"insert cart unsuccessful\"}, status_code=500)\nThe asynchronous add_cart_item() service asynchronously inserts the cart account into the \ndatabase using CartRepository. \nAnother asynchronous mapper that can integrate perfectly with FastAPI is ODMantic.\nBuilding async repository for FastAPI using ODMantic\nThe dependencies of Beanie and ODMantic come from Motor and Pydantic. ODMantic also utilizes \nMotor’s AsyncIOMotorClient class to open a database connection. It also uses Pydantic features \nfor class attribute validation, Python’s typing extension for type hinting, and other Python components \nfor management. But its edge over Beanie is that it complies with ASGI frameworks such as FastAPI.\nTo pursue ODMantic, we need to install the extension using the following pip command:\npip install odmantic",
      "content_length": 1773,
      "extraction_method": "Direct"
    },
    {
      "page_number": 224,
      "chapter": null,
      "content": "Building async repository for FastAPI using ODMantic\n201\nCreating the database connection\nSetting up the database connectivity in ODMantic is the same as what we do with the Beanie mapper, \nexcept that the setup includes creating an engine that will handle all its CRUD operations. This engine \nis AIOEngine from the odmantic module, which requires both the motor client object and the \ndatabase name to be created successfully. The following is a complete implementation of the database \nconnectivity needed by the ODMantic mapper:\nfrom odmantic import AIOEngine\nfrom motor.motor_asyncio import AsyncIOMotorClient\ndef create_db_connection():\n   global client_od\n   client_od = \n     AsyncIOMotorClient(f\"mongodb://localhost:27017/\")\ndef create_db_engine():\n   engine = AIOEngine(motor_client=client_od, \n         database=\"obrs\")\n   return engine\ndef close_db_connection():\n    client_od.close() \nWe need to create event handlers in APIRouter to run create_db_connection() and \nclose_db_connection() for our repository transactions to work. Let us now implement the \nmodel layer of the ODM.\nCreating the model layer\nODMantic has a Model API class that provides properties to model classes when subclassed. It relies \non Python types and BSON specifications to define the class attributes. When transforming field types, \nsuch as converting a BSON datetime value into a Python datetime.date value, the mapper allows \nyou to add custom @validator methods into the model classes to implement the appropriate object \nserializer. Generally, ODMantic relies on the pydantic module when it comes to data validation, \nunlike in the Beanie mapper. The following is a standard ODMantic model class definition:\nfrom odmantic import Model\nfrom bson import datetime",
      "content_length": 1753,
      "extraction_method": "Direct"
    },
    {
      "page_number": 225,
      "chapter": null,
      "content": "Using a Non-Relational Database\n202\nclass Purchase(Model): \n    purchase_id: int\n    buyer_id: int \n    book_id: int \n    items: int \n    price: float \n    date_purchased: datetime.datetime\n    \n    class Config:\n        collection = \"purchase\"\nFor advanced configurations, we can add a nested Config class to the model class to set these additional \noptions, such as the collection option, which replaces the default name of the collection with a \ncustom one. We can also configure some familiar options, such as json_encoders, to convert one \nfield type into another supported one.\nEstablishing document association\nWhen creating associations, the typical Python approach of declaring fields so that they refer to \nan embedded document(s) is still applicable in this ODM. However, this ODM mapper has an \nEmbeddedModel API class to create a model with no _id field; this can be linked to another \ndocument. The Model classes, on the other hand, can define a field attribute that will refer to an \nEmbeddedModel class to establish a one-to-one association or a list of EmbeddedModel instances \nfor a many-to-one association.\nImplementing the CRUD transactions\nCreating the repository layer using ODMantic always requires the engine object that was created in \nthe startup event. This is because all the CRUD operations that are needed will come from this engine. \nThe following PurchaseRepository shows the operations from the AIOEngine object that we \nneed to create CRUD transactions:\nfrom typing import List, Dict, Any\nfrom models.data.odmantic import Purchase\nclass PurchaseRepository: \n    \n    def __init__(self, engine): \n        self.engine = engine",
      "content_length": 1658,
      "extraction_method": "Direct"
    },
    {
      "page_number": 226,
      "chapter": null,
      "content": "Building async repository for FastAPI using ODMantic\n203\n    async def insert_purchase(self, \n              details:Dict[str, Any]) -> bool: \n        try:\n           purchase = Purchase(**details)\n           await self.engine.save(purchase)\n                  \n        except Exception as e:\n            print(e)\n            return False \n        return True\nThis insert_purchase() method shows the standard way to insert a record into the database \nusing ODMantic. Through the engine’s save() method, we can persist one document at a time using \nthe model class. AIOEngine also provides the save_all() method for inserting a list of multiple \ndocuments into the associated MongoDB collection. \nNow, there is no specific way to update transactions, but ODMantic allows you to fetch the record that \nneeds to be updated. The following code can be used to update a record using ODMantic:\n    async def update_purchase(self, id:int, \n              details:Dict[str, Any]) -> bool: \n       try:\n          purchase = await self.engine.find_one(\n                Purchase, Purchase.purchase_id == id)\n                  \n          for key,value in details.items():\n            setattr(purchase,key,value)\n          \n          await self.engine.save(purchase)\n       except Exception as e:\n           print(e) \n           return False \n       return True",
      "content_length": 1344,
      "extraction_method": "Direct"
    },
    {
      "page_number": 227,
      "chapter": null,
      "content": "Using a Non-Relational Database\n204\nAfter accessing and changing the field values, the fetched document object will be re-saved using the \nsave() method to reflect the changes in physical storage. The complete process is implemented in \nthe preceding update_purchase() transaction:\n     async def delete_purchase(self, id:int) -> bool: \n        try:\n            purchase = await self.engine.find_one(\n                Purchase, Purchase.purchase_id == id) \n            await self.engine.delete(purchase)\n        except: \n            return False \n        return True\nWhen it comes to document removal, you must fetch the document to be deleted. We pass the \nfetched document object to the delete() method of the engine to pursue the removal process. \nThis implementation is shown in the delete_purchase() method.\nWhen fetching a single document so that it can be updated or deleted, AIOEngine has a find_\none() method that requires two arguments: the model class name and the conditional expression, \nwhich involves either the id primary key or some non-unique fields. All the fields can be accessed \nlike class variables. The following get_purchase() method retrieves a Purchase document \nwith the specified id:\n    async def get_all_purchase(self):\n        purchases = await self.engine.find(Purchase)\n        return purchases\n            \n    async def get_purchase(self, id:int): \n        purchase = await self.engine.find_one(\n            Purchase, Purchase.purchase_id == id) \n        return purchase\nThe engine has a find() operation to retrieve all Purchase documents, for instance, from the \ndatabase. It only needs an argument – the name of the model class. Let now apply our repository \nlayer to the API services.",
      "content_length": 1723,
      "extraction_method": "Direct"
    },
    {
      "page_number": 228,
      "chapter": null,
      "content": "Building async repository for FastAPI using ODMantic\n205\nRunning the CRUD transaction\nFor the repository classes to run, all the router services must be asynchronous. Then, we need to \ncreate the startup and shutdown event handlers for create_db_connection() and close_\ndb_connection(), respectively, to open the connection for repository transactions. Lastly, for \nthe repository class to work, create_db_engine() must be injected into each API service to \nderive the engine object:\nfrom fastapi import APIRouter, Depends\nfrom fastapi.responses import JSONResponse\nfrom models.request.purchase import PurchaseReq\nfrom repository.odmantic.purchase import PurchaseRepository\nfrom db_config.odmantic_config import create_db_engine, \n    create_db_connection, close_db_connection\nfrom datetime import date, datetime\nfrom json import dumps, loads\nrouter = APIRouter()\nrouter.add_event_handler(\"startup\", create_db_connection)\nrouter.add_event_handler(\"shutdown\", close_db_connection)\n@router.post(\"/purchase/add\")\nasync def add_purchase(req: PurchaseReq, \n          engine=Depends(create_db_engine)): \n     purchase_dict = req.dict(exclude_unset=True) \n     purchase_json = dumps(purchase_dict, \n                default=json_serial)\n     repo:PurchaseRepository = PurchaseRepository(engine)\n     result = await \n            repo.insert_purchase(loads(purchase_json))\n     if result == True: \n        return req \n     else: \n        return JSONResponse(content={\"message\":",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 229,
      "chapter": null,
      "content": "Using a Non-Relational Database\n206\n          \"insert purchase unsuccessful\"}, status_code=500)\n     return req\nAt this point, we should know how to compare these mappers and drivers when it comes to the setup \nand procedures needed to manage MongoDB documents. Each has its strengths and weaknesses based \non the code they produce and the performance, popularity, support, and complexity of its solution. \nSome may work on other requirements, while others may not. The final ODM we will cover focuses \non being the lightest and least obtrusive mapper. It aims to fit into an existing application without \ngenerating syntax and performance problems. \nCreating CRUD transactions using MongoFrames\nIf you are tired of using complicated and heavy-loaded ODMs, then MongoFrames is ideal for your \nrequirements. MongoFrames is one of the newest ODMs and is very convenient to use, especially \nwhen building a new repository layer for an already existing complex and legacy FastAPI microservice \napplication. But this mapper can only create synchronous and standard types of CRUD transactions.\nBut before we proceed, let us install the extension module using pip:\npip install MongoFrames\nCreating the database connection\nThe MongoFrames platform runs on top of PyMongo, which is why it cannot build an asynchronous \nrepository layer. To create the database connection, it uses the MongoClient API class from the \npymongo module, with the database URL in string format. Unlike in the other ODMs, where we \ncreate a client variable, in this mapper, we access the variable _client class from the Frame \nAPI class to refer to the client connection object. The following code shows create_db_client(), \nwhich will open the database connection for our app, and disconnect_db_client(), which \nwill close this connection:\nfrom pymongo import MongoClient\nfrom mongoframes import Frame\ndef create_db_client():\n    Frame._client = \n        MongoClient('mongodb://localhost:27017/obrs')\n        \ndef disconnect_db_client():\n    Frame._client.close()",
      "content_length": 2031,
      "extraction_method": "Direct"
    },
    {
      "page_number": 230,
      "chapter": null,
      "content": "Creating CRUD transactions using MongoFrames\n207\nJust like in the previous ODMs, we need event handlers to execute these core methods to start building \nthe model and repository layers.\nBuilding the model layer\nThe process of creating model classes in MongoFrames is called framing because it uses the Frame \nAPI class to define the model classes. Once inherited, Frame does not require a model class to define \nits attributes. It uses the _fields property to contain all the necessary fields of the document without \nindicating any metadata. The following model classes are defined by the Frame API class:\nfrom mongoframes import Frame, SubFrame\nclass Book(Frame):\n    _fields = {\n        'id ',\n        'isbn',\n        'author', \n        'date_published', \n        'title', \n        'edition',\n        'price',\n        'category'\n    }\n    _collection = \"book\"\n    \n    \nclass Category(SubFrame):\n    \n    _fields = {\n        'id',\n        'name',\n        'description',\n        'date_added'\n        }\n    \n    _collection = \"category\"\nclass Reference(Frame):",
      "content_length": 1061,
      "extraction_method": "Direct"
    },
    {
      "page_number": 231,
      "chapter": null,
      "content": "Using a Non-Relational Database\n208\n    _fields = {\n        'id',\n        'name',\n        'description',\n        'categories'\n        }\n    \n    _collection = \"reference\"\nA Frame model class can wrap a document in dictionary form or in a kwargs that contains the \nkey-value details of the document’s structure. It can also provide attributes and helper methods that \ncan help pursue CRUD transactions. All the fields of the model class can be accessed through dot (.) \nnotation, just like typical class variables.\nCreating the document association\nWe need to define the SubFrame model before creating associations among these documents. A \nSubFrame model class is mapped to an embedded document structure and has no collection table \nof its own. The MongoFrames mapper provides operations that allow you to append, update, remove, \nand query the SubFrame class of the Frame instance. These operations will determine the type of \nassociation among documents since the field references of Frame do not have specific field types. \nThe Reference document, for instance, will have a list of categories linked to its categories \nfield because our transaction will build that association as designed. A Book document, on the other \nhand, will refer to a Category child document through its category field because a transaction \nwill build that association at runtime. So, MongoFrames is both restrained and non-strict when it \ncomes to defining the type of association among these documents.\nCreating the repository layer\nThe Frame API class provides the model classes and the necessary helper methods to implement the \nasynchronous repository transactions. The following code shows an implementation of a repository \nclass that uses MongoFrames to create its CRUD transactions:\nfrom mongoframes.factory.makers import Q\nfrom models.data.mongoframe import Book, Category\nfrom typing import List, Dict, Any\nclass BookRepository: \n    def insert_book(self, \n             details:Dict[str, Any]) -> bool:",
      "content_length": 1993,
      "extraction_method": "Direct"
    },
    {
      "page_number": 232,
      "chapter": null,
      "content": "Creating CRUD transactions using MongoFrames\n209\n        try:\n           book = Book(**details)\n           book.insert()\n                  \n        except Exception as e:\n            return False \n        return True\nThe given insert_book() transaction inserts a book instance into its mapped collection. The \nFrame API provides an insert() method that saves the given model object into the database. \nIt also has insert_many(), which inserts a list of multiple BSON documents or a list of model \ninstances. The following script shows how to create an UPDATE transaction in MongoFrames:\n    def update_book(self, id:int, \n            details:Dict[str, Any]) -> bool: \n       try:\n        book = Book.one(Q.id == id)\n        for key,value in details.items():\n            setattr(book,key,value)\n        book.update()\n       except: \n           return False \n       return True\nThe given update_book() transaction shows that the Frame model class also has an update() \nmethod, which recognizes and saves the changes reflected in the field values of a document object \nright after fetching them from the collection. A similar process is applied to the delete_book() \nprocess, which calls the delete() operation of the document object right after fetching it from \nthe collection:\n    def delete_book(self, id:int) -> bool: \n        try:\n           book = Book.one(Q.id == id)\n           book.delete()\n        except: \n            return False \n        return True",
      "content_length": 1460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 233,
      "chapter": null,
      "content": "Using a Non-Relational Database\n210\nWhen creating query transactions, the Frame API provides two class methods – the many() method, \nwhich extracts all BSON documents, and the one() method, which returns a single document object. \nBoth operations can accept a query expression as an argument if there are any constraints. Moreover, \nMongoFrames has a Q query maker class that’s used to build conditionals in a query expression. The \nexpression starts with Q, followed by dot (.) notation to define the field name or path – for example, \nQ.categories.fiction – followed by an operator (for example, ==, !=, >, >=, <, or <=) and \nfinally a value. The following code shows examples of the query transactions being translated using \nthe MongoFrames ODM syntax:\n    def get_all_book(self):\n        books = [b.to_json_type() for b in Book.many()]\n        return books\n    \n    def get_book(self, id:int): \n        book = Book.one(Q.id == id).to_json_type()\n        return book\nThe get_book() method shows how to extract a single Book document with a Q expression that \nfilters the correct id, while get_all_book() retrieves all Book documents without any constraints. \nThe many() operator returns a list of Frame objects, while the one() operator returns a single \nFrame instance. To convert the result into JSON-able components, we need to invoke the to_json_\ntype() method in each Frame instance.\nAs explained earlier, adding embedded documents is determined by the operation and not by the \nmodel attributes. In the following add_category() transaction, it is clear that a Category object \nhas been assigned to a category field of a Book instance, even if the field is not defined to refer \nto an embedded document of the Category type. Instead of throwing an exception, MongoFrame \nwill update the Book document right after the update() call:\n    def add_category(self, id:int, \n               category:Category) -> bool: \n       try:\n        book = Book.one(Q.id == id)\n        book.category = category\n        book.update()\n       except: \n           return False \n       return True\nNow, it is time to apply these CRUD transactions to our API services.",
      "content_length": 2153,
      "extraction_method": "Direct"
    },
    {
      "page_number": 234,
      "chapter": null,
      "content": "Creating CRUD transactions using MongoFrames\n211\nApplying the repository layer\nOur repository classes will not work if we do not inject the create_db_client() injectable into \nthe router. The following solution injects the component into APIRouter, even if it is acceptable to \ninject it into each API service implementation:\nfrom fastapi import APIRouter, Depends\nfrom fastapi.responses import JSONResponse\nfrom models.request.category import BookReq\nfrom repository.mongoframe.book import BookRepository\nfrom db_config.mongoframe_config import create_db_client\nfrom datetime import date, datetime\nfrom json import dumps, loads\nrouter = APIRouter(\n         dependencies=[Depends(create_db_client)])\n@router.post(\"/book/create\")\ndef create_book(req:BookReq): \n    book_dict = req.dict(exclude_unset=True) \n    book_json = dumps(book_dict, default=json_serial)\n    repo:BookRepository = BookRepository()\n    result = repo.insert_book(loads(book_json))\n    if result == True: \n        return req \n    else: \n        return JSONResponse(content={\"message\": \n          \"insert book unsuccessful\"}, status_code=500)\nThe create_book() service uses BookRepository to insert book details into the MongoDB \ndatabase. In general, MongoFrames has an easy setup because it requires fewer configuration details \nfor creating the database connection, building the model layer, and implementing the repository \ntransactions. Its platform can be adapted to the existing requirements of the application and can easily \nreflect changes if modifications need to be made to its mapping mechanisms.",
      "content_length": 1577,
      "extraction_method": "Direct"
    },
    {
      "page_number": 235,
      "chapter": null,
      "content": "Using a Non-Relational Database\n212\nSummary\nIn this chapter, we looked at various ways to manage data using MongoDB. We utilized MongoDB \nto store non-relational data for our online book reselling system since we expect the data to become \nlarge when information is exchanged between the book buyers and resellers. Additionally, the details \ninvolved in the transactions are mainly strings, floats, and integers, which are all order and purchase \nvalues that will be easier to mine and analyze if they’re stored in schema-less storage. \nThis chapter took the non-relational data management roadmap for utilizing the data in sales forecasting, \nregression analysis of book readers’ demands, and other descriptive data analysis forms.\nFirst, you learned how the PyMongo and Motor drivers connect the FastAPI application to the \nMongoDB database. After understanding the nuts and bolts of creating CRUD transactions using these \ndrivers, you learned that ODM is the better option for pursuing MongoDB connectivity. We explored \nthe features of MongoEngine, Beanie, ODMantic, and MongoFrames and studied their strengths and \nweaknesses as ODM mappers. All these ODMs can be integrated well with the FastAPI platform and \nprovide the application with a standardized way to back up data.\nNow that we’ve spent two chapters covering data management, in the next chapter, we will learn how \nto secure our FastAPI microservice applications.",
      "content_length": 1430,
      "extraction_method": "Direct"
    },
    {
      "page_number": 236,
      "chapter": null,
      "content": "7\nSecuring the REST APIs\nBuilding microservices means exposing the entire application to the worldwide web. For every request-\nresponse transaction, the client accesses the endpoint of the API publicly, which poses potential risks \nto the application. Unlike web-based applications, API services have weak mechanisms to manage user \naccess using login controls. Thus, this chapter will provide several ways to protect the API services \ncreated using the FastAPI framework. \nThere is no such thing as perfect security. The main goal is to establish policies and solutions related \nto the confidentiality, integrity, and availability of these services. The confidentiality policy requires \ntokens, encryption and decryption, and certificates as mechanisms to make some APIs private. On \nthe other hand, the integrity policy involves maintaining the data exchange as authentic, accurate, \nand reliable by using a \"state\" and hashed codes during the authentication and authorization process. \nThe availability policy means protecting the endpoint access from DoS attacks, phishing, and timing \nattacks using reliable tools and Python modules. Overall, these three aspects of the security model are \nthe essential elements to consider when building security solutions for our microservices.\nAlthough FastAPI has no built-in security framework, it supports different authentication modes \nsuch as Basic and Digest. It also has built-in modules that implement security specifications such as \nOAuth2, OpenID, and OpenAPI. The following main topics will be covered in this chapter to explain \nand illustrate the concepts and solutions for securing our FastAPI services:\n•\t Implementing Basic and Digest authentication\n•\t Implementing password-based authentication\n•\t Applying JWTs\n•\t Creating scope-based authorization\n•\t Building the authorization code flow\n•\t Applying the OpenID Connect specification\n•\t Using built-in middleware for authentication",
      "content_length": 1943,
      "extraction_method": "Direct"
    },
    {
      "page_number": 237,
      "chapter": null,
      "content": "Securing the REST APIs\n214\nTechnical requirements\nThe software prototype for this chapter is a secure online auction system designed to manage online \nbidding on various items auctioned by its registered users. The system can bid on any items based within \na price range and even declare those who won the bidding. The system needs to secure some sensitive \ntransactions to avoid data breaches and biased results. The prototype will be using SQLAlchemy as the \nORM for managing data. There will be 10 versions of our prototype and each will showcase a different \nauthentication scheme. All 10 of these projects (ch07a to ch07j) can be found here: https://\ngithub.com/PacktPublishing/Building-Python-Microservices-with-FastAPI.\nImplementing Basic and Digest authentication\nThe Basic and Digest authentication schemes are the easiest authentication solutions that we can use \nto secure API endpoints. Both schemes are alternative authentication mechanisms that can be applied \nto small and low-risk applications without requiring complex configuration and coding. Let us now \nuse these schemes to secure our prototype.\nUsing Basic authentication\nThe most straightforward way to secure the API endpoint is the Basic authentication approach. However, \nthis authentication mechanism must not be applied to high-risk applications because the credentials, \ncommonly a username and password, sent from the client to the security scheme provider are in the \nBase64-encoded format, which is vulnerable to many attacks such as brute force, timing attacks, and \nsniffing. Base64 is not an encryption algorithm but simply a way of representing the credentials in \nciphertext format.\nApplying HttpBasic and HttpBasicCredentials\nThe prototype, ch07a, uses the Basic authentication mode to secure its administration and bidding \nand auctioning transactions. Its implementation in the /security/secure.py module is shown \nin the following code:\nfrom passlib.context import CryptContext\nfrom fastapi.security import HTTPBasicCredentials\nfrom fastapi.security import HTTPBasic\nfrom secrets import compare_digest\nfrom models.data.sqlalchemy_models import Login\ncrypt_context = CryptContext(schemes=[\"sha256_crypt\", \n                    \"md5_crypt\"])",
      "content_length": 2229,
      "extraction_method": "Direct"
    },
    {
      "page_number": 238,
      "chapter": null,
      "content": "Implementing Basic and Digest authentication\n215\nhttp_basic = HTTPBasic()\nThe FastAPI framework supports different authentication modes and specifications through its \nfastapi.security module. To pursue the Basic authentication scheme, we need to instantiate \nthe HTTPBasic class of the module and inject it into each API service to secure the endpoint access. \nThe http_basic instance, once injected into the API services, causes the browser to pop up a login \nform, through which we type the username and password credentials. Logging in will trigger \nthe browser to send a header with the credentials to the application. If the application encounters \na problem with receiving it, the HTTPBasic scheme will throw an HTTP status code 401 with an \n\"Unauthorized\" message. If there are no errors in the form handling, the application must receive a \nWWW-Authenticate header with a Basic value and an optional realm parameter.\nOn the other hand, the /ch07/login service will call the authentication()method to verify \nwhether the browser credentials are authentic and correct. We need to be very careful in accepting \nuser credentials from browsers since they are prone to various attacks. First, we can require endpoint \nusers to use an email address as their username and require long passwords with a combination of \ndifferent characters, numbers, and symbols. All stored passwords must be encoded using the most \nreliable encryption tools, such as the CryptContext class from the passlib module. The \npasslib extension provides more secured hashing algorithms than any Python encryption module. \nOur application uses SHA256 and MD5 hashing algorithms instead of the recommended bcrypt, \nwhich is slower and prone to attacks.\nSecond, we can avoid storing the credentials in the source code and use database storage or a.env \nfile instead. The authenticate() method checks the credentials against the Login database \nrecord provided by the API service for correctness. \nLastly, always use the compare_digest() from the secret module when comparing credentials \nfrom the browser with the Login credentials stored in the database. This function randomly compares \ntwo strings while guarding the operation against timing attacks. A timing attack is a kind of attack \nthat compromises the crypto-algorithm execution, which happens when there is a linear comparison \nof strings in the system:\ndef verify_password(plain_password, hashed_password):\n    return crypt_context.verify(plain_password, \n        hashed_password)\ndef authenticate(credentials: HTTPBasicCredentials, \n         account:Login):\n    try:\n        is_username = compare_digest(credentials.username,\n             account.username)",
      "content_length": 2693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 239,
      "chapter": null,
      "content": "Securing the REST APIs\n216\n        is_password = compare_digest(credentials.password, \n             account.username)\n        verified_password = \n             verify_password(credentials.password, \n                   account.passphrase)\n        return (verified_password and is_username and \n               is_password)\n    except Exception as e:\n        return False\nOur authenticate() method has all the needed requirements to help reduce attacks from outside \nfactors. But the ultimate solution to secure Basic authentication is to install and configure a Transport \nLayer Security (TLS) (or HTTPS, or SSL) connection for the application. \nNow, we need to implement a /ch07/login endpoint to apply the Basic authentication scheme. The \nhttp_basic instance is injected into this API service to extract HTTPBasicCredentials, which \nis the object that contains the username and password details from the browser. This service is also the \none that calls the authenticate() method to check the user credentials. If the method returns a \nFalse value, the service will raise an HTTP status code 400 with an \"Incorrect credentials\" message:\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom fastapi.security import HTTPBasicCredentials\nfrom security.secure import authenticate, \n            get_password_hash, http_basic\nrouter = APIRouter()\n@router.get(\"/login\")\ndef login(credentials: HTTPBasicCredentials = \n     Depends(http_basic), sess:Session = Depends(sess_db)):\n    \n    loginrepo = LoginRepository(sess)\n    account = loginrepo.get_all_login_username(\n                     credentials.username)\n    if authenticate(credentials, account) and \n            not account == None:\n        return account",
      "content_length": 1713,
      "extraction_method": "Direct"
    },
    {
      "page_number": 240,
      "chapter": null,
      "content": "Implementing Basic and Digest authentication\n217\n    else:\n        raise HTTPException(\n            status_code=400, \n               detail=\"Incorrect credentials\")\n        \n@router.get(\"/login/users/list\")\ndef list_all_login(credentials: HTTPBasicCredentials = \n     Depends(http_basic), sess:Session = Depends(sess_db)):\n    loginrepo = LoginRepository(sess)\n    users = loginrepo.get_all_login()\n    return jsonable_encoder(users)\nEach endpoint of the online auction system must have the injected http_basic instance to secure \nit from public access. For instance, the cited list_all_login() service can only return a list of \nall users if the user is an authenticated one. By the way, there is no reliable procedure to log off using \nBasic authentication. If the WWW-Authenticate header has been issued and recognized by the \nbrowser, we will seldom see the login form of the browser pop up.\nExecuting the login transaction\nWe can use either the curl command or the browser to perform the /ch07/login transaction. \nBut to highlight the support of FastAPI, we will be using its OpenAPI dashboard to run /ch07/\nlogin. After accessing http://localhost:8000/docs on the browser, locate the /ch07/\nlogin GET transaction and click the Try it out button. The browser’s login form, as shown in Figure \n7.1, will pop up after clicking the button:\nFigure 7.1 – The browser’s login form",
      "content_length": 1379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 241,
      "chapter": null,
      "content": "Securing the REST APIs\n218\nAfter the Username and Password input, click the Sign in button on the login form to check whether \nthe credentials are in the database. Otherwise, the app has /ch07/signup/add and /ch07/\napprove/signup to add the user credentials you want to test. Remember that all stored passwords \nare encrypted. Figure 7.2 shows how /ch07/login will output the user’s Login record after the \nauthentication process finds that the user credentials are valid:\nFigure 7.2 – The /login response\nNow that the user is authenticated, run /ch07/login/users/list through the OpenAPI \ndashboard to retrieve the list of login details. The uvicorn server log will show the following \nlog message:\nINFO: 127.0.0.1:53150 - \"GET /ch07/login/users/list HTTP/1.1\" \n200 OK\nThis means that the user is authorized to run the endpoint. Now, let us apply the Digest authentication \nscheme to our prototype.\nUsing Digest authentication\nDigest authentication is more secure than the Basic scheme because the former needs to hash the \nuser credentials first before sending the hashed version to the application. Digest authentication in \nFastAPI does not include an automatic encryption process of user credentials using the default MD5 \nencryption. It is an authentication scheme that stores credentials in a .env or .config property \nfile and creates a hashed string value for these credentials before the authentication. The ch07b \nproject applies the Digest authentication scheme to secure the bidding and auctioning transactions.",
      "content_length": 1524,
      "extraction_method": "Direct"
    },
    {
      "page_number": 242,
      "chapter": null,
      "content": "Implementing Basic and Digest authentication\n219\nGenerating the hashed credentials\nSo, before we start the implementation, we first need to create a custom utility script, generate_\nhash.py, that generates a digest in binary form using Base64 encoding. The script must have the \nfollowing code:\nfrom base64 import urlsafe_b64encode\nh = urlsafe_b64encode(b\"sjctrags:sjctrags\")\nThe urlsafe_b64encode() function from the base64 module creates a digest in binary format \nfrom the username:password credential format. After running the script, we save the digest value \nanywhere safe, but not in the source code. \nPassing the user credentials\nAside from the digest, we also need to save the user credentials for the Digest scheme provider later. \nUnlike the standard Digest authentication procedure, where the user negotiates with the browser, \nFastAPI requires storing the user credentials in a.env or .config file inside our application to be \nretrieved by the authentication process. In the ch07b project, we save the username and password \ninside the .config file, in this manner:\n[CREDENTIALS]\nUSERNAME=sjctrags\nPASSWORD=sjctrags\nThen, we create a parser through the ConfigParser utility to extract the following details from \nthe .config file and build a dict out of the serialized user details. The following build_map() \nis an example of the parser implementation:\nimport os\nfrom configparser import ConfigParser\ndef build_map():\n    env = os.getenv(\"ENV\", \".config\")\n    if env == \".config\":\n        config = ConfigParser()\n        config.read(\".config\")\n        config = config[\"CREDENTIALS\"]\n    else:\n        config = {\n            \"USERNAME\": os.getenv(\"USERNAME\", \"guest\"),",
      "content_length": 1682,
      "extraction_method": "Direct"
    },
    {
      "page_number": 243,
      "chapter": null,
      "content": "Securing the REST APIs\n220\n            \"PASSWORD\": os.getenv(\"PASSWORD\", \"guest\"),\n        }\n    return config\nUsing HTTPDigest and HTTPAuthorizationCredentials\nThe FastAPI framework has an HTTPDigest from its fastapi.security module that implements \na Digest authentication scheme with a different approach to managing user credentials and generating \nthe digest. Unlike in Basic authentication, the HTTPDigest authentication process happens at the \nAPIRouter level. We inject the following authenticate()dependable into the API services \nthrough the HTTP operator, including /login, where the authentication starts:\nfrom fastapi import Security, HTTPException, status\nfrom fastapi.security import HTTPAuthorizationCredentials\nfrom fastapi.security import HTTPDigest\nfrom secrets import compare_digest\nfrom base64 import standard_b64encode\nhttp_digest = HTTPDigest()\ndef authenticate(credentials: \n    HTTPAuthorizationCredentials = Security(http_digest)):\n    \n    hashed_credentials = credentials.credentials\n    config = build_map()\n    expected_credentials = standard_b64encode(\n        bytes(f\"{config['USERNAME']}:{config['PASSWORD']}\",\n           encoding=\"UTF-8\")\n    )\n    is_credentials = compare_digest(\n          bytes(hashed_credentials, encoding=\"UTF-8\"),\n               expected_credentials)\n    \n    if not is_credentials:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect digest token\",",
      "content_length": 1467,
      "extraction_method": "Direct"
    },
    {
      "page_number": 244,
      "chapter": null,
      "content": "Implementing Basic and Digest authentication\n221\n            headers={\"WWW-Authenticate\": \"Digest\"},\n        )\nThe authenticate() method is where the http_digest is injected to extract the \nHTTPAuthorizationCredentials that contains the digest byte value. After extraction, it \nchecks whether the digest matches the credentials saved in the .config file. We also use compare_\ndigest to compare hashed_credentials from the header and the Base64-encoded credentials \nfrom the .config file. \nExecuting the login transaction\nAfter implementing the authenticate() method, we inject it into the API services, not in the \nmethod parameter, but in its HTTP operator. Notice that the http_digest object is not injected \ndirectly into the API services, unlike in the Basic authentication scheme. The following implementation \nshows how the authenticate() dependable is applied to secure all the crucial endpoints of the \napplication:\nfrom security.secure import authenticate\n@router.get(\"/login\", dependencies=[Depends(authenticate)])\ndef login(sess:Session = Depends(sess_db)):\n    return {\"success\": \"true\"}\n        \n@router.get(\"/login/users/list\",   \n      dependencies=[Depends(authenticate)])\ndef list_all_login(sess:Session = Depends(sess_db)):\n    loginrepo = LoginRepository(sess)\n    users = loginrepo.get_all_login()\n    return jsonable_encoder(users)\nSince the Digest authentication scheme behaves like the OpenID authentication, we will be using the \ncurl command to run /ch07/login. The crucial part of the command is the issuance of the \nAuthorization header with the value containing the Base64-encoded username:password digest \ngenerated by the generate_hash.py script we executed beforehand. The following curl command \nis the correct way of logging into our FastAPI application that uses the Digest authentication scheme:\ncurl --request GET --url http://localhost:8000/ch07/login \n--header \"accept: application/json\"                  --header \n\"Authorization: Digest c2pjdHJhZ3M6c2pjdHJhZ3M=\" --header \n\"Content-Type: application/json\"\nWe also use the same command to run the rest of the secured API services.",
      "content_length": 2118,
      "extraction_method": "Direct"
    },
    {
      "page_number": 245,
      "chapter": null,
      "content": "Securing the REST APIs\n222\nMost enterprise applications nowadays seldom use Basic and Digest authentication schemes because \nof their vulnerability to many attacks. More than that, both authentication schemes require sending \ncredentials to the secured API services, which is also another risk. Moreover, at the time of writing, \nFastAPI does not yet fully support the standard Digest authentication, which is also a disadvantage \nto other applications that need the standard one. So, let us now explore the solution to secure API \nendpoints using the OAuth 2.0 specification.\nImplementing password-based authentication\nThe OAuth 2.0 specification, or OAuth2, is the most preferred solution for authenticating API endpoint \naccess. The OAuth2 authorization framework defines the four authorization flows, which are implicit, \nclient credentials, authorization code, and resource password flows. The first three of these can be used \nwith third-party authentication providers, which will authorize the access of the API endpoints. In \nthe FastAPI platform, the resource password flow can be customized and implemented within the \napplication to carry out the authentication procedure. Let us now explore how FastAPI supports the \nOAuth2 specification.\nInstalling the python-multipart module\nSince OAuth2 authentication will not be possible without a form handling procedure, we need to \ninstall the python-multipart module before pursuing the implementation part. We can run the \nfollowing command to install the extension:\npip install python-multipart\nUsing OAuth2PasswordBearer and OAuth2PasswordRequestForm\nThe FastAPI framework fully supports OAuth2, especially the password flow type of the OAuth2 \nspecification. Its fastapi.security module has an OAuth2PasswordBearer that serves \nas the provider for password-based authentication. It also has OAuth2PasswordRequestForm, \nwhich can declare a form body with required parameters, username and password, and some \noptional ones such as scope, grant_type, client_id, and client_secret. This class is \ndirectly injected into the /ch07/login API endpoint to extract all the parameter values from the \nbrowser’s login form. But it is always an option to use Form(…) to capture all the individual parameters.\nSo, let us start the solution by creating the OAuth2PasswordBearer to be injected into a custom \nfunction dependency that will validate the user credentials. The following implementation shows that \nget_current_user() is the injectable function in our new application, ch07c, which utilizes \nthe oath2_scheme injectable to extract a token:\nfrom fastapi.security import OAuth2PasswordBearer\nfrom sqlalchemy.orm import Session\nfrom repository.login import LoginRepository",
      "content_length": 2726,
      "extraction_method": "Direct"
    },
    {
      "page_number": 246,
      "chapter": null,
      "content": "Implementing password-based authentication\n223\nfrom db_config.sqlalchemy_connect import sess_db\noauth2_scheme = \n    OAuth2PasswordBearer(tokenUrl=\"ch07/login/token\")\ndef get_current_user(token: str = Depends(oauth2_scheme), \n           sess:Session = Depends(sess_db) ):\n    loginrepo = LoginRepository(sess)\n    user = loginrepo.get_all_login_username(token)\n    if user == None:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return user\nFor the resource password flow, injecting oauth2_scheme will return a username as a token. \nget_current_user() will check whether that username belongs to a valid user account stored \nin the database. \nExecuting the login transaction\nIn this authentication scheme, /ch07/login/token is also the tokenUrl parameter of \nOAuth2PasswordBearer. The tokenUrl parameter is required for password-based OAuth2 \nauthentication because this is the endpoint service that will capture the user credentials from the \nbrowser’s login form. OAuth2PasswordRequestForm is injected into /cho07/login/token \nto retrieve the username, password, and grant_type parameters of the unauthenticated user. \nThese three parameters are the essential requirements to invoke /ch07/login/token for token \ngeneration. This dependency is shown in the following implementation of the login API service:\nfrom sqlalchemy.orm import Session\nfrom db_config.sqlalchemy_connect import sess_db\nfrom repository.login import LoginRepository\nfrom fastapi.security import OAuth2PasswordRequestForm\nfrom security.secure import get_current_user, authenticate\n@router.post(\"/login/token\")\ndef login(form_data: OAuth2PasswordRequestForm = Depends(),\n             sess:Session = Depends(sess_db)):",
      "content_length": 1846,
      "extraction_method": "Direct"
    },
    {
      "page_number": 247,
      "chapter": null,
      "content": "Securing the REST APIs\n224\n    username = form_data.username\n    password = form_data.password\n    loginrepo = LoginRepository(sess)\n    account = loginrepo.get_all_login_username(username)\n    if authenticate(username, password, account) and \n              not account == None:\n        return {\"access_token\": form_data.username, \n                  \"token_type\": \"bearer\"}\n    else:\n        raise HTTPException(\n            status_code=400, \n               detail=\"Incorrect username or password\")\nAside from verifying from the database, the login() service will also check whether the password \nvalue matches the encrypted passphrase from the queried account. If all the verification succeeds,  /\nch07/login/token must return a JSON object with the required properties, access_token and \ntoken_type. The access_token property must have the username value, and token_type \nthe \"bearer\" value.\nInstead of creating a custom frontend for the login form, we will be utilizing the OAuth2 form provided \nby OpenAPI in the framework. We just click the Authorize button on the upper-right-hand side of \nthe OpenAPI dashboard, as shown in Figure 7.3:\nFigure 7.3 – The Authorize button",
      "content_length": 1176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 248,
      "chapter": null,
      "content": "Implementing password-based authentication\n225\nThe button will trigger a built-in login form to pop up, shown in Figure 7.4, which we can use to test \nour solution:\nFigure 7.4 – The OAuth2 login form",
      "content_length": 199,
      "extraction_method": "Direct"
    },
    {
      "page_number": 249,
      "chapter": null,
      "content": "Securing the REST APIs\n226\nEverything is fine if the OAuth2 login form detects the correct tokenURL specified in the \nOAuth2PasswordBearer instantiation. The OAuth2 flow or grant_type indicated in the \nlogin form must be \"password\". After logging the verified credential, the form’s Authorize button \nwill redirect the user to an authorization form, shown in Figure 7.5, which will prompt the user to log \nout or proceed with the authenticated access:\nFigure 7.5 – The authorization form\nGenerally, the OAuth2 specification recognizes two client or application types: confidential and public \nclients. The confidential clients utilize authentication servers for security, such as in this online auction \nsystem that uses the FastAPI server through the OpenAPI platform. In its setup, it is not mandatory \nto provide the client_id and client_secret values to the login form since the server will \ngenerate these parameters during the authentication process. But unfortunately, these values are not \nrevealed to the client, as shown in Figure 7.5. On the other hand, the public clients do not have any \nmeans to generate and use client secrets as in typical web-based and mobile applications. Therefore, \nthese applications must include client_id, client_secret, and other required parameters \nduring login.\nSecuring the endpoints\nTo secure the API endpoints, we need to inject the get_current_user() method into each API \nservice method. The following is an implementation of a secured add_auction() service that \nutilizes the get_current_user() method:\n@router.post(\"/auctions/add\")\ndef add_auction(req: AuctionsReq,",
      "content_length": 1616,
      "extraction_method": "Direct"
    },
    {
      "page_number": 250,
      "chapter": null,
      "content": "Implementing password-based authentication\n227\n      current_user: Login = Depends(get_current_user), \n      sess:Session = Depends(sess_db)): \n    auc_dict = req.dict(exclude_unset=True)\n    repo:AuctionsRepository = AuctionsRepository(sess)\n    auction = Auctions(**auc_dict)\n    result = repo.insert_auction(auction)\n    if result == True:\n        return auction\n    else: \n        return JSONResponse(content=\n         {'message':'create auction problem encountered'}, \n            status_code=500)  \nThe get_current_user() injectable will return a valid Login account if the access is allowed. \nMoreover, you will notice that all padlock icons of the secured API endpoints that include /ch07/\nauctions/add, shown in Figure 7.6, are closed. This indicates that they are ready to be executed \nsince the user is already an authenticated one:\nFigure 7.6 – An OpenAPI dashboard showing secured APIs\nThis solution is a problem for an open network setup, for instance, because the token used is a password. \nThis setup allows attackers to easily forge or modify the token during its transmission from the issuer \nto the client. One way to protect the token is to use a JSON Web Token (JWT).",
      "content_length": 1188,
      "extraction_method": "Direct"
    },
    {
      "page_number": 251,
      "chapter": null,
      "content": "Securing the REST APIs\n228\nApplying JWTs\nJWT is an open source standard used to define a solution for sending any information during the \nauthentication and authorization between issuers and clients. Its goal is to generate access_token \nproperties that are digitally signed, URL-safe, and always verifiable by the client. However, it is not \nperfectly safe because anyone can decode the token if needed. Thus, it is advisable not to include all \nthe valuable and confidential information in the token string. A JWT is an effective way of providing \nOAuth2 and OpenID specifications with more reliable tokens than passwords.\nGenerating the secret key\nBut before we start building the authentication scheme, we first need to generate a secret key, which is \nan essential element in creating the signature. The JWT has a JSON Object Signing and Encryption \n(JOSE) header, which is the metadata that describes which algorithm to use for plain-text encoding, \nwhile the payload is the data we need to encode into the token. When the client requests to log in, \nthe authorization server signs a JWT using a signature. But the signature will only be generated by \nthe algorithm indicated in the header, which will take the header, payload, and secret key as inputs. \nThis secret key is a Base64-encoded string manually created outside of the server and should be stored \nseparately within the authorization server. ssh or openssl is the appropriate utility to generate this \nlong and randomized key. Here, in ch07d, we run the following openssl command from a GIT \ntool or any SSL generator to create the key:\nopenssl rand -hex 32\nCreating the access_token\nIn the ch07d project, we will store the secret key and algorithm type in some reference variables in its \n/security/secure.py module script. These variables are used by the JWT-encoding procedure \nto generate the token, as shown in the following code:\nfrom jose import jwt, JWTError\nfrom datetime import datetime, timedelta\nSECRET_KEY = \"tbWivbkVxfsuTxCP8A+Xg67LcmjXXl/sszHXwH+TX9w=\"\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\ndef create_access_token(data: dict, \n           expires_after: timedelta):\n    plain_text = data.copy()\n    expire = datetime.utcnow() + expires_after",
      "content_length": 2238,
      "extraction_method": "Direct"
    },
    {
      "page_number": 252,
      "chapter": null,
      "content": "Applying JWTs\n229\n    plain_text.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(plain_text, SECRET_KEY, \n            algorithm=ALGORITHM)\n    return encoded_jwt\nWithin the JWT Python extension, we chose the python-jose module to generate the token \nbecause it is reliable and has additional cryptographic functions that can sign complex data content. \nInstall this module first using the pip command before using it. \nSo now, the /ch07/login/token endpoint will invoke the create_access_token() method \nto request the JWT. The login service will provide the data, usually username, to comprise the payload \nportion of the token. Since the JWT must be short-lived, the process must update the expire portion \nof the payload to some datetime value in minutes or seconds suited to the application. \nCreating the login transaction\nThe implementation of the login service is similar to the previous password-based OAuth2 authentication, \nexcept that this version has a create_access_token() call for the JWT generation to replace \nthe password credential. The following script shows the /ch07/login/token service of the \nch07d project:\n@router.post(\"/login/token\")\ndef login(form_data: OAuth2PasswordRequestForm = Depends(),\n          sess:Session = Depends(sess_db)):\n    username = form_data.username\n    password = form_data.password\n    loginrepo = LoginRepository(sess)\n    account = loginrepo.get_all_login_username(username)\n    if authenticate(username, password, account):\n        access_token = create_access_token(\n          data={\"sub\": username}, \n           expires_after=timedelta(\n              minutes=ACCESS_TOKEN_EXPIRE_MINUTES))\n        return {\"access_token\": access_token, \n             \"token_type\": \"bearer\"}\n    else:\n        raise HTTPException(\n            status_code=400, \n            detail=\"Incorrect username or password\")",
      "content_length": 1855,
      "extraction_method": "Direct"
    },
    {
      "page_number": 253,
      "chapter": null,
      "content": "Securing the REST APIs\n230\nThe endpoint should still return access_token and token_type since this is still a password-based \nOAuth2 authentication, which retrieves the user credentials from OAuth2PasswordRequestForm. \nAccessing the secured endpoints\nAs with the previous OAuth2 schemes, we need to inject get_current_user()into every API \nservice to impose security and restrict access. The injected OAuthPasswordBearer instance will \nreturn the JWT for payload extraction using the JOSE decoders with the specified decoding algorithm. \nIf the token is tampered with, modified, or expired, the method will throw an change to - exception. \nOtherwise, we need to continue the payload data extraction, retrieve the username, and store that in an \n@dataclass instance, such as TokenData. Then, the username will undergo further verification, \nsuch as checking the database for a Login account with that username. The following snippet shows \nthis decoding process, found in the /security/secure.py module of the ch07d project:\nfrom models.request.tokens import TokenData\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import jwt, JWTError\nfrom models.data.sqlalchemy_models import Login\nfrom sqlalchemy.orm import Session\nfrom db_config.sqlalchemy_connect import sess_db\nfrom repository.login import LoginRepository\nfrom datetime import datetime, timedelta\noauth2_scheme = \n     OAuth2PasswordBearer(tokenUrl=\"ch07/login/token\")\ndef get_current_user(token: str = Depends(oauth2_scheme),\n    sess:Session = Depends(sess_db)):\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"}\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, \n           algorithms=[ALGORITHM])",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 254,
      "chapter": null,
      "content": "Creating scope-based authorization\n231\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        token_data = TokenData(username=username)\n    except JWTError:\n        raise credentials_exception\n    \n    loginrepo = LoginRepository(sess)\n    user = \n      loginrepo.get_all_login_username(token_data.username)\n    if user is None:\n        raise credentials_exception\n    return user\nget_current_user() must be injected into each service implementation to restrict access from \nusers. But this time, the method will not only verify the credentials but also perform JWT payload \ndecoding. The next step is adding user authorization to the OAuth2 solution.\nCreating scope-based authorization\nFastAPI fully supports scope-based authentication, which uses the scopes parameter of the OAuth2 \nprotocol to specify which endpoints are accessible to a group of users. A scopes parameter is a \nkind of permission placed in a token to provide additional fine-grained restrictions to users. In this \nversion of the project, ch07e, we will be showcasing OAuth2 password-based authentication with \nuser authorization. \nCustomizing the OAuth2 class\nFirst, we need to create a custom class that inherits the properties of the OAuth2 API class from \nthe fastapi.security module to include the scopes parameter or \"role\" options in the user \ncredentials. The following is the OAuth2PasswordBearerScopes class, a custom OAuth2 class \nthat will implement the authentication flow with authorization:\nclass OAuth2PasswordBearerScopes(OAuth2):\n    def __init__(\n        self,\n        tokenUrl: str,\n        scheme_name: str = None,\n        scopes: dict = None,\n        auto_error: bool = True,",
      "content_length": 1733,
      "extraction_method": "Direct"
    },
    {
      "page_number": 255,
      "chapter": null,
      "content": "Securing the REST APIs\n232\n        \n    ):\n    if not scopes:\n         scopes = {}\n    flows = OAuthFlowsModel(\n       password={\"tokenUrl\": tokenUrl, \"scopes\": scopes})\n    super().__init__(flows=flows, \n       scheme_name=scheme_name, auto_error=auto_error)\n    async def __call__(self, request: Request) -> \n             Optional[str]:\n        header_authorization: str = \n              request.headers.get(\"Authorization\")\n        … … … … … …\n        return param\nThis OAuth2PasswordBearerScopes class requires two constructor parameters, tokenUrl \nand scopes, to pursue an auth flow. OAuthFlowsModel defines the scopes parameter as part \nof the user credentials for authentication using the Authorization header.\nBuilding the permission dictionary\nBefore we proceed with the auth implementation, we need to first build the scopes parameters \nthat the OAuth2 scheme will be applying during authentication. This setup is part of the \nOAuth2PasswordBearerScopes instantiation, where we assign these parameters to its scopes \nparameter. The following script shows how all the custom-defined user scopes are saved in a dictionary, \nwith the keys as the scope names and the values as their corresponding descriptions:\noauth2_scheme = OAuth2PasswordBearerScopes(\n    tokenUrl=\"/ch07/login/token\",\n    scopes={\"admin_read\": \n              \"admin role that has read only role\",\n            \"admin_write\":\n              \"admin role that has write only role\",\n            \"bidder_read\":\n              \"customer role that has read only role\",\n            \"bidder_write\":\n              \"customer role that has write only role\",\n            \"auction_read\":",
      "content_length": 1647,
      "extraction_method": "Direct"
    },
    {
      "page_number": 256,
      "chapter": null,
      "content": "Creating scope-based authorization\n233\n              \"buyer role that has read only role\",\n            \"auction_write\":\n              \"buyer role that has write only role\",\n            \"user\":\"valid user of the application\",\n            \"guest\":\"visitor of the site\"},\n)\nThere is no feasible way to directly connect the OAuth2PasswordBearerScopes class to \nthe database for the dynamic lookup of permission sets during the implementation of this project. \nThe only solution is to statically store all these authorization \"roles\" directly into the constructor of \nOAuth2PasswordBearerScopes.\nImplementing the login transaction\nAll the scopes will be added to the OAuth2 form login as an option and will be part of the user’s login \ncredentials. The following implementation of /ch07/login/token in this new ch07e project shows \nhow to retrieve the scope parameter(s) and the credentials from OAuth2PasswordRequestForm:\n@router.post(\"/login/token\")\ndef login(form_data: OAuth2PasswordRequestForm = Depends(),\n         sess:Session = Depends(sess_db)):\n    username = form_data.username\n    password = form_data.password\n    loginrepo = LoginRepository(sess)\n    account = loginrepo.get_all_login_username(username)\n    if authenticate(username, password, account):\n        access_token = create_access_token(\n            data={\"sub\": username, \"scopes\": \n              form_data.scopes},  \n               expires_delta=timedelta(\n               minutes=ACCESS_TOKEN_EXPIRE_MINUTES))\n        return {\"access_token\": access_token, \n                \"token_type\": \"bearer\"}\n    else:\n        raise HTTPException(\n            status_code=400, \n            detail=\"Incorrect username or password\")",
      "content_length": 1689,
      "extraction_method": "Direct"
    },
    {
      "page_number": 257,
      "chapter": null,
      "content": "Securing the REST APIs\n234\nThe selected scopes are stored in a list, such as ['user', 'admin_read', 'admin_write', \n'bidder_write'], which means that a user has user, administrator (write), administrator (read), \nand bidder (write) permissions. create_access_token() will include this list of scopes or \"roles\" \nas part of the payload, which will be decoded and extracted by get_current_valid_user() \nthrough the get_current_user() injectable. By the way, get_current_valid_user() \nsecures every API from the user access by applying the authentication scheme.\nApplying the scopes to endpoints\nThe Security API from the fastapi module replaces the Depends class in injecting the get_\ncurrent_valid_user() because of its capability to assign scopes to each API service, aside from \nits capability to perform DI. It has the scopes attribute, where a list of valid scope parameters is \ndefined that restricts the user from access. For instance, the following update_profile() service \nis accessible only to users whose scopes contain the bidder_write and buyer_write roles:   \nfrom fastapi.security import SecurityScopes\n@router.patch(\"/profile/update\")\ndef update_profile(id:int, req: ProfileReq, \n    current_user: Login = Security(get_current_valid_user, \n        scopes=[\"bidder_write\", \"buyer_write\"]), \n    sess:Session = Depends(sess_db)): \n    … … … … … …\n    if result: \n        return JSONResponse(content=\n         {'message':'profile updated successfully'}, \n              status_code=201)\n    else: \n        return JSONResponse(content=\n           {'message':'update profile error'}, \n               status_code=500)\nNow, the following code snippet shows the implementation of the get_current_valid_user() \ninjected into every API service by Security:\ndef get_current_valid_user(current_user: \n   Login = Security(get_current_user, scopes=[\"user\"])):\n    if current_user == None:\n        raise HTTPException(status_code=400, \n           detail=\"Invalid user\")\n    return current_user",
      "content_length": 1992,
      "extraction_method": "Direct"
    },
    {
      "page_number": 258,
      "chapter": null,
      "content": "Creating scope-based authorization\n235\nThis method relies on get_current_user() when it comes to JWT payload decoding, credential \nvalidation, and user scope verification. The user must at least have the user scope for the authorization \nprocess to proceed. The Security class is responsible for injecting get_current_user() into \nget_current_valid_user() together with the default user scope. Here is the implementation \nof the get_current_user() method:\ndef get_current_user(security_scopes: SecurityScopes, \n      token: str = Depends(oauth2_scheme), \n           sess:Session = Depends(sess_db)):\n    if security_scopes.scopes:\n        authenticate_value = \n          f'Bearer scope=\"{security_scopes.scope_str}\"'\n    else:\n        authenticate_value = f\"Bearer\" \n    … … … … … …\n    try:\n        payload = jwt.decode(token, SECRET_KEY, \n                   algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        token_scopes = payload.get(\"scopes\", [])\n        token_data = TokenData(scopes=token_scopes, \n               username=username)\n    except JWTError:\n        raise credentials_exception\n    … … … … … …\n    for scope in security_scopes.scopes:\n        if scope not in token_data.scopes:\n            raise HTTPException(\n                status_code=status.HTTP_401_UNAUTHORIZED,\n                detail=\"Not enough permissions\",\n                headers={\"WWW-Authenticate\": \n                    authenticate_value},\n            )\n    return user",
      "content_length": 1545,
      "extraction_method": "Direct"
    },
    {
      "page_number": 259,
      "chapter": null,
      "content": "Securing the REST APIs\n236\nThe SecurityScopes class of the given get_current_user() extracts the scopes assigned \nto the API service that the user is trying to access. It has a scope instance variable that contains all \nthese scope parameters of the API. On the other hand, token_scopes carries all the scopes or \n\"roles\" of the user extracted from the decoded JWT payload. get_current_user() traverses the \nAPI scopes in SecurityScopes to check whether all of them appear in the token_scopes of \nthe user. If True, get_current_user() authenticates and authorizes the user to access the API \nservice. Otherwise, it throws an change to - exception. The purpose of TokenData is to manage the \nscope parameters from the token_scopes payload value and the username. \nThe next type of OAuth2 authentication scheme that FastAPI can support is the authorization code \nflow approach.\nBuilding the authorization code flow\nIf the application is a public type and there is no authorization server to process the client_id \nparameter, the client_secret parameter, and other related parameters, this OAuth2 authorization \ncode flow approach is appropriate to use. In this scheme, the client creates an authorization request \nfor a short-lived authorization code from an authorizationUrl. The client will then ask for the \ntoken from tokenUrl in exchange for the generated code. In this discussion, we will be showcasing \nanother version of our online auction system that will use the OAuth2 authorization code flow scheme.\nApplying OAuth2AuthorizationCodeBearer\nThe OAuth2AuthorizationCodeBearer class is a class from the fastapi.security \nmodule that builds the authorization code flow. Its constructor requires authorizationUrl, \ntokenUrl, and the optional scopes before instantiation. The following code shows how this API \nclass is created before its injection into the get_current_user() method:\nfrom fastapi.security import OAuth2AuthorizationCodeBearer\noauth2_scheme = OAuth2AuthorizationCodeBearer(\n    authorizationUrl='ch07/oauth2/authorize',\n    tokenUrl=\"ch07/login/token\",  \n    scopes={\"admin_read\": \"admin … read only role\",\n            \"admin_write\":\"admin … write only role\",\n            … … … … … …\n            \"guest\":\"visitor of the site\"},\n    )",
      "content_length": 2253,
      "extraction_method": "Direct"
    },
    {
      "page_number": 260,
      "chapter": null,
      "content": "Building the authorization code flow\n237\nThe two endpoints, authorizationUrl and tokenUrl, are crucial parameters in the \nauthentication and authorization process of this scheme. Unlike the previous solutions, we will not \nrely on the authorization server when generating access_token. Instead, we will be implementing \nan authorizationUrl endpoint that will capture essential parameters from the client that will \ncomprise the authorization request for access_token generation. The client_secret parameter \nwill always remain unexposed to the client.\nImplementing the authorization request\nIn the previous schemes, the /ch07/login/ token or the tokenUrl endpoint is always the \nredirection point after a login transaction. But this time, the user will be forwarded to the custom \n/ch07/oauth2/authorize or the authorizationUrl endpoint for auth code generation. \nQuery parameters such as response_type, client_id, redirect_uri, scope, and \nstate are the essential inputs to the authorizationUrl service. The following code from \nthe /security/secure.py module of the ch07f project will showcase the implementation of \nthe authorizationUrl transaction:\n@router.get(\"/oauth2/authorize\")\ndef authorizationUrl(state:str, client_id: str, \n       redirect_uri: str, scope: str, response_type: str, \n       sess:Session = Depends(sess_db)):\n      \n    global state_server\n    state_server = state\n    \n    loginrepo = LoginRepository(sess)\n    account = loginrepo.get_all_login_username(client_id)\n    auth_code = f\"{account.username}:{account.password}\n                    :{scope}\"\n    if authenticate(account.username, \n              account.password, account):\n        return RedirectResponse(url=redirect_uri \n          + \"?code=\" + auth_code \n          + \"&grant_type=\" + response_type\n          + \"&redirect_uri=\" + redirect_uri \n          + \"&state=\" + state)\n    else:\n        raise HTTPException(status_code=400, \n               detail=\"Invalid account\")",
      "content_length": 1958,
      "extraction_method": "Direct"
    },
    {
      "page_number": 261,
      "chapter": null,
      "content": "Securing the REST APIs\n238\nThese are the query parameters needed by the authorizationUrl transaction:\n•\t response_type: Custom-generated authorization code \n•\t client_id: The public identifier of the app, such as username\n•\t redirect_uri: The server default URI or a custom endpoint designed to redirect the user \nback to the application\n•\t scope: A scope parameter(s) string, separated by spaces if at least two parameters are involved\n•\t state: An arbitrary string value that determines the state of the request\nThe redirect_uri parameter is the destination point where the authentication and authorization \nprocesses will occur together with these query parameters. \nThe generation of auth_code is one of the crucial tasks of the authorizationUrl transaction, \nincluding the authentication process. The auth code indicates an ID for the authentication process \nand is usually unique from all other authentication. There are many ways to generate the code, but in \nour app, it is simply the combination of user credentials. Conventionally, auth_code needs to be \nencrypted because it comprises the user credentials, scope, and other request-related details.\nIf the user is valid, the authorizationUrl transaction will redirect the user to the redirect_uri \nparameter, back to the FastAPI layer, with the auth_code, grant_type, and state parameters, \nand the redirect_uri parameter itself. The grant_type and redirect_uri parameters \nare optional only if the application does not require them. This response will invoke the tokenUrl \nendpoint, which happens to be the redirectURL parameter, to pursue the continuation of the \nauthentication process with scoped-based authorization.\nImplementing the authorization code response\nThe /ch07/login/token service, or tokenUrl, must have the Form(…) parameter to capture the \ncode, grant_type, and redirect_uri parameters from the authorizationUrl transaction \ninstead of OAuth2PasswordRequestForm. The following code snippet shows its implementation:\n@router.post(\"/login/token\")\ndef access_token(code: str = Form(...), \n  grant_type:str = Form(...), redirect_uri:str = Form(...), \n  sess:Session = Depends(sess_db)):\n    access_token_expires = \n       timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    \n    code_data = code.split(':')\n    scopes = code_data[2].split(\"+\")",
      "content_length": 2318,
      "extraction_method": "Direct"
    },
    {
      "page_number": 262,
      "chapter": null,
      "content": "Building the authorization code flow\n239\n    password = code_data[1]\n    username = code_data[0]\n    \n    loginrepo = LoginRepository(sess)\n    account = loginrepo.get_all_login_username(username)\n    if authenticate(username, password, account):\n        access_token = create_access_token(\n            data={\"sub\": username, \"scopes\": scopes},\n            expires_delta=access_token_expires,\n        )\n    \n        global state_server\n        state = state_server\n        return {\n            \"access_token\": access_token,\n            \"expires_in\": access_token_expires,\n            \"token_type\": \"Bearer\",\n            \"userid\": username,\n            \"state\": state,\n            \"scope\": \"SCOPE\"\n        }\n    else:\n        raise HTTPException(\n            status_code=400, \n             detail=\"Incorrect credentials\")\nThe only response data sent by authorizationUrl that is not accessible by tokenUrl is the \nstate parameter. One workaround is to declare the state variable in authorizationURL as \na global one to make it accessible anywhere. The state variable is part of the JSON response of \nthe service, which the API authentication requires. Likewise, tokenUrl has no access to the user \ncredentials but parsing auth_code is a possible way to derive the username, password, and scopes.\nIf the user is valid, tokenUrl must submit the JSON data containing access_token, expires_in, \ntoken_type, userid, and state to proceed with the authentication scheme. \nThis authorization code flow scheme provides the baseline protocol for the OpenID Connect authentication. \nVarious identity and access management solutions, such as Okta, Auth0, and Keycloak, apply the \nauthorization requests and responses involving response_type code. The next topic will highlight the \nFastAPI's support of the OpenID Connect specification.",
      "content_length": 1822,
      "extraction_method": "Direct"
    },
    {
      "page_number": 263,
      "chapter": null,
      "content": "Securing the REST APIs\n240\nApplying the OpenID Connect specification\nThere are three online auction projects created to impose the OAuth2 OpenID Connect authentication \nscheme. All these projects use third-party tools to perform authentication and authorization \nprocedures. The ch07g project uses Auth0, ch07h uses Okta, and ch07i applies a Keycloak policy \nin authenticating client access to the API services. Let us first highlight Keycloak’s support for the \nOpenID Connect protocol. \nUsing HTTPBearer\nThe HTTPBearer class is a utility class from the fastapi.security module that provides an \nauthorization scheme that relies directly on the authorization header with the Bearer tokens. Unlike \nthe other OAuth2 schemes, this requires the generation of an access_token on the Keycloak side \nbefore running the authentication server. At this point, the framework has no straightforward way of \naccessing the credentials and the access_token from Keycloak’s identity provider. To utilize this \nclass, we only need to instantiate it without any constructor parameters.\nInstalling and configuring the Keycloak environment\nKeycloak is a Java-based application that we can download from the following link: https://www.\nkeycloak.org/downloads. After downloading, we can unzip its content to any directory. But \nbefore running it, we need to install at least the Java 12 SDK on our development machine. Once you \nhave completed the setup, run its bin\\standalone.bat or bin\\standalone.sh on the console \nand then open http://localhost:8080 on the browser. Afterward, create an administration \naccount to set up the realm, clients, users, and scopes.\nSetting the Keycloak realm and clients\nA Keycloak realm is an object that encompasses all the clients together with their credentials, scopes, \nand roles. The first step before creating the user profiles is to build a realm, as shown in Figure 7.7:\nFigure 7.7 - Creating a Keycloak realm",
      "content_length": 1933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 264,
      "chapter": null,
      "content": "Applying the OpenID Connect specification\n241\nAfter the realm, the Keycloak client, which manages the user profiles and credentials, is the next \npriority. It is created on the Configure | Clients panel, as shown:\nFigure 7.8 – Creating the Keycloak clients\nAfter creating the clients, we need to edit each Client profile to input the following details:\n•\t Its access type must be confidential\n•\t Authorization Enabled is turned ON\n•\t Provide values for Root URL, Base URL, and Admin URL, which all refer to the http://\nlocalhost:8000 of the API service application\n•\t Specify a Valid Redirect URI endpoint, or we can just assign http://\nlocalhost:8080/* if we have no specific custom endpoint \n•\t In Advanced Settings, set Access Token Lifespan (e.g., 15 minutes)\n•\t Under Authentication Flow Overrides, set Browser Flow to browser and \nDirect Grant Flow to direct grant.\nIn the Credentials panel, we can find the client credentials, in which the auto-generated client_\nsecret value is located. After the setup, we can now assign users to the clients.",
      "content_length": 1051,
      "extraction_method": "Direct"
    },
    {
      "page_number": 265,
      "chapter": null,
      "content": "Securing the REST APIs\n242\nCreating users and user roles\nFirst, we create roles on the Configure | Roles panel, in preparation for user assignment later. Figure 7.9 \nshows three user roles that will handle the application’s administration, auctioning, and bidding tasks:\nFigure 7.9 – Creating user roles\nAfter creating the roles, we need to build the list of users on the Manage | Users panel. Figure 7.10 \nshows the three created users, each with the mapped roles: \nFigure 7.10 – Creating client users",
      "content_length": 502,
      "extraction_method": "Direct"
    },
    {
      "page_number": 266,
      "chapter": null,
      "content": "Applying the OpenID Connect specification\n243\nTo provide users with their roles, we need to click the Edit link for each user and assign the appropriate \nRealm Roles. Figure 7.11 shows that the user joey_admin has the auc_admin_role role, \nauthorizing the user to do the administrative tasks for the app. By the way, do not forget to create a \npassword for each user on the Credentials panel:\nFigure 7.11 – Mapping user roles\nAssigning roles to clients\nAside from user roles, clients can also have assigned roles. A client role defines the type of users a client \nmust have under its coverage. It also provides the client’s boundary when accessing the API services. \nFigure 7.12 shows auc_admin with an admin role:\n \nFigure 7.12 – Creating client roles",
      "content_length": 752,
      "extraction_method": "Direct"
    },
    {
      "page_number": 267,
      "chapter": null,
      "content": "Securing the REST APIs\n244\nThen, we need to return to the Manage | Users panel and assign the user its role(s) through the client. \nFor instance, Figure 7.13 shows that joey_admin has the admin role because the auc_admin \nrole was added to its profile. All users with the auc_admin client added to their setup have admin \naccess to the app, including joey_admin:\nFigure 7.13 – Mapping client roles to users\nCreating user permissions through scopes\nTo assign permission to each client, we need to create client scopes on the Configure | Client Scopes panel. \nEach client scope must have an Audience-type token mapper. Figure 7.14 shows the admin:read \nand admin:write scopes for the auc_admin client, auction:read and auction:write \nfor auc_customer, and bidder:write and bidder:read for auc_bidder:",
      "content_length": 798,
      "extraction_method": "Direct"
    },
    {
      "page_number": 268,
      "chapter": null,
      "content": "Applying the OpenID Connect specification\n245\nFigure 7.14 – Creating the client scopes\nThese client scopes are essential details within the Security injection for each API service if scope-\nbased authorization is part of the scheme.\nIntegrating Keycloak with FastAPI\nSince the FastAPI application cannot directly access the Keycloak client credentials for authentication, \nthe application has a login_keycloak() service to redirect the user to the AuctionRealm URI, \nour custom authorizationUrl in Keycloak. The URI is /auth/realms/AuctionRealm/\nprotocol/openid-connect/auth. First, access http://localhost:8080/auth/\nrealms/AuctionRealm/account/ to log in using the authorized user credentials, such as \njoey_admin, before invoking the login_keycloak() service.\nNow, the redirection must include client_id, as with the auc_admin client, and the custom \ncallback handler called redirect_uri. All the Keycloak realm details must be in the .config \nproperty file. The following code shows the implementation of the login_keycloak() service:\nimport hashlib\nimport os\nimport urllib.parse as parse",
      "content_length": 1092,
      "extraction_method": "Direct"
    },
    {
      "page_number": 269,
      "chapter": null,
      "content": "Securing the REST APIs\n246\n@router.get(\"/auth/login\")\ndef login_keycloak() -> RedirectResponse:\n    config = set_up()\n    state = hashlib.sha256(os.urandom(32)).hexdigest()\n \n    AUTH_BASE_URL = f\"{config['KEYCLOAK_BASE_URL']}\n     /auth/realms/AuctionRealm/protocol/\n         openid-connect/auth\"\n    AUTH_URL = AUTH_BASE_URL + \n     '?{}'.format(parse.urlencode({\n        'client_id': config[\"CLIENT_ID\"],\n        'redirect_uri': config[\"REDIRECT_URI\"],\n        'state': state,\n        'response_type': 'code'\n    }))\n    response = RedirectResponse(AUTH_URL)\n    response.set_cookie(key=\"AUTH_STATE\", value=state)\n    return response\nA state is part of login_keycloak()’s response for the callback method to verify the authentication, \na similar approach we had in utilizing OAuth2AuthorizationCodeBearer. The service used \nthe hashlib module to generate a randomized hash string value for the state using the SHA256 \nencryption algorithm. On the other hand, Keycloak’s AuctionRealm URI must return a JSON result \nas follows:\n{\"access_token\":\"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkI\niA6ICJJMFR3YVhiZnh0MVNQSnNzVTByQ09hMzVDaTdZNDkzUnJIeDJTM3paa0V\nVIn0.eyJleHAiOjE2NDU0MTgzNTAsImlhdCI6MTY0NTQxNzQ1MCwiYXV0aF90a\nW1lIjoxNjQ1NDE3NDM3LCJqdGkiOiI4YTQzMjBmYi0xMzg5LTQ2NzU……………………\n………2YTU2In0.UktwOX7H2ZdoyP1VZ5V2MXUX2Gj41D2cuusvwEZXBtVMvnoTDh\nKJgN8XWL7P3ozv4A1ZlBmy4NX1HHjPbSGsp2cvkAWwlyXmhyUzfQslf8Su00-4\ne9FR4i4rOQtNQfqHM7cLhrzr3-od-uyj1m9KsrpbqdLvPEl3KZnmOfFbTwUXfE\n9YclBFa8zwytEWb4qvLvKrA6nPv7maF2_MagMD_0Mh9t95N9_aY9dfquS9tcEV\nWhr3d9B3ZxyOtjO8WiQSJyjLCT7IW1hesa8RL3WsiG3QQQ4nUKVHhnciK8efRm\nXeaY6iZ_-8jm-mqMBxw00-jchJE8hMtLUPQTMIK0eopA\",\"expires_in\":900,\n\"refresh_expires_in\":1800,\"refresh_token\":\"eyJhbGciOiJIUzI1NiIs\nInR5cCIgOiAiSldUIiwia2lkIiA6ICJhNmVmZGQ0OS0yZDIxLTQ0NjQtOGUyOC0\n4ZWJkMjdiZjFmOTkifQ.eyJleHAiOjE2NDU0MTkyNTAsImlhdCI6MTY0NTQxNzQ\n1MCwianRpIjoiMzRiZmMzMmYtYjAzYi00MDM3LTk5YzMt………………………zc2lvbl9z",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 270,
      "chapter": null,
      "content": "Applying the OpenID Connect specification\n247\ndGF0ZSI6ImM1NTE3ZDIwLTMzMTgtNDFlMi1hNTlkLWU2MGRiOWM1NmE1NiIsIn\nNjb3BlIjoiYWRtaW46d3JpdGUgYWRtaW46cmVhZCB1c2VyIiwic2lkIjoiYzU1\nMTdkMjAtMzMxOC00MWUyLWE1OWQtZTYwZGI5YzU2YTU2In0.xYYQPr8dm7_o1G\nKplnS5cWmLbpJTCBDfm1WwZLBhM6k\",\"token_type\":\"Bearer\",\"not-\nbefore-policy\":0,\"session_state\":\"c5517d20-3318-41e2-a59d-e60d\nb9c56a56\",\"scope\":\"admin:write admin:read user\"}\nThis contains the essential credentials, such as access_token, expires_in, session_state, \nand scope.\nImplementing the token verification\nThe application’s HTTPBearer needs access_token to pursue the client-side authentication. On \nthe OpenAPI dashboard, we click the Authorize button and paste the access_token value provided \nby Keycloak’s authorizationUrl. After the successful authentication, get_current_user() \nwill verify the access to each API endpoint based on the credentials extracted from access_token. \nThe following code highlights the get_current_user(), which builds the user credentials from \nKeycloak’s token using the PyJWT utility and algorithms such as RSAAlgorithm:\nfrom jwt.algorithms import RSAAlgorithm\nfrom urllib.request import urlopen\nimport jwt\ndef get_current_user(security_scopes: SecurityScopes, \n        token: str = Depends(token_auth_scheme)):\n    token = token.credentials\n    config = set_up()\n    jsonurl = urlopen(f'{config[\"KEYCLOAK_BASE_URL\"]}\n        /auth/realms/AuctionRealm/protocol\n        /openid-connect/certs')\n    jwks = json.loads(jsonurl.read())\n    unverified_header = jwt.get_unverified_header(token)\n   \n    rsa_key = {}\n    for key in jwks[\"keys\"]:\n        if key[\"kid\"] == unverified_header[\"kid\"]:\n            rsa_key = {",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 271,
      "chapter": null,
      "content": "Securing the REST APIs\n248\n                \"kty\": key[\"kty\"],\n                \"kid\": key[\"kid\"],\n                \"use\": key[\"use\"],\n                \"n\": key[\"n\"],\n                \"e\": key[\"e\"]\n            }\n    \n    if rsa_key:\n        try:\n                public_key = RSAAlgorithm.from_jwk(rsa_key)\n                payload = jwt.decode(\n                    token,\n                    public_key,\n                    algorithms=config[\"ALGORITHMS\"],\n                    options=dict(\n                           verify_aud=False,\n                           verify_sub=False,\n                           verify_exp=False,\n                     )\n                )\n    … … … … … …\n    token_scopes = payload.get(\"scope\", \"\").split()\n   \n    for scope in security_scopes.scopes:\n        if scope not in token_scopes:\n            raise AuthError(\n               {\n                 \"code\": \"Unauthorized\",\n                 \"description\": Invalid Keycloak details,\n               },403,\n            )\n    return payload\nInstall the PyJWT module first to utilize the needed encoders and decoder functions. The jwt module \nhas RSAAlgorithm, which can help decode the rsa_key from the token with some options disabled, \nsuch as the verification of the client’s audience.",
      "content_length": 1259,
      "extraction_method": "Direct"
    },
    {
      "page_number": 272,
      "chapter": null,
      "content": "Applying the OpenID Connect specification\n249\nIntegrating Auth0 with FastAPI\nAuth0 can also be a third-party authentication provider that can authenticate and authorize access to \nthe API endpoints of our app. But first, we need to sign up for an account at https://auth0.com/. \nAfter signing up for an account, create an Auth0 application to derive Domain, Client ID, and Client \nSecret, and configure some URI- and token-related details. Figure 7.15 shows the dashboard that \ncreates the Auth0 application:\nFigure 7.15 – Creating the Auth0 application\nThe Auth0 application also has the generated Audience API URI that the client-side authentication \nneeds. On the other hand, part of the authentication parameters is the issuer, which we can derive \nfrom the Domain value of the Auth0 application. The issuer is a base URI to the /oauth/token \nservice that generates the auth_token once requested, similar to the Keycloak’s realm. We place all \nthese Auth0 details in the .config file, including the PyJWT algorithm for decoding auth_token.\nch07g has its own version of get_current_user() that processes the payload for API \nauthentication and authorization from the Auth0 details in the .config file. But first, the \nHTTPBearer class needs the auth_token value and gets it by running the following tokenURL \nof our Auth0 application, AuctionApp:\ncurl --request POST                                      --url \nhttps://dev-fastapi1.us.auth0.com/oauth/token   --header",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 273,
      "chapter": null,
      "content": "Securing the REST APIs\n250\n'content-type: application/json'              --data \"{\"client_\nid\":\"KjdwFzHrOLXC3IKe\nkw8t6YhX4xUV1ZNd\",   \"client_secret\":\"_\nKyPEUOB7DA5Z3mmRXpnqWA3EXfrjLw2R5SoUW7m1wLMj7\nKoElMyDLiZU8SgMQYr\",\"audience\":\"https://fastapi.auction.com/\",\n\"grant_type\":\"client_credentials\"}\"\nIntegrating Okta with FastAPI\nSome of the processes performed in Auth0 are also found in Okta’s procedures when extracting the \nClient ID, Client Secret, Domain, issuer, and Audience API information from the Okta account. The \nch07h project has these details stored in the app.env file to be retrieved by its get_current_\nuser() for the payload generation. But then again, the HTTPBearer class needs an auth_token \nfrom executing the following Okta’s tokenURL, based on the account’s issuer:\ncurl --location --request POST \"https://dev-5180227.\nokta.com/oauth2/default/v1/token?grant_type=client_\ncredentials&client_id=0oa3tvejee5UPt7QZ5d7&client_\nsecret=LA4WP8lACWKu4Ke9fReol0fNSUvxsxTvGLZdDS5-\"   --header \n\"Content-Type: application/x-www-form-urlencoded\"\nAside from the Basic, Digest, OAuth2, and OpenID Connect authentication schemes, FastAPI can \nutilize some built-in middlewares to help secure API endpoints. Let us now determine whether these \nmiddlewares can provide a custom authentication process.\nUsing built-in middlewares for authentication\nFastAPI can use Starlette middleware such as AuthenticationMiddleware to implement \nany custom authentication. It needs AuthenticationBackend to implement the scheme for \nour app’s security model. The following custom AuthenticationBackend checks whether \nthe Authorization credential is a Bearer class and verifies whether the username token is \nequivalent to a fixed username credential provided by the middleware:\nclass UsernameAuthBackend(AuthenticationBackend):\n    def __init__(self, username): \n        self.username = username    \n        \n    async def authenticate(self, request):\n        if \"Authorization\" not in request.headers:\n            return\n        auth = request.headers[\"Authorization\"]\n        try:",
      "content_length": 2075,
      "extraction_method": "Direct"
    },
    {
      "page_number": 274,
      "chapter": null,
      "content": "Summary\n251\n            scheme, username = auth.split()\n            if scheme.lower().strip() != 'bearer'.strip():\n                return\n        except:\n            raise AuthenticationError(\n             'Invalid basic auth credentials')\n        if not username == self.username:\n            return\n       \n        return AuthCredentials([\"authenticated\"]), \n             SimpleUser(username)\nActivating this UsernameAuthBackend means injecting it into the FastAPI constructor in \nmain.py with AuthenticationMiddleware. It also needs the designated username for its \nauthentication process to work. The following snippet shows how to activate the whole authentication \nscheme in the main.py file:\nfrom security.secure import UsernameAuthBackend\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.authentication import \n    AuthenticationMiddleware\nmiddleware = [Middleware(AuthenticationMiddleware, \n    backend=UsernameAuthBackend(\"sjctrags\"))]\napp = FastAPI(middleware=middleware)\nInjecting FastAPI’s Request is the first step in applying the authentication scheme. Then, we decorate \neach API with @requires(\"authenticated\") after the @router decorator. We can extend \nthe UsernameAuthBackend process further by adding JWT encoding and decoding, encryption, \nor custom roles-based authorization.\nSummary\nSecuring any applications is always the main priority in producing quality software. We always choose \nframeworks that support reliable and credible security solutions, and that can at least prevent malicious \nattacks from the outside environment. Although we know for a fact that a perfect security model is a \nmyth, we always develop security solutions that can cope with the threats we know.",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 275,
      "chapter": null,
      "content": "Securing the REST APIs\n252\nFastAPI is one of the API frameworks that has built-in support for many popular authentication \nprocesses, from Basic to the OpenID Connect specification. It fully supports all effective OAuth2 \nauthentication schemes and is even open to further customization of its security APIs. \nAlthough it has no direct support for the OpenID Connect specification, it can still integrate seamlessly \nwith different popular identities and user management systems, such as Auth0, Okta, and Keycloak. \nThis framework may still surprise us with many security utilities and classes in the future that we can \napply to build scalable microservice applications.\nThe next chapter will focus on topics regarding non-blocking API services, events, and message-\ndriven transactions.",
      "content_length": 788,
      "extraction_method": "Direct"
    },
    {
      "page_number": 276,
      "chapter": null,
      "content": "8\nCreating Coroutines, \nEvents, and Message-Driven \nTransactions\nThe FastAPI framework is an asynchronous framework that runs over the asyncio platform, which \nutilizes the ASGI protocol. It is well known for its 100% support for asynchronous endpoints and \nnon-blocking tasks. This chapter will focus on how we create highly scalable applications with \nasynchronous tasks and event-driven and message-driven transactions.\nWe learned in Chapter 2, Exploring the Core Features, that Async/Await or asynchronous programming \nis a design pattern that enables other services or transactions to run outside the main thread. The \nframework uses the async keyword to create asynchronous processes that will run on top of other \nthread pools and will be awaited, instead of invoking them directly. The number of external threads \nis defined during the Uvicorn server startup through the --worker option.\nIn this chapter, we will delve into the framework and scrutinize the various components of the FastAPI \nFramework that can run asynchronously using multiple threads. The following highlights will help \nus understand how asynchronous FastAPI is: \n•\t Implementing coroutines\n•\t Creating asynchronous background tasks\n•\t Understanding Celery tasks\n•\t Building message-driven transactions using RabbitMQ\n•\t Building publish/subscribe messaging using Kafka\n•\t Applying reactive programming in tasks\n•\t Customizing events",
      "content_length": 1411,
      "extraction_method": "Direct"
    },
    {
      "page_number": 277,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n254\n•\t Implementing asynchronous Server-Sent Events (SSE)\n•\t Building an asynchronous WebSocket\nTechnical requirements\nThis chapter will cover asynchronous features, software specifications, and the components of a newsstand \nmanagement system prototype. The discussions will use this online newspaper management system \nprototype as a specimen to understand, explore, and implement asynchronous transactions that will \nmanage the newspaper content, subscription, billing, user profiles, customers, and other business-related \ntransactions. The code has all been uploaded to https://github.com/PacktPublishing/\nBuilding-Python-Microservices-with-FastAPI under the ch08 project.\nImplementing coroutines\nIn the FastAPI framework, a thread pool is always present to execute both synchronous API and \nnon-API transactions for every request. For ideal cases where both the transactions have minimal \nperformance overhead with CPU-bound and I/O-bound transactions, the overall performance of using \nthe FastAPI framework is still better than those frameworks that use non-ASGI-based platforms. \nHowever, when contention occurs due to high CPU-bound traffic or heavy CPU workloads, the \nperformance of FastAPI starts to wane due to thread switching. \nThread switching is a context switch from one thread to another within the same process. So, if we \nhave several transactions with varying workloads running in the background and on the browser, \nFastAPI will run these transactions in the thread pool with several context switches. This scenario \nwill cause contention and degradation to lighter workloads. To avoid performance issues, we apply \ncoroutine switching instead of threads.\nApplying coroutine switching\nThe FastAPI framework works at the optimum speed with a mechanism called coroutine switching. This \napproach allows transaction-tuned tasks to work cooperatively by allowing other running processes to \npause so that the thread can execute and finish more urgent tasks, and resume \"awaited\" transactions \nwithout preempting the thread. These coroutine switches are programmer-defined components and not \nkernel-related or memory-related features. In FastAPI, there are two ways of implementing coroutines: \n(a) applying the @asyncio.coroutine decorator, and (b) using the async/await construct.",
      "content_length": 2363,
      "extraction_method": "Direct"
    },
    {
      "page_number": 278,
      "chapter": null,
      "content": "Implementing coroutines\n255\nApplying @asyncio.coroutine \nasyncio is a Python extension that implements the Python concurrency paradigm using a single-\nthreaded and single-process model and provides API classes and methods for running and managing \ncoroutines. This extension provides an @asyncio.coroutine decorator that transforms API \nand native services into generator-based coroutines. However, this is an old approach and can only be \nused in FastAPI that uses Python 3.9 and below. The following is a login service transaction of our \nnewsstand management system prototype implemented as a coroutine:\n@asyncio.coroutine\ndef build_user_list(query_list):\n    user_list = []\n    for record in query_list:\n        yield from asyncio.sleep(2)\n        user_list.append(\" \".join([str(record.id), \n            record.username, record.password]))\n    return user_list\nbuild_user_list() is a native service that converts all login records into the str format. \nIt is decorated with the @asyncio.coroutine decorator to transform the transaction into an \nasynchronous task or coroutine. A coroutine can invoke another coroutine function or method using \nonly the yield from clause. This construct pauses the coroutine and passes the control of the thread \nto the coroutine function invoked. By the way, the asyncio.sleep() method is one of the most \nwidely used asynchronous utilities of the asyncio module, which can pause a process for a few \nseconds, but is not the ideal one. On the other hand, the following code is an API service implemented \nas a coroutine that can minimize contention and performance degradation in client-side executions:\n@router.get(\"/login/list/all\")\n@asyncio.coroutine\ndef list_login():\n    repo = LoginRepository()\n    result = yield from repo.get_all_login()\n    data = jsonable_encoder(result)\n    return data\nThe list_login() API service retrieves all the login details of the application’s users through a \ncoroutine CRUD transaction implemented in GINO ORM. The API service again uses the yield \nfrom clause to run and execute the get_all_login() coroutine function.",
      "content_length": 2095,
      "extraction_method": "Direct"
    },
    {
      "page_number": 279,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n256\nA coroutine function can invoke and await multiple coroutines concurrently using the asyncio.\ngather() utility. This asyncio method manages a list of coroutines and waits until all its \ncoroutines have completed their tasks. Then, it will return a list of results from the corresponding \ncoroutines. The following code is an API that retrieves login records through an asynchronous CRUD \ntransaction and then invokes count_login() and build_user_list() concurrently to \nprocess these records:\n@router.get(\"/login/list/records\")\n@asyncio.coroutine\ndef list_login_records():\n    repo = LoginRepository()\n    login_data = yield from repo.get_all_login()\n    result = yield from \n       asyncio.gather(count_login(login_data), \n            build_user_list(login_data))\n    data = jsonable_encoder(result[1])\n    return {'num_rec': result[0], 'user_list': data}\nlist_login_records() uses asyncio.gather() to run the count_login() and \nbuild_user_list() tasks and later extract their corresponding returned values for processing.\nUsing the async/await construct\nAnother way of implementing a coroutine is using async/await constructs. As with the previous \napproach, this syntax creates a task that can pause anytime during its operation before it reaches the \nend. But the kind of coroutine that this approach produces is called a native coroutine, which is not \niterable in the way that the generator type is. The async/await syntax also allows the creation of \nother asynchronous components such as the async with context managers and async for \niterators. The following code is the count_login() task previously invoked in the generator-based \ncoroutine service, list_login_records():\nasync def count_login(query_list):\n    await asyncio.sleep(2)\n    return len(query_list)",
      "content_length": 1836,
      "extraction_method": "Direct"
    },
    {
      "page_number": 280,
      "chapter": null,
      "content": "Implementing coroutines\n257\nThe count_login() native service is a native coroutine because of the async keyword placed \nbefore its method definition. It only uses await to invoke other coroutines. The await keyword \nsuspends the execution of the current coroutine and passes the control of the thread to the invoked \ncoroutine function. After the invoked coroutine finishes its process, the thread control will yield \nback to the caller coroutine. Using the yield from construct instead of await will raise an error \nbecause our coroutine here is not generator-based. The following is an API service implemented as a \nnative coroutine that manages data entry for the new administrator profiles:\n@router.post(\"/admin/add\")\nasync def add_admin(req: AdminReq):\n    admin_dict = req.dict(exclude_unset=True)\n    repo = AdminRepository()\n    result = await repo.insert_admin(admin_dict)\n    if result == True: \n        return req \n    else: \n        return JSONResponse(content={'message':'update \n            trainer profile problem encountered'}, \n              status_code=500)\nBoth generator-based and native coroutines are monitored and managed by an event loop, which \nrepresents an infinite loop inside a thread. Technically, it is an object found in the thread, and each \nthread in the thread pool can only have one event loop, which contains a list of helper objects called \ntasks. Each task, pre-generated or manually created, executes one coroutine. For instance, when the \nprevious add_admin() API service invokes the insert_admin() coroutine transaction, the event \nloop will suspend add_admin() and tag its task as an awaited task. Afterward, the event loop will \nassign a task to run the insert_admin() transaction. Once the task has completed its execution, \nit will yield the control back to add_admin(). The thread that manages the FastAPI application is \nnot interrupted during these shifts of execution since it is the event loop and its tasks that participate \nin the coroutine switching mechanism. Let us now use these coroutines to build our application",
      "content_length": 2071,
      "extraction_method": "Direct"
    },
    {
      "page_number": 281,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n258\nDesigning asynchronous transactions\nThere are a few programming paradigms that we can follow when creating coroutines for our application. \nUtilizing more coroutine switching in the process can help improve the software performance. In \nour newsstand application, there is an endpoint, /admin/login/list/enc, in the admin.py \nrouter that returns a list of encrypted user details. In its API service, shown in the following code, each \nrecord is managed by an extract_enc_admin_profile() transaction call instead of passing \nthe whole data record to a single call, thus allowing the concurrent executions of tasks. This strategy \nis better than running the bulk of transactions in a thread without context switches:\n@router.get(\"/admin/login/list/enc\")\nasync def generate_encypted_profile():\n    repo = AdminLoginRepository()\n    result = await repo.join_login_admin()\n    encoded_data = await asyncio.gather(\n       *(extract_enc_admin_profile(rec) for rec in result))\n    return encoded_data\nNow, the extract_enc_admin_profile() coroutine, shown in the following code, implements \na chaining design pattern, where it calls the other smaller coroutines through a chain. Simplifying and \nbreaking down the monolithic and complex processes into smaller but more robust coroutines will \nimprove the application’s performance by utilizing more context switches. In this API, extract_\nenc_admin_profile() creates three context switches in a chain, better than thread switches:\nasync def extract_enc_admin_profile(admin_rec):\n    p = await extract_profile(admin_rec)\n    pinfo = await extract_condensed(p)\n    encp = await decrypt_profile(pinfo)\n    return encp\nOn the other hand, the following implementation is the smaller subroutines awaited and executed by \nextract_enc_admin_profile():\nasync def extract_profile(admin_details):\n    profile = {}\n    login = admin_details.parent\n    profile['firstname'] = admin_details.firstname\n    … … … … … …\n    profile['password'] = login.password \n    await asyncio.sleep(1)",
      "content_length": 2077,
      "extraction_method": "Direct"
    },
    {
      "page_number": 282,
      "chapter": null,
      "content": "Implementing coroutines\n259\n    return profile\nasync def extract_condensed(profiles):\n    profile_info = \" \".join([profiles['firstname'], \n       profiles['lastname'], profiles['username'], \n       profiles['password']])\n    await asyncio.sleep(1)\n    return profile_info \nasync def decrypt_profile(profile_info):\n    key = Fernet.generate_key()\n    fernet = Fernet(key)\n    encoded_profile = fernet.encrypt(profile_info.encode())\n    return encoded_profile\nThese three subroutines will give the main coroutine the encrypted str that contains the details \nof an administrator profile. All these encrypted strings will be collated by the API service using the \nasyncio.gather() utility.\nAnother programming approach to utilizing the coroutine switching is the use of pipelines created by \nasyncio.Queue. In this programming design, the queue structure is the common point between \ntwo tasks: (a) the task that will place a value to the queue called the producer, and (b) the task that \nwill fetch the item from the queue, the consumer. We can implement a one producer/one consumer \ninteraction or a multiple producers/multiple consumers setup with this approach. \nThe following code highlights the process_billing() native service that builds a producer/\nconsumer transaction flow. The extract_billing() coroutine is the producer that retrieves \nthe billing records from the database and passes each record one at a time to the queue. build_\nbilling_sheet(), on the other hand, is the consumer that fetches the record from the queue \nstructure and generates the billing sheet:\nasync def process_billing(query_list):\n    billing_list = []\n    \n    async def extract_billing(qlist, q: Queue):\n        assigned_billing = {}\n        for record in qlist:\n            await asyncio.sleep(2)\n            assigned_billing['admin_name'] = \"{} {}\"\n              .format(record.firstname, record.lastname)",
      "content_length": 1893,
      "extraction_method": "Direct"
    },
    {
      "page_number": 283,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n260\n            if not len(record.children) == 0:\n                assigned_billing['billing_items'] = \n                      record.children\n            else:\n                assigned_billing['billing_items'] = None\n            \n            await q.put(assigned_billing)\n    async def build_billing_sheet(q: Queue):\n        while True: \n            await asyncio.sleep(2)\n            assigned_billing = await q.get()\n            name = assigned_billing['admin_name']\n            billing_items = \n                assigned_billing['billing_items']\n            if not billing_items == None:\n                for item in billing_items:\n                    billing_list.append(\n                    {'admin_name': name, 'billing': item})\n            else: \n                billing_list.append(\n                    {'admin_name': name, 'billing': None})\n            q.task_done()\nIn this programming design, the build_billing() coroutine will explicitly wait for the record \nqueued by extract_billing(). This setup is possible due to the asyncio.create_task() \nutility, which directly assigns and schedules a task to each coroutine. \nThe queue is the only method parameter common to the coroutines because it is their common \npoint. The join()of asyncio.Queue ensures that all the items passed to the pipeline by \nextract_billing() are fetched and processed by build_billing_sheet(). It also blocks \nthe external controls that would affect the coroutine interactions. The following code shows how to \ncreate asyncio.Queue and schedule a task for execution:\n    q = asyncio.Queue()\n    build_sheet = asyncio.create_task(\n               build_billing_sheet(q))\n    await asyncio.gather(asyncio.create_task(\n             extract_billing(query_list, q)))",
      "content_length": 1803,
      "extraction_method": "Direct"
    },
    {
      "page_number": 284,
      "chapter": null,
      "content": "Creating asynchronous background tasks\n261\n    \n    await q.join()\n    build_sheet.cancel()\n    return billing_list\nBy the way, always pass cancel()to the task right after its coroutine has completed the process. On \nthe other hand, we can also apply other ways so that the performance of our coroutines can improve.\nUsing the HTTP/2 protocol\nCoroutine execution can be faster in applications running on the HTTP/2 protocol. We can replace \nthe Uvicorn server with Hypercorn, which now supports ASGI-based frameworks such as FastAPI. \nBut first, we need to install hypercorn using pip:\npip install hypercorn\nFor HTTP/2 to work, we need to create an SSL certificate. Using OpenSSL, our app has two PEM files \nfor our newsstand prototype: (a) the private encryption (key.pem) and (b) the certificate information \n(cert.pem.) We place these files in the main project folder before executing the following hypercorn \ncommand to run our FastAPI application:\nhypercorn --keyfile key.pem --certfile cert.pem main:app       \n--bind 'localhost:8000' --reload\nNow, let us explore other FastAPI tasks that can also use coroutines.\nCreating asynchronous background tasks\nIn Chapter 2, Exploring the Core Features, we first showcased the BackgroundTasks injectable \nAPI class, but we didn’t mention creating asynchronous background tasks. In this discussion, we will \nbe focusing on creating asynchronous background tasks using the asyncio module and coroutines.\nUsing the coroutines\nThe framework supports the creation and execution of asynchronous background processes using the \nasync/await structure. The following native service is an asynchronous transaction that generates \na billing sheet in CSV format in the background:\nasync def generate_billing_sheet(billing_date, query_list):\n    filepath = os.getcwd() + '/data/billing-' + \n                  str(billing_date) +'.csv'\n    with open(filepath, mode=\"a\") as sheet:\n        for vendor in query_list:",
      "content_length": 1947,
      "extraction_method": "Direct"
    },
    {
      "page_number": 285,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n262\n            billing = vendor.children\n            for record in billing:\n                if billing_date == record.date_billed:\n                    entry = \";\".join(\n             [str(record.date_billed), vendor.account_name, \n              vendor.account_number, str(record.payable),\n              str(record.total_issues) ])\n                    sheet.write(entry)\n                await asyncio.sleep(1) \nThis generate_billing_sheet() coroutine service will be executed as a background task in \nthe following API service, save_vendor_billing():\n@router.post(\"/billing/save/csv\")\nasync def save_vendor_billing(billing_date:date, \n              tasks: BackgroundTasks):\n    repo = BillingVendorRepository()\n    result = await repo.join_vendor_billing()\n    tasks.add_task(generate_billing_sheet, \n            billing_date, result)\n    tasks.add_task(create_total_payables_year, \n            billing_date, result)\n    return {\"message\" : \"done\"}\nNow, nothing has changed when it comes to defining background processes. We usually inject \nBackgroundTasks into the API service method and apply add_task() to provide task schedules, \nassignments, and execution for a specific process. But since the approach is now to utilize coroutines, \nthe background task will use the event loop instead of waiting for the current thread to finish its jobs.\nIf the background process requires arguments, we pass these arguments to add_task() right after its \nfirst parameter. For instance, the arguments for the billing_date and query_list parameters \nof generate_billing_sheet() should be placed after the generate_billing_sheet \ninjection into add_task(). Moreover, the billing_date value should be passed before \nthe result argument because add_task() still follows the order of parameter declaration in \ngenerate_billing_sheet() to avoid a type mismatch.\nAll asynchronous background tasks will continuously execute and will not be awaited even if their \ncoroutine API service has already returned a response to the client.",
      "content_length": 2073,
      "extraction_method": "Direct"
    },
    {
      "page_number": 286,
      "chapter": null,
      "content": "Understanding Celery tasks\n263\nCreating multiple tasks\nBackgroundTasks allows the creation of multiple asynchronous transactions that will execute \nconcurrently in the background. In the save_vendor_billing() service, there is another task \ncreated for a new transaction called the create_total_payables_year() transaction, which \nrequires the same arguments as generate_billing_sheet(). Again, this newly created task \nwill be utilizing the event loop instead of the thread.\nThe application always encounters performance issues when the background processes have high-\nCPU workloads. Also, tasks generated by BackgroundTasks are not capable of returning values \nfrom the transactions. Let us look for another solution where tasks can manage high workloads and \nexecute processes with returned values.\nUnderstanding Celery tasks\nCelery is a non-blocking task queue that runs on a distributed system. It can manage asynchronous \nbackground processes that are huge and heavy with CPU workloads. It is a third-party tool, so we \nneed to install it first through pip:\npip install celery\nIt schedules and runs tasks concurrently on a single server or distributed environment. But it requires \na message transport to send and receive messages, such as Redis, an in-memory database that can be \nused as a message broker for messages in strings, dictionaries, lists, sets, bitmaps, and stream types. \nAlso, we can install Redis on Linux, macOS, and Windows. Now, after the installation, run its redis-\nserver.exe command to start the server. In Windows, the Redis service is set to run by default after \ninstallation, which causes a TCP bind listener error. So, we need to stop it before running the startup \ncommand. Figure 8.1 shows Windows Task Manager with the Redis service giving a Stopped status:\nFigure 8.1 – Stopping the Redis service",
      "content_length": 1835,
      "extraction_method": "Direct"
    },
    {
      "page_number": 287,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n264\nAfter stopping the service, we should now see Redis running as shown in Figure 8.2:\nFigure 8.2 – A running Redis server\nCreating and configuring the Celery instance\nBefore creating Celery tasks, we need a Celery instance placed in a dedicated module of our application. \nThe newsstand prototype has the Celery instance in the /services/billing.py module, and \nthe following is part of the code that shows the process of Celery instantiation:\nfrom celery import Celery\nfrom celery.utils.log import get_task_logger \ncelery = Celery(\"services.billing\",   \n   broker='redis://localhost:6379/0', \n   backend='redis://localhost', \n   include=[\"services.billing\", \"models\", \"config\"])\nclass CeleryConfig:\n    task_create_missing_queues = True\n    celery_store_errors_even_if_ignored = True\n    task_store_errors_even_if_ignored = True",
      "content_length": 892,
      "extraction_method": "Direct"
    },
    {
      "page_number": 288,
      "chapter": null,
      "content": "Understanding Celery tasks\n265\n    task_ignore_result = False\n    task_serializer = \"pickle\"\n    result_serializer = \"pickle\"\n    event_serializer = \"json\"\n    accept_content = [\"pickle\", \"application/json\", \n          \"application/x-python-serialize\"]\n    result_accept_content = [\"pickle\", \"application/json\",\n          \"application/x-python-serialize\"]\ncelery.config_from_object(CeleryConfig)\ncelery_log = get_task_logger(__name__)\nTo create the Celery instance, we need the following details:\n•\t The name of the current module containing the Celery instance (the first argument)\n•\t The URL of Redis as our message broker (broker)\n•\t The backend result where the results of tasks are stored and monitored (backend)\n•\t The list of other modules used in the message body or by the Celery task (include)\nAfter the instantiation, we need to set the appropriate serializer and content types to process the incoming \nand outgoing message body of the tasks involved, if there are any. To allow the passing of full Python \nobjects with non-JSON-able values, we need to include pickle as a supported content type, then \ndeclare a default task and result serializer to the object stream. However, using a pickle serializer \nposes some security issues because it tends to expose some transaction data. To avoid compromising \nthe app, apply sanitation to message objects, such as removing sensitive values or credentials, before \npursuing the messaging operation. \nApart from the serialization options, other important properties such as task_create_missing_\nqueues, task_ignore_result, and error-related configuration should also be part of the \nCeleryConfig class. Now, we declare all these details in a custom class, which we will inject into \nthe config_from_object() method of the Celery instance.\nAdditionally, we can create a Celery logger through its get_task_logger()with the name of \nthe current task.",
      "content_length": 1902,
      "extraction_method": "Direct"
    },
    {
      "page_number": 289,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n266\nCreating the task\nThe main goal of the Celery instance is to annotate Python methods to become tasks. The Celery \ninstance has a task() decorator that we can apply to all callable procedures we want to define as \nasynchronous tasks. Part of the task() decorator is the task’s name, an optional unique name \ncomposed of the package, module name(s), and the method name of the transaction. It has other \nattributes that can add more refinement to the task definition, such as the auto_retry list, which \nregisters Exception classes that may cause execution retries when emitted, and max_tries, \nwhich limits the number of retry executions of a task. By the way, Celery 5.2.3 and below can only \ndefine tasks from non-coroutine methods. \nThe services.billing.tasks.create_total_payables_year_celery task shown \nhere adds all the payable amounts per date and returns the total amount:\n@celery.task(\n    name=\"services.billing.tasks\n            .create_total_payables_year_celery\", \n                auto_retry=[ValueError, TypeError], \n                  max_tries=5)\ndef create_total_payables_year_celery(billing_date,\n              query_list):\n        total = 0.0\n        for vendor in query_list:\n            billing = vendor.children\n            for record in billing:\n                if billing_date == record.date_billed:\n                    total += record.payable      \n        celery_log.info('computed result: ' + str(total))\n        return total   \nThe given task has only five (5) retries to recover when it encounters either ValueError or TypeError \nat runtime. Also, it is a function that returns a computed amount, which is impossible to create when \nusing BackgroundTasks. All functional tasks use the Redis database as the temporary storage \nfor their returned values, which is the reason there is a backend parameter in the Celery constructor.",
      "content_length": 1921,
      "extraction_method": "Direct"
    },
    {
      "page_number": 290,
      "chapter": null,
      "content": "Understanding Celery tasks\n267\nCalling the task\nFastAPI services can call these tasks using the apply_async()or delay()function. The latter \nis the easier option since it is preconfigured and only needs the parameters for the transaction to get \nthe result. The apply_async() function is a better option since it accepts more details that can \noptimize the task execution. These details are queue, time_limit, retry, ignore_result, \nexpires, and some kwargs of arguments. But both these functions return an AsyncResult \nobject, which returns resources such as the task’s state, the wait() function to help the task finish \nits operation, and the get() function to return its computed value or an exception. The following \ncode is a coroutine API service that calls the services.billing.tasks.create_total_\npayables_year_celery task using the apply_async method:\n@router.post(\"/billing/total/payable\")\nasync def compute_payables_yearly(billing_date:date):\n    repo = BillingVendorRepository()\n    result = await repo.join_vendor_billing()\n    total_result = create_total_payables_year_celery\n       .apply_async(queue='default', \n            args=(billing_date, result))\n    total_payable = total_result.get(timeout=1)\n    return {\"total_payable\": total_payable }\nSetting task_create_missing_queues to True at the CeleryConfig setup is always \nrecommended because it automatically creates the task queue, default or not, once the worker server \nstarts. The worker server places all the loaded tasks in a task queue for execution, monitoring, and \nresult retrieval. Thus, we should always define a task queue in the apply_async() function’s \nargument before extracting AsyncResult.\nThe AsyncResult object has a get() method that releases the returned value of the task from \nthe AsyncResult instance, with or without a timeout. In the compute_payables_yearly() \nservice, the amount payable in AsyncResult is retrieved by the get() function with a timeout \nof 5 seconds. Let us now deploy and run our tasks using the Celery server\nStarting the worker server\nRunning the Celery worker creates a single process that handles and manages all the queued tasks. \nThe worker needs to know in which module the Celery instance is created, together with the tasks to \nestablish the server process. In our prototype, the services.billing module is where we place \nour Celery application. Thus, the complete command to start the worker is the following:\ncelery  -A services.billing worker -Q default -P solo -c 2 -l \ninfo",
      "content_length": 2506,
      "extraction_method": "Direct"
    },
    {
      "page_number": 291,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n268\nHere, -A specifies the module of our Celery object and tasks. The -Q option indicates that the worker will \nbe using a low-, normal-, or high-priority queue. But first, we need to set task_create_missing_\nqueues to True in the Celery setup. We also need to indicate the number of threads that the worker \nneeds for task execution by adding the -c option. The -P option specifies the type of thread pool that \nthe worker will be utilizing. By default, the Celery worker uses the prefork pool applicable to most \nCPU-bound transactions. Other options are solo, eventlet, and gevent, but our setup will be utilizing \nsolo, the most suitable choice for running CPU-intensive tasks in a microservice environment. On the \nother hand, the -l option enables the logger we set using get_task_logger() during the setup. \nNow, there are also ways to monitor our running tasks and one of those options is to use the Flower tool.\nMonitoring the tasks\nFlower is Celery’s monitoring tool that observes and monitors all tasks executions by generating a \nreal-time audit on a web-based platform. But first, we need to install it using pip:\npip install flower\nAnd then, we run the following celery command with the flower option:\ncelery -A services.billing flower\nTo view the audit, we run http://localhost:5555/tasks on a browser. Figure 8.3 shows \na Flower snapshot of an execution log incurred by the services.billing.tasks.create_\ntotal_payables_year_celery task:\nFigure 8.3 – The Flower monitoring tool",
      "content_length": 1554,
      "extraction_method": "Direct"
    },
    {
      "page_number": 292,
      "chapter": null,
      "content": "Building message-driven transactions using RabbitMQ\n269\nSo far, we have used Redis as our in-memory backend database for task results and a message broker. \nLet us now use another asynchronous message broker that can replace Redis, RabbitMQ. \nBuilding message-driven transactions using RabbitMQ\nRabbitMQ is a lightweight asynchronous message broker that supports multiple messaging protocols \nsuch as AMQP, STOM, WebSocket, and MQTT. It requires erlang before it works properly in Windows, \nLinux, or macOS. Its installer can be downloaded from https://www.rabbitmq.com/\ndownload.html.\nCreating the Celery instance\nInstead of using Redis as the broker, RabbitMQ is a better replacement as a message broker that will \nmediate messages between the client and the Celery worker threads. For multiple tasks, RabbitMQ \ncan command the Celery worker to work on these tasks one at a time. The RabbitMQ broker is good \nfor huge messages and it saves these messages to disk memory.\nTo start, we need to set up a new Celery instance that will utilize the RabbitMQ message broker using \nits guest account. We will use the AMQP protocol as the mechanism for a producer/consumer type of \nmessaging setup. Here is the snippet that will replace the previous Celery configuration:\ncelery = Celery(\"services.billing\",   \n    broker='amqp://guest:guest@127.0.0.1:5672',   \n    result_backend='redis://localhost:6379/0', \n    include=[\"services.billing\", \"models\", \"config\"])\nRedis will still be the backend resource, as indicated in Celery’s backend_result, since it is still \nsimple and easy to control and manage when message traffic increases. Let us now use the RabbitMQ \nto create and manage message-driven transactions.\nMonitoring AMQP messaging\nWe can configure the RabbitMQ management dashboard to monitor the messages handled by \nRabbitMQ. After the setup, we can log in to the dashboard using the account details to set the broker. \nFigure 8.4 shows a screenshot of RabbitMQ’s analytics of a situation where the API services called the \nservices.billing.tasks.create_total_payables_year_celery task several times:",
      "content_length": 2105,
      "extraction_method": "Direct"
    },
    {
      "page_number": 293,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n270\nFigure 8.4 – The RabbitMQ management tool\nIf the RabbitMQ dashboard fails to capture the behavior of the tasks, the Flower tool will always be \nan option for gathering the details about the arguments, kwargs, UUID, state, and processing date \nof the tasks. And if RabbitMQ is not the right messaging tool, we can always resort to Apache Kafka.\nBuilding publish/subscribe messaging using Kafka\nAs with RabbitMQ, Apache Kafka is an asynchronous messaging tool used by applications to send and \nstore messages between producers and consumers. However, it is faster than RabbitMQ because it \nuses topics with partitions where producers can append various types of messages across these minute \nfolder-like structures. In this architecture, the consumers can consume all these messages in a parallel \nmode, unlike in queue-based messaging, which enables producers to send multiple messages to a queue \nthat can only allow message consumption sequentially. Within this publish/subscribe architecture, \nKafka can handle an exchange of large quantities of data per second in continuous and real-time mode.\nThere are three Python extensions that we can use to integrate the FastAPI services with Kafka, namely \nthe kafka-python, confluent-kafka, and pykafka extensions. Our online newsstand \nprototype will use kafka-python, so we need to install it using the pip command:\npip install kafka-python",
      "content_length": 1453,
      "extraction_method": "Direct"
    },
    {
      "page_number": 294,
      "chapter": null,
      "content": "Building publish/subscribe messaging using Kafka\n271\nAmong the three extensions, it is only with kafka-python that we can channel and apply Java \nAPI libraries to Python for the implementation of a client. We can download Kafka from https://\nkafka.apache.org/downloads. \nRunning the Kafka broker and server\nKafka has a ZooKeeper server that manages and synchronizes the exchange of messages within Kafka’s \ndistributed system. The ZooKeeper server runs as the broker that monitors and maintains the Kafka \nnodes and topics. The following command starts the server:\nC:\\..\\kafka\\bin\\windows\\zookeeper-server-start.bat            \nC:\\..\\kafka\\config\\zookeeper.properties\nNow, we can start the Kafka server by running the following console command:\nC:\\..\\kafka\\bin\\windows\\kafka-server-start.bat                \nC:\\..\\kafka\\config\\server.properties\nBy default, the server will run on localhost at port 9092.\nCreating the topic\nWhen the two servers have started, we can now create a topic called newstopic through the \nfollowing command:\nC:\\..\\kafka-topics.bat --create --bootstrap-server             \nlocalhost:9092 --replication-factor 1 --partitions 3         \n--topic newstopic\nThe newstopic topic has three (3) partitions that will hold all the appended messages of our \nFastAPI services. These are also the points where the consumers will simultaneously access all the \npublished messages.\nImplementing the publisher\nAfter creating the topic, we can now implement a producer that publishes messages to the Kafka cluster. \nThe kafka-python extension has a KafkaProducer class that instantiates a single thread-safe \nproducer for all the running FastAPI threads. The following is an API service that sends a newspaper \nmessenger record to the Kafka newstopic topic for the consumer to access and process:\nfrom kafka import KafkaProducer\nproducer = KafkaProducer(",
      "content_length": 1861,
      "extraction_method": "Direct"
    },
    {
      "page_number": 295,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n272\n     bootstrap_servers='localhost:9092')\ndef json_date_serializer(obj):\n    if isinstance(obj, (datetime, date)):\n        return obj.isoformat()\n    raise TypeError (\"Data %s not serializable\" % \n             type(obj))\n@router.post(\"/messenger/kafka/send\")\nasync def send_messnger_details(req: MessengerReq): \n    messenger_dict = req.dict(exclude_unset=True)\n    producer.send(\"newstopic\", \n       bytes(str(json.dumps(messenger_dict, \n          default=json_date_serializer)), 'utf-8')) \n    return {\"content\": \"messenger details sent\"}\nThe coroutine API service, send_messenger_details(), asks for details about a newspaper \nmessenger and stores them in a BaseModel object. And then, it sends the dictionary of profile details \nto the cluster in byte format. Now, one of the options to consume Kafka tasks is to run its built-in \nkafka-console-consumer.bat command.\nRunning a consumer on a console\nRunning the following command from the console is one way to consume the current messages from \nthe newstopic topic:\nkafka-console-consumer.bat --bootstrap-server                                                                \n                                                                                  \n127.0.0.1:9092 --topic newstopic\nThis command creates a consumer that will connect to the Kafka cluster to read in real time the current \nmessages from newtopic sent by the producer. Figure 8.5 shows the capture of the consumer while \nit is running on the console:\nFigure 8.5 – The Kafka consumer",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 296,
      "chapter": null,
      "content": "Implementing asynchronous Server-Sent Events (SSE)\n273\nIf we want the consumer to read all the messages sent by the producer starting from the point where \nthe Kafka server and broker began running, we need to add the --from-beginning option to \nthe command. The following will read all the messages from newstopic and continuously capture \nincoming messages in real time:\nkafka-console-consumer.bat --bootstrap-server 127.0.0.1:9092 \n--topic newstopic --from-beginning\nAnother way of implementing a consumer using the FastAPI framework is through SSE. Typical \nAPI service implementation will not work with the Kafka consumer requirement since we need a \ncontinuously running service that subscribes to newstopic for real-time data. So, let us now explore \nhow we create SSE in the FastAPI framework and how it will consume Kafka messages. \nImplementing asynchronous Server-Sent Events (SSE)\nSSE is a server push mechanism that sends data to the browser without reloading the page. Once \nsubscribed, it generates event-driven streams in real time for various purposes. \nCreating SSE in the FastAPI framework only requires the following:\n•\t The EventSourceResponse class from the sse_starlette.see module\n•\t An event generator\nAbove all, the framework also allows non-blocking implementation of the whole server push \nmechanism using coroutines that can run even on HTTP/2. The following is a coroutine API service \nthat implements a Kafka consumer using SSE’s open and lightweight protocol:\nfrom sse_starlette.sse import EventSourceResponse\n@router.get('/messenger/sse/add')\nasync def send_message_stream(request: Request):\n        \n    async def event_provider():\n        while True:\n            if await request.is_disconnected():\n                break\n            message = consumer.poll()\n            if not len(message.items()) == 0:\n                for tp, records in message.items():\n                   for rec in records:",
      "content_length": 1930,
      "extraction_method": "Direct"
    },
    {
      "page_number": 297,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n274\n                     messenger_dict = \n                      json.loads(rec.value.decode('utf-8'),\n                       object_hook=date_hook_deserializer )\n                                             \n                     repo = MessengerRepository()\n                     result = await \n                      repo.insert_messenger(messenger_dict)\n                     id = uuid4()\n                     yield {\n                       \"event\": \"Added … status: {},  \n                           Received: {}\". format(result, \n                            datetime.utcfromtimestamp(\n                               rec.timestamp // 1000)\n                               .strftime(\"%B %d, %Y \n                                      [%I:%M:%S %p]\")),\n                       \"id\": str(id),\n                       \"retry\": SSE_RETRY_TIMEOUT,\n                       \"data\": rec.value.decode('utf-8')\n                      }\n            \n            await asyncio.sleep(SSE_STREAM_DELAY)\n    return EventSourceResponse(event_provider())\nsend_message_stream() is a coroutine API service that implements the whole SSE. It returns \na special response generated by an EventSourceResponse function. While the HTTP stream is \nopen, it continuously retrieves data from its source and converts any internal events into SSE signals \nuntil the connection is closed. \nOn the other hand, event generator functions create internal events, which can also be asynchronous. \nsend_message_stream(), for instance, has a nested generator function, event_provider(), \nwhich consumes the last message sent by the producer service using the consumer.poll() method. \nIf the message is valid, the generator converts the message retrieved into a dict object and inserts \nall its details into the database through MessengerRepository. Then, it yields all the internal \ndetails for the EventSourceResponse function to convert into SSE signals. Figure 8.6 shows the \ndata streams generated by send_message_stream()rendered from the browser:",
      "content_length": 2068,
      "extraction_method": "Direct"
    },
    {
      "page_number": 298,
      "chapter": null,
      "content": "Building an asynchronous WebSocket\n275\nFigure 8.6 – The SSE data streams\nAnother way to implement a Kafka consumer is through WebSocket. But this time, we will focus \non the general procedure of how to create an asynchronous WebSocket application using the \nFastAPI framework.\nBuilding an asynchronous WebSocket\nUnlike in SSE, connection in WebSocket is always bi-directional, which means the server and client \ncommunicate with each other using a long TCP socket connection. The communication is always in \nreal time and it doesn’t require the client or the server to reply to every event sent.\nImplementing the asynchronous WebSocket endpoint\nThe FastAPI framework allows the implementation of an asynchronous WebSocket that can also run \non the HTTP/2 protocol. The following is an example of an asynchronous WebSocket created using \nthe coroutine block:\nimport asyncio\nfrom fastapi import WebSocket\n@router.websocket(\"/customer/list/ws\")\nasync def customer_list_ws(websocket: WebSocket):\n    await websocket.accept()\n    repo = CustomerRepository()",
      "content_length": 1052,
      "extraction_method": "Direct"
    },
    {
      "page_number": 299,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n276\n    result = await repo.get_all_customer()\n    \n    for rec in result:\n        data = rec.to_dict()\n        await websocket.send_json(json.dumps(data, \n           default=json_date_serializer))\n        await asyncio.sleep(0.01)\n        client_resp = await websocket.receive_json()\n        print(\"Acknowledging receipt of record id \n           {}.\".format(client_resp['rec_id']))\n    await websocket.close()    \nFirst, we decorate a coroutine function with @router.websocket() when using APIRouter, or \n@api.websocket() when using the FastAPI decorator to declare a WebSocket component. The \ndecorator must also define a unique endpoint URL for the WebSocket. Then, the WebSocket function \nmust have an injected WebSocket as its first method argument. It can also include other parameters \nsuch as query and header parameters.\nThe WebSocket injectable has four ways for sending messages, namely send(), send_text(), \nsend_json(), and send_bytes(). Applying send() will always manage every message as \nplain text by default. The previous customer_list_ws()coroutine is a WebSocket that sends \nevery customer record in JSON format.\nOn the other hand, there are also four methods the WebSocket injectable can provide, and these \nare the receive(), receive_text(), receive_json(), and receive_bytes() \nmethods. The receive() method expects the message to be in plain-text format by default. Now, \nour customer_list_ws() endpoint expects a JSON reply from a client because it invokes the \nreceive_json() method after its send message operation. \nThe WebSocket endpoint must close the connection right after its transaction is done.\nImplementing the WebSocket client\nThere are many ways to create a WebSocket client but this chapter will focus on utilizing a coroutine \nAPI service that will perform a handshake with the asynchronous customer_list_ws() endpoint \nonce called on a browser or a curl command. Here is the code of our WebSocket client implemented \nusing the websockets library that runs on top of the asyncio framework:\nimport websockets\n@router.get(\"/customer/wsclient/list/\")  \nasync def customer_list_ws_client():",
      "content_length": 2187,
      "extraction_method": "Direct"
    },
    {
      "page_number": 300,
      "chapter": null,
      "content": "Applying reactive programming in tasks\n277\n    uri = \"ws://localhost:8000/ch08/customer/list/ws\"\n    async with websockets.connect(uri) as websocket:\n        while True:\n           try:\n             res = await websocket.recv()\n             data_json = json.loads(res, \n                object_hook=date_hook_deserializer)\n                   \n             print(\"Received record: \n                       {}.\".format(data_json))\n                   \n             data_dict = json.loads(data_json)\n             client_resp = {\"rec_id\": data_dict['id'] }\n             await websocket.send(json.dumps(client_resp))\n                    \n           except websockets.ConnectionClosed:\n                 break\n        return {\"message\": \"done\"}\nAfter a successful handshake is created by the websockets.connect() method, customer_\nlist_ws_client() will have a loop running continuously to fetch all incoming consumer details \nfrom the WebSocket endpoint. The message received will be converted into its dictionary needed \nby other processes. Now, our client also sends an acknowledgment notification message back to the \nWebSocket coroutine with JSON data containing the customer ID of the profile. The loop will stop \nonce the WebSocket endpoint closes its connection.\nLet us now explore other asynchronous programming features that can work with the FastAPI framework.\nApplying reactive programming in tasks\nReactive programming is a paradigm that involves the generation of streams that undergo a series \nof operations to propagate some changes during the process. Python has an RxPY library that offers \nseveral methods that we can apply to these streams asynchronously to extract the terminal result as \ndesired by the subscribers. \nIn the reactive programming paradigm, all intermediate operators working along the streams will \nexecute to propagate some changes if there is an Observable instance beforehand and an Observer \nthat subscribes to this instance. The main goal of this paradigm is to achieve the desired result at the \nend of the propagation process using functional programming.",
      "content_length": 2088,
      "extraction_method": "Direct"
    },
    {
      "page_number": 301,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n278\nCreating the Observable data using coroutines\nIt all starts with the implementation of a coroutine function that will emit these streams of data based \non a business process. The following is an Observable function that emits publication details in \nstr format for those publications that did well in sales:\nimport asyncio\nfrom rx.disposable import Disposable\nasync def process_list(observer):\n      repo = SalesRepository()\n      result = await repo.get_all_sales()\n      \n      for item in result:\n        record = \" \".join([str(item.publication_id),  \n          str(item.copies_issued), str(item.date_issued), \n          str(item.revenue), str(item.profit), \n          str(item.copies_sold)])\n        cost = item.copies_issued * 5.0\n        projected_profit = cost - item.revenue\n        diff_err = projected_profit - item.profit\n        if (diff_err <= 0):\n            observer.on_next(record)\n        else:\n            observer.on_error(record)\n      observer.on_completed()\nAn Observable function can be synchronous or asynchronous. Our target is to create an asynchronous \none such as process_list(). The coroutine function should have the following callback methods \nto qualify as an Observable function:\n•\t An on_next() method that emits items given a certain condition\n•\t An on_completed() method that is executed once when the function has completed \nthe operation\n•\t An on_error() method that is called when an error occurs on Observable",
      "content_length": 1514,
      "extraction_method": "Direct"
    },
    {
      "page_number": 302,
      "chapter": null,
      "content": "Applying reactive programming in tasks\n279\nOur process_list() emits the details of the publication that gained some profit. Then, we create \nan asyncio task for the call of the process_list() coroutine. We created a nested function, \nevaluate_profit(), which returns the Disposable task required by RxPY’s create() \nmethod for the production of the Observable stream. The cancellation of this task happens when \nthe Observable stream is all consumed. The following is the complete implementation for the \nexecution of the asynchronous Observable function and the use of the create() method to \ngenerate streams of data from this Observable function:\ndef create_observable(loop):\n    def evaluate_profit(observer, scheduler):\n        task = asyncio.ensure_future(\n            process_list(observer), loop=loop)\n        return Disposable(lambda: task.cancel())\n    return rx.create(evaluate_profit)\nThe subscriber created by create_observable()is our application’s list_sales_by_quota() \nAPI service. It needs to get the current event loop running for the method to generate the observable. \nAfterward, it invokes the subscribe() method to send a subscription to the stream and extract \nthe needed result. The Observable’s subscribe() method is invoked for a client to subscribe to \nthe stream and observe the occurring propagations:\n@router.get(\"/sales/list/quota\")\nasync def list_sales_by_quota():\n    loop = asyncio.get_event_loop()\n    observer = create_observable(loop)\n    \n    observer.subscribe(\n        on_next=lambda value: print(\"Received Instruction \n              to buy {0}\".format(value)),\n        on_completed=lambda: print(\"Completed trades\"),\n        on_error=lambda e: print(e),\n        scheduler = AsyncIOScheduler(loop)   \n    )\n    return {\"message\": \"Notification \n           sent to the background\"}",
      "content_length": 1821,
      "extraction_method": "Direct"
    },
    {
      "page_number": 303,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n280\nThe list_sales_by_quote() coroutine service shows us how to subscribe to an Observable. \nA subscriber should utilize the following callback methods: \n•\t An on_next() method to consume all the items from the stream\n•\t An on_completed() method to indicate the end of the subscription \n•\t An on_error() method to flag an error during the subscription process\nAnd since the Observable processes run asynchronously, the scheduler is an optional argument \nthat provides the right manager to schedule and run these processes. The API service used \nAsyncIOScheduler as the appropriate schedule for the subscription. But there are other shortcuts \nto generating Observables that do not use a custom function.\nCreating background process\nAs when we create continuously running Observables, we use the interval() function instead of \nusing a custom Observable function. Some observables are designed to end successfully, but some \nare created to run continuously in the background. The following Observable runs in the background \nperiodically to provide some updates on the total amount received from newspaper subscriptions:\nimport asyncio\nimport rx\nimport rx.operators as ops\nasync def compute_subscriptions():\n    total = 0.0\n    repo = SubscriptionCustomerRepository()\n    result = await repo.join_customer_subscription_total()\n    \n    for customer in result:\n        subscription = customer.children\n        for item in subscription:\n            total = total + (item.price * item.qty)\n    await asyncio.sleep(1)\n    return total\ndef fetch_records(rate, loop) -> rx.Observable:\n    return rx.interval(rate).pipe(\n        ops.map(lambda i: rx.from_future(\n          loop.create_task(compute_subscriptions()))),",
      "content_length": 1770,
      "extraction_method": "Direct"
    },
    {
      "page_number": 304,
      "chapter": null,
      "content": "Applying reactive programming in tasks\n281\n        ops.merge_all()\n    )\nThe interval() method creates a stream of data periodically in seconds. But this Observable imposes \nsome propagations on its stream because of the execution of the pipe() method. The Observable’s \npipe() method creates a pipeline of reactive operators called the intermediate operators. This pipeline \ncan consist of a chain of operators running one at a time to change items from the streams. It seems \nthat this series of operations creates multiple subscriptions on the subscriber. So, fetch_records() \nhas a map() operator in its pipeline to extract the result from the compute_subcription() \nmethod. It uses merge_all() at the end of the pipeline to merge and flatten all substreams created \ninto one final stream, the stream expected by the subscriber. Now, we can also generate Observable \ndata from files or API response.\nAccessing API resources\nAnother way of creating an Observable is using the from_() method, which extracts resources from \nfiles, databases, or API endpoints. The Observable function retrieves its data from a JSON document \ngenerated by an API endpoint from our application. The assumption is that we are running the \napplication using hypercorn, which uses HTTP/2, and so we need to bypass the TLS certificate by \nsetting the verify parameter of httpx.AsyncClient() to False. \nThe following code highlights the from_() in the fetch_subscription() operation, which \ncreates an Observable that emits streams of str data from the https://localhost:8000/\nch08/subscription/list/all endpoint. These reactive operators of the Observable, namely \nfilter(), map(), and merge_all(), are used to propagate the needed contexts along the stream:\nasync def fetch_subscription(min_date:date, \n         max_date:date, loop) -> rx.Observable:\n    headers = {\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\"\n        }\n    async with httpx.AsyncClient(http2=True, \n             verify=False) as client:\n        content = await \n          client.get('https://localhost:8000/ch08/\n            subscription/list/all', headers=headers)\n    y = json.loads(content.text)\n    source = rx.from_(y)\n    observable = source.pipe(\n      ops.filter(lambda c: filter_within_dates(",
      "content_length": 2294,
      "extraction_method": "Direct"
    },
    {
      "page_number": 305,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n282\n               c, min_date, max_date)),\n      ops.map(lambda a: rx.from_future(loop.create_task(\n            convert_str(a)))),\n      ops.merge_all(),\n    )\n    return observable\nThe filter() method is another pipeline operator that returns Boolean values from a validation \nrule. It executes the following filter_within_dates() to verify whether the record retrieved \nfrom the JSON document is within the date range specified by the subscriber:\ndef filter_within_dates(rec, min_date:date, max_date:date):\n    date_pur = datetime.strptime(\n             rec['date_purchased'], '%Y-%m-%d')\n    if date_pur.date() >= min_date and \n             date_pur.date() <= max_date:\n        return True\n    else:\n        return False\nOn the other hand, the following convert_str() is a coroutine function executed by the map() \noperator to generate a concise profile detail of the newspaper subscribers derived from the JSON data: \nasync def convert_str(rec):\n    if not rec == None:\n        total = rec['qty'] * rec['price']\n        record = \" \".join([rec['branch'], \n            str(total), rec['date_purchased']])\n        await asyncio.sleep(1)\n        return record\nRunning these two functions modifies the original emitted data stream from JSON to a date-filtered \nstream of str data. The coroutine list_dated_subscription()API service, on the other \nhand, subscribes to fetch_subscription() to extract the newspaper subscriptions within the \nmin_date and max_date range:\n@router.post(\"/subscription/dated\")\nasync def list_dated_subscription(min_date:date, \n            max_date:date):",
      "content_length": 1642,
      "extraction_method": "Direct"
    },
    {
      "page_number": 306,
      "chapter": null,
      "content": "Customizing events\n283\n     \n    loop = asyncio.get_event_loop()\n    observable = await fetch_subscription(min_date, \n             max_date, loop)\n    \n    observable.subscribe(\n       on_next=lambda item: \n         print(\"Subscription details: {}.\".format(item)),\n       scheduler=AsyncIOScheduler(loop)\n    )\nAlthough the FastAPI framework does not yet fully support reactive programming, we can still create \ncoroutines that can work with various RxPY utilities. Now, we will explore how coroutines are not \nonly for background processes but also for FastAPI event handlers.\nCustomizing events\nThe FastAPI framework has special functions called event handlers that execute before the application \nstarts up and during shutdown. These events are activated every time the uvicorn or hypercorn \nserver reloads. Event handlers can also be coroutines.\nDefining the startup event\nThe startup event is an event handler that the server executes when it starts up. We decorate the \nfunction with the @app.on_event(\"startup\") decorator to create a startup event. Applications \nmay require a startup event to centralize some transactions, such as the initial configuration of some \ncomponents or the set up of data-related resources. The following example is the application startup \nevent that opens a database connection for the GINO repository transactions:\napp = FastAPI()\n@app.on_event(\"startup\")\nasync def initialize():\n    engine = await db.set_bind(\"postgresql+asyncpg://\n          postgres:admin2255@localhost:5433/nsms\")\nThis initialize() event is defined in our application’s main.py file so that GINO can only \ncreate the connection once every server reload or restart.",
      "content_length": 1673,
      "extraction_method": "Direct"
    },
    {
      "page_number": 307,
      "chapter": null,
      "content": "Creating Coroutines, Events, and Message-Driven Transactions\n284\nDefining shutdown events\nMeanwhile, the shutdown event cleans up unwanted memory, destroys unwanted connections, and logs \nthe reason for shutting down the application. The following is the shutdown event of our application \nthat closes the GINO database connection:\n@app.on_event(\"shutdown\")\nasync def destroy():\n    engine, db.bind = db.bind, None\n    await engine.close()\nWe can define startup and shutdown events in APIRouter but be sure this will not cause transaction \noverlapping or collision with other routers. Moreover, event handlers do not work in mounted \nsub-applications.\nSummary\nThe use of coroutines is one of the factors that makes the FastAPI microservice application fast, aside \nfrom its use of an ASGI-based server. This chapter has proven that using coroutines to implement API \nservices will improve the performance better than utilizing more threads in the thread pool. Since \nthe framework runs on an asyncio platform, we can utilize asyncio utilities to design various design \npatterns to manage the CPU-bound and I/O-bound services. \nThis chapter used Celery and Redis for creating and managing asynchronous background tasks \nfor behind-the-scenes transactions such as logging, system monitoring, time-sliced computations, \nand batch jobs. We learned that RabbitMQ and Apache Kafka provided an integrated solution for \nbuilding asynchronous and loosely coupled communication between FastAPI components, especially \nfor the message-passing part of these interactions. Most importantly, coroutines were applied to \ncreate these asynchronous and non-blocking background processes and message-passing solutions \nto enhance performance. Reactive programming was also introduced in this chapter through the \nRxPy extension module.\nThis chapter, in general, concludes that the FastAPI framework is ready to build a microservice \napplication that has a reliable, asynchronous, message-driven, real-time message-passing, and distributed \ncore system. The next chapter will highlight other FastAPI features that provide integrations with \nUI-related tools and frameworks, API documentation using OpenAPI Specification, session handling, \nand circumventing CORS.",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 308,
      "chapter": null,
      "content": "Part 3: \nInfrastructure-Related Issues, \nNumerical and Symbolic \nComputations, and Testing \nMicroservices\nIn this final part of the book, we will discuss other essential microservice features, such as distributed \ntracing and logging, service registries, virtual environments, and API metrics. Serverless deployment \nusing Docker and Docker Compose with NGINX as a reverse proxy will also be covered. Furthermore, \nwe will look at FastAPI as a framework for building scientific applications using numerical algorithms \nfrom the numpy, scipy, sympy, and pandas modules to model, analyze, and visualize the \nmathematical and statistical solutions of its API services. \nThis part comprises the following chapters:\n•\t Chapter 9, Utilizing Other Advanced Features\n•\t Chapter 10, Solving Numerical, Symbolic, and Graphical Problems \n•\t Chapter 11, Adding Other Microservice Features",
      "content_length": 876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 309,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 310,
      "chapter": null,
      "content": "9\nUtilizing Other \nAdvanced Features\nThe previous chapters have already showcased several essential core features of the FastAPI framework. \nHowever, there are features not truly inherent to the framework that can help fine-tune performance \nand patch missing links in our implementations. These include session handling, managing Cross-\nOrigin Resource Sharing (CORS)-related issues, and selecting the appropriate rendition types for \nan application.\nAside from the built-in features, there are workaround solutions proven to work with FastAPI when \napplied to the application, such as its session handling mechanism, which can function well using a \nJWT, and SessionMiddleware. Concerning middleware, this chapter will also explore ways of \ncustomizing request and response filters other than applying the @app.middleware decorator. \nOther issues such as using custom APIRoute and Request will be covered in this chapter to guide \nus on managing incoming byte body, form, or JSON data. Moreover, this chapter will highlight how to \ntest FastAPI components using the pytest framework and the fastapi.testclient library \nand how we can document the endpoint using the OpenAPI 3.x specification.\nOverall, the main objective of this chapter is to provide us with other solutions that can help us complete \nour microservice applications. In this chapter, the following topics are included:\n•\t Applying session management\n•\t Managing the CORS mechanism\n•\t Customizing APIRoute and Request\n•\t Choosing appropriate responses\n•\t Applying the OpenAPI 3.x specification\n•\t Testing the API endpoints",
      "content_length": 1589,
      "extraction_method": "Direct"
    },
    {
      "page_number": 311,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n288\nTechnical requirements\nAlthough not data analysis-related, our application prototype for this chapter is the online restaurant \nreview system, which will gather ordinal and nominal ratings and feedback for restaurants. The \nsoftware aims to gather rates and feedback to establish the user profiles of restaurants and conduct \nsurveys concerning their food menus, facilities, ambiance, and services. The prototype will use \nMongoDB as the data storage and asynchronous ODMantic as its ORM. All of the code is uploaded \nto https://github.com/PacktPublishing/Building-Python-Microservices-\nwith-FastAPI under the ch09 project.\nApplying session management\nSession management is a feature used for managing requests and responses created by a user’s access \nto an application. It is also about creating and sharing data across a user session. Many frameworks \nusually include session handling features in their security plugins but not FastAPI. Creating user \nsessions and storing session data are two separate programming concerns in FastAPI. We use a JWT \nto establish a user session and Starlette’s SessionMiddleware to create and retrieve session data. \nCreating user sessions and storing session data are two entirely different programming solutions in \nFastAPI. We use JWT to establish a user session and Starlette’s SessionMiddleware to create \nand retrieve session data.\nCreating user sessions\nWe have already proven the importance of JWT when it comes to securing FastAPI microservice \napplications in Chapter 7, Securing the REST APIs. However, here, the JWT is applied to create a session \nbased on user credentials. In the api/login.py router, an authenticate() API service is \nimplemented to create a user session for an authenticated user. It is inherent for FastAPI to generate \nuser sessions utilizing the browser cookies. The following snippet shows the authentication process \nthat uses the cookie values:\nfrom util.auth_session import secret_key\nfrom jose import jwt\n@router.post(\"/login/authenticate\")\nasync def authenticate(username:str, password: str, \n   response: Response, engine=Depends(create_db_engine)):\n    repo:LoginRepository = LoginRepository(engine)\n    login = await repo.get_login_username(username, \n                       password)\n    if login == None:\n            raise HTTPException(",
      "content_length": 2357,
      "extraction_method": "Direct"
    },
    {
      "page_number": 312,
      "chapter": null,
      "content": "Applying session management\n289\n                status_code=status.HTTP_403_FORBIDDEN, \n                detail=\"Invalid authentication\"\n            )\n    token = jwt.encode({\"sub\": username}, secret_key)\n    response.set_cookie(\"session\", token)\n    return {\"username\": username}\nThe service will verify through LoginRepository whether the user is a valid account using its \nusername and password credentials. If the user is a certified one, it will use a JWT to create a \ntoken derived from a certain secret_key generated using the following command:\nopenssl rand -hex 32\nThe token key will serve as the session ID of the cookie-based session. With the username credential \nas its payload, the JWT will be stored as a browser cookie named session.\nTo ensure that session has been applied, all subsequent requests must undergo authentication by the \ncookie-based session through the APIKeyCookie class, an API class of the fastapi.security \nmodule that implements cookie-based authentication. The APIKeyCookie class fetches the session \nbefore it is injected into a dependable function for the JWT decoding through the secret_key value \nused to generate the session ID. The following dependable function in util/auth_session.py \nwill verify every access to each endpoint of the application: \nfrom fastapi.security import APIKeyCookie\nfrom jose import jwt\ncookie_sec = APIKeyCookie(name=\"session\")\nsecret_key = \"pdCFmblRt4HWKNpWkl52Jnq3emH3zzg4b80f+4AFVC8=\"\nasync def get_current_user(session: str = \n   Depends(cookie_sec), engine=Depends(create_db_engine)):\n    try:\n        payload = jwt.decode(session, secret_key)\n        repo:LoginRepository = LoginRepository(engine)\n        login = await repo.validate_login(\n                    payload[\"sub\"])\n        if login == None:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN, \n                detail=\"Invalid authentication\"",
      "content_length": 1914,
      "extraction_method": "Direct"
    },
    {
      "page_number": 313,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n290\n            )\n        else:\n            return login\n    except Exception:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN, \n            detail=\"Invalid authentication\"\n        )\nThe preceding function is injected into every API endpoint to impose user session verification. \nWhen an endpoint is requested, this function will decode the token and extract the username \ncredential for account validation. Then, it will issue Status Code 403 (Forbidden) if the user is an \nunauthenticated one or the session is not valid. An example of an authenticated service can be found \nin the following implementation:\nfrom util.auth_session import get_current_user\n@router.post(\"/restaurant/add\")\nasync def add_restaurant(req:RestaurantReq, \n         engine=Depends(create_db_engine), \n         user: str = Depends(get_current_user)):\n    restaurant_dict = req.dict(exclude_unset=True) \n    restaurant_json = dumps(restaurant_dict, \n              default=json_datetime_serializer)\n    repo:RestaurantRepository = \n             RestaurantRepository(engine)\n    result = await repo.insert_restaurant(\n               loads(restaurant_json))\n    if result == True: \n        return req \n    else: \n        return JSONResponse(content={\"message\": \n         \"insert login unsuccessful\"}, status_code=500)\nThe add_restaurant() service is an endpoint that adds a restaurant Document to the MongoDB \ncollection. But before the transaction proceeds, it checks first whether there is a cookie-based session \nthrough the injected get_current_user() dependable function.",
      "content_length": 1613,
      "extraction_method": "Direct"
    },
    {
      "page_number": 314,
      "chapter": null,
      "content": "Applying session management\n291\nManaging session data\nUnfortunately, adding and retrieving session data is not part of APIKeyCookie-based session \nauthentication. The JWT payload must only include the username but not all credentials and body of data. \nTo manage session data, we need to create a separate session using Starlette’s SessionMiddleware. \nAlthough FastAPI has its fastapi.middleware module, it still supports Starlette’s built-in middleware.\nWe mentioned middleware in Chapter 2, Exploring the Core Features, and showed its implementation \nusing the @app.middleware decorator. And we have proven that it acts as a filter for all incoming \nrequests and outgoing responses to the services. This time, we will not custom implement a middleware \nbut built-in middleware classes.\nMiddleware is implemented, configured, and activated in the main.py module where the \ninstance of FastAPI is located because APIRouter cannot add middleware. We enable the \nmiddleware parameter of the FastAPI constructor and add to that List-type parameter the built-in \nSessionMiddleware with its secret_key and the name of the new session as constructor \nparameters using the injectable class, Middleware. The following code snippet of main.py shows \nyou how to configure this:\nfrom starlette.middleware.sessions import SessionMiddleware\napp = FastAPI(middleware=[\n        Middleware(SessionMiddleware, \n        secret_key=\n            '7UzGQS7woBazLUtVQJG39ywOP7J7lkPkB0UmDhMgBR8=', \n        session_cookie=\"session_vars\")])\nAnother way of adding middleware is to utilize the add_middleware() function of the FastAPI \ndecorator. Initially, adding SessionMiddleware will create another cookie-based session that \nwill handle session-scoped data. It is the only way since there is no direct support from FastAPI \nregarding session handling mechanisms where a user session is created not only for security but also \nfor handling session objects.\nTo add session data to our newly created session, session_vars, we need to inject Request into \neach endpoint service and utilize its session dictionary to store the session-scoped objects. The following \nlist_restaurants() service retrieves the list of restaurants from the database, extracts all the \nrestaurant names, and shares the list of names across the session through request.session[]:\n@router.get(\"/restaurant/list/all\")\nasync def list_restaurants(request: Request, \n       engine=Depends(create_db_engine), \n       user: str = Depends(get_current_user)):",
      "content_length": 2500,
      "extraction_method": "Direct"
    },
    {
      "page_number": 315,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n292\n    repo:RestaurantRepository = \n             RestaurantRepository(engine)\n    result = await repo.get_all_restaurant()\n    resto_names = [resto.name for resto in result]\n    request.session['resto_names'] = resto_names\n    return result\n@router.get(\"/restaurant/list/names\")\nasync def list_restaurant_names(request: Request, \n           user: str = Depends(get_current_user)):\n    resto_names = request.session['resto_names']\n    return resto_names\nOn the other hand, the list_restaurant_names() service retrieves the resto_names \nsession data through request.session[] and returns it as its response. By the way, it is due to \nSessionMiddleware that session[] exists. Otherwise, the use of this dictionary will raise \nan change to - exception.\nRemoving the sessions\nIt is always mandatory to log out from the application when done with the transactions to remove \nall the sessions created. Since the easiest and most direct way of creating sessions is through browser \ncookies, removing all the sessions protects the application from any compromise. The following \n/ch09/logout endpoint removes our sessions, session and session_vars, which \ntechnically logs out the user from the application:\n@router.get(\"/logout\")\nasync def logout(response: Response, \n            user: str = Depends(get_current_user)):\n    response.delete_cookie(\"session\")\n    response.delete_cookie(\"session_vars\")\n    return {\"ok\": True}\nThe delete_cookie() method of the Response class removes any existing browser session \nutilized by the application.",
      "content_length": 1567,
      "extraction_method": "Direct"
    },
    {
      "page_number": 316,
      "chapter": null,
      "content": "Applying session management\n293\nCustomizing BaseHTTPMiddleware\nThe default approach in managing FastAPI sessions is through cookies, and it does not offer any other \noptions such as database-backed, cached, and file-based sessions. The best way to implement non-cookie-\nbased strategies for managing user sessions and session data is to customize BaseHTTPMiddleware. \nThe following custom middleware is a prototype that creates user sessions for authenticated users:\nfrom repository.login import LoginRepository\nfrom repository.session import DbSessionRepository\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom datetime import date, datetime\nimport re\nfrom odmantic import AIOEngine\nfrom motor.motor_asyncio import AsyncIOMotorClient\nclass SessionDbMiddleware(BaseHTTPMiddleware):\n    def __init__(self, app, sess_key: str, \n                    sess_name:str, expiry:str):\n        super().__init__(app)\n        self.sess_key = sess_key\n        self.sess_name = sess_name \n        self.expiry = expiry\n        self.client_od = \n         AsyncIOMotorClient(f\"mongodb://localhost:27017/\")\n        self.engine = \n         AIOEngine(motor_client=self.client_od, \n            database=\"orrs\")\n                \n    async def dispatch(self, request: Request, call_next):\n        try:\n            if re.search(r'\\bauthenticate\\b', \n                    request.url.path):\n                credentials = request.query_params\n                username = credentials['username']\n                password = credentials['password']\n                repo_login:LoginRepository =",
      "content_length": 1573,
      "extraction_method": "Direct"
    },
    {
      "page_number": 317,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n294\n                      LoginRepository(self.engine)\n                repo_session:DbSessionRepository = \n                      DbSessionRepository(self.engine)\n               \n                login = await repo_login.\n                  get_login_credentials(username, password)\n               \n                if login == None:\n                    self.client_od.close()\n                    return JSONResponse(status_code=403) \n                else:\n                    token = jwt.encode({\"sub\": username}, \n                        self.sess_key)\n                    sess_record = dict()\n                    sess_record['session_key'] = \n                        self.sess_key\n                    sess_record['session_name'] = \n                        self.sess_name\n                    sess_record['token'] = token\n                    sess_record['expiry_date'] = \n                       datetime.strptime(self.expiry, \n                            '%Y-%m-%d')\n                    await repo_session.\n                        insert_session(sess_record)\n                    self.client_od.close()\n                    response = await call_next(request)\n                    return response\n            else:\n                response = await call_next(request)\n                return response\n        except Exception as e :\n            return JSONResponse(status_code=403)",
      "content_length": 1407,
      "extraction_method": "Direct"
    },
    {
      "page_number": 318,
      "chapter": null,
      "content": "Managing the CORS mechanism\n295\nAs discussed in Chapter 2, Exploring the Core Features, middleware is a low-level implementation of \na filter for all requests and responses of the applications. So, first, SessionDbMiddleware will \nfilter our /ch09/login/authenticate endpoint for the username and password query \nparameters, check whether the user is a registered one, and generate a database-backed session from \nthe JWT. Afterward, endpoints can validate all their requests from the session stored in the database. \nThe /ch09/logout endpoint will not include the deletion of the session from the database using \nits repository transactions, as shown in the following code:\n@router.get(\"/logout\")\nasync def logout(response: Response, \n       engine=Depends(create_db_engine), \n       user: str = Depends(get_current_user)):\n    repo_session:DbSessionRepository = \n             DbSessionRepository(engine)\n    await repo_session.delete_session(\"session_db\")\n    return {\"ok\": True}\nNote that DbSessionRepository is a custom repository implementation for our prototype, \nand it has a delete_session() method that will remove the session through its name from the \ndb_session collection of our MongoDB database. \nAnother type of middleware that can help FastAPI applications resolve issues regarding the CORS \nbrowser mechanism is CORSMiddleware. \nManaging the CORS mechanism\nWhen integrating API endpoints with various frontend frameworks, we often encounter the \"no ‘access-\ncontrol-allow-origin’ header present\" error from our browser. Nowadays, this setup is an HTTP-header-\nbased mechanism of any browser, which requires the backend server to provide the browser with the \n\"origin\" details of the server-side application, which includes the server domain, scheme, and port. \nThis mechanism is called CORS, which happens when the frontend application and its web resources \nbelong to a different domain area than the backend app. Nowadays, browsers prohibit cross-origin \nrequests between the server-side and frontend applications for security reasons.",
      "content_length": 2054,
      "extraction_method": "Direct"
    },
    {
      "page_number": 319,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n296\nTo resolve this issue, we need our main.py module to place all the origins of our application \nand other integrated resources used by the prototype inside a List. Then, we import the built-in \nCORSMiddleware from the fastapi.middleware.cors module and add that to the \nFastAPI constructor with the list of origins, which should not be too long to avoid overhead from \nvalidating each URL. The following code snippet shows the injection of CORSMiddleware into \nthe FastAPI constructor:\norigins = [\n    \"https://192.168.10.2\",\n    \"http://192.168.10.2\",\n    \"https://localhost:8080\",\n    \"http://localhost:8080\"\n]\napp = FastAPI(middleware=[\n           Middleware(SessionMiddleware, secret_key=\n            '7UzGQS7woBazLUtVQJG39ywOP7J7lkPkB0UmDhMgBR8=', \n               session_cookie=\"session_vars\"),\n           Middleware(SessionDbMiddleware, sess_key=\n            '7UzGQS7woBazLUtVQJG39ywOP7J7lkPkB0UmDhMgBR8=',\n              sess_name='session_db', expiry='2020-10-10')\n            ])\napp.add_middleware(CORSMiddleware, max_age=3600,\n     allow_origins=origins, allow_credentials=True,\n     allow_methods= [\"POST\", \"GET\", \"DELETE\", \n       \"PATCH\", \"PUT\"], allow_headers=[\n            \"Access-Control-Allow-Origin\", \n            \"Access-Control-Allow-Credentials\", \n            \"Access-Control-Allow-Headers\",\n            \"Access-Control-Max-Age\"])\nThis time, we used FastAPI’s add_middleware() function to add CORS support to our application. \nAside from allow_origins, we also need to add into CORSMiddleware the allow_credentials \nparameter, which adds Access-Control-Allow-Credentials: true to the response header \nfor the browser to recognize the domain origin matches and send an Authorization cookie to \nallow the request. Also, we must include the allow_headers parameter, which registers a list of \nacceptable header keys during browser interaction. Aside from Accept, Accept-Language, \nContent-Language, and Content-Type, which are included by default, we need to register \nAccess-Control-Allow-Origin, Access-Control-Allow-Credentials, Access-\nControl-Allow-Headers, and Access-Control-Max-Age explicitly instead of using the",
      "content_length": 2176,
      "extraction_method": "Direct"
    },
    {
      "page_number": 320,
      "chapter": null,
      "content": "Customizing APIRoute and Request\n297\nasterisk (*). The allow_headers parameter must also be part of the middleware to specify other \nHTTP methods that need to be supported by the browser. And lastly, the max_age parameter must \nalso be in the configuration because we need to tell the browser the amount of time it will cache all \nthe resources loaded into the browser.\nIf the application needs additional CORS support features, customizing the CORSMiddleware to \nextend some built-in utilities and features to manage CORS is a better solution. \nBy the way, it is not only the middleware that we can subclass and use to create custom implementations \nof but also the Request data and API routes. \nCustomizing APIRoute and Request\nMiddleware can process incoming Request data and outgoing Response objects of all API \nmethods in a FastAPI application, except that it cannot manipulate the message body, attach state \nobjects from the Request data, or modify the response object before the client consumes it. Only \nAPIRoute and Request customization can give us a full grasp of how to control the request and \nresponse transaction. The control might include determining whether the incoming data is a byte \nbody, form, or JSON and providing an effective logging mechanism, exception handling, content \ntransformation, and extraction.  \nManaging body, form, or JSON data\nUnlike in middleware, customizing APIRoute does not apply to all the API endpoints. Implementing \nAPIRoute for some APIRouter will only impose new routing rules to those affected endpoints, \nwhile the other services can pursue the default request and response process. For instance, the \nfollowing customization is responsible for data extraction that only applies to the endpoints of api.\nroute_extract.router:\nfrom fastapi.routing import APIRoute\nfrom typing import Callable\nfrom fastapi import Request, Response\nclass ExtractContentRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = \n                super().get_route_handler()\n        \n        async def custom_route_handler(request: Request) \n                    -> Response:\n            request = ExtractionRequest(request.scope,",
      "content_length": 2198,
      "extraction_method": "Direct"
    },
    {
      "page_number": 321,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n298\n                        request.receive)\n            response: Response = await \n                    original_route_handler(request)\n            return response\n        return custom_route_handler\nCustomizing APIRoute requires the creation of a Python closure that will directly manage the \nRequest and Response flow from APIRoute’s original_route_handler. On the other \nhand, our ExtractContentRoute filter uses a custom ExtractionRequest that identifies \nand processes each type of incoming request data separately. The following is the implementation of \nExtractionRequest that will replace the default Request object: \nclass ExtractionRequest(Request):\n    async def body(self):\n        body = await super().body()\n        data = ast.literal_eval(body.decode('utf-8'))\n        if isinstance(data, list):\n            sum = 0\n            for rate in data:\n                sum += rate \n            average = sum / len(data)\n            self.state.sum = sum \n            self.state.avg = average\n        return body \n    \n    async def form(self):\n        body = await super().form()\n        user_details = dict()\n        user_details['fname'] = body['firstname']\n        user_details['lname'] = body['lastname']\n        user_details['age'] = body['age']\n        user_details['bday'] = body['birthday']\n        self.session[\"user_details\"] = user_details\n        return body\n    \n    async def json(self):\n        body = await super().json()",
      "content_length": 1479,
      "extraction_method": "Direct"
    },
    {
      "page_number": 322,
      "chapter": null,
      "content": "Customizing APIRoute and Request\n299\n        if isinstance(body, dict):\n                \n            sum = 0\n            for rate in body.values():\n                sum += rate  \n                    \n            average = sum / len(body.values())\n            self.state.sum = sum \n            self.state.avg = average\n        return body\nTo activate this ExtractionRequest, we need to set the route_class of the APIRouter of \nthe endpoints to ExtractContentRoute, as shown in the following snippet:\nrouter = APIRouter()\nrouter.route_class = ExtractContentRoute\nThere are three methods of choice to override when managing various request bodies:\n•\t body(): This manages incoming request data that is in bytes\n•\t form(): This processes incoming form data\n•\t json(): This manages incoming parsed JSON data\n•\t stream(): This accesses the body via a chunk of bytes using the async for construct\nAll of these methods return the original request body in bytes back to the service.\nIn ExtractionRequest, we have implemented three interface methods from the given choices to \nfilter and process all incoming requests of the API endpoints defined in the /api/route_extract.\npy module.\nThe following create_profile() service accepts profile data from the client and implements \nthe ExtractContentRoute filter, which will store all of this profile data in the dictionary using \nsession handling:\n@router.post(\"/user/profile\")\nasync def create_profile(req: Request, \n        firstname: str = Form(...), \n        lastname: str = Form(...), age: int = Form(...), \n        birthday: date = Form(...), \n        user: str = Depends(get_current_user)):",
      "content_length": 1632,
      "extraction_method": "Direct"
    },
    {
      "page_number": 323,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n300\n    user_details = req.session[\"user_details\"]\n    return {'profile' : user_details} \nThe overridden form() method of ExtractionRequest is responsible for the user_details \nattribute containing all the user details.\nOn the other hand, the given set_ratings() method has an incoming dictionary of various \nratings in which the json() override will derive some basic statistics. All the results will be returned \nas Request’s state objects or request attributes:\n@router.post(\"/rating/top/three\")\nasync def set_ratings(req: Request, data : \n Dict[str, float], user: str = Depends(get_current_user)):\n    stats = dict()\n    stats['sum'] = req.state.sum\n    stats['average'] = req.state.avg\n    return {'stats' : stats } \nAnd lastly, the preceding compute_data() service will have an incoming list of ratings as \na source of some basic statistics like in the previous service. The body() method override of \nExtractionRequest will process the computation:\n@router.post(\"/rating/data/list\")\nasync def compute_data(req: Request, data: List[float], \n  user: str = Depends(get_current_user)):\n    stats = dict()\n    stats['sum'] = req.state.sum\n    stats['average'] = req.state.avg\n    return {'stats' : stats }\nEncrypting and decrypting the message body\nAnother scenario where we need to customize the routing of the endpoints is when we must secure \nthe message body through encryption. The following custom request decrypts an encrypted body \nusing Python’s cryptography module and the key of the encrypted body:\nfrom cryptography.fernet import Fernet\nclass DecryptRequest(Request):",
      "content_length": 1615,
      "extraction_method": "Direct"
    },
    {
      "page_number": 324,
      "chapter": null,
      "content": "Customizing APIRoute and Request\n301\n    async def body(self):\n        body = await super().body()\n        login_dict = ast.literal_eval(body.decode('utf-8'))\n        fernet = Fernet(bytes(login_dict['key'], \n             encoding='utf-8'))\n        data = fernet.decrypt(\n          bytes(login_dict['enc_login'], encoding='utf-8'))\n        self.state.dec_data = json.loads(\n             data.decode('utf-8'))\n        return body\nImportant note\nThe cryptography module requires the installation of the itsdangerous extension for \nthe encryption/decryption procedure used in this project.\nDecryptRequest will decrypt the message and return the list of login records as a request state \nobject. The following service provides the encrypted message body and key and returns the decrypted \nlist of login records from DecryptRequest as a response:\n@router.post(\"/login/decrypt/details\")\nasync def send_decrypt_login(enc_data: EncLoginReq, \n   req:Request, user: str = Depends(get_current_user)):\n    return {\"data\" : req.state.dec_data}\nNote that send_decrypt_login() has an EncLoginReq request model that contains the \nencrypted message body and the encryption key from the client. \nCustomizing the routes and their Request objects can help optimize and streamline microservice \ntransactions, especially those API endpoints that require heavy loads on message body conversions, \ntransformations, and computations.\nNow, our next discussion will focus on applying different Response types for the API services.",
      "content_length": 1503,
      "extraction_method": "Direct"
    },
    {
      "page_number": 325,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n302\nChoosing the appropriate responses\nThe FastAPI framework offers other options for rendering API endpoint responses other than the \nmost common JsonResponse option. Here is a list of some of the response types supported by \nFastAPI and their corresponding samples from our application:\n•\t The API endpoints can utilize the PlainTextResponse type if their response is text-based \nonly. The following intro_list_restaurants() service returns a text-based message \nto the client: \n@router.get(\"/restaurant/index\")\ndef intro_list_restaurants():\n  return PlainTextResponse(content=\"The Restaurants\")\n•\t Services can use RedirectResponse if they need to pursue navigation to another entirely \ndifferent application or another endpoint of the same application. The following endpoint jumps \nto a hypertext reference about some known Michelin-starred restaurants:\n@router.get(\"/restaurant/michelin\")\ndef redirect_restaurants_rates():\n  return RedirectResponse(\n      url=\"https://guide.michelin.com/en/restaurants\")\n•\t A FileResponse type can help services render some content of a file, preferably text-based \nfiles. The following load_questions() service shows the list of questions saved in the \nquestions.txt file placed inside the /file folder of the application:\n@router.get(\"/question/load/questions\")\nasync def load_questions(user: str = \n                    Depends(get_current_user)):\n    file_path = os.getcwd() + \n      '\\\\files\\\\questions.txt';\n    return FileResponse(path=file_path, \n                 media_type=\"text/plain\")\n•\t StreamingResponse is another response type that can provide us with another approach to \nthe Server-Sent Events (SSE) implementation. Chapter 8, Creating Coroutines, Events, and Message-\nDriven Transactions, has provided us with an SSE that utilizes the EventSourceResponse type:\n@router.get(\"/question/sse/list\")    \nasync def list_questions(req:Request, \n         engine=Depends(create_db_engine),",
      "content_length": 1972,
      "extraction_method": "Direct"
    },
    {
      "page_number": 326,
      "chapter": null,
      "content": "Choosing the appropriate responses\n303\n            user: str = Depends(get_current_user)):\n    async def print_questions():\n        repo:QuestionRepository = \n                QuestionRepository(engine)\n        result = await repo.get_all_question()\n        for q in result:\n            disconnected = await req.is_disconnected()\n            if disconnected:\n                break\n            yield 'data: {}\\n\\n.format(\n               json.dumps(jsonable_encoder(q), \n                      cls=MyJSONEncoder))\n            await asyncio.sleep(1)\n    return StreamingResponse(print_questions(), \n                media_type=\"text/event-stream\")\n•\t Services that render images can also use the StreamingResponse type. The following \nlogo_upload_png() service uploads any JPEG or PNG file and renders it in the browser:\n@router.post(\"/restaurant/upload/logo\")\nasync def logo_upload_png(logo: UploadFile = File(...)):\n    original_image = Image.open(logo.file)\n    original_image = \n         original_image.filter(ImageFilter.SHARPEN)\n    filtered_image = BytesIO()\n    if logo.content_type == \"image/png\":\n        original_image.save(filtered_image, \"PNG\")\n        filtered_image.seek(0)\n        return StreamingResponse(filtered_image, \n                 media_type=\"image/png\")\n    elif logo.content_type == \"image/jpeg\":\n        original_image.save(filtered_image, \"JPEG\")\n        filtered_image.seek(0)\n        return StreamingResponse(filtered_image,",
      "content_length": 1449,
      "extraction_method": "Direct"
    },
    {
      "page_number": 327,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n304\n               media_type=\"image/jpeg\") \n•\t The StreamingResponse type is also effective in rendering videos in various formats such \nas MP4. The following service reads a file inside the application named sample.mp4 and \npublishes it to the browser:\n@router.get(\"/restaurant/upload/video\")\ndef video_presentation():\n    file_path = os.getcwd() + '\\\\files\\\\sample.mp4'\n    def load_file():  \n        with open(file_path, mode=\"rb\") as video_file:  \n            yield from video_file  \n    return StreamingResponse(load_file(), \n              media_type=\"video/mp4\")\n•\t If the service wants to publish a simple HTML markup page without making references to static \nCSS or JavaScript files, then HTMLResponse is the right choice. The following service renders \nan HTML page with a Bootstrap framework provided by some CDN libraries:\n@router.get(\"/signup\")\nasync def signup(engine=Depends(create_db_engine), \n       user: str = Depends(get_current_user) ):\n   signup_content = \"\"\"\n    <html lang='en'>\n        <head>\n          <meta charset=\"UTF-8\">\n          <script src=\"https://code.jquery.com/jquery-\n                    3.4.1.min.js\"></script>\n          <link rel=\"stylesheet\" \n            href=\"https://stackpath.bootstrapcdn.com/\n              bootstrap/4.4.1/css/bootstrap.min.css\">\n          <script src=\"https://cdn.jsdelivr.net/npm/\n            popper.js@1.16.0/dist/umd/popper.min.js\">\n          </script>\n          <script",
      "content_length": 1470,
      "extraction_method": "Direct"
    },
    {
      "page_number": 328,
      "chapter": null,
      "content": "Choosing the appropriate responses\n305\n           src=\"https://stackpath.bootstrapcdn.com/\n       bootstrap/4.4.1/js/bootstrap.min.js\"></script>\n   \n        </head>\n        <body>\n          <div class=\"container\">\n            <h2>Sign Up Form</h2>\n            <form>\n                <div class=\"form-group\">\n                   <label for=\"firstname\">\n                          Firstname:</label>\n                   <input type='text' \n                       class=\"form-control\" \n                       name='firstname' \n                       id='firstname'/><br/>\n                </div>\n                … … … … … … … …\n                <div class=\"form-group\">\n                   <label for=\"role\">Role:</label>\n                   <input type='text' \n                     class=\"form-control\" \n                     name='role' id='role'/><br/>\n                </div>\n                <button type=\"submit\" class=\"btn \n                    btn-primary\">Sign Up</button>\n            </form>\n           </div>\n        </body>\n    </html>\n    \"\"\"\n    return HTMLResponse(content=signup_content, \n               status_code=200)",
      "content_length": 1122,
      "extraction_method": "Direct"
    },
    {
      "page_number": 329,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n306\n•\t If the API endpoints have other rendition types needed to be published, the Response \nclass can customize them through its media_type property. The following is a service that \nconverts JSON data into XML content by setting the media_type property of Response \nto the application/xml MIME type:  \n@router.get(\"/keyword/list/all/xml\")\nasync def \n   convert_to_xml(engine=Depends(create_db_engine), \n        user: str = Depends(get_current_user)): \n    repo:KeyRepository = KeyRepository(engine)\n    list_of_keywords = await repo.get_all_keyword()\n    root = minidom.Document() \n    xml = root.createElement('keywords') \n    root.appendChild(xml) \n  \n    for keyword in list_of_keywords:\n        key = root.createElement('keyword')\n        word = root.createElement('word')\n        key_text = root.createTextNode(keyword.word)\n        weight= root.createElement('weight')\n        weight_text = \n             root.createTextNode(str(keyword.weight))\n        word.appendChild(key_text)\n        weight.appendChild(weight_text)\n        key.appendChild(word)\n        key.appendChild(weight)\n        xml.appendChild(key)\n    xml_str = root.toprettyxml(indent =\"\\t\") \n    return Response(content=xml_str, \n            media_type=\"application/xml\")\nAlthough FastAPI is not a web framework, it can support Jinja2 templating for rare cases where API \nservices require rendering their response as an HTML page. Let us highlight how API services utilize \nJinja2 templates as part of the response.",
      "content_length": 1523,
      "extraction_method": "Direct"
    },
    {
      "page_number": 330,
      "chapter": null,
      "content": "Choosing the appropriate responses\n307\nSetting up the Jinja2 template engine\nFirst, we need to install the jinja2 module using pip: \npip install jinja2\nThen, we need to create a folder that will hold all the Jinja2 templates. Jinja2 must define this folder, \nusually named templates, by creating the Jinja2Templates instance in FastAPI or any \nAPIRouter. The following snippet is part of the /api/login.py router that shows the setup \nand configuration of the Jinja2 templating engine:\nfrom fastapi.templating import Jinja2Templates\nrouter = APIRouter()\ntemplates = Jinja2Templates(directory=\"templates\")\nSetting up the static resources\nAfter the templates folder, the Jinja2 engine requires the application to have a folder named \nstatic in the project directory to hold the CSS, JavaScript, images, and other static files for the \nJinja2 templates. Then, we need to instantiate the StaticFiles instance to define the static \nfolder and map it with a virtual name. Additionally, the StaticFiles instance must be mounted \nto a specific path through FastAPI’s mount() method. We also need to set the html property of \nthe StaticFiles instance to True to set the folder in HTML mode. The following configuration \nshows how to set up the static resource folder in the main.py module:\nfrom fastapi.staticfiles import StaticFiles\napp.mount(\"/static\", StaticFiles(directory=\"static\", \n          html=True), name=\"static\")\nFor the FastAPI components to access these static files, the engine needs the aiofiles \nextension installed: \npip install aiofiles",
      "content_length": 1546,
      "extraction_method": "Direct"
    },
    {
      "page_number": 331,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n308\nCreating the template layout\nThe following template is the base or parent template for the application that can now access the \nBootstrap resources from the static folder due to the template engine and aiofiles module:\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta http-equiv=\"X-UA-Compatible\" \n              content=\"IE=edge\">\n        <meta name=\"viewport\" content=\"width=device-width, \n             initial-scale=1.0, shrink-to-fit=no\">\n        <meta name=\"apple-mobile-web-app-capable\" \n             content=\"yes\">\n        \n        <link rel=\"stylesheet\" type=\"text/css\" \n            href=\"{{url_for('static', \n               path='/css/bootstrap.min.css')}}\">\n        <script src=\"{{url_for('static', path='/js/\n               jquery-3.6.0.js')}}\"></script>\n        <script src=\"{{url_for('static', \n              path='/js/bootstrap.min.js')}}\"></script>\n    </head>\n    <body>\n        {% block content %}\n        {% endblock content %}\n    </body>\n</html>\nOther templates can inherit the structure and design of this layout.html using the \n{% extends %} tags. The Jinja2 base template, like our layout.html, has these Jinja2 tags, \nnamely the {% block content %} and {% endblock %} tags, which indicate where child \ntemplates can insert their content during the translation phase. But for all these templates to work, \nthey must be saved in the /templates directory. The following is a sample child template named \nusers.html that generates a table of profiles from the context data:\n{% extends \"layout.html\" %}",
      "content_length": 1598,
      "extraction_method": "Direct"
    },
    {
      "page_number": 332,
      "chapter": null,
      "content": "Choosing the appropriate responses\n309\n{% block content %}\n<div class=\"container\">\n<h2>List of users </h2>\n<p>This is a Boostrap 4 table applied to JinjaTemplate.</p>\n<table class=\"table\">\n    <thead>\n        <tr>\n          <th>Login ID</th>\n          <th>Username</th>\n          <th>Password</th>\n          <th>Passphrase</th>\n        </tr>\n      </thead>\n      <tbody>\n    {% for login in data %} \n    <tr>\n        <td>{{ login.login_id}}</td>\n        <td>{{ login.username}}</td>\n        <td>{{ login.password}}</td>\n        <td>{{ login.passphrase}}</td>\n    </tr>\n    {% endfor%}\n</tbody>\n</table>\n</div>\n{% endblock %}\nObserve that the child Jinja2 template also has the \"block\" tags to mark the content to be merged into \nthe parent template. \nFor the API to render the templates, the service must use the Jinja2 engine’s TemplateResponse \ntype as the response type. TemplateResponse needs the filename of the template, the Request \nobject, and the context data if there is any. The following is the API service that renders the previous \nusers.html template:\n@router.get(\"/login/html/list\")\nasync def list_login_html(req: Request,\n       engine=Depends(create_db_engine),",
      "content_length": 1179,
      "extraction_method": "Direct"
    },
    {
      "page_number": 333,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n310\n       user: str = Depends(get_current_user)):\n    repo:LoginRepository = LoginRepository(engine)\n    result = await repo.get_all_login()\n    return templates.TemplateResponse(\"users.html\", \n           {\"request\": req, \"data\": result})\nUsing ORJSONResponse and UJSONResponse\nWhen it comes to yielding numerous dictionaries or JSON-able-components, it is appropriate to use \neither ORJSONResponse or UJSONResponse. ORJSONResponse uses orjson to serialize a \nhumongous listing of dictionary objects into a JSON string as a response. So, first, we need to install \norjson using the pip command before using ORJSONResponse. ORJSONResponse serializes \nUUID, numpy, data classes, and datetime objects faster than the common JSONResponse.\nHowever, UJSONResponse is relatively faster than ORJSONResponse because it uses the ujson \nserializer. The ujson serializer must first be installed before using UJSONResponse.\nThe following are the two API services that use these two fast alternatives for a JSON serializer:\n@router.get(\"/login/list/all\")\nasync def list_all_login(engine=Depends(create_db_engine), \n         user: str = Depends(get_current_user)): \n    repo:LoginRepository = LoginRepository(engine)\n    result = await repo.get_all_login()\n    return ORJSONResponse(content=jsonable_encoder(result),\n             status_code=201)\n@router.get(\"/login/account\")\nasync def get_login(id:int, \n       engine=Depends(create_db_engine), \n       user: str = Depends(get_current_user) ):\n    repo:LoginRepository = LoginRepository(engine)\n    result = await repo.get_login_id(id)\n    return UJSONResponse(content=jsonable_encoder(result), \n             status_code=201)\nWe still need to apply the jsonable_encoder() component to convert BSON’s ObjectId of \nthe result into str before the two responses pursue their serialization processes. Now, let us focus \non how we provide internal API documentation using the OpenAPI 3.0 specification.",
      "content_length": 1968,
      "extraction_method": "Direct"
    },
    {
      "page_number": 334,
      "chapter": null,
      "content": "Applying the OpenAPI 3.x specification\n311\nApplying the OpenAPI 3.x specification\nThe OpenAPI 3.0 specification is a standard API documentation and language-agnostic specification \nthat can describe the API services without knowing its sources, reading its documentation, and \nunderstanding its business logic. Additionally, FastAPI supports OpenAPI, and it can even automatically \ngenerate the default internal documentation of the API based on OpenAPI standards.\nThere are three ways to document our API services using the specification:\n•\t By extending the OpenAPI schema definition\n•\t By using the internal code base properties\n•\t By using the Query, Body, Form, and Path functions\nExtending the OpenAPI schema definition\nFastAPI has a get_openapi() method from its fastapi.openapi.utils extension that \ncan override some schema descriptions. We can modify the info, servers, and paths details of \nthe schema definition through the get_openapi() function. The function returns a dict of all \ndetails of the OpenAPI schema definition of the application.\nThe default OpenAPI schema documentation is always set up in the main.py module because it is \nconsistently associated with the FastAPI instance. For the function to generate the dict of schema \ndetails, it must accept at least the title, version, and routes parameter values. The following \ncustom function extracts the default OpenAPI schema for updating:\ndef update_api_schema():\n   DOC_TITLE = \"The Online Restaurant Rating System API\"\n   DOC_VERSION = \"1.0\"\n   openapi_schema = get_openapi(\n       title=DOC_TITLE,\n       version=DOC_VERSION,\n       routes=app.routes,\n   )\napp.openapi_schema = openapi_schema\nreturn openapi_schema",
      "content_length": 1693,
      "extraction_method": "Direct"
    },
    {
      "page_number": 335,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n312\nThe title parameter value is the document title, the version parameter value is the version of \nthe API implementation, and routes contains a list of registered API services. Observe that the last \nline before the return statement updates FastAPI’s built-in openapi_schema defaults. Now, to \nupdate the general information details, we use the info key of the schema definition to change some \nvalues, as shown in the following sample:\nopenapi_schema[\"info\"] = {\n       \"title\": DOC_TITLE,\n       \"version\": DOC_VERSION,\n       \"description\": \"This application is a prototype.\",\n       \"contact\": {\n           \"name\": \"Sherwin John Tragura\",\n           \"url\": \"https://ph.linkedin.com/in/sjct\",\n           \"email\": \"cowsky@aol.com\"\n       },\n       \"license\": {\n           \"name\": \"Apache 2.0\",\n           \"url\": \"https://www.apache.org/\n                  licenses/LICENSE-2.0.html\"\n       },\n   }\nThe preceding info schema update must also be part of the update_api_schema() function \ntogether with the update on the documentation of each registered API service. These details can \nincludeAPI service's description and summary, the POST endpoint's description of its requestBody \nand GET endpoint's details about its parameters, and the API tags. Add the following paths updates:\nopenapi_schema[\"paths\"][\"/ch09/login/authenticate\"][\"post\"]\n[\"description\"] = \"User Authentication Session\"\nopenapi_schema[\"paths\"][\"/ch09/login/authenticate\"][\"post\"]\n[\"summary\"] = \"This is an API that stores credentials in \nsession.\"\nopenapi_schema[\"paths\"][\"/ch09/login/authenticate\"][\"post\"]\n[\"tags\"] = [\"auth\"]\n   \nopenapi_schema[\"paths\"][\"/ch09/login/add\"][\"post\"]\n[\"description\"] = \"Adding Login User\"\nopenapi_schema[\"paths\"][\"/ch09/login/add\"][\"post\"]",
      "content_length": 1777,
      "extraction_method": "Direct"
    },
    {
      "page_number": 336,
      "chapter": null,
      "content": "Applying the OpenAPI 3.x specification\n313\n[\"summary\"] = \"This is an API adds new user.\"\nopenapi_schema[\"paths\"][\"/ch09/login/add\"][\"post\"]\n[\"tags\"] = [\"operation\"]\nopenapi_schema[\"paths\"][\"/ch09/login/add\"][\"post\"]\n[\"requestBody\"][\"description\"]=\"Data for LoginReq\"\n   \nopenapi_schema[\"paths\"][\"/ch09/login/profile/add\"]\n[\"description\"] = \"Updating Login User\"\nopenapi_schema[\"paths\"][\"/ch09/login/profile/add\"]\n[\"post\"][\"summary\"] = \"This is an API updating existing user \nrecord.\"\nopenapi_schema[\"paths\"][\"/ch09/login/profile/add\"]\n[\"post\"][\"tags\"] = [\"operation\"]\nopenapi_schema[\"paths\"][\"/ch09/login/profile/add\"]\n[\"post\"][\"requestBody\"][\"description\"]=\"Data for LoginReq\"\n   \nopenapi_schema[\"paths\"][\"/ch09/login/html/list\"][\"get\"]\n[\"description\"] = \"Renders Jinja2Template with context data.\"\nopenapi_schema[\"paths\"][\"/ch09/login/html/list\"][\"get\"]\n[\"summary\"] = \"Uses Jinja2 template engine for rendition.\"\nopenapi_schema[\"paths\"][\"/ch09/login/html/list\"][\"get\"][\"tags\"] \n= [\"rendition\"]\nopenapi_schema[\"paths\"][\"/ch09/login/list/all\"][\"get\"]\n[\"description\"] = \"List all the login records.\"\nopenapi_schema[\"paths\"][\"/ch09/login/list/all\"][\"get\"]\n[\"summary\"] = \"Uses JsonResponse for rendition.\"\nopenapi_schema[\"paths\"][\"/ch09/login/list/all\"][\"get\"][\"tags\"] \n= [\"rendition\"]\nThe preceding will give us a new OpenAPI document dashboard, as shown in Figure 9.1:",
      "content_length": 1367,
      "extraction_method": "Direct"
    },
    {
      "page_number": 337,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n314\nFigure 9.1 – A customized OpenAPI dashboard\nTags are essential variables of the OpenAPI documentation because they organize the API endpoints \naccording to the routers, business processes, requirements, and modules. It is a best practice to use tags.\nOnce all the updates have been set, replace FastAPI’s openapi() function with the new update_\napi_schema() function.\nUsing the internal code base properties\nFastAPI’s constructor has parameters that can replace the default info document details without using \nthe get_openapi() function. The following snippet showcases a sample documentation update \non the title, description, version, and servers details of the OpenAPI documentation:\napp = FastAPI(… … … …, \n            title=\"The Online Restaurant Rating \n                       System API\",\n            description=\"This a software prototype.\",\n            version=\"1.0.0\",\n            servers= [\n                {\n                    \"url\": \"http://localhost:8000\",\n                    \"description\": \"Development Server\"\n                },",
      "content_length": 1085,
      "extraction_method": "Direct"
    },
    {
      "page_number": 338,
      "chapter": null,
      "content": "Applying the OpenAPI 3.x specification\n315\n                {\n                    \"url\": \"https://localhost:8002\",\n                    \"description\": \"Testing Server\",\n                }\n            ])\nWhen adding documentation to the API endpoints, the path operators of FastAPI and APIRouter \nalso have parameters that allow changes to the default OpenAPI variables attributed to each endpoint. The \nfollowing is a sample service that updates its summary, description, response_description, \nand other response details through the post() path operator:\n@router.post(\"/restaurant/add\",\n     summary=\"This API adds new restaurant details.\",\n     description=\"This operation adds new record to the \n          database. \",\n     response_description=\"The message body.\",\n     responses={\n        200: {\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\n                        \"restaurant_id\": 100,\n                        \"name\": \"La Playa\",\n                        \"branch\": \"Manila\",\n                        \"address\": \"Orosa St.\",\n                        \"province\": \"NCR\",\n                        \"date_signed\": \"2022-05-23\",\n                        \"city\": \"Manila\",\n                        \"country\": \"Philippines\",\n                        \"zipcode\": 1603\n                    }\n                }\n            },\n        },\n        404: {\n            \"description\": \"An error was encountered during \n                     saving.\",",
      "content_length": 1476,
      "extraction_method": "Direct"
    },
    {
      "page_number": 339,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n316\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\"message\": \"insert login \n                       unsuccessful\"}\n                }\n            },\n        },\n    },\n    tags=[\"operation\"])\nasync def add_restaurant(req:RestaurantReq, \n        engine=Depends(create_db_engine), \n          user: str = Depends(get_current_user)):\n    restaurant_dict = req.dict(exclude_unset=True) \n    restaurant_json = dumps(restaurant_dict, \n           default=json_datetime_serializer)\n    repo:RestaurantRepository = \n            RestaurantRepository(engine)\n    result = await repo.insert_restaurant(\n              loads(restaurant_json))\n    if result == True: \n        return req \n    else: \n        return JSONResponse(content={\"message\": \n           \"insert login unsuccessful\"}, status_code=500)\nUsing the Query, Form, Body, and Path functions\nAside from the declaration and additional validations, the Query, Path, Form, and Body parameter \nfunctions can also be used to add some metadata to the API endpoints. The following authenticate() \nendpoint has added descriptions and validations through the Query() function:\n@router.post(\"/login/authenticate\")\nasync def authenticate(response: Response, \n    username:str = Query(..., \n       description='The username of the credentials.', \n       max_length=50), \n    password: str = Query(...,",
      "content_length": 1415,
      "extraction_method": "Direct"
    },
    {
      "page_number": 340,
      "chapter": null,
      "content": "Applying the OpenAPI 3.x specification\n317\n     description='The password of the of the credentials.', \n     max_length=20), \n    engine=Depends(create_db_engine)):\n    repo:LoginRepository = LoginRepository(engine)\n    … … … … … …\n    response.set_cookie(\"session\", token)\n    return {\"username\": username}\nThe following get_login() uses the Path() directive to insert a description of the id parameter:\n@router.get(\"/login/account/{id}\")\nasync def get_login(id:int = Path(..., \n         description=\"The user ID of the user.\"), \n   engine=Depends(create_db_engine), \n   user: str = Depends(get_current_user) ):\n    … … … … … …\n    return UJSONResponse(content=jsonable_encoder(result),\n         status_code=201)\nThe description and max_length metadata of the Query() function will become part of the \nOpenAPI documentation for authenticate(), as shown in Figure 9.2:\nFigure 9.2 – The Query metadata",
      "content_length": 900,
      "extraction_method": "Direct"
    },
    {
      "page_number": 341,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n318\nAdditionally, the description metadata of the Path() directive will also appear in the get_\nlogin() documentation, as presented in Figure 9.3:  \nFigure 9.3 – The Path metadata\nLikewise, we can add descriptions to form parameters using the Form directive. The following service \nshows you how to insert documentation through the Form directive: \n@router.post(\"/user/profile\")\nasync def create_profile(req: Request, \n        firstname: str = Form(..., \n          description='The first name of the user.'), \n        lastname: str = Form(..., \n          description='The last name of the user.'), \n        age: int = Form(..., \n          description='The age of the user.'), \n        birthday: date = Form(..., \n           description='The birthday of the user.'), \n        user: str = Depends(get_current_user)):\n    user_details = req.session[\"user_details\"]\n    return {'profile' : user_details}",
      "content_length": 933,
      "extraction_method": "Direct"
    },
    {
      "page_number": 342,
      "chapter": null,
      "content": "Applying the OpenAPI 3.x specification\n319\nMoreover, it is also possible to document all types of HTTP responses or status codes that the API \nservice can throw through the path operator’s responses parameter. The following video_\npresentation() service provides metadata regarding the nature of its response when it encounters \nno errors (HTTP Status Code 200) and with runtime errors (HTTP Status Code 500):\nfrom models.documentation.response import Error500Model\n… … … … …\n@router.get(\"/restaurant/upload/video\",responses={\n        200: {\n            \"content\": {\"video/mp4\": {}},\n            \"description\": \"Return an MP4 encoded video.\",\n        },\n        500:{\n            \"model\": Error500Model, \n            \"description\": \"The item was not found\"\n        }\n    },)\ndef video_presentation():\n    file_path = os.getcwd() + '\\\\files\\\\sample.mp4'\n    def load_file():  \n        with open(file_path, mode=\"rb\") as video_file:  \n            yield from video_file  \n    return StreamingResponse(load_file(), \n              media_type=\"video/mp4\")\nError500Model is a BaseModel class that will give you a clear picture of the response once \nthe application encounters an HTTP Status Code 500 error and will only be used in the OpenAPI \ndocumentation. It contains metadata such as the message that holds a hardcoded error message. Figure \n9.4 shows the resulting OpenAPI documentation for video_presentation() after adding the \nmetadata for its responses:",
      "content_length": 1455,
      "extraction_method": "Direct"
    },
    {
      "page_number": 343,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n320\nFigure 9.4 – The documentation for API responses\nAnd now, for our last discussion, let us explore how we can perform unit testing in FastAPI, which \ncould lead to a test-driven development setup.\nTesting the API endpoints\nFastAPI uses the pytest framework to run its test classes. So, before we create our test classes, first, \nwe need to install the pytest framework using the pip command:\npip install pytest\nFastAPI has a module called fastapi.testclient where all components are Request-based, \nincluding the TestClient class. To access all the API endpoints, we need the TestClient \nobject. But first, we need to create a folder such as test, which will contain test modules where we \nimplement our test methods. We place our test methods outside main.py or the router modules to \nmaintain clean code and organization.",
      "content_length": 860,
      "extraction_method": "Direct"
    },
    {
      "page_number": 344,
      "chapter": null,
      "content": "Testing the API endpoints\n321\nWriting the unit test cases\nIt is a best practice to write one test module per router component, except for cases where there is a \ntight connection between these routers. We place these test modules inside the test directory. To \npursue the automated testing, we need to import the APIRouter instance or the FastAPI instance \ninto the test module to set up TestClient. TestClient is almost like Python’s client module, \nrequests, when it comes to the helper methods used to consume APIs.\nThe method names of the test cases must start with a test_ prefix, which is a pytest requirement. \nTest methods are all standard Python methods and should not be asynchronous. The following is a \ntest method in test/test_restaurants.py that checks whether the endpoint returns the \nproper text-based response:\nfrom fastapi.testclient import TestClient\nfrom api import restaurant\nclient = TestClient(restaurant.router)\ndef test_restaurant_index():\n    response = client.get(\"/restaurant/index\")\n    assert response.status_code == 200\n    assert response.text == \"The Restaurants\"\nTestClient supports assert statements that check the response of its helper methods, like get(), \npost(), put(), and delete() the status code and response body of the API. The test_\nrestaurant_index(), for instance, uses the get() method of the TestClient API to run /\nrestaurant/index GET service and extract its response. The assert statements are used if the \nstatuc_code and response.text are correct. The endpoint has no imposed dependencies, \nso the test module is router-based. \nMocking the dependencies\nTesting API endpoints with dependencies is not as straightforward as the previous example. Our \nendpoints have session-based security through the JWT and the APIKeyCookie class, so we cannot \njust run pytest to test them. First, we need to apply mocking to these dependencies by adding them \nto the dependency_overrides of the FastAPI instance. Since APIRouter cannot mock \ndependencies, we need to use the FastAPI instance to set up TestClient. All endpoints can \nbe unit tested if the routers are part of the FastAPI configuration through include_router():\nfrom fastapi.testclient import TestClient\nfrom models.data.orrs import Login",
      "content_length": 2244,
      "extraction_method": "Direct"
    },
    {
      "page_number": 345,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n322\nfrom main import app\nfrom util.auth_session import get_current_user\nclient = TestClient(app)\nasync def get_user():\n    return Login(**{\"username\": \"sjctrags\", \n      \"login_id\": 101,  \n      \"password\":\"sjctrags\", \"passphrase\": None, \n      \"profile\": None})\napp.dependency_overrides[get_current_user] =  get_user\ndef test_rating_top_three():\n    response = client.post(\"/ch09/rating/top/three\", \n     json={\n          \"rate1\": 10.0, \n          \"rate2\": 20.0 ,\n          \"rate3\": 30.0\n        \n    })\n    assert response.status_code == 200\n    assert response.json() == { \"stats\": {\n          \"sum\": 60.0,\n          \"average\": 20.0\n      }\n}\nThe /rating/top/three API from the /api/route_extract.py router requires a dict \nof ratings to derive a JSON result containing average and sum. TestClient’s path operators \nhave JSON and data parameters, where we can pass test data to the API. Likewise, TestClient’s \nresponse has methods that can derive the expected response body, such as, in this example, the \njson() function.",
      "content_length": 1060,
      "extraction_method": "Direct"
    },
    {
      "page_number": 346,
      "chapter": null,
      "content": "Testing the API endpoints\n323\nRunning the test method will result in some APIKeyCookie exceptions due to the dependency \non session-based security. To bypass this issue, we need to create a fake get_current_user() \ndependable function to proceed with the testing. We add the get_current_user() dependable \nfunction into the roster of overrides and map it with the fake ones, such as our get_user() function, \nto replace its execution. This process is what we call mocking in the FastAPI context.\nAside from security, we can also mock the database connection by creating a mock database object \nor database engine, depending on whether it is a relational database or a NoSQL database. In the \nfollowing test case, we are performing a unit test in /ch09/login/list/all, which needs \nMongoDB connectivity to access the list of login profiles. For the test to work, we need to create a \nmock AsyncIOMotorClient object with a dummy test database called orrs_test. Here is the \ntest_list_login() test, which implements this database mocking:\ndef db_connect():\n    client_od = \n         AsyncIOMotorClient(f\"mongodb://localhost:27017/\")\n    engine = AIOEngine(motor_client=client_od, \n            database=\"orrs_test\")\n    return engine\nasync def get_user():\n    return Login(**{\"username\": \"sjctrags\", \"login_id\": 101,\n           \"password\":\"sjctrags\", \"passphrase\": None, \n           \"profile\": None})\napp.dependency_overrides[get_current_user] =  get_user\napp.dependency_overrides[create_db_engine] = db_connect\ndef test_list_login():\n    response = client.get(\"/ch09/login/list/all\")\n    assert response.status_code == 201\nRunning test methods\nRun the pytest command on the command line to execute all unit tests. The pytest engine will \ncompile and run all TestClient apps in the test folder, thus running all the test methods. Figure \n9.5 shows a snapshot of the test result:",
      "content_length": 1874,
      "extraction_method": "Direct"
    },
    {
      "page_number": 347,
      "chapter": null,
      "content": "Utilizing Other Advanced Features\n324\nFigure 9.5 – The test result\nLearning more about the pytest framework offers a heads-up in understanding the automation of \ntest cases in FastAPI. Organizing all test methods through modules is essential in the testing phase \nof the application since we run all of them in bulk.\nSummary\nThis chapter showcased some essential features that were not part of the previous chapters but can help \nfill some gaps during microservice development. One involves choosing better and more appropriate \nJSON serializers and de-serializers when converting a huge amount of data into JSON. Also, the \nadvanced customizations, session handling, message body encryption and decryption, and testing \nAPI endpoints gave us a clear understanding of the potential of FastAPI to create cutting-edge and \nprogressive microservice solutions. Also, this chapter introduced different API responses supported \nby FastAPI, including Jinja2’s TemplateResponse. \nThe next chapter will show us the strength of FastAPI in cracking numerical and symbolic computations.",
      "content_length": 1074,
      "extraction_method": "Direct"
    },
    {
      "page_number": 348,
      "chapter": null,
      "content": "10\nSolving Numerical, Symbolic, \nand Graphical Problems\nMicroservice architecture is not only used to build fine-grained, optimized, and scalable applications \nin the banking, insurance, production, human resources, and manufacturing industries. It is also \nused to develop scientific and computation-related research and scientific software prototypes for \napplications such as laboratory information management systems (LIMSs), weather forecasting \nsystems, geographical information systems (GISs), and healthcare systems.\nFastAPI is one of the best choices in building these granular services since they usually involve highly \ncomputational tasks, workflows, and reports. This chapter will highlight some transactions not yet \ncovered in the previous chapters, such as symbolic computations using sympy, solving linear systems \nusing numpy, plotting mathematical models using matplotlib, and generating data archives using \npandas. This chapter will also show you how FastAPI is flexible when solving workflow-related \ntransactions by simulating some Business Process Modeling Notation (BPMN) tasks. For developing \nbig data applications, a portion of this chapter will showcase GraphQL queries for big data applications \nand Neo4j graph databases for graph-related projects with the framework.\nThe main objective of this chapter is to introduce the FastAPI framework as a tool for providing \nmicroservice solutions for scientific research and computational sciences. \nIn this chapter, we will cover the following topics:\n•\t Setting up the projects\n•\t Implementing the symbolic computations\n•\t Creating arrays and DataFrames\n•\t Performing statistical analysis\n•\t Generating CSV and XLSX reports\n•\t Plotting data models",
      "content_length": 1722,
      "extraction_method": "Direct"
    },
    {
      "page_number": 349,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n326\n•\t Simulating a BPMN workflow\n•\t Using GraphQL queries and mutations\n•\t Utilizing the Neo4j graph database\nTechnical requirements\nThis chapter provides the base skeleton of a periodic census and computational system that enhances \nfast data collection procedures in different areas of a specific country. Although unfinished, the prototype \nprovides FastAPI implementations that highlight important topics of this chapter, such as creating \nand plotting mathematical models, gathering answers from respondents, providing questionnaires, \ncreating workflow templates, and utilizing a graph database. The code for this chapter can be found \nat https://github.com/PacktPublishing/Building-Python-Microservices-\nwith-FastAPI in the ch10 project. \nSetting up the projects\nThe PCCS project has two versions: ch10-relational, which uses a PostgreSQL database with \nPiccolo ORM as the data mapper, and ch10-mongo, which saves data as MongoDB documents \nusing Beanie ODM.\nUsing the Piccolo ORM \nch10-relational uses a fast Piccolo ORM that can support both sync and async CRUD transactions. \nThis ORM was not introduced in Chapter 5, Connecting to a Relational Database, because it is more \nappropriate for computational, data science-related, and big data applications. The Piccolo ORM is \ndifferent from other ORMs because it scaffolds a project containing the initial project structure and \ntemplates for customization. But before creating the project, we need to install the piccolo module \nusing pip:\npip install piccolo\nAfterward, install the piccolo-admin module, which provides helper classes for the GUI \nadministrator page of its projects:\npip install piccolo-admin\nNow, we can create a project inside a newly created root project folder by running piccolo asgi \nnew, a CLI command that scaffolds the Piccolo project directory. The process will ask for the API \nframework and application server to utilize, as shown in the following screenshot:",
      "content_length": 2001,
      "extraction_method": "Direct"
    },
    {
      "page_number": 350,
      "chapter": null,
      "content": "Setting up the projects\n327\nFigure 10.1 – Scaffolding a Piccolo ORM project\nYou must use FastAPI for the application framework and uvicorn is the recommended ASGI \nserver. Now, we can add Piccolo applications inside the project by running the piccolo app new \ncommand inside the project folder. The following screenshot shows the main project directory, where \nwe execute the CLI command to create a Piccolo application:\nFigure 10.2 – Piccolo project directory",
      "content_length": 460,
      "extraction_method": "Direct"
    },
    {
      "page_number": 351,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n328\nThe scaffolded project always has a default application called home, but it can be modified or even \ndeleted. Once removed, the Piccolo platform allows you to replace home by adding a new application \nto the project by running the piccolo app new command inside the project folder, as shown \nin the preceding screenshot. A Piccolo application contains the ORM models, BaseModel, services, \nrepository classes, and API methods. Each application has an auto-generated piccolo_app.py \nmodule where we need to configure an APP_CONFIG variable to register all the ORM details. The \nfollowing is the configuration of our project’s survey application:\nAPP_CONFIG = AppConfig(\n    app_name=\"survey\",\n    migrations_folder_path=os.path.join(\n        CURRENT_DIRECTORY, \"piccolo_migrations\"\n    ),\n    table_classes=[Answers, Education, Question, Choices, \n       Profile, Login, Location, Occupation, Respondent],\n    migration_dependencies=[],\n    commands=[],\n)\nFor the ORM platform to recognize the new Piccolo application, its piccolo_app.py must be \nadded to APP_REGISTRY of the main project’s piccolo_conf.py module. The following is the \ncontent of the piccolo_conf.py file of our ch10-piccolo project:\nfrom piccolo.engine.postgres import PostgresEngine\nfrom piccolo.conf.apps import AppRegistry\nDB = PostgresEngine(\n    config={\n        \"database\": \"pccs\",\n        \"user\": \"postgres\",\n        \"password\": \"admin2255\",\n        \"host\": \"localhost\",\n        \"port\": 5433,\n    }\n)",
      "content_length": 1531,
      "extraction_method": "Direct"
    },
    {
      "page_number": 352,
      "chapter": null,
      "content": "Setting up the projects\n329\nAPP_REGISTRY = AppRegistry(\n    apps=[\"survey.piccolo_app\", \n          \"piccolo_admin.piccolo_app\"]\n)\nThe piccolo_conf.py file is also the module where we establish the PostgreSQL database \nconnection. Aside from PostgreSQL, the Piccolo ORM also supports SQLite databases.\nCreating the data models\nLike in Django ORM, Piccolo ORM has migration commands to generate the database tables based \non model classes. But first, we need to create model classes by utilizing its Table API class. It also \nhas helper classes to establish column mappings and foreign key relationships. The following are some \ndata model classes that comprise our database pccs:\nfrom piccolo.columns import ForeignKey, Integer, Varchar,\n       Text, Date, Boolean, Float\nfrom piccolo.table import Table\nclass Login(Table):\n    username = Varchar(unique=True)\n    password = Varchar()\nclass Education(Table):\n    name = Varchar()\nclass Profile(Table):\n    fname = Varchar()\n    lname = Varchar()\n    age = Integer()\n    position = Varchar()\n    login_id = ForeignKey(Login, unique=True)\n    official_id = Integer()\n    date_employed = Date()",
      "content_length": 1140,
      "extraction_method": "Direct"
    },
    {
      "page_number": 353,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n330\nAfter creating the model classes, we can update the database by creating the migrations files. Migration \nis a way of updating the database of a project. In the Piccolo platform, we can run the piccolo \nmigrations new <app_name> command to generate files in the piccolo_migrations \nfolder. These are called migration files and they contain migration scripts. But to save time, we will \ninclude the --auto option for the command to let the ORM check the recently executed migration \nfiles and auto-generate the migration script containing the newly reflected schema updates. Check \nthe newly created migration file first before running the piccolo migrations forward \n<app_name> command to execute the migration script. This last command will auto-create all the \ntables in the database based on the model classes.\nImplementing the repository layer\nCreating the repository layer comes after performing all the necessary migrations. Piccolo’s CRUD \noperations are like those in the Peewee ORM. It is swift, short, and easy to implement. The following \ncode shows an implementation of the insert_respondent() transaction, which adds a new \nrespondent profile:\nfrom survey.tables import Respondent\nfrom typing import Dict, List, Any\nclass RespondentRepository:\n    async def insert_respondent(self, \n             details:Dict[str, Any]) -> bool: \n        try:\n            respondent = Respondent(**details)\n            await respondent.save()\n        except Exception as e: \n            return False \n        return True\nLike Peewee, Piccolo’s model classes can persist records, as shown by insert_respondent(), which \nimplements an asynchronous INSERT transaction. On the other hand, get_all_respondent() \nretrieves all respondent profiles and has the same approach as Peewee, as shown here:  \n    async def get_all_respondent(self):\n        return await Respondent.select()\n                  .order_by(Respondent.id)\nThe remaining Peewee-like DELETE and UPDATE respondent transactions are created in the project’s \n/survey/repository/respondent.py module.",
      "content_length": 2109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 354,
      "chapter": null,
      "content": "Implementing symbolic computations\n331\nThe Beanie ODM\nThe second version of the PCCS project, ch10-mongo, utilizes a MongoDB datastore and uses the \nBeanie ODM to implement its asynchronous CRUD transactions. We covered Beanie in Chapter 6, \nUsing a Non-Relational Database. Now, let us learn how to apply FastAPI in symbolic computations. \nWe will be using the ch10-piccolo project for this.\nImplementing symbolic computations\nSymbolic computation is a mathematical approach to solving problems using symbols or mathematical \nvariables. It uses mathematical equations or expressions formulated using symbolic variables to solve \nlinear and nonlinear systems, rational expressions, logarithmic expressions, and other complex real-\nworld models. To perform symbolic computation in Python, you must install the sympy module \nusing the pip command:\npip install sympy\nLet us now start creating our first symbolic expressions.\nCreating symbolic expressions\nOne way of implementing the FastAPI endpoint that performs symbolic computation is to create \na service that accepts a mathematical model or equation as a string and converts that string into a \nsympy symbolic expression. The following substitute_eqn() processes an equation in str \nformat and converts it into valid linear or nonlinear bivariate equations with the x and y variables. \nIt also accepts values for x and y to derive the solution of the expression:\nfrom sympy import symbols, sympify\n@router.post(\"/sym/equation\")\nasync def substitute_bivar_eqn(eqn: str, xval:int, \n               yval:int):\n    try:\n        x, y = symbols('x, y')\n        expr = sympify(eqn)\n        return str(expr.subs({x: xval, y: yval}))\n    except:\n        return JSONResponse(content={\"message\": \n            \"invalid equations\"}, status_code=500)",
      "content_length": 1787,
      "extraction_method": "Direct"
    },
    {
      "page_number": 355,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n332\nBefore converting a string equation into a sympy expression, we need to define the x and y variables \nas Symbols objects using the symbols() utility. This method accepts a string of comma-delimited \nvariable names and returns a tuple of symbols equivalent to the variables. After creating all the \nneeded Symbols() objects, we can convert our equation into sympy expressions by using any of \nthe following sympy methods:\n•\t sympify(): This uses eval() to convert the string equation into a valid sympy expression \nwith all Python types converted into their sympy equivalents\n•\t parse_expr(): A full-fledged expression parser that transforms and modifies the tokens \nof the expression and converts them into their sympy equivalents\nSince the substitute_bivar_eqn() service utilizes the sympify() method, the string \nexpression needs to be sanitized from unwanted code before sympifying to avoid any compromise. \nOn the other hand, the sympy expression object has a subs() method to substitute values to derive \nthe solution. Its resulting object must be converted into str format for Response to render the \ndata. Otherwise, Response will raise ValueError, regarding the result as non-iterable. \nSolving linear expressions\nThe sympy module allows you to implement services that solve multivariate systems of linear equations. \nThe following API service highlights an implementation that accepts two bivariate linear models in \nstring format with their respective solutions:\nfrom sympy import Eq, symbols, Poly, solve, sympify\n@router.get(\"/sym/linear\")\nasync def solve_linear_bivar_eqns(eqn1:str, \n            sol1: int, eqn2:str, sol2: int):\n    x, y = symbols('x, y')\n    \n    expr1 = parse_expr(eqn1, locals())\n    expr2 = parse_expr(eqn2, locals())\n    \n    if Poly(expr1, x).is_linear and \n                 Poly(expr1, x).is_linear:\n        eq1 = Eq(expr1, sol1)\n        eq2 = Eq(expr2, sol2)\n        sol = solve([eq1, eq2], [x, y])\n        return str(sol)",
      "content_length": 2016,
      "extraction_method": "Direct"
    },
    {
      "page_number": 356,
      "chapter": null,
      "content": "Implementing symbolic computations\n333\n    else:\n        return None\nThe solve_linear_bivar_eqns() service accepts two bivariate linear equations and their \nrespective outputs (or intercepts) and aims to establish a system of linear equations. First, it registers \nthe x and y variables as sympy objects and then uses the parser_expr() method to transform \nthe string expressions into their sympy equivalents. Afterward, the service needs to establish linear \nequality of these equations using the Eq() solver, which maps each sympy expression to its solution. \nThen, the API service passes all these linear equations to the solve() method to derive the x and y \nvalues. The result of solve() also needs to be rendered as a string, like in the substitution.\nAside from the solve() method, the API also uses the Poly() utility to create a polynomial object \nfrom an expression to be able to access essential properties of an equation, such as is_linear().\nSolving non-linear expressions\nThe previous solve_linear_bivar_eqns() can be reused to solve non-linear systems. The \ntweak is to shift the validation from filtering the linear equations to any non-linear equations. The \nfollowing script highlights this code change:\n@router.get(\"/sym/nonlinear\")\nasync def solve_nonlinear_bivar_eqns(eqn1:str, sol1: int, \n           eqn2:str, sol2: int):\n    … … … … … …\n    … … … … … …    \n    if not Poly(expr1, x, y).is_linear or \n              not Poly(expr1, x, y).is_linear:\n    … … … … … …\n    … … … … … …\n        return str(sol)\n    else:\n        return None\nSolving linear and non-linear inequalities\nThe sympy module supports solving solutions for both linear and non-linear inequalities but on \nunivariate equations only. The following is an API service that accepts a univariate string expression \nwith its output or intercepts, and extracts the solution using the solve() method:\n@router.get(\"/sym/inequality\")\nasync def solve_univar_inequality(eqn:str, sol:int):",
      "content_length": 1965,
      "extraction_method": "Direct"
    },
    {
      "page_number": 357,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n334\n    x= symbols('x')\n    expr1 = Ge(parse_expr(eqn, locals()), sol)\n    sol = solve([expr1], [x])\n    return str(sol)\nThe sympy module has Gt() or StrictGreaterThan, Lt() or StrictLessThan, Ge() \nor GreaterThan, and Le() or LessThan solvers, which we can use to create inequality. But \nfirst, we need to convert the str expression into a Symbols() object using the parser_expr() \nmethod before passing them to these solvers. The preceding service uses the GreaterThan solver, \nwhich creates an equation where the left-hand side of the expression is generally larger than the left. \nMost applications designed and developed for mathematical modeling and data science use sympy \nto create complex mathematical models symbolically, plot data directly from the sympy equation, or \ngenerate results based on datasets or live data. Now, let us proceed to the next group of API services, \nwhich deals with data analysis and manipulation using numpy, scipy, and pandas.\nCreating arrays and DataFrames\nWhen numerical algorithms require some arrays to store data, a module called NumPy, short for \nNumerical Python, is a good resource for utility functions, objects, and classes that are used to create, \ntransform, and manipulate arrays.\nThe module is best known for its n-dimensional arrays or ndarrays, which consume less memory storage \nthan the typical Python lists. An ndarray incurs less overhead when performing data manipulation \nthan executing the list operations in totality. Moreover, ndarray is strictly heterogeneous, unlike \nPython’s list collections.\nBut before we start our NumPy-FastAPI service implementation, we need to install the numpy module \nusing the pip command:\npip install numpy\nOur first API service will process some survey data and return it in ndarray form. The following \nget_respondent_answers() API retrieves a list of survey data from PostgreSQL through \nPiccolo and transforms the list of data into an ndarray:\nfrom survey.repository.answers import AnswerRepository\nfrom survey.repository.location import LocationRepository\nimport ujson\nimport numpy as np\n@router.get(\"/answer/respondent\")\nasync def get_respondent_answers(qid:int):\n    repo_loc = LocationRepository()",
      "content_length": 2250,
      "extraction_method": "Direct"
    },
    {
      "page_number": 358,
      "chapter": null,
      "content": "Creating arrays and DataFrames\n335\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    data = []\n    for loc in locations:\n        loc_q = await repo_answers\n            .get_answers_per_q(loc[\"id\"], qid)\n        if not len(loc_q) == 0:\n            loc_data = [ weights[qid-1]\n              [str(item[\"answer_choice\"])] \n                for item in loc_q]\n            data.append(loc_data)\n    arr = np.array(data)\n    return ujson.loads(ujson.dumps(arr.tolist())) \nDepending on the size of the data retrieved, it would be faster if we apply the ujson or orjson \nserializers and de-serializers to convert ndarray into JSON data. Even though numpy has data types \nsuch as uint, single, double, short, byte, and long, JSON serializers can still manage to \nconvert them into their standard Python equivalents. Our given API sample prefers ujson utilities \nto convert the array into a JSON-able response.\nAside from NumPy, pandas is another popular module that’s used in data analysis, manipulation, \ntransformation, and retrieval. But to use pandas, we need to install NumPy, followed by the pandas, \nmatplotlib, and openpxyl modules:\npip install pandas matplotlib openpxyl\nLet us now discuss about the ndarray in numpy module.\nApplying NumPy’s linear system operations\nData manipulation in an ndarray is easier and faster, unlike in a list collection, which requires list \ncomprehension and loops. The vectors and matrices created by numpy have operations to manipulate \ntheir items, such as scalar multiplication, matrix multiplication, transposition, vectorization, and \nreshaping. The following API service shows how the product between a scalar gradient and an array \nof survey data is derived using the numpy module:\n@router.get(\"/answer/increase/{gradient}\")\nasync def answers_weight_multiply(gradient:int, qid:int):\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    data = []",
      "content_length": 1998,
      "extraction_method": "Direct"
    },
    {
      "page_number": 359,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n336\n    for loc in locations:\n        loc_q = await repo_answers\n            .get_answers_per_q(loc[\"id\"], qid)\n        if not len(loc_q) == 0:\n            loc_data = [ weights[qid-1]\n             [str(item[\"answer_choice\"])] \n                 for item in loc_q]\n            data.append(loc_data)\n    arr = np.array(list(itertools.chain(*data)))\n    arr = arr * gradient\n    return ujson.loads(ujson.dumps(arr.tolist()))\nAs shown in the previous scripts, all ndarray instances resulting from any numpy operations can be \nserialized as JSON-able components using various JSON serializers. There are other linear algebraic \noperations that numpy can implement without sacrificing the performance of the microservice \napplication. Let us take a look now on panda's DataFrame.\nApplying the pandas module\nIn this module, datasets are created as a DataFrame object, similar to in Julia and R. It contains \nrows and columns of data. FastAPI can render these DataFrames using any JSON serializers. The \nfollowing API service retrieves all survey results from all survey locations and creates a DataFrame \nfrom these datasets:\nimport ujson\nimport numpy as np\nimport pandas as pd\n@router.get(\"/answer/all\")\nasync def get_all_answers():\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    temp = []\n    data = []\n    for loc in locations:\n        for qid in range(1, 13):\n            loc_q1 = await repo_answers",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 360,
      "chapter": null,
      "content": "Performing statistical analysis\n337\n               .get_answers_per_q(loc[\"id\"], qid)\n            if not len(loc_q1) == 0:\n                loc_data = [ weights[qid-1]\n                   [str(item[\"answer_choice\"])] \n                      for item in loc_q1]\n                temp.append(loc_data)\n        temp = list(itertools.chain(*temp))\n        if not len(temp) == 0:\n            data.append(temp)\n        temp = list()\n    arr = np.array(data)\n    return ujson.loads(pd.DataFrame(arr)\n           .to_json(orient='split'))\nThe DataFrame object has a to_json() utility method, which returns a JSON object with an \noption to format the resulting JSON according to the desired type. On another note, pandas can also \ngenerate time series, a one-dimensional array depicting a column of a DataFrame. Both DataFrames \nand time series have built-in methods that are useful for adding, removing, updating, and saving the \ndatasets to CSV and XLSX files. But before we discuss pandas’ data transformation processes, let us \nlook at another module that works with numpy in many statistical computations, differentiation, \nintegration, and linear optimizations: the scipy module.\nPerforming statistical analysis\nThe scipy module uses numpy as its base module, which is why installing scipy requires numpy \nto be installed first. We can use the pip command to install the module:\npip install scipy\nOur application uses the module to derive the declarative statistics of the survey data. The following \nget_respondent_answers_stats() API service computes the mean, variance, skewness, \nand kurtosis of the dataset using the describe() method from scipy:\nfrom scipy import stats\ndef ConvertPythonInt(o):\n    if isinstance(o, np.int32): return int(o)  \n    raise TypeError\n@router.get(\"/answer/stats\")",
      "content_length": 1789,
      "extraction_method": "Direct"
    },
    {
      "page_number": 361,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n338\nasync def get_respondent_answers_stats(qid:int):\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    data = []\n    for loc in locations:\n        loc_q = await repo_answers\n           .get_answers_per_q(loc[\"id\"], qid)\n             if not len(loc_q) == 0:\n                 loc_data = [ weights[qid-1]\n                   [str(item[\"answer_choice\"])] \n                       for item in loc_q]\n            data.append(loc_data)\n    result = stats.describe(list(itertools.chain(*data)))\n    return json.dumps(result._asdict(), \n                  default=ConvertPythonInt)\nThe describe() method returns a DescribeResult object, which contains all the computed \nresults. To render all the statistics as part of Response, we can invoke the as_dict() method of \nthe DescribeResult object and serialize it using the JSON serializer.\nOur API sample also uses additional utilities such as the chain() method from itertools to \nflatten the list of data and a custom converter, ConvertPythonInt, to convert NumPy’s int32 \ntypes into Python int types. Now, let us explore how to save data to CSV and XLSX files using the \npandas module.\nGenerating CSV and XLSX reports\nThe DataFrame object has built-in to_csv() and to_excel() methods that save its data in \nCSV or XLSX files, respectively. But the main goal is to create an API service that will return these \nfiles as responses. The following implementation shows how a FastAPI service can return a CSV file \ncontaining a list of respondent profiles:\nfrom fastapi.responses import StreamingResponse\nimport pandas as pd\nfrom io import StringIO\nfrom survey.repository.respondent import \n        RespondentRepository",
      "content_length": 1783,
      "extraction_method": "Direct"
    },
    {
      "page_number": 362,
      "chapter": null,
      "content": "Generating CSV and XLSX reports\n339\n@router.get(\"/respondents/csv\", response_description='csv')\nasync def create_respondent_report_csv():\n    repo = RespondentRepository()\n    result = await repo.get_all_respondent()\n    \n    ids = [ item[\"id\"] for item in result ]\n    fnames = [ f'{item[\"fname\"]}' for item in result ]\n    lnames = [ f'{item[\"lname\"]}' for item in result ]\n    ages = [ item[\"age\"] for item in result ]\n    genders = [ f'{item[\"gender\"]}' for item in result ]\n    maritals = [ f'{item[\"marital\"]}' for item in result ]\n   \n    dict = {'Id': ids, 'First Name': fnames, \n            'Last Name': lnames, 'Age': ages, \n            'Gender': genders, 'Married?': maritals} \n  \n    df = pd.DataFrame(dict)\n    outFileAsStr = StringIO()\n    df.to_csv(outFileAsStr, index = False)\n    return StreamingResponse(\n        iter([outFileAsStr.getvalue()]),\n        media_type='text/csv',\n        headers={\n            'Content-Disposition': \n              'attachment;filename=list_respondents.csv',\n            'Access-Control-Expose-Headers': \n               'Content-Disposition'\n        }\n    )\nWe need to create a dict() containing columns of data from the repository to create a DataFrame \nobject. From the given script, we store each data column in a separate list(), add all the lists in \ndict() with keys as column header names, and pass dict() as a parameter to the constructor \nof DataFrame.",
      "content_length": 1409,
      "extraction_method": "Direct"
    },
    {
      "page_number": 363,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n340\nAfter creating the DataFrame object, invoke the to_csv() method to convert its columnar dataset \ninto a text stream, io.StringIO, which supports Unicode characters. Finally, we must render the \nStringIO object through FastAPI’s StreamResponse with the Content-Disposition \nheader set to rename the default filename of the CSV object.\nInstead of using the pandas ExcelWriter, our Online Survey application opted for another way of \nsaving DataFrame through the xlsxwriter module. This module has a Workbook class, which \ncreates a workbook containing worksheets where we can plot all column data per row. The following \nAPI service uses this module to render XLSX content:\nimport xlsxwriter\nfrom io import BytesIO\n@router.get(\"/respondents/xlsx\", \n          response_description='xlsx')\nasync def create_respondent_report_xlsx():\n    repo = RespondentRepository()\n    result = await repo.get_all_respondent()\n    output = BytesIO()\n    workbook = xlsxwriter.Workbook(output)\n    worksheet = workbook.add_worksheet()\n    worksheet.write(0, 0, 'ID')\n    worksheet.write(0, 1, 'First Name')\n    worksheet.write(0, 2, 'Last Name')\n    worksheet.write(0, 3, 'Age')\n    worksheet.write(0, 4, 'Gender')\n    worksheet.write(0, 5, 'Married?')\n    row = 1\n    for respondent in result:\n        worksheet.write(row, 0, respondent[\"id\"])\n        … … … … … …\n        worksheet.write(row, 5, respondent[\"marital\"])\n        row += 1\n    workbook.close()\n    output.seek(0)\n    headers = {\n        'Content-Disposition': 'attachment;",
      "content_length": 1572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 364,
      "chapter": null,
      "content": "Generating CSV and XLSX reports\n341\n             filename=\"list_respondents.xlsx\"'\n    }\n    return StreamingResponse(output, headers=headers)\nThe given create_respondent_report_xlsx() service retrieves all the respondent records \nfrom the database and plots each profile record per row in the worksheet from the newly created \nWorkbook. Instead of writing to a file, Workbook will store its content in a byte stream, io.ByteIO, \nwhich will be rendered by StreamResponse.\nThe pandas module can also help FastAPI services read CSV and XLSX files for rendition or data \nanalysis. It has a read_csv() that reads data from a CSV file and converts it into JSON content. \nThe io.StringIO stream object will contain the full content, including its Unicode characters. The \nfollowing service retrieves the content of a valid CSV file and returns JSON data:\n@router.post(\"/upload/csv\")\nasync def upload_csv(file: UploadFile = File(...)):\n    df = pd.read_csv(StringIO(str(file.file.read(), \n            'utf-8')), encoding='utf-16')\n    return orjson.loads(df.to_json(orient='split'))\nThere are two ways to handle multipart file uploads in FastAPI:\n•\t Use bytes to contain the file\n•\t Use UploadFile to wrap the file object\nChapter 9, Utilizing Other Advanced Features, introduced the UploadFile class for capturing \nuploaded files because it supports more Pydantic features and has built-in operations that can work \nwith coroutines. It can handle large file uploads without raising an change to - exception when the \nuploading process reaches the memory limit, unlike using the bytes type for file content storage. \nThus, the given read-csv() service uses UploadFile to capture any CSV files for data analysis \nwith orjson as its JSON serializer.\nAnother way to handle file upload transactions is through Jinja2 form templates. We can use \nTemplateResponse to pursue file uploading and render the file content using the Jinja2 templating \nlanguage. The following service reads a CSV file using read_csv() and serializes it into HTML \ntable-formatted content:\n@router.get(\"/upload/survey/form\", \n          response_class = HTMLResponse)\ndef upload_survey_form(request:Request):\n    return templates.TemplateResponse(\"upload_survey.html\",\n             {\"request\": request})",
      "content_length": 2264,
      "extraction_method": "Direct"
    },
    {
      "page_number": 365,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n342\n@router.post(\"/upload/survey/form\")\nasync def submit_survey_form(request: Request, \n              file: UploadFile = File(...)):\n    df = pd.read_csv(StringIO(str(file.file.read(), \n               'utf-8')), encoding='utf-8')\n    return templates.TemplateResponse('render_survey.html', \n         {'request': request, 'data': df.to_html()})\nAside from to_json() and to_html(), the TextFileReader object also has other converters \nthat can help FastAPI render various content types, including to_latex(), to_excel(), \nto_hdf(), to_dict(), to_pickle(), and to_xarray(). Moreover, the pandas module \nhas a read_excel() that can read XLSX content and convert it into any rendition type, just like \nits read_csv() counterpart.\nNow, let us explore how FastAPI services can plot charts and graphs and output their graphical result \nthrough Response.\nPlotting data models\nWith the help of the numpy and pandas modules, FastAPI services can generate and render different \ntypes of graphs and charts using the matplotlib utilities. Like in the previous discussions, we will \nutilize an io.ByteIO stream and StreamResponse to generate graphical results for the API \nendpoints. The following API service retrieves survey data from the repository, computes the mean \nfor each data strata, and returns a line graph of the data in PNG format:\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom survey.repository.answers import AnswerRepository\nfrom survey.repository.location import LocationRepository\n@router.get(\"/answers/line\")\nasync def plot_answers_mean():\n    x = [1, 2, 3, 4, 5, 6, 7]\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    temp = []\n    data = []\n    for loc in locations:",
      "content_length": 1818,
      "extraction_method": "Direct"
    },
    {
      "page_number": 366,
      "chapter": null,
      "content": "Plotting data models\n343\n        for qid in range(1, 13):\n            loc_q1 = await repo_answers\n               .get_answers_per_q(loc[\"id\"], qid)\n            if not len(loc_q1) == 0:\n                loc_data = [ weights[qid-1]\n                  [str(item[\"answer_choice\"])] \n                     for item in loc_q1]\n                temp.append(loc_data)\n        temp = list(itertools.chain(*temp))\n        if not len(temp) == 0:\n            data.append(temp)\n        temp = list()\n    y = list(map(np.mean, data))\n    filtered_image = BytesIO()\n    plt.figure()\n    \n    plt.plot(x, y)\n \n    plt.xlabel('Question Mean Score')\n    plt.ylabel('State/Province')\n    plt.title('Linear Plot of Poverty Status')\n \n    plt.savefig(filtered_image, format='png')\n    filtered_image.seek(0)\n   \n    return StreamingResponse(filtered_image, \n                media_type=\"image/png\")\nThe plot_answers_mean() service utilizes the plot() method of the matplotlib module \nto plot the app’s mean survey results per location on a line graph. Instead of saving the file to the \nfilesystem, the service stores the image in the io.ByteIO stream using the module’s savefig() \nmethod. The stream is rendered using StreamResponse, like in the previous samples. The following \nfigure shows the rendered stream image in PNG format through StreamResponse:",
      "content_length": 1330,
      "extraction_method": "Direct"
    },
    {
      "page_number": 367,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n344\nFigure 10.3 – Line graph from StreamResponse\nThe other API services of our app, such as plot_sparse_data(), create a bar chart image in \nJPEG format of some simulated or derived data:\n@router.get(\"/sparse/bar\")\nasync def plot_sparse_data():\n   df = pd.DataFrame(np.random.randint(10, size=(10, 4)),\n      columns=[\"Area 1\", \"Area 2\", \"Area 3\", \"Area 4\"])\n   filtered_image = BytesIO()\n   plt.figure()\n   df.sum().plot(kind='barh', color=['red', 'green', \n          'blue', 'indigo', 'violet'])\n   plt.title(\"Respondents in Survey Areas\")\n   plt.xlabel(\"Sample Size\")\n   plt.ylabel(\"State\")\n   plt.savefig(filtered_image, format='png')\n   \n   filtered_image.seek(0)\n   return StreamingResponse(filtered_image, \n           media_type=\"image/jpeg\")",
      "content_length": 801,
      "extraction_method": "Direct"
    },
    {
      "page_number": 368,
      "chapter": null,
      "content": "Plotting data models\n345\nThe approach is the same as our line graph rendition. With the same strategy, the following service \ncreates a pie chart that shows the percentage of male and female respondents that were surveyed:\n@router.get(\"/respondents/gender\")\nasync def plot_pie_gender():\n    repo = RespondentRepository()\n    count_male = await repo.list_gender('M')\n    count_female = await repo.list_gender('F')\n    gender = [len(count_male), len(count_female)]\n    filtered_image = BytesIO()\n    my_labels = 'Male','Female'\n    plt.pie(gender,labels=my_labels,autopct='%1.1f%%')\n    plt.title('Gender of Respondents')\n    plt.axis('equal')\n    plt.savefig(filtered_image, format='png')\n    filtered_image.seek(0)\n   \n    return StreamingResponse(filtered_image, \n               media_type=\"image/png\")\nThe responses generated by the plot_sparse_data() and plot_pie_gender() services \nare as follows:\nFigure 10.4 – The bar and pie charts generated by StreamResponse",
      "content_length": 966,
      "extraction_method": "Direct"
    },
    {
      "page_number": 369,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n346\nThis section will introduce an approach to creating API endpoints that produce graphical results using \nmatplotlib. But there are other descriptive, complex, and stunning graphs and charts that you \ncan create in less time using numpy, pandas, matplotlib, and the FastAPI framework. These \nextensions can even solve complex mathematical and data science-related problems, given the right \nhardware resources.\nNow, let us shift our focus to the other project, ch10-mongo, to tackle topics regarding workflows, \nGraphQL, and Neo4j graph database transactions and how FastAPI can utilize them.\nSimulating a BPMN workflow\nAlthough the FastAPI framework has no built-in utilities to support its workflows, it is flexible and \nfluid enough to be integrated into other workflow tools such as Camunda and Apache Airflow through \nextension modules, middleware, and other customizations. But this section will only focus on the \nraw solution of simulating BPMN workflows using Celery, which can be extended to a more flexible, \nreal-time, and enterprise-grade approach such as Airflow integration.\nDesigning the BPMN workflow\nThe ch10-mongo project has implemented the following BPMN workflow design using Celery:\n•\t A sequence of service tasks that derives the percentage of the survey data result, as shown in \nthe following diagram:\nFigure 10.5 – Percentage computation workflow design\n•\t A group of batch operations that saves data to CSV and XLSX files, as shown in the \nfollowing diagram:",
      "content_length": 1540,
      "extraction_method": "Direct"
    },
    {
      "page_number": 370,
      "chapter": null,
      "content": "Simulating a BPMN workflow\n347\nFigure 10.6 – Data archiving workflow design\n•\t A group of chained tasks that operates on each location's data independently, as shown in the \nfollowing diagram:\nFigure 10.7 – Workflow design for stratified survey data analysis\nThere are many ways to implement the given design, but the most immediate solution is to utilize the \nCelery setup that we used in Chapter 7, Securing the REST APIs.",
      "content_length": 424,
      "extraction_method": "Direct"
    },
    {
      "page_number": 371,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n348\nImplementing the workflow\nCelery’s chain() method implements a workflow of linked task executions, as depicted in Figure 10.5, \nwhere every parent task returns the result to the first parameter of next task. The chained workflow \nworks if each task runs successfully without encountering any exceptions at runtime. The following \nis the API service in /api/survey_workflow.py that implements the chained workflow:\n@router.post(\"/survey/compute/avg\")\nasync def chained_workflow(surveydata: SurveyDataResult):\n    survey_dict = surveydata.dict(exclude_unset=True)\n    result = chain(compute_sum_results\n        .s(survey_dict['results']).set(queue='default'), \n            compute_avg_results.s(len(survey_dict))\n             .set(queue='default'), derive_percentile.s()\n             .set(queue='default')).apply_async()\n    return {'message' : result.get(timeout = 10) }\ncompute_sum_results(), compute_avg_results(), and derive_percentile() are \nbound tasks. Bound tasks are Celery tasks that are implemented to have the first method parameter \nallocated to the task instance itself, thus the self keyword appearing in its parameter list. Their task \nimplementation always has the @celery.task(bind=True) decorator. The Celery task manager \nprefers bound tasks when applying workflow primitive signatures to create workflows. The following \ncode shows the bound tasks that are used in the chained workflow design: \n@celery.task(bind=True)\ndef compute_sum_results(self, results:Dict[str, int]):\n    scores = []\n    for key, val in results.items():\n        scores.append(val)\n    return sum(scores)\ncompute_sum_results() computes the total survey result per state, while compute_avg_\nresults()consumes the sum computed by compute_sum_results() to derive the mean value:\n@celery.task(bind=True)\ndef compute_avg_results(self, value, len):\n    return (value/len)",
      "content_length": 1912,
      "extraction_method": "Direct"
    },
    {
      "page_number": 372,
      "chapter": null,
      "content": "Simulating a BPMN workflow\n349\nOn the other hand, derive_percentile() consumes the mean values produced by compute_\navg_results() to return a percentage value:\n@celery.task(bind=True)\ndef derive_percentile(self, avg):\n    percentage = f\"{avg:.0%}\"\n    return percentage\nThe given derive_percentile() consumes the mean values produced by compute_avg_\nresults() to return a percentage value.\nTo implement the gateway approach, Celery has a group() primitive signature, which is used \nto implement parallel task executions, as depicted in Figure 10.6. The following API shows the \nimplementation of the workflow structure with parallel executions: \n@router.post(\"/survey/save\")\nasync def grouped_workflow(surveydata: SurveyDataResult):\n    survey_dict = surveydata.dict(exclude_unset=True)\n    result = group([save_result_xlsx\n       .s(survey_dict['results']).set(queue='default'), \n         save_result_csv.s(len(survey_dict))\n          .set(queue='default')]).apply_async()\n    return {'message' : result.get(timeout = 10) } \nThe workflow shown in Figure 10.7 depicts a mix of grouped and chained workflows. It is common \nfor many real-world microservice applications to solve workflow-related problems with a mixture \nof different Celery signatures, including chord(), map(), and starmap(). The following script \nimplements a workflow with mixed signatures: \n@router.post(\"/process/surveys\")\nasync def process_surveys(surveys: List[SurveyDataResult]):\n    surveys_dict = [s.dict(exclude_unset=True) \n         for s in surveys]\n    result = group([chain(compute_sum_results\n       .s(survey['results']).set(queue='default'), \n         compute_avg_results.s(len(survey['results']))\n         .set(queue='default'), derive_percentile.s()\n         .set(queue='default')) for survey in \n                surveys_dict]).apply_async()\n    return {'message': result.get(timeout = 10) }",
      "content_length": 1876,
      "extraction_method": "Direct"
    },
    {
      "page_number": 373,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n350\nThe Celery signature plays an essential role in building workflows. A signature() method or \ns() that appears in the construct manages the execution of the task, which includes accepting the \ninitial task parameter value(s) and utilizing the queues that the Celery worker uses to load tasks. As \ndiscussed in Chapter 7, Securing the REST APIs, apply_async() triggers the whole workflow \nexecution and retrieves the result. \nAside from workflows, the FastAPI framework can also use the GraphQL platform to build CRUD \ntransactions, especially when dealing with a large amount of data in a microservice architecture.\nUsing GraphQL queries and mutations\nGraphQL is an API standard that implements REST and CRUD transactions at the same time. It is a \nhigh-performing platform that’s used in building REST API endpoints that only need a few steps to \nset up. Its objective is to create endpoints for data manipulation and query transactions.\nSetting up the GraphQL platform\nPython extensions such as Strawberry, Ariadne, Tartiflette, and Graphene support GraphQL-FastAPI \nintegration. This chapter introduces the use of the new Ariadne 3.x to build CRUD transactions for \nthis ch10-mongo project with MongoDB as the repository.\nFirst, we need to install the latest graphene extension using the pip command:\npip install graphene\nAmong the GraphQL libraries, Graphene is the easiest to set up, with fewer decorators and methods \nto override. It easily integrates with the FastAPI framework without requiring additional middleware \nand too much auto-wiring. \nCreating the record insertion, update, and deletion\nData manipulation operations are always part of GraphQL’s mutation mechanism. This is a GraphQL \nfeature that modifies the server-side state of the application and returns arbitrary data as a sign of a \nsuccessful change in the state. The following is an implementation of a GraphQL mutation that inserts, \ndeletes, and updates records:\nfrom models.data.pccs_graphql import LoginData\nfrom graphene import String, Int, Mutation, Field\nfrom repository.login import LoginRepository\nclass CreateLoginData(Mutation):\n    class Arguments:",
      "content_length": 2192,
      "extraction_method": "Direct"
    },
    {
      "page_number": 374,
      "chapter": null,
      "content": "Using GraphQL queries and mutations\n351\n      id = Int(required=True)\n      username = String(required=True)\n      password = String(required=True)\n    ok = Boolean()\n    loginData = Field(lambda: LoginData)\n    async def mutate(root, info, id, username, password):\n        login_dict = {\"id\": id, \"username\": username, \n                   \"password\": password}\n        login_json = dumps(login_dict, default=json_serial)\n        repo = LoginRepository()\n        result = await repo.add_login(loads(login_json))\n        if not result == None:\n          ok = True\n        else: \n          ok = False\n        return CreateLoginData(loginData=result, ok=ok)\nCreateLoginData is a mutation that adds a new login record to the data store. The inner class, \nArguments, indicates the record fields that will comprise the new login record for insertion. These \narguments must appear in the overridden mutate() method to capture the values of these fields. \nThis method will also call the ORM, which will persist the newly created record.\nAfter a successful insert transaction, mutate() must return the class variables defined inside a \nmutation class such as ok and the loginData object. These returned values must be part of the \nmutation instance. \nUpdating a login attribute has a similar implementation to CreateLoginData except the arguments \nneed to be exposed. The following is a mutation class that updates the password field of a login \nrecord that’s been retrieved using its username:\nclass ChangeLoginPassword(Mutation):\n    class Arguments:\n      username = String(required=True)\n      password = String(required=True)\n    ok = Boolean()\n    loginData = Field(lambda: LoginData)",
      "content_length": 1681,
      "extraction_method": "Direct"
    },
    {
      "page_number": 375,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n352\n    async def mutate(root, info, username, password):       \n        repo = LoginRepository()\n        result = await repo.change_password(username, \n                  password)\n        \n        if not result == None:\n          ok = True\n        else: \n          ok = False\n        return CreateLoginData(loginData=result, ok=ok)\nSimilarly, the delete mutation class retrieves a record through an id and deletes it from the data store:\nclass DeleteLoginData(Mutation):\n    class Arguments:\n      id = Int(required=True)\n      \n    ok = Boolean()\n    loginData = Field(lambda: LoginData)\n    async def mutate(root, info, id):       \n        repo = LoginRepository()\n        result = await repo.delete_login(id)\n        if not result == None:\n          ok = True\n        else: \n          ok = False\n        return DeleteLoginData(loginData=result, ok=ok)\nNow, we can store all our mutation classes in an ObjectType class that exposes these transactions \nto the client. We assign field names to each Field instance of the given mutation classes. These \nfield names will serve as the query names of the transactions. The following code shows the \nObjectType class, which defines our CreateLoginData, ChangeLoginPassword, and \nDeleteLoginData mutations:\nclass LoginMutations(ObjectType):\n    create_login = CreateLoginData.Field()",
      "content_length": 1380,
      "extraction_method": "Direct"
    },
    {
      "page_number": 376,
      "chapter": null,
      "content": "Using GraphQL queries and mutations\n353\n    edit_login = ChangeLoginPassword.Field()\n    delete_login = DeleteLoginData.Field()\nImplementing the query transactions\nGraphQL query transactions are implementations of the ObjectType base class. Here, LoginQuery \nretrieves all login records from the data store:\nclass LoginQuery(ObjectType):\n    login_list = None\n    get_login = Field(List(LoginData))\n  \n    async def resolve_get_login(self, info):\n      repo = LoginRepository()\n      login_list = await repo.get_all_login()\n      return login_list\nThe class must have a query field name, such as get_login, that will serve as its query name \nduring query execution. The field name must be part of the resolve_*() method name for it to be \nregistered under the ObjectType class. A class variable, such as login_list, must be declared \nfor it to contain all the retrieved records.\nRunning the CRUD transactions\nWe need a GraphQL schema to integrate the GraphQL components and register the mutation and \nquery classes for the FastAPI framework before running the GraphQL transactions. The following script \nshows the instantiation of GraphQL’s Schema class with LoginQuery and LoginMutations:\nfrom graphene import Schema \nschema = Schema(query=LoginQuery, mutation=LoginMutations,\n    auto_camelcase=False)\nWe set the auto_camelcase property of the Schema instance to False to maintain the use of \nthe original field names with an underscore and avoid the camel case notation approach.",
      "content_length": 1482,
      "extraction_method": "Direct"
    },
    {
      "page_number": 377,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n354\nAfterward, we use the schema instance to create the GraphQLApp() instance. GraphQLApp is \nequivalent to an application that needs mounting to the FastAPI framework. We can use the mount() \nutility of FastAPI to integrate the GraphQLApp() instance with its URL pattern and the chosen \nGraphQL browser tool to run the API transactions. The following code shows how to integrate the \nGraphQL applications with Playground as the browser tool to run the APIs:\nfrom starlette_graphene3 import GraphQLApp,     \n          make_playground_handler\napp = FastAPI()\napp.mount(\"/ch10/graphql/login\", \n       GraphQLApp(survey_graphene_login.schema, \n          on_get=make_playground_handler()) )\napp.mount(\"/ch10/graphql/profile\", \n       GraphQLApp(survey_graphene_profile.schema, \n          on_get=make_playground_handler()) )\nWe can use the left-hand side panel to insert a new record through a JSON script containing the field \nname of the CreateLoginData mutation, which is create_login, along with passing the \nnecessary record data, as shown in the following screenshot:\nFigure 10.8 – Running the create_login mutation\nTo perform query transactions, we must create a JSON script with a field name of LoginQuery, \nwhich is get_login, together with the record fields needed to be retrieved. The following screenshot \nshows how to run the LoginQuery transaction:",
      "content_length": 1409,
      "extraction_method": "Direct"
    },
    {
      "page_number": 378,
      "chapter": null,
      "content": "Utilizing the Neo4j graph database\n355\nFigure 10.9 – Running the get_login query transaction\nGraphQL can help consolidate all the CRUD transactions from different microservices with easy setup \nand configuration. It can serve as an API Gateway where all GraphQLApps from multiple microservices \nare mounted to create a single façade application. Now, let us integrate FastAPI into a graph database. \nUtilizing the Neo4j graph database\nFor an application that requires storage that emphasizes relationships among data records, a graph \ndatabase is an appropriate storage method to use. One of the platforms that use graph databases is \nNeo4j. FastAPI can easily integrate with Neo4j, but we need to install the Neo4j module using the \npip command:\npip install neo4j\nNeo4j is a NoSQL database with a flexible and powerful data model that can manage and connect \ndifferent enterprise-related data based on related attributes. It has a semi-structured database architecture \nwith simple ACID properties and a non-JOIN policy that make its operations fast and easy to execute.\nNote\nACID, which stands for atomicity, consistency, isolation, and durability, describes a database \ntransaction as a group of operations that performs as a single unit with correctness and consistency.",
      "content_length": 1274,
      "extraction_method": "Direct"
    },
    {
      "page_number": 379,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n356\nSetting the Neo4j database\nThe neo4j module includes neo4j-driver, which is needed to establish a connection with the \ngraph database. It needs a URI that contains the bolt protocol, server address, and port. The default \ndatabase port to use is 7687. The following script shows how to create Neo4j database connectivity:\nfrom neo4j import GraphDatabase\nuri = \"bolt://127.0.0.1:7687\"\ndriver = GraphDatabase.driver(uri, auth=(\"neo4j\", \n      \"admin2255\"))\nCreating the CRUD transactions\nNeo4j has a declarative graph query language called Cypher that allows CRUD transactions of the \ngraph database. These Cypher scripts need to be encoded as str SQL commands to be executed by \nits query runner. The following API service adds a new database record to the graph database:\n@router.post(\"/neo4j/location/add\")\ndef create_survey_loc(node_name: str, \n        node_req_atts: LocationReq):\n    node_attributes_dict = \n          node_req_atts.dict(exclude_unset=True)\n    node_attributes = '{' + ', '.join(f'{key}:\\'{value}\\''\n        for (key, value) in node_attributes_dict.items()) \n              + '}'\n    query = f\"CREATE ({node_name}:Location  \n         {node_attributes})\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\":\n         \"add node location successful\"}, status_code=201)\n    except Exception as e:\n        print(e)\n        return JSONResponse(content={\"message\": \"add node \n            location unsuccessful\"}, status_code=500)",
      "content_length": 1580,
      "extraction_method": "Direct"
    },
    {
      "page_number": 380,
      "chapter": null,
      "content": "Utilizing the Neo4j graph database\n357\ncreate_survey_loc() adds new survey location details to the Neo4j database. A record is \nconsidered a node in the graph database with a name and attributes equivalent to the record fields in \nthe relational databases. We use the connection object to create a session that has a run() method \nto execute Cypher scripts.\nThe command to add a new node is CREATE, while the syntax to update, delete, and retrieve nodes \ncan be added with the MATCH command. The following update_node_loc() service searches for \na particular node based on the node’s name and performs the SET command to update the given fields:\n@router.patch(\"/neo4j/update/location/{id}\")\nasync def update_node_loc(id:int, \n           node_req_atts: LocationReq):\n    node_attributes_dict = \n         node_req_atts.dict(exclude_unset=True)\n    node_attributes = '{' + ', '.join(f'{key}:\\'{value}\\'' \n       for (key, value) in \n            node_attributes_dict.items()) + '}'\n    query = f\"\"\"\n        MATCH (location:Location)\n        WHERE ID(location) = {id}\n        SET location += {node_attributes}\"\"\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\": \n          \"update location successful\"}, status_code=201)\n    except Exception as e:\n        print(e)\n        return JSONResponse(content={\"message\": \"update \n           location  unsuccessful\"}, status_code=500)\nLikewise, the delete transaction uses the MATCH command to search for the node to be deleted. The \nfollowing service implements Location node deletion:\n@router.delete(\"/neo4j/delete/location/{node}\")\ndef delete_location_node(node:str):\n    node_attributes = '{' + f\"name:'{node}'\" + '}'\n    query = f\"\"\"\n        MATCH (n:Location {node_attributes})",
      "content_length": 1806,
      "extraction_method": "Direct"
    },
    {
      "page_number": 381,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n358\n        DETACH DELETE n\n    \"\"\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\": \n          \"delete location node successful\"}, \n             status_code=201)\n    except:\n        return JSONResponse(content={\"message\": \n           \"delete location node unsuccessful\"}, \n               status_code=500)\nWhen retrieving nodes, the following service retrieves all the nodes from the database:\n@router.get(\"/neo4j/nodes/all\")\nasync def list_all_nodes():\n    query = f\"\"\"\n        MATCH (node)\n        RETURN node\"\"\"\n    try:\n        with driver.session() as session:\n            result = session.run(query=query)\n            nodes = result.data()\n        return nodes\n    except Exception as e:\n        return JSONResponse(content={\"message\": \"listing\n            all nodes unsuccessful\"}, status_code=500)\nThe following service only retrieves a single node based on the node’s id:\n@router.get(\"/neo4j/location/{id}\")\nasync def get_location(id:int):\n    query = f\"\"\"\n        MATCH (node:Location)\n        WHERE ID(node) = {id}\n        RETURN node\"\"\"\n    try:",
      "content_length": 1196,
      "extraction_method": "Direct"
    },
    {
      "page_number": 382,
      "chapter": null,
      "content": "Utilizing the Neo4j graph database\n359\n        with driver.session() as session:\n            result = session.run(query=query)\n            nodes = result.data()\n        return nodes\n    except Exception as e:\n        return JSONResponse(content={\"message\": \"get \n          location node unsuccessful\"}, status_code=500)\nOur implementation will not be complete if we have no API endpoint that will link nodes based on \nattributes. Nodes are linked to each other based on relationship names and attributes that are updatable \nand removable. The following API endpoint creates a node relationship between the Location \nnodes and Respondent nodes:\n@router.post(\"/neo4j/link/respondent/loc\")\ndef link_respondent_loc(respondent_node: str, \n    loc_node: str, node_req_atts:LinkRespondentLoc):\n    node_attributes_dict = \n         node_req_atts.dict(exclude_unset=True)\n   \n    node_attributes = '{' + ', '.join(f'{key}:\\'{value}\\'' \n       for (key, value) in \n          node_attributes_dict.items()) + '}'\n  \n    query = f\"\"\"\n        MATCH (respondent:Respondent), (loc:Location)\n        WHERE respondent.name = '{respondent_node}' AND \n            loc.name = '{loc_node}'\n        CREATE (respondent) -[relationship:LIVES_IN \n              {node_attributes}]->(loc)\"\"\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\": \"add … \n            relationship successful\"}, status_code=201)\n    except:\n        return JSONResponse(content={\"message\": \"add",
      "content_length": 1528,
      "extraction_method": "Direct"
    },
    {
      "page_number": 383,
      "chapter": null,
      "content": "Solving Numerical, Symbolic, and Graphical Problems\n360\n          respondent-loc relationship unsuccessful\"}, \n                 status_code=500)\nThe FastAPI framework can easily integrate into any database platform. The previous chapters have \nproven that FastAPI can deal with relational database transactions with ORM and document-based \nNoSQL transactions with ODM, while this chapter has proven the same for the Neo4j graph database \ndue to its easy configurations.\nSummary\nThis chapter introduced the scientific side of FastAPI by showing that API services can provide \nnumerical computation, symbolic formulation, and graphical interpretation of data via the numpy, \npandas, sympy, and matplotlib modules. This chapter also helped us understand how far we can \nintegrate FastAPI with new technology and design strategies to provide new ideas for the microservice \narchitecture, such as using GraphQL to manage CRUD transactions and Neo4j for real-time and \nnode-based data management. We also introduced the basic approach that FastAPI can apply to solve \nvarious BPMN workflows using Celery tasks. With this, we have started to understand the power and \nflexibility of the framework in building microservice applications. \nThe next chapter will cover the last set of topics to complete our deep dive into FastAPI. We will cover \nsome deployment strategies, Django and Flask integrations, and other microservice design patterns \nthat haven’t been discussed in the previous chapters.",
      "content_length": 1488,
      "extraction_method": "Direct"
    },
    {
      "page_number": 384,
      "chapter": null,
      "content": "11\nAdding Other \nMicroservice Features\nOur long journey of exploring FastAPI’s extensibility in building microservice applications will \nend with this chapter, which covers standard recommendations on project setup, maintenance, and \ndeployment using some microservice-related tools based on design patterns. This chapter will discuss \nthe OpenTracing mechanism and its use in a distributed FastAPI architecture setup using tools such as \nJaeger and StarletteTracingMiddleWare. The service registry and client-side discovery design \npatterns are included likewise in the detailed discussions on how to manage access to the API endpoints \nof the microservices. A microservice component that checks for the health of the API endpoints will \nalso be part of the discussion. Moreover, the chapter will not end without recommendations on the \nFastAPI application’s deployment, which might lead to other design strategies and network setups.\nThe main goal of this chapter is to complete the design architecture of a FastAPI application before its \nsign-off. Here are the topics that will complete our FastAPI application development venture:\n•\t Setting up the virtual environment\n•\t Checking the API properties\n•\t Implementing open tracing mechanisms\n•\t Setting up service registry and client-side service discovery\n•\t Deploying and running applications using Docker\n•\t Using Docker Compose for deployment\n•\t Utilizing NGINX as an API gateway \n•\t Integrating Django and Flask sub-applications",
      "content_length": 1486,
      "extraction_method": "Direct"
    },
    {
      "page_number": 385,
      "chapter": null,
      "content": "Adding Other Microservice Features\n362\nTechnical requirements\nOur last software prototype will be an Online Sports Management System (OSMS) that will manage \nadministrators, referees, players, schedules, and game results of a tournament or league. The application \nwill utilize MongoDB as the database storage. All of the code has been uploaded to https://github.\ncom/PacktPublishing/Building-Python-Microservices-with-FastAPI under \nch11 and other Chapter 11-related projects.\nSetting up the virtual environment\nLet us start with the proper way of setting up the development environment of our FastAPI \napplication. In Python development, it is common to manage the libraries and extension modules \nthat are needed using a virtual environment. A virtual environment is a way of creating multiple \ndifferent and parallel installations of Python interpreters and their dependencies where each has the \napplication(s) to be compiled and run. Each instance has its own set of libraries depending on the \nrequirements of its application(s). But first, we need to install the virtualenv module to pursue \nthe creation of these instances:\npip install virtualenv\nThe following list describes the benefits of having a virtual environment:\n•\t To avoid the overlapping of the library version\n•\t To avoid broken installed module files due to namespace collisions\n•\t To localize the libraries to avoid conflicts with the globally installed modules on which some \napplications are very dependent\n•\t To create a template or baseline copy of the set of modules to be replicated on some \nrelated projects\n•\t To maintain operating system performance and setup\nAfter the installation, we need to run the python -m virtualenv command to create an instance. \nFigure 11.1 shows how the ch01-env virtual environment for the ch01 project is created:\nFigure 11.1 – Creating a Python virtual environment",
      "content_length": 1878,
      "extraction_method": "Direct"
    },
    {
      "page_number": 386,
      "chapter": null,
      "content": "Setting up the virtual environment\n363\nTo use the virtual environment, we need to configure our VS Code editor to utilize the Python \ninterpreter of the virtual environment instead of the global interpreter to install modules, compile, \nand run the application. Pressing Ctrl + Shift + P will open the Command Palette showing the Python \ncommand to select the interpreter. Figure 11.2 shows the process of choosing the Python interpreter \nfor the ch01 project:\n \nFigure 11.2 – Choosing the Python interpreter\nThe select command will open a pop-up Windows File Explorer window to search for the appropriate \nvirtual environment with the Python interpreter, as shown in Figure 11.3:\nFigure 11.3 – Searching for the virtual environment\nOpening a Terminal console for the project will automatically activate the virtual environment by \nrunning the /Scripts/activate.bat command for the Windows operating system. Additionally, \nthis activate.bat script can be manually run if the automated activation was not successful. By \nthe way, the activation will not be feasible with the Powershell terminal, but only with the command \nconsole, as shown in Figure 11.4:",
      "content_length": 1155,
      "extraction_method": "Direct"
    },
    {
      "page_number": 387,
      "chapter": null,
      "content": "Adding Other Microservice Features\n364\nFigure 11.4 – Activating the virtual environment\nAfter activation, we can determine the name of the activated virtual environment from the leftmost \npart of the command line. Figure 11.4 shows that the Python interpreter of ch11-env is the chosen \ninterpreter for the project. Anything installed by its pip command will only be available within \nthat instance.\nEach of our projects has a virtual environment, thus having multiple virtual environments containing \ndifferent set of installed module dependencies, as shown in Figure 11.5:\nFigure 11.5 – Creating multiple virtual environments\nSetting up the virtual environment is only one of the best practices when it comes to initiating a \nPython microservice application. Aside from localizing the module installation, it helps prepare the \ndeployment of the application in terms of identifying what modules to install in the cloud servers. \nHowever, before we discuss FastAPI deployment approaches, first, let us discuss what microservice \nutilities to include before deploying a project, such as Prometheus.\nChecking the API properties\nPrometheus is a popular monitoring total that can monitor and check API services in any microservice \napplication. It can check the number of concurrent request transactions, the number of responses at \na certain period, and the total incoming requests of an endpoint. To apply Prometheus to FastAPI \napplications, first, we need to install the following module:\npip install starlette-exporter",
      "content_length": 1520,
      "extraction_method": "Direct"
    },
    {
      "page_number": 388,
      "chapter": null,
      "content": "Implementing open tracing mechanisms\n365\nThen, we add PrometheusMiddleware to the application and enable its endpoint to observe the \nAPI’s properties at runtime. The following script shows the application setup with the Prometheus \nmonitoring module:\nfrom starlette_exporter import PrometheusMiddleware, \n         handle_metrics\napp = FastAPI()\napp.add_middleware(PrometheusMiddleware, app_name=”osms”) \napp.add_route(“/metrics”, handle_metrics)\nHere, we add PrometheusMiddleware using the add_middleware() method of FastAPI. \nThen, we add an arbitrary URI pattern to the handle_metrics() utility to expose all of the API \nhealth details. Accessing http://localhost:8000/metrics will provide us with something \nas shown in Figure 11.6:\nFigure 11.6 – Monitoring the endpoints\nThe data in Figure 11.6 displays the time duration, in seconds, used by each API in processing requests, \nproviding response to clients, and emitting the status code of each API transaction. Additionally, it \nincludes some buckets that are built-in values used by the tool to create histograms. Aside from the \nhistogram, Prometheus also allows the customization of some metrics inherent to a particular application.\nAnother way of monitoring a FastAPI microservice application is by adding an open tracing tool.\nImplementing open tracing mechanisms\nWhen monitoring multiple, independent, and distributed microservices, the OpenTracing mechanism \nis preferred when managing API logs and traces. Tools such as Zipkin, Jaeger, and Skywalking are \npopular distributed tracing systems that can provide the setup for trace and log collections. In this \nprototype, we will be using the Jaeger tool to manage the application’s API traces and logs.",
      "content_length": 1716,
      "extraction_method": "Direct"
    },
    {
      "page_number": 389,
      "chapter": null,
      "content": "Adding Other Microservice Features\n366\nThe current way to integrate an OpenTracing tool into FastAPI microservices is through the OpenTelemetry \nmodules since the Opentracing for Python extension is already a deprecated module. To use Jaeger as \nthe tracing service, OpenTelemetry has an OpenTelemetry Jaeger Thrift Exporter utility, which allows \nyou to export traces to the Jaeger client applications. This exporter utility sends these traces to the \nconfigured agent using the Thrift compact protocol over UDP. But first, we need to install the following \nextension to utilize this exporter:\npip install opentelemetry-exporter-jaeger\nAfterward, add the following configuration to the main.py file:\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import \n          JaegerExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, \n          Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import \n          BatchSpanProcessor\nfrom opentelemetry.instrumentation.fastapi import \n          FastAPIInstrumentor\nfrom opentelemetry.instrumentation.logging import \n         LoggingInstrumentor\napp = FastAPI()\nresource=Resource.create(\n        {SERVICE_NAME: “online-sports-tracer”})\ntracer = TracerProvider(resource=resource)\ntrace.set_tracer_provider(tracer)\njaeger_exporter = JaegerExporter(\n    # configure client / agent\n    agent_host_name=’localhost’,\n    agent_port=6831,\n    # optional: configure also collector\n    # collector_endpoint=\n    #     ‘http://localhost:14268/api/traces?",
      "content_length": 1565,
      "extraction_method": "Direct"
    },
    {
      "page_number": 390,
      "chapter": null,
      "content": "Implementing open tracing mechanisms\n367\n    #            format=jaeger.thrift’,\n    # username=xxxx, # optional\n    # password=xxxx, # optional\n    # max_tag_value_length=None # optional\n)\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntracer.add_span_processor(span_processor)\nFastAPIInstrumentor.instrument_app(app, \n          tracer_provider=tracer)\nLoggingInstrumentor().instrument(set_logging_format=True)\nThe first step in the preceding setup is to create a tracing service with a name using OpenTelemetry’s \nResource class. Then, we instantiate a tracer from the service resource. To complete the setup, we need \nto provide the tracer with BatchSpanProcessor instantiated through the JaegerExporter \ndetails to manage all of the traces and logs using a Jaeger client. A trace includes full-detailed information \nabout the exchange of requests and responses among all API services and other components across \nthe distributed setup. This is unlike a log, which only contains the details regarding a transaction \nwithin an application.\nAfter the completed Jaeger tracer setup, we integrate the tracer client with FastAPI through \nFastAPIInstrumentor. To utilize this class, first, we need to install the following extension:\npip install opentelemetry-instrumentation-fastapi\nBefore we can run our application, first, we need to download a Jaeger client from \nhttps://www.jaegertracing.io/download/, unzip the jaeger-xxxx-windows-\namd64.tar.gz file, and run jaeger-all-in-one.exe. Installers for Linux and macOS are \nalso available.\nNow, open a browser and access the Jaeger client through the default http://localhost:16686. \nFigure 11.7 shows a snapshot of the tracer client:",
      "content_length": 1688,
      "extraction_method": "Direct"
    },
    {
      "page_number": 391,
      "chapter": null,
      "content": "Adding Other Microservice Features\n368\nFigure 11.7 – Monitoring microservices through a Jaeger client\nAfter some browser reloads, the Jaeger app will detect our tracer through its service name, online-\nsports-tracer, after running our microservice application. All accessed API endpoints are detected \nand monitored, thus creating traces and visual analyses regarding all requests and response transactions \nincurred by these endpoints. Figure 11.8 shows the traces and graphical plots generated by Jaeger:\nFigure 11.8 – Searching the traces of every API transaction",
      "content_length": 566,
      "extraction_method": "Direct"
    },
    {
      "page_number": 392,
      "chapter": null,
      "content": "Setting up service registry and client-side service discovery\n369\nA span in OpenTelemetry is equivalent to a trace with a unique ID, and we can scrutinize each span \nto view all the details by clicking on the search traces for every endpoint. Clicking on the searched \ntrace for the /ch11/login/list/all endpoint, as shown in Figure 11.8, can provide us with \nthe following trace details:\nFigure 11.9 – Scrutinizing the trace details of an endpoint\nAside from the traces shown in Figure 11.9, the Jaeger client can also collect the uvicorn logs through \nan OpenTelemetry module called opentelemetry-instrumentation-logging. After \ninstalling the module, we can enable the integration by instantiating LoggingInstrumentor in \nthe main.py file, as shown in the previous code snippet.\nNow, let us add the service registry and client-side service discovery mechanisms to our application.\nSetting up service registry and client-side service \ndiscovery\nA service registry tool such as Netflix Eureka enables the registration of microservice applications \nwithout knowing the exact DNS locations of their servers. It manages all access to these registered \nservices using a load-balancing algorithm and dynamically assigns these service instances with network \nlocations. This service registration is helpful to microservice applications deployed to servers with \nchanging DNS names due to failures, upgrades, and enhancements.\nFor the service registry to work, the service instances should have a mechanism to discover the registry \nserver before the server registration. For FastAPI, we need to utilize the py_eureka_client \nmodule to implement the service discovery design pattern.",
      "content_length": 1677,
      "extraction_method": "Direct"
    },
    {
      "page_number": 393,
      "chapter": null,
      "content": "Adding Other Microservice Features\n370\nImplementing client-side service discovery\nCreating a FastAPI microservice application to discover and register to a service registry server such \nas Netflix Eureka is straightforward. First, we need to install py_eureka_client through pip:\npip install py_eureka_client\nThen, we instantiate its EurekaClient component class with the correct eureka_server, \napp_name, instance_port, and instance_host parameter details. The eureka_server \nparameter must be the exact machine address of the Eureka server and not localhost. Additionally, \nthe client instance must have the appropriate app_name parameter for the FastAPI microservice \napplication (or client app), with the instance_port parameter set to 8000 and the instance_\nhost to 192.XXX.XXX.XXX (not localhost or 127.0.0.1). The following snippet depicts \nthe location in main.py in which to instantiate the EurekaClient component class:\nfrom py_eureka_client.eureka_client import EurekaClient\napp = FastAPI()\n@app.on_event(“startup”)\nasync def init():\n    create_async_db() \n    global client\n    client = EurekaClient(\n     eureka_server=”http://DESKTOP-56HNGC9:8761/eureka”, \n     app_name=”sports_service”, instance_port=8000, \n     instance_host=”192.XXX.XXX.XXX”)\n    await client.start()\n@app.on_event(“shutdown”)\nasync def destroy():\n    close_async_db() \n    await client.stop()",
      "content_length": 1379,
      "extraction_method": "Direct"
    },
    {
      "page_number": 394,
      "chapter": null,
      "content": "Setting up service registry and client-side service discovery\n371\nThe client discovery happens in the startup event of the application. It starts with the instantiation \nof the EurekaClient component class and invoking its start() method either asynchronously \nor not. The EurekaClient component class can handle asynchronous or synchronous FastAPI \nstartup events. To close the server discovery process, always invoke Eurek a Client’s stop() \nmethod in the shutdown event. Now, let us build our Netflix Eureka server registry before running \nand performing the client-side service discovery.\nSetting up the Netflix Eureka service registry\nLet us utilize the Spring Boot platform to create our Eureka server. We can create an application through \nhttps://start.spring.io/ or the Spring STS IDE, using either a Maven- or Gradle-driven \napplication. Ours is a Maven application with pom.xml that has the following dependency for the \nEureka Server setup:\n<dependency>\n      <groupId>org.springframework.cloud</groupId>\n      <artifactId>\n       spring-cloud-starter-netflix-eureka-server\n     </artifactId>\n</dependency>\nIn this case, application.properties must have server.port set to 8761, server.\nshutdown enabled for a graceful server shutdown, and a spring.cloud.inetutils.\ntimeout-seconds property set to 10 for its hostname calculation.\nNow, run the Eureka Server application before the FastAPI client application. The Eureka server’s \nlogs will show us the automatic detection and registration of FastAPI’s EurekaClient, as shown \nin Figure 11.10:\nFigure 11.10 – Discovering the FastAPI microservice application\nThe result of the client-side service discovery is also evident on the Eureka server’s dashboard at \nhttp://localhost:8761. The page will show us all the services that consist of the registry \nand through which we can access and test each service. Figure 11.11 shows a sample snapshot of \nthe dashboard:",
      "content_length": 1922,
      "extraction_method": "Direct"
    },
    {
      "page_number": 395,
      "chapter": null,
      "content": "Adding Other Microservice Features\n372\nFigure 11.11 – Creating the service registry\nOur SPORTS_SERVICE being part of the Eureka server registry, as depicted in Figure 11.11, means \nwe successfully implemented the client-side service discovery design pattern, and it is time to deploy \nour application to a Docker container.\nDeploying and running applications using Docker\nDockerization is a process of packaging, deploying, and running applications using Docker \ncontainers. Containerizing FastAPI microservices saves installation and setup time, space, and \nresources. And containerized apps are replaceable, replicable, efficient, and scalable compared to \nthe usual deployment packaging.   \nTo pursue Dockerization, we need to install Docker Hub and/or Docker Engine for the CLI commands. \nBut be aware of the new Docker Desktop License Agreement (https://www.docker.com/\nlegal/docker-software-end-user-license-agreement/) regarding its new subscription \nmodel. This chapter mainly focuses on how to run CLI commands rather than the Docker Hub GUI \ntool. Now, let us generate the list of modules to be installed in the docker image.\nGenerating the requirements.txt file\nSince we are using a virtual environment instance for module management, it is easy to identify what \nextension modules to install in the Docker image. We can run the following command to generate a \ncomplete list of modules and their versions to the requirements.txt file:\npip freeze > requirements.txt \nThen, we can create a command to copy this file to the image through the Dockerfile.",
      "content_length": 1562,
      "extraction_method": "Direct"
    },
    {
      "page_number": 396,
      "chapter": null,
      "content": "Deploying and running applications using Docker\n373\nCreating the Docker image \nThe next step is to build a container image from any available Linux-based container images in Docker \nHub. But we need a Dockerfile containing all the commands associated with pulling an available \nPython image from Docker Hub, creating a working directory, and copying project files from the \nlocal directory. The following is a Dockerfile set of instructions we use to deploy our prototype \nto a Python image:\nFROM python:3.9\nWORKDIR /code\nCOPY ./requirements.txt /code/requirements.txt\nRUN pip install --no-cache-dir --upgrade -r \n                /code/requirements.txt\nCOPY ./ch11 /code\nEXPOSE 8000\nCMD [“uvicorn”, “main:app”, “--host=0.0.0.0” , “--reload” ,\n     “--port”, “8000”]\nThe first line is an instruction that will derive a Python image, usually Linux-based, with an \ninstalled Python 3.9 interpreter. The command after that creates an arbitrary folder, /code, which \nwill become the application’s main folder. The COPY command copies our requirements.\ntxt file to the /code folder, and then the RUN instruction installs the updated modules from the \nrequirements.txt list using the following command:\npip install -r requirements.txt \nAfterward, the second COPY command copies our ch11 application to the working directory. The \nEXPOSE command binds port 8000 to the local machine’s port 8000 to run the CMD command, \nwhich is the last instruction of the Dockerfile. The CMD instruction uses uvicorn to run the \napplication at port 8000 using host 0.0.0.0 and not localhost to automatically map and utilize \nthe IP address assigned to the image.\nThe Dockerfile must be in the same folder as the requirements.txt file and the ch11 \napplication. Figure 11.12 shows the organization of the files and folders that needed to be Dockerized \nto a Python container image:",
      "content_length": 1857,
      "extraction_method": "Direct"
    },
    {
      "page_number": 397,
      "chapter": null,
      "content": "Adding Other Microservice Features\n374\nFigure 11.12 – Setting up the Docker folder structure\nOnce all the files and folders are complete, we run the following CLI command within the folder \nusing the terminal console:\ndocker build -t ch11-app .\nTo check the image, run the docker image ls CLI command.\nUsing the Mongo Docker image\nThe backend of our application is MongoDB, so we need to pull the latest mongo image from Docker \nHub using the following CLI command:\ndocker pull mongo:latest\nAnd before we run both the ch11-app application and the mongo:latest images, first, we need \nto create a ch11-network by running the following command:\ndocker network create ch11-network\nThis network becomes a bridge between mongo and ch11-app once they are deployed as containers. \nIt will establish the connectivity between the two containers to pursue the Motor-ODM transactions.\nCreating the containers\nA container is a running instance of a container image. We use the docker run command to start \nand run a pulled or created image. So, running the Mongo image using the ch11-network routes \nrequires the execution of the following CLI command:\ndocker run --name=mongo --rm -p 27017:27017 \n-d                 --network=ch11-network mongo\nInspect the mongo:latest container using the docker inspect command to derive and use its \nIP address for Motor-ODM’s connectivity. Replace the localhost used in AsyncIOMotorClient, \nwhich is found in the config/db.py module of ch11-app with the “inspected” IP address. Be \nsure to re-build the ch11-app Docker image after the update.",
      "content_length": 1568,
      "extraction_method": "Direct"
    },
    {
      "page_number": 398,
      "chapter": null,
      "content": "Using Docker Compose for deployment\n375\nNow, run the ch11-app image with ch11-network using the following command:\ndocker run --name=ch11-app --rm -p 8000:8000-\nd             --network=ch11-network ch11-app\nAccess the application through http://localhost:8000/docs to check all the API endpoints \nfrom the OpenAPI documentation.\nNow, another approach to simplifying containerization is to use the Docker Compose tool.\nUsing Docker Compose for deployment\nHowever, you need to install the Docker Compose utility in your operating system, which requires \nDocker Engine as the pre-installation requirement. After the installation, the next step is to create the \ndocker-decompose.yaml file containing all the services needed to build the images, process the \nDockerfile, build the Docker network, and create and run the containers. The following snippet shows \nthe content of our configuration file that sets up the mongo and ch11-app containers: \nversion: “3”\nservices: \n    ch11-mongo:\n        image: “mongo”\n        ports:\n            - 27017:27017\n        expose:\n            - 27017\n        networks:\n            - ch11-network\n    \n    ch11-app:\n        build: .     # requires the Dockerfile\n        depends_on: \n            - ch11-mongo\n        ports:\n            - 8000:8000\n        networks:\n            - ch11-network\nnetworks:\n    ch11-network:\n      driver: bridge",
      "content_length": 1373,
      "extraction_method": "Direct"
    },
    {
      "page_number": 399,
      "chapter": null,
      "content": "Adding Other Microservice Features\n376\nInstead of running separate Docker CLI commands, Docker Compose creates services, such as ch11-\nmongo and ch11-app, to manage the containerization and only uses one CLI command to execute \nthese services, docker-compose up. The command not only creates the network of images but \nalso runs all the containers.\nOne advantage of using Docker Compose is the ease of ORM and ODM configuration. Instead of \nperforming a container inspection to understand which IP address to use, we can use the service \nname of the database setup as the hostname to establish database connectivity. It is convenient since \nthe IP address of the mongo container varies for every instance created. The following is the new \nAsyncIOMotorClient with the ch11-mongo service as the hostname:\ndef create_async_db():\n    global client\n    client = AsyncIOMotorClient(str(“ch11-mongo:27017”))\nNow, let us implement an API Gateway design pattern for the containerized applications using the \nNGINX utility.\nUsing NGINX as an API Gateway\nIn Chapter 4, Building the Microservice Application, we implemented the API Gateway design pattern \nusing only some FastAPI components. In this last chapter, we will build a reverse proxy server through \nNGINX that will assign a proxy IP address to each containerized microservice application. These proxy \nIPs will redirect client requests to the actual microservices running on their respective containers.\nInstead of building an actual NGINX environment, we will be pulling an available NGINX image \nfrom Docker Hub to implement the reverse proxy server. This image creation requires a new Docker \napp folder with a different Dockerfile containing the following instructions:\nFROM nginx:latest\nCOPY ./nginx_config.conf /etc/nginx/conf.d/default.conf\nThe Dockerfile instructs the creation of the latest NGINX image and a copy of a nginx_config.\nconf file to that image. The file is an NGINX configuration file that contains the mapping of a proxy \nIP address to the actual container address of each microservice application. It also exposes 8080 as \nits official port. The following is the content of our nginx_config.conf file:\nserver {\n    listen 8080;\n    location / {\n        proxy_pass http://192.168.1.7:8000;",
      "content_length": 2262,
      "extraction_method": "Direct"
    },
    {
      "page_number": 400,
      "chapter": null,
      "content": "Integrating Flask and Django sub-applications\n377\n    }\n} \nThe application’s OpenAPI documentation can now be accessed through \nhttp://localhost:8080/docs.\nThe Dockerization of NGINX must come after deploying applications to the containers. But another \napproach is to include NGINX’s Dockerfile instructions in the application’s Dockerfile to \nsave time and effort. Or we can create another service in the docker-decompose.yaml file to \nbuild and run the NGINX image.  \nAnd for the last time, let us explore the power of FastAPI in its integration with other popular Python \nframeworks such as Flask and Django.\nIntegrating Flask and Django sub-applications\nFlask is a lightweight framework that is popular for its Jinja2 templates and WSGI server. On the other \nhand, Django is a Python framework that promotes rapid development using CLI commands and \napplies the scaffolding of files and folders to build projects and applications. Django applications can \nrun on either WSGI- or ASGI-based servers. \nWe can create, deploy, and run Flask and Django projects inside a FastAPI microservice application. \nThe framework has WSGIMiddleware to wrap both Flask and Django applications and integrate \nthem into the FastAPI platform. Running the FastAPI application through uvicorn will also run both \napplications. \nOf the two, it is easier to integrate the Flask application with a project than Django. We only need to \nimport the Flask app object into the main.py file, wrap it with WSGIMiddleware, and mount \nit into the FastAPI app object. The following script shows the part of main.py that integrates our \nch11_flask project:\nfrom ch11_flask.app import app as flask_app\nfrom fastapi.middleware.wsgi import WSGIMiddleware\napp.mount(“/ch11/flask”, WSGIMiddleware(flask_app))\nAll API endpoints implemented in ch11_flask will be accessed using the URL prefix, /ch11/\nflask, as indicated in the mount() method. Figure 11.13 shows the location of ch11_flask \ninside the ch11 project:",
      "content_length": 1979,
      "extraction_method": "Direct"
    },
    {
      "page_number": 401,
      "chapter": null,
      "content": "Adding Other Microservice Features\n378\nFigure 11.13 – Creating a Flask application inside the FastAPI project\nOn the other hand, the following main.py script integrates our ch11_django application into \nthe ch11 project:\nimport os\nfrom django.core.wsgi import get_wsgi_application\nfrom importlib.util import find_spec\nfrom fastapi.staticfiles import StaticFiles\nos.environ.setdefault(‘DJANGO_SETTINGS_MODULE’, \n           ‘ch11_django.settings’)\ndjango_app = get_wsgi_application()\napp = FastAPI()\napp.mount(‘/static’,\n    StaticFiles(\n         directory=os.path.normpath(",
      "content_length": 572,
      "extraction_method": "Direct"
    },
    {
      "page_number": 402,
      "chapter": null,
      "content": "Integrating Flask and Django sub-applications\n379\n              os.path.join(\n           find_spec(‘django.contrib.admin’).origin, \n                  ‘..’, ‘static’)\n         )\n   ),\n   name=’static’,\n)\napp.mount(‘/ch11/django’, WSGIMiddleware(django_app))\nThe Django framework has a get_wsgi_application() method that is uses to retrieve its \napp instance. This instance needs to be wrapped by WSGIMiddleware and mounted into the \nFastAPI app object. Moreover, we need to load the settings.py module of the ch11_django \nproject into the FastAPI platform for global access. Also, we need to mount all the static files of the \ndjango.contrib.main module, which includes some HTML templates of the Django \nsecurity module. \nAll views and endpoints created by the sports application of the ch11_django project must be \naccessed using the /ch11/django URL prefix. Figure 11.14 shows the placement of the ch11_\ndjango project within the ch11 app:\nFigure 11.14 – Creating a Django project and application inside a FastAPI object",
      "content_length": 1022,
      "extraction_method": "Direct"
    },
    {
      "page_number": 403,
      "chapter": null,
      "content": "Adding Other Microservice Features\n380\nSummary\nThe last chapter has given us the avenue on how to start, deploy, and run a FastAPI microservice application \nthat follows the standards and best practices. It introduces the use of a virtual environment instance to \ncontrol and manage the installation of modules from the start of the development until the deployment \nof our applications to Docker containers. The chapter has extensively explained the approaches on how \nto package, deploy, and run containerized applications. And lastly, the chapter has implemented an \nNGINX reverse proxy server for the application to build the API Gateway for our specimen.\nRight from the start, we have witnessed the simplicity, power, adaptability, and scalability of the FastAPI \nframework, from creating background processes to rendering data using HTML templates. Its fast \nexecution of API endpoints through its coroutines gives the framework the edge to become one of \nthe most popular Python frameworks in the future. As the community of FastAPI continues to grow, \nwe hope for more promising features in its future updates, such as support for reactive programming, \ncircuit breakers, and a signature security module. We're hoping for the best for the FastAPI framework!",
      "content_length": 1265,
      "extraction_method": "Direct"
    },
    {
      "page_number": 404,
      "chapter": null,
      "content": "Index\nSymbols\n@asyncio.coroutine\napplying  255, 256\nA\nAMQP messaging\nmonitoring  269\nApache Kafka\nabout  270\nconsumer, running on console  272, 273\ndownload link  271\npublisher, implementing  271, 272\ntopic, creating  271\nused, for building publish/\nsubscribe messaging  270\nAPI endpoints testing\ndependencies, mocking  321-323\nperforming  320\ntest methods, running  323\nunit test cases, writing  321\nAPI-first microservices development  6\nAPI properties\nchecking  364, 365\nAPI-related exceptions, managing\nabout  30\ncustom exceptions  34-36\ndefault handler override  37\nHTTPException, raising  33, 34\nmultiple status codes  32, 33\nsingle status code response  30-32\nAPI responses\nmanaging  39-41\nAPIRoute and Request customization\nabout  297\nform, managing  297-300\nJSON data, managing  297-300\nmessage body, decrypting  300, 301\nmessage body, encrypting  300, 301\nmessage body, managing  297-300\nAPI services\nimplementing  27\napplication deployment, with Docker\ncontainers, creating  374\nDocker image, creating  373\nMongo Docker image, using  374\nperforming  372\nrequirements.txt file, generating  372",
      "content_length": 1103,
      "extraction_method": "Direct"
    },
    {
      "page_number": 405,
      "chapter": null,
      "content": "Index\n382\napplication settings\nmanaging  100\nstoring, as class attributes  101, 102\nstoring, in properties file  102-104\narrays\ncreating  334, 335\nasync/await construct\nusing  256, 257\nasync CRUD transactions, creating with Motor\nabout  182\nasynchronous repository layer, \ncreating  183-185\nCRUD transactions, running  185, 186\ndatabase connectivity, setting up  182, 183\nmodel layer, creating  183\nasync CRUD transactions, implementing \nwith SQLAlchemy\nabout  124\nasyncio-compliant database \ndrivers, installing  124\nBase class, creating  125\nCRUD transactions, running  129, 130\ndatabase connectivity, setting up  124, 125\nmodel layer, creating  125\nrepository layer, building  126-128\nsession factory, creating  125\nasynchronous background tasks\ncoroutines, using  261, 262\ncreating  261\nmultiple tasks, creating  263\nasynchronous dependencies\ninjecting  55, 56\nasynchronous path operations\nusing  43, 44\nAsynchronous Server Gateway \nInterface (ASGI)  5\nasynchronous Server-Sent Events (SSE)\nimplementing  273, 274\nasynchronous transactions\ndesigning  258-260\nasynchronous WebSocket\nbuilding  275\nasynchronous WebSocket endpoint\nimplementing  275, 276\nasyncio  255\nasync repository, for FastAPI\nbuilding, with ODMantic  200\nasync transactions implementation, \nwith Beanie\nCRUD transactions, implementing  198, 199\ndatabase connectivity, creating  195, 196\ndocument associations, creating  198\nmodel classes, defining  196, 197\nrepository transactions, running  200\nauthentication\nbuilt-in middlewares, using for  250, 251\nauthorization code flow\nauthorization request, \nimplementing  237, 238\nbuilding  236\nOAuth2AuthorizationCodeBearer, \napplying  236, 237\nauthorization code response\nimplementing  238, 239\nB\nbackground processes\ncreating  41, 42\nBaseHTTPMiddleware\ncustomizing  293-295\nBaseModel classes\nusing, for transactions  168-170\nBasic authentication\nHttpBasic, applying  214-217\nHttpBasicCredentials, applying  214-217",
      "content_length": 1932,
      "extraction_method": "Direct"
    },
    {
      "page_number": 406,
      "chapter": null,
      "content": "Index\n383\nimplementing  214\nlogin transaction, executing  217, 218\nusing  214\nBeanie\nasync transactions, implementing with  195\nBeanie ORM  331\nBinary JSON (BSON)\nabout  167\nreference link  167\nBPMN workflow\ndesigning  346, 347\nimplementing  348-350\nsimulating  346\nbuilt-in middlewares\nusing, for authentication  250, 251\nC\ncallable class\ninjecting  50\nCelery  263\nCelery instance\nconfiguring  264, 265\ncreating  264-269\nCelery tasks\nabout  263\ncalling  267\ncreating  266\nmonitoring  268\nworker server, starting  267, 268\nclass diagram  163\nclient-side service discovery\nimplementing  370, 371\nCommand and Query Responsibility \nSegregation (CQRS)  155\ncommon gateway\ncreating  82\nconfigurable containers\nusing  69\ncontainer  374\nContent-Language  40\ncookies\nabout  23\nmanaging  23, 24\ncoroutines\nasync/await construct, using  256, 257\nasynchronous transactions, \ndesigning  258-260\n@asyncio.coroutine, applying  255, 256\nHTTP/2 protocol, using  261\nimplementing  254\nused, for creating Observable  278-280\nusing  261, 262\ncoroutine switching\napplying  254\nCORS mechanism\nmanaging  295-297\nCQRS design pattern\nabout  156\napplying  156\ncommand and query classes, \ncreating  156, 157\ncommand and query handlers, \ncreating  157, 158\nhandler interfaces, defining  156\nhandlers, accessing  158-160\nCRUD transactions, creating \nwith MongoFrames\ndatabase connection, creating  206\ndocument association, creating  208\nmodel layer, creating  207, 208\nrepository layer, applying  211\nrepository layer, creating  208-210",
      "content_length": 1509,
      "extraction_method": "Direct"
    },
    {
      "page_number": 407,
      "chapter": null,
      "content": "Index\n384\nCRUD transactions, creating \nwith SQLAlchemy\nabout  110, 116-118\nBase class, defining  112\ndatabase connection, setting up  110, 111\ndatabase driver, installing  110\nJOIN queries, creating  119, 120\nmodel layer, building  112, 113\nrepository layer, implementing  116\nsession factory, initializing  111\ntable relationships, mapping  113-115\ntables, creating  123\ntransactions, running  120-122\nCRUD transactions implementation, \nwith MongoEngine\nabout  190, 191\nCRUD transactions, running  194, 195\ncustom serialization and \ndeserialization, applying  190\ndatabase connection, establishing  187\ndocument association, creating  189, 190\nembedded documents, managing  192-194\nmodel layer, building  187, 188\nCSV report\ngenerating  338-342\ncycle  29\nD\ndatabase connectivity\npreparing for  108, 109\ndatabase environment\nsetting up  162-164\nDataFrames\ncreating  334, 335\ndata models\nabout  62\nplotting  342-346\ndecomposition pattern\napplying  78, 79\ndefault parameters  14, 15\ndependables\nscoping  75, 76\ndependencies\ncaching  53\ndependency function\ninjecting  49\ndependency injection (DI)\napplying  48\non main.py  61, 62\non path operators  57, 58\non routers  58-61\non services  56, 57\nways, exploring  56\nDependency Injector\ncontainers  70\ncontainer types  70\nFastAPI integration  71, 72\nmultiple-container setup  72, 73\nproviders module  70\nusing  69\nDepends() parameter types\ndeclaring  54, 55\ndevelopment environment\nsetting up  4, 5\nDigest authentication\nhashed credentials, generating  219\nHTTPAuthorizationCredentials, \nusing  220, 221\nHTTPDigest, using  220, 221\nimplementing  214\nlogin transaction, executing  221, 222\nuser credentials, passing  219\nusing  218\nDjango  377",
      "content_length": 1685,
      "extraction_method": "Direct"
    },
    {
      "page_number": 408,
      "chapter": null,
      "content": "Index\n385\nDjango applications\nintegrating, with Flask \nsub-applications  377-379\nDocker Compose\nadvantages  376\nusing, for deployment  375, 376\nDockerization  372\ndomain modeling approach\napplying  94\ndomain model layer  95\ndomain models\nidentifying  95-97\nE\nendpoint\nimplementing  82, 83\nError500Model  319\nevent handlers  283\nevents\ncustomizing  283\nexception handlers\napplying  84, 85\nF\nfactory method pattern\nabout  100\nusing  100\nFastAPI\nabout  4\nconfiguring  5, 6\ncookies, managing  23, 24\ndefault parameters  14, 15\nform parameters, handling  22, 23\ninitializing  5, 6\noptional parameters  15, 16\nparameter type declaration  10\npath parameters  10-12\nquery parameters  12, 13\nrequest body  17-19\nrequest headers  19, 20\nresponse data  20, 21\ntypes of parameters, mixing  16, 17\nfeedback module  26\nFlask  377\nFlask applications\nintegrating, with Django \nsub-applications  377-379\nFlower  268\nframing  207\nG\ngeographical information systems (GIS)  325\nGINO  130\nGINO, using for async transactions\nabout  131\nCRUD transactions, implementing  135-138\nCRUD transactions, running  138, 139\ndatabase connection, establishing  131\ndatabase driver, installing  131\nmodel layer, building  131, 132\ntable relationships, mapping  132-134\ntables, creating  139, 140\nGraphene  350\nGraphQL\nabout  350\nCRUD transactions, running  353-355\nmutations  350\nqueries  350\nquery transactions, implementing  353\nrecord delete  352\nrecord insertion  350\nrecord update  351\nsetting up  350",
      "content_length": 1471,
      "extraction_method": "Direct"
    },
    {
      "page_number": 409,
      "chapter": null,
      "content": "Index\n386\nH\nHTTP/2 protocol\nusing  261\nhttpx module\nabout  91\nusing  91, 92\nI\nIntegrated Development Environment (IDE)  4\nInversion of Control (IoC) principle\napplying  48\nJ\nJSON-compatible types\nobjects, converting to  38, 39\nJSON Object Signing and \nEncryption (JOSE)  228\nJWT tokens\naccess_token, creating  228\napplying  228\nlogin transaction, creating  229, 230\nsecret key, generating  228\nsecured endpoints, accessing  230, 231\nK\nKafka broker\nrunning  271\nKeycloak  240\nKeycloak client  241\nKeycloak realm  240\nL\nlaboratory information management \nsystems (LIMS)  325\nLagom module\ncontainer  74\nFastAPI integration  74\nusing  74\nlayers\ncreating  94\nlinear, and non-linear inequalities\nsolving  333, 334\nlinear expressions\nsolving  332, 333\nlogging mechanism\ncentralizing  86\nlogging middleware\nbuilding  88-90\nLoguru module\nutilizing  86-88\nM\nmain.py file\nimplementing  29, 30\nmessage-driven transactions\nbuilding, with RabbitMQ  269\nmicroservice configuration\nmanaging  100\nmicroservice ID\nevaluating  83, 84\nmiddleware\nabout  295\napplying, to filter path operations  45, 46\nmocking  323\nmodel layer  63",
      "content_length": 1109,
      "extraction_method": "Direct"
    },
    {
      "page_number": 410,
      "chapter": null,
      "content": "Index\n387\nmodel layer, PyMongo driver\nBaseModel classes, using for \ntransactions  168-170\nbuilding  166, 167\ndocument associations, establishing  168\nPydantic @dataclass, using to \nquery documents  171-174\nPydantic validation, using  170, 171\nmodule components\nimporting  28\nMongoDB\nURL  162\nMongoDB Compass  162\nMongoEngine\nCRUD transactions, implementing with  186\nMongoFrames\nCRUD transactions, creating with  206\nMotor\nasync CRUD transactions, creating with  182\nN\nnative coroutine  256\nNeo4J database\nsetting  356\nNeo4J graph database\nCRUD transactions, creating  356-360\nutilizing  355\nnested dependencies\nbuilding  51-53\nnested model approach  18\nNetflix Eureka service registry\nabout  369\nsetting up  371, 372\nNGINX\nusing, as API Gateway  376\nnon-linear expressions\nsolving  333\nNumPy\nabout  334\nlinear system operations  335, 336\nO\nOAuth2  222\nObject Document Mapping (ODM)  186\nObject Relational Model (ORM) \nframework  97, 110\nobjects\nconverting, to JSON-compatible types  38, 39\nObservable\ncreating, with coroutines  278-280\nODMantic, used for building async \nrepository for FastAPI\nabout  200\nCRUD transactions, implementing  202-204\nCRUD transactions, running  205, 206\ndatabase connection, creating  201\ndocument association, establishing  202\nmodel layer, creating  201, 202\nOnline Recipe System\nproject structure  68, 69\nOpenAPI 3.x specification\nabout  311\napplying  311\nForm directive, using  318, 319\ninternal code base properties, using  314, 315\nOpenAPI schema definition, \nextending  311-314\nPath() directive, using  317, 318\nQuery() function, using  316\nOpenID Connect specification\napplying  240\nAuth0, integrating with FastAPI  249\nHTTPBearer, using  240\nKeycloak client, setting  241",
      "content_length": 1710,
      "extraction_method": "Direct"
    },
    {
      "page_number": 411,
      "chapter": null,
      "content": "Index\n388\nKeycloak environment, configuring  240\nKeycloak environment, installing  240\nKeycloak, integrating with FastAPI  245-247\nKeycloak realm, setting  240\nOkta, integrating with FastAPI  250\nroles, assigning to clients  243\ntoken verification, implementing  247, 248\nuser permissions, creating \nthrough scopes  244\nuser roles, creating  242\nusers, creating  242\nopen tracing mechanisms\nimplementing  365-369\noptional parameters  15, 16\nORJSONResponse\nusing  310\nP\npackages  26\npandas module\napplying  336, 337\npassword-based authentication\nendpoints, securing  226, 227\nimplementing  222\nlogin transaction, executing  223-226\nOAuth2PasswordBearer, using  222, 223\nOAuth2PasswordRequestForm, \nusing  222, 223\npython-multipart module, installing  222\npath operations\nfiltering, by applying middleware  45, 46\npath parameters  10-12\npayload  228\nPCCS project\nsetting up  326\nPeewee  148\nPeewee, used for building repository\nabout  148\nCRUD transactions, implementing  153-155\nCRUD transactions, running  155\ndatabase connection, creating  149, 150\ndatabase driver, installing  148\ndomain layer, creating  150-153\ntables, creating  150-153\nPiccolo ORM\ndata models, creating  329, 330\nrepository layer, implementing  330\nusing  326-329\nPony ORM  140\nPony ORM, using for repository layer\nabout  140\nCRUD transactions, implementing  144-148\ndatabase connectivity, establishing  141\ndatabase driver, installing  140\nmodel classes, defining  141-144\nrepository transactions, running  148\ntables, creating  148\nprojects\nhuge projects, organizing  26, 27\nhuge projects, structuring  26, 27\norganizing, based on dependencies  62\nPrometheus  364\npsycopg2  110\npublish/subscribe messaging\nbuilding, with Apache Kafka  270\npydantic module  5\nPydantic @dataclass\nusing, to query documents  171-174\nPydantic validation\nusing  170\nPyMongo driver, applying for \nsynchronous connections\ndatabase connectivity, setting up  165, 166",
      "content_length": 1915,
      "extraction_method": "Direct"
    },
    {
      "page_number": 412,
      "chapter": null,
      "content": "Index\n389\nmodel layer, building  166, 167\nrepository layer, implementing  174\ntransactions, running  179-182\npytest command  323\npython-multipart module  5\nPython Virtual Machine (PVM)  69\nQ\nquery parameters  12, 13\nR\nRabbitMQ\nabout  269\nAMQP messaging, monitoring  269\nCelery instance, creating  269\ndownload link  269\nused, for building message-\ndriven transactions  269\nreactive programming\nabout  277\nAPI resources, accessing  281-283\napplying, in tasks  277\nbackground process, creating  280, 281\nObservable, creating with coroutines  278\nrepository factory methods  66, 67\nrepository layer  62-66\nrepository layer pattern\nbuilding  97-100\nrepository layer, PyMongo driver\nCRUD transactions, building  174-177\ndocument association, creating  177-179\nimplementing  174\nrepository-service layers\nmodel layer  63\nrepository factory methods  66, 67\nrepository layer  64, 66\nREST API  67, 68\nservice layer  67, 68\nrequest body  17-19\nrequest headers  19, 20\nrequests module\nabout  92\nusing  92, 93\nresponse data  20, 21\nresponse management\nJinja2 template engine, setting up  307\nORJSONResponse, using  310\nresponses, selecting  302-306\nstatic resources, setting up  307\ntemplate layout, creating  308, 309\nUJSONResponse, using  310\nresponse types\nEventSourceResponse  302\nFileResponse  302\nHTMLResponse  304\nJsonResponse  302\nPlainTextResponse  302\nRedirectResponse  302\nStreamingResponse  302-304\nREST API\nabout  6, 67, 68\ndesigning  6-8\nimplementing  6-8\nREST API services\nconsuming  90, 91\nS\nscope-based authorization\ncreating  231\nlogin transaction, implementing  233, 234\nOAuth2 class, customizing  231, 232\npermission dictionary, building  232, 233\nscopes, applying to endpoints  234-236",
      "content_length": 1694,
      "extraction_method": "Direct"
    },
    {
      "page_number": 413,
      "chapter": null,
      "content": "Index\n390\nsecret key  228\nserver response\nmanaging  10\nServer-Sent Events (SSE)  273, 302\nservice layer  62, 67, 68\nservice layer pattern\nbuilding  97-100\nservice registry\nsetting up  369\nsession management\nabout  288\napplying  288\nBaseHTTPMiddleware, \ncustomizing  293-295\nsession data, managing  291, 292\nsessions, removing  292\nuser sessions, creating  288-290\nshutdown event\ndefining  284\nSQLAlchemy\nabout  110\nasync CRUD transactions, \nimplementing  124\nCRUD transactions, creating  110, 116-118\nstarlette module  5\nstartup event\nabout  283\ndefining  283\nstatistical analysis\nperforming  337, 338\nstatus code  31\nsub-applications\ncreating  80, 81\nsubmodules\nmounting  81\nSwagger/OpenAPI dashboard  9\nsymbolic computations\nimplementing  331\nsymbolic expressions\ncreating  331, 332\nsynchronous connections\nPyMongo driver, applying for  165\nT\ntasks  257\nthird-party containers\nusing  69\nthread pool  254\nthread switching  254\ntiming attack  215\ntype hints  10\nU\nUJSONResponse\nusing  310\nuser requests\nmanaging  10\nV\nvirtual environment\nbenefits  362\nsetting up  362-364\nvisit module  26\nVisual Studio Code (VS Code)\nabout  4\nURL  4",
      "content_length": 1133,
      "extraction_method": "Direct"
    },
    {
      "page_number": 414,
      "chapter": null,
      "content": "Index\n391\nW\nWebSocket client\nimplementing  276, 277\nX\nXLSX report\ngenerating  338-342\nZ\nZookeeper server\nrunning  271",
      "content_length": 117,
      "extraction_method": "Direct"
    },
    {
      "page_number": 415,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    },
    {
      "page_number": 416,
      "chapter": null,
      "content": "Packt.com\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as \nindustry leading tools to help you plan your personal development and advance your career. For more \ninformation, please visit our website.\nWhy subscribe?\n•\t Spend less time learning and more time coding with practical eBooks and Videos from over \n4,000 industry professionals\n•\t Improve your learning with Skill Plans built especially for you\n•\t Get a free eBook or video every month\n•\t Fully searchable for easy access to vital information\n•\t Copy and paste, print, and bookmark content\nDid you know that Packt offers eBook versions of every book published, with PDF and ePub files \navailable? You can upgrade to the eBook version at packt.com and as a print book customer, you \nare entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub.\ncom for more details.\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of \nfree newsletters, and receive exclusive discounts and offers on Packt books and eBooks.",
      "content_length": 1094,
      "extraction_method": "Direct"
    },
    {
      "page_number": 417,
      "chapter": null,
      "content": "Other Books You May Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nBuilding Python Web APIs with FastAPI\nAbdulazeez Abdulazeez Adeshina\nISBN: 978-1-80107-663-0\n•\t Set up a FastAPI application that is fully functional and secure\n•\t Perform CRUD operations using SQL and FastAPI\n•\t Manage concurrency in FastAPI applications\n•\t Implement authentication in a FastAPI application\n•\t Deploy a FastAPI application to any platform",
      "content_length": 461,
      "extraction_method": "Direct"
    },
    {
      "page_number": 418,
      "chapter": null,
      "content": "Other Books You May Enjoy\n395\nPython Web Development with Sanic\nAdam Hopkins\nISBN: 978-1-80181-441-6\n•\t Understand the difference between WSGI, Async, and ASGI servers\n•\t Discover how Sanic organizes incoming data, why it does it, and how to make the most of it\n•\t Implement best practices for building reliable, performant, and secure web apps\n•\t Explore useful techniques for successfully testing and deploying a Sanic web app\n•\t Create effective solutions for the modern web, including task management, bot integration, \nand GraphQL",
      "content_length": 535,
      "extraction_method": "Direct"
    },
    {
      "page_number": 419,
      "chapter": null,
      "content": "396\nPackt is searching for authors like you\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and \napply today. We have worked with thousands of developers and tech professionals, just like you, to \nhelp them share their insight with the global tech community. You can make a general application, \napply for a specific hot topic that we are recruiting an author for, or submit your own idea.\nShare your thoughts\nNow you’ve finished Building Python Microservices with FastAPI, we’d love to hear your thoughts! \nIf you purchased the book from Amazon, please click here to go straight to the Amazon review page \nfor this book and share your feedback or leave a review on the site that you purchased it from.\nYour review is important to us and the tech community and will help us make sure we’re delivering \nexcellent quality content.",
      "content_length": 868,
      "extraction_method": "Direct"
    },
    {
      "page_number": 420,
      "chapter": null,
      "content": "",
      "content_length": 0,
      "extraction_method": "OCR"
    }
  ]
}